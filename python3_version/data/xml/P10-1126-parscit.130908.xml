<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015880">
<title confidence="0.890476">
How Many Words is a Picture Worth?
Automatic Caption Generation for News Images
</title>
<author confidence="0.992075">
Yansong Feng and Mirella Lapata
</author>
<affiliation confidence="0.999565">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.984811">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.998724">
Y.Feng-4@sms.ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999100066666667">
In this paper we tackle the problem of au-
tomatic caption generation for news im-
ages. Our approach leverages the vast re-
source of pictures available on the web
and the fact that many of them are cap-
tioned. Inspired by recent work in sum-
marization, we propose extractive and ab-
stractive caption generation models. They
both operate over the output of a proba-
bilistic image annotation model that pre-
processes the pictures and suggests key-
words to describe their content. Exper-
imental results show that an abstractive
model defined over phrases is superior to
extractive methods.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997296">
Recent years have witnessed an unprecedented
growth in the amount of digital information avail-
able on the Internet. Flickr, one of the best known
photo sharing websites, hosts more than three bil-
lion images, with approximately 2.5 million im-
ages being uploaded every day.1 Many on-line
news sites like CNN, Yahoo!, and BBC publish
images with their stories and even provide photo
feeds related to current events. Browsing and find-
ing pictures in large-scale and heterogeneous col-
lections is an important problem that has attracted
much interest within information retrieval.
Many of the search engines deployed on the
web retrieve images without analyzing their con-
tent, simply by matching user queries against col-
located textual information. Examples include
meta-data (e.g., the image’s file name and for-
mat), user-annotated tags, captions, and gener-
ally text surrounding the image. As this limits
the applicability of search engines (images that
</bodyText>
<footnote confidence="0.951639">
1http://www.techcrunch.com/2008/11/03/
three-billion-photos-at-flickr/
</footnote>
<bodyText confidence="0.999943214285714">
do not coincide with textual data cannot be re-
trieved), a great deal of work has focused on the
development of methods that generate description
words for a picture automatically. The literature
is littered with various attempts to learn the as-
sociations between image features and words us-
ing supervised classification (Vailaya et al., 2001;
Smeulders et al., 2000), instantiations of the noisy-
channel model (Duygulu et al., 2002), latent vari-
able models (Blei and Jordan, 2003; Barnard et al.,
2002; Wang et al., 2009), and models inspired by
information retrieval (Lavrenko et al., 2003; Feng
et al., 2004).
In this paper we go one step further and gen-
erate captions for images rather than individual
keywords. Although image indexing techniques
based on keywords are popular and the method of
choice for image retrieval engines, there are good
reasons for using more linguistically meaningful
descriptions. A list of keywords is often ambigu-
ous. An image annotated with the words blue,
sky, car could depict a blue car or a blue sky,
whereas the caption “car running under the blue
sky” would make the relations between the words
explicit. Automatic caption generation could im-
prove image retrieval by supporting longer and
more targeted queries. It could also assist journal-
ists in creating descriptions for the images associ-
ated with their articles. Beyond image retrieval, it
could increase the accessibility of the web for vi-
sually impaired (blind and partially sighted) users
who cannot access the content of many sites in
the same ways as sighted users can (Ferres et al.,
2006).
We explore the feasibility of automatic caption
generation in the news domain, and create descrip-
tions for images associated with on-line articles.
Obtaining training data in this setting does not re-
quire expensive manual annotation as many ar-
ticles are published together with captioned im-
ages. Inspired by recent work in summarization,
we propose extractive and abstractive caption gen-
</bodyText>
<page confidence="0.954419">
1239
</page>
<note confidence="0.942431">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239–1249,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999736466666666">
eration models. The backbone for both approaches
is a probabilistic image annotation model that sug-
gests keywords for an image. We can then simply
identify (and rank) the sentences in the documents
that share these keywords or create a new caption
that is potentially more concise but also informa-
tive and fluent. Our abstractive model operates
over image description keywords and document
phrases. Their combination gives rise to many
caption realizations which we select probabilisti-
cally by taking into account dependency and word
order constraints. Experimental results show that
the model’s output compares favorably to hand-
written captions and is often superior to extractive
methods.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999982298701299">
Although image understanding is a popular topic
within computer vision, relatively little work has
focused on the interplay between visual and lin-
guistic information. A handful of approaches gen-
erate image descriptions automatically following
a two-stage architecture. The picture is first ana-
lyzed using image processing techniques into an
abstract representation, which is then rendered
into a natural language description with a text gen-
eration engine. A common theme across differ-
ent models is domain specificity, the use of hand-
labeled data, and reliance on background ontolog-
ical information.
For example, H´ede et al. (2004) generate de-
scriptions for images of objects shot in uniform
background. Their system relies on a manually
created database of objects indexed by an image
signature (e.g., color and texture) and two key-
words (the object’s name and category). Images
are first segmented into objects, their signature is
retrieved from the database, and a description is
generated using templates. Kojima et al. (2002,
2008) create descriptions for human activities in
office scenes. They extract features of human mo-
tion and interleave them with a concept hierarchy
of actions to create a case frame from which a nat-
ural language sentence is generated. Yao et al.
(2009) present a general framework for generating
text descriptions of image and video content based
on image parsing. Specifically, images are hierar-
chically decomposed into their constituent visual
patterns which are subsequently converted into a
semantic representation using WordNet. The im-
age parser is trained on a corpus, manually an-
notated with graphs representing image structure.
A multi-sentence description is generated using a
document planner and a surface realizer.
Within natural language processing most previ-
ous efforts have focused on generating captions to
accompany complex graphical presentations (Mit-
tal et al., 1998; Corio and Lapalme, 1999; Fas-
ciano and Lapalme, 2000; Feiner and McKeown,
1990) or on using the captions accompanying in-
formation graphics to infer their intended mes-
sage, e.g., the author’s goal to convey ostensible
increase or decrease of a quantity of interest (Elzer
et al., 2005). Little emphasis is placed on image
processing; it is assumed that the data used to cre-
ate the graphics are available, and the goal is to
enable users understand the information expressed
in them.
The task of generating captions for news im-
ages is novel to our knowledge. Instead of relying
on manual annotation or background ontological
information we exploit a multimodal database of
news articles, images, and their captions. The lat-
ter is admittedly noisy, yet can be easily obtained
from on-line sources, and contains rich informa-
tion about the entities and events depicted in the
images and their relations. Similar to previous
work, we also follow a two-stage approach. Us-
ing an image annotation model, we first describe
the picture with keywords which are subsequently
realized into a human readable sentence. The
caption generation task bears some resemblance
to headline generation (Dorr et al., 2003; Banko
et al., 2000; Jin and Hauptmann, 2002) where the
aim is to create a very short summary for a doc-
ument. Importantly, we aim to create a caption
that not only summarizes the document but is also
a faithful to the image’s content (i.e., the caption
should also mention some of the objects or indi-
viduals depicted in the image). We therefore ex-
plore extractive and abstractive models that rely
on visual information to drive the generation pro-
cess. Our approach thus differs from most work in
summarization which is solely text-based.
</bodyText>
<sectionHeader confidence="0.983899" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999902875">
We formulate image caption generation as fol-
lows. Given an image I, and a related knowl-
edge database ic, create a natural language descrip-
tion C which captures the main content of the im-
age under ic. Specifically, in the news story sce-
nario, we will generate a caption C for an image I
and its accompanying document D. The training
data thus consists of document-image-caption tu-
</bodyText>
<page confidence="0.978791">
1240
</page>
<bodyText confidence="0.990495980769231">
Thousands of Tongans have
attended the funeral of King
Taufa’ahau Tupou IV, who
died last week at the age
of 88. Representatives
from 30 foreign countries
watched as the king’s coffin
was carried by 1,000 men
to the official royal burial
ground.
King Tupou, who was 88,
died a week ago.
Contaminated Cadbury’s
chocolate was the most
likely cause of an outbreak
of salmonella poisoning,
the Health Protection
Agency has said. About 36
out of a total of 56 cases of
the illness reported between
March and July could be
linked to the product.
Cadbury will increase its
contamination testing levels.
A Nasa satellite has doc-
umented startling changes
in Arctic sea ice cover be-
tween 2004 and 2005. The
extent of “perennial” ice
declined by 14%, losing an
area the size of Pakistan
or Turkey. The last few
decades have seen ice cover
shrink by about 0.7% per
year.
Satellite instruments can
distinguish “old” Arctic
ice from “new”.
A third of children in the
UK use blogs and social
network websites but two
thirds of parents do not
even know what they
are, a survey suggests.
The children’s charity
NCH said there was “an
alarming gap” in techno-
logical knowledge between
generations.
Children were found to be
far more internet-wise than
parents.
</bodyText>
<tableCaption confidence="0.996441">
Table 1: Each entry in the BBC News database contains a document an image, and its caption.
</tableCaption>
<bodyText confidence="0.999977307692308">
ples like the ones shown in Table 1. During test-
ing, we are given a document and an associated
image for which we must generate a caption.
Our experiments used the dataset created by
Feng and Lapata (2008).2 It contains 3,361 articles
downloaded from the BBC News website3 each of
which is associated with a captioned news image.
The latter is usually 203 pixels wide and 152 pix-
els high. The average caption length is 9.5 words,
the average sentence length is 20.5 words, and
the average document length 421.5 words. The
caption vocabulary is 6,180 words and the docu-
ment vocabulary is 26,795. The vocabulary shared
between captions and documents is 5,921 words.
The captions tend to use half as many words as
the document sentences, and more than 50% of the
time contain words that are not attested in the doc-
ument (even though they may be attested in the
collection).
Generating image captions is a challenging task
even for humans, let alone computers. Journalists
are given explicit instructions on how to write cap-
tions4 and laypersons do not always agree on what
a picture depicts (von Ahn and Dabbish, 2004).
Along with the title, the lead, and section head-
ings, captions are the most commonly read words
</bodyText>
<footnote confidence="0.998799333333333">
2Available from http://homepages.inf.ed.ac.uk/
s677528/data/
3http://news.bbc.co.uk/
4See http://www.theslot.com/captions.html and
http://www.thenewsmanual.net/ for tips on how to write
good captions.
</footnote>
<bodyText confidence="0.999957454545454">
in an article. A good caption must be succinct and
informative, clearly identify the subject of the pic-
ture, establish the picture’s relevance to the arti-
cle, provide context for the picture, and ultimately
draw the reader into the article. It is also worth
noting that journalists often write their own cap-
tions rather than simply extract sentences from the
document. In doing so they rely on general world
knowledge but also expertise in current affairs that
goes beyond what is described in the article or
shown in the picture.
</bodyText>
<sectionHeader confidence="0.98989" genericHeader="method">
4 Image Annotation
</sectionHeader>
<bodyText confidence="0.999991277777778">
As mentioned earlier, our approach relies on an
image annotation model to provide description
keywords for the picture. Our experiments made
use of the probabilistic model presented in Feng
and Lapata (2010). The latter is well-suited to our
task as it has been developed with noisy, multi-
modal data sets in mind. The model is based on the
assumption that images and their surrounding text
are generated by mixtures of latent topics which
are inferred from a concatenated representation of
words and visual features.
Specifically, images are preprocessed so that
they are represented by word-like units. Lo-
cal image descriptors are computed using the
Scale Invariant Feature Transform (SIFT) algo-
rithm (Lowe, 1999). The general idea behind the
algorithm is to first sample an image with the
difference-of-Gaussians point detector at different
</bodyText>
<page confidence="0.942481">
1241
</page>
<bodyText confidence="0.99971315625">
scales and locations. Importantly, this detector is,
to some extent, invariant to translation, scale, ro-
tation and illumination changes. Each detected re-
gion is represented with a SIFT descriptor which
is a histogram of edge directions at different lo-
cations. Subsequently SIFT descriptors are quan-
tized into a discrete set of visual terms via a clus-
tering algorithm such as K-means.
The model thus works with a bag-of-words rep-
resentation and treats each article-image-caption
tuple as a single document dMix consisting of tex-
tual and visual words. Latent Dirichlet Allocation
(LDA, Blei et al. 2003) is used to infer the latent
topics assumed to have generated dMix. The ba-
sic idea underlying LDA, and topic models in gen-
eral, is that each document is composed of a prob-
ability distribution over topics, where each topic
represents a probability distribution over words.
The document-topic and topic-word distributions
are learned automatically from the data and pro-
vide information about the semantic themes cov-
ered in each document and the words associated
with each semantic theme. The image annotation
model takes the topic distributions into account
when finding the most likely keywords for an im-
age and its associated document.
More formally, given an image-caption-
document tuple (I,C,D) the model finds the
subset of keywords WI (WI C W) which appro-
priately describe I. Assuming that keywords
are conditionally independent, and I, D are
represented jointly by dMix, the model estimates:
</bodyText>
<equation confidence="0.961544">
WI Pz� argmax fl P(wt|dMix) (1)
Wt wtEWt
= argmax H
Wt wtEWt
</equation>
<bodyText confidence="0.999755904761905">
Wt denotes a set of description keywords (the sub-
script t is used to discriminate from the visual
words which are not part of the model’s output),
K the number of topics, P(wt|zk) the multimodal
word distributions over topics, and P(zk|dMix) the
estimated posterior of the topic proportions over
documents. Given an unseen image-document
pair and trained multimodal word distributions
over topics, it is possible to infer the posterior of
topic proportions over the new data by maximizing
the likelihood. The model delivers a ranked list of
textual words wt, the n-best of which are used as
annotations for image I.
It is important to note that the caption gener-
ation models we propose are not especially tied
to the above annotation model. Any probabilis-
tic model with broadly similar properties could
serve our purpose. Examples include PLSA-based
approaches to image annotation (e.g., Monay
and Gatica-Perez 2007) and correspondence LDA
(Blei and Jordan, 2003).
</bodyText>
<sectionHeader confidence="0.988432" genericHeader="method">
5 Extractive Caption Generation
</sectionHeader>
<bodyText confidence="0.9996636875">
Much work in summarization to date focuses on
sentence extraction where a summary is created
simply by identifying and subsequently concate-
nating the most important sentences in a docu-
ment. Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range of
documents, independently of style, text type, and
subject matter. For our caption generation task, we
need only extract a single sentence. And our guid-
ing hypothesis is that this sentence must be max-
imally similar to the description keywords gener-
ated by the annotation model. We discuss below
different ways of operationalizing similarity.
Word Overlap Perhaps the simplest way of
measuring the similarity between image keywords
and document sentences is word overlap:
</bodyText>
<equation confidence="0.9993675">
Overlap(WI,Sd) = |WI nSd |(2)
|WI � Sd|
</equation>
<bodyText confidence="0.9934658">
where WI is the set of keywords and Sd a sentence
in the document. The caption is then the sentence
that has the highest overlap with the keywords.
Cosine Similarity Word overlap is admittedly
a naive measure of similarity, based on lexical
identity. We can overcome this by representing
keywords and sentences in vector space (Salton
and McGill, 1983). The latter is a word-sentence
co-occurrence matrix where each row represents
a word, each column a sentence, and each en-
try the frequency with which the word appeared
within the sentence. More precisely matrix cells
are weighted by their tf-idf values. The similarity
WI �� and
suring the cosine of their angle:
Probabilistic Similarity Recall that the back-
bone of our image annotation model is a topic
model with images and documents represented as
a probability distribution over latent topics. Un-
der this framework, the similarity between an im-
</bodyText>
<figure confidence="0.881976214285714">
�� Sd) =
WI �� -
����� ��Sd|
|WI||
��
Sd
sim(WI,
(3)
K
E P(wt|zk)P(zk|dMix)
k=1
of the vectors representing the keywords
��
document sentence Sd can be quantified by mea-
</figure>
<page confidence="0.969652">
1242
</page>
<bodyText confidence="0.9996468">
age and a sentence can be broadly measured by the
extent to which they share the same topic distribu-
tions (Steyvers and Griffiths, 2007). For example,
we may use the KL divergence to measure the dif-
ference between the distributions p and q:
</bodyText>
<equation confidence="0.981962">
D(p,q) =
</equation>
<bodyText confidence="0.955059">
where p and q are shorthand for the image
topic distribution PdMix and sentence topic distri-
bution PSd, respectively. When doing inference on
the document sentence, we also take its neighbor-
ing sentences into account to avoid estimating in-
accurate topic proportions on short sentences.
The KL divergence is asymmetric and in many
applications, it is preferable to apply a symmet-
ric measure such as the Jensen Shannon (JS) di-
vergence. The latter measures the “distance” be-
tween p and q through (p+q)
2 , the average of p
and q:
JS(p, q) = 2 [D (p, (p 2 q)) +D(q, (p 2 q) )] (5)
</bodyText>
<sectionHeader confidence="0.976661" genericHeader="method">
6 Abstractive Caption Generation
</sectionHeader>
<bodyText confidence="0.999759911764706">
Although extractive methods yield grammatical
captions and require relatively little linguistic
analysis, there are a few caveats to consider.
Firstly, there is often no single sentence in the doc-
ument that uniquely describes the image’s content.
In most cases the keywords are found in the doc-
ument but interspersed across multiple sentences.
Secondly, the selected sentences make for long
captions (sometimes longer than the average doc-
ument sentence), are not concise and overall not
as catchy as human-written captions. For these
reasons we turn to abstractive caption generation
and present models based on single words but also
phrases.
Word-based Model Our first abstractive model
builds on and extends a well-known probabilistic
model of headline generation (Banko et al., 2000).
The task is related to caption generation, the aim is
to create a short, title-like headline for a given doc-
ument, without however taking visual information
into account. Like captions, headlines have to be
catchy to attract the reader’s attention.
Banko et al. (2000) propose a bag-of-words
model for headline generation. It consists of con-
tent selection and surface realization components.
Content selection is modeled as the probability of
a word appearing in the headline given the same
word appearing in the corresponding document
and is independent from other words in the head-
line. The likelihood of different surface realiza-
tions is estimated using a bigram model. They also
take the distribution of the length of the headlines
into account in an attempt to bias the model to-
wards generating concise output:
</bodyText>
<equation confidence="0.996834142857143">
P(w1, w2, ..., wn) = 11
i=1
�n7 P(wi ∈ H|wi ∈ D) (6)
·P(len(H) = n)
n
· n P(wi|wi−1)
i=2
</equation>
<bodyText confidence="0.999831818181818">
where wi is a word that may appear in head-
line H, D the document being summarized,
and P(len(H) = n) a headline length distribution
model.
The above model can be easily adapted to the
caption generation task. Content selection is now
the probability of a word appearing in the cap-
tion given the image and its associated document
which we obtain from the output of our image an-
notation model (see Section 4). In addition we re-
place the bigram surface realizer with a trigram:
</bodyText>
<equation confidence="0.996338375">
P(w1 , w2, ..., wn) = 11
i=1
�n7 P(wi ∈ C|I,D) (7)
·P(len(C) = n)
·n
n
P(wi|wi−1,wi−2)
i=3
</equation>
<bodyText confidence="0.999931578947368">
where C is the caption, I the image, D the accom-
panying document, and P(wi ∈ C|I,D) the image
annotation probability.
Despite its simplicity, the caption generation
model in (7) has a major drawback. The content
selection component will naturally tend to ignore
function words, as they are not descriptive of the
image’s content. This will seriously impact the
grammaticality of the generated captions, as there
will be no appropriate function words to glue the
content words together. One way to remedy this
is to revert to a content selection model that ig-
nores the image and simply estimates the prob-
ability of a word appearing in the caption given
the same word appearing in the document. At the
same time we modify our surface realization com-
ponent so that it takes note of the image annotation
probabilities. Specifically, we use an adaptive lan-
guage model (Kneser et al., 1997) that modifies an
</bodyText>
<equation confidence="0.991154208333334">
pj
pj log2 qj
K
�
j=1
(4)
1243
n-gram model with local unigram probabilities:
n
P(w1,w2,...,wn) = ∏ P(wi ∈ C|wi ∈ D) (8)
i=1
·P(len(C) = n)
n
· ∏ Padap(wi|wi−1,wi−2)
i=3
tion (8) as follows:
P(ρ1,ρ2,...,ρm) ≈
P(ρj ∈ C|ρj ∈ D) (12)
m
·P(len(C) = ∑ len(ρj))
j=1
m
∏
j=1
</equation>
<bodyText confidence="0.99994">
where P(wi ∈C|wi ∈ D) is the probability of wi ap-
pearing in the caption given that it appears in
the document D, and Padap(wi|wi−1,wi−2) the lan-
guage model adapted with probabilities from our
image annotation model:
</bodyText>
<equation confidence="0.993484125">
α(w)
Padap(w|h) Pback(w|h)
= z(h)
α(w) ≈ (Padap(w)
)β
Pback(w)
z(h) = ∑ α(w) · Pback(w|h)
w
</equation>
<bodyText confidence="0.9999122">
where Pback(w|h) is the probability of w given
the history h of preceding words (i.e., the orig-
inal trigram model), Padap(w) the probability
of w according to the image annotation model,
Pback(w) the probability of w according to the orig-
inal model, and β a scaling parameter.
Phrase-based Model The model outlined in
equation (8) will generate captions with function
words. However, there is no guarantee that these
will be compatible with their surrounding context
or that the caption will be globally coherent be-
yond the trigram horizon. To avoid these prob-
lems, we turn our attention to phrases which are
naturally associated with function words and can
potentially capture long-range dependencies.
Specifically, we obtain phrases from the out-
put of a dependency parser. A phrase is sim-
ply a head and its dependents with the exception
of verbs, where we record only the head (other-
wise, an entire sentence could be a phrase). For
example, from the first sentence in Table 1 (first
row, left document) we would extract the phrases:
thousands of Tongans, attended, the funeral, King
Taufa‘ahau Tupou IV, last weep at the age, died,
and so on. We only consider dependencies whose
heads are nouns, verbs, and prepositions, as these
constitute 80% of all dependencies attested in our
caption data. We define a bag-of-phrases model
for caption generation by modifying the content
selection and caption length components in equa-
</bodyText>
<equation confidence="0.960296">
∑mj=1 len(ρj)
· ∏ Padap(wi|wi−1,wi−2)
i=3
</equation>
<bodyText confidence="0.995861333333333">
Here, P(ρj ∈ C|ρj ∈ D) models the probability of
phrase ρj appearing in the caption given that it also
appears in the document and is estimated as:
</bodyText>
<equation confidence="0.982199">
P(ρj ∈ C|ρj ∈ D) = ∏ P(wj ∈ C|wj ∈ D) (13)
wj∈ρj
</equation>
<bodyText confidence="0.995428083333333">
where wj is a word in the phrase ρj.
One problem with the models discussed thus
far is that words or phrases are independent of
each other. It is up to the trigram model to en-
force coarse ordering constraints. These may be
sufficient when considering isolated words, but
phrases are longer and their combinations are sub-
ject to structural constraints that are not captured
by sequence models. We therefore attempt to take
phrase attachment constraints into account by es-
timating the probability of phrase ρj attaching to
the right of phrase ρi as:
</bodyText>
<equation confidence="0.997671">
P(ρj|ρi)= ∑ ∑ p(wj|wi) (14)
wi∈ρi wj∈ρj
∑{ f (wi,wj)
f (wi,−) + f (wi,wj)
f (−,wj) }
wj∈ρj
</equation>
<bodyText confidence="0.9994905">
where p(wj|wi) is the probability of a phrase con-
taining word wj appearing to the right of a phrase
containing word wi, f (wi,wj) indicates the num-
ber of times wi and wj are adjacent, f (wi,−) is
the number of times wi appears on the left of any
phrase, and f (−,wi) the number of times it ap-
pears on the right.5
After integrating the attachment probabilities
into equation (12), the caption generation model
becomes:
</bodyText>
<equation confidence="0.992790473684211">
P(ρj ∈ C|ρj ∈ D) (15)
P(ρj|ρj−1)
·P(len(C) = ∑mj=1len(ρj))
m
∑ len(ρj)
·∏ j=1 Padap(wi|wi−1,wi−2)
i=3
5Equation (14) is smoothed to avoid zero probabilities.
1
2 ∑
wi∈ρi
=
P(ρ1,ρ2,...,ρm) ≈
m
· ∏
j=2
m
∏
j=1
</equation>
<page confidence="0.92251">
1244
</page>
<bodyText confidence="0.999972392857143">
On the one hand, the model in equation (15) takes
long distance dependency constraints into ac-
count, and has some notion of syntactic structure
through the use of attachment probabilities. On
the other hand, it has a primitive notion of caption
length estimated by P(len(C) = Emj=1 len(pj)) and
will therefore generate captions of the same
(phrase) length. Ideally, we would like the model
to vary the length of its output depending on the
chosen context. However, we leave this to future
work.
Search To generate a caption it is neces-
sary to find the sequence of words that maxi-
mizes P(w1,w2,...,wn) for the word-based model
(equation (8)) and P(p1,p2,...,pm) for the
phrase-based model (equation (15)). We rewrite
both probabilities as the weighted sum of their log
form components and use beam search to find a
near-optimal sequence. Note that we can make
search more efficient by reducing the size of the
document D. Using one of the models from Sec-
tion 5, we may rank its sentences in terms of
their relevance to the image keywords and con-
sider only the n-best ones. Alternatively, we could
consider the single most relevant sentence together
with its surrounding context under the assumption
that neighboring sentences are about the same or
similar topics.
</bodyText>
<sectionHeader confidence="0.997127" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999985186440678">
In this section we discuss our experimental design
for assessing the performance of the caption gen-
eration models presented above. We give details
on our training procedure, parameter estimation,
and present the baseline methods used for com-
parison with our models.
Data All our experiments were conducted on
the corpus created by Feng and Lapata (2008),
following their original partition of the data
(2,881 image-caption-document tuples for train-
ing, 240 tuples for development and 240 for test-
ing). Documents and captions were parsed with
the Stanford parser (Klein and Manning, 2003) in
order to obtain dependencies for the phrase-based
abstractive model.
Model Parameters For the image annotation
model we extracted 150 (on average) SIFT fea-
tures which were quantized into 750 visual
terms. The underlying topic model was trained
with 1,000 topics using only content words
(i.e., nouns, verbs, and adjectives) that appeared
no less than five times in the corpus. For all
models discussed here (extractive and abstractive)
we report results with the 15 best annotation key-
words. For the abstractive models, we used a
trigram model trained with the SRI toolkit on a
newswire corpus consisting of BBC and Yahoo!
news documents (6.9 M words). The attachment
probabilities (see equation (14)) were estimated
from the same corpus. We tuned the caption
length parameter on the development set using a
range of [5,14] tokens for the word-based model
and [2,5] phrases for the phrase-based model. Fol-
lowing Banko et al. (2000), we approximated the
length distribution with a Gaussian. The scaling
parameter R for the adaptive language model was
also tuned on the development set using a range
of [0.5,0.9]. We report results with R set to 0.5.
For the abstractive models the beam size was set
to 500 (with at least 50 states for the word-based
model). For the phrase-based model, we also ex-
perimented with reducing the search scope, ei-
ther by considering only the n most similar sen-
tences to the keywords (range [2,10]), or simply
the single most similar sentence and its neighbors
(range [2,5]). The former method delivered better
results with 10 sentences (and the KL divergence
similarity function).
Evaluation We evaluated the performance of
our models automatically, and also by eliciting hu-
man judgments. Our automatic evaluation was
based on Translation Edit Rate (TER, Snover et al.
2006), a measure commonly used to evaluate the
quality of machine translation output. TER is de-
fined as the minimum number of edits a human
would have to perform to change the system out-
put so that it exactly matches a reference transla-
tion. In our case, the original captions written by
the BBC journalists were used as reference:
</bodyText>
<equation confidence="0.988627333333333">
Ins + Del + Sub + Shft
TER(E,Er) = (16)
Nr
</equation>
<bodyText confidence="0.999779818181818">
where E is the hypothetical system output, Er the
reference caption, and Nr the reference length.
The number of possible edits include insertions
(Ins), deletions (Del), substitutions (Sub) and
shifts (Shft). TER is similar to word error rate,
the only difference being that it allows shifts. A
shift moves a contiguous sequence to a different
location within the the same system output and is
counted as a single edit. The perfect TER score
is 0, however note that it can be higher than 1 due
to insertions. The minimum translation edit align-
</bodyText>
<page confidence="0.961158">
1245
</page>
<table confidence="0.998886">
Model TER AvgLen
Lead sentence 2.12† 21.0
Word Overlap 2.46*† 24.3
Cosine 2.26† 22.0
KL Divergence 1.77*† 18.4
JS Divergence 1.77*† 18.6
Abstract Words 1.11*† 10.0
Abstract Phrases 1.06*† 10.1
</table>
<tableCaption confidence="0.97634125">
Table 2: TER results for extractive, abstractive
models, and lead sentence baseline; *: sig. dif-
ferent from lead sentence; †: sig. different from
KL and JS divergence.
</tableCaption>
<bodyText confidence="0.999591826086956">
ment is usually found through beam search. We
used TER to compare the output of our extractive
and abstractive models and also for parameter tun-
ing (see the discussion above).
In our human evaluation study participants were
presented with a document, an associated image,
and its caption, and asked to rate the latter on two
dimensions: grammaticality (is the sentence flu-
ent or word salad?) and relevance (does it de-
scribe succinctly the content of the image and doc-
ument?). We used a 1–7 rating scale, participants
were encouraged to give high ratings to captions
that were grammatical and appropriate descrip-
tions of the image given the accompanying docu-
ment. We randomly selected 12 document-image
pairs from the test set and generated captions for
them using the best extractive system, and two ab-
stractive systems (word-based and phrase-based).
We also included the original human-authored
caption as an upper bound. We collected ratings
from 23 unpaid volunteers, all self reported native
English speakers. The study was conducted over
the Internet.
</bodyText>
<sectionHeader confidence="0.999526" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.9997545">
Table 2 reports our results on the test set us-
ing TER. We compare four extractive models
based on word overlap, cosine similarity, and two
probabilistic similarity measures, namely KL and
JS divergence and two abstractive models based
on words (see equation (8)) and phrases (see equa-
tion (15)). We also include a simple baseline that
selects the first document sentence as a caption
and show the average caption length (AvgLen) for
each model. We examined whether performance
differences among models are statistically signifi-
cant, using the Wilcoxon test.
</bodyText>
<table confidence="0.9993232">
Model Grammaticality Relevance
KL Divergence 6.42*† 4.10*†
Abstract Words 2.08† 3.20†
Abstract Phrases 4.80* 4.96*
Gold Standard 6.39*† 5.55*
</table>
<tableCaption confidence="0.999797">
Table 3: Mean ratings on caption output elicited
</tableCaption>
<bodyText confidence="0.999385275">
by humans; *: sig. different from word-
based abstractive system; †: sig. different from
phrase-based abstractive system.
As can be seen the probabilistic models (KL and
JS divergence) outperform word overlap and co-
sine similarity (all differences are statistically sig-
nificant, p &lt; 0.01).6 They make use of the same
topic model as the image annotation model, and
are thus able to select sentences that cover com-
mon content. They are also significantly better
than the lead sentence which is a competitive base-
line. It is well known that news articles are written
so that the lead contains the most important infor-
mation in a story.7 This is an encouraging result
as it highlights the importance of the visual infor-
mation for the caption generation task. In general,
word overlap is the worst performing model which
is not unexpected as it does not take any lexical
variation into account. Cosine is slightly better
but not significantly different from the lead sen-
tence. The abstractive models obtain the best TER
scores overall, however they generate shorter cap-
tions in comparison to the other models (closer to
the length of the gold standard) and as a result TER
treats them favorably, simply because the number
of edits is less. For this reason we turn to the re-
sults of our judgment elicitation study which as-
sesses in more detail the quality of the generated
captions.
Recall that participants judge the system out-
put on two dimensions, grammaticality and rele-
vance. Table 3 reports mean ratings for the out-
put of the extractive system (based on the KL di-
vergence), the two abstractive systems, and the
human-authored gold standard caption. We per-
formed an Analysis of Variance (ANOVA) to ex-
amine the effect of system type on the generation
task. Post-hot Tukey tests were carried out on the
mean of the ratings shown in Table 3 (for gram-
maticality and relevance).
</bodyText>
<footnote confidence="0.99838025">
6We also note that mean length differences are not signif-
icant among these models.
7As a rule of thumb the lead should answer most or all of
the five W’s (who, what, when, where, why).
</footnote>
<page confidence="0.914416">
1246
</page>
<table confidence="0.998201206896552">
G: King Tupou, who was 88, died a week ago.
KL: Last year, thousands of Tongans took part in unprece-
dented demonstrations to demand greater democracy
and public ownership of key national assets.
AW: King Toupou IV died at the age of Tongans last week.
AP: King Toupou IV died at the age of 88 last week.
G: Cadbury will increase its contamination testing levels.
KL: Contaminated Cadbury’s chocolate was the most
likely cause of an outbreak of salmonella poisoning,
the Health Protection Agency has said.
AW: Purely dairy milk buttons Easter had agreed to work
has caused.
AP: The 105g dairy milk buttons Easter egg affected by
the recall.
G: Satellite instruments can distinguish “old” Arctic ice
from “new”.
KL: So a planet with less ice warms faster, potentially turn-
ing the projected impacts of global warming into real-
ity sooner than anticipated.
AW: Dr less winds through ice cover all over long time
when.
AP: The area of the Arctic covered in Arctic sea ice cover.
G: Children were found to be far more internet-wise than
parents.
KL: That’s where parents come in.
AW: The survey found a third of children are about mobile
phones.
AP: The survey found a third of children in the driving
seat.
</table>
<tableCaption confidence="0.700017666666667">
Table 4: Captions written by humans (G) and gen-
erated by extractive (KL), word-based abstractive
(AW), and phrase-based extractive (AP systems).
</tableCaption>
<bodyText confidence="0.999987041666667">
The word-based system yields the least gram-
matical output. It is significantly worse than the
phrase-based abstractive system (a &lt; 0.01), the
extractive system (a &lt; 0.01), and the gold stan-
dard (a &lt; 0.01). Unsurprisingly, the phrase-based
system is significantly less grammatical than the
gold standard and the extractive system, whereas
the latter is perceived as equally grammatical as
the gold standard (the difference in the means is
not significant). With regard to relevance, the
word-based system is significantly worse than the
phrase-based system, the extractive system, and
the gold-standard. Interestingly, the phrase-based
system performs on the same level with the hu-
man gold standard (the difference in the means is
not significant) and significantly better than the ex-
tractive system. Overall, the captions generated by
the phrase-based system, capture the same content
as the human-authored captions, even though they
tend to be less grammatical. Examples of system
output for the image-document pairs shown in Ta-
ble 1 are given in Table 4 (the first row corresponds
to the left picture (top row) in Table 1, the second
row to the right picture, and so on).
</bodyText>
<sectionHeader confidence="0.994381" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999998166666667">
We have presented extractive and abstractive mod-
els that generate image captions for news articles.
A key aspect of our approach is to allow both
the visual and textual modalities to influence the
generation task. This is achieved through an im-
age annotation model that characterizes pictures
in terms of description keywords that are subse-
quently used to guide the caption generation pro-
cess. Our results show that the visual information
plays an important role in content selection. Sim-
ply extracting a sentence from the document often
yields an inferior caption. Our experiments also
show that a probabilistic abstractive model defined
over phrases yields promising results. It generates
captions that are more grammatical than a closely
related word-based system and manages to capture
the gist of the image (and document) as well as the
captions written by journalists.
Future extensions are many and varied. Rather
than adopting a two-stage approach, where the im-
age processing and caption generation are carried
out sequentially, a more general model should in-
tegrate the two steps in a unified framework. In-
deed, an avenue for future work would be to de-
fine a phrase-based model for both image annota-
tion and caption generation. We also believe that
our approach would benefit from more detailed
linguistic and non-linguistic information. For in-
stance, we could experiment with features related
to document structure such as titles, headings, and
sections of articles and also exploit syntactic infor-
mation more directly. The latter is currently used
in the phrase-based model by taking attachment
probabilities into account. We could, however, im-
prove grammaticality more globally by generating
a well-formed tree (or dependency graph).
</bodyText>
<sectionHeader confidence="0.997898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997959416666667">
Banko, Michel, Vibhu O. Mittal, and Micheael J.
Witbrock. 2000. Headline generation based on
statistical translation. In Proceedings of the 38th
Annual Meeting on Association for Computa-
tional Linguistics. Hong Kong, pages 318–325.
Barnard, Kobus, Pinar Duygulu, David Forsyth,
Nando de Freitas, David Blei, and Michael
Jordan. 2002. Matching words and pictures.
Journal of Machine Learning Research 3:1107–
1135.
Blei, David and Michael Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th An-
</reference>
<page confidence="0.916301">
1247
</page>
<reference confidence="0.998310722772278">
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval. Toronto, ON, pages 127–134.
Blei, David, Andrew Ng, and Michael Jordan.
2003. Latent Dirichlet allocation. Journal of
Machine Learning Research 3:993–1022.
Corio, Marc and Guy Lapalme. 1999. Generation
of texts for information graphics. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation. Toulouse, France, pages
49–58.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim ap-
proach to headline generation. In Proceed-
ings of the HLT-NAACL 2003 Workshop on Text
Summarization. Edmonton, Canada, pages 1–8.
Duygulu, Pinar, Kobus Barnard, Nando de Freitas,
and David Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a
fixed image vocabulary. In Proceedings of the
7th European Conference on Computer Vision.
Copenhagen, Denmark, pages 97–112.
Elzer, Stephanie, Sandra Carberry, Ingrid Zuker-
man, Daniel Chester, Nancy Green, , and Seniz
Demir. 2005. A probabilistic framework for rec-
ognizing intention in information graphics. In
Proceedings of the 19th International Confer-
ence on Artificial Intelligence. Edinburgh, Scot-
land, pages 1042–1047.
Fasciano, Massimo and Guy Lapalme. 2000. In-
tentions in the coordinated generation of graph-
ics and text from tabular data. Knowledge In-
formation Systems 2(3):310–339.
Feiner, Steven and Kathleen McKeown. 1990. Co-
ordinating text and graphics in explanation gen-
eration. In Proceedings of National Conference
on Artificial Intelligence. Boston, MA, pages
442–449.
Feng, Shaolei Feng, Victor Lavrenko, and R Man-
matha. 2004. Multiple Bernoulli relevance
models for image and video annotation. In
Proceedings of the International Conference
on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002–1009.
Feng, Yansong and Mirella Lapata. 2008. Au-
tomatic image annotation using auxiliary text
information. In Proceedings of the 46th An-
nual Meeting of the Association of Computa-
tional Linguistics: Human Language Technolo-
gies. Columbus, OH, pages 272–280.
Feng, Yansong and Mirella Lapata. 2010. Topic
models for image annotation and text illustra-
tion. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics. Los
Angeles, LA.
Ferres, Leo, Avi Parush, Shelley Roberts, and
Gitte Lindgaard. 2006. Helping people with
visual impairments gain access to graphical in-
formation through natural language: The graph
system. In Proceedings of 11th International
Conference on Computers Helping People with
Special Needs. Linz, Austria, pages 1122–1130.
H´ede, Patrick, Pierre Allain Mo¨ellic, Jo¨el Bour-
geoys, Magali Joint, and Corinne Thomas.
2004. Automatic generation of natural lan-
guage descriptions for images. In Proceed-
ings of Computer-Assisted Information Re-
trieval (Recherche d’Information et ses Appli-
cations Ordinateur) (RIAO). Avignon, France.
Jin, Rong and Alexander G. Hauptmann. 2002. A
new probabilistic model for title generation. In
Proceedings of the 19th International Confer-
ence on Computational linguistics. Taipei, Tai-
wan, pages 1–7.
Klein, Dan and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting of the Association
of Computational Linguistics. Sapporo, Japan,
pages 423–430.
Kneser, Reinhard, Jochen Peters, and Dietrich
Klakow. 1997. Language model adaptation
using dynamic marginals. In Proceedings of
5th European Conference on Speech Commu-
nication and Technology. Rhodes, Greece, vol-
ume 4, pages 1971–1974.
Kojima, Atsuhiro, Mamoru Takaya, Shigeki Aoki,
Takao Miyamoto, and Kunio Fukunaga. 2008.
Recognition and textual description of human
activities by mobile robot. In Proceedings of
the 3rd International Conference on Innova-
tive Computing Information and Control. IEEE
Computer Society, Washington, DC, pages 53–
56.
Kojima, Atsuhiro, Takeshi Tamura, and Kunio
Fukunaga. 2002. Natural language description
of human activities from video images based
on concept hierarchy of actions. International
Journal of Computer Vision 50(2):171–184.
Lavrenko, Victor, R. Manmatha, and Jiwoon Jeon.
2003. A model for learning the semantics of
</reference>
<page confidence="0.78653">
1248
</page>
<reference confidence="0.992511727272727">
pictures. In Proceedings of the 16th Conference
on Advances in Neural Information Processing
Systems. Vancouver, BC.
Lowe, David G. 1999. Object recognition from
local scale-invariant features. In Proceedings of
International Conference on Computer Vision.
IEEE Computer Society, pages 1150–1157.
Mittal, Vibhu O., Johanna D. Moore, Giuseppe
Carenini, and Steven Roth. 1998. Describing
complex charts in natural language: A caption
generation system. Computational Linguistics
24:431–468.
Monay, Florent and Daniel Gatica-Perez. 2007.
Modeling semantic aspects for cross-media
image indexing. IEEE Transactions on
Pattern Analysis and Machine Intelligence
29(10):1802–1817.
Salton, Gerard and M.J. McGill. 1983. In-
troduction to Modern Information Retrieval.
McGraw-Hill, New York.
Smeulders, Arnols W.M., Marcel Worring, Si-
mone Santini, Amarnath Gupta, and Ramesh
Jain. 2000. Content-based image retrieval at
the end of the early years. IEEE Transactions
on Pattern Analysis and Machine Intelligence
22(12):1349–1380.
Snover, Matthew, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas. Cambridge, pages 223–
231.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Vailaya, Aditya, M´ario A. T. Figueiredo, Anil K.
Jain, and Hong-Jiang Zhang. 2001. Image clas-
sification for content-based indexing. IEEE
Transactions on Image Processing 10:117–130.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
images with a computer game. In ACM Confer-
ence on Human Factors in Computing Systems.
New York, NY, pages 319–326.
Wang, Chong, David Blei, and Li Fei-Fei. 2009.
Simultaneous image classification and annota-
tion. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recog-
nition. Miami, FL, pages 1903–1910.
Yao, Benjamin, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song chun Zhu. 2009. I2t: Image pars-
ing to text description. Proceedings of IEEE (in-
vited for the special issue on Internet Vision) .
</reference>
<page confidence="0.994898">
1249
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963666">
<title confidence="0.9989655">How Many Words is a Picture Worth? Automatic Caption Generation for News Images</title>
<author confidence="0.984418">Feng Lapata</author>
<affiliation confidence="0.999965">School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.999422">10 Crichton Street, Edinburgh EH8 9AB, UK</address>
<abstract confidence="0.9988411875">In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in sumwe propose abgeneration models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michel Banko</author>
<author>Vibhu O Mittal</author>
<author>Micheael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. Hong Kong,</booktitle>
<pages>318--325</pages>
<contexts>
<context position="7870" citStr="Banko et al., 2000" startWordPosition="1219" endWordPosition="1222">or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an </context>
<context position="19017" citStr="Banko et al., 2000" startWordPosition="3054" endWordPosition="3057">no single sentence in the document that uniquely describes the image’s content. In most cases the keywords are found in the document but interspersed across multiple sentences. Secondly, the selected sentences make for long captions (sometimes longer than the average document sentence), are not concise and overall not as catchy as human-written captions. For these reasons we turn to abstractive caption generation and present models based on single words but also phrases. Word-based Model Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al., 2000). The task is related to caption generation, the aim is to create a short, title-like headline for a given document, without however taking visual information into account. Like captions, headlines have to be catchy to attract the reader’s attention. Banko et al. (2000) propose a bag-of-words model for headline generation. It consists of content selection and surface realization components. Content selection is modeled as the probability of a word appearing in the headline given the same word appearing in the corresponding document and is independent from other words in the headline. The likel</context>
<context position="27708" citStr="Banko et al. (2000)" startWordPosition="4542" endWordPosition="4545">adjectives) that appeared no less than five times in the corpus. For all models discussed here (extractive and abstractive) we report results with the 15 best annotation keywords. For the abstractive models, we used a trigram model trained with the SRI toolkit on a newswire corpus consisting of BBC and Yahoo! news documents (6.9 M words). The attachment probabilities (see equation (14)) were estimated from the same corpus. We tuned the caption length parameter on the development set using a range of [5,14] tokens for the word-based model and [2,5] phrases for the phrase-based model. Following Banko et al. (2000), we approximated the length distribution with a Gaussian. The scaling parameter R for the adaptive language model was also tuned on the development set using a range of [0.5,0.9]. We report results with R set to 0.5. For the abstractive models the beam size was set to 500 (with at least 50 states for the word-based model). For the phrase-based model, we also experimented with reducing the search scope, either by considering only the n most similar sentences to the keywords (range [2,10]), or simply the single most similar sentence and its neighbors (range [2,5]). The former method delivered b</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Banko, Michel, Vibhu O. Mittal, and Micheael J. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. Hong Kong, pages 318–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Pinar Duygulu</author>
<author>David Forsyth</author>
<author>Nando de Freitas</author>
<author>David Blei</author>
<author>Michael Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>1135</pages>
<marker>Barnard, Duygulu, Forsyth, de Freitas, Blei, Jordan, 2002</marker>
<rawString>Barnard, Kobus, Pinar Duygulu, David Forsyth, Nando de Freitas, David Blei, and Michael Jordan. 2002. Matching words and pictures. Journal of Machine Learning Research 3:1107– 1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Michael Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<pages>127--134</pages>
<location>Toronto, ON,</location>
<contexts>
<context position="2339" citStr="Blei and Jordan, 2003" startWordPosition="354" endWordPosition="357"> image. As this limits the applicability of search engines (images that 1http://www.techcrunch.com/2008/11/03/ three-billion-photos-at-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often ambiguous. An image annotated with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption “car running under t</context>
<context position="15462" citStr="Blei and Jordan, 2003" startWordPosition="2467" endWordPosition="2470">r and trained multimodal word distributions over topics, it is possible to infer the posterior of topic proportions over the new data by maximizing the likelihood. The model delivers a ranked list of textual words wt, the n-best of which are used as annotations for image I. It is important to note that the caption generation models we propose are not especially tied to the above annotation model. Any probabilistic model with broadly similar properties could serve our purpose. Examples include PLSA-based approaches to image annotation (e.g., Monay and Gatica-Perez 2007) and correspondence LDA (Blei and Jordan, 2003). 5 Extractive Caption Generation Much work in summarization to date focuses on sentence extraction where a summary is created simply by identifying and subsequently concatenating the most important sentences in a document. Without a great deal of linguistic analysis, it is possible to create summaries for a wide range of documents, independently of style, text type, and subject matter. For our caption generation task, we need only extract a single sentence. And our guiding hypothesis is that this sentence must be maximally similar to the description keywords generated by the annotation model.</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>Blei, David and Michael Jordan. 2003. Modeling annotated data. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Toronto, ON, pages 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="13542" citStr="Blei et al. 2003" startWordPosition="2157" endWordPosition="2160"> different 1241 scales and locations. Importantly, this detector is, to some extent, invariant to translation, scale, rotation and illumination changes. Each detected region is represented with a SIFT descriptor which is a histogram of edge directions at different locations. Subsequently SIFT descriptors are quantized into a discrete set of visual terms via a clustering algorithm such as K-means. The model thus works with a bag-of-words representation and treats each article-image-caption tuple as a single document dMix consisting of textual and visual words. Latent Dirichlet Allocation (LDA, Blei et al. 2003) is used to infer the latent topics assumed to have generated dMix. The basic idea underlying LDA, and topic models in general, is that each document is composed of a probability distribution over topics, where each topic represents a probability distribution over words. The document-topic and topic-word distributions are learned automatically from the data and provide information about the semantic themes covered in each document and the words associated with each semantic theme. The image annotation model takes the topic distributions into account when finding the most likely keywords for an</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Corio</author>
<author>Guy Lapalme</author>
</authors>
<title>Generation of texts for information graphics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation.</booktitle>
<pages>49--58</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="6679" citStr="Corio and Lapalme, 1999" startWordPosition="1025" endWordPosition="1028">nerating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or background ontological i</context>
</contexts>
<marker>Corio, Lapalme, 1999</marker>
<rawString>Corio, Marc and Guy Lapalme. 1999. Generation of texts for information graphics. In Proceedings of the 7th European Workshop on Natural Language Generation. Toulouse, France, pages 49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Text Summarization.</booktitle>
<pages>1--8</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="7850" citStr="Dorr et al., 2003" startWordPosition="1215" endWordPosition="1218"> manual annotation or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation a</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 2003 Workshop on Text Summarization. Edmonton, Canada, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Duygulu</author>
<author>Kobus Barnard</author>
<author>Nando de Freitas</author>
<author>David Forsyth</author>
</authors>
<title>Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th European Conference on Computer Vision.</booktitle>
<pages>97--112</pages>
<location>Copenhagen, Denmark,</location>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>Duygulu, Pinar, Kobus Barnard, Nando de Freitas, and David Forsyth. 2002. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of the 7th European Conference on Computer Vision. Copenhagen, Denmark, pages 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Elzer</author>
<author>Sandra Carberry</author>
<author>Ingrid Zukerman</author>
<author>Daniel Chester</author>
<author>Nancy Green</author>
</authors>
<title>A probabilistic framework for recognizing intention in information graphics.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Conference on Artificial Intelligence.</booktitle>
<pages>1042--1047</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="6939" citStr="Elzer et al., 2005" startWordPosition="1068" endWordPosition="1071">is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and th</context>
</contexts>
<marker>Elzer, Carberry, Zukerman, Chester, Green, 2005</marker>
<rawString>Elzer, Stephanie, Sandra Carberry, Ingrid Zukerman, Daniel Chester, Nancy Green, , and Seniz Demir. 2005. A probabilistic framework for recognizing intention in information graphics. In Proceedings of the 19th International Conference on Artificial Intelligence. Edinburgh, Scotland, pages 1042–1047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Fasciano</author>
<author>Guy Lapalme</author>
</authors>
<title>Intentions in the coordinated generation of graphics and text from tabular data.</title>
<date>2000</date>
<journal>Knowledge Information Systems</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="6707" citStr="Fasciano and Lapalme, 2000" startWordPosition="1029" endWordPosition="1033">s of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or background ontological information we exploit a mult</context>
</contexts>
<marker>Fasciano, Lapalme, 2000</marker>
<rawString>Fasciano, Massimo and Guy Lapalme. 2000. Intentions in the coordinated generation of graphics and text from tabular data. Knowledge Information Systems 2(3):310–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Feiner</author>
<author>Kathleen McKeown</author>
</authors>
<title>Coordinating text and graphics in explanation generation.</title>
<date>1990</date>
<booktitle>In Proceedings of National Conference on Artificial Intelligence.</booktitle>
<pages>442--449</pages>
<location>Boston, MA,</location>
<contexts>
<context position="6734" citStr="Feiner and McKeown, 1990" startWordPosition="1034" endWordPosition="1037"> based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or background ontological information we exploit a multimodal database of news art</context>
</contexts>
<marker>Feiner, McKeown, 1990</marker>
<rawString>Feiner, Steven and Kathleen McKeown. 1990. Coordinating text and graphics in explanation generation. In Proceedings of National Conference on Artificial Intelligence. Boston, MA, pages 442–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaolei Feng Feng</author>
<author>Victor Lavrenko</author>
<author>R Manmatha</author>
</authors>
<title>Multiple Bernoulli relevance models for image and video annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computer Vision and Pattern Recognition.</booktitle>
<pages>1002--1009</pages>
<location>Washington, DC,</location>
<contexts>
<context position="2470" citStr="Feng et al., 2004" startWordPosition="376" endWordPosition="379">-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often ambiguous. An image annotated with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption “car running under the blue sky” would make the relations between the words explicit. Automatic caption generation could improve image retrieval by sup</context>
</contexts>
<marker>Feng, Lavrenko, Manmatha, 2004</marker>
<rawString>Feng, Shaolei Feng, Victor Lavrenko, and R Manmatha. 2004. Multiple Bernoulli relevance models for image and video annotation. In Proceedings of the International Conference on Computer Vision and Pattern Recognition. Washington, DC, pages 1002–1009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies.</booktitle>
<pages>272--280</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="10334" citStr="Feng and Lapata (2008)" startWordPosition="1647" endWordPosition="1650">m “new”. A third of children in the UK use blogs and social network websites but two thirds of parents do not even know what they are, a survey suggests. The children’s charity NCH said there was “an alarming gap” in technological knowledge between generations. Children were found to be far more internet-wise than parents. Table 1: Each entry in the BBC News database contains a document an image, and its caption. ples like the ones shown in Table 1. During testing, we are given a document and an associated image for which we must generate a caption. Our experiments used the dataset created by Feng and Lapata (2008).2 It contains 3,361 articles downloaded from the BBC News website3 each of which is associated with a captioned news image. The latter is usually 203 pixels wide and 152 pixels high. The average caption length is 9.5 words, the average sentence length is 20.5 words, and the average document length 421.5 words. The caption vocabulary is 6,180 words and the document vocabulary is 26,795. The vocabulary shared between captions and documents is 5,921 words. The captions tend to use half as many words as the document sentences, and more than 50% of the time contain words that are not attested in t</context>
<context position="26539" citStr="Feng and Lapata (2008)" startWordPosition="4356" endWordPosition="4359">to the image keywords and consider only the n-best ones. Alternatively, we could consider the single most relevant sentence together with its surrounding context under the assumption that neighboring sentences are about the same or similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times </context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Feng, Yansong and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies. Columbus, OH, pages 272–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<location>Los Angeles, LA.</location>
<contexts>
<context position="12300" citStr="Feng and Lapata (2010)" startWordPosition="1961" endWordPosition="1964">levance to the article, provide context for the picture, and ultimately draw the reader into the article. It is also worth noting that journalists often write their own captions rather than simply extract sentences from the document. In doing so they rely on general world knowledge but also expertise in current affairs that goes beyond what is described in the article or shown in the picture. 4 Image Annotation As mentioned earlier, our approach relies on an image annotation model to provide description keywords for the picture. Our experiments made use of the probabilistic model presented in Feng and Lapata (2010). The latter is well-suited to our task as it has been developed with noisy, multimodal data sets in mind. The model is based on the assumption that images and their surrounding text are generated by mixtures of latent topics which are inferred from a concatenated representation of words and visual features. Specifically, images are preprocessed so that they are represented by word-like units. Local image descriptors are computed using the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Ga</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Feng, Yansong and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Ferres</author>
<author>Avi Parush</author>
<author>Shelley Roberts</author>
<author>Gitte Lindgaard</author>
</authors>
<title>Helping people with visual impairments gain access to graphical information through natural language: The graph system.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th International Conference on Computers Helping People with Special Needs.</booktitle>
<pages>1122--1130</pages>
<location>Linz, Austria,</location>
<contexts>
<context position="3449" citStr="Ferres et al., 2006" startWordPosition="537" endWordPosition="540"> with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption “car running under the blue sky” would make the relations between the words explicit. Automatic caption generation could improve image retrieval by supporting longer and more targeted queries. It could also assist journalists in creating descriptions for the images associated with their articles. Beyond image retrieval, it could increase the accessibility of the web for visually impaired (blind and partially sighted) users who cannot access the content of many sites in the same ways as sighted users can (Ferres et al., 2006). We explore the feasibility of automatic caption generation in the news domain, and create descriptions for images associated with on-line articles. Obtaining training data in this setting does not require expensive manual annotation as many articles are published together with captioned images. Inspired by recent work in summarization, we propose extractive and abstractive caption gen1239 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239–1249, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics eration models. T</context>
</contexts>
<marker>Ferres, Parush, Roberts, Lindgaard, 2006</marker>
<rawString>Ferres, Leo, Avi Parush, Shelley Roberts, and Gitte Lindgaard. 2006. Helping people with visual impairments gain access to graphical information through natural language: The graph system. In Proceedings of 11th International Conference on Computers Helping People with Special Needs. Linz, Austria, pages 1122–1130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick H´ede</author>
<author>Pierre Allain Mo¨ellic</author>
<author>Magali Joint Jo¨el Bourgeoys</author>
<author>Corinne Thomas</author>
</authors>
<title>Automatic generation of natural language descriptions for images.</title>
<date>2004</date>
<booktitle>In Proceedings of Computer-Assisted Information Retrieval (Recherche d’Information et ses Applications Ordinateur) (RIAO).</booktitle>
<location>Avignon, France.</location>
<marker>H´ede, Mo¨ellic, Jo¨el Bourgeoys, Thomas, 2004</marker>
<rawString>H´ede, Patrick, Pierre Allain Mo¨ellic, Jo¨el Bourgeoys, Magali Joint, and Corinne Thomas. 2004. Automatic generation of natural language descriptions for images. In Proceedings of Computer-Assisted Information Retrieval (Recherche d’Information et ses Applications Ordinateur) (RIAO). Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Alexander G Hauptmann</author>
</authors>
<title>A new probabilistic model for title generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics.</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="7896" citStr="Jin and Hauptmann, 2002" startWordPosition="1223" endWordPosition="1226">gical information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an image I, and a related kno</context>
</contexts>
<marker>Jin, Hauptmann, 2002</marker>
<rawString>Jin, Rong and Alexander G. Hauptmann. 2002. A new probabilistic model for title generation. In Proceedings of the 19th International Conference on Computational linguistics. Taipei, Taiwan, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="26773" citStr="Klein and Manning, 2003" startWordPosition="4391" endWordPosition="4394">similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times in the corpus. For all models discussed here (extractive and abstractive) we report results with the 15 best annotation keywords. For the abstractive models, we used a trigram model trained with the SRI toolkit on a newswire corpus co</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics. Sapporo, Japan, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Jochen Peters</author>
<author>Dietrich Klakow</author>
</authors>
<title>Language model adaptation using dynamic marginals.</title>
<date>1997</date>
<booktitle>In Proceedings of 5th European Conference on Speech Communication and Technology. Rhodes, Greece,</booktitle>
<volume>4</volume>
<pages>1971--1974</pages>
<contexts>
<context position="21374" citStr="Kneser et al., 1997" startWordPosition="3464" endWordPosition="3467">nction words, as they are not descriptive of the image’s content. This will seriously impact the grammaticality of the generated captions, as there will be no appropriate function words to glue the content words together. One way to remedy this is to revert to a content selection model that ignores the image and simply estimates the probability of a word appearing in the caption given the same word appearing in the document. At the same time we modify our surface realization component so that it takes note of the image annotation probabilities. Specifically, we use an adaptive language model (Kneser et al., 1997) that modifies an pj pj log2 qj K � j=1 (4) 1243 n-gram model with local unigram probabilities: n P(w1,w2,...,wn) = ∏ P(wi ∈ C|wi ∈ D) (8) i=1 ·P(len(C) = n) n · ∏ Padap(wi|wi−1,wi−2) i=3 tion (8) as follows: P(ρ1,ρ2,...,ρm) ≈ P(ρj ∈ C|ρj ∈ D) (12) m ·P(len(C) = ∑ len(ρj)) j=1 m ∏ j=1 where P(wi ∈C|wi ∈ D) is the probability of wi appearing in the caption given that it appears in the document D, and Padap(wi|wi−1,wi−2) the language model adapted with probabilities from our image annotation model: α(w) Padap(w|h) Pback(w|h) = z(h) α(w) ≈ (Padap(w) )β Pback(w) z(h) = ∑ α(w) · Pback(w|h) w where </context>
</contexts>
<marker>Kneser, Peters, Klakow, 1997</marker>
<rawString>Kneser, Reinhard, Jochen Peters, and Dietrich Klakow. 1997. Language model adaptation using dynamic marginals. In Proceedings of 5th European Conference on Speech Communication and Technology. Rhodes, Greece, volume 4, pages 1971–1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsuhiro Kojima</author>
<author>Mamoru Takaya</author>
<author>Shigeki Aoki</author>
<author>Takao Miyamoto</author>
<author>Kunio Fukunaga</author>
</authors>
<title>Recognition and textual description of human activities by mobile robot.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Conference on Innovative Computing Information and Control. IEEE Computer Society,</booktitle>
<pages>53--56</pages>
<location>Washington, DC,</location>
<marker>Kojima, Takaya, Aoki, Miyamoto, Fukunaga, 2008</marker>
<rawString>Kojima, Atsuhiro, Mamoru Takaya, Shigeki Aoki, Takao Miyamoto, and Kunio Fukunaga. 2008. Recognition and textual description of human activities by mobile robot. In Proceedings of the 3rd International Conference on Innovative Computing Information and Control. IEEE Computer Society, Washington, DC, pages 53– 56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsuhiro Kojima</author>
<author>Takeshi Tamura</author>
<author>Kunio Fukunaga</author>
</authors>
<title>Natural language description of human activities from video images based on concept hierarchy of actions.</title>
<date>2002</date>
<journal>International Journal of Computer Vision</journal>
<volume>50</volume>
<issue>2</issue>
<contexts>
<context position="5767" citStr="Kojima et al. (2002" startWordPosition="887" endWordPosition="890">ge description with a text generation engine. A common theme across different models is domain specificity, the use of handlabeled data, and reliance on background ontological information. For example, H´ede et al. (2004) generate descriptions for images of objects shot in uniform background. Their system relies on a manually created database of objects indexed by an image signature (e.g., color and texture) and two keywords (the object’s name and category). Images are first segmented into objects, their signature is retrieved from the database, and a description is generated using templates. Kojima et al. (2002, 2008) create descriptions for human activities in office scenes. They extract features of human motion and interleave them with a concept hierarchy of actions to create a case frame from which a natural language sentence is generated. Yao et al. (2009) present a general framework for generating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with</context>
</contexts>
<marker>Kojima, Tamura, Fukunaga, 2002</marker>
<rawString>Kojima, Atsuhiro, Takeshi Tamura, and Kunio Fukunaga. 2002. Natural language description of human activities from video images based on concept hierarchy of actions. International Journal of Computer Vision 50(2):171–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>R Manmatha</author>
<author>Jiwoon Jeon</author>
</authors>
<title>A model for learning the semantics of pictures.</title>
<date>2003</date>
<booktitle>In Proceedings of the 16th Conference on Advances in Neural Information Processing Systems.</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="2450" citStr="Lavrenko et al., 2003" startWordPosition="372" endWordPosition="375">three-billion-photos-at-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often ambiguous. An image annotated with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption “car running under the blue sky” would make the relations between the words explicit. Automatic caption generation could improve im</context>
</contexts>
<marker>Lavrenko, Manmatha, Jeon, 2003</marker>
<rawString>Lavrenko, Victor, R. Manmatha, and Jiwoon Jeon. 2003. A model for learning the semantics of pictures. In Proceedings of the 16th Conference on Advances in Neural Information Processing Systems. Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object recognition from local scale-invariant features.</title>
<date>1999</date>
<booktitle>In Proceedings of International Conference on Computer Vision. IEEE Computer Society,</booktitle>
<pages>1150--1157</pages>
<contexts>
<context position="12807" citStr="Lowe, 1999" startWordPosition="2044" endWordPosition="2045">for the picture. Our experiments made use of the probabilistic model presented in Feng and Lapata (2010). The latter is well-suited to our task as it has been developed with noisy, multimodal data sets in mind. The model is based on the assumption that images and their surrounding text are generated by mixtures of latent topics which are inferred from a concatenated representation of words and visual features. Specifically, images are preprocessed so that they are represented by word-like units. Local image descriptors are computed using the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Gaussians point detector at different 1241 scales and locations. Importantly, this detector is, to some extent, invariant to translation, scale, rotation and illumination changes. Each detected region is represented with a SIFT descriptor which is a histogram of edge directions at different locations. Subsequently SIFT descriptors are quantized into a discrete set of visual terms via a clustering algorithm such as K-means. The model thus works with a bag-of-words representation and treats each article-im</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>Lowe, David G. 1999. Object recognition from local scale-invariant features. In Proceedings of International Conference on Computer Vision. IEEE Computer Society, pages 1150–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhu O Mittal</author>
<author>Johanna D Moore</author>
<author>Giuseppe Carenini</author>
<author>Steven Roth</author>
</authors>
<title>Describing complex charts in natural language: A caption generation system.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<pages>24--431</pages>
<contexts>
<context position="6654" citStr="Mittal et al., 1998" startWordPosition="1020" endWordPosition="1024">eral framework for generating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or</context>
</contexts>
<marker>Mittal, Moore, Carenini, Roth, 1998</marker>
<rawString>Mittal, Vibhu O., Johanna D. Moore, Giuseppe Carenini, and Steven Roth. 1998. Describing complex charts in natural language: A caption generation system. Computational Linguistics 24:431–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Monay</author>
<author>Daniel Gatica-Perez</author>
</authors>
<title>Modeling semantic aspects for cross-media image indexing.</title>
<date>2007</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>29</volume>
<issue>10</issue>
<contexts>
<context position="15415" citStr="Monay and Gatica-Perez 2007" startWordPosition="2460" endWordPosition="2463">ns over documents. Given an unseen image-document pair and trained multimodal word distributions over topics, it is possible to infer the posterior of topic proportions over the new data by maximizing the likelihood. The model delivers a ranked list of textual words wt, the n-best of which are used as annotations for image I. It is important to note that the caption generation models we propose are not especially tied to the above annotation model. Any probabilistic model with broadly similar properties could serve our purpose. Examples include PLSA-based approaches to image annotation (e.g., Monay and Gatica-Perez 2007) and correspondence LDA (Blei and Jordan, 2003). 5 Extractive Caption Generation Much work in summarization to date focuses on sentence extraction where a summary is created simply by identifying and subsequently concatenating the most important sentences in a document. Without a great deal of linguistic analysis, it is possible to create summaries for a wide range of documents, independently of style, text type, and subject matter. For our caption generation task, we need only extract a single sentence. And our guiding hypothesis is that this sentence must be maximally similar to the descript</context>
</contexts>
<marker>Monay, Gatica-Perez, 2007</marker>
<rawString>Monay, Florent and Daniel Gatica-Perez. 2007. Modeling semantic aspects for cross-media image indexing. IEEE Transactions on Pattern Analysis and Machine Intelligence 29(10):1802–1817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="16648" citStr="Salton and McGill, 1983" startWordPosition="2658" endWordPosition="2661">rds generated by the annotation model. We discuss below different ways of operationalizing similarity. Word Overlap Perhaps the simplest way of measuring the similarity between image keywords and document sentences is word overlap: Overlap(WI,Sd) = |WI nSd |(2) |WI � Sd| where WI is the set of keywords and Sd a sentence in the document. The caption is then the sentence that has the highest overlap with the keywords. Cosine Similarity Word overlap is admittedly a naive measure of similarity, based on lexical identity. We can overcome this by representing keywords and sentences in vector space (Salton and McGill, 1983). The latter is a word-sentence co-occurrence matrix where each row represents a word, each column a sentence, and each entry the frequency with which the word appeared within the sentence. More precisely matrix cells are weighted by their tf-idf values. The similarity WI �� and suring the cosine of their angle: Probabilistic Similarity Recall that the backbone of our image annotation model is a topic model with images and documents represented as a probability distribution over latent topics. Under this framework, the similarity between an im�� Sd) = WI �� - ����� ��Sd| |WI|| �� Sd sim(WI, (3</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, Gerard and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnols W M Smeulders</author>
<author>Marcel Worring</author>
<author>Simone Santini</author>
<author>Amarnath Gupta</author>
<author>Ramesh Jain</author>
</authors>
<title>Content-based image retrieval at the end of the early years.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>22</volume>
<issue>12</issue>
<contexts>
<context position="2227" citStr="Smeulders et al., 2000" startWordPosition="336" endWordPosition="339">a-data (e.g., the image’s file name and format), user-annotated tags, captions, and generally text surrounding the image. As this limits the applicability of search engines (images that 1http://www.techcrunch.com/2008/11/03/ three-billion-photos-at-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often ambiguous. An image annotat</context>
</contexts>
<marker>Smeulders, Worring, Santini, Gupta, Jain, 2000</marker>
<rawString>Smeulders, Arnols W.M., Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(12):1349–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas. Cambridge,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="28578" citStr="Snover et al. 2006" startWordPosition="4685" endWordPosition="4688">am size was set to 500 (with at least 50 states for the word-based model). For the phrase-based model, we also experimented with reducing the search scope, either by considering only the n most similar sentences to the keywords (range [2,10]), or simply the single most similar sentence and its neighbors (range [2,5]). The former method delivered better results with 10 sentences (and the KL divergence similarity function). Evaluation We evaluated the performance of our models automatically, and also by eliciting human judgments. Our automatic evaluation was based on Translation Edit Rate (TER, Snover et al. 2006), a measure commonly used to evaluate the quality of machine translation output. TER is defined as the minimum number of edits a human would have to perform to change the system output so that it exactly matches a reference translation. In our case, the original captions written by the BBC journalists were used as reference: Ins + Del + Sub + Shft TER(E,Er) = (16) Nr where E is the hypothetical system output, Er the reference caption, and Nr the reference length. The number of possible edits include insertions (Ins), deletions (Del), substitutions (Sub) and shifts (Shft). TER is similar to wor</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas. Cambridge, pages 223– 231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2007</date>
<booktitle>A Handbook of Latent Semantic Analysis,</booktitle>
<editor>In T. Landauer, D. McNamara, S Dennis, and W Kintsch, editors,</editor>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="17507" citStr="Steyvers and Griffiths, 2007" startWordPosition="2804" endWordPosition="2807"> by their tf-idf values. The similarity WI �� and suring the cosine of their angle: Probabilistic Similarity Recall that the backbone of our image annotation model is a topic model with images and documents represented as a probability distribution over latent topics. Under this framework, the similarity between an im�� Sd) = WI �� - ����� ��Sd| |WI|| �� Sd sim(WI, (3) K E P(wt|zk)P(zk|dMix) k=1 of the vectors representing the keywords �� document sentence Sd can be quantified by mea1242 age and a sentence can be broadly measured by the extent to which they share the same topic distributions (Steyvers and Griffiths, 2007). For example, we may use the KL divergence to measure the difference between the distributions p and q: D(p,q) = where p and q are shorthand for the image topic distribution PdMix and sentence topic distribution PSd, respectively. When doing inference on the document sentence, we also take its neighboring sentences into account to avoid estimating inaccurate topic proportions on short sentences. The KL divergence is asymmetric and in many applications, it is preferable to apply a symmetric measure such as the Jensen Shannon (JS) divergence. The latter measures the “distance” between p and q t</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Steyvers, Mark and Tom Griffiths. 2007. Probabilistic topic models. In T. Landauer, D. McNamara, S Dennis, and W Kintsch, editors, A Handbook of Latent Semantic Analysis, Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Vailaya</author>
<author>M´ario A T Figueiredo</author>
<author>Anil K Jain</author>
<author>Hong-Jiang Zhang</author>
</authors>
<title>Image classification for content-based indexing.</title>
<date>2001</date>
<journal>IEEE Transactions on Image Processing</journal>
<pages>10--117</pages>
<contexts>
<context position="2202" citStr="Vailaya et al., 2001" startWordPosition="332" endWordPosition="335">. Examples include meta-data (e.g., the image’s file name and format), user-annotated tags, captions, and generally text surrounding the image. As this limits the applicability of search engines (images that 1http://www.techcrunch.com/2008/11/03/ three-billion-photos-at-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often am</context>
</contexts>
<marker>Vailaya, Figueiredo, Jain, Zhang, 2001</marker>
<rawString>Vailaya, Aditya, M´ario A. T. Figueiredo, Anil K. Jain, and Hong-Jiang Zhang. 2001. Image classification for content-based indexing. IEEE Transactions on Image Processing 10:117–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In ACM Conference on Human Factors in Computing Systems.</booktitle>
<pages>319--326</pages>
<location>New York, NY,</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>von Ahn, Luis and Laura Dabbish. 2004. Labeling images with a computer game. In ACM Conference on Human Factors in Computing Systems. New York, NY, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David Blei</author>
<author>Li Fei-Fei</author>
</authors>
<title>Simultaneous image classification and annotation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Computer Vision and Pattern Recognition.</booktitle>
<pages>1903--1910</pages>
<location>Miami, FL,</location>
<contexts>
<context position="2381" citStr="Wang et al., 2009" startWordPosition="362" endWordPosition="365">earch engines (images that 1http://www.techcrunch.com/2008/11/03/ three-billion-photos-at-flickr/ do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically. The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al., 2001; Smeulders et al., 2000), instantiations of the noisychannel model (Duygulu et al., 2002), latent variable models (Blei and Jordan, 2003; Barnard et al., 2002; Wang et al., 2009), and models inspired by information retrieval (Lavrenko et al., 2003; Feng et al., 2004). In this paper we go one step further and generate captions for images rather than individual keywords. Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. A list of keywords is often ambiguous. An image annotated with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption “car running under the blue sky” would make the relations betw</context>
</contexts>
<marker>Wang, Blei, Fei-Fei, 2009</marker>
<rawString>Wang, Chong, David Blei, and Li Fei-Fei. 2009. Simultaneous image classification and annotation. In Proceedings of the International Conference on Computer Vision and Pattern Recognition. Miami, FL, pages 1903–1910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song chun Zhu</author>
</authors>
<title>I2t: Image parsing to text description.</title>
<date>2009</date>
<booktitle>Proceedings of IEEE (invited for the special issue on Internet Vision) .</booktitle>
<contexts>
<context position="6021" citStr="Yao et al. (2009)" startWordPosition="930" endWordPosition="933">f objects shot in uniform background. Their system relies on a manually created database of objects indexed by an image signature (e.g., color and texture) and two keywords (the object’s name and category). Images are first segmented into objects, their signature is retrieved from the database, and a description is generated using templates. Kojima et al. (2002, 2008) create descriptions for human activities in office scenes. They extract features of human motion and interleave them with a concept hierarchy of actions to create a case frame from which a natural language sentence is generated. Yao et al. (2009) present a general framework for generating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical p</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2009</marker>
<rawString>Yao, Benjamin, Xiong Yang, Liang Lin, Mun Wai Lee, and Song chun Zhu. 2009. I2t: Image parsing to text description. Proceedings of IEEE (invited for the special issue on Internet Vision) .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>