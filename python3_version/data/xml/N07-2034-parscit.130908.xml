<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000640">
<title confidence="0.995039">
An integrated architecture for speech-input multi-target machine translation
</title>
<author confidence="0.988168">
Alicia P´erez, M. In´es Torres M. Teresa Gonz´alez, Francisco Casacuberta
</author>
<affiliation confidence="0.93781">
Dep. of Electricity and Electronics Dep. of Information Systems and Computation
University of the Basque Country Technical University of Valencia
</affiliation>
<email confidence="0.981913">
manes@we.lc.ehu.es fcn@dsic.upv.es
</email>
<sectionHeader confidence="0.995302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952133333334">
The aim of this work is to show the abil-
ity of finite-state transducers to simultane-
ously translate speech into multiple lan-
guages. Our proposal deals with an ex-
tension of stochastic finite-state transduc-
ers that can produce more than one out-
put at the same time. These kind of de-
vices offer great versatility for the inte-
gration with other finite-state devices such
as acoustic models in order to produce a
speech translation system. This proposal
has been evaluated in a practical situation,
and its results have been compared with
those obtained using a standard mono-
target speech transducer.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969704545455">
Finite-state models constitute an important frame-
work both in syntactic pattern recognition and in
language processing. Specifically, stochastic finite-
state transducers (SFSTs) have proved to be useful
for machine translation tasks within restricted do-
mains; they usually offer high speed during the de-
coding step and they provide competitive results in
terms of error rates (Mohri et al., 2002). Moreover,
SFSTs have proved to be versatile models, which
can be easily integrated with other finite-state mod-
els (Pereira and Riley, 1997).
The article (Casacuberta and Vidal, 2004) ex-
plored an automatic method to learn an SFST from a
bilingual set of samples for machine translation pur-
poses, the so-called GIATI (Grammar Inference and
Alignments for Transducers Inference). It described
how to learn both the structural and the probabilistic
components of an SFST making use of underlying
alignment models.
A multi-target SFST is a generalization of stan-
dard SFSTs, in such a way that every input string
in the source language results in a tuple of output
strings each being associated to a different target
language. An extension of GIATI that allowed to in-
fer a multi-target SFST from a multilingual corpus
was proposed in (Gonz´alez and Casacuberta, 2006).
A syntactic variant of this method (denoted as GI-
AMTI) has been used in this work in order to infer
the models from training samples as it is summa-
rized in section 3.
On the other hand, speech translation has been al-
ready carried out by integrating acoustic models into
a SFST (Casacuberta et al., 2004). Our main goal
in this work is to extend and assess these method-
ologies to accomplish spoken language multi-target
translation. Section 2 deals with this proposal by
presenting a new integrated architecture for speech-
input multi-target translation. Under this approach
spoken language can be simultaneously decoded and
translated into m languages using a unique network.
In section 4, the performance of the system has
been experimentally evaluated over a trilingual task
which aims to translate TV weather forecast into two
languages at the same time.
</bodyText>
<sectionHeader confidence="0.97498" genericHeader="method">
2 An integrated architecture for
speech-input multi-target translation
</sectionHeader>
<bodyText confidence="0.997362">
The classical architecture for spoken language
multi-target translation involves a speech recogni-
</bodyText>
<page confidence="0.988922">
133
</page>
<note confidence="0.465303">
Proceedings of NAACL HLT 2007, Companion Volume, pages 133–136,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99972775">
tion system in a serial architecture with m decoupled
text-to-text translators. Thus, the whole process in-
volves m + 1 searching stages, a first one for the
speech signal transcription into the source language
text string, and further m for the source language
translation into the m target languages. If we re-
placed the m translators by the multi-target SFST,
the problem would be reduced to 2 searching stages.
Nevertheless, in this paper we propose a natural way
for acoustic models to be integrated in the same net-
work. As a result, the input speech-signal can be
simultaneously decoded and translated into m target
languages just in a single searching stage.
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i E {1, ... , m}). This approach is summarized
in eq. (1), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
</bodyText>
<equation confidence="0.940967857142857">
�P(tm|x) = arg max
M $
(1)
Making use of Bayes’ rule, the former expression
turns into:
P(tm, s)P(x|tm, s) (2)
s
</equation>
<bodyText confidence="0.99951375">
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string: i.e., that P(x|tm, s) is in-
dependent of tm. In this sense, eq. (2) can be rewrit-
</bodyText>
<equation confidence="0.9262152">
ten as:
�
�tm = arg max
tM
s
</equation>
<bodyText confidence="0.99321725">
Equation (3) combines a standard acoustic model,
P(x|s), and a multi-target translation model,
P(tm, s), both of whom can be integrated on the fly
during the searching routine. Nevertheless, the outer
maximization is computationally very expensive to
search for the optimal tuple of target strings tm in
an effective way. Thus we make use of the so called
Viterbi approximation, which finds the best path.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.932429857142857">
Given a multilingual corpus, that is, a finite set of
multilingual samples (s, t 1 ... , tm) E E∗ x A∗1 x
· · ·x A∗ m, where ti denotes the translation of the
source sentence s (formed by words of the input vo-
cabulary E) into the i-th target language, which, in
its turn, has a vocabulary Ai, the GIAMTI method
can be outlined as follows:
</bodyText>
<listItem confidence="0.90579352173913">
1. Each multilingual sample is transformed into a
single string from an extended vocabulary (I&apos; C_
E x A∗ x · · · x A∗ m) using a labelling function
(Gm). This transformation searches an ade-
quate monotonous segmentation for each of the
m source-target language pairs. A monotonous
segmentation copes with monotonous align-
ments, that is, j &lt; k ==&gt;. aj &lt; ak following
the notation of (Brown et al., 1993). Each
source word is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a word from the source language plus zero
or more words from each target language.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z E I&apos;∗), a stochastic regular grammar can be
inferred.
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input word and m output phrases
(w/p1/ ... /pm) by the inverse labeling func-
tion (G−m), leading to the required transducer.
</listItem>
<bodyText confidence="0.999813555555556">
In this work, the first step of the algorithm (as
described above), which is the one that handles
the alignment and segmentation routines, relies on
statistical alignments obtained with GIZA++ (Och,
2000). The second step was implemented us-
ing our own language modeling toolkit, which
learns stochastic k-testable in the string-sense gram-
mars (Torres and Varona, 2001), and allows for
back-off smoothing.
</bodyText>
<sectionHeader confidence="0.998062" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.991053">
4.1 Task and corpus
</subsectionHeader>
<bodyText confidence="0.9999816">
We have implemented a highly practical application
that could be used to translate on-line TV weather
forecasts into several languages, taking the speech
of the presenter as the input and producing as output
text-strings, or sub-titles, in several languages. For
</bodyText>
<equation confidence="0.9942389">
�
tm = arg max
tM
P(tm, s|x)
�
�
max
tM
tm = arg
P(tm, s)P(x|s) (3)
</equation>
<page confidence="0.985399">
134
</page>
<bodyText confidence="0.999835666666667">
this purpose, we used the corpus METEUS (see Ta-
ble 1) which consists of a set of trilingual sentences,
in English, Spanish and Basque, as extracted from
weather forecast reports that had been published on
the Internet. Basque language is a minority lan-
guage, spoken in a small area of Europe and also
within some small American communities (such as
that in Boise, Idaho). In the Basque Country it has
an official status along with Spanish. However both
languages differs greatly in syntax and in semantics.
The differences in the size of the vocabulary (see
Table 1), for instance, are due to the agglutinative
nature of the Basque language.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
independent if it is to be realistic.
</bodyText>
<table confidence="0.9992075">
Spanish Basque English
Sentences 14,615
Different Sent. 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Different Sent. 500
Words 8,706 8,274 9,150
T Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
</table>
<tableCaption confidence="0.999896">
Table 1: Main features of the METEUS corpus.
</tableCaption>
<bodyText confidence="0.99946675">
input machine translation applications. These values
can be objectively measured based on the size and on
the average branching factor of the model displayed
in Table 2.
</bodyText>
<table confidence="0.9928918">
multi-target mono-target
S2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Braching factor 3.30 3.13 3.46
</table>
<tableCaption confidence="0.982091">
Table 2: Features of multi-target model and the two
</tableCaption>
<bodyText confidence="0.956013368421053">
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the states and the edges up for the two
mono-target SFSTs that take part in the decoupled
architecture (see Table 2), we conclude that the de-
coupled model needs a total of 185, 216 edges to be
allocated in memory, which represents an increment
of 13% in memory-space with respect to the multi-
target model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which means that searching
for a translation can be faster. In fact, experimental
results in Table 3 show that the mono-target archi-
tecture works %11 more slowly than the multi-target
one.
</bodyText>
<subsectionHeader confidence="0.998656">
4.2 System evaluation
</subsectionHeader>
<bodyText confidence="0.999984555555555">
The experimental setup was as follows: the multi-
target SFST was learned from the training set in Ta-
ble 1 using the GIAMTI algorithm described in sec-
tion 1; then, the speech test was translated, and the
output provided by the system in each language was
compared to the corresponding reference sentence.
Additionally, two mono-target SFST were inferred
from the same training set with their outputs for the
aforementioned test to be taken as baseline.
</bodyText>
<subsectionHeader confidence="0.621391">
4.2.1 Computational cost
</subsectionHeader>
<bodyText confidence="0.999556">
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
</bodyText>
<table confidence="0.985142666666667">
multi-target mono-target
S2B S2E S2B+S2E
Time (s) 30,514 24,398 9,501 33,899
</table>
<tableCaption confidence="0.869932">
Table 3: Time needed to translate the speech-test
into two languages.
</tableCaption>
<bodyText confidence="0.991975">
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
</bodyText>
<subsectionHeader confidence="0.530827">
4.2.2 Performance
</subsectionHeader>
<bodyText confidence="0.9997065">
So far, the capability of the systems have been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In order
to assess the performance of the system in a quan-
titative manner, the following evaluation parameters
</bodyText>
<page confidence="0.99758">
135
</page>
<bodyText confidence="0.9997704">
were computed for each scenario: bilingual evalua-
tion under study (BLEU), position independent er-
ror rate (PER) and word error rate (WER).
As can be derived from the Speech-input trans-
lation results shown in Table 4, slightly better re-
sults are obtained with the classical mono-target SF-
STs, compared with the multi-target approach. From
Spanish into English the improvement is around
3.4% but from Spanish into Basque, multi-target ap-
proach works better with an improvement of a 0.8%.
</bodyText>
<table confidence="0.9933782">
multi-target mono-target
S2B S2E S2B S2E
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
</table>
<tableCaption confidence="0.991005">
Table 4: Speech-input translation results for Spanish
</tableCaption>
<bodyText confidence="0.81241975">
into Basque (S2B) and Spanish into English (S2E)
using a multi-target SFST or two mono-target SF-
STs.
The process of speech signal decoding is itself
introducing some errors. In an attempt to measure
these errors, the text transcription of the recognized
input signal was extracted and compared to the input
reference in terms of WER as shown in Table 5.
multi-target
WER 10.7
Table 5: Spanish speech decoding results for the
multi-target SFST and the two mono target SFSTs.
</bodyText>
<sectionHeader confidence="0.74665" genericHeader="conclusions">
5 Concluding remarks and further work
</sectionHeader>
<bodyText confidence="0.999860368421053">
A fully embedded architecture that integrates the
acoustic model into the multi-target translation
model for multiple speech translation has been pro-
posed. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm. The most significant feature of this
approach is its ability to carry out both the recogni-
tion and the translation into multiple languages inte-
grated in a unique model.
In contrast to the classical decoupled systems,
multi-target SFSTs enable the translation from one
source language simultaneously into several target
languages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults.
In future work we intend to make a deeper study
on the performance of the multi-target system as the
amount of targets increase, since the amount of pa-
rameters to be estimated also increases.
</bodyText>
<sectionHeader confidence="0.996369" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997665">
This work has been partially supported by the Uni-
versity of the Basque Country and by the Spanish
CICYT under grants 9/UPV 00224.310-15900/2004
and TIC2003-08681-C02-02 respectively.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999206588235294">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205–
225.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M.
Vilar, S. Barrachina, I. Garc´ıa-Varea, D. Llorens,
C. Mart´ınez, S. Molau, F. Nevado, M. Pastor, D. Pic´o,
A. Sanchis, and C. Tillmann. 2004. Some approaches
to statistical and finite-state speech-to-speech transla-
tion. Computer Speech and Language, 18:25–47, Jan-
uary.
M. Teresa Gonz´alez and Francisco Casacuberta. 2006.
Multi-Target Machine Translation using Finite-State
Transducers. In Proceedings of TC-Star Speech to
Speech Translation Workshop, pages 105–110.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69–88, January.
Franz J. Och. 2000. GIZA++: Training of statistical
translation models.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431–
453. The MIT Press, Cambridge, Massachusetts.
M. In´es Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127–149.
</reference>
<figure confidence="0.963161666666667">
mono-target
S2B S2E
9.3 9.1
</figure>
<page confidence="0.971284">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698005">
<title confidence="0.998916">An integrated architecture for speech-input multi-target machine translation</title>
<author confidence="0.99823">Alicia P´erez</author>
<author confidence="0.99823">M In´es Torres M Teresa Gonz´alez</author>
<author confidence="0.99823">Francisco Casacuberta</author>
<affiliation confidence="0.9985245">Dep. of Electricity and Electronics Dep. of Information Systems and Computation University of the Basque Country Technical University of Valencia</affiliation>
<email confidence="0.720804">manes@we.lc.ehu.esfcn@dsic.upv.es</email>
<abstract confidence="0.9982460625">The aim of this work is to show the ability of finite-state transducers to simultaneously translate speech into multiple languages. Our proposal deals with an extension of stochastic finite-state transducers that can produce more than one output at the same time. These kind of devices offer great versatility for the integration with other finite-state devices such as acoustic models in order to produce a speech translation system. This proposal has been evaluated in a practical situation, and its results have been compared with those obtained using a standard monotarget speech transducer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5914" citStr="Brown et al., 1993" startWordPosition="968" endWordPosition="971">re ti denotes the translation of the source sentence s (formed by words of the input vocabulary E) into the i-th target language, which, in its turn, has a vocabulary Ai, the GIAMTI method can be outlined as follows: 1. Each multilingual sample is transformed into a single string from an extended vocabulary (I&apos; C_ E x A∗ x · · · x A∗ m) using a labelling function (Gm). This transformation searches an adequate monotonous segmentation for each of the m source-target language pairs. A monotonous segmentation copes with monotonous alignments, that is, j &lt; k ==&gt;. aj &lt; ak following the notation of (Brown et al., 1993). Each source word is then joined with a target phrase of each language as the corresponding segmentation suggests. Each extended symbol consists of a word from the source language plus zero or more words from each target language. 2. Once the set of multilingual samples has been converted into a set of single extended strings (z E I&apos;∗), a stochastic regular grammar can be inferred. 3. The extended symbols associated with the transitions of the automaton are transformed into one input word and m output phrases (w/p1/ ... /pm) by the inverse labeling function (G−m), leading to the required tran</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<pages>225</pages>
<contexts>
<context position="1531" citStr="Casacuberta and Vidal, 2004" startWordPosition="226" endWordPosition="229">ard monotarget speech transducer. 1 Introduction Finite-state models constitute an important framework both in syntactic pattern recognition and in language processing. Specifically, stochastic finitestate transducers (SFSTs) have proved to be useful for machine translation tasks within restricted domains; they usually offer high speed during the decoding step and they provide competitive results in terms of error rates (Mohri et al., 2002). Moreover, SFSTs have proved to be versatile models, which can be easily integrated with other finite-state models (Pereira and Riley, 1997). The article (Casacuberta and Vidal, 2004) explored an automatic method to learn an SFST from a bilingual set of samples for machine translation purposes, the so-called GIATI (Grammar Inference and Alignments for Transducers Inference). It described how to learn both the structural and the probabilistic components of an SFST making use of underlying alignment models. A multi-target SFST is a generalization of standard SFSTs, in such a way that every input string in the source language results in a tuple of output strings each being associated to a different target language. An extension of GIATI that allowed to infer a multi-target SF</context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>Francisco Casacuberta and Enrique Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(2):205– 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>H Ney</author>
<author>F J Och</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>S Barrachina</author>
<author>I Garc´ıa-Varea</author>
<author>D Llorens</author>
<author>C Mart´ınez</author>
<author>S Molau</author>
<author>F Nevado</author>
<author>M Pastor</author>
<author>D Pic´o</author>
<author>A Sanchis</author>
<author>C Tillmann</author>
</authors>
<title>Some approaches to statistical and finite-state speech-to-speech translation. Computer Speech and Language,</title>
<date>2004</date>
<pages>18--25</pages>
<marker>Casacuberta, Ney, Och, Vidal, Vilar, Barrachina, Garc´ıa-Varea, Llorens, Mart´ınez, Molau, Nevado, Pastor, Pic´o, Sanchis, Tillmann, 2004</marker>
<rawString>F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar, S. Barrachina, I. Garc´ıa-Varea, D. Llorens, C. Mart´ınez, S. Molau, F. Nevado, M. Pastor, D. Pic´o, A. Sanchis, and C. Tillmann. 2004. Some approaches to statistical and finite-state speech-to-speech translation. Computer Speech and Language, 18:25–47, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Teresa Gonz´alez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Multi-Target Machine Translation using Finite-State Transducers.</title>
<date>2006</date>
<booktitle>In Proceedings of TC-Star Speech to Speech Translation Workshop,</booktitle>
<pages>105--110</pages>
<marker>Gonz´alez, Casacuberta, 2006</marker>
<rawString>M. Teresa Gonz´alez and Francisco Casacuberta. 2006. Multi-Target Machine Translation using Finite-State Transducers. In Proceedings of TC-Star Speech to Speech Translation Workshop, pages 105–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer, Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="1347" citStr="Mohri et al., 2002" startWordPosition="198" endWordPosition="201">n order to produce a speech translation system. This proposal has been evaluated in a practical situation, and its results have been compared with those obtained using a standard monotarget speech transducer. 1 Introduction Finite-state models constitute an important framework both in syntactic pattern recognition and in language processing. Specifically, stochastic finitestate transducers (SFSTs) have proved to be useful for machine translation tasks within restricted domains; they usually offer high speed during the decoding step and they provide competitive results in terms of error rates (Mohri et al., 2002). Moreover, SFSTs have proved to be versatile models, which can be easily integrated with other finite-state models (Pereira and Riley, 1997). The article (Casacuberta and Vidal, 2004) explored an automatic method to learn an SFST from a bilingual set of samples for machine translation purposes, the so-called GIATI (Grammar Inference and Alignments for Transducers Inference). It described how to learn both the structural and the probabilistic components of an SFST making use of underlying alignment models. A multi-target SFST is a generalization of standard SFSTs, in such a way that every inpu</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer, Speech and Language, 16(1):69–88, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>GIZA++: Training of statistical translation models.</title>
<date>2000</date>
<contexts>
<context position="6726" citStr="Och, 2000" startWordPosition="1104" endWordPosition="1105">ds from each target language. 2. Once the set of multilingual samples has been converted into a set of single extended strings (z E I&apos;∗), a stochastic regular grammar can be inferred. 3. The extended symbols associated with the transitions of the automaton are transformed into one input word and m output phrases (w/p1/ ... /pm) by the inverse labeling function (G−m), leading to the required transducer. In this work, the first step of the algorithm (as described above), which is the one that handles the alignment and segmentation routines, relies on statistical alignments obtained with GIZA++ (Och, 2000). The second step was implemented using our own language modeling toolkit, which learns stochastic k-testable in the string-sense grammars (Torres and Varona, 2001), and allows for back-off smoothing. 4 Experimental results 4.1 Task and corpus We have implemented a highly practical application that could be used to translate on-line TV weather forecasts into several languages, taking the speech of the presenter as the input and producing as output text-strings, or sub-titles, in several languages. For � tm = arg max tM P(tm, s|x) � � max tM tm = arg P(tm, s)P(x|s) (3) 134 this purpose, we used</context>
</contexts>
<marker>Och, 2000</marker>
<rawString>Franz J. Och. 2000. GIZA++: Training of statistical translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael D Riley</author>
</authors>
<title>Speech Recognition by Composition of Weighted Finite Automata.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, Language, Speech and Communication series,</booktitle>
<pages>431--453</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1488" citStr="Pereira and Riley, 1997" startWordPosition="220" endWordPosition="223">pared with those obtained using a standard monotarget speech transducer. 1 Introduction Finite-state models constitute an important framework both in syntactic pattern recognition and in language processing. Specifically, stochastic finitestate transducers (SFSTs) have proved to be useful for machine translation tasks within restricted domains; they usually offer high speed during the decoding step and they provide competitive results in terms of error rates (Mohri et al., 2002). Moreover, SFSTs have proved to be versatile models, which can be easily integrated with other finite-state models (Pereira and Riley, 1997). The article (Casacuberta and Vidal, 2004) explored an automatic method to learn an SFST from a bilingual set of samples for machine translation purposes, the so-called GIATI (Grammar Inference and Alignments for Transducers Inference). It described how to learn both the structural and the probabilistic components of an SFST making use of underlying alignment models. A multi-target SFST is a generalization of standard SFSTs, in such a way that every input string in the source language results in a tuple of output strings each being associated to a different target language. An extension of GI</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando C.N. Pereira and Michael D. Riley. 1997. Speech Recognition by Composition of Weighted Finite Automata. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, Language, Speech and Communication series, pages 431– 453. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M In´es Torres</author>
<author>Amparo Varona</author>
</authors>
<title>k-tss language models in speech recognition systems.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="6890" citStr="Torres and Varona, 2001" startWordPosition="1127" endWordPosition="1130"> regular grammar can be inferred. 3. The extended symbols associated with the transitions of the automaton are transformed into one input word and m output phrases (w/p1/ ... /pm) by the inverse labeling function (G−m), leading to the required transducer. In this work, the first step of the algorithm (as described above), which is the one that handles the alignment and segmentation routines, relies on statistical alignments obtained with GIZA++ (Och, 2000). The second step was implemented using our own language modeling toolkit, which learns stochastic k-testable in the string-sense grammars (Torres and Varona, 2001), and allows for back-off smoothing. 4 Experimental results 4.1 Task and corpus We have implemented a highly practical application that could be used to translate on-line TV weather forecasts into several languages, taking the speech of the presenter as the input and producing as output text-strings, or sub-titles, in several languages. For � tm = arg max tM P(tm, s|x) � � max tM tm = arg P(tm, s)P(x|s) (3) 134 this purpose, we used the corpus METEUS (see Table 1) which consists of a set of trilingual sentences, in English, Spanish and Basque, as extracted from weather forecast reports that ha</context>
</contexts>
<marker>Torres, Varona, 2001</marker>
<rawString>M. In´es Torres and Amparo Varona. 2001. k-tss language models in speech recognition systems. Computer Speech and Language, 15(2):127–149.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>