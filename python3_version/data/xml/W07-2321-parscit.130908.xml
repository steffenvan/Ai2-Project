<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000887">
<title confidence="0.947647">
Extending the Entity-grid Coherence Model to Semantically Related Entities
</title>
<author confidence="0.559449">
Katja Filippova and Michael Strube
</author>
<affiliation confidence="0.333073">
EML Research gGmbH
</affiliation>
<address confidence="0.47764">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.824049">
http://www.eml-research.de/nlp/
</email>
<sectionHeader confidence="0.993387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997975375">
This paper reports on work in progress on extending
the entity-based approach on measuring coherence
(Barzilay &amp; Lapata, 2005; Lapata &amp; Barzilay, 2005)
from coreference to semantic relatedness. We use
a corpus of manually annotated German newspaper
text (T¨uBa-D/Z) and aim at improving the perfor-
mance by grouping related entities with the WikiRe-
late! API (Strube &amp; Ponzetto, 2006).
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999165086956522">
Evaluation is a well-known problem for Natu-
ral Language Generation (NLG). Human labor re-
quired to evaluate the output of a NLG system is
expensive since every text should be read by sev-
eral human judges and evaluated according to sev-
eral parameters. Automatic summarization is an ap-
plication using a NLG component which is hard
to evaluate. The Document Understanding Con-
ference1, which every year issues a summarization
task, distinguishes five aspects of linguistic qual-
ity of a summary: grammaticality, non-redundancy,
referential clarity, focus and coherence. The param-
eter for which most participants get very low scores
is coherence. This may reflect the difficulty which
(mostly) extractive methods face during the order-
ing phase. Even if selected sentences are relevant
and related, being in a wrong order they will make
the summary hard to understand. The same is true
for any other text-to-text generation system with a
multisentential output.
In this paper we consider a way of automatic
coherence assessment (Barzilay &amp; Lapata, 2005)
which is beneficial for such NLG systems. This
</bodyText>
<footnote confidence="0.905785">
1http://duc.nist.gov
</footnote>
<bodyText confidence="0.99770975">
method is based on how patterns of entity distri-
bution differ for coherent and incoherent texts. It
utilizes information of three kinds: coreference,
salience and syntax. As a suggestion for future
work, Barzilay &amp; Lapata hypothesize that integrat-
ing semantic knowledge for entity grouping (as op-
posed to coreference) should improve the results.
So, the purpose of the current study is threefold:
</bodyText>
<listItem confidence="0.999407833333333">
• to check how the method performs on a lan-
guage other than English;
• to estimate the contribution of the three knowl-
edge sources on mannualy annotated data;
• to see whether semantic clustering of entities
outperfoms the coreference baseline.
</listItem>
<sectionHeader confidence="0.848491" genericHeader="method">
2 The Entity-based Approach
</sectionHeader>
<bodyText confidence="0.999379235294118">
Barzilay &amp; Lapata (2005) describe a method for co-
herence assessment which grounds on the premises
that (1) for a text to be globally coherent it has to be
locally coherent as well; and (2) the patterns of how
entities appear throughout the text differ for coher-
ent and incoherent data.
To test their method, they consider a collection
of coherent texts2 and for each of them generate a
number of incoherent variants by putting the sen-
tences in a random order. Then, for each rendering,
they create an entity-grid where each column repre-
sents an entity and each row represents a sentence
from the text. A cell in a grid tells which syntactic
function a given entity has in a given sentence. The
set of possible functions is reduced to four: subject
(s), object (o), other (x), or nothing (-) if the entity
is not mentioned in a sentence. Two example grids
</bodyText>
<footnote confidence="0.637504">
2They experiment with a corpus of newspaper articles and
a corpus of accident reports, all in English.
</footnote>
<page confidence="0.977212">
139
</page>
<equation confidence="0.964556090909091">
e1 e2 e3 e4 e5 e6
s1 s o z - - -
s2 o s - o - -
s3 s - - - - -
s4 - - - - - s
Table 1: Coherent text grid
e1 e2 e3 e4 e5 e6
s4 - - - - - s
s1 s o z - - -
s3 s - - - - -
s2 o s - o - -
</equation>
<tableCaption confidence="0.68258">
Table 2: Incoherent text grid
</tableCaption>
<bodyText confidence="0.964124235294118">
– for a coherent text and for its shuffled version –
are presented in Tables 1 and 2 respectively.
To compare two texts which differ only in their
sentence order, each of them is represented by a fea-
ture vector. A feature stands for a possible transition
between syntactic functions of an entity (e.g. -o, sz,
sso). Unigram, bigram and trigram transitions are
distinguished. The value of a transition feature is
its probability calculated from the grid. For binary
transitions there are, thus, 4× 4 possible features. If
there are no full parses available so that one cannot
distinguish between syntactic realizations and fills
a cell with z or - only, the number of binary tran-
sitions is reduced to 2 × 2 = 4. These simplified
(i.e. without syntactic information) feature vectors
for the grids in Tables 1 and 2 are given in Table 3.
zz z- -z --
</bodyText>
<table confidence="0.962342">
91 0.17 0.28 0.17 0.39
92 0.11 0.22 0.33 0.33
</table>
<tableCaption confidence="0.998861">
Table 3: Feature vectors for grids in Tables 1 &amp; 2
</tableCaption>
<bodyText confidence="0.9999834">
The coherence assessment is then formulated as
a ranking learning problem. 5V Mlight (Joachims,
2002) is used for this task. Pairwise rankings (a co-
herent text vs. an incoherent rendering) are supplied
to the learner as the relative quality of incoherent
renderings is not known. For each document 20
pairs are generated in total.
Barzilay &amp; Lapata (2005) obtain impressive re-
sults – about 90% of ranking accuracy which is the
ratio of how often a coherent order is ranked higher
</bodyText>
<equation confidence="0.79779125">
than its incoherent variant3:
correct pairs
RA =
all pairs
</equation>
<bodyText confidence="0.999450636363637">
Barzilay &amp; Lapata (2005) demonstrate that richer
syntactic representation, as well as coreference res-
olution instead of string identity for entities identifi-
cation, improve the performance. Another finding is
that it is effective to distinguish between salient en-
tities (those mentioned more than once: e1, e2 in Ta-
bles 1 &amp; 2) and the rest. Given that they preprocess
the data automatically by employing a state-of-the-
art parser and a noun phrase coreference resolution
system, manual annotation is expected to refine the
model.
</bodyText>
<sectionHeader confidence="0.998404" genericHeader="method">
3 Reimplementation
</sectionHeader>
<bodyText confidence="0.99998605">
We reimplemented the algorithm of Barzilay &amp;
Lapata (2005) and tested it on a German corpus
of newpaper articles T¨uBa-D/Z (Telljohann et al.,
2003). This corpus provides manual syntactic4,
morphological and NP coreference annotation (Hin-
richs et al., 2004). We used the same 5VMlight
package for learning of a ranking function. Like
Barzilay &amp; Lapata, we took 100 articles for train-
ing, testing and development sets each. The results
we report below are all computed from the develop-
ment set. As results might differ considerably de-
pending on how incoherent random orders are, for
every article we continued to use the set of random
orders generated during the first try. This allowed
us to make objective judgements about the impact
of a certain parameter on the performance. We also
selected a subset of articles from the T¨uBa-D/Z in
order to make the average article length equal to
the average length of the articles Barzilay &amp; Lap-
ata used (i.e. 10.5 sentences).
</bodyText>
<subsectionHeader confidence="0.994059">
3.1 Settings
</subsectionHeader>
<bodyText confidence="0.979236111111111">
Similar to Barzilay &amp; Lapata, we experimented with
the following settings:
COREF: coreference vs. word identity for entity
identification;
SYNT: syntax-rich vs. simplified representation;
SAL: distinguishing between salient entities (men-
tioned exactly once) and the rest vs. without
this distinction.
3Note, that random baseline ensures RA of 50%.
</bodyText>
<footnote confidence="0.9829455">
4Yannick Versley kindly helped us to to convert the syntac-
tic annotation (Versley, 2005).
</footnote>
<page confidence="0.964746">
140
</page>
<table confidence="0.998565">
+COREF -COREF
+SYNT+SAL 72% 62%
+SYNT-SAL 69% 53%
-SYNT+SAL 75% 66%
-SYNT-SAL 71% 59%
</table>
<tableCaption confidence="0.999804">
Table 4: Ranking accuracy for different settings
</tableCaption>
<bodyText confidence="0.9823498">
The results for each of the settings are pre-
sented in Table 4. Although obtained from human-
annotated data, they are strikingly lower than the
results Barzilay &amp; Lapata report for English. We
concluded the following:
</bodyText>
<listItem confidence="0.803844807692308">
1. Coreference information definitely improves
the performance. Using word match for en-
tity clustering works only if combined with
salience, otherwise the method is hardly better
than the baseline.
2. The fact that quite some correct decisions
could be made with all parameters set nega-
tive (-SYNT-SAL-COREF) brought us to the idea
that there is a difference in the amount of enti-
ties mentioned in the first, the last and a middle
sentences of a text. Having calculated the av-
erage number of entities5 in these three types
of sentences, we concluded that indeed the
amount decreases as the text continues. In a
coherent text the first sentence generally intro-
duces more entities than any further sentence
mentions. The last sentence is shorter and, on
average, contains less entities than other ones.
3. Surprisingly, for our data syntactic information
turned out to have a negative impact on the re-
sults, although it may be that a larger training
set is needed to benefit from it.
4. The RA of 59% for -SYNT-SAL-COREF demon-
strates that the method can be of use even if ap-
plied to data without any information but sen-
tence boundaries.
</listItem>
<subsectionHeader confidence="0.998571">
3.2 Extended Rankings
</subsectionHeader>
<bodyText confidence="0.928327444444444">
Apart from the settings described above, we also ex-
perimented with the training data representation by
extending the pairwise ranking to longer rankings.
According to Lapata’s (2006) psycholinguistic ex-
periment, Kendall’s T correlates reliably with hu-
man judgements regarding ordering tasks. It varies
5Both new and already mentioned entities count.
between -1 and 1 and is calculated as 1 − 4t
N(N−1),
where t is the number of interchanges of adjacent
elements required to bring the total of N elements
in the right order. Assuming that the lower the T, the
less coherent a text is, we supplied the learner with
rankings of 3 sentences instead of pairwise rankings
as well as with rankings of all 21 renderings. Unfor-
tunately, this modification did not improve the re-
sults but caused a slight drop in performance: for the
best setting (-SYNT+SAL+COREF) the RA was 73%.
</bodyText>
<subsectionHeader confidence="0.999176">
3.3 Beyond Entities
</subsectionHeader>
<bodyText confidence="0.999264736842105">
For entity clustering we used the WikiRelate! API
(Strube &amp; Ponzetto, 2006) to compute relatedness
between entities. We preferred it to the GermaNet
API (Gurevych, 2005) because the latter works bet-
ter for computing semantic similarity whereas the
former is more suitable for computing semantic re-
latedness. Apart from that, given that our data is a
collection of newspaper articles containing named
entities (persons, locations, organizations) which
can be related as well, Wikipedia is a better choice
as it covers named entities as well as common nouns
(the version from 09/25/2006 has 471,065 entries).
Future work should make use of both semantic re-
sources. From the 6 possible measures implemented
in WikiRelate!, we selected the Wu&amp;Palmer mea-
sure as Strube &amp; Ponzetto (2006) report that it
demonstrated the highest correlation with humans.
The experiments with semantic relatedness had
two goals:
</bodyText>
<listItem confidence="0.969816666666667">
• to see whether it can improve the best results
achieved with coreference sets,
• to check whether semantic relatedness alone
can be reliably used for entity clustering in
case there is no coreference resolution system
available.
</listItem>
<bodyText confidence="0.99984825">
Since syntactic information affects the results nega-
tively while distinguishing between salient entites
and the rest has a positive impact on them, we
did not further experiment with all possible settings
combinations and used -SYNT+SAL.
To group similar entities together, our algorithm
proceeds as follows: when a new entity ei is en-
countered, it is measured whether it is related to al-
ready found entities E. If there is an entity ej E E
such that SemRel(ei, ej) &gt; t, where t is a thresh-
old, then its history is further assigned to this en-
tity. We experimented with different values for t:
</bodyText>
<page confidence="0.996595">
141
</page>
<bodyText confidence="0.988425">
the smaller the value, the denser the grid but the less
related words within one entity group are.
</bodyText>
<table confidence="0.996602714285714">
t -SYNT+SAL+COREF -SYNT+SAL-COREF
w/o 75% 66%
0.1 71% 66%
0.2 72% 66%
0.3 72% 68%
0.4 73% 68%
0.5 73% 69%
</table>
<tableCaption confidence="0.9859235">
Table 5: Ranking accuracy with different related-
ness thresholds
</tableCaption>
<bodyText confidence="0.999950363636363">
The results demonstrate a significant improve-
ment over the word-identity model although se-
mantic relatedness is not as good as coreference,
the difference between them still being about 5%.
Semantic clustering of entities on top of corefer-
ence grouping does not bring an improvement, at
least when done incrementally. A better approach
might be to require any two entities from one clus-
ter to have the minimum relatedness of t rather than
adding an entity to a cluster when it is related to at
least one element from the cluster.
</bodyText>
<sectionHeader confidence="0.998662" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999965285714285">
We presented our work on extending the entity-
based coherence assessment from coreference to se-
mantic relatedness and its application to German. In
spite of the fact that we used human-annotated data,
our results are considerably worse than the results
for English. This may be caused by differences be-
tween the corpora. We analysed the impact of dif-
ferent settings and problem formulations (pairwise
vs. multi-element rankings) and reported the best
parameters for German.
Our initial experiments with entity clustering us-
ing semantic relatedness gave us some evidence that
this is a promising direction to pursue. In particu-
lar, we would like to depart from the manually an-
notated data and explore cheaper approaches which
require neither a deep parser, nor a coreference res-
olution system and work fully automatically. The
RA of 69% obtained without syntactic and coref-
erence information motivates this direction of re-
search. Such an approach would provide a low-cost
coherence evaluation strategy for NLG applications
with a multisentential output.
Future work should compare (or combine) the in-
formation from Wikipedia with information from
GermaNet and determine constraints on entity
grouping. We experimented with incremental clus-
tering although it may be that “shrinking” of a com-
plete grid with a constraint on the size of a cluster
would be more effective. We would also like to test
the extended model on the data sets used by Barzi-
lay &amp; Lapata (2005).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004).
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999746904761905">
Barzilay, Regina &amp; Mirella Lapata (2005). Modelling lo-
cal coherence: An entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, Ann Arbor, Michigan,
25–30 June 2005, pp. 141–148.
Gurevych, Iryna (2005). Using the structure of a con-
ceptual network in computing semantic relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, Jeju Island, South
Korea, 11-13 October, 2005, pp. 767–778.
Hinrichs, Erhard, Sandra K¨ubler, Karin Naumann, Heike
Telljohann &amp; Julia Trushkina (2004). Recent develop-
ments in linguistic annotations of the T¨uBa-D/Z tree-
bank. In Proceedings of the 3rd Workshop on Tree-
banks and Linguistic Theories, T¨ubingen, Germany,
10–11 December 2004.
Joachims, Thorsten (2002). Optimizing search engines
using clickthrough data. In Proceedings of the 8th In-
ternational Conference on Knowledge Discovery and
Data Mining, Edmonton, Canada, 23–26 July 2002,
pp. 133–142.
Lapata, Mirella (2006). Automatic evaluation of infor-
mation ordering: Kendall’s tau. Computational Lin-
guistics, 32(4):471–484.
Lapata, Mirella &amp; Regina Barzilay (2005). Automatic
evaluation of text coherence: Models and represen-
tations. In Proceedings of the 19th International
Joint Conference on Artificial Intelligence, Edinburgh,
Schotland, 30 July–5 August, 2005, pp. 1085–1090.
Strube, Michael &amp; Simone Paolo Ponzetto (2006).
WikiRelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, Boston, Mass., 16–
20 July 2006, pp. 1219–1224.
Telljohann, Heike, Erhard Hinrichs &amp; Sandra K¨ubler
(2003). Stylebook for the T¨ubingen treebank of writ-
ten German (T¨uBa-D/Z). Technical Report: Seminar
f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.
Versley, Yannick (2005). Parser evaluation across text
types. In Proceedings of the 4th Workshop on Tree-
banks and Linguistic Theories, Barcelona, Spain, 9-10
December 2005.
</reference>
<page confidence="0.997711">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337533">
<title confidence="0.999524">Extending the Entity-grid Coherence Model to Semantically Related Entities</title>
<author confidence="0.852765">Filippova</author>
<affiliation confidence="0.829603">EML Research Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.990951">69118 Heidelberg,</address>
<web confidence="0.988529">http://www.eml-research.de/nlp/</web>
<abstract confidence="0.991263875">This paper reports on work in progress on extending the entity-based approach on measuring coherence (Barzilay &amp; Lapata, 2005; Lapata &amp; Barzilay, 2005) from coreference to semantic relatedness. We use a corpus of manually annotated German newspaper text (T¨uBa-D/Z) and aim at improving the performance by grouping related entities with the WikiRe-</abstract>
<note confidence="0.629513">late! API (Strube &amp; Ponzetto, 2006).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1667" citStr="Barzilay &amp; Lapata, 2005" startWordPosition="246" endWordPosition="249">task, distinguishes five aspects of linguistic quality of a summary: grammaticality, non-redundancy, referential clarity, focus and coherence. The parameter for which most participants get very low scores is coherence. This may reflect the difficulty which (mostly) extractive methods face during the ordering phase. Even if selected sentences are relevant and related, being in a wrong order they will make the summary hard to understand. The same is true for any other text-to-text generation system with a multisentential output. In this paper we consider a way of automatic coherence assessment (Barzilay &amp; Lapata, 2005) which is beneficial for such NLG systems. This 1http://duc.nist.gov method is based on how patterns of entity distribution differ for coherent and incoherent texts. It utilizes information of three kinds: coreference, salience and syntax. As a suggestion for future work, Barzilay &amp; Lapata hypothesize that integrating semantic knowledge for entity grouping (as opposed to coreference) should improve the results. So, the purpose of the current study is threefold: • to check how the method performs on a language other than English; • to estimate the contribution of the three knowledge sources on </context>
<context position="4881" citStr="Barzilay &amp; Lapata (2005)" startWordPosition="840" endWordPosition="843">ransitions is reduced to 2 × 2 = 4. These simplified (i.e. without syntactic information) feature vectors for the grids in Tables 1 and 2 are given in Table 3. zz z- -z -- 91 0.17 0.28 0.17 0.39 92 0.11 0.22 0.33 0.33 Table 3: Feature vectors for grids in Tables 1 &amp; 2 The coherence assessment is then formulated as a ranking learning problem. 5V Mlight (Joachims, 2002) is used for this task. Pairwise rankings (a coherent text vs. an incoherent rendering) are supplied to the learner as the relative quality of incoherent renderings is not known. For each document 20 pairs are generated in total. Barzilay &amp; Lapata (2005) obtain impressive results – about 90% of ranking accuracy which is the ratio of how often a coherent order is ranked higher than its incoherent variant3: correct pairs RA = all pairs Barzilay &amp; Lapata (2005) demonstrate that richer syntactic representation, as well as coreference resolution instead of string identity for entities identification, improve the performance. Another finding is that it is effective to distinguish between salient entities (those mentioned more than once: e1, e2 in Tables 1 &amp; 2) and the rest. Given that they preprocess the data automatically by employing a state-of-t</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Barzilay, Regina &amp; Mirella Lapata (2005). Modelling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan, 25–30 June 2005, pp. 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
</authors>
<title>Using the structure of a conceptual network in computing semantic relatedness.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>767--778</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="9604" citStr="Gurevych, 2005" startWordPosition="1613" endWordPosition="1614">adjacent elements required to bring the total of N elements in the right order. Assuming that the lower the T, the less coherent a text is, we supplied the learner with rankings of 3 sentences instead of pairwise rankings as well as with rankings of all 21 renderings. Unfortunately, this modification did not improve the results but caused a slight drop in performance: for the best setting (-SYNT+SAL+COREF) the RA was 73%. 3.3 Beyond Entities For entity clustering we used the WikiRelate! API (Strube &amp; Ponzetto, 2006) to compute relatedness between entities. We preferred it to the GermaNet API (Gurevych, 2005) because the latter works better for computing semantic similarity whereas the former is more suitable for computing semantic relatedness. Apart from that, given that our data is a collection of newspaper articles containing named entities (persons, locations, organizations) which can be related as well, Wikipedia is a better choice as it covers named entities as well as common nouns (the version from 09/25/2006 has 471,065 entries). Future work should make use of both semantic resources. From the 6 possible measures implemented in WikiRelate!, we selected the Wu&amp;Palmer measure as Strube &amp; Pon</context>
</contexts>
<marker>Gurevych, 2005</marker>
<rawString>Gurevych, Iryna (2005). Using the structure of a conceptual network in computing semantic relatedness. In Proceedings of the 2nd International Joint Conference on Natural Language Processing, Jeju Island, South Korea, 11-13 October, 2005, pp. 767–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Karin Naumann</author>
</authors>
<title>Heike Telljohann &amp; Julia Trushkina</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>T¨ubingen,</location>
<marker>Hinrichs, K¨ubler, Naumann, 2004</marker>
<rawString>Hinrichs, Erhard, Sandra K¨ubler, Karin Naumann, Heike Telljohann &amp; Julia Trushkina (2004). Recent developments in linguistic annotations of the T¨uBa-D/Z treebank. In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories, T¨ubingen, Germany, 10–11 December 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>133--142</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="4627" citStr="Joachims, 2002" startWordPosition="799" endWordPosition="800">lculated from the grid. For binary transitions there are, thus, 4× 4 possible features. If there are no full parses available so that one cannot distinguish between syntactic realizations and fills a cell with z or - only, the number of binary transitions is reduced to 2 × 2 = 4. These simplified (i.e. without syntactic information) feature vectors for the grids in Tables 1 and 2 are given in Table 3. zz z- -z -- 91 0.17 0.28 0.17 0.39 92 0.11 0.22 0.33 0.33 Table 3: Feature vectors for grids in Tables 1 &amp; 2 The coherence assessment is then formulated as a ranking learning problem. 5V Mlight (Joachims, 2002) is used for this task. Pairwise rankings (a coherent text vs. an incoherent rendering) are supplied to the learner as the relative quality of incoherent renderings is not known. For each document 20 pairs are generated in total. Barzilay &amp; Lapata (2005) obtain impressive results – about 90% of ranking accuracy which is the ratio of how often a coherent order is ranked higher than its incoherent variant3: correct pairs RA = all pairs Barzilay &amp; Lapata (2005) demonstrate that richer syntactic representation, as well as coreference resolution instead of string identity for entities identificatio</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, Thorsten (2002). Optimizing search engines using clickthrough data. In Proceedings of the 8th International Conference on Knowledge Discovery and Data Mining, Edmonton, Canada, 23–26 July 2002, pp. 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Lapata, 2006</marker>
<rawString>Lapata, Mirella (2006). Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence, Edinburgh, Schotland, 30 July–5</booktitle>
<pages>1085--1090</pages>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Lapata, Mirella &amp; Regina Barzilay (2005). Automatic evaluation of text coherence: Models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, Edinburgh, Schotland, 30 July–5 August, 2005, pp. 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>WikiRelate! Computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<volume>16</volume>
<pages>1219--1224</pages>
<location>Boston, Mass.,</location>
<contexts>
<context position="9510" citStr="Strube &amp; Ponzetto, 2006" startWordPosition="1597" endWordPosition="1600">s count. between -1 and 1 and is calculated as 1 − 4t N(N−1), where t is the number of interchanges of adjacent elements required to bring the total of N elements in the right order. Assuming that the lower the T, the less coherent a text is, we supplied the learner with rankings of 3 sentences instead of pairwise rankings as well as with rankings of all 21 renderings. Unfortunately, this modification did not improve the results but caused a slight drop in performance: for the best setting (-SYNT+SAL+COREF) the RA was 73%. 3.3 Beyond Entities For entity clustering we used the WikiRelate! API (Strube &amp; Ponzetto, 2006) to compute relatedness between entities. We preferred it to the GermaNet API (Gurevych, 2005) because the latter works better for computing semantic similarity whereas the former is more suitable for computing semantic relatedness. Apart from that, given that our data is a collection of newspaper articles containing named entities (persons, locations, organizations) which can be related as well, Wikipedia is a better choice as it covers named entities as well as common nouns (the version from 09/25/2006 has 471,065 entries). Future work should make use of both semantic resources. From the 6 p</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Strube, Michael &amp; Simone Paolo Ponzetto (2006). WikiRelate! Computing semantic relatedness using Wikipedia. In Proceedings of the 21st National Conference on Artificial Intelligence, Boston, Mass., 16– 20 July 2006, pp. 1219–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
</authors>
<title>Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z).</title>
<date>2003</date>
<tech>Technical Report: Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</tech>
<marker>Telljohann, Hinrichs, K¨ubler, 2003</marker>
<rawString>Telljohann, Heike, Erhard Hinrichs &amp; Sandra K¨ubler (2003). Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z). Technical Report: Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
</authors>
<title>Parser evaluation across text types.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="7031" citStr="Versley, 2005" startWordPosition="1185" endWordPosition="1186">of articles from the T¨uBa-D/Z in order to make the average article length equal to the average length of the articles Barzilay &amp; Lapata used (i.e. 10.5 sentences). 3.1 Settings Similar to Barzilay &amp; Lapata, we experimented with the following settings: COREF: coreference vs. word identity for entity identification; SYNT: syntax-rich vs. simplified representation; SAL: distinguishing between salient entities (mentioned exactly once) and the rest vs. without this distinction. 3Note, that random baseline ensures RA of 50%. 4Yannick Versley kindly helped us to to convert the syntactic annotation (Versley, 2005). 140 +COREF -COREF +SYNT+SAL 72% 62% +SYNT-SAL 69% 53% -SYNT+SAL 75% 66% -SYNT-SAL 71% 59% Table 4: Ranking accuracy for different settings The results for each of the settings are presented in Table 4. Although obtained from humanannotated data, they are strikingly lower than the results Barzilay &amp; Lapata report for English. We concluded the following: 1. Coreference information definitely improves the performance. Using word match for entity clustering works only if combined with salience, otherwise the method is hardly better than the baseline. 2. The fact that quite some correct decisions</context>
</contexts>
<marker>Versley, 2005</marker>
<rawString>Versley, Yannick (2005). Parser evaluation across text types. In Proceedings of the 4th Workshop on Treebanks and Linguistic Theories, Barcelona, Spain, 9-10 December 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>