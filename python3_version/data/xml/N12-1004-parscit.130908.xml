<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.995425">
Fast Inference in Phrase Extraction Models with Belief Propagation
</title>
<author confidence="0.998661">
David Burkett and Dan Klein
</author>
<affiliation confidence="0.998757">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.997566">
{dburkett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998111866666667">
Modeling overlapping phrases in an align-
ment model can improve alignment quality
but comes with a high inference cost. For
example, the model of DeNero and Klein
(2010) uses an ITG constraint and beam-based
Viterbi decoding for tractability, but is still
slow. We first show that their model can be
approximated using structured belief propaga-
tion, with a gain in alignment quality stem-
ming from the use of marginals in decoding.
We then consider a more flexible, non-ITG
matching constraint which is less efficient for
exact inference but more efficient for BP. With
this new constraint, we achieve a relative error
reduction of 40% in F5 and a 5.5x speed-up.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999375">
Modern statistical machine translation (MT) sys-
tems most commonly infer their transfer rules from
word-level alignments (Koehn et al., 2007; Li and
Khudanpur, 2008; Galley et al., 2004), typically
using a deterministic heuristic to convert these to
phrase alignments (Koehn et al., 2003). There have
been many attempts over the last decade to develop
model-based approaches to the phrase alignment
problem (Marcu and Wong, 2002; Birch et al., 2006;
DeNero et al., 2008; Blunsom et al., 2009). How-
ever, most of these have met with limited success
compared to the simpler heuristic method. One key
problem with typical models of phrase alignment
is that they choose a single (latent) segmentation,
giving rise to undesirable modeling biases (DeNero
et al., 2006) and reducing coverage, which in turn
reduces translation quality (DeNeefe et al., 2007;
DeNero et al., 2008). On the other hand, the extrac-
tion heuristic identifies many overlapping options,
and achieves high coverage.
</bodyText>
<page confidence="0.978468">
29
</page>
<bodyText confidence="0.999991621621621">
In response to these effects, the recent phrase
alignment work of DeNero and Klein (2010) mod-
els extraction sets: collections of overlapping phrase
pairs that are consistent with an underlying word
alignment. Their extraction set model is empirically
very accurate. However, the ability to model over-
lapping – and therefore non-local – features comes
at a high computational cost. DeNero and Klein
(2010) handle this in part by imposing a structural
ITG constraint (Wu, 1997) on the underlying word
alignments. This permits a polynomial-time algo-
rithm, but it is still O(n6), with a large constant
factor once the state space is appropriately enriched
to capture overlap. Therefore, they use a heavily
beamed Viterbi search procedure to find a reason-
able alignment within an acceptable time frame. In
this paper, we show how to use belief propagation
(BP) to improve on the model’s ITG-based struc-
tural formulation, resulting in a new model that is
simultaneously faster and more accurate.
First, given the model of DeNero and Klein
(2010), we decompose it into factors that admit
an efficient BP approximation. BP is an inference
technique that can be used to efficiently approxi-
mate posterior marginals on variables in a graphical
model; here the marginals of interest are the phrase
pair posteriors. BP has only recently come into use
in the NLP community, but it has been shown to be
effective in other complex structured classification
tasks, such as dependency parsing (Smith and Eis-
ner, 2008). There has also been some prior success
in using BP for both discriminative (Niehues and
Vogel, 2008) and generative (Cromi`eres and Kuro-
hashi, 2009) word alignment models.
By aligning all phrase pairs whose posterior under
BP exceeds some fixed threshold, our BP approxi-
mation of the model of DeNero and Klein (2010) can
</bodyText>
<page confidence="0.4656285">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999944529411765">
achieve a comparable phrase pair F1. Furthermore,
because we have posterior marginals rather than a
single Viterbi derivation, we can explicitly force the
aligner to choose denser extraction sets simply by
lowering the marginal threshold. Therefore, we also
show substantial improvements over DeNero and
Klein (2010) in recall-heavy objectives, such as F5.
More importantly, we also show how the BP fac-
torization allows us to relax the ITG constraint, re-
placing it with a new set of constraints that per-
mit a wider family of alignments. Compared to
ITG, the resulting model is less efficient for exact
inference (where it is exponential), but more effi-
cient for our BP approximation (where it is only
quadratic). Our new model performs even better
than the ITG-constrained model on phrase align-
ment metrics while being faster by a factor of 5.5x.
</bodyText>
<sectionHeader confidence="0.988694" genericHeader="introduction">
2 Extraction Set Models
</sectionHeader>
<bodyText confidence="0.999095307692308">
Figure 1 shows part of an aligned sentence pair, in-
cluding the word-to-word alignments, and the ex-
tracted phrase pairs licensed by those alignments.
Formally, given a sentence pair (e, f), a word-level
alignment a is a collection of links between target
words ez and source words fj. Following past work,
we further divide word links into two categories:
sure and possible, shown in Figure 1 as solid and
hatched grey squares, respectively. We represent a
as a grid of ternary word link variables azj, each of
which can take the value sure to represent a sure link
between ez and fj, poss to represent a possible link,
or off to represent no link.
An extraction set π is a set of aligned phrase pairs
to be extracted from (e, f), shown in Figure 1 as
green rounded rectangles. We represent π as a set of
boolean variables πghkt, which each have the value
true when the target span [g, h] is phrase-aligned to
the source span [k, `]. Following previous work on
phrase extraction, we limit the size of π by imposing
a phrase length limit d: π only contains a variable
πghkt if h − g &lt; d and ` − k &lt; d.
There is a deterministic mapping π(a) from a
word alignment to the extraction set licensed by that
word alignment. We will briefly describe it here, and
then present our factorized model.
</bodyText>
<figure confidence="0.769713833333333">
A
fs
f7
fs
f9
e3 e4 e5 e6 e7
</figure>
<figureCaption confidence="0.858465">
Figure 1: A schematic representation of part of a sen-
tence pair. Solid grey squares indicate sure links (e.g.
a48 = sure), and hatched squares possible links (e.g.
</figureCaption>
<bodyText confidence="0.793762625">
a67 = poss). Rounded green rectangles are extracted
phrase pairs (e.g. π5667 = true). Target spans are shown
as blue vertical lines and source spans as red horizontal
lines. Because there is a sure link at a48, σA = [4,4] does
not include the possible link at a38. However, f7 only
has possible links, so σ�7 = [5,6] is the span containing
those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks
all phrase pairs containing f9 from being extracted.
</bodyText>
<subsectionHeader confidence="0.998332">
2.1 Extraction Sets from Word Alignments
</subsectionHeader>
<bodyText confidence="0.992355">
The mapping from a word alignment to the set of
licensed phrase pairs π(a) is based on the standard
rule extraction procedures used in most modern sta-
tistical systems (Koehn et al., 2003; Galley et al.,
2006; Chiang, 2007), but extended to handle pos-
sible links (DeNero and Klein, 2010). We start by
using a to find a projection from each target word ez
onto a source span, represented as blue vertical lines
in Figure 1. Similarly, source words project onto
target spans (red horizontal lines in Figure 1). π(a)
contains a phrase pair iff every word in the target
span projects within the source span and vice versa.
Figure 1 contains an example for d = 2.
Formally, the mapping introduces a set of spans
σ. We represent the spans as variables whose values
are intervals, where σz = [k, `] means that the tar-
get word ez projects to the source span [k, `]. The
set of legal values for σ zincludes any interval with
0 ≤ k ≤ ` &lt; |f |and ` − k &lt; d, plus the special in-
terval [−1, ∞] that indicates ez is null-aligned. The
span variables for source words σ� have target spans
[g, h] as values and are defined analogously.
For a set I of positions, we define the range func-
</bodyText>
<equation confidence="0.976518666666667">
σS = [
� , ]
σ6 = [5,6]
�
σ7 = [5,6]
�
σ8 = [4, 4]
�
σ� ∞�
30
tion:
�
[−1, ∞] I = ∅
range(I) = (1)
[mini∈I i, maxi∈I i] else
</equation>
<bodyText confidence="0.742362">
For a fixed word alignment a we set the target
span variable Qe i:
</bodyText>
<equation confidence="0.997757">
Qe i,s = range({j : aij = sure}) (2)
Qei,p = range({j : aij =6 off}) (3)
Qei = Qei,s ∩ Qi,
p (4)
</equation>
<bodyText confidence="0.999928846153846">
As illustrated in Figure 1, this sets Qei to the min-
imal span containing all the source words with a
sure link to ei if there are any. Otherwise, because
of the special case for range(I) when I is empty,
Qe i,s = [−1, ∞], so Qe iis the minimal span containing
all poss-aligned words. If all word links to ei are off,
indicating that ei is null-aligned, then Qei is [−1, ∞],
preventing the alignment of any phrase pairs con-
taining ei.
Finally, we specify which phrase pairs should be
included in the extraction set 7r. Given the spans Q
based on a, 7r(a) sets 7rghk` = true iff every word in
each phrasal span projects within the other:
</bodyText>
<equation confidence="0.9363845">
Qei ⊆ [k, E] ∀i ∈ [g, h] (5)
Qj ⊆ [g, h] f∀j ∈ [k, E]
</equation>
<subsectionHeader confidence="0.977026">
2.2 Formulation as a Graphical Model
</subsectionHeader>
<bodyText confidence="0.946154">
We score triples (a, 7r, Q) as the dot product of a
weight vector w that parameterizes our model and a
feature vector O(a, 7r, Q). The feature vector decom-
poses into word alignment features Oa, phrase pair
features Oπ and target and source null word features
Oe∅ and Of∅:1
</bodyText>
<equation confidence="0.9758108">
1: 1:
O(a, 7r, Q) = Oa(aij) +
i,j g,h,k,`
1: Oe∅(Qei ) + 1: Of∅(Qfj ) (6)
i j
</equation>
<bodyText confidence="0.993314925">
This feature function is exactly the same as that
used by DeNero and Klein (2010).2 However, while
1In addition to the arguments we write out explicitly, all fea-
ture functions have access to the observed sentence pair (e, f).
2Although the null word features are not described in DeN-
ero and Klein (2010), all of their reported results include these
features (DeNero, 2010).
they formulated their inference problem as a search
for the highest scoring triple (a, 7r, Q) for an ob-
served sentence pair (e, f), we wish to derive a con-
ditional probability distribution p(a, 7r, Q|e, f). We
do this with the standard transformation for linear
models: p(a, 7r, Q|e, f) ∝ exp(w·O(a, 7r, Q)). Due to
the factorization in Eq. (6), this exponentiated form
becomes a product of local multiplicative factors,
and hence our model forms an undirected graphical
model, or Markov random field.
In addition to the scoring function, our model
also includes constraints on which triples (a, 7r, Q)
have nonzero probability. DeNero and Klein (2010)
implicitly included these constraints in their repre-
sentation: instead of sets of variables, they used a
structured representation that only encodes triples
(a, 7r, Q) satisfying both the mapping 7r = 7r(a) and
the structural constraint that a can be generated by
a block ITG grammar. However, our inference pro-
cedure, BP, requires that we represent (a, 7r, Q) as an
assignment of values to a set of variables. Therefore,
we must explicitly encode all constraints into the
multiplicative factors that define the model. To ac-
complish this, in addition to the soft scoring factors
we have already mentioned, our model also includes
a set of hard constraint factors. Hard constraint fac-
tors enforce the relationships between the variables
of the model by taking a value of 0 when the con-
straints they encode are violated and a value of 1
when they are satisfied. The full factor graph rep-
resentation of our model, including both soft scor-
ing factors and hard constraint factors, is drawn
schematically in Figure 2.
</bodyText>
<subsectionHeader confidence="0.727647">
2.2.1 Soft Scoring Factors
</subsectionHeader>
<bodyText confidence="0.999930153846154">
The scoring factors all take the form exp(w · O),
and so can be described in terms of their respective
local feature vectors, O. Depending on the values of
the variables each factor depends on, the factor can
be active or inactive. Features are only extracted for
active factors; otherwise O is empty and the factor
produces a value of 1.
SURELINK. Each word alignment variable aij
has a corresponding SURELINK factor Lij to incor-
porate scores from the features Oa(aij). Lij is ac-
tive whenever aij = sure. Oa(aij) includes poste-
riors from unsupervised jointly trained HMM word
alignment models (Liang et al., 2006), dictionary
</bodyText>
<equation confidence="0.738264">
Oπ(7rghk`)+
</equation>
<page confidence="0.990889">
31
</page>
<figure confidence="0.999921696969697">
A
all a i
a,
al a
LI,
L1
L1
a
L 1
L
al
L 1
L
a Irl a Irl
a a
a a
S
σ σ
N
S
N
al-1
al-1
P
Sf
Sf
π
σf
σf
R
Nf
Nf
(a) ITG factor (b) SPAN and EXTRACT factors
</figure>
<figureCaption confidence="0.993447">
Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph
separated into two components: one containing the factors that only neighbor word link variables, and one containing
the remaining factors.
</figureCaption>
<bodyText confidence="0.999982941176471">
and identical word features, a position distortion fea-
ture, and features for numbers and punctuation.
PHRASEPAIR. For each phrase pair variable
7rghk`, scores from 0π(7rghk`) come from the factor
Rghk`, which is active if 7rghk` = true. Most of the
model’s features are on these factors, and include
relative frequency statistics, lexical template indica-
tor features, and indicators for numbers of words and
Chinese characters. See DeNero and Klein (2010)
for a more comprehensive list.
NULLWORD. We can determine if a word is
null-aligned by looking at its corresponding span
variable. Thus, we include features from 0e�(Qei ) in
a factor Nei that is active if Qe i= [−1, oc]. The
features are mostly indicators for common words.
There are also factors Nfj for source words, which
are defined analogously.
</bodyText>
<subsectionHeader confidence="0.489649">
2.2.2 Hard Constraint Factors
</subsectionHeader>
<bodyText confidence="0.9972835625">
We encode the hard constraints on relationships
between variables in our model using three fami-
lies of factors, shown graphically in Figure 2. The
SPAN and EXTRACT factors together ensure that
7r = 7r(a). The ITG factor encodes the structural
constraint on a.
SPAN. First, for each target word ei we include
a factor Sei to ensure that the span variable Qe ihas
a value that agrees with the projection of the word
alignment a. As shown in Figure 2b, Sei depends
on Qei and all the word alignment variables aij in
column i of the word alignment grid. Sei has value
1 iff the equality in Eq. (4) holds. Our model also
includes a factor Sfj to enforce the analogous rela-
tionship between each Qfj and corresponding row j
of a.
EXTRACT. For each phrase pair variable 7rghk`
we have a factor Pghk` to ensure that 7rghk` = true
iff it is licensed by the span projections Q. As shown
in Figure 2b, in addition to 7rghk`, Pghk` depends on
the range of span variables Qei for i E [g, h] and Qfj
for j E [k, e]. Pghk` is satisfied when 7rghk` = true
and the relations in Eq. (5) all hold, or when 7rghk` =
false and at least one of those relations does not hold.
ITG. Finally, to enforce the structural constraint
on a, we include a single global factor A that de-
pends on all the word link variables in a (see Fig-
ure 2a). A is satisfied iff a is in the family of
block inverse transduction grammar (ITG) align-
ments. The block ITG family permits multiple links
to be on (aij =� off) for a particular word ei via termi-
nal block productions, but ensures that every word is
</bodyText>
<page confidence="0.996452">
32
</page>
<bodyText confidence="0.999434">
in at most one such terminal production, and that the
full set of terminal block productions is consistent
with ITG reordering patterns (Zhang et al., 2008).
</bodyText>
<sectionHeader confidence="0.981681" genericHeader="method">
3 Relaxing the ITG Constraint
</sectionHeader>
<bodyText confidence="0.985728081081081">
The ITG factor can be viewed as imposing two dif-
ferent types of constraints on allowable word align-
ments a. First, it requires that each word is aligned
to at most one relatively short subspan of the other
sentence. This is a linguistically plausible con-
straint, as it is rarely the case that a single word will
translate to an extremely long phrase, or to multiple
widely separated phrases.3
The other constraint imposed by the ITG factor
is the ITG reordering constraint. This constraint
is imposed primarily for reasons of computational
tractability: the standard dynamic program for bi-
text parsing depends on ITG reordering (Wu, 1997).
While this constraint is not dramatically restric-
tive (Haghighi et al., 2009), it is plausible that re-
moving it would permit the model to produce better
alignments. We tested this hypothesis by develop-
ing a new model that enforces only the constraint
that each word align to one limited-length subspan,
which can be viewed as a generalization of the at-
most-one-to-one constraint frequently considered in
the word-alignment literature (Taskar et al., 2005;
Cromi`eres and Kurohashi, 2009).
Our new model has almost exactly the same form
as the previous one. The only difference is that A is
replaced with a new family of simpler factors:
ONESPAN. For each target word ei (and each
source word fj) we include a hard constraint factor
Uei (respectively Ufj ). Uei is satisfied iff |ue i,p |&lt; d
(length limit) and either ue P = [−1, oc] or bj E
uei,p, aij � off (no gaps), with ue p as in Eq. (3). Fig-
i,
ure 3 shows the portion of the factor graph from Fig-
ure 2a redrawn with the ONESPAN factors replacing
the ITG factor. As Figure 3 shows, there is no longer
a global factor; each Uei depends only on the word
link variables from column i.
</bodyText>
<footnote confidence="0.929243666666667">
3Short gaps can be accomodated within block ITG (and in
our model are represented as possible links) as long as the total
aligned span does not exceed the block size.
</footnote>
<figureCaption confidence="0.996773">
Figure 3: ONESPAN factors
</figureCaption>
<sectionHeader confidence="0.98813" genericHeader="method">
4 Belief Propagation
</sectionHeader>
<bodyText confidence="0.999896777777778">
Belief propagation is a generalization of the well
known sum-product algorithm for undirected graph-
ical models. We will provide only a procedural
sketch here, but a good introduction to BP for in-
ference in structured NLP models can be found
in Smith and Eisner (2008), and Chapters 16 and 23
of MacKay (2003) contain a general introduction to
BP in the more general context of message-passing
algorithms.
At a high level, each variable maintains a local
distribution over its possible values. These local dis-
tribution are updated via messages passed between
variables and factors. For a variable V , N(V ) de-
notes the set of factors neighboring V in the fac-
tor graph. Similarly, N(F) is the set of variables
neighboring the factor F. During each round of BP,
messages are sent from each variable to each of its
neighboring factors:
</bodyText>
<equation confidence="0.965273666666667">
(k+1)TT
qV ,—&gt;F (v) a 11
GEN(V ),G�F
</equation>
<bodyText confidence="0.994473">
and from each factor to each of its neighboring vari-
ables:
</bodyText>
<equation confidence="0.92884225">
�
(
rF,—&gt;V (v) a
XF,XF [V]=v UEN(F),U�V
</equation>
<bodyText confidence="0.982432">
where XF is a partial assignment of values to just
the variables in N(F).
</bodyText>
<figure confidence="0.995950904761905">
Ul U U
Uf
Uf
1
Uf
all a 1
al
al a
Lll
Ll
Ll
a
Ll
L
al
Ll
L
rG,—&gt;V (v)
(k) (7)
� qU,—&gt;F(v) (8)
F(XF) (k)
</figure>
<page confidence="0.993929">
33
</page>
<bodyText confidence="0.972345333333333">
Marginal beliefs at time k can be computed by
simply multiplying together all received messages
and normalizing:
</bodyText>
<equation confidence="0.7893265">
b(k) (v) a fl k
GEN(V ) rG—,V (v) (9)
</equation>
<bodyText confidence="0.971054142857143">
Although messages can be updated according to
any schedule, generally one iteration of BP updates
each message once. The process iterates until some
stopping criterion has been met: either a fixed num-
ber of iterations or some convergence metric.
For our models, we say that BP has converged
whenever EV v (buk) (v) − buk−1) (v) )2 &lt; S for
</bodyText>
<listItem confidence="0.820463">
some small S &gt; 0.4 While we have no theoretical
convergence guarantees, it usually converges within
10 iterations in practice.
</listItem>
<sectionHeader confidence="0.910391" genericHeader="method">
5 Efficient BP for Extraction Set Models
</sectionHeader>
<bodyText confidence="0.999852833333334">
In general, the efficiency of BP depends directly on
the arity of the factors in the model. Performed
naively, the sum in Eq. (8) will take time that grows
exponentially with the size of N(F). For the soft-
scoring factors, which each depend only on a single
variable, this isn’t a problem. However, our model
also includes factors whose arity grows with the in-
put size: for example, explicitly enumerating all as-
signments to the word link variables that the ITG
factor depends on would take O(3n2) time.5
To run BP in a reasonable time frame, we need
efficient factor-specific propagators that can exploit
the structure of the factor functions to compute out-
going messages in polynomial time (Duchi et al.,
2007; Smith and Eisner, 2008). Fortunately, all of
our hard constraints permit dynamic programs that
accomplish this propagation. Space does not permit
a full description of these dynamic programs, but we
will briefly sketch the intuitions behind them.
SPAN and ONESPAN. Marginal beliefs for Sei or
Uei can be computed in O(nd2) time. The key obser-
vation is that for any legal value Qei = [k, E], Sei and
Uei require that aij = off for all j V [k, E].6 Thus, we
start by computing the product of all the off beliefs:
</bodyText>
<footnote confidence="0.9713392">
4We set S = 0.001.
5For all asymptotic analysis, we define n = max(Jel, Ifl).
6For ease of exposition, we assume that all alignments are
either sure or off; the modifications to account for the general
case are straightforward.
</footnote>
<table confidence="0.999933875">
Factor Runtime Count Total
SURELINK O(1) O(n2) O(n2)
PHRASEPAIR O(1) O(n2d2) O(n2d2)
NULLWORD O(nd) O(n) O(n2d)
SPAN O(nd2) O(n) O(n2d2)
EXTRACT O(d3) O(n2d2) O(n2d5)
ITG O(n6) 1 O(n6)
ONESPAN O(nd2) O(n) O(n2d2)
</table>
<tableCaption confidence="0.999949">
Table 1: Asymptotic complexity for all factors.
</tableCaption>
<bodyText confidence="0.99577009375">
b = Hj qa,,(off). Then, for each of the O(nd) legal
source spans [k, E] we can efficiently find a joint be-
lief by summing over consistent assignments to the
O(d) link variables in that span.
EXTRACT. Marginal beliefs for Pghk` can be
computed in O(d3) time. For each of the O(d) target
words, we can find the total incoming belief that Qei
is within [k, E] by summing over the O(d2) values
[k&apos;, E&apos;] where [k&apos;, E&apos;] C_ [k, E]. Likewise for source
words. Multiplying together these per-word beliefs
and the belief that 7rghk` = true yields the joint be-
lief of a consistent assignment with 7rghk` = true,
which can be used to efficiently compute outgoing
messages.
ITG. To build outgoing messages, the ITG fac-
tor A needs to compute marginal beliefs for all of the
word link variables aij. These can all be computed
in O(n6) time by using a standard bitext parser to
run the inside-outside algorithm. By using a normal
form grammar for block ITG with nulls (Haghighi
et al., 2009), we ensure that there is a 1-1 correspon-
dence between the ITG derivations the parser sums
over and word alignments a that satisfy A.
The asymptotic complexity for all the factors is
shown in Table 1. The total complexity for inference
in each model is simply the sum of the complexities
of its factors, so the complexity of the ITG model is
O(n2d5 + n6), while the complexity of the relaxed
model is just O(n2d5). The complexity of exact in-
ference, on the other hand, is exponential in d for the
ITG model and exponential in both d and n for the
relaxed model.
</bodyText>
<page confidence="0.998346">
34
</page>
<sectionHeader confidence="0.914211" genericHeader="method">
6 Training and Decoding
</sectionHeader>
<bodyText confidence="0.999972375">
We use BP to compute marginal posteriors, which
we use at training time to get expected feature counts
and at test time for posterior decoding. For each sen-
tence pair, we continue to pass messages until either
the posteriors converge, or some maximum number
of iterations has been reached.7 After running BP,
the marginals we are interested in can all be com-
puted with Eq. (9).
</bodyText>
<subsectionHeader confidence="0.998176">
6.1 Training
</subsectionHeader>
<bodyText confidence="0.999988166666667">
We train the model to maximize the log likelihood of
manually word-aligned gold training sentence pairs
(with L2 regularization). Because 7r and Q are deter-
mined when a is observed, the model has no latent
variables. Therefore, the gradient takes the standard
form for loglinear models:
</bodyText>
<equation confidence="0.996625666666667">
VLL = 0(a,7r,Q) − (10)
p(a0, 7r0, Q0|e, f)0(a0, 7r0, Q0) − Aw
a/�-/�v/
</equation>
<bodyText confidence="0.9690194">
The feature vector 0 contains features on sure
word links, extracted phrase pairs, and null-aligned
words. Approximate expectations of these features
can be efficiently computed using the marginal be-
liefs baij(sure), b-ghk`(true), and b,e i([−1, oc]) and
baf ([−1, oc] ), respectively. We learned our final
j
weight vector w using AdaGrad (Duchi et al., 2010),
an adaptive subgradient version of standard stochas-
tic gradient ascent.
</bodyText>
<subsectionHeader confidence="0.999795">
6.2 Testing
</subsectionHeader>
<bodyText confidence="0.999880444444444">
We evaluate our model by measuring precision and
recall on extracted phrase pairs. Thus, the decod-
ing problem takes a sentence pair (e, f) as input, and
must produce an extraction set 7r as output. Our ap-
proach, posterior thresholding, is extremely simple:
we set 7rghkt = true iff b-ghk`(true) &gt; T for some
fixed threshold T. Note that this decoding method
does not require that there be any underlying word
alignment a licensing the resulting extraction set 7r,8
</bodyText>
<footnote confidence="0.8569522">
7See Section 7.2 for an empirical investigation of this maxi-
mum.
8This would be true even if we computed posteriors ex-
actly, but is especially true with approximate marginals from
BP, which are not necessarily consistent.
</footnote>
<bodyText confidence="0.995835625">
but the structure of the model is such that two con-
flicting phrase pairs are unlikely to simultaneously
have high posterior probability.
Most publicly available translation systems ex-
pect word-level alignments as input. These can
also be generated by applying posterior threshold-
ing, aligning target word i to source word j when-
ever baij(sure) &gt; t.9
</bodyText>
<sectionHeader confidence="0.9969" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99999">
Our experiments are performed on Chinese-to-
English alignment. We trained and evaluated all
models on the NIST MT02 test set, which consists
of 150 training and 191 test sentences and has been
used previously in alignment experiments (Ayan and
Dorr, 2006; Haghighi et al., 2009; DeNero and
Klein, 2010). The unsupervised HMM word aligner
used to generate features for the model was trained
on 11.3 million words of FBIS newswire data. We
test three models: the Viterbi ITG model of DeNero
and Klein (2010), our BP ITG model that uses the
ITG factor, and our BP Relaxed model that replaces
the ITG factor with the ONESPAN factors. In all of
our experiments, the phrase length d was set to 3.10
</bodyText>
<subsectionHeader confidence="0.997715">
7.1 Phrase Alignment
</subsectionHeader>
<bodyText confidence="0.999682611111111">
We tested the models by computing precision and
recall on extracted phrase pairs, relative to the gold
phrase pairs of up to length 3 induced by the gold
word alignments. For the BP models, we trade
off precision and recall by adjusting the decoding
threshold T. The Viterbi ITG model was trained to
optimize F5, a recall-biased measure, so in addition
to F1, we also report the recall-biased F2 and F5
measures. The maximum number of BP iterations
was set to 5 for the BP ITG model and to 10 for the
BP Relaxed model.
The phrase alignment results are shown in Fig-
ure 4. The BP ITG model performs comparably to
the Viterbi ITG model. However, because posterior
decoding permits explicit tradeoffs between preci-
sion and recall, it can do much better in the recall-
biased measures, even though the Viterbi ITG model
was explicitly trained to maximize F5 (DeNero and
</bodyText>
<footnote confidence="0.99920825">
9For our experiments, we set t = 0.2.
10Because the runtime of the Viterbi ITG model grows expo-
nentially with d, it was not feasible to perform comparisons for
higher phrase lengths.
</footnote>
<page confidence="0.996099">
35
</page>
<figure confidence="0.981374714285714">
Recall
80
75
70
65
60
60 65 70 75 80 85
</figure>
<table confidence="0.914793285714286">
Precision
Viterbi ITG BP ITG BP Relaxed
Model Best Scores Sentences
F1 F2 F5 per Second
Viterbi ITG 71.6 73.1 74.0 0.21
BP ITG 71.8 74.8 83.5 0.11
BP Relaxed 72.6 75.2 84.5 1.15
</table>
<figureCaption confidence="0.994872">
Figure 4: Phrase alignment results. A portion of the Pre-
cision/Recall curve is plotted for the BP models, with the
result from the Viterbi ITG model provided for reference.
</figureCaption>
<bodyText confidence="0.998273666666667">
Klein, 2010). The BP Relaxed model performs the
best of all, consistently achieving higher recall for
fixed precision than either of the other models. Be-
cause of its lower asymptotic runtime, it is also much
faster: over 5 times as fast as the Viterbi ITG model
and over 10 times as fast as the BP ITG model.11
</bodyText>
<subsectionHeader confidence="0.992487">
7.2 Timing
</subsectionHeader>
<bodyText confidence="0.99997375">
BP approximates marginal posteriors by iteratively
updating beliefs for each variable based on cur-
rent beliefs about other variables. The iterative na-
ture of the algorithm permits us to make an explicit
speed/accuracy tradeoff by limiting the number of
iterations. We tested this tradeoff by limiting both
of the BP models to run for 2, 3, 5, 10, and 20 iter-
ations. The results are shown in Figure 5. Neither
model benefits from running more iterations than
used to obtain the results in Figure 4, but each can
be sped up by a factor of almost 1.5x in exchange
for a modest (&lt; 1 F1) drop in accuracy.
</bodyText>
<footnote confidence="0.8193655">
11The speed advantage of Viterbi ITG over BP ITG comes
from Viterbi ITG’s aggressive beaming.
</footnote>
<page confidence="0.462627">
73
</page>
<figure confidence="0.990699111111111">
72
71
70
69
68
67
0.0625 0.125 0.25 0.5 1 2
Speed (sentences per second)
Viterbi ITG BP ITG BP Relaxed
</figure>
<figureCaption confidence="0.999114833333333">
Figure 5: Speed/accuracy tradeoff. The speed axis is on
a logarithmic scale. From fastest to slowest, data points
correspond to maximums of 2, 5, 10, and 20 BP itera-
tions. Fl for the BP Relaxed model was very low when
limited to 2 iterations, so that data point is outside the
visible area of the graph.
</figureCaption>
<table confidence="0.9992264">
Model BLEU Relative Hours to
Improve. Train/Align
Baseline 32.8 +0.0 5
Viterbi ITG 33.5 +0.7 831
BP Relaxed 33.6 +0.8 39
</table>
<tableCaption confidence="0.991213">
Table 2: Machine translation results.
</tableCaption>
<subsectionHeader confidence="0.957688">
7.3 Translation
</subsectionHeader>
<bodyText confidence="0.999970285714286">
We ran translation experiments using Moses (Koehn
et al., 2007), which we trained on a 22.1 mil-
lion word parallel corpus from the GALE program.
We compared alignments generated by the baseline
HMM model, the Viterbi ITG model and the Re-
laxed BP model.12 The systems were tuned and
evaluated on sentences up to length 40 from the
NIST MT04 and MT05 test sets. The results, shown
in Table 2, show that the BP Relaxed model achives
a 0.8 BLEU improvement over the HMM baseline,
comparable to that of the Viterbi ITG model, but tak-
ing a fraction of the time,13 making the BP Relaxed
model a practical alternative for real translation ap-
plications.
</bodyText>
<footnote confidence="0.991776166666667">
12Following a simplified version of the procedure described
by DeNero and Klein (2010), we added rule counts from the
HMM alignments to the extraction set aligners’ counts.
13Some of the speed difference between the BP Relaxed and
Viterbi ITG models comes from better parallelizability due to
drastically reduced memory overhead of the BP Relaxed model.
</footnote>
<note confidence="0.374203">
Best F1
</note>
<page confidence="0.993157">
36
</page>
<sectionHeader confidence="0.996392" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999539846153846">
For performing inference in a state-of-the-art, but in-
efficient, alignment model, belief propagation is a
viable alternative to greedy search methods, such as
beaming. BP also results in models that are much
more scalable, by reducing the asymptotic complex-
ity of inference. Perhaps most importantly, BP per-
mits the relaxation of artificial constraints that are
generally taken for granted as being necessary for
efficient inference. In particular, a relatively mod-
est relaxation of the ITG constraint can directly be
applied to any model that uses ITG-based inference
(e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007;
Haghighi et al., 2009).
</bodyText>
<sectionHeader confidence="0.99797" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999098333333333">
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.998646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999508951219512">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In ACL.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In AMTA.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL-IJCNLP.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Fabien Cromi`eres and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In EMNLP-CoNLL.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
ACL.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In EMNLP.
John DeNero. 2010. Personal Communication.
John Duchi, Danny Tarlow, Gal Elidan, and Daphne
Koller. 2007. Using combinatorial optimization
within max-product belief propagation. In NIPS 2006.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning and
stochastic optimization. In COLT.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL-IJCNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
David J.C. MacKay. 2003. Information theory, infer-
ence, and learning algorithms. Cambridge Univ Press.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
ACL Workshop on Statistical Machine Translation.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL.
</reference>
<page confidence="0.991258">
37
</page>
<reference confidence="0.98935625">
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL:HLT.
</reference>
<page confidence="0.999344">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925747">
<title confidence="0.999938">Fast Inference in Phrase Extraction Models with Belief Propagation</title>
<author confidence="0.989567">Burkett</author>
<affiliation confidence="0.999893">Computer Science University of California,</affiliation>
<abstract confidence="0.99592075">Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for inference but for BP. With this new constraint, we achieve a relative error of 40% in a 5.5x speed-up.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24558" citStr="Ayan and Dorr, 2006" startWordPosition="4292" endWordPosition="4295">. but the structure of the model is such that two conflicting phrase pairs are unlikely to simultaneously have high posterior probability. Most publicly available translation systems expect word-level alignments as input. These can also be generated by applying posterior thresholding, aligning target word i to source word j whenever baij(sure) &gt; t.9 7 Experiments Our experiments are performed on Chinese-toEnglish alignment. We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments (Ayan and Dorr, 2006; Haghighi et al., 2009; DeNero and Klein, 2010). The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data. We test three models: the Viterbi ITG model of DeNero and Klein (2010), our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the ONESPAN factors. In all of our experiments, the phrase length d was set to 3.10 7.1 Phrase Alignment We tested the models by computing precision and recall on extracted phrase pairs, relative to the gold phrase pairs of up to length 3 induce</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constraining the phrase-based, joint probability statistical translation model.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="1321" citStr="Birch et al., 2006" startWordPosition="200" endWordPosition="203"> efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work o</context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, 2006</marker>
<rawString>Alexandra Birch, Chris Callison-Burch, and Miles Osborne. 2006. Constraining the phrase-based, joint probability statistical translation model. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1365" citStr="Blunsom et al., 2009" startWordPosition="208" endWordPosition="211">fficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction </context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In NAACL Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In NAACL Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6966" citStr="Chiang, 2007" startWordPosition="1154" endWordPosition="1155">re shown as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48, σA = [4,4] does not include the possible link at a38. However, f7 only has possible links, so σ�7 = [5,6] is the span containing those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ez onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σz = [k, `] means that the target word ez p</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromi`eres</author>
<author>Sadao Kurohashi</author>
</authors>
<title>An alignment algorithm using belief propagation and a structure-based distortion model.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<marker>Cromi`eres, Kurohashi, 2009</marker>
<rawString>Fabien Cromi`eres and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1721" citStr="DeNeefe et al., 2007" startWordPosition="264" endWordPosition="267">c to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative modeling of extraction sets for machine translation.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1946" citStr="DeNero and Klein (2010)" startWordPosition="300" endWordPosition="303">DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments. This permits a polynomial-time algorithm, but it is still O(n6), with a large constant factor once the state space is appropriately enriched to capture overlap. Therefore, they u</context>
<context position="3674" citStr="DeNero and Klein (2010)" startWordPosition="580" endWordPosition="583"> marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors. BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromi`eres and Kurohashi, 2009) word alignment models. By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics achieve a comparable phrase pair F1. Furthermore, because we have posterior marginals rather than a single Viterbi derivation, we can explicitly force the aligner to choose denser extraction sets simply by lowering the marginal threshold. Therefore, we also show substantial improvements over DeNero and Klein (2010) in recall-heavy objectives, such as F5. More importantly</context>
<context position="7030" citStr="DeNero and Klein, 2010" startWordPosition="1163" endWordPosition="1166">d horizontal lines. Because there is a sure link at a48, σA = [4,4] does not include the possible link at a38. However, f7 only has possible links, so σ�7 = [5,6] is the span containing those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ez onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σz = [k, `] means that the target word ez projects to the source span [k, `]. The set of legal values for σ</context>
<context position="9360" citStr="DeNero and Klein (2010)" startWordPosition="1637" endWordPosition="1640">based on a, 7r(a) sets 7rghk` = true iff every word in each phrasal span projects within the other: Qei ⊆ [k, E] ∀i ∈ [g, h] (5) Qj ⊆ [g, h] f∀j ∈ [k, E] 2.2 Formulation as a Graphical Model We score triples (a, 7r, Q) as the dot product of a weight vector w that parameterizes our model and a feature vector O(a, 7r, Q). The feature vector decomposes into word alignment features Oa, phrase pair features Oπ and target and source null word features Oe∅ and Of∅:1 1: 1: O(a, 7r, Q) = Oa(aij) + i,j g,h,k,` 1: Oe∅(Qei ) + 1: Of∅(Qfj ) (6) i j This feature function is exactly the same as that used by DeNero and Klein (2010).2 However, while 1In addition to the arguments we write out explicitly, all feature functions have access to the observed sentence pair (e, f). 2Although the null word features are not described in DeNero and Klein (2010), all of their reported results include these features (DeNero, 2010). they formulated their inference problem as a search for the highest scoring triple (a, 7r, Q) for an observed sentence pair (e, f), we wish to derive a conditional probability distribution p(a, 7r, Q|e, f). We do this with the standard transformation for linear models: p(a, 7r, Q|e, f) ∝ exp(w·O(a, 7r, Q))</context>
<context position="12854" citStr="DeNero and Klein (2010)" startWordPosition="2236" endWordPosition="2239"> clarity, we draw the graph separated into two components: one containing the factors that only neighbor word link variables, and one containing the remaining factors. and identical word features, a position distortion feature, and features for numbers and punctuation. PHRASEPAIR. For each phrase pair variable 7rghk`, scores from 0π(7rghk`) come from the factor Rghk`, which is active if 7rghk` = true. Most of the model’s features are on these factors, and include relative frequency statistics, lexical template indicator features, and indicators for numbers of words and Chinese characters. See DeNero and Klein (2010) for a more comprehensive list. NULLWORD. We can determine if a word is null-aligned by looking at its corresponding span variable. Thus, we include features from 0e�(Qei ) in a factor Nei that is active if Qe i= [−1, oc]. The features are mostly indicators for common words. There are also factors Nfj for source words, which are defined analogously. 2.2.2 Hard Constraint Factors We encode the hard constraints on relationships between variables in our model using three families of factors, shown graphically in Figure 2. The SPAN and EXTRACT factors together ensure that 7r = 7r(a). The ITG facto</context>
<context position="24606" citStr="DeNero and Klein, 2010" startWordPosition="4300" endWordPosition="4303">t two conflicting phrase pairs are unlikely to simultaneously have high posterior probability. Most publicly available translation systems expect word-level alignments as input. These can also be generated by applying posterior thresholding, aligning target word i to source word j whenever baij(sure) &gt; t.9 7 Experiments Our experiments are performed on Chinese-toEnglish alignment. We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments (Ayan and Dorr, 2006; Haghighi et al., 2009; DeNero and Klein, 2010). The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data. We test three models: the Viterbi ITG model of DeNero and Klein (2010), our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the ONESPAN factors. In all of our experiments, the phrase length d was set to 3.10 7.1 Phrase Alignment We tested the models by computing precision and recall on extracted phrase pairs, relative to the gold phrase pairs of up to length 3 induced by the gold word alignments. For the BP models</context>
<context position="28790" citStr="DeNero and Klein (2010)" startWordPosition="5050" endWordPosition="5053">rallel corpus from the GALE program. We compared alignments generated by the baseline HMM model, the Viterbi ITG model and the Relaxed BP model.12 The systems were tuned and evaluated on sentences up to length 40 from the NIST MT04 and MT05 test sets. The results, shown in Table 2, show that the BP Relaxed model achives a 0.8 BLEU improvement over the HMM baseline, comparable to that of the Viterbi ITG model, but taking a fraction of the time,13 making the BP Relaxed model a practical alternative for real translation applications. 12Following a simplified version of the procedure described by DeNero and Klein (2010), we added rule counts from the HMM alignments to the extraction set aligners’ counts. 13Some of the speed difference between the BP Relaxed and Viterbi ITG models comes from better parallelizability due to drastically reduced memory overhead of the BP Relaxed model. Best F1 36 8 Conclusion For performing inference in a state-of-the-art, but inefficient, alignment model, belief propagation is a viable alternative to greedy search methods, such as beaming. BP also results in models that are much more scalable, by reducing the asymptotic complexity of inference. Perhaps most importantly, BP perm</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In NAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1634" citStr="DeNero et al., 2006" startWordPosition="251" endWordPosition="254"> Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational co</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In NAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
</authors>
<date>2010</date>
<tech>Personal Communication.</tech>
<contexts>
<context position="9651" citStr="DeNero, 2010" startWordPosition="1687" endWordPosition="1688"> vector O(a, 7r, Q). The feature vector decomposes into word alignment features Oa, phrase pair features Oπ and target and source null word features Oe∅ and Of∅:1 1: 1: O(a, 7r, Q) = Oa(aij) + i,j g,h,k,` 1: Oe∅(Qei ) + 1: Of∅(Qfj ) (6) i j This feature function is exactly the same as that used by DeNero and Klein (2010).2 However, while 1In addition to the arguments we write out explicitly, all feature functions have access to the observed sentence pair (e, f). 2Although the null word features are not described in DeNero and Klein (2010), all of their reported results include these features (DeNero, 2010). they formulated their inference problem as a search for the highest scoring triple (a, 7r, Q) for an observed sentence pair (e, f), we wish to derive a conditional probability distribution p(a, 7r, Q|e, f). We do this with the standard transformation for linear models: p(a, 7r, Q|e, f) ∝ exp(w·O(a, 7r, Q)). Due to the factorization in Eq. (6), this exponentiated form becomes a product of local multiplicative factors, and hence our model forms an undirected graphical model, or Markov random field. In addition to the scoring function, our model also includes constraints on which triples (a, 7r</context>
</contexts>
<marker>DeNero, 2010</marker>
<rawString>John DeNero. 2010. Personal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Danny Tarlow</author>
<author>Gal Elidan</author>
<author>Daphne Koller</author>
</authors>
<title>Using combinatorial optimization within max-product belief propagation.</title>
<date>2007</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="19494" citStr="Duchi et al., 2007" startWordPosition="3427" endWordPosition="3430">Performed naively, the sum in Eq. (8) will take time that grows exponentially with the size of N(F). For the softscoring factors, which each depend only on a single variable, this isn’t a problem. However, our model also includes factors whose arity grows with the input size: for example, explicitly enumerating all assignments to the word link variables that the ITG factor depends on would take O(3n2) time.5 To run BP in a reasonable time frame, we need efficient factor-specific propagators that can exploit the structure of the factor functions to compute outgoing messages in polynomial time (Duchi et al., 2007; Smith and Eisner, 2008). Fortunately, all of our hard constraints permit dynamic programs that accomplish this propagation. Space does not permit a full description of these dynamic programs, but we will briefly sketch the intuitions behind them. SPAN and ONESPAN. Marginal beliefs for Sei or Uei can be computed in O(nd2) time. The key observation is that for any legal value Qei = [k, E], Sei and Uei require that aij = off for all j V [k, E].6 Thus, we start by computing the product of all the off beliefs: 4We set S = 0.001. 5For all asymptotic analysis, we define n = max(Jel, Ifl). 6For ease</context>
</contexts>
<marker>Duchi, Tarlow, Elidan, Koller, 2007</marker>
<rawString>John Duchi, Danny Tarlow, Gal Elidan, and Daphne Koller. 2007. Using combinatorial optimization within max-product belief propagation. In NIPS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="23168" citStr="Duchi et al., 2010" startWordPosition="4066" endWordPosition="4069">pairs (with L2 regularization). Because 7r and Q are determined when a is observed, the model has no latent variables. Therefore, the gradient takes the standard form for loglinear models: VLL = 0(a,7r,Q) − (10) p(a0, 7r0, Q0|e, f)0(a0, 7r0, Q0) − Aw a/�-/�v/ The feature vector 0 contains features on sure word links, extracted phrase pairs, and null-aligned words. Approximate expectations of these features can be efficiently computed using the marginal beliefs baij(sure), b-ghk`(true), and b,e i([−1, oc]) and baf ([−1, oc] ), respectively. We learned our final j weight vector w using AdaGrad (Duchi et al., 2010), an adaptive subgradient version of standard stochastic gradient ascent. 6.2 Testing We evaluate our model by measuring precision and recall on extracted phrase pairs. Thus, the decoding problem takes a sentence pair (e, f) as input, and must produce an extraction set 7r as output. Our approach, posterior thresholding, is extremely simple: we set 7rghkt = true iff b-ghk`(true) &gt; T for some fixed threshold T. Note that this decoding method does not require that there be any underlying word alignment a licensing the resulting extraction set 7r,8 7See Section 7.2 for an empirical investigation o</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1059" citStr="Galley et al., 2004" startWordPosition="159" endWordPosition="162"> but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, w</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="6951" citStr="Galley et al., 2006" startWordPosition="1150" endWordPosition="1153">true). Target spans are shown as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48, σA = [4,4] does not include the possible link at a38. However, f7 only has possible links, so σ�7 = [5,6] is the span containing those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ez onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σz = [k, `] means that the t</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="15703" citStr="Haghighi et al., 2009" startWordPosition="2746" endWordPosition="2749">lignments a. First, it requires that each word is aligned to at most one relatively short subspan of the other sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.3 The other constraint imposed by the ITG factor is the ITG reordering constraint. This constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997). While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments. We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromi`eres and Kurohashi, 2009). Our new model has almost exactly the same form as the previous one. The only difference is that A is replaced with a new family of simpler factors: ONESPAN. For each target word ei (and ea</context>
<context position="21471" citStr="Haghighi et al., 2009" startWordPosition="3773" endWordPosition="3776"> E] by summing over the O(d2) values [k&apos;, E&apos;] where [k&apos;, E&apos;] C_ [k, E]. Likewise for source words. Multiplying together these per-word beliefs and the belief that 7rghk` = true yields the joint belief of a consistent assignment with 7rghk` = true, which can be used to efficiently compute outgoing messages. ITG. To build outgoing messages, the ITG factor A needs to compute marginal beliefs for all of the word link variables aij. These can all be computed in O(n6) time by using a standard bitext parser to run the inside-outside algorithm. By using a normal form grammar for block ITG with nulls (Haghighi et al., 2009), we ensure that there is a 1-1 correspondence between the ITG derivations the parser sums over and word alignments a that satisfy A. The asymptotic complexity for all the factors is shown in Table 1. The total complexity for inference in each model is simply the sum of the complexities of its factors, so the complexity of the ITG model is O(n2d5 + n6), while the complexity of the relaxed model is just O(n2d5). The complexity of exact inference, on the other hand, is exponential in d for the ITG model and exponential in both d and n for the relaxed model. 34 6 Training and Decoding We use BP t</context>
<context position="24581" citStr="Haghighi et al., 2009" startWordPosition="4296" endWordPosition="4299">f the model is such that two conflicting phrase pairs are unlikely to simultaneously have high posterior probability. Most publicly available translation systems expect word-level alignments as input. These can also be generated by applying posterior thresholding, aligning target word i to source word j whenever baij(sure) &gt; t.9 7 Experiments Our experiments are performed on Chinese-toEnglish alignment. We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments (Ayan and Dorr, 2006; Haghighi et al., 2009; DeNero and Klein, 2010). The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data. We test three models: the Viterbi ITG model of DeNero and Klein (2010), our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the ONESPAN factors. In all of our experiments, the phrase length d was set to 3.10 7.1 Phrase Alignment We tested the models by computing precision and recall on extracted phrase pairs, relative to the gold phrase pairs of up to length 3 induced by the gold word alig</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1161" citStr="Koehn et al., 2003" startWordPosition="174" endWordPosition="177">ion, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other han</context>
<context position="6930" citStr="Koehn et al., 2003" startWordPosition="1146" endWordPosition="1149">pairs (e.g. π5667 = true). Target spans are shown as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48, σA = [4,4] does not include the possible link at a38. However, f7 only has possible links, so σ�7 = [5,6] is the span containing those. f9 is null-aligned, so σ9 = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ez onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σz = [k</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1013" citStr="Koehn et al., 2007" startWordPosition="151" endWordPosition="154">eam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases </context>
<context position="28123" citStr="Koehn et al., 2007" startWordPosition="4933" endWordPosition="4936">.125 0.25 0.5 1 2 Speed (sentences per second) Viterbi ITG BP ITG BP Relaxed Figure 5: Speed/accuracy tradeoff. The speed axis is on a logarithmic scale. From fastest to slowest, data points correspond to maximums of 2, 5, 10, and 20 BP iterations. Fl for the BP Relaxed model was very low when limited to 2 iterations, so that data point is outside the visible area of the graph. Model BLEU Relative Hours to Improve. Train/Align Baseline 32.8 +0.0 5 Viterbi ITG 33.5 +0.7 831 BP Relaxed 33.6 +0.8 39 Table 2: Machine translation results. 7.3 Translation We ran translation experiments using Moses (Koehn et al., 2007), which we trained on a 22.1 million word parallel corpus from the GALE program. We compared alignments generated by the baseline HMM model, the Viterbi ITG model and the Relaxed BP model.12 The systems were tuned and evaluated on sentences up to length 40 from the NIST MT04 and MT05 test sets. The results, shown in Table 2, show that the BP Relaxed model achives a 0.8 BLEU improvement over the HMM baseline, comparable to that of the Viterbi ITG model, but taking a fraction of the time,13 making the BP Relaxed model a practical alternative for real translation applications. 12Following a simpl</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In ACL SSST.</booktitle>
<contexts>
<context position="1037" citStr="Li and Khudanpur, 2008" startWordPosition="155" endWordPosition="158">coding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) an</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In ACL SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="11959" citStr="Liang et al., 2006" startWordPosition="2073" endWordPosition="2076">tors The scoring factors all take the form exp(w · O), and so can be described in terms of their respective local feature vectors, O. Depending on the values of the variables each factor depends on, the factor can be active or inactive. Features are only extracted for active factors; otherwise O is empty and the factor produces a value of 1. SURELINK. Each word alignment variable aij has a corresponding SURELINK factor Lij to incorporate scores from the features Oa(aij). Lij is active whenever aij = sure. Oa(aij) includes posteriors from unsupervised jointly trained HMM word alignment models (Liang et al., 2006), dictionary Oπ(7rghk`)+ 31 A all a i a, al a LI, L1 L1 a L 1 L al L 1 L a Irl a Irl a a a a S σ σ N S N al-1 al-1 P Sf Sf π σf σf R Nf Nf (a) ITG factor (b) SPAN and EXTRACT factors Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph separated into two components: one containing the factors that only neighbor word link variables, and one containing the remaining factors. and identical word features, a position distortion feature, and features for numbers and punctuation. PHRASEPAIR. For each phrase pair variable 7rghk`, scores f</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Information theory, inference, and learning algorithms.</title>
<date>2003</date>
<publisher>Cambridge Univ Press.</publisher>
<contexts>
<context position="17285" citStr="MacKay (2003)" startWordPosition="3031" endWordPosition="3032">no longer a global factor; each Uei depends only on the word link variables from column i. 3Short gaps can be accomodated within block ITG (and in our model are represented as possible links) as long as the total aligned span does not exceed the block size. Figure 3: ONESPAN factors 4 Belief Propagation Belief propagation is a generalization of the well known sum-product algorithm for undirected graphical models. We will provide only a procedural sketch here, but a good introduction to BP for inference in structured NLP models can be found in Smith and Eisner (2008), and Chapters 16 and 23 of MacKay (2003) contain a general introduction to BP in the more general context of message-passing algorithms. At a high level, each variable maintains a local distribution over its possible values. These local distribution are updated via messages passed between variables and factors. For a variable V , N(V ) denotes the set of factors neighboring V in the factor graph. Similarly, N(F) is the set of variables neighboring the factor F. During each round of BP, messages are sent from each variable to each of its neighboring factors: (k+1)TT qV ,—&gt;F (v) a 11 GEN(V ),G�F and from each factor to each of its nei</context>
</contexts>
<marker>MacKay, 2003</marker>
<rawString>David J.C. MacKay. 2003. Information theory, inference, and learning algorithms. Cambridge Univ Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Daniel Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1301" citStr="Marcu and Wong, 2002" startWordPosition="196" endWordPosition="199">nstraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 1 Introduction Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phr</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and Daniel Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative word alignment via alignment matrix modeling.</title>
<date>2008</date>
<booktitle>In ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3458" citStr="Niehues and Vogel, 2008" startWordPosition="545" endWordPosition="548">re accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that admit an efficient BP approximation. BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors. BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromi`eres and Kurohashi, 2009) word alignment models. By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics achieve a comparable phrase pair F1. Furthermore, because we have posterior marginals rather than a single Viterbi derivation, we can explicitly force the al</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative word alignment via alignment matrix modeling. In ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3356" citStr="Smith and Eisner, 2008" startWordPosition="527" endWordPosition="531">del’s ITG-based structural formulation, resulting in a new model that is simultaneously faster and more accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that admit an efficient BP approximation. BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors. BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromi`eres and Kurohashi, 2009) word alignment models. By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics achieve a comparable phrase pair F1. Furthermore, becau</context>
<context position="17244" citStr="Smith and Eisner (2008)" startWordPosition="3021" endWordPosition="3024">lacing the ITG factor. As Figure 3 shows, there is no longer a global factor; each Uei depends only on the word link variables from column i. 3Short gaps can be accomodated within block ITG (and in our model are represented as possible links) as long as the total aligned span does not exceed the block size. Figure 3: ONESPAN factors 4 Belief Propagation Belief propagation is a generalization of the well known sum-product algorithm for undirected graphical models. We will provide only a procedural sketch here, but a good introduction to BP for inference in structured NLP models can be found in Smith and Eisner (2008), and Chapters 16 and 23 of MacKay (2003) contain a general introduction to BP in the more general context of message-passing algorithms. At a high level, each variable maintains a local distribution over its possible values. These local distribution are updated via messages passed between variables and factors. For a variable V , N(V ) denotes the set of factors neighboring V in the factor graph. Similarly, N(F) is the set of variables neighboring the factor F. During each round of BP, messages are sent from each variable to each of its neighboring factors: (k+1)TT qV ,—&gt;F (v) a 11 GEN(V ),G�</context>
<context position="19519" citStr="Smith and Eisner, 2008" startWordPosition="3431" endWordPosition="3434">he sum in Eq. (8) will take time that grows exponentially with the size of N(F). For the softscoring factors, which each depend only on a single variable, this isn’t a problem. However, our model also includes factors whose arity grows with the input size: for example, explicitly enumerating all assignments to the word link variables that the ITG factor depends on would take O(3n2) time.5 To run BP in a reasonable time frame, we need efficient factor-specific propagators that can exploit the structure of the factor functions to compute outgoing messages in polynomial time (Duchi et al., 2007; Smith and Eisner, 2008). Fortunately, all of our hard constraints permit dynamic programs that accomplish this propagation. Space does not permit a full description of these dynamic programs, but we will briefly sketch the intuitions behind them. SPAN and ONESPAN. Marginal beliefs for Sei or Uei can be computed in O(nd2) time. The key observation is that for any legal value Qei = [k, E], Sei and Uei require that aij = off for all j V [k, E].6 Thus, we start by computing the product of all the off beliefs: 4We set S = 0.001. 5For all asymptotic analysis, we define n = max(Jel, Ifl). 6For ease of exposition, we assume</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16080" citStr="Taskar et al., 2005" startWordPosition="2806" endWordPosition="2809">his constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997). While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments. We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromi`eres and Kurohashi, 2009). Our new model has almost exactly the same form as the previous one. The only difference is that A is replaced with a new family of simpler factors: ONESPAN. For each target word ei (and each source word fj) we include a hard constraint factor Uei (respectively Ufj ). Uei is satisfied iff |ue i,p |&lt; d (length limit) and either ue P = [−1, oc] or bj E uei,p, aij � off (no gaps), with ue p as in Eq. (3). Figi, ure 3 shows the portion of the factor graph from Figure 2a redrawn with the ONESPAN factors replacing the ITG factor. As Figure 3 shows, there is no longe</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2332" citStr="Wu, 1997" startWordPosition="362" endWordPosition="363">DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. 29 In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments. This permits a polynomial-time algorithm, but it is still O(n6), with a large constant factor once the state space is appropriately enriched to capture overlap. Therefore, they use a heavily beamed Viterbi search procedure to find a reasonable alignment within an acceptable time frame. In this paper, we show how to use belief propagation (BP) to improve on the model’s ITG-based structural formulation, resulting in a new model that is simultaneously faster and more accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that </context>
<context position="15624" citStr="Wu, 1997" startWordPosition="2736" endWordPosition="2737">as imposing two different types of constraints on allowable word alignments a. First, it requires that each word is aligned to at most one relatively short subspan of the other sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.3 The other constraint imposed by the ITG factor is the ITG reordering constraint. This constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997). While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments. We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromi`eres and Kurohashi, 2009). Our new model has almost exactly the same form as the previous one. The only difference is that A is replaced</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In ACL:HLT.</booktitle>
<contexts>
<context position="14954" citStr="Zhang et al., 2008" startWordPosition="2623" endWordPosition="2626">e and at least one of those relations does not hold. ITG. Finally, to enforce the structural constraint on a, we include a single global factor A that depends on all the word link variables in a (see Figure 2a). A is satisfied iff a is in the family of block inverse transduction grammar (ITG) alignments. The block ITG family permits multiple links to be on (aij =� off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al., 2008). 3 Relaxing the ITG Constraint The ITG factor can be viewed as imposing two different types of constraints on allowable word alignments a. First, it requires that each word is aligned to at most one relatively short subspan of the other sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.3 The other constraint imposed by the ITG factor is the ITG reordering constraint. This constraint is imposed primarily for reasons of computational tractability: the standard d</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In ACL:HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>