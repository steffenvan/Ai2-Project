<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000252">
<title confidence="0.9526525">
Using Lexical Dependency and Ontological Knowledge to Improve a
Detailed Syntactic and Semantic Tagger of English
</title>
<note confidence="0.8369025">
Andrew Finch Ezra Black Young-Sook Hwang Eiichiro Sumita
NiCT*ATR† Epimenides Corp. ETRI NiCT-ATR
Kyoto, Japan New York, USA Seoul, Korea Kyoto, Japan
andrew.finch ezra.black yshwang7 eiichiro.sumita
</note>
<email confidence="0.703311">
@atr.jp @epimenides.com @etri.re.kr @atr.jp
</email>
<sectionHeader confidence="0.986743" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972375">
This paper presents a detailed study of
the integration of knowledge from both
dependency parses and hierarchical word
ontologies into a maximum-entropy-based
tagging model that simultaneously labels
words with both syntax and semantics.
Our findings show that information from
both these sources can lead to strong im-
provements in overall system accuracy:
dependency knowledge improved perfor-
mance over all classes of word, and knowl-
edge of the position of a word in an on-
tological hierarchy increased accuracy for
words not seen in the training data. The
resulting tagger offers the highest reported
tagging accuracy on this tagset to date.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947625">
Part-of-speech (POS) tagging has been one of the
fundamental areas of research in natural language
processing for many years. Most of the prior re-
search has focussed on the task of labeling text
with tags that reflect the words’ syntactic role in
the sentence. In parallel to this, the task of word
sense disambiguation (WSD), the process of de-
ciding in which semantic sense the word is being
used, has been actively researched. This paper ad-
dresses a combination of these two fields, that is:
labeling running words with tags that comprise, in
addition to their syntactic function, a broad seman-
tic class that signifies the semantics of the word in
the context of the sentence, but does not neces-
sarily provide information that is sufficiently fine-
grained as to disambiguate its sense. This differs
</bodyText>
<affiliation confidence="0.848754666666667">
*National Institute of Information and Communications
Technology
†ATR Spoken Language Communication Research Labs
</affiliation>
<bodyText confidence="0.999798952380952">
from what is commonly meant by WSD in that al-
though each word may have many “senses” (by
senses here, we mean the set of semantic labels
the word may take), these senses are not specific
to the word itself but are drawn from a vocabulary
applicable to the subset of all types in the corpus
that may have the same semantics.
In order to perform this task, we draw on re-
search from several related fields, and exploit pub-
licly available linguistic resources, namely the
WordNet database (Fellbaum, 1998). Our aim is
to simultaneously disambiguate the semantics of
the words being tagged while tagging their POS
syntax. We treat the task as fundamentally a POS
tagging task, with a larger, more ambiguous tag
set. However, as we will show later, the ‘n-gram’
feature set traditionally employed to perform POS
tagging, while basically competent, is not up to
this challenge, and needs to be augmented by fea-
tures specifically targeted at semantic disambigua-
tion.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998254533333333">
Our work is a synthesis of POS tagging and WSD,
and as such, research from both these fields is di-
rectly relevant here.
The basic engine used to perform the tagging
in these experiments is a direct descendent of the
maximum entropy (ME) tagger of (Ratnaparkhi,
1996) which in turn is related to the taggers of
(Kupiec, 1992) and (Merialdo, 1994). The ME
approach is well-suited to this kind of labeling be-
cause it allows the use of a wide variety of features
without the necessity to explicitly model the inter-
actions between them.
The literature on WSD is extensive. For a good
overview we direct the reader to (Nancy and Jean,
1998). Typically, the local context around the
</bodyText>
<page confidence="0.986431">
215
</page>
<note confidence="0.7722445">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.993401676470588">
word to be sense-tagged is used to disambiguate
the sense (Yarowsky, 1993), and it is common for
linguistic resources such as WordNet (Li et al.,
1995; Mihalcea and Moldovan, 1998; Ramakrish-
nan and Prithviraj, 2004), or bilingual data (Li and
Li, 2002) to be employed as well as more long-
range context. An ME-system for WSD that op-
erates on similar principles to our system (Suarez,
2002) was based on an array of local features that
included the words/POS tags/lemmas occurring in
a window of +/-3 words of the word being dis-
ambiguated. (Lamjiri et al., 2004) also developed
an ME-based system that used a very simple set
of features: the article before; the POS before
and after; the preposition before and after, and the
syntactic category before and after the word be-
ing labeled. The features used in both of these
approaches resemble those present in the feature
set of a standard n-gram tagger, such as the one
used as the baseline for the experiments in this pa-
per. The semantic tags we use can be seen as a
form of semantic categorization acting in a similar
manner to the semantic class of a word in the sys-
tem of (Lamjiri et al., 2004). The major difference
is that with a left-to-right beam-search tagger, la-
beled context to the right of the word being labeled
is not available for use in the feature set.
Although POS tag information has been utilized
in WSD techniques (e.g. (Suarez, 2002)), there
has been relatively little work addressing the prob-
lem of assigning a part-of-speech tag to a word
together with its semantics, despite the fact that
the tasks involve a similar process of label disam-
biguation for a word in running text.
</bodyText>
<sectionHeader confidence="0.993648" genericHeader="method">
3 Experimental Data
</sectionHeader>
<bodyText confidence="0.999956894736842">
The primary corpus used for the experiments pre-
sented in this paper is the ATR General English
Treebank. This consists of 518,080 words (ap-
proximately 20 words per sentence, on average) of
text annotated with a detailed semantic and syntac-
tic tagset.
To understand the nature of the task involved
in the experiments presented in this paper, one
needs some familiarity with the ATR General
English Tagset. For detailed presentations,
see (Black et al., 1996b; Black et al., 1996a;
Black and Finch, 2001). An apercu can be
gained, however, from Figure 1, which shows
two sample sentences from the ATR Treebank
(and originally from a Chinese take–out food
flier), tagged with respect to the ATR General
English Tagset. Each verb, noun, adjective and
adverb in the ATR tagset includes a semantic
label, chosen from 42 noun/adjective/adverb
categories and 29 verb/verbal categories, some
overlap existing between these category sets.
Proper nouns, plus certain adjectives and
certain numerical expressions, are further cat-
egorized via an additional 35 “proper–noun”
categories. These semantic categories are in-
tended for any “Standard–American–English”
text, in any domain. Sample categories include:
“physical.attribute” (nouns/adjectives/adverbs),
“alter” (verbs/verbals), “interpersonal.act”
(nouns/adjectives/adverbs/verbs/verbals),
“orgname” (proper nouns), and “zipcode”
(numericals). They were developed by the ATR
grammarian and then proven and refined via
day–in–day–out tagging for six months at ATR by
two human “treebankers”, then via four months of
tagset–testing–only work at Lancaster University
(UK) by five treebankers, with daily interactions
among treebankers, and between the treebankers
and the ATR grammarian. The semantic catego-
rization is, of course, in addition to an extensive
syntactic classification, involving some 165 basic
syntactic tags.
The test corpus has been designed specifically
to cope with the ambiguity of the tagset. It is pos-
sible to correctly assign any one of a number of
‘allowable’ tags to a word in context. For exam-
ple, the tag of the word battle in the phrase “a
legal battle” could be either NN1PROBLEM or
NN1INTER-ACT, indicating that the semantics is
either a problem, or an inter-personal action. The
test corpus consists of 53,367 words sampled from
the same domains as, and in approximately the
same proportions as the training data, and labeled
with a set of up to 6 allowable tags for each word.
During testing, only if the predicted tag fails to
match any of the allowed tags is it considered an
error.
</bodyText>
<sectionHeader confidence="0.997795" genericHeader="method">
4 Tagging Model
</sectionHeader>
<subsectionHeader confidence="0.802085">
4.1 ME Model
</subsectionHeader>
<bodyText confidence="0.999863">
Our tagging framework is based on a maximum
entropy model of the following form:
</bodyText>
<equation confidence="0.8619049">
α��(c,t)
k p0 (1)
where:
K
p(t, c) = ry 11
k=0
216
(_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT
when_CSWHEN ordering_VVGINTER-ACT
OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD
</equation>
<figureCaption confidence="0.78186">
Figure 1: Two ATR Treebank Sentences from a Take–Out Food Flier
</figureCaption>
<bodyText confidence="0.551278">
- t is tag being predicted;
- c is the context of t;
- γ is a normalization coefficient that ensures:
</bodyText>
<equation confidence="0.8171864">
ELt=0γ 11Kk=0 αf�(c,t)
k p0 = 1;
- K is the number of features in the model;
- L is the number of tags in our tag set;
- αk is the weight of feature fk;
</equation>
<bodyText confidence="0.980142285714286">
- fk are feature functions and fkE{0,1};
- p0 is the default tagging model (in our case,
the uniform distribution, since all of the in-
formation in the model is specified using ME
constraints).
Our baseline model contains the following fea-
ture predecate set:
</bodyText>
<equation confidence="0.998191666666667">
w0 t−1 pos0 pref1(w0)
w−1 t−2 pos−1 pref2(w0)
w−2 pos−2 pref3(w0)
w+1 pos+1 suff1(w0)
w+2 pos+2 suff2(w0)
suff3(w0)
</equation>
<bodyText confidence="0.969385851851852">
where:
- wn is the word at offset n relative to the word
whose tag is being predicted;
- tn is the tag at offset n;
- posn is the syntax-only tag at offset n as-
signed by a syntax-only tagger;
- prefn(w0) is the first n characters of w0;
- suffn(w0) is the last n characters of w0;
This feature set contains a typical selection of
n-gram and basic morphological features. When
the tagger is trained in tested on the UPENN tree-
bank (Marcus et al., 1994), its accuracy (excluding
the posn features) is over 96%, close to the state of
the art on this task. (Black et al., 1996b) adopted
a two-stage approach to prediction, first predicting
syntax, then semantics given the syntax, whereas
in (Black et al., 1998) both syntax and semantics
were predicted together in one step. In using syn-
tactic tags as features, we take a softer approach
to the two-stage process. The tagger has access
to accurate syntactic information; however, it is
not necessarily constrained to accept this choice
of syntax. Rather, it is able to decide both syn-
tax and semantics while taking semantic context
into account. In order to find the most probable
sequence of tags, we tag in a left-to-right manner
using a beam-search algorithm.
</bodyText>
<subsectionHeader confidence="0.992796">
4.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.9772776">
For reasons of practicability, it is not always pos-
sible to use the full set of features in a model: of-
ten it is necessary to control the number of fea-
tures to reduce resource requirements during train-
ing. We use mutual information (MI) to select
the most useful feature predicates (for more de-
tails, see (Rosenfeld, 1996)). It can be viewed as
a means of determining how much information a
given predicate provides when used to predict an
outcome.
That is, we use the following formula to gauge
a feature’s usefulness to the model:
p(f, t)log p(f, t) p(f)p(t) (2)
where:
- t ∈ T is a tag in the tagset;
- f ∈ {0, 1} is the value of any kind of predi-
cate feature.
Using mutual information is not without its
shortcomings. It does not take into account any
of the interactions between features. It is possi-
ble for a feature to be pronounced useful by this
procedure, whereas in fact it is merely giving the
same information as another feature but in differ-
ent form. Nonetheless this technique is invaluable
in practice. It is possible to eliminate features
</bodyText>
<equation confidence="0.993017">
I(f; T) = � 11
fE{0,11 tET
</equation>
<page confidence="0.979145">
217
</page>
<bodyText confidence="0.9999465">
which provide little or no benefit to the model,
thus speeding up the training. In some cases it
even allows a model to be trained where it would
not otherwise be possible to train one. For the pur-
poses of our experiments, we use the top 50,000
predicates for each model to form the feature set.
</bodyText>
<sectionHeader confidence="0.991125" genericHeader="method">
5 External Knowledge Sources
</sectionHeader>
<subsectionHeader confidence="0.976578">
5.1 Lexical Dependencies
</subsectionHeader>
<bodyText confidence="0.99998624">
Features derived from n-grams of words and tags
in the immediate vicinity of the word being tagged
have underpinned the world of POS tagging for
many years (Kupiec, 1992; Merialdo, 1994; Rat-
naparkhi, 1996), and have proven to be useful fea-
tures in WSD (Yarowsky, 1993). Lower-order
n-grams which are closer to word being tagged
offer the greatest predictive power (Black et al.,
1998). However, in the field of WSD, relational
information extracted from grammatical analysis
of the sentence has been employed to good effect,
and in particular, subject-object relationships be-
tween verbs and nouns have been shown be effec-
tive in disambiguating semantics (Nancy and Jean,
1998). We take the broader view that dependency
relationships in general between any classes of
words may help, and use the ME training process
to weed out the irrelevant relationships. The prin-
ciple is exactly the same as when using a word in
the local context as a feature, except that the word
in this case has a grammatical relationship with the
word being tagged, and can be outside the local
neighborhood of the word being tagged. For both
types of dependency, we encoded the model con-
straints fstl(d) as boolean functions of the form:
</bodyText>
<equation confidence="0.976629">
( 1 if d.s = s ∧ d.t = t ∧ d.l = l
fstl (d) = Sl 0 otherwise
(3)
</equation>
<bodyText confidence="0.973517846153846">
where:
- d is a lexical dependency, consisting of a
source word (the word being tagged) d.s, a
target word d.t and a label d.l
- s and t (words), and l (link label) are specific
to the feature
We generated two distinct features for each de-
pendency. The source and target were exchanged
to create these features. This was to allow the
models to capture the bidirectional nature of the
dependencies. For example, when tagging a verb,
the model should be aware of the dependent ob-
ject, and conversely when tagging that object, the
model should have a feature imposing a constraint
arising from the identity of the dependent verb.
5.1.1 Dependencies from the CMU Link
Grammar
We parsed our corpus using the parser detailed
in (Grinberg et al., 1995). The dependencies out-
put by this parser are labeled with the type of de-
pendency (connector) involved. For example, sub-
jects (connector type S) and direct objects of verbs
(O) are explicitly marked by the process (a full list
of connectors is provided in the paper). We used
all of the dependencies output by the parser as fea-
tures in the models.
</bodyText>
<subsubsectionHeader confidence="0.923451">
5.1.2 Dependencies from Phrasal Structure
</subsubsectionHeader>
<bodyText confidence="0.9999942">
It is possible to extract lexical dependencies
from a phrase-structure parse. The procedure is
explained in detail in (Collins, 1996). In essence,
each non-terminal node in the parse tree is as-
signed a head word, which is the head of one of
its children denoted the ‘head child’. Dependen-
cies are established between this headword and
the heads of each of the children (except for the
head child). In these experiments we used the
MXPOST tagger (Ratnaparkhi, 1996) combined
with Collins’ parser (Collins, 1996) to assign parse
trees to the corpus. The parser had a 98.9% cover-
age of the sentences in our corpora. Again, all of
the dependencies output by the parser were used
as features in the models.
</bodyText>
<subsectionHeader confidence="0.997647">
5.2 Hierarchical Word Ontologies
</subsectionHeader>
<bodyText confidence="0.999984">
In this section we consider the effect of features
derived from hierarchical sets of words. The pri-
mary advantage is that we are able to construct
these hierarchies using knowledge from outside
the training corpus of the tagger itself, and thereby
glean knowledge about rare words. In these exper-
iments we use the human annotated word taxon-
omy of hypernyms (IS-A relations) in the Word-
Net database, and an automatically acquired on-
tology made by clustering words in a large corpus
of unannotated text.
We have chosen to use hierarchical schemes for
both the automatic and manually acquired ontolo-
gies because this offers the opportunity to com-
bat data-sparseness issues by allowing features de-
rived from all levels of the hierarchy to be used.
The process of training the model is able to de-
</bodyText>
<page confidence="0.993066">
218
</page>
<figure confidence="0.999444833333333">
Hierarchy for sense 1
pear
grape
natural object
reproductive
structure
plant organ
edible fruit
plant part
Top-level category
object
fruit
apple
angiospermous
tree
vascular plant
woody plant
apple tree
fruit tree
plant
tree
crab apple
wild apple
Hierarchy for sense 2
</figure>
<figureCaption confidence="0.99998">
Figure 2: The WordNet taxonomy for both (WordNet) senses of the word apple
</figureCaption>
<bodyText confidence="0.9999825">
cide the levels of granularity that are most useful
for disambiguation. For the purposes of generat-
ing features for the ME tagger we treat both types
of hierarchy in the same fashion. One of these fea-
tures is illustrated in Figure 5.3. Each predicate
is effectively a question which asks whether the
word (or word being used in a particular sense in
the case of the WordNet hierarchy) is a descendent
of the node to which the predicate applies. These
predicates become more and more general as one
moves up the hierarchy. For example in the hierar-
chy shown in Figure 5.2, looking at the nodes on
the right hand branch, the lowest node represents
the class of apple trees whereas the top node rep-
resents the class of all plants.
We expect these hierarchies to be particularly
useful when tagging out of vocabulary words
(OOV’s). The identity of the word being tagged
is by far the most important feature in our baseline
model. When tagging an OOV this information is
not available to the tagger. The automatic cluster-
ing has been trained on 100 times as much data
as our tagger, and therefore will have information
about words that tagger has not seen during train-
ing. To illustrate this point, suppose that we are
tagging the OOV pomegranate. This word is in the
WordNet database, and is in the same synset as the
‘fruit’ sense of the word apple. It is reasonable to
assume that the model will have learned (from the
many examples of all fruit words) that the predi-
cate representing membership of this fruit synset
should, if true, favor the selection of the correct tag
for fruit words: NN1FOOD. The predicate will be
true for the word pomegranate which will thereby
benefit from the model’s knowledge of how to tag
the other words in its class. Even if this is not so
at this level in the hierarchy, it is likely to be so at
some level of granularity. Precisely which levels
of detail are useful will be learned by the model
during training.
</bodyText>
<subsectionHeader confidence="0.751238">
5.2.1 Automatic Clustering of Text
</subsectionHeader>
<bodyText confidence="0.99998205882353">
We used the automatic agglomerative mutual-
information-based clustering method of (Ushioda,
1996) to form hierarchical clusters from approx-
imately 50 million words of tokenized, unanno-
tated text drawn from similar domains as the tree-
bank used to train the tagger. Figure 5.2 shows
the position of the word apple within the hierar-
chy of clusters. This example highlights both the
strengths and weaknesses of this approach. One
strength is that the process of clustering proceeds
in a purely objective fashion and associations be-
tween words that may not have been considered
by a human annotator are present. Moreover, the
clustering process considers all types that actually
occur in the corpus, and not just those words that
might appear in a dictionary (we will return to this
later). A major problem with this approach is that
</bodyText>
<page confidence="0.99922">
219
</page>
<figureCaption confidence="0.86374275">
coffee chicken diamond tin newsstand
wellhead calf after-market palm-oil
winter-wheat meat milk timber É
Figure 3: The dendrogram for the automatically acquired ontology, showing the word apple
</figureCaption>
<figure confidence="0.885259142857143">
egg
apple
PREDICATE:
Is the word in the
subtree below this
node?
coca
</figure>
<bodyText confidence="0.994812047619048">
the clusters tend to contain a lot of noise. Rare
words can easily find themselves members of clus-
ters to which they do not seem to belong, by virtue
of the fact that there are too few examples of the
word to allow the clustering to work well for these
words. This problem can be mitigated somewhat
by simply increasing the size of the text that is
clustered. However the clustering process is com-
putationally expensive. Another problem is that a
word may only be a member of a single cluster;
thus typically the cluster set assigned to a word
will only be appropriate for that word when used
in its most common sense.
Approximately 93% of running words in the test
corpus, and 95% in the training corpus were cov-
ered by the words in the clusters (when restricted
to verbs, nouns, adjectives and adverbs, these fig-
ures were 94.5% and 95.2% respectively). Ap-
proximately 81% of the words in the vocabulary
from the test corpus were covered, and 71% of the
training corpus vocabulary was covered.
</bodyText>
<subsectionHeader confidence="0.910568">
5.2.2 WordNet Taxonomy
</subsectionHeader>
<bodyText confidence="0.999976347826087">
For this class of features, we used the hypernym
taxonomy of WordNet (Fellbaum, 1998). Fig-
ure 5.2 shows the WordNet hypernym taxonomy
for the two senses of the word apple that are in
the database. The set of predicates query member-
ship of all levels of the taxonomy for all WordNet
senses of the word being tagged. An example of
one such predicate is shown in the figure.
Only 63% of running words in both the train-
ing and the test corpus were covered by the words
in the clusters. Although this figure appears low,
it can be explained by the fact that WordNet only
contains entries for words that have senses in cer-
tain parts of speech. Some very frequent classes of
words, for example determiners, are not in Word-
Net. The coverage of only nouns, verbs, adjectives
and adverbs in running text is 94.5% for both train-
ing and test sets. Moreover, approximately 84%
of the words in the vocabulary from the test cor-
pus were covered, and 79% on the training cor-
pus. Thus, the effective coverage of WordNet on
the important classes of words is similar to that of
the automatic clustering method.
</bodyText>
<sectionHeader confidence="0.99205" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999974095238095">
The results of our experiments are shown in Ta-
ble 1. The task of assigning semantic and syntac-
tic tags is considerably more difficult than simply
assigning syntactic tags due to the inherent ambi-
guity of the tagset. To gauge the level of human
performance on this task, experiments were con-
ducted to determine inter-annotator consistency;
in addition, annotator accuracy was measured on
5,000 words of data. Both the agreement and ac-
curacy were found to be approximately 97%, with
all of the inconsistencies and tagging errors aris-
ing from the semantic component of the tags. 97%
accuracy is therefore an approximate upper bound
for the performance one would expect from an au-
tomatic tagger. As a point of reference for a lower
bound, the overall accuracy of a tagger which uses
only a single feature representing the identity of
the word being tagged is approximately 73%.
The overall baseline accuracy was 82.58% with
only 30.58% of OOV’s being tagged correctly.
Of the two lexical dependency-based approaches,
</bodyText>
<page confidence="0.987151">
220
</page>
<bodyText confidence="0.999987317460318">
the features derived from Collins’ parser were the
most effective, improving accuracy by 0.8% over-
all. To put the magnitude of this gain into perspec-
tive, dropping the features for the identity of the
previous word from the baseline model, only de-
graded performance by 0.2%. The features from
the link grammar parser were handicapped due to
the fact that only 31% of the sentences were able
to be parsed. When the model (Model 3 in Ta-
ble 1) was evaluated on only the parsable portion
on the test set, the accuracy obtained was roughly
comparable to that using the dependencies from
Collins’ parses. To control for the differences be-
tween these parseable sentences and the full test
set, Model 4 was tested on the same 31% of sen-
tence that parsed. Its accuracy was within 0.2% of
the accuracy on the whole test set in all cases. Nei-
ther of the lexical dependency-based approaches
had a particularly strong effect on the performance
on OOV’s. This is in line with our intuition, since
these features rely on the identity of the word be-
ing tagged, and the performance gain we see is
due to the improvement in labeling accuracy of the
context around the OOV.
In contrast to this, for the word-ontology-based
feature sets, one would hope to see a marked im-
provement on OOV’s, since these features were
designed specifically to address this issue. We do
see a strong response to these features in the ac-
curacy of the models. The overall accuracy when
using the automatically acquired ontology is only
0.1% higher than the accuracy using dependencies
from Collins’ parser. However the accuracy on
OOV’s jumps 3.5% to 35.08% compared to just
0.7% for Model 4. Performance for both cluster-
ing techniques was quite similar, with the Word-
Net taxonomical features being slightly more use-
ful, especially for OOV’s. One possible explana-
tion for this is that overall, the coverage of both
techniques is similar, but for rarer words, the MI
clustering can be inconsistent due to lack of data
(for an example, see Figure 5.2: the word news-
stand is a member of a cluster of words that appear
to be commodities), whereas the WordNet clus-
tering remains consistent even for rare words. It
seems reasonable to expect, however, that the au-
tomatic method would do better if trained on more
data. Furthermore, all uses of words can be cov-
ered by automatic clustering, whereas for exam-
ple, the common use of the word apple as a com-
pany name is beyond the scope of WordNet.
In Model 7 we combined the best lexical depen-
dency feature set (Model 4) with the best cluster-
ing feature set (Model 6) to investigate the amount
of information overlap existing between the fea-
ture sets. Models 4 and 6 improved the base-
line performance by 0.8% and 1.3% respectively.
In combination, accuracy was increased by 2.3%,
0.2% more than the sum of the component mod-
els’ gains. This is very encouraging and indicates
that these models provide independent informa-
tion, with virtually all of the benefit from both
models manifesting itself in the combined model.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999974176470588">
We have described a method for simultaneously
labeling the syntax and semantics of words in run-
ning text. We develop this method starting from
a state-of-the-art maximum entropy POS tagger
which itself outperforms previous attempts to tag
this data (Black et al., 1996b). We augment this
tagging model with two distinct types of knowl-
edge: the identity of dependent words in the sen-
tence, and word class membership information of
the word being tagged. We define the features in
such a manner that the useful lexical dependen-
cies are selected by the model, as is the granu-
larity of the word classes used. Our experimental
results show that large gains in performance are
obtained using each of the techniques. The de-
pendent words boosted overall performance, es-
pecially when tagging verbs. The hierarchical
ontology-based approaches also increased over-
all performance, but with particular emphasis on
OOV’s, the intended target for this feature set.
Moreover, when features from both knowledge
sources were applied in combination, the gains
were cumulative, indicating little overlap.
Visual inspection the output of the tagger on
held-out data suggests there are many remaining
errors arising from special cases that might be bet-
ter handled by models separate from the main tag-
ging model. In particular, numerical expressions
and named entities cause OOV errors that the tech-
niques presented in this paper are unable to handle.
In future work we would like to address these is-
sues, and also evaluate our system when used as a
component of a WSD system, and when integrated
within a machine translation system.
</bodyText>
<page confidence="0.992372">
221
</page>
<table confidence="0.840998">
# Model Accuracy (± c.i.) OOV’s Nouns Verbs Adj/Adv
1 Baseline 82.58± 0.32 30.58 68.47 74.32 70.99
2 + Dependencies (link grammar) 82.74± 0.32 30.92 68.18 74.96 73.02
3 As above (only parsed sentences) 83.59± 0.53 30.92 69.16 77.21 73.52
4 + Dependencies (Collins’ parser) 83.37± 0.31 31.24 69.36 75.78 72.62
5 + Automatically acquired ontology 83.71± 0.31 35.08 71.89 75.83 75.34
6 + WordNet ontology 83.90± 0.31 36.18 72.28 76.29 74.47
7 + Model 4 + Model 6 84.90± 0.31 37.02 72.80 78.36 76.16
</table>
<tableCaption confidence="0.8793515">
Table 1: Tagging accuracy (%), ‘+’ being shorthand for “Baseline +”, ‘c.i.’ denotes the confidence
interval of the mean at a 95% significance level, calculated using bootstrap resampling.
</tableCaption>
<sectionHeader confidence="0.997351" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764765432098">
E. Black and A. Finch. 2001. Developing and prov-
ing effective broad-coverage semantic-and-syntactic
tagsets for natural language: The atr approach. In
Proceedings of ICCPOL-2001.
E. Black, S. Eubank, H. Kashioka, R. Garside,
G. Leech, and D. Magerman. 1996a. Beyond
skeleton parsing: producing a comprehensive large–
scale general–english treebank with full grammati-
cal analysis. In Proceedings of the 16th Annual Con-
ference on Computational Linguistics, pages 107–
112, Copenhagen.
E. Black, S. Eubank, H. Kashioka, and J. Saia. 1996b.
Reinventing part-of-speech tagging. Journal of Nat-
ural Language Processing (Japan), 5:1.
Ezra Black, Andrew Finch, and Hideki Kashioka.
1998. Trigger-pair predictors in parsing and tag-
ging. In Proceedings, 36th Annual Meeting of
the Association for Computational Linguistics, 17th
Annual Conference on Computational Linguistics,
Montreal, Canada.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of
the Thirty-Fourth Annual Meeting of the Association
for Computational Linguistics, pages 184–191, San
Francisco. Morgan Kaufmann Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for LINK
grammars. Technical Report CMU-CS-TR-95-125,
CMU, Pittsburgh, PA.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden markov model. Computer Speech and Lan-
guage, 6:225–242.
A. K. Lamjiri, O. El Demerdash, and L.Kosseim. 2004.
Simple features for statistical word sense disam-
biguation. In Proc. ACL 2004 – Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text (Senseval-3), Barcelona,
Spain, July. ACL-2004.
C. Li and H. Li. 2002. Word translation disambigua-
tion using bilingual bootstrapping.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995.
A wordnet-based algorithm for word sense disam-
biguation. In IJCAI, pages 1368–1374.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguistics,
20(2):155–172.
Rada Mihalcea and Dan I. Moldovan. 1998. Word
sense disambiguation based on semantic density. In
Sanda Harabagiu, editor, Use of WordNet in Natural
Language Processing Systems: Proceedings of the
Conference, pages 16–22. Association for Compu-
tational Linguistics, Somerset, New Jersey.
I. Nancy and V. Jean. 1998. Word sense disambigua-
tion: The state of the art. Computational Linguis-
tics, 24:1:1–40.
G. Ramakrishnan and B. Prithviraj. 2004. Soft word
sense disambiguation. In International Conference
on Global Wordnet (GWC 04), Brno, Czeck Repub-
lic.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Confer-
ence.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modelling. Computer
Speech and Language, 10:187–228.
A. Suarez. 2002. A maximum entropy-based word
sense disambiguation system. In Proc. International
Conference on Computational Linguistics.
A. Ushioda. 1996. Hierarchical clustering of words.
In In Proceedings of COLING 96, pages 1159–1162.
D. Yarowsky. 1993. One sense per collocation. In
In the Proceedings ofARPA Human Language Tech-
nology Workshop.
</reference>
<page confidence="0.99799">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502371">
<title confidence="0.9971865">Using Lexical Dependency and Ontological Knowledge to Improve a Detailed Syntactic and Semantic Tagger of English</title>
<author confidence="0.949889">Andrew Finch Ezra Black Young-Sook Hwang Eiichiro Sumita</author>
<affiliation confidence="0.815862">Epimenides Corp. ETRI NiCT-ATR</affiliation>
<address confidence="0.711509">Kyoto, Japan New York, USA Seoul, Korea Kyoto, Japan</address>
<abstract confidence="0.983553894736842">andrew.finch ezra.black yshwang7 eiichiro.sumita @atr.jp @epimenides.com @etri.re.kr @atr.jp Abstract This paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics. Our findings show that information from both these sources can lead to strong improvements in overall system accuracy: dependency knowledge improved performance over all classes of word, and knowledge of the position of a word in an ontological hierarchy increased accuracy for words not seen in the training data. The resulting tagger offers the highest reported tagging accuracy on this tagset to date.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>A Finch</author>
</authors>
<title>Developing and proving effective broad-coverage semantic-and-syntactic tagsets for natural language: The atr approach.</title>
<date>2001</date>
<booktitle>In Proceedings of ICCPOL-2001.</booktitle>
<contexts>
<context position="5915" citStr="Black and Finch, 2001" startWordPosition="975" endWordPosition="978">e fact that the tasks involve a similar process of label disambiguation for a word in running text. 3 Experimental Data The primary corpus used for the experiments presented in this paper is the ATR General English Treebank. This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset. To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1996b; Black et al., 1996a; Black and Finch, 2001). An apercu can be gained, however, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food flier), tagged with respect to the ATR General English Tagset. Each verb, noun, adjective and adverb in the ATR tagset includes a semantic label, chosen from 42 noun/adjective/adverb categories and 29 verb/verbal categories, some overlap existing between these category sets. Proper nouns, plus certain adjectives and certain numerical expressions, are further categorized via an additional 35 “proper–noun” categories. These semantic categories are</context>
</contexts>
<marker>Black, Finch, 2001</marker>
<rawString>E. Black and A. Finch. 2001. Developing and proving effective broad-coverage semantic-and-syntactic tagsets for natural language: The atr approach. In Proceedings of ICCPOL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Eubank</author>
<author>H Kashioka</author>
<author>R Garside</author>
<author>G Leech</author>
<author>D Magerman</author>
</authors>
<title>Beyond skeleton parsing: producing a comprehensive large– scale general–english treebank with full grammatical analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Annual Conference on Computational Linguistics,</booktitle>
<pages>107--112</pages>
<location>Copenhagen.</location>
<contexts>
<context position="5869" citStr="Black et al., 1996" startWordPosition="967" endWordPosition="970">rd together with its semantics, despite the fact that the tasks involve a similar process of label disambiguation for a word in running text. 3 Experimental Data The primary corpus used for the experiments presented in this paper is the ATR General English Treebank. This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset. To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1996b; Black et al., 1996a; Black and Finch, 2001). An apercu can be gained, however, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food flier), tagged with respect to the ATR General English Tagset. Each verb, noun, adjective and adverb in the ATR tagset includes a semantic label, chosen from 42 noun/adjective/adverb categories and 29 verb/verbal categories, some overlap existing between these category sets. Proper nouns, plus certain adjectives and certain numerical expressions, are further categorized via an additional 35 “proper–n</context>
<context position="9571" citStr="Black et al., 1996" startWordPosition="1569" endWordPosition="1572">pos+1 suff1(w0) w+2 pos+2 suff2(w0) suff3(w0) where: - wn is the word at offset n relative to the word whose tag is being predicted; - tn is the tag at offset n; - posn is the syntax-only tag at offset n assigned by a syntax-only tagger; - prefn(w0) is the first n characters of w0; - suffn(w0) is the last n characters of w0; This feature set contains a typical selection of n-gram and basic morphological features. When the tagger is trained in tested on the UPENN treebank (Marcus et al., 1994), its accuracy (excluding the posn features) is over 96%, close to the state of the art on this task. (Black et al., 1996b) adopted a two-stage approach to prediction, first predicting syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right </context>
<context position="25474" citStr="Black et al., 1996" startWordPosition="4320" endWordPosition="4323">aseline performance by 0.8% and 1.3% respectively. In combination, accuracy was increased by 2.3%, 0.2% more than the sum of the component models’ gains. This is very encouraging and indicates that these models provide independent information, with virtually all of the benefit from both models manifesting itself in the combined model. 7 Conclusion We have described a method for simultaneously labeling the syntax and semantics of words in running text. We develop this method starting from a state-of-the-art maximum entropy POS tagger which itself outperforms previous attempts to tag this data (Black et al., 1996b). We augment this tagging model with two distinct types of knowledge: the identity of dependent words in the sentence, and word class membership information of the word being tagged. We define the features in such a manner that the useful lexical dependencies are selected by the model, as is the granularity of the word classes used. Our experimental results show that large gains in performance are obtained using each of the techniques. The dependent words boosted overall performance, especially when tagging verbs. The hierarchical ontology-based approaches also increased overall performance,</context>
</contexts>
<marker>Black, Eubank, Kashioka, Garside, Leech, Magerman, 1996</marker>
<rawString>E. Black, S. Eubank, H. Kashioka, R. Garside, G. Leech, and D. Magerman. 1996a. Beyond skeleton parsing: producing a comprehensive large– scale general–english treebank with full grammatical analysis. In Proceedings of the 16th Annual Conference on Computational Linguistics, pages 107– 112, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Eubank</author>
<author>H Kashioka</author>
<author>J Saia</author>
</authors>
<title>Reinventing part-of-speech tagging.</title>
<date>1996</date>
<journal>Journal of Natural Language Processing (Japan),</journal>
<volume>5</volume>
<contexts>
<context position="5869" citStr="Black et al., 1996" startWordPosition="967" endWordPosition="970">rd together with its semantics, despite the fact that the tasks involve a similar process of label disambiguation for a word in running text. 3 Experimental Data The primary corpus used for the experiments presented in this paper is the ATR General English Treebank. This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset. To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1996b; Black et al., 1996a; Black and Finch, 2001). An apercu can be gained, however, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food flier), tagged with respect to the ATR General English Tagset. Each verb, noun, adjective and adverb in the ATR tagset includes a semantic label, chosen from 42 noun/adjective/adverb categories and 29 verb/verbal categories, some overlap existing between these category sets. Proper nouns, plus certain adjectives and certain numerical expressions, are further categorized via an additional 35 “proper–n</context>
<context position="9571" citStr="Black et al., 1996" startWordPosition="1569" endWordPosition="1572">pos+1 suff1(w0) w+2 pos+2 suff2(w0) suff3(w0) where: - wn is the word at offset n relative to the word whose tag is being predicted; - tn is the tag at offset n; - posn is the syntax-only tag at offset n assigned by a syntax-only tagger; - prefn(w0) is the first n characters of w0; - suffn(w0) is the last n characters of w0; This feature set contains a typical selection of n-gram and basic morphological features. When the tagger is trained in tested on the UPENN treebank (Marcus et al., 1994), its accuracy (excluding the posn features) is over 96%, close to the state of the art on this task. (Black et al., 1996b) adopted a two-stage approach to prediction, first predicting syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right </context>
<context position="25474" citStr="Black et al., 1996" startWordPosition="4320" endWordPosition="4323">aseline performance by 0.8% and 1.3% respectively. In combination, accuracy was increased by 2.3%, 0.2% more than the sum of the component models’ gains. This is very encouraging and indicates that these models provide independent information, with virtually all of the benefit from both models manifesting itself in the combined model. 7 Conclusion We have described a method for simultaneously labeling the syntax and semantics of words in running text. We develop this method starting from a state-of-the-art maximum entropy POS tagger which itself outperforms previous attempts to tag this data (Black et al., 1996b). We augment this tagging model with two distinct types of knowledge: the identity of dependent words in the sentence, and word class membership information of the word being tagged. We define the features in such a manner that the useful lexical dependencies are selected by the model, as is the granularity of the word classes used. Our experimental results show that large gains in performance are obtained using each of the techniques. The dependent words boosted overall performance, especially when tagging verbs. The hierarchical ontology-based approaches also increased overall performance,</context>
</contexts>
<marker>Black, Eubank, Kashioka, Saia, 1996</marker>
<rawString>E. Black, S. Eubank, H. Kashioka, and J. Saia. 1996b. Reinventing part-of-speech tagging. Journal of Natural Language Processing (Japan), 5:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Andrew Finch</author>
<author>Hideki Kashioka</author>
</authors>
<title>Trigger-pair predictors in parsing and tagging.</title>
<date>1998</date>
<booktitle>In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics, 17th Annual Conference on Computational Linguistics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="9707" citStr="Black et al., 1998" startWordPosition="1589" endWordPosition="1592">tn is the tag at offset n; - posn is the syntax-only tag at offset n assigned by a syntax-only tagger; - prefn(w0) is the first n characters of w0; - suffn(w0) is the last n characters of w0; This feature set contains a typical selection of n-gram and basic morphological features. When the tagger is trained in tested on the UPENN treebank (Marcus et al., 1994), its accuracy (excluding the posn features) is over 96%, close to the state of the art on this task. (Black et al., 1996b) adopted a two-stage approach to prediction, first predicting syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right manner using a beam-search algorithm. 4.2 Feature selection For reasons of practicability, it is not always possible to use the full set</context>
<context position="12052" citStr="Black et al., 1998" startWordPosition="2002" endWordPosition="2005">l to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. 5 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of words may help, and use the ME training process to weed out the irrelevant relationships. The principle is exactly the same as when using a word in the local context as a feature, except that the word in this case has</context>
</contexts>
<marker>Black, Finch, Kashioka, 1998</marker>
<rawString>Ezra Black, Andrew Finch, and Hideki Kashioka. 1998. Trigger-pair predictors in parsing and tagging. In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics, 17th Annual Conference on Computational Linguistics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="14213" citStr="Collins, 1996" startWordPosition="2381" endWordPosition="2382">e CMU Link Grammar We parsed our corpus using the parser detailed in (Grinberg et al., 1995). The dependencies output by this parser are labeled with the type of dependency (connector) involved. For example, subjects (connector type S) and direct objects of verbs (O) are explicitly marked by the process (a full list of connectors is provided in the paper). We used all of the dependencies output by the parser as features in the models. 5.1.2 Dependencies from Phrasal Structure It is possible to extract lexical dependencies from a phrase-structure parse. The procedure is explained in detail in (Collins, 1996). In essence, each non-terminal node in the parse tree is assigned a head word, which is the head of one of its children denoted the ‘head child’. Dependencies are established between this headword and the heads of each of the children (except for the head child). In these experiments we used the MXPOST tagger (Ratnaparkhi, 1996) combined with Collins’ parser (Collins, 1996) to assign parse trees to the corpus. The parser had a 98.9% coverage of the sentences in our corpora. Again, all of the dependencies output by the parser were used as features in the models. 5.2 Hierarchical Word Ontologie</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics, pages 184–191, San Francisco. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2440" citStr="Fellbaum, 1998" startWordPosition="385" endWordPosition="386">*National Institute of Information and Communications Technology †ATR Spoken Language Communication Research Labs from what is commonly meant by WSD in that although each word may have many “senses” (by senses here, we mean the set of semantic labels the word may take), these senses are not specific to the word itself but are drawn from a vocabulary applicable to the subset of all types in the corpus that may have the same semantics. In order to perform this task, we draw on research from several related fields, and exploit publicly available linguistic resources, namely the WordNet database (Fellbaum, 1998). Our aim is to simultaneously disambiguate the semantics of the words being tagged while tagging their POS syntax. We treat the task as fundamentally a POS tagging task, with a larger, more ambiguous tag set. However, as we will show later, the ‘n-gram’ feature set traditionally employed to perform POS tagging, while basically competent, is not up to this challenge, and needs to be augmented by features specifically targeted at semantic disambiguation. 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The bas</context>
<context position="20130" citStr="Fellbaum, 1998" startWordPosition="3398" endWordPosition="3399">uster; thus typically the cluster set assigned to a word will only be appropriate for that word when used in its most common sense. Approximately 93% of running words in the test corpus, and 95% in the training corpus were covered by the words in the clusters (when restricted to verbs, nouns, adjectives and adverbs, these figures were 94.5% and 95.2% respectively). Approximately 81% of the words in the vocabulary from the test corpus were covered, and 71% of the training corpus vocabulary was covered. 5.2.2 WordNet Taxonomy For this class of features, we used the hypernym taxonomy of WordNet (Fellbaum, 1998). Figure 5.2 shows the WordNet hypernym taxonomy for the two senses of the word apple that are in the database. The set of predicates query membership of all levels of the taxonomy for all WordNet senses of the word being tagged. An example of one such predicate is shown in the figure. Only 63% of running words in both the training and the test corpus were covered by the words in the clusters. Although this figure appears low, it can be explained by the fact that WordNet only contains entries for words that have senses in certain parts of speech. Some very frequent classes of words, for exampl</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Grinberg</author>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
</authors>
<title>A robust parsing algorithm for LINK grammars.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-TR-95-125,</tech>
<location>CMU, Pittsburgh, PA.</location>
<contexts>
<context position="13691" citStr="Grinberg et al., 1995" startWordPosition="2292" endWordPosition="2295">d.l - s and t (words), and l (link label) are specific to the feature We generated two distinct features for each dependency. The source and target were exchanged to create these features. This was to allow the models to capture the bidirectional nature of the dependencies. For example, when tagging a verb, the model should be aware of the dependent object, and conversely when tagging that object, the model should have a feature imposing a constraint arising from the identity of the dependent verb. 5.1.1 Dependencies from the CMU Link Grammar We parsed our corpus using the parser detailed in (Grinberg et al., 1995). The dependencies output by this parser are labeled with the type of dependency (connector) involved. For example, subjects (connector type S) and direct objects of verbs (O) are explicitly marked by the process (a full list of connectors is provided in the paper). We used all of the dependencies output by the parser as features in the models. 5.1.2 Dependencies from Phrasal Structure It is possible to extract lexical dependencies from a phrase-structure parse. The procedure is explained in detail in (Collins, 1996). In essence, each non-terminal node in the parse tree is assigned a head word</context>
</contexts>
<marker>Grinberg, Lafferty, Sleator, 1995</marker>
<rawString>Dennis Grinberg, John Lafferty, and Daniel Sleator. 1995. A robust parsing algorithm for LINK grammars. Technical Report CMU-CS-TR-95-125, CMU, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="3237" citStr="Kupiec, 1992" startWordPosition="520" endWordPosition="521">, more ambiguous tag set. However, as we will show later, the ‘n-gram’ feature set traditionally employed to perform POS tagging, while basically competent, is not up to this challenge, and needs to be augmented by features specifically targeted at semantic disambiguation. 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common </context>
<context position="11837" citStr="Kupiec, 1992" startWordPosition="1969" endWordPosition="1970"> invaluable in practice. It is possible to eliminate features I(f; T) = � 11 fE{0,11 tET 217 which provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. 5 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of wo</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language, 6:225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Lamjiri</author>
<author>O El Demerdash</author>
<author>L Kosseim</author>
</authors>
<title>Simple features for statistical word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proc. ACL 2004 – Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<location>Barcelona, Spain,</location>
<marker>Lamjiri, El Demerdash, Kosseim, 2004</marker>
<rawString>A. K. Lamjiri, O. El Demerdash, and L.Kosseim. 2004. Simple features for statistical word sense disambiguation. In Proc. ACL 2004 – Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), Barcelona, Spain, July. ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>H Li</author>
</authors>
<title>Word translation disambiguation using bilingual bootstrapping.</title>
<date>2002</date>
<contexts>
<context position="3996" citStr="Li and Li, 2002" startWordPosition="641" endWordPosition="644">the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present</context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>C. Li and H. Li. 2002. Word translation disambiguation using bilingual bootstrapping.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobin Li</author>
<author>Stan Szpakowicz</author>
<author>Stan Matwin</author>
</authors>
<title>A wordnet-based algorithm for word sense disambiguation.</title>
<date>1995</date>
<booktitle>In IJCAI,</booktitle>
<pages>1368--1374</pages>
<contexts>
<context position="3894" citStr="Li et al., 1995" startWordPosition="625" endWordPosition="628">well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before </context>
</contexts>
<marker>Li, Szpakowicz, Matwin, 1995</marker>
<rawString>Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A wordnet-based algorithm for word sense disambiguation. In IJCAI, pages 1368–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9450" citStr="Marcus et al., 1994" startWordPosition="1546" endWordPosition="1549"> model contains the following feature predecate set: w0 t−1 pos0 pref1(w0) w−1 t−2 pos−1 pref2(w0) w−2 pos−2 pref3(w0) w+1 pos+1 suff1(w0) w+2 pos+2 suff2(w0) suff3(w0) where: - wn is the word at offset n relative to the word whose tag is being predicted; - tn is the tag at offset n; - posn is the syntax-only tag at offset n assigned by a syntax-only tagger; - prefn(w0) is the first n characters of w0; - suffn(w0) is the last n characters of w0; This feature set contains a typical selection of n-gram and basic morphological features. When the tagger is trained in tested on the UPENN treebank (Marcus et al., 1994), its accuracy (excluding the posn features) is over 96%, close to the state of the art on this task. (Black et al., 1996b) adopted a two-stage approach to prediction, first predicting syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics wh</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3258" citStr="Merialdo, 1994" startWordPosition="523" endWordPosition="524">g set. However, as we will show later, the ‘n-gram’ feature set traditionally employed to perform POS tagging, while basically competent, is not up to this challenge, and needs to be augmented by features specifically targeted at semantic disambiguation. 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resour</context>
<context position="11853" citStr="Merialdo, 1994" startWordPosition="1971" endWordPosition="1972"> practice. It is possible to eliminate features I(f; T) = � 11 fE{0,11 tET 217 which provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. 5 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of words may help, an</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>Word sense disambiguation based on semantic density.</title>
<date>1998</date>
<booktitle>In Sanda Harabagiu, editor, Use of WordNet in Natural Language Processing Systems: Proceedings of the Conference,</booktitle>
<pages>16--22</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="3923" citStr="Mihalcea and Moldovan, 1998" startWordPosition="629" endWordPosition="632">is kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labe</context>
</contexts>
<marker>Mihalcea, Moldovan, 1998</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 1998. Word sense disambiguation based on semantic density. In Sanda Harabagiu, editor, Use of WordNet in Natural Language Processing Systems: Proceedings of the Conference, pages 16–22. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nancy</author>
<author>V Jean</author>
</authors>
<title>Word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="3547" citStr="Nancy and Jean, 1998" startWordPosition="573" endWordPosition="576"> synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on a</context>
<context position="12344" citStr="Nancy and Jean, 1998" startWordPosition="2046" endWordPosition="2049">immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of words may help, and use the ME training process to weed out the irrelevant relationships. The principle is exactly the same as when using a word in the local context as a feature, except that the word in this case has a grammatical relationship with the word being tagged, and can be outside the local neighborhood of the word being tagged. For both types of dependency, we encoded the model constraints fstl(d) as boolean functions of the form: ( 1 if d.s = s ∧ d.t = t ∧ d.l = l fstl (d) = Sl 0 otherwise (3</context>
</contexts>
<marker>Nancy, Jean, 1998</marker>
<rawString>I. Nancy and V. Jean. 1998. Word sense disambiguation: The state of the art. Computational Linguistics, 24:1:1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ramakrishnan</author>
<author>B Prithviraj</author>
</authors>
<title>Soft word sense disambiguation.</title>
<date>2004</date>
<booktitle>In International Conference on Global Wordnet (GWC 04),</booktitle>
<location>Brno, Czeck Republic.</location>
<contexts>
<context position="3959" citStr="Ramakrishnan and Prithviraj, 2004" startWordPosition="633" endWordPosition="637">t allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of th</context>
</contexts>
<marker>Ramakrishnan, Prithviraj, 2004</marker>
<rawString>G. Ramakrishnan and B. Prithviraj. 2004. Soft word sense disambiguation. In International Conference on Global Wordnet (GWC 04), Brno, Czeck Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="3179" citStr="Ratnaparkhi, 1996" startWordPosition="509" endWordPosition="510">eat the task as fundamentally a POS tagging task, with a larger, more ambiguous tag set. However, as we will show later, the ‘n-gram’ feature set traditionally employed to perform POS tagging, while basically competent, is not up to this challenge, and needs to be augmented by features specifically targeted at semantic disambiguation. 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to </context>
<context position="11873" citStr="Ratnaparkhi, 1996" startWordPosition="1973" endWordPosition="1975"> possible to eliminate features I(f; T) = � 11 fE{0,11 tET 217 which provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. 5 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of words may help, and use the ME trainin</context>
<context position="14544" citStr="Ratnaparkhi, 1996" startWordPosition="2439" endWordPosition="2440">tors is provided in the paper). We used all of the dependencies output by the parser as features in the models. 5.1.2 Dependencies from Phrasal Structure It is possible to extract lexical dependencies from a phrase-structure parse. The procedure is explained in detail in (Collins, 1996). In essence, each non-terminal node in the parse tree is assigned a head word, which is the head of one of its children denoted the ‘head child’. Dependencies are established between this headword and the heads of each of the children (except for the head child). In these experiments we used the MXPOST tagger (Ratnaparkhi, 1996) combined with Collins’ parser (Collins, 1996) to assign parse trees to the corpus. The parser had a 98.9% coverage of the sentences in our corpora. Again, all of the dependencies output by the parser were used as features in the models. 5.2 Hierarchical Word Ontologies In this section we consider the effect of features derived from hierarchical sets of words. The primary advantage is that we are able to construct these hierarchies using knowledge from outside the training corpus of the tagger itself, and thereby glean knowledge about rare words. In these experiments we use the human annotated</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="10553" citStr="Rosenfeld, 1996" startWordPosition="1735" endWordPosition="1736">ecessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right manner using a beam-search algorithm. 4.2 Feature selection For reasons of practicability, it is not always possible to use the full set of features in a model: often it is necessary to control the number of features to reduce resource requirements during training. We use mutual information (MI) to select the most useful feature predicates (for more details, see (Rosenfeld, 1996)). It can be viewed as a means of determining how much information a given predicate provides when used to predict an outcome. That is, we use the following formula to gauge a feature’s usefulness to the model: p(f, t)log p(f, t) p(f)p(t) (2) where: - t ∈ T is a tag in the tagset; - f ∈ {0, 1} is the value of any kind of predicate feature. Using mutual information is not without its shortcomings. It does not take into account any of the interactions between features. It is possible for a feature to be pronounced useful by this procedure, whereas in fact it is merely giving the same information</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Suarez</author>
</authors>
<title>A maximum entropy-based word sense disambiguation system.</title>
<date>2002</date>
<booktitle>In Proc. International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4132" citStr="Suarez, 2002" startWordPosition="668" endWordPosition="669">er to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present in the feature set of a standard n-gram tagger, such as the one used as the baseline for the experiments in this paper. The semantic ta</context>
</contexts>
<marker>Suarez, 2002</marker>
<rawString>A. Suarez. 2002. A maximum entropy-based word sense disambiguation system. In Proc. International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
</authors>
<title>Hierarchical clustering of words. In</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96,</booktitle>
<pages>1159--1162</pages>
<contexts>
<context position="18028" citStr="Ushioda, 1996" startWordPosition="3039" endWordPosition="3040">redicate representing membership of this fruit synset should, if true, favor the selection of the correct tag for fruit words: NN1FOOD. The predicate will be true for the word pomegranate which will thereby benefit from the model’s knowledge of how to tag the other words in its class. Even if this is not so at this level in the hierarchy, it is likely to be so at some level of granularity. Precisely which levels of detail are useful will be learned by the model during training. 5.2.1 Automatic Clustering of Text We used the automatic agglomerative mutualinformation-based clustering method of (Ushioda, 1996) to form hierarchical clusters from approximately 50 million words of tokenized, unannotated text drawn from similar domains as the treebank used to train the tagger. Figure 5.2 shows the position of the word apple within the hierarchy of clusters. This example highlights both the strengths and weaknesses of this approach. One strength is that the process of clustering proceeds in a purely objective fashion and associations between words that may not have been considered by a human annotator are present. Moreover, the clustering process considers all types that actually occur in the corpus, an</context>
</contexts>
<marker>Ushioda, 1996</marker>
<rawString>A. Ushioda. 1996. Hierarchical clustering of words. In In Proceedings of COLING 96, pages 1159–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One sense per collocation. In</title>
<date>1993</date>
<booktitle>In the Proceedings ofARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="3818" citStr="Yarowsky, 1993" startWordPosition="613" endWordPosition="614">d to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, Sydney, July 2006. c�2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and</context>
<context position="11936" citStr="Yarowsky, 1993" startWordPosition="1986" endWordPosition="1987">ch provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. 5 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998). However, in the field of WSD, relational information extracted from grammatical analysis of the sentence has been employed to good effect, and in particular, subject-object relationships between verbs and nouns have been shown be effective in disambiguating semantics (Nancy and Jean, 1998). We take the broader view that dependency relationships in general between any classes of words may help, and use the ME training process to weed out the irrelevant relationships. The princip</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>D. Yarowsky. 1993. One sense per collocation. In In the Proceedings ofARPA Human Language Technology Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>