<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.997628">
Approximating Context-Free Grammars
with a Finite-State Calculus
</title>
<author confidence="0.996212">
Edmund GRIMLEY EVANS
</author>
<affiliation confidence="0.9923745">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.969281">
Cambridge, CB2 3QG, GB
</address>
<email confidence="0.830436">
Edmund. Grimley-Evans@cl. cam. ac .uk
</email>
<sectionHeader confidence="0.960442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911105263158">
Although adequate models of human lan-
guage for syntactic analysis and seman-
tic interpretation are of at least context-
free complexity, for applications such as
speech processing in which speed is impor-
tant finite-state models are often preferred.
These requirements may be reconciled by
using the more complex grammar to auto-
matically derive a finite-state approxima-
tion which can then be used as a filter to
guide speech recognition or to reject many
hypotheses at an early stage of processing.
A method is presented here for calculat-
ing such finite-state approximations from
context-free grammars. It is essentially dif-
ferent from the algorithm introduced by
Pereira and Wright (1991; 1996), is faster
in some cases, and has the advantage of be-
ing open-ended and adaptable.
</bodyText>
<sectionHeader confidence="0.981349" genericHeader="method">
1 Finite-state approximations
</sectionHeader>
<bodyText confidence="0.99980005">
Adequate models of human language for syntac-
tic analysis and semantic interpretation are typi-
cally of context-free complexity or beyond. Indeed,
Prolog-style definite clause grammars (DCGs) and
formalisms such as PATR with feature-structures
and unification have the power of Turing machines
to recognise arbitrary recursively enumerable sets.
Since recognition and analysis using such models
may be computationally expensive, for applications
such as speech processing in which speed is impor-
tant finite-state models are often preferred.
When natural language processing and speech
recognition are integrated into a single system one
may have the situation of a finite-state language
model being used to guide speech recognition while
a unification-based formalism is used for subsequent
processing of the same sentences. Rather than
write these two grammars separately, which is likely
to lead to problems in maintaining consistency, it
would be preferable to derive the finite-state gram-
mar automatically from the (unification-based) anal-
ysis grammar.
The finite-state grammar derived in this way can
not in general recognise the same language as the
more powerful grammar used for analysis, but, since
it is being used as a front-end or filter, one would
like it not to reject any string that is accepted by
the analysis grammar, so we are primarily interested
in &apos;sound approximations&apos; or &apos;approximations from
above&apos;.
Attention is restricted here to approximations
of context-free grammars because context-free lan-
guages are the smallest class of formal language that
can realistically be applied to the analysis of natural
language. Techniques such as restriction (Shieber,
1985) can be used to construct context-free approx-
imations of many unification-based formalisms, so
techniques for constructing finite-state approxima-
tions of context-free grammars can then be applied
to these formalisms too.
</bodyText>
<sectionHeader confidence="0.976132" genericHeader="method">
2 Finite-state calculus
</sectionHeader>
<bodyText confidence="0.999818533333333">
A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos;
is a set of programs for manipulating finite-state
automata and the regular languages and transduc-
ers that they describe. Standard operations in-
clude intersection, union, difference, determinisation
and minimisation. Recently a number of automata
toolkits have been made publicly available, such as
FIRE Lite (Watson, 1996), Grail (Raymond and
Wood, 1996), and FSA Utilities (van Noord, 1996).
Finite-state calculus has been successfully applied
both to morphology (Kaplan and Kay, 1994; Kempe
and Karttunen, 1996) and to syntax (constraint
grammar, finite-state syntax).
The work described here used a finite-state calcu-
lus implemented by the author in SICStus Prolog.
</bodyText>
<page confidence="0.997896">
452
</page>
<bodyText confidence="0.9990555625">
The use of Prolog rather than C or C++ causes large
overheads in the memory and time required. How-
ever, careful account has been taken of the way Pro-
log operates, its indexing in particular, in order to
ensure that the asymptotic complexity is as good as
that of the best published algorithms, with the result
that for large problems the Prolog implementation
outperforms some of the publicly available imple-
mentations in C++. Some versions of the calculus
allow transitions to be labelled with arbitrary Prolog
terms, including variables, a feature that proved to
be very convenient for prototyping although it does
not essentially alter the power of the machinery. (It
is assumed that the string being tested consists of
ground terms so no unification is performed, just
matching.)
</bodyText>
<sectionHeader confidence="0.971648" genericHeader="method">
3 An approximation algorithm
</sectionHeader>
<bodyText confidence="0.999018214285714">
There are two main ideas behind this algorithm. The
first is to describe the finite-state approximation us-
ing formulae with regular languages and finite-state
operations and to evaluate the formulae directly us-
ing the finite-state calculus. The second is to use,
in intermediate stages of the calculation, additional,
auxiliary symbols which do not appear in the final
result. A similar approach has been used for compil-
ing a two-level formalism for morphology (Grimley
Evans et al., 1996).
In this case the auxiliary symbols are dotted rules
from the given context-free grammar. A dotted rule
is a grammar rule with a dot inserted somewhere on
the right-hand side, e.g.
</bodyText>
<equation confidence="0.902480666666667">
S --4 NP VP
S -4 NP • VP
S -4 NP VP •
</equation>
<bodyText confidence="0.991438615384615">
However, since these dotted rules are to be used
as terminal symbols of a regular language, it is con-
venient to use a more compact notation: they can
be replaced by a triple made out of the nonterminal
symbol on the left-hand side, an integer to determine
one of the productions for that nonterminal, and an
integer to denote the position of the dot on the right-
hand side by counting the number of symbols to the
left of the dot. So, if &apos;S NP VP&apos; is the fourth
production for S, the dotted rules given above may
be denoted by (S, 4,0), (5, 4,1) and (S, 4,2), respec-
tively.
It will turn out to be convenient to use a slightly
more complicated notation: when the dot is located
after the last symbol on the right-hand side we use z
as the third element of the triple instead of the corre-
sponding integer, so the last triple is (S, 4, z) instead
of (S, 4, 2). (Note that z is an additional symbol,
not a variable.) Moreover, for epsilon-rules, where
there are no symbols on the right-hand side, we treat
the e as it were a real symbol and consider there to
be two corresponding dotted rules, e.g. (MOD, 1,0)
and (MOD ,l, z) corresponding to &apos;MOD —&gt; • E&apos; and
&apos;MOD -4 f for the rule &apos;MOD e&apos;.
Using these dotted rules as auxiliary symbols we
can work with regular languages over the alphabet
</bodyText>
<equation confidence="0.9430675">
E=TUI(X,m,n)IXEVAm= 1,...,mxA
n = 0, , max{nx,„, — 1, 0}, z }
</equation>
<bodyText confidence="0.904664769230769">
where T is the set of terminal symbols, V is the set of
nonterminals, mx is the number of productions for
nonterminal X, and nx,„•,, is the number of symbols
on the right-hand side of the mth production for X.
It will be convenient to use the symbol * as a
&apos;wildcard&apos;, so (s, *, 0) means { (X ,m, n) EEIX=
s, n = 0 } and (*, *, z) means { (X, m, n) EEIn=
z }. (This last example explains why we use z rather
than nx,,i; it would otherwise not be possible to use
the `wildcard&apos; notation to denote concisely the set
{ (X, m, n) In = nx,,, }.)
We can now attempt to derive an expression for
the set of strings over E that represent a valid parse
tree for the given grammar: the tree is traversed in a
top-down left-to-right fashion and the daughters of a
node X expanded with the mth production for X are
separated by the symbols (X, m, *). (Equivalently,
one can imagine the auxiliary symbols inserted in
the appropriate places in the right-hand side of each
production so that the grammar is then unambigu-
ous.) Consider, for example, the following grammar:
S—&gt;aSb
S e
Then the following is one of the strings over E that
we would like to accept, corresponding to the string
aabb accepted by the grammar:
</bodyText>
<equation confidence="0.888776">
(s, 1, 0)a(s, 1, 1)(s, 1, 0)a(s, 1, 1)(s, 2, 0)(s, 2, z)
(s, 1, 2)b(s, 1, z)(8,1,2)b(s,1, z)
</equation>
<bodyText confidence="0.997493545454545">
Our first approximation to the set of acceptable
strings is (S, * , 0)E* (S,*, z) , i.e. strings that start
with beginning to parse an S and end with having
parsed an S. From this initial approximation we sub-
tract (that is, we intersect with the complement of)
a series of expressions representing restrictions on
the set of acceptable strings:&apos;
1In these expressions over regular languages set union
and set difference are denoted by + and —, respectively,
while juxtaposition denotes concatenation and the bar
denotes complementation CZ -7.= E* — x).
</bodyText>
<page confidence="0.936781">
453
</page>
<equation confidence="0.99421">
(E*((*,*,*) — (*,*, z))) + e(*,*, 0)E* (1)
</equation>
<bodyText confidence="0.999850571428571">
Formula 1 expresses the restriction that a dotted
rule of the form (*, *, 0), which represents starting to
parse the right-hand side of a rule, may be preceded
only by nothing (the start of the string) or by a
dotted rule that is not of the form (*,*, z) (which
would represent the end of parsing the right-hand
side of a rule).
</bodyText>
<equation confidence="0.973112">
E*(*,*, Z)C ((* * *) (* *3 0))E* (2)
</equation>
<bodyText confidence="0.9853056">
Formula 2 similarly expresses the restriction that
a dotted rule of the form (*,*, z) may be followed
only by nothing or by a dotted rule that is not of
the form (*,*,0).
For each non-epsilon-rule with dotted rules
</bodyText>
<equation confidence="0.948138">
n = 0, . . . — 1,z, for each n =
0, ... ,nx,„ — 1:
E* (X, m, n)next (X, m, n + 1)E* (3)
where
next (X, m, n) =
a(X,m,n) (rhs(X, m, n) = a, a E T, n &lt;n,m)
a(X,m,z) (rhs(X, m, n) = a, a E T, n =
(A, *, 0) (rhs(X, m, n) = A, A E V)
</equation>
<bodyText confidence="0.99685725">
where rhs(X, m, n) is the nth symbol on the right-
hand side of the mth production for X.
Formula 3 states that the dotted rule (X, m, n)
must be followed by a(X,m,n + 1) (or a(X,m, z)
when 71+1 = 71X ,m) when the next item to be parsed
is the terminal a, or by (A, *, 0) (starting to parse
an A) when the next item is the nonterminal A.
For each non-epsilon-rule with dotted rules
</bodyText>
<equation confidence="0.908958625">
(X,m,n), Ti = 0, ,nx,,, — 1,z, for each Ti
...,nx,m — 1,z:
E*prev(X, m, n)(X, m, n)E* (4)
where
prey (X,771, n) =
(X, m, n — 1)a (rhs(X, m, n) = a, a E T, n z)
(X, m, nx,,, — 1)a (rhs(X, m, n) = a, a E T, n = z)
(A, *, z) (rhs(X, m, n) = A, A E V)
</equation>
<bodyText confidence="0.9995934">
Formula 4 similarly states that the dotted rule
(X, m, n) must be preceded by (X, m, n — 1)a (or
— 1) when n = z) when the previous
item was the terminal a, or by (A, *, z) when the
previous item was the nonterminal A.
</bodyText>
<equation confidence="0.87539925">
For each epsilon-rule corresponding to dotted
rules (X, m, 0) and (X, m, z):
E* (X, m, 0)(X, m, z)E*, and
E* (X, m, 0)(X, m, z)E* (6)
</equation>
<bodyText confidence="0.745938111111111">
Formulae 5 and 6 state that the dotted rule
(X, m, 0) must be followed by (X, m, z), and
(X, m, z) must be preceded by (X, m, 0).
For each non-epsilon rule with dotted rules
(X,m,n), Ti = ,nX,m — 1,z, for each n =
0, , nx,m — 1:
E*(X, m, n)(E — (X, m, *))*((X, m, 0) +(X, m,
and
where
</bodyText>
<equation confidence="0.97747525">
n + 1, if n nx,m, — 1;
Ti&apos; =
z, if n nx,m — 1.
Formula 7 states that the next instance of
</equation>
<bodyText confidence="0.999406642857143">
(X,m,*) that follows (X, in, must be either
(X, in, 0) (a recursive application of the same rule)
or (X,m,n!) (the next stage in parsing the same
rule), and there must be such an instance. Formula 8
states similarly that the closest instance of (X, m, *)
that precedes (X, in, Ti&apos;) must be either (X, m, z) (a
recursive application of the same rule) or (X, m, n)
(the previous stage in parsing the same rule), and
there must be such an instance.
When each of these sets has been subtracted from
the initial approximation we can remove the auxil-
iary symbols (by applying the regular operator that
replaces them with f) to give the final finite-state
approximation to the context-free grammar.
</bodyText>
<sectionHeader confidence="0.99671" genericHeader="method">
4 A small example
</sectionHeader>
<bodyText confidence="0.980708">
It may be admitted that the notation used for the
dotted rules was partly motivated by the possibil-
ity of immediately testing the algorithm using the
finite-state calculus in Prolog: the regular expres-
sions listed above can be evaluated directly using the
&apos;wildcard&apos; capabilities of the finite-state calculus.
Figure 2 shows the sequence of calculations that
corresponds to applying the algorithm to the follow-
ing grammar:
S—*aSb
S
With the following notational explanations it should
be possible to understand the code and compare it
with the description of the algorithm.
</bodyText>
<listItem confidence="0.84581025">
• The procedure r(RE,X) evaluates the regu-
lar expression RE and puts the resulting (mM-
imised) automaton into a register with the name
(5) X.
</listItem>
<page confidence="0.838438">
454
</page>
<listItem confidence="0.994211535714286">
• list_f sa (X) prints out the transition table for
the automaton in register X.
• Terminal symbols may be any Prolog terms, so
the terminal alphabet is implicit. Here atoms
are used for the terminal symbols of the gram-
mar (a and b) and terms of the form _/_/_ are
used for the triples representing dotted rules.
The terms need not be ground, so the Prolog
variable symbol _ is used instead of the &apos;wild-
card&apos; symbol * in the description of the algo-
rithm.
• In a regular expression:
- #X refers to the contents of register X;
- $ represents E, any single terminal symbol;
- s represents a string of terminals with
length equal to the number of arguments;
so s with no arguments represents the
empty string 6, s (a) represents the single
terminal a, and s (s/_/0) represents the
dotted rules (s, *, 0);
- Kleene star is * (redefined as a postfix op-
erator), and concatenation and union are -
and +, respectively;
- other operators provided include 84 (inter-
section) and - (difference); there is no oper-
ator for complementation; instead subtrac-
tion from E* may be used, e.g. ($ *) - (#1)
instead of L;
</listItem>
<bodyText confidence="0.913184285714286">
- rem(RE,L) denotes the result of removing
from the language RE all terminals that
match one of the expressions in the list L.
The context-free language recognised by the origi-
nal context-free grammar is { ab&apos; I n &gt; 0 }. The re-
sult of applying the approximation algorithm is a 3-
state automaton recognising the language c + a+ .
</bodyText>
<sectionHeader confidence="0.989065" genericHeader="method">
5 Computational complexity
</sectionHeader>
<bodyText confidence="0.9999604">
Applying the restrictions expressed by formulae 1-6
gives an automaton whose size is at most a small
constant multiple of the size of the input grammar.
This is because these restrictions apply locally: the
state that the automaton is in after reading a dotted
rule is a function of that dotted rule.
When restrictions 7-8 are applied the final au-
tomaton may have size exponential in the size of the
input grammar. For example, exponential behaviour
is exhibited by the following class of grammars:
</bodyText>
<figure confidence="0.455486666666667">
S al S
S an S an
S
</figure>
<bodyText confidence="0.998943904761905">
Here the final automaton has 3&apos; states. (It records,
in effect, one of three possibilities for each terminal
symbol: whether it has not yet appeared, has ap-
peared and must appear again, or has appeared and
need not appear again.)
There is an important computational improve-
ment that can be made to the algorithm as described
above: instead of removing all the auxiliary symbols
right at the end they can be removed progressively
as soon as they are no longer required; after formulae
7-8 have been applied for each non-epsilon rule with
dotted rules (X, m, *), those dotted rules may be
removed from the finite-state language (which typi-
cally makes the automaton smaller); and the dotted
rules corresponding to an epsilon production may
be removed before formulae 7-8 are applied. (To
&apos;remove&apos; a symbol means to substitute it by e: a
regular operation.)
With this important improvement the algorithm
gives exact approximations for the left-linear gram-
mars
</bodyText>
<figure confidence="0.993979714285714">
S -+ S
S S an
S
and the right-linear grammars
S S
S an S
S
</figure>
<bodyText confidence="0.998458727272727">
in space bounded by n and time bounded by n2. (It
is easiest to test this empirically with an implemen-
tation, though it is also possible to check the cal-
culations by hand.) Pereira and Wright&apos;s algorithm
gives an intermediate unfolded recogniser of size ex-
ponential in n for these right-linear grammars.
There are, however, both left-linear and right-
linear grammars for which the number of states in
the final automaton is not bounded by any polyno-
mial function of the size of the grammar. An exam-
ples is:
</bodyText>
<equation confidence="0.985939">
S al S S ai Ai
S -&gt; an S S an An
A1 al X A1 -4 a2 Ai ••• Ai an Ai
A2 al A2 A2 -+ a2 X • • • A2 -4 an A2
An -4 ai An An -4 a2 An an X
X -4
</equation>
<bodyText confidence="0.9998545">
Here the grammar has size 0(n2) and the final ap-
proximation has 2n+1 - 1 states.
</bodyText>
<page confidence="0.999195">
455
</page>
<figureCaption confidence="0.991571">
Figure 1: An 18-rule CFG derived from a unification
grammar.
</figureCaption>
<bodyText confidence="0.997570857142857">
Pereira and Wright (1996) point out in the context
of their algorithm that a grammar may be decom-
posed into &apos;strongly connected&apos; subgrammars, each
of which may be approximated separately and the
results composed. The same method can be used
with the finite-state calculus approach: Define the
relation &apos;IL over nonterminals of the grammar s.t.
A&apos;R,B if B appears on the right-hand side of a pro-
duction for A. Then the relation S = R,* n (7?-*)-1,
the reflexive transitive closure of TZ intersected with
its inverse, is an equivalence relation. A subgram-
mar consists of all the productions for nonterminals
in one of the equivalence classes of S. Calculate
the approximations for each nonterminal by treating
the nonterminals that belong to other equivalence
classes as if they were terminals. Finally, combine
the results from each subgrammar by starting with
the approximation for the start symbol S and substi-
tuting the approximations from the other subgram-
mars in an order consistent with the partial ordering
that is induced by R, on the subgrammars.
</bodyText>
<sectionHeader confidence="0.98674" genericHeader="method">
6 Results with a larger grammar
</sectionHeader>
<bodyText confidence="0.995269936170213">
When the algorithm was applied to the 18-rule gram-
mar shown in figure 1 it was not possible to com-
plete the calculations for any ordering of the rules,
even with the improvement mentioned in the previ-
ous section, as the automata became too large for
the finite-state calculus on the computer that was
being used. (Note that the grammar forms a single
strongly connected component.)
However, it was found possible to simplify the cal-
culation by omitting the application of formulae 7-8
for some of the rules. (The auxiliary symbols not
involved in those rules could then be removed be-
fore the application of 7-8.) In particular, when re-
strictions 7-8 were applied only for the S and VP
rules the calculations could be completed relatively
quickly, as the largest intermediate automaton had
only 406 states. Yet the final result was still a useful
approximation with 16 states.
Pereira and Wright&apos;s algorithm applied to the
same problem gave an intermediate automaton (the
&apos;unfolded recogniser&apos;) with 56272 states, and the fi-
nal result (after flattening and minimisation) was a
finite-state approximation with 13 states.
The two approximations are shown for comparison
in figure 3. Each has the property that the symbols
d, a and n occur only in the combination d a* n. This
fact has been used to simplify the state diagrams by
treating this combination as a single terminal symbol
dan; hence the approximations are drawn with 10
and 9 states, respectively.
Neither of the approximations is better than the
other; their intersection (with 31 states) is a bet-
ter approximation than either. The two approxima-
tions have therefore captured different aspects of the
context-free language.
In general it appears that the approximations pro-
duced by the present algorithm tend to respect the
necessity for certain constituents to be present, at
whatever point in the string the symbols that &apos;trig-
ger&apos; them appear, without necessarily insisting on
their order, while Pereira and Wright&apos;s approxima-
tion tends to take greater account of the constituents
whose appearance is triggered early on in the string:
most of the complexity in Pereira and Wright&apos;s ap-
proximation of the 18-rule grammar is concerned
with what is possible before the first accepting state
is encountered.
</bodyText>
<sectionHeader confidence="0.803998" genericHeader="method">
7 Comparison with previous work
</sectionHeader>
<bodyText confidence="0.991450277777778">
Rimon and Herz (1991; 1991) approximate the
recognition capacity of a context-free grammar by
extracting &apos;local syntactic constraints&apos; in the form of
the Left or Right Short Context of length n of a ter-
minal. When n = 1 this reduces to next(t), the set of
terminals that may follow the terminal t. The effect
of filtering with Rimon and Herz&apos;s next(t) is similar
to applying conditions 1-6 from section 3, but the
use of auxiliary symbols causes two differences which
can both be illustrated with the following grammar:
S-4aXalbXb
X
On the one hand, Rimon and Herz&apos;s &apos;next&apos; does not
distinguish between different instances of the same
terminal symbol, so any a, and not just the first one,
may be followed by another a. On the other hand,
Rimon and Herz&apos;s &apos;next&apos; looks beyond the empty
constituent in a way that conditions 1-6 do not, so
</bodyText>
<table confidence="0.875596833333333">
MOD —&gt;
MOD -+ p NP
NOM -4 a NOM
NOM —&gt; n
NOM —&gt; NOM MOD
NOM —&gt; NOM S
NP
NP d NOM
VP -4 v NP
VP -4 v S
VP -4 v VP
VP v
VP -4 VP c VP
VP -4 VP MOD
S -4 MOD S
S NP S
S—&gt;ScS
S v NP VP
</table>
<page confidence="0.706059">
456
</page>
<table confidence="0.878954448275862">
% initial approximation:
r( s(s/J0)-($ *)-s(s/Jz) , a).
% formulae (1)-(2):
r( (#a) - (($ *)-(($ *)-(s(_/_/_)-s(_/_/z))+s))-s(_/_/0)-($ *) , a).
r( (#a) - ($ *)-s(_/ _/z)-(($ *)-(s+(s(_/_/_)-s(_/_/0))-($ *))) , a).
% formula (3) for &amp;quot;S -&gt; a S b&amp;quot;:
r( (#a) - ($ *)-s(s/1/0)-(($ *)-s(a)-s(s/1/1)-($ *)) a).
r( (#a) - ($ *)-s(s/1/1)-(($ *)-s(s/J0)-($ *)) , a).
r( (#a) - ($ *)-s(s/1/2)-(($ *)-s(b)-s(s/l/z)-($ *)) , a).
% formula (4) for &amp;quot;S -&gt; a S b&amp;quot;:
r( (#a) - (($ *)-($ *)-s(s/1/0)-s(a))-s(s/1/1)-($ *) , a).
r( (#a) - (($ *)-($ *)-s(s/Jz))-s(vp/2/1)-($ *) , a).
r( (#a) - (($ *)-($ *)-s(s/1/2)-s(b))-s(s/l/z)-($ *) , a).
% formulae (5)-(6) for &amp;quot;S -&gt; &amp;quot;:
r( (#a) - ($ *)-s(s/2/0)-(($ *)-s(s/2/z)-($ *)) , a).
r( (#a) - (($ *)-($ *)-s(s/2/0))-s(s/2/z)-($ *) , a).
% formula (7) for &amp;quot;S -&gt; a S b&amp;quot;:
r((#a)-($ *)-s(s/1/0)-(($ *)-(($ -s(s/1/_))*)-(s(s/1/0)+s(s/1/1))-($ *)),a).
r((#a)-($ Ws(s/1/1)-(($ *)-(($ -s(s/1/_))*)-(s(s/1/0)+s(s/1/2))-($ *)),a).
r((#a)-($ *)-s(s/1/2)-(($ *)-(($ -s(s/1/_))*)-(s(s/1/0)+s(s/1/z))-($ *)),a).
% formula (8) for &amp;quot;S -&gt; a S b&amp;quot;:
r((#a)-(($ *)-($ *)-(s(s/1/z)+s(s/1/0))-(($ -s(s/1/_))*))-s(s/1/1)-($ *),a).
r((#a)-(($ *)-($ *)-(s(s/1/z)+s(s/1/1))-(($ -s(s/1/_))*))-s(s/1/2)-($ *),a).
r((#a)-(($ *)-($ *)-(s(s/1/z)+s(s/1/2))-(($ -s(s/1/_))*))-s(s/1/z)-($ *),a).
% define the terminal alphabet:
r( s(s/1/0)+s(s/1/1)+s(s/1/2)+s(s/1/z)+s(s/2/0)+s(s/2/z)+s(a)+s(b), sigma).
% remove the auxiliary symbols to give final result:
r( rem((#a)&amp;((#sigma) *),[_/_/_]) , f).
list_fsa(f).
</table>
<figureCaption confidence="0.989833">
Figure 2: The sequence of calculations for approximating S —&gt; aSb I e, coded for the finite-state calculus.
</figureCaption>
<figure confidence="0.996481">
4 \
U dan
</figure>
<figureCaption confidence="0.81742">
Figure 3: Finite-state approximations for the grammar in figure 1 calculated with the finite-state calculus
(left) and by Pereira and Wright&apos;s algorithm (right).
</figureCaption>
<page confidence="0.993764">
457
</page>
<bodyText confidence="0.999942837837838">
ab is disallowed. Thus an approximation based on
Rimon and Herz&apos;s &apos;next&apos; would be aa* + bb*, and
an approximation based on conditions 1-6 would be
(a+ b)(a+b). (However, the approximation becomes
exact when conditions 7-8 are added.)
Both Pereira and Wright (1991; 1996) and Rood
(1996) start with the LR(0) characteristic machine,
which they first &apos;unfold&apos; (with respect to &apos;stacks&apos; or
&apos;paths&apos;, respectively) and then &apos;flatten&apos;. The char-
acteristic machine is defined in terms of dotted rules
with transitions between them that are analagous
to the conditions implied by formula 3 of section
3. When the machine is flattened, 6-transitions are
added in a way that is in effect simulated by condi-
tions 2 and 4. (Condition 1 turns out to be implied
by conditions 2-4.) It can be shown that the approx-
imation Lo obtained by flattening the characteristic
machine (without unfolding it) is as good as the ap-
proximation L1-6 obtained by applying conditions
1-6 (Lo C L1_6). Moreover, if no nonterminal for
which there is an &amp;production is used more than
once in the grammar, then Lo = L1_6. (The gram-
mar in figure 1 is an example for which Lo L1-6;
the approximation found in section 6 includes strings
such as vvccvv which are not accepted by Lo for
this grammar.) It can also be shown that Li-6 is
the same as the result of flattening the character-
istic machine for the same grammar modifed so as
to fulfil the afore-mentioned condition by replacing
the right-hand side of every &amp;production with a new
nonterminal for which there is a single &amp;production.
However, there does not seem to be a simple corre-
spondence between conditions 7-8 and the &apos;unfold-
ing&apos; used by Pereira and Wright or Rood: even some
simple grammars such as &apos;S aS al b Sb I c&apos; are
approximated differently by 1-8 than by Pereira and
Wright&apos;s and Rood&apos;s methods.
</bodyText>
<sectionHeader confidence="0.915319" genericHeader="discussions">
8 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.99976303030303">
In the case of some simple examples (such as the
grammar &apos;S aSbj f&apos; used earlier) the approxi-
mation algorithm presented in this paper gives the
same result as Pereira and Wright&apos;s algorithm. How-
ever, in many other cases (such as the grammar &apos;S
aS albSbj 6&apos; or the 18-rule grammar in the
previous section) the results are essentially different
and neither of the approximations is better than the
other.
The new algorithm does not share the problem of
Pereira and Wright&apos;s algorithm that certain right-
linear grammars give an intermediate automaton of
exponential size, and it was possible to calculate a
useful approximation fairly rapidly in the case of the
18-rule grammar in the previous section. However, it
is not yet possible to draw general conclusions about
the relative efficiency of the two procedures. Never-
theless, the new algorithm seems to have the advan-
tage of being open-ended and adaptable: in the pre-
vious section it was possible to complete a difficult
calculation by relaxing the conditions of formulae 7-
8, and it is easy to see how those conditions might
also be strengthened. For example, a more compli-
cated version of formulae 7-8 might check two levels
of recursive application of the same rule rather than
just one level and it might be useful to generalise
this to 11 levels of recursion in a manner analagous to
Rood&apos;s (1996) generalisation of Pereira and Wright&apos;s
algorithm.
The algorithm also demonstrates how the general
machinery of a finite-state calculus can be usefully
applied as a framework for expressing and solving
problems in natural language processing.
</bodyText>
<sectionHeader confidence="0.998531" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998870096774193">
Grimley Evans, Edmund, George Kiraz, and
Stephen Pulman. 1996. Compiling a Partition-
Based Two-Level Formalism. COLING-96, 454—
459.
Herz, Jacky, and Mori Rimon. 1991. Local Syntac-
tic Constraints. Second International Workshop
on Parsing Technology (IWPT-2).
Kaplan, Ronald, and Martin Kay. 1994. Regular
models of phonological rule systems. Computa-
tional Linguistics, 20(3): 331-78.
Kempe, Ande, and Lauri Karttunen. 1996. Parallel
Replacement in Finite State Calculus. COLING-
96, 622.
Pereira, Fernando, and Rebecca Wright. 1991.
Finite-state approximation of phrase structure
grammars. Proceedings of the 29th Annual Meet-
ing of the Association for Computational Linguis-
tics, 246-255.
Pereira, Fernando, and Rebecca Wright. 1996.
Finite-State Approximation of Phrase-Structure
Grammars. cmp-lg/9603002.
Raymond, Darrell, and Derick Wood. March 1996.
The Grail Papers. University of Western Ontario,
Department of Computer Science, Technical Re-
port TR-491.
Rimon, Mori, and Jacky Herz. 1991. The recogni-
tion capacity of local syntactic constraints. ACL
Proceedings, 5th European Meeting.
Rood, Cathy. 1996. Efficient Finite-State Approxi-
mation of Context Free Grammars. Proceedings
of ECAI 96.
</reference>
<page confidence="0.9839">
458
</page>
<reference confidence="0.977214416666667">
Shieber, Stuart. 1985. Using restriction to extend
parsing algorithms for complex-feature-based for-
malisms. Proceedings of the 23nd Annual Meeting
of the Association for Computational Linguistics,
145-152.
Van Noord, Gertjan. 1996. FSA Utilities: Manipu-
lation of Finite-State Automata implemented in
Prolog. First International Workshop on Imple-
menting Automata, University of Western On-
tario, London Ontario, 29-31 August 1996.
Watson, Bruce. 1996. Implementing and using finite
automata toolkits. Proccedings of ECAI 96.
</reference>
<page confidence="0.999127">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000132">
<title confidence="0.9999055">Approximating Context-Free Grammars with a Finite-State Calculus</title>
<author confidence="0.997528">Edmund GRIMLEY EVANS</author>
<affiliation confidence="0.9999805">Computer Laboratory University of Cambridge</affiliation>
<address confidence="0.999719">Cambridge, CB2 3QG, GB</address>
<author confidence="0.537874">ac uk</author>
<abstract confidence="0.989279944606414">Although adequate models of human language for syntactic analysis and semantic interpretation are of at least contextfree complexity, for applications such as speech processing in which speed is important finite-state models are often preferred. These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing. A method is presented here for calculating such finite-state approximations from context-free grammars. It is essentially different from the algorithm introduced by Pereira and Wright (1991; 1996), is faster in some cases, and has the advantage of being open-ended and adaptable. 1 Finite-state approximations Adequate models of human language for syntactic analysis and semantic interpretation are typically of context-free complexity or beyond. Indeed, Prolog-style definite clause grammars (DCGs) and formalisms such as PATR with feature-structures and unification have the power of Turing machines to recognise arbitrary recursively enumerable sets. Since recognition and analysis using such models may be computationally expensive, for applications such as speech processing in which speed is important finite-state models are often preferred. When natural language processing and speech recognition are integrated into a single system one may have the situation of a finite-state language model being used to guide speech recognition while a unification-based formalism is used for subsequent processing of the same sentences. Rather than write these two grammars separately, which is likely to lead to problems in maintaining consistency, it would be preferable to derive the finite-state grammar automatically from the (unification-based) analysis grammar. The finite-state grammar derived in this way can not in general recognise the same language as the more powerful grammar used for analysis, but, since it is being used as a front-end or filter, one would like it not to reject any string that is accepted by the analysis grammar, so we are primarily interested in &apos;sound approximations&apos; or &apos;approximations from above&apos;. Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language. Techniques such as restriction (Shieber, 1985) can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as FIRE Lite (Watson, 1996), Grail (Raymond and Wood, 1996), and FSA Utilities (van Noord, 1996). Finite-state calculus has been successfully applied both to morphology (Kaplan and Kay, 1994; Kempe and Karttunen, 1996) and to syntax (constraint grammar, finite-state syntax). The work described here used a finite-state calculus implemented by the author in SICStus Prolog. 452 The use of Prolog rather than C or C++ causes large overheads in the memory and time required. However, careful account has been taken of the way Prolog operates, its indexing in particular, in order to ensure that the asymptotic complexity is as good as that of the best published algorithms, with the result that for large problems the Prolog implementation outperforms some of the publicly available implementations in C++. Some versions of the calculus allow transitions to be labelled with arbitrary Prolog terms, including variables, a feature that proved to be very convenient for prototyping although it does not essentially alter the power of the machinery. (It is assumed that the string being tested consists of ground terms so no unification is performed, just matching.) 3 An approximation algorithm There are two main ideas behind this algorithm. The first is to describe the finite-state approximation using formulae with regular languages and finite-state operations and to evaluate the formulae directly using the finite-state calculus. The second is to use, in intermediate stages of the calculation, additional, auxiliary symbols which do not appear in the final result. A similar approach has been used for compiling a two-level formalism for morphology (Grimley al., In this case the auxiliary symbols are dotted rules from the given context-free grammar. A dotted rule is a grammar rule with a dot inserted somewhere on the right-hand side, e.g. S --4 NP VP S -4 NP • VP S -4 NP VP • However, since these dotted rules are to be used terminal symbols of a regular language, it is convenient to use a more compact notation: they can be replaced by a triple made out of the nonterminal symbol on the left-hand side, an integer to determine one of the productions for that nonterminal, and an integer to denote the position of the dot on the righthand side by counting the number of symbols to the left of the dot. So, if &apos;S NP VP&apos; is the fourth production for S, the dotted rules given above may denoted by and respectively. It will turn out to be convenient to use a slightly more complicated notation: when the dot is located the last symbol on the right-hand side we use as the third element of the triple instead of the correinteger, so the last triple is 2). (Note that an additional symbol, not a variable.) Moreover, for epsilon-rules, where there are no symbols on the right-hand side, we treat the e as it were a real symbol and consider there to two corresponding dotted rules, e.g. ,l, z) to &apos;MOD —&gt; • &apos;MOD -4 f for the rule &apos;MOD e&apos;. Using these dotted rules as auxiliary symbols we can work with regular languages over the alphabet E=TUI(X,m,n)IXEVAm= 1,...,mxA = , max{nx,„, — 1, 0}, the set of terminal symbols, V is the set of the number of productions for the number of symbols the right-hand side of the mth production for It will be convenient to use the symbol * as a so 0) means { ,m, n) = 0 } and (*, *, { n) }. (This last example explains why we use it would otherwise not be possible to use the `wildcard&apos; notation to denote concisely the set n) In = nx,,, }.) We can now attempt to derive an expression for the set of strings over E that represent a valid parse tree for the given grammar: the tree is traversed in a top-down left-to-right fashion and the daughters of a node X expanded with the mth production for X are by the symbols *). (Equivalently, one can imagine the auxiliary symbols inserted in the appropriate places in the right-hand side of each production so that the grammar is then unambiguous.) Consider, for example, the following grammar: S—&gt;aSb S e Then the following is one of the strings over E that we would like to accept, corresponding to the string by the grammar: 0)a(s, 1, 1)(s, 1, 0)a(s, 1, 1)(s, 2, 0)(s, 2, z) (s, 1, 2)b(s, 1, z)(8,1,2)b(s,1, z) Our first approximation to the set of acceptable is * , 0)E* (S,*, z) , strings that start with beginning to parse an S and end with having parsed an S. From this initial approximation we subtract (that is, we intersect with the complement of) a series of expressions representing restrictions on the set of acceptable strings:&apos; these expressions over regular languages set union and set difference are denoted by + and —, respectively, while juxtaposition denotes concatenation and the bar complementation E* — 453 (E*((*,*,*) — (*,*, z))) + e(*,*, 0)E* (1) the restriction that a dotted rule of the form (*, *, 0), which represents starting to parse the right-hand side of a rule, may be preceded only by nothing (the start of the string) or by a dotted rule that is not of the form (*,*, z) (which would represent the end of parsing the right-hand side of a rule). ((* * *) (* *3 Formula 2 similarly expresses the restriction that a dotted rule of the form (*,*, z) may be followed only by nothing or by a dotted rule that is not of the form (*,*,0). For each non-epsilon-rule with dotted rules = 0, . . . — for each n = ... 1: E* n)next n + 1)E* where n) = m, n) = a, a m, n) = a, a n = *, (rhs(X, m, n) = A, A E V) where rhs(X, m, n) is the nth symbol on the rightside of the mth production for 3 states that the dotted rule n) be followed by + 1) z) = 71X the next item to be parsed is the terminal a, or by (A, *, 0) (starting to parse an A) when the next item is the nonterminal A. For each non-epsilon-rule with dotted rules Ti = 0, ,nx,,, — for each Ti ...,nx,m — 1,z: m, n)E* (4) where = n — 1)a (rhs(X, m, n) = a, a E n z) m, nx,,, — (rhs(X, m, n) = a, a n = z) *, z) m, n) = A, A Formula 4 similarly states that the dotted rule n) must be preceded by n — 1)a (or — 1) when n = z) when the previous item was the terminal a, or by (A, *, z) when the previous item was the nonterminal A. For each epsilon-rule corresponding to dotted 0) and 0)(X, m, z)E*, and 0)(X, m, z)E* (6) Formulae 5 and 6 state that the dotted rule 0) must be followed by z) must be preceded by 0). For each non-epsilon rule with dotted rules — 1,z, each n = , — 1: m, n)(E (X, m, *))*((X,m, (7) and (8) where + 1, if n — 1; Ti&apos; = n — 1. Formula 7 states that the next instance of follows in, be either in, (a recursive application of the same rule) next stage in parsing the same rule), and there must be such an instance. Formula 8 similarly that the closest instance of m, *) precedes be either m, z) application of the same rule) or m, (the previous stage in parsing the same rule), and there must be such an instance. When each of these sets has been subtracted from the initial approximation we can remove the auxiliary symbols (by applying the regular operator that replaces them with f) to give the final finite-state approximation to the context-free grammar. 4 A small example It may be admitted that the notation used for the dotted rules was partly motivated by the possibility of immediately testing the algorithm using the finite-state calculus in Prolog: the regular expressions listed above can be evaluated directly using the &apos;wildcard&apos; capabilities of the finite-state calculus. Figure 2 shows the sequence of calculations that corresponds to applying the algorithm to the following grammar: S—*aSb S With the following notational explanations it should be possible to understand the code and compare it with the description of the algorithm. The procedure the reguexpression puts the resulting (mMimised) automaton into a register with the name 454 list_f sa (X) out the transition table for the automaton in register X. • Terminal symbols may be any Prolog terms, so the terminal alphabet is implicit. Here atoms are used for the terminal symbols of the grammar (a and b) and terms of the form _/_/_ are used for the triples representing dotted rules. The terms need not be ground, so the Prolog variable symbol _ is used instead of the &apos;wildcard&apos; symbol * in the description of the algorithm. • In a regular expression: - #X refers to the contents of register X; - $ represents E, any single terminal symbol; s a string of terminals with length equal to the number of arguments; no arguments represents the string 6, (a) the single terminal a, and s (s/_/0) represents the rules 0); - Kleene star is * (redefined as a postfix operator), and concatenation and union are and +, respectively; other operators provided include (intersection) and - (difference); there is no operator for complementation; instead subtraction from E* may be used, e.g. ($ *) - (#1) of rem(RE,L) the result of removing the language terminals that one of the expressions in the list The context-free language recognised by the origicontext-free grammar is { ab&apos; I n &gt; }. result of applying the approximation algorithm is a 3automaton recognising the language c + a+ 5 Computational complexity Applying the restrictions expressed by formulae 1-6 gives an automaton whose size is at most a small constant multiple of the size of the input grammar. This is because these restrictions apply locally: the state that the automaton is in after reading a dotted rule is a function of that dotted rule. When restrictions 7-8 are applied the final automaton may have size exponential in the size of the input grammar. For example, exponential behaviour is exhibited by the following class of grammars: S al S S S Here the final automaton has 3&apos; states. (It records, in effect, one of three possibilities for each terminal symbol: whether it has not yet appeared, has appeared and must appear again, or has appeared and need not appear again.) There is an important computational improvement that can be made to the algorithm as described above: instead of removing all the auxiliary symbols right at the end they can be removed progressively as soon as they are no longer required; after formulae 7-8 have been applied for each non-epsilon rule with rules *), those dotted rules may be removed from the finite-state language (which typically makes the automaton smaller); and the dotted rules corresponding to an epsilon production may be removed before formulae 7-8 are applied. (To a symbol means to substitute it by regular operation.) With this important improvement the algorithm gives exact approximations for the left-linear grammars</abstract>
<title confidence="0.871915285714286">S -+ S S S and the right-linear grammars S S S S</title>
<abstract confidence="0.994720181818182">space bounded by n and time bounded by (It is easiest to test this empirically with an implementation, though it is also possible to check the calculations by hand.) Pereira and Wright&apos;s algorithm gives an intermediate unfolded recogniser of size exponential in n for these right-linear grammars. There are, however, both left-linear and rightlinear grammars for which the number of states in the final automaton is not bounded by any polynomial function of the size of the grammar. An examples is: S S ai S S An XA1 Ai ••• Ai an Ai al A2 -+ a2 X• • • A2 -4 A2 -4 -4 X X -4 the grammar has size and the final aphas - 455 18-rule CFG derived from a unification grammar. Pereira and Wright (1996) point out in the context of their algorithm that a grammar may be decomposed into &apos;strongly connected&apos; subgrammars, each of which may be approximated separately and the results composed. The same method can be used with the finite-state calculus approach: Define the relation &apos;IL over nonterminals of the grammar s.t. on the right-hand side of a profor A. Then the relation = R,* the reflexive transitive closure of TZ intersected with its inverse, is an equivalence relation. A subgrammar consists of all the productions for nonterminals one of the equivalence classes of the approximations for each nonterminal by treating the nonterminals that belong to other equivalence classes as if they were terminals. Finally, combine the results from each subgrammar by starting with approximation for the start symbol substituting the approximations from the other subgrammars in an order consistent with the partial ordering that is induced by R, on the subgrammars. 6 Results with a larger grammar When the algorithm was applied to the 18-rule grammar shown in figure 1 it was not possible to complete the calculations for any ordering of the rules, even with the improvement mentioned in the previous section, as the automata became too large for the finite-state calculus on the computer that was being used. (Note that the grammar forms a single strongly connected component.) However, it was found possible to simplify the calculation by omitting the application of formulae 7-8 for some of the rules. (The auxiliary symbols not involved in those rules could then be removed before the application of 7-8.) In particular, when restrictions 7-8 were applied only for the S and VP rules the calculations could be completed relatively quickly, as the largest intermediate automaton had only 406 states. Yet the final result was still a useful approximation with 16 states. Pereira and Wright&apos;s algorithm applied to the same problem gave an intermediate automaton (the &apos;unfolded recogniser&apos;) with 56272 states, and the final result (after flattening and minimisation) was a finite-state approximation with 13 states. The two approximations are shown for comparison in figure 3. Each has the property that the symbols a n occur only in the combination n. This fact has been used to simplify the state diagrams by treating this combination as a single terminal symbol the approximations are drawn with 10 and 9 states, respectively. Neither of the approximations is better than the their intersection (with 31 states) is a better approximation than either. The two approximations have therefore captured different aspects of the context-free language. In general it appears that the approximations produced by the present algorithm tend to respect the necessity for certain constituents to be present, at whatever point in the string the symbols that &apos;trigger&apos; them appear, without necessarily insisting on order, while Pereira and Wright&apos;s approximation tends to take greater account of the constituents whose appearance is triggered early on in the string: most of the complexity in Pereira and Wright&apos;s approximation of the 18-rule grammar is concerned with what is possible before the first accepting state is encountered. 7 Comparison with previous work Rimon and Herz (1991; 1991) approximate the recognition capacity of a context-free grammar by extracting &apos;local syntactic constraints&apos; in the form of the Left or Right Short Context of length n of a terminal. When n = 1 this reduces to next(t), the set of terminals that may follow the terminal t. The effect of filtering with Rimon and Herz&apos;s next(t) is similar to applying conditions 1-6 from section 3, but the use of auxiliary symbols causes two differences which can both be illustrated with the following grammar: S-4aXalbXb X On the one hand, Rimon and Herz&apos;s &apos;next&apos; does not distinguish between different instances of the same terminal symbol, so any a, and not just the first one, may be followed by another a. On the other hand, Rimon and Herz&apos;s &apos;next&apos; looks beyond the empty constituent in a way that conditions 1-6 do not, so</abstract>
<title confidence="0.638304470588235">MOD —&gt; MOD -+ p NP NOM -4 a NOM NOM —&gt; n NOM —&gt; NOM MOD NOM —&gt; NOM S NP NP d NOM VP -4 v NP VP -4 v S VP VP v VP -4 VP c VP VP -4 VP MOD S -4 MOD S S NP S S—&gt;ScS</title>
<note confidence="0.955208736842105">S v NP VP 456 % initial approximation: , a). % formulae (1)-(2): (#a) - (($ *)-(($ *) , a). (#a) - ($ *))) , a). % formula (3) for &amp;quot;S -&gt; a S b&amp;quot;: (#a) - ($ *)) a). (#a) - ($ *)) , a). (#a) - ($ *)) , a). % formula (4) for &amp;quot;S -&gt; a S b&amp;quot;: (#a) - (($ *)-($ *) , a). (#a) - (($ *)-($ *) , a). (#a) - (($ *)-($ *) , a). % formulae (5)-(6) for &amp;quot;S -&gt; &amp;quot;: (#a) - ($ *)) , a). (#a) - (($ *)-($ *) , a). % formula (7) for &amp;quot;S -&gt; a S b&amp;quot;:</note>
<abstract confidence="0.978676602272727">a). *)-(($ *)),a). *)-(($ *)),a). % formula (8) for &amp;quot;S -&gt; a S b&amp;quot;: *)-($ *),a). *)-($ *),a). *)-($ *),a). % define the terminal alphabet: r( s(s/1/0)+s(s/1/1)+s(s/1/2)+s(s/1/z)+s(s/2/0)+s(s/2/z)+s(a)+s(b), sigma). % remove the auxiliary symbols to give final result: r( rem((#a)&amp;((#sigma) *),[_/_/_]) , f). list_fsa(f). 2: The sequence of calculations for approximating S —&gt; aSb I for the finite-state calculus. U dan Figure 3: Finite-state approximations for the grammar in figure 1 calculated with the finite-state calculus (left) and by Pereira and Wright&apos;s algorithm (right). 457 disallowed. Thus an approximation based on and Herz&apos;s &apos;next&apos; would be aa* + an approximation based on conditions 1-6 would be the approximation becomes exact when conditions 7-8 are added.) Both Pereira and Wright (1991; 1996) and Rood (1996) start with the LR(0) characteristic machine, which they first &apos;unfold&apos; (with respect to &apos;stacks&apos; or &apos;paths&apos;, respectively) and then &apos;flatten&apos;. The characteristic machine is defined in terms of dotted rules with transitions between them that are analagous to the conditions implied by formula 3 of section 3. When the machine is flattened, 6-transitions are added in a way that is in effect simulated by conditions 2 and 4. (Condition 1 turns out to be implied by conditions 2-4.) It can be shown that the approxobtained by flattening the characteristic machine (without unfolding it) is as good as the apby applying conditions 1-6 (Lo C L1_6). Moreover, if no nonterminal for which there is an &amp;production is used more than in the grammar, then Lo = (The gramin figure 1 is an example for which Lo the approximation found in section 6 includes strings as vvccvv which are not accepted by for grammar.) It can also be shown that the same as the result of flattening the characteristic machine for the same grammar modifed so as to fulfil the afore-mentioned condition by replacing the right-hand side of every &amp;production with a new nonterminal for which there is a single &amp;production. However, there does not seem to be a simple correspondence between conditions 7-8 and the &apos;unfolding&apos; used by Pereira and Wright or Rood: even some simple grammars such as &apos;S aS al b Sb I c&apos; are approximated differently by 1-8 than by Pereira and Wright&apos;s and Rood&apos;s methods. 8 Discussion and conclusions In the case of some simple examples (such as the &apos;S aSbj f&apos; used earlier) the mation algorithm presented in this paper gives the same result as Pereira and Wright&apos;s algorithm. However, in many other cases (such as the grammar &apos;S aS albSbj 6&apos; or the 18-rule grammar in the previous section) the results are essentially different and neither of the approximations is better than the other. The new algorithm does not share the problem of Pereira and Wright&apos;s algorithm that certain rightlinear grammars give an intermediate automaton of exponential size, and it was possible to calculate a useful approximation fairly rapidly in the case of the 18-rule grammar in the previous section. However, it is not yet possible to draw general conclusions about the relative efficiency of the two procedures. Nevertheless, the new algorithm seems to have the advantage of being open-ended and adaptable: in the previous section it was possible to complete a difficult calculation by relaxing the conditions of formulae 7- 8, and it is easy to see how those conditions might also be strengthened. For example, a more complicated version of formulae 7-8 might check two levels of recursive application of the same rule rather than just one level and it might be useful to generalise to of recursion in a manner analagous to Rood&apos;s (1996) generalisation of Pereira and Wright&apos;s algorithm. The algorithm also demonstrates how the general machinery of a finite-state calculus can be usefully applied as a framework for expressing and solving problems in natural language processing.</abstract>
<note confidence="0.95442047826087">References Grimley Evans, Edmund, George Kiraz, and Stephen Pulman. 1996. Compiling a Partition- Based Two-Level Formalism. COLING-96, 454— 459. Herz, Jacky, and Mori Rimon. 1991. Local Syntactic Constraints. Second International Workshop on Parsing Technology (IWPT-2). Kaplan, Ronald, and Martin Kay. 1994. Regular of phonological rule systems. Computa- Linguistics, 20(3): Kempe, Ande, and Lauri Karttunen. 1996. Parallel Replacement in Finite State Calculus. COLING- 96, 622. Pereira, Fernando, and Rebecca Wright. 1991. Finite-state approximation of phrase structure grammars. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, 246-255. Pereira, Fernando, and Rebecca Wright. 1996. Finite-State Approximation of Phrase-Structure Grammars. cmp-lg/9603002. Raymond, Darrell, and Derick Wood. March 1996. The Grail Papers. University of Western Ontario, Department of Computer Science, Technical Report TR-491. Rimon, Mori, and Jacky Herz. 1991. The recognition capacity of local syntactic constraints. ACL Proceedings, 5th European Meeting. Rood, Cathy. 1996. Efficient Finite-State Approximation of Context Free Grammars. Proceedings of ECAI 96. 458 Shieber, Stuart. 1985. Using restriction to extend parsing algorithms for complex-feature-based formalisms. Proceedings of the 23nd Annual Meeting of the Association for Computational Linguistics, 145-152. Van Noord, Gertjan. 1996. FSA Utilities: Manipulation of Finite-State Automata implemented in Prolog. First International Workshop on Implementing Automata, University of Western Ontario, London Ontario, 29-31 August 1996. Watson, Bruce. 1996. Implementing and using finite automata toolkits. Proccedings of ECAI 96. 459</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Grimley Evans</author>
<author>George Kiraz Edmund</author>
<author>Stephen Pulman</author>
</authors>
<title>Compiling a PartitionBased Two-Level Formalism.</title>
<date>1996</date>
<tech>COLING-96,</tech>
<pages>454--459</pages>
<contexts>
<context position="4966" citStr="Evans et al., 1996" startWordPosition="746" endWordPosition="749">hat the string being tested consists of ground terms so no unification is performed, just matching.) 3 An approximation algorithm There are two main ideas behind this algorithm. The first is to describe the finite-state approximation using formulae with regular languages and finite-state operations and to evaluate the formulae directly using the finite-state calculus. The second is to use, in intermediate stages of the calculation, additional, auxiliary symbols which do not appear in the final result. A similar approach has been used for compiling a two-level formalism for morphology (Grimley Evans et al., 1996). In this case the auxiliary symbols are dotted rules from the given context-free grammar. A dotted rule is a grammar rule with a dot inserted somewhere on the right-hand side, e.g. S --4 NP VP S -4 NP • VP S -4 NP VP • However, since these dotted rules are to be used as terminal symbols of a regular language, it is convenient to use a more compact notation: they can be replaced by a triple made out of the nonterminal symbol on the left-hand side, an integer to determine one of the productions for that nonterminal, and an integer to denote the position of the dot on the righthand side by count</context>
</contexts>
<marker>Evans, Edmund, Pulman, 1996</marker>
<rawString>Grimley Evans, Edmund, George Kiraz, and Stephen Pulman. 1996. Compiling a PartitionBased Two-Level Formalism. COLING-96, 454— 459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacky Herz</author>
<author>Mori Rimon</author>
</authors>
<title>Local Syntactic Constraints.</title>
<date>1991</date>
<booktitle>Second International Workshop on Parsing Technology (IWPT-2).</booktitle>
<marker>Herz, Rimon, 1991</marker>
<rawString>Herz, Jacky, and Mori Rimon. 1991. Local Syntactic Constraints. Second International Workshop on Parsing Technology (IWPT-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>331--78</pages>
<contexts>
<context position="3476" citStr="Kaplan and Kay, 1994" startWordPosition="508" endWordPosition="511"> grammars can then be applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as FIRE Lite (Watson, 1996), Grail (Raymond and Wood, 1996), and FSA Utilities (van Noord, 1996). Finite-state calculus has been successfully applied both to morphology (Kaplan and Kay, 1994; Kempe and Karttunen, 1996) and to syntax (constraint grammar, finite-state syntax). The work described here used a finite-state calculus implemented by the author in SICStus Prolog. 452 The use of Prolog rather than C or C++ causes large overheads in the memory and time required. However, careful account has been taken of the way Prolog operates, its indexing in particular, in order to ensure that the asymptotic complexity is as good as that of the best published algorithms, with the result that for large problems the Prolog implementation outperforms some of the publicly available implement</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, Ronald, and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3): 331-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ande Kempe</author>
<author>Lauri Karttunen</author>
</authors>
<date>1996</date>
<booktitle>Parallel Replacement in Finite State Calculus. COLING96,</booktitle>
<pages>622</pages>
<contexts>
<context position="3504" citStr="Kempe and Karttunen, 1996" startWordPosition="512" endWordPosition="515">applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as FIRE Lite (Watson, 1996), Grail (Raymond and Wood, 1996), and FSA Utilities (van Noord, 1996). Finite-state calculus has been successfully applied both to morphology (Kaplan and Kay, 1994; Kempe and Karttunen, 1996) and to syntax (constraint grammar, finite-state syntax). The work described here used a finite-state calculus implemented by the author in SICStus Prolog. 452 The use of Prolog rather than C or C++ causes large overheads in the memory and time required. However, careful account has been taken of the way Prolog operates, its indexing in particular, in order to ensure that the asymptotic complexity is as good as that of the best published algorithms, with the result that for large problems the Prolog implementation outperforms some of the publicly available implementations in C++. Some versions</context>
</contexts>
<marker>Kempe, Karttunen, 1996</marker>
<rawString>Kempe, Ande, and Lauri Karttunen. 1996. Parallel Replacement in Finite State Calculus. COLING96, 622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Rebecca Wright</author>
</authors>
<title>Finite-state approximation of phrase structure grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>246--255</pages>
<contexts>
<context position="884" citStr="Pereira and Wright (1991" startWordPosition="129" endWordPosition="132">lysis and semantic interpretation are of at least contextfree complexity, for applications such as speech processing in which speed is important finite-state models are often preferred. These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing. A method is presented here for calculating such finite-state approximations from context-free grammars. It is essentially different from the algorithm introduced by Pereira and Wright (1991; 1996), is faster in some cases, and has the advantage of being open-ended and adaptable. 1 Finite-state approximations Adequate models of human language for syntactic analysis and semantic interpretation are typically of context-free complexity or beyond. Indeed, Prolog-style definite clause grammars (DCGs) and formalisms such as PATR with feature-structures and unification have the power of Turing machines to recognise arbitrary recursively enumerable sets. Since recognition and analysis using such models may be computationally expensive, for applications such as speech processing in which </context>
<context position="22252" citStr="Pereira and Wright (1991" startWordPosition="3880" endWordPosition="3883">s to give final result: r( rem((#a)&amp;((#sigma) *),[_/_/_]) , f). list_fsa(f). Figure 2: The sequence of calculations for approximating S —&gt; aSb I e, coded for the finite-state calculus. 4 \ U dan Figure 3: Finite-state approximations for the grammar in figure 1 calculated with the finite-state calculus (left) and by Pereira and Wright&apos;s algorithm (right). 457 ab is disallowed. Thus an approximation based on Rimon and Herz&apos;s &apos;next&apos; would be aa* + bb*, and an approximation based on conditions 1-6 would be (a+ b)(a+b). (However, the approximation becomes exact when conditions 7-8 are added.) Both Pereira and Wright (1991; 1996) and Rood (1996) start with the LR(0) characteristic machine, which they first &apos;unfold&apos; (with respect to &apos;stacks&apos; or &apos;paths&apos;, respectively) and then &apos;flatten&apos;. The characteristic machine is defined in terms of dotted rules with transitions between them that are analagous to the conditions implied by formula 3 of section 3. When the machine is flattened, 6-transitions are added in a way that is in effect simulated by conditions 2 and 4. (Condition 1 turns out to be implied by conditions 2-4.) It can be shown that the approximation Lo obtained by flattening the characteristic machine (wit</context>
</contexts>
<marker>Pereira, Wright, 1991</marker>
<rawString>Pereira, Fernando, and Rebecca Wright. 1991. Finite-state approximation of phrase structure grammars. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, 246-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Rebecca Wright</author>
</authors>
<title>Finite-State Approximation of Phrase-Structure Grammars.</title>
<date>1996</date>
<tech>cmp-lg/9603002.</tech>
<contexts>
<context position="15828" citStr="Pereira and Wright (1996)" startWordPosition="2835" endWordPosition="2838">t&apos;s algorithm gives an intermediate unfolded recogniser of size exponential in n for these right-linear grammars. There are, however, both left-linear and rightlinear grammars for which the number of states in the final automaton is not bounded by any polynomial function of the size of the grammar. An examples is: S al S S ai Ai S -&gt; an S S an An A1 al X A1 -4 a2 Ai ••• Ai an Ai A2 al A2 A2 -+ a2 X • • • A2 -4 an A2 An -4 ai An An -4 a2 An an X X -4 Here the grammar has size 0(n2) and the final approximation has 2n+1 - 1 states. 455 Figure 1: An 18-rule CFG derived from a unification grammar. Pereira and Wright (1996) point out in the context of their algorithm that a grammar may be decomposed into &apos;strongly connected&apos; subgrammars, each of which may be approximated separately and the results composed. The same method can be used with the finite-state calculus approach: Define the relation &apos;IL over nonterminals of the grammar s.t. A&apos;R,B if B appears on the right-hand side of a production for A. Then the relation S = R,* n (7?-*)-1, the reflexive transitive closure of TZ intersected with its inverse, is an equivalence relation. A subgrammar consists of all the productions for nonterminals in one of the equiv</context>
</contexts>
<marker>Pereira, Wright, 1996</marker>
<rawString>Pereira, Fernando, and Rebecca Wright. 1996. Finite-State Approximation of Phrase-Structure Grammars. cmp-lg/9603002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darrell Raymond</author>
<author>Derick Wood</author>
</authors>
<title>The Grail Papers.</title>
<date>1996</date>
<tech>Technical Report TR-491.</tech>
<institution>University of Western Ontario, Department of Computer Science,</institution>
<contexts>
<context position="3345" citStr="Raymond and Wood, 1996" startWordPosition="489" endWordPosition="492">t-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as FIRE Lite (Watson, 1996), Grail (Raymond and Wood, 1996), and FSA Utilities (van Noord, 1996). Finite-state calculus has been successfully applied both to morphology (Kaplan and Kay, 1994; Kempe and Karttunen, 1996) and to syntax (constraint grammar, finite-state syntax). The work described here used a finite-state calculus implemented by the author in SICStus Prolog. 452 The use of Prolog rather than C or C++ causes large overheads in the memory and time required. However, careful account has been taken of the way Prolog operates, its indexing in particular, in order to ensure that the asymptotic complexity is as good as that of the best published</context>
</contexts>
<marker>Raymond, Wood, 1996</marker>
<rawString>Raymond, Darrell, and Derick Wood. March 1996. The Grail Papers. University of Western Ontario, Department of Computer Science, Technical Report TR-491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mori Rimon</author>
<author>Jacky Herz</author>
</authors>
<title>The recognition capacity of local syntactic constraints.</title>
<date>1991</date>
<booktitle>ACL Proceedings, 5th European Meeting.</booktitle>
<contexts>
<context position="19205" citStr="Rimon and Herz (1991" startWordPosition="3390" endWordPosition="3393"> appears that the approximations produced by the present algorithm tend to respect the necessity for certain constituents to be present, at whatever point in the string the symbols that &apos;trigger&apos; them appear, without necessarily insisting on their order, while Pereira and Wright&apos;s approximation tends to take greater account of the constituents whose appearance is triggered early on in the string: most of the complexity in Pereira and Wright&apos;s approximation of the 18-rule grammar is concerned with what is possible before the first accepting state is encountered. 7 Comparison with previous work Rimon and Herz (1991; 1991) approximate the recognition capacity of a context-free grammar by extracting &apos;local syntactic constraints&apos; in the form of the Left or Right Short Context of length n of a terminal. When n = 1 this reduces to next(t), the set of terminals that may follow the terminal t. The effect of filtering with Rimon and Herz&apos;s next(t) is similar to applying conditions 1-6 from section 3, but the use of auxiliary symbols causes two differences which can both be illustrated with the following grammar: S-4aXalbXb X On the one hand, Rimon and Herz&apos;s &apos;next&apos; does not distinguish between different instanc</context>
</contexts>
<marker>Rimon, Herz, 1991</marker>
<rawString>Rimon, Mori, and Jacky Herz. 1991. The recognition capacity of local syntactic constraints. ACL Proceedings, 5th European Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cathy Rood</author>
</authors>
<title>Efficient Finite-State Approximation of Context Free Grammars.</title>
<date>1996</date>
<booktitle>Proceedings of ECAI 96.</booktitle>
<contexts>
<context position="22275" citStr="Rood (1996)" startWordPosition="3886" endWordPosition="3887">((#sigma) *),[_/_/_]) , f). list_fsa(f). Figure 2: The sequence of calculations for approximating S —&gt; aSb I e, coded for the finite-state calculus. 4 \ U dan Figure 3: Finite-state approximations for the grammar in figure 1 calculated with the finite-state calculus (left) and by Pereira and Wright&apos;s algorithm (right). 457 ab is disallowed. Thus an approximation based on Rimon and Herz&apos;s &apos;next&apos; would be aa* + bb*, and an approximation based on conditions 1-6 would be (a+ b)(a+b). (However, the approximation becomes exact when conditions 7-8 are added.) Both Pereira and Wright (1991; 1996) and Rood (1996) start with the LR(0) characteristic machine, which they first &apos;unfold&apos; (with respect to &apos;stacks&apos; or &apos;paths&apos;, respectively) and then &apos;flatten&apos;. The characteristic machine is defined in terms of dotted rules with transitions between them that are analagous to the conditions implied by formula 3 of section 3. When the machine is flattened, 6-transitions are added in a way that is in effect simulated by conditions 2 and 4. (Condition 1 turns out to be implied by conditions 2-4.) It can be shown that the approximation Lo obtained by flattening the characteristic machine (without unfolding it) is a</context>
</contexts>
<marker>Rood, 1996</marker>
<rawString>Rood, Cathy. 1996. Efficient Finite-State Approximation of Context Free Grammars. Proceedings of ECAI 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-feature-based formalisms.</title>
<date>1985</date>
<booktitle>Proceedings of the 23nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="2690" citStr="Shieber, 1985" startWordPosition="400" endWordPosition="401"> grammar derived in this way can not in general recognise the same language as the more powerful grammar used for analysis, but, since it is being used as a front-end or filter, one would like it not to reject any string that is accepted by the analysis grammar, so we are primarily interested in &apos;sound approximations&apos; or &apos;approximations from above&apos;. Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language. Techniques such as restriction (Shieber, 1985) can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as F</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart. 1985. Using restriction to extend parsing algorithms for complex-feature-based formalisms. Proceedings of the 23nd Annual Meeting of the Association for Computational Linguistics, 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan Van Noord</author>
</authors>
<title>FSA Utilities: Manipulation of Finite-State Automata implemented in</title>
<date>1996</date>
<booktitle>Prolog. First International Workshop on Implementing Automata, University of</booktitle>
<pages>29--31</pages>
<location>Western Ontario, London Ontario,</location>
<marker>Van Noord, 1996</marker>
<rawString>Van Noord, Gertjan. 1996. FSA Utilities: Manipulation of Finite-State Automata implemented in Prolog. First International Workshop on Implementing Automata, University of Western Ontario, London Ontario, 29-31 August 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Watson</author>
</authors>
<title>Implementing and using finite automata toolkits.</title>
<date>1996</date>
<booktitle>Proccedings of ECAI 96.</booktitle>
<contexts>
<context position="3313" citStr="Watson, 1996" startWordPosition="486" endWordPosition="487">ed to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too. 2 Finite-state calculus A &apos;finite-state calculus&apos; or &apos;finite automata toolkit&apos; is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe. Standard operations include intersection, union, difference, determinisation and minimisation. Recently a number of automata toolkits have been made publicly available, such as FIRE Lite (Watson, 1996), Grail (Raymond and Wood, 1996), and FSA Utilities (van Noord, 1996). Finite-state calculus has been successfully applied both to morphology (Kaplan and Kay, 1994; Kempe and Karttunen, 1996) and to syntax (constraint grammar, finite-state syntax). The work described here used a finite-state calculus implemented by the author in SICStus Prolog. 452 The use of Prolog rather than C or C++ causes large overheads in the memory and time required. However, careful account has been taken of the way Prolog operates, its indexing in particular, in order to ensure that the asymptotic complexity is as go</context>
</contexts>
<marker>Watson, 1996</marker>
<rawString>Watson, Bruce. 1996. Implementing and using finite automata toolkits. Proccedings of ECAI 96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>