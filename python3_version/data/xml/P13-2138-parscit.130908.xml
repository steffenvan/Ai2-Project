<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000698">
<title confidence="0.996834">
Generalizing Image Captions for Image-Text Parallel Corpus
</title>
<author confidence="0.979892">
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg and Yejin Choi
</author>
<affiliation confidence="0.9572415">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.928398">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.999279">
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998952173913">
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.972049947368421">
The vast number of online images with accom-
panying text raises hope for drawing synergistic
connections between human language technolo-
gies and computer vision. However, subtleties and
complexity in the relationship between image con-
tent and text make exploiting paired visual-textual
data an open and interesting problem.
Some recent work has approached the prob-
lem of composing natural language descriptions
for images by using computer vision to retrieve
images with similar content and then transferring
Circumstantial Visually relevant, Visually truthful,
information that is not but with overly but for an uncommon
visually present extraneous details situation
“I saw her in the light “Sections of the “A house being
of her reading lamp bridge sitting in the pulled by a boat.”
and sneaked back to Dyer Construction
her door with the yard south of
camera.” Cabelas Driver.”
</bodyText>
<figureCaption confidence="0.986977">
Figure 1: Examples of captions that are not readily
applicable to other visually similar images.
</figureCaption>
<bodyText confidence="0.99987484">
text from the retrieved samples to the query im-
age (e.g. Farhadi et al. (2010), Ordonez et al.
(2011), Kuznetsova et al. (2012)). Other work
(e.g. Feng and Lapata (2010a), Feng and Lapata
(2010b)) uses computer vision to bias summariza-
tion of text associated with images to produce de-
scriptions. All of these approaches rely on ex-
isting text that describes visual content, but many
times existing image descriptions contain signifi-
cant amounts of extraneous, non-visual, or other-
wise non-desirable content. The goal of this paper
is to develop techniques to automatically clean up
visually descriptive text to make it more directly
usable for applications exploiting the connection
between images and language.
As a concrete example, consider the first image
in Figure 1. This caption was written by the photo
owner and therefore contains information related
to the context of when and where the photo was
taken. Objects such as “lamp”, “door”, “camera”
are not visually present in the photo. The second
image shows a similar but somewhat different is-
sue. Its caption describes visible objects such as
“bridge” and “yard”, but “Cabelas Driver” are
overly specific and not visually detectable. The
</bodyText>
<page confidence="0.950728">
790
</page>
<note confidence="0.5362315">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790–796,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.6612125">
Dependency Constraints with Examples Additional Dependency Constraints
Constraints Sentence Dependency
advcl*(+-) Taken when it was running... taken+-running acomp*(++), advmod(+-), agent*(+-), attr(++)
amod(+-) A wooden chair in the living room chair+- wooden auxpass(++), cc*(++),complm(+-), cop*(++)
aux(++) This crazy dog was jumping... jumping++was csubj*/csubjpass*(++),expl(++), mark*(++)
ccomp*(-+) I believe a bear was in the box... believe-+was infmod*(++), mwe(++), nsubj*/nsubjpass*(++)
prep(+-) A view from the balcony view+-from npadvmod(+-), nn(+-), conj*(++), num*(+-)
det(++) A cozy street cafe... cafe++A number(++), parataxis(+-), ++
dobj*(++) A curious cow surveys the road... surveys++road partmod*(+-), pcomp*(++), purpcl*(+-)
iobj*(++) ...rock gives the water the color gives++water possessive(++), preconj*(+-), predet*(+-)
neg(++) Not a cloud in the sky... cloud++Not prt(++), quantmod(+-), rcmod(+-), ref(+-)
pobj*(++) This branch was on the ground... on++ground rel*(++), tmod*(+-), xcomp*(-+), xsubj(-+)
</table>
<tableCaption confidence="0.994366">
Table 1: Dependency-based Constraints
</tableCaption>
<bodyText confidence="0.999988565217391">
text of the third image, “A house being pulled by a
boat”, pertains directly to the visual content of the
image, but is unlikely to be useful for tasks such as
caption transfer because the depiction is unusual.1
This phenomenon of information gap between the
visual content of the images and their correspond-
ing narratives has been studied closely by Dodge
et al. (2012).
The content misalignment between images and
text limits the extent to which visual detectors
can learn meaningful mappings between images
and text. To tackle this challenge, we introduce
the new task of image caption generalization that
rewrites captions to be more visually relevant and
more readily applicable to other visually similar
images. Our end goal is to convert noisy image-
text pairs in the wild (Ordonez et al., 2011) into
pairs with tighter content alignment, resulting in
new simplified captions over 1 million images.
Evaluation results show both the intrinsic quality
of the generalized captions and the extrinsic util-
ity of the new image-text parallel corpus. The new
parallel corpus will be made publicly available.2
</bodyText>
<sectionHeader confidence="0.9207595" genericHeader="general terms">
2 Sentence Generalization as Constraint
Optimization
</sectionHeader>
<bodyText confidence="0.999860285714286">
Casting the generalization task as visually-guided
sentence compression with lightweight revisions,
we formulate a constraint optimization problem
that aims to maximize content selection and lo-
cal linguistic fluency while satisfying constraints
driven from dependency parse trees. Dependency-
based constraints guide the generalized caption
</bodyText>
<footnote confidence="0.734405">
1Open domain computer vision remains to be an open
problem, and it would be difficult to reliably distinguish pic-
tures of subtle visual differences, e.g., pictures of “a water
front house with a docked boat” from those of “a floating
house pulled by a boat”.
2Available at http://www.cs.stonybrook.edu/
˜ychoi/imgcaption/
</footnote>
<bodyText confidence="0.999092166666667">
to be grammatically valid (e.g., keeping articles
in place, preventing dangling modifiers) while re-
maining semantically compatible with respect to a
given image-text pair (e.g., preserving predicate-
argument relations). More formally, we maximize
the following objective function:
</bodyText>
<equation confidence="0.9013955">
F(y; x) = Φ(y; x, v) + IF(y; x)
subject to C(y; x, v)
</equation>
<bodyText confidence="0.995793375">
where x = {xi} is the input caption (a sentence),
v is the accompanying image, y = {yi} is the
output sentence, Φ(y; x, v) is the content selection
score, IF(y; x) is the linguistic fluency score, and
C(y; x, v) is the set of hard constraints. Let l(yi)
be the index of the word in x that is selected as the
i’th word in the output y so that xl(yi) = yi. Then,
we factorize Φ(·) and IF(·) as:
</bodyText>
<equation confidence="0.992887">
Φ(y; x, v) = E Eφ(yi, x, v) = φ(xl(yi), v)
i i
IF(y; x) = E ψ(yi, ..., yi−K)
i
E= ψ(xl(yi), ..., xl(yi−x))
i
</equation>
<bodyText confidence="0.958546">
where K is the size of local context.
</bodyText>
<subsectionHeader confidence="0.985005">
Content Selection – Visual Estimates:
</subsectionHeader>
<bodyText confidence="0.999580818181818">
The computer vision system used consists of 7404
visual classifiers for recognizing leaf level Word-
Net synsets (Fellbaum, 1998). Each classifier is
trained using labeled images from the ImageNet
dataset (Deng et al., 2009) – an image database
of over 14 million hand labeled images orga-
nized according to the WordNet hierarchy. Image
similarity is represented using a Spatial Pyramid
Match Kernel (SPM) (Lazebnik et al., 2006) with
Locality-constrained Linear Coding (Wang et al.,
2010) on shape based SIFT features (Lowe, 2004).
</bodyText>
<page confidence="0.970309">
791
</page>
<figure confidence="0.995021428571429">
(a) (b)
sentences (in thousands)
1200
400
800
0
0 1 2 3
sentences (in thousands)
400
800
600
200
0
0 1 2 3 4 5 6 7
</figure>
<table confidence="0.988480875">
Method-1 (M1) v.s. Method-2 (M2) M1 wins
over M2
SALIENCY ORIG 76.34%
VISUAL ORIG 81.75%
VISUAL SALIENCY 72.48%
VISUAL VISUAL W/O CONSTR 83.76%
VISUAL NGRAM-ONLY 90.20%
VISUAL HUMAN 19.00%
</table>
<figureCaption confidence="0.993658">
Figure 2: Number of sentences (y-axis) for each
average (x-axis in (a)) and maximum (x-axis in
(b)) number of words with future dependencies
</figureCaption>
<bodyText confidence="0.984069">
Models are linear SVMs followed by a sigmoid to
produce probability for each node.3
</bodyText>
<subsectionHeader confidence="0.958352">
Content Selection – Salient Topics:
</subsectionHeader>
<bodyText confidence="0.999730857142857">
We consider Tf.Idf driven scores to favor salient
topics, as those are more likely to generalize
across many different images. Additionally, we
assign a very low content selection score (−oc) for
proper nouns and numbers and a very high score
(larger then maximum idf or visual score) for the
2k most frequent words in our corpus.
</bodyText>
<subsectionHeader confidence="0.922011">
Local Linguistic Fluency:
</subsectionHeader>
<bodyText confidence="0.9995205">
We model linguistic fluency with 3-gram condi-
tional probabilities:
</bodyText>
<equation confidence="0.9470005">
V(xl(yi), xl(yi−1), xl(yi−2)) (1)
= p(xl(yi)|xl(yi−2), xl(yi−1))
</equation>
<bodyText confidence="0.9998606">
We experiment with two different ngram statis-
tics, one extracted from the Google Web 1T cor-
pus (Brants and Franz., 2006), and the other com-
puted from the 1M image-caption corpus (Or-
donez et al., 2011).
</bodyText>
<subsectionHeader confidence="0.559283">
Dependency-driven Constraints:
</subsectionHeader>
<bodyText confidence="0.999887928571429">
Table 1 defines the list of dependencies used
as constraints driven from the typed dependen-
cies (de Marneffe and Manning, 2009; de Marn-
effe et al., 2006). The direction of arrows indi-
cate the direction of inclusion requirements. For
example, dep(X +− Y ), denotes that “X” must
be included whenever “Y ” is included. Similarly,
dep(X +� Y ) denotes that “X” and “Y ” must
either be included together or eliminated together.
We determine the uni- or bi-directionality of these
constraints by manually examining a few example
sentences corresponding to each of these typed de-
pendencies. Note that some dependencies such as
det(+�) would hold regardless of the particular
</bodyText>
<footnote confidence="0.597406">
3Code was provided by Deng et al. (2012).
</footnote>
<tableCaption confidence="0.8895735">
Table 2: Forced Choice Evaluation (LM Corpus =
Google)
</tableCaption>
<bodyText confidence="0.992786105263158">
lexical items, while others, e.g., dobj(+�) may
or may not be necessary depending on the context.
Those dependencies that we determine as largely
context dependent are marked with * in Table 1.
One could consider enforcing all dependency
constraints in Table 1 as hard constraints so that
the compressed sentence must not violate any of
those directed dependency constraints. Doing so
would lead to overly conservative compression
with least compression ratio however. Therefore,
we relax those that are largely context dependent
as soft constraints (marked in Table 1 with *) by
introducing a constant penalty term in the objec-
tive function. Alternatively, the dependency based
constraints can be learned statistically from the
training corpus of paired original and compressed
sentences. Since we do not have such in-domain
training data at this time, we leave this exploration
as future research.
</bodyText>
<subsectionHeader confidence="0.930425">
Dynamic Programming with Dynamic Beam:
</subsectionHeader>
<bodyText confidence="0.999652727272727">
The constraint optimization we formulated corre-
sponds to an NP-hard problem. In our work, hard
constraints are based only on typed dependencies,
and we find that long range dependencies occur in-
frequently in actual image descriptions, as plotted
in Figure 2. With this insight, we opt for decoding
based on dynamic programming with dynamically
adjusted beam.4 Alternatively, one can find an ap-
proximate solution using Integer Linear Program-
ming (e.g., Clarke and Lapata (2006), Clarke and
Lapata (2007), Martins and Smith (2009)).
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99708275">
Since there is no existing benchmark data for im-
age caption generalization, we crowdsource evalu-
ation using Amazon Mechanical Turk (AMT). We
empirically compare the following options:
</bodyText>
<footnote confidence="0.9970125">
4The required beam size at each step depends on how
many words have dependency constraints involving any word
following the current one – beam size is at most 2p, where P
is the max number of words dependent on any future words.
</footnote>
<page confidence="0.967303">
792
</page>
<table confidence="0.946307">
Query Image Retrieved Images
Big elm tree over Abandonned A woman paints a tree in Pillbox in field Flowering tree in The insulbrick matches
the house is no houses in the bloom near the duck pond behind a pub mixed forest at the yard. This is outside
their anymore. forest. in the Boston Public car park. Wakehurst. of medina ohio near the
Garden, April 15, 2006. tonka truck house.
 Tree over the house.  Houses in the  A tree in bloom .  Pub car.  Flowering tree  The yard. This is
forest. in forest. outside the house.
Figure 3: Example Image Caption Transfer
Method LM strict matching F semantic matching F
Corpus BLEU P R BLEU P R
ORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276
SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356
VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360
SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340
VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350
</table>
<tableCaption confidence="0.997462">
Table 3: Image Description Transfer: performance in BLEU and F1 with strict &amp; semantic matching.
</tableCaption>
<listItem confidence="0.999886222222222">
• ORIG: original uncompressed captions
• HUMAN: compressed by humans (See § 3.2)
• SALIENCY: linguistic fluency + saliency-based
content selection + dependency constraints
• VISUAL: linguistic fluency + visually-guided
content selection + dependency constraints
• x W/O CONSTR: method x without dependency
constraints
• NGRAM-ONLY: linguistic fluency only
</listItem>
<subsectionHeader confidence="0.948122">
3.1 Intrinsic Evaluation: Forced Choice
</subsectionHeader>
<bodyText confidence="0.999966846153846">
Turkers are provided with an image and two cap-
tions (produced by different methods) and are
asked to select a better one, i.e., the most relevant
and plausible caption that contains the least extra-
neous information. Results are shown in Table 2.
We observe that VISUAL (full model with visually
guided content selection) performs the best, being
selected over SALIENCY (content-selection with-
out visual information) in 72.48% cases, and even
over the original image caption in 81.75% cases.
This forced-selection experiment between VI-
SUAL and ORIG demonstrates the degree of noise
prevalent in the image captions in the wild. Of
course, if compared against human-compressed
captions, the automatic captions are preferred
much less frequently – in 19% of the cases. In
those 19% cases when automatic captions are pre-
ferred over human-compressed ones, it is some-
times that humans did not fully remove informa-
tion that is not visually present or verifiable, and
other times humans overly compressed. To ver-
ify the utility of dependency-based constraints,
we also compare two variations of VISUAL, with
and without dependency-based constraints. As ex-
pected, the algorithm with constraints is preferred
in the majority of cases.
</bodyText>
<subsectionHeader confidence="0.9931905">
3.2 Extrinsic Evaluation: Image-based
Caption Retrieval
</subsectionHeader>
<bodyText confidence="0.999956291666667">
We evaluate the usefulness of our new image-text
parallel corpus for automatic generation of image
descriptions. Here the task is to produce, for a
query image, a relevant description, i.e., a visu-
ally descriptive caption. Following Ordonez et al.
(2011), we produce a caption for a query image
by finding top k most similar images within the
1M image-text corpus (Ordonez et al., 2011) and
then transferring their captions to the query im-
age. To compute evaluation measures, we take the
average scores of BLEU(1) and F-score (unigram-
based with respect to content-words) over k = 5
candidate captions.
Image similarity is computed using two global
(whole) image descriptors. The first is the GIST
feature (Oliva and Torralba, 2001), an image de-
scriptor related to perceptual characteristics of
scenes – naturalness, roughness, openness, etc.
The second descriptor is also a global image de-
scriptor, computed by resizing the image into a
“tiny image” (Torralba et al., 2008), which is ef-
fective in matching the structure and overall color
of images. To find visually relevant images, we
compute the similarity of the query image to im-
</bodyText>
<page confidence="0.996656">
793
</page>
<table confidence="0.418612">
Huge wall of glass A view of the post office My footprint in a James the cat is This little boy was so Cell phone shot of
at the Conference building in Manila from sand box dreaming of running cute. He was flying his a hat stall in the
Centre in the other side of the in a wide green spiderman kite all by Northeast Market,
Yohohama Japan. Pasig River valley himself on top of Max Baltimore, MD.
Patch
4 Wall of glass 4 A view of the post 4 A sand box 4 Running in 4 This little boy was so 4 Cell phone shot.
office building from a valley (not cute. He was flying (visually not
the side relevant) (semantically odd) verifiable)
</table>
<figureCaption confidence="0.99871">
Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captions
</figureCaption>
<bodyText confidence="0.996559083333334">
ages in the whole dataset using an unweighted sum
of gist similarity and tiny image similarity.
Gold standard (human compressed) captions are
obtained using AMT for 1K images. The results
are shown in Table 3. Strict matching gives credit
only to identical words between the gold-standard
caption and the automatically produced caption.
However, words in the original caption of the
query image (and its compressed caption) do not
overlap exactly with words in the retrieved cap-
tions, even when they are semantically very close,
which makes it hard to see improvements even
when the captions of the new corpus are more gen-
eral and transferable over other images. Therefore,
we also report scores based on semantic matching,
which gives partial credits to word pairs based on
their lexical similarity.5 The best performing ap-
proach with semantic matching is VISUAL (with
LM = Image corpus), improving BLEU, Precision,
F-score substantially over those of ORIG, demon-
strating the extrinsic utility of our newly gener-
ated image-text parallel corpus in comparison to
the original database. Figure 3 shows an example
of caption transfer.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999854">
Several recent studies presented approaches to
automatic caption generation for images (e.g.,
Farhadi et al. (2010), Feng and Lapata (2010a),
Feng and Lapata (2010b), Yang et al. (2011),
Kulkarni et al. (2011), Li et al. (2011), Kuznetsova
et al. (2012)). The end goal of our work differs in
that we aim to revise original image captions into
</bodyText>
<footnote confidence="0.957358666666667">
5We take Wu-Palmer Similarity as similarity mea-
sure (Wu and Palmer, 1994). When computing BLEU with
semantic matching, we look for the match with the highest
similarity score among words that have not been matched be-
fore. Any word matched once (even with a partial credit) will
be removed from consideration when matching next words.
</footnote>
<bodyText confidence="0.9992338125">
descriptions that are more general and align more
closely to the visual image content.
In comparison to prior work on sentence com-
pression, our approach falls somewhere between
unsupervised to distant-supervised approach (e.g.,
Turner and Charniak (2005), Filippova and Strube
(2008)) in that there is not an in-domain train-
ing corpus to learn generalization patterns directly.
Future work includes exploring more direct su-
pervision from human edited sample generaliza-
tion (e.g., Knight and Marcu (2000), McDonald
(2006)) Galley and McKeown (2007), Zhu et al.
(2010)), and the inclusion of edits beyond dele-
tion, e.g., substitutions, as has been explored by
e.g., Cohn and Lapata (2008), Cordeiro et al.
(2009), Napoles et al. (2011).
</bodyText>
<sectionHeader confidence="0.994532" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999965625">
We have introduced the task of image caption gen-
eralization as a means to reduce noise in the paral-
lel corpus of images and text. Intrinsic and extrin-
sic evaluations confirm that the captions in the re-
sulting corpus align better with the image contents
(are often preferred over the original captions by
people), and can be practically more useful with
respect to a concrete application.
</bodyText>
<sectionHeader confidence="0.989771" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990991666666667">
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. Additionally, Tamara Berg is supported
by NSF #1054133 and NSF #1161876. We thank
reviewers for many insightful comments and sug-
gestions.
</bodyText>
<page confidence="0.997636">
794
</page>
<sectionHeader confidence="0.959587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999033770642202">
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144–151, Sydney, Australia, July. Association
for Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1–11, Prague, Czech Republic, June.
Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137–144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009.
Unsupervised induction of sentence compression
rules. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 15–22, Suntec, Singapore,
August. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2009. Stanford typed dependencies manual.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Language Resources and Evaluation Conference
2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hi-
erarchical Image Database. In Conference on Com-
puter Vision and Pattern Recognition.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012. Hedging your bets: Optimizing
accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daume III, Alex Berg, and
Tamara Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762–772, Montr´eal, Canada, June. Association for
Computational Linguistics.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young1, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
In European Conference on Computer Vision.
Christiane D. Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Hu-
man Language Technologies.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ’08, pages 25–32,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180–187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI/IAAI, pages 703–710.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and gener-
ating simple image descriptions. In Conference on
Computer Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gen-
eration of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 359–368, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching. In Conference on Computer Vision and
Pattern Recognition, June.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220–
228, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
60:91–110, November.
</reference>
<page confidence="0.9808">
795
</page>
<reference confidence="0.999819777777778">
Andre Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on
Integer Linear Programming for Natural Language
Processing, pages 1–9, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL
2006, 11st Conference of the European Chapter of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, April 3-7, 2006, Trento,
Italy. The Association for Computer Linguistics.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84–90, Portland, Oregon, June.
Association for Computational Linguistics.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: a holistic representation of the
spatial envelope. International Journal of Computer
Vision.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Antonio Torralba, Rob Fergus, and William T. Free-
man. 2008. 80 million tiny images: a large dataset
for non-parametric object and scene recognition.
Pattern Analysis and Machine Intelligence, 30.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 290–297, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,
T. Huang, and Yihong Gong. 2010. Locality-
constrained linear coding for image classification.
In Conference on Computer Vision and Pattern
Recognition (CVPR).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ’94, pages 133–138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444–454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353–1361, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
</reference>
<page confidence="0.998546">
796
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816544">
<title confidence="0.999937">Generalizing Image Captions for Image-Text Parallel Corpus</title>
<author confidence="0.9685715">Polina Kuznetsova</author>
<author confidence="0.9685715">Vicente Ordonez</author>
<author confidence="0.9685715">Alexander Tamara Berg</author>
<author confidence="0.9685715">Yejin</author>
<affiliation confidence="0.993414">Department of Computer</affiliation>
<author confidence="0.941084">Stony Brook Stony Brook</author>
<author confidence="0.941084">NY</author>
<abstract confidence="0.998774958333333">The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. However, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. We address this challenge with contributions in two folds: first, we introduce the new of caption formulated as visually-guided sentence compression, and present an efficient algorithm based on dynamic beam search with dependency-based constraints. Second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1.</title>
<date>2006</date>
<booktitle>In Linguistic Data Consortium.</booktitle>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. In Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Constraintbased sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="11422" citStr="Clarke and Lapata (2006)" startWordPosition="1758" endWordPosition="1761">ince we do not have such in-domain training data at this time, we leave this exploration as future research. Dynamic Programming with Dynamic Beam: The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). 3 Evaluation Since there is no existing benchmark data for image caption generalization, we crowdsource evaluation using Amazon Mechanical Turk (AMT). We empirically compare the following options: 4The required beam size at each step depends on how many words have dependency constraints involving any word following the current one – beam size is at most 2p, where P is the max number of words dependent on any future words. 792 Query Image Retrieved Images Big elm tree over Abandonned A woman paints a tree in Pillbox in field Flowering tree </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Constraintbased sentence compression: An integer programming approach. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144–151, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11448" citStr="Clarke and Lapata (2007)" startWordPosition="1762" endWordPosition="1765">n-domain training data at this time, we leave this exploration as future research. Dynamic Programming with Dynamic Beam: The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). 3 Evaluation Since there is no existing benchmark data for image caption generalization, we crowdsource evaluation using Amazon Mechanical Turk (AMT). We empirically compare the following options: 4The required beam size at each step depends on how many words have dependency constraints involving any word following the current one – beam size is at most 2p, where P is the max number of words dependent on any future words. 792 Query Image Retrieved Images Big elm tree over Abandonned A woman paints a tree in Pillbox in field Flowering tree in The insulbrick matches </context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1–11, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>137--144</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="19032" citStr="Cohn and Lapata (2008)" startWordPosition="3008" endWordPosition="3011"> visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research. Additionally, Tamara Berg</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137–144, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Cordeiro</author>
<author>Gael Dias</author>
<author>Pavel Brazdil</author>
</authors>
<title>Unsupervised induction of sentence compression rules.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum</booktitle>
<pages>15--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="19056" citStr="Cordeiro et al. (2009)" startWordPosition="3012" endWordPosition="3015">n comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research. Additionally, Tamara Berg is supported by NSF #10</context>
</contexts>
<marker>Cordeiro, Dias, Brazdil, 2009</marker>
<rawString>Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009. Unsupervised induction of sentence compression rules. In Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum 2009), pages 15–22, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2009</date>
<note>Stanford typed dependencies manual.</note>
<marker>de Marneffe, Manning, 2009</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2009. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Language Resources and Evaluation Conference</booktitle>
<marker>de Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Language Resources and Evaluation Conference 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet: A Large-Scale Hierarchical Image Database.</title>
<date>2009</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="7642" citStr="Deng et al., 2009" startWordPosition="1150" endWordPosition="1153">fluency score, and C(y; x, v) is the set of hard constraints. Let l(yi) be the index of the word in x that is selected as the i’th word in the output y so that xl(yi) = yi. Then, we factorize Φ(·) and IF(·) as: Φ(y; x, v) = E Eφ(yi, x, v) = φ(xl(yi), v) i i IF(y; x) = E ψ(yi, ..., yi−K) i E= ψ(xl(yi), ..., xl(yi−x)) i where K is the size of local context. Content Selection – Visual Estimates: The computer vision system used consists of 7404 visual classifiers for recognizing leaf level WordNet synsets (Fellbaum, 1998). Each classifier is trained using labeled images from the ImageNet dataset (Deng et al., 2009) – an image database of over 14 million hand labeled images organized according to the WordNet hierarchy. Image similarity is represented using a Spatial Pyramid Match Kernel (SPM) (Lazebnik et al., 2006) with Locality-constrained Linear Coding (Wang et al., 2010) on shape based SIFT features (Lowe, 2004). 791 (a) (b) sentences (in thousands) 1200 400 800 0 0 1 2 3 sentences (in thousands) 400 800 600 200 0 0 1 2 3 4 5 6 7 Method-1 (M1) v.s. Method-2 (M2) M1 wins over M2 SALIENCY ORIG 76.34% VISUAL ORIG 81.75% VISUAL SALIENCY 72.48% VISUAL VISUAL W/O CONSTR 83.76% VISUAL NGRAM-ONLY 90.20% VISU</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Jonathan Krause</author>
<author>Alexander C Berg</author>
<author>L Fei-Fei</author>
</authors>
<title>Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition.</title>
<date>2012</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="9950" citStr="Deng et al. (2012)" startWordPosition="1531" endWordPosition="1534">cies (de Marneffe and Manning, 2009; de Marneffe et al., 2006). The direction of arrows indicate the direction of inclusion requirements. For example, dep(X +− Y ), denotes that “X” must be included whenever “Y ” is included. Similarly, dep(X +� Y ) denotes that “X” and “Y ” must either be included together or eliminated together. We determine the uni- or bi-directionality of these constraints by manually examining a few example sentences corresponding to each of these typed dependencies. Note that some dependencies such as det(+�) would hold regardless of the particular 3Code was provided by Deng et al. (2012). Table 2: Forced Choice Evaluation (LM Corpus = Google) lexical items, while others, e.g., dobj(+�) may or may not be necessary depending on the context. Those dependencies that we determine as largely context dependent are marked with * in Table 1. One could consider enforcing all dependency constraints in Table 1 as hard constraints so that the compressed sentence must not violate any of those directed dependency constraints. Doing so would lead to overly conservative compression with least compression ratio however. Therefore, we relax those that are largely context dependent as soft const</context>
</contexts>
<marker>Deng, Krause, Berg, Fei-Fei, 2012</marker>
<rawString>Jia Deng, Jonathan Krause, Alexander C. Berg, and L. Fei-Fei. 2012. Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition. In Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Karl Stratos</author>
<author>Kota Yamaguchi</author>
<author>Yejin Choi</author>
<author>Hal Daume Alex Berg</author>
<author>Tamara Berg</author>
</authors>
<title>Detecting visual text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>762--772</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5057" citStr="Dodge et al. (2012)" startWordPosition="733" endWordPosition="736"> preconj*(+-), predet*(+-) neg(++) Not a cloud in the sky... cloud++Not prt(++), quantmod(+-), rcmod(+-), ref(+-) pobj*(++) This branch was on the ground... on++ground rel*(++), tmod*(+-), xcomp*(-+), xsubj(-+) Table 1: Dependency-based Constraints text of the third image, “A house being pulled by a boat”, pertains directly to the visual content of the image, but is unlikely to be useful for tasks such as caption transfer because the depiction is unusual.1 This phenomenon of information gap between the visual content of the images and their corresponding narratives has been studied closely by Dodge et al. (2012). The content misalignment between images and text limits the extent to which visual detectors can learn meaningful mappings between images and text. To tackle this challenge, we introduce the new task of image caption generalization that rewrites captions to be more visually relevant and more readily applicable to other visually similar images. Our end goal is to convert noisy imagetext pairs in the wild (Ordonez et al., 2011) into pairs with tighter content alignment, resulting in new simplified captions over 1 million images. Evaluation results show both the intrinsic quality of the general</context>
</contexts>
<marker>Dodge, Goyal, Han, Mensch, Mitchell, Stratos, Yamaguchi, Choi, Berg, Berg, 2012</marker>
<rawString>Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin Choi, Hal Daume III, Alex Berg, and Tamara Berg. 2012. Detecting visual text. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762–772, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young1</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences for images.</title>
<date>2010</date>
<booktitle>In European Conference on Computer Vision.</booktitle>
<marker>Farhadi, Hejrati, Sadeghi, Young1, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young1, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences for images. In European Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane D Fellbaum</author>
<author>editor</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<marker>Fellbaum, editor, 1998</marker>
<rawString>Christiane D. Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images. In Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="2397" citStr="Feng and Lapata (2010" startWordPosition="356" endWordPosition="359">isually relevant, Visually truthful, information that is not but with overly but for an uncommon visually present extraneous details situation “I saw her in the light “Sections of the “A house being of her reading lamp bridge sitting in the pulled by a boat.” and sneaked back to Dyer Construction her door with the yard south of camera.” Cabelas Driver.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and language. As a concrete example, consider the first image in Figure</context>
<context position="17807" citStr="Feng and Lapata (2010" startWordPosition="2811" endWordPosition="2814">herefore, we also report scores based on semantic matching, which gives partial credits to word pairs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? automatic caption generation for news images. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration. In Human Language Technologies.</title>
<date>2010</date>
<contexts>
<context position="2397" citStr="Feng and Lapata (2010" startWordPosition="356" endWordPosition="359">isually relevant, Visually truthful, information that is not but with overly but for an uncommon visually present extraneous details situation “I saw her in the light “Sections of the “A house being of her reading lamp bridge sitting in the pulled by a boat.” and sneaked back to Dyer Construction her door with the yard south of camera.” Cabelas Driver.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and language. As a concrete example, consider the first image in Figure</context>
<context position="17807" citStr="Feng and Lapata (2010" startWordPosition="2811" endWordPosition="2814">herefore, we also report scores based on semantic matching, which gives partial credits to word pairs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18629" citStr="Filippova and Strube (2008)" startWordPosition="2944" endWordPosition="2947">ions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intri</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Dependency tree based sentence compression. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>180--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="18893" citStr="Galley and McKeown (2007)" startWordPosition="2984" endWordPosition="2987"> partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Acknowledgm</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 180–187, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="18849" citStr="Knight and Marcu (2000)" startWordPosition="2978" endWordPosition="2981">before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with re</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In AAAI/IAAI, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Babytalk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="17878" citStr="Kulkarni et al. (2011)" startWordPosition="2823" endWordPosition="2826">s partial credits to word pairs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compr</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Babytalk: Understanding and generating simple image descriptions. In Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander Berg</author>
<author>Tamara Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>359--368</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2356" citStr="Kuznetsova et al. (2012)" startWordPosition="349" endWordPosition="352">ntent and then transferring Circumstantial Visually relevant, Visually truthful, information that is not but with overly but for an uncommon visually present extraneous details situation “I saw her in the light “Sections of the “A house being of her reading lamp bridge sitting in the pulled by a boat.” and sneaked back to Dyer Construction her door with the yard south of camera.” Cabelas Driver.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and language. As a concrete ex</context>
<context position="17922" citStr="Kuznetsova et al. (2012)" startWordPosition="2831" endWordPosition="2834">heir lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander Berg, Tamara Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 359–368, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Lazebnik</author>
<author>Cordelia Schmid</author>
<author>Jean Ponce</author>
</authors>
<title>Beyond bags of features: Spatial pyramid matching.</title>
<date>2006</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition,</booktitle>
<contexts>
<context position="7846" citStr="Lazebnik et al., 2006" startWordPosition="1183" endWordPosition="1186"> IF(·) as: Φ(y; x, v) = E Eφ(yi, x, v) = φ(xl(yi), v) i i IF(y; x) = E ψ(yi, ..., yi−K) i E= ψ(xl(yi), ..., xl(yi−x)) i where K is the size of local context. Content Selection – Visual Estimates: The computer vision system used consists of 7404 visual classifiers for recognizing leaf level WordNet synsets (Fellbaum, 1998). Each classifier is trained using labeled images from the ImageNet dataset (Deng et al., 2009) – an image database of over 14 million hand labeled images organized according to the WordNet hierarchy. Image similarity is represented using a Spatial Pyramid Match Kernel (SPM) (Lazebnik et al., 2006) with Locality-constrained Linear Coding (Wang et al., 2010) on shape based SIFT features (Lowe, 2004). 791 (a) (b) sentences (in thousands) 1200 400 800 0 0 1 2 3 sentences (in thousands) 400 800 600 200 0 0 1 2 3 4 5 6 7 Method-1 (M1) v.s. Method-2 (M2) M1 wins over M2 SALIENCY ORIG 76.34% VISUAL ORIG 81.75% VISUAL SALIENCY 72.48% VISUAL VISUAL W/O CONSTR 83.76% VISUAL NGRAM-ONLY 90.20% VISUAL HUMAN 19.00% Figure 2: Number of sentences (y-axis) for each average (x-axis in (a)) and maximum (x-axis in (b)) number of words with future dependencies Models are linear SVMs followed by a sigmoid to</context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching. In Conference on Computer Vision and Pattern Recognition, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="17896" citStr="Li et al. (2011)" startWordPosition="2827" endWordPosition="2830">d pairs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approa</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220– 228, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>Int. J. Comput. Vision,</journal>
<pages>60--91</pages>
<contexts>
<context position="7948" citStr="Lowe, 2004" startWordPosition="1200" endWordPosition="1201">−x)) i where K is the size of local context. Content Selection – Visual Estimates: The computer vision system used consists of 7404 visual classifiers for recognizing leaf level WordNet synsets (Fellbaum, 1998). Each classifier is trained using labeled images from the ImageNet dataset (Deng et al., 2009) – an image database of over 14 million hand labeled images organized according to the WordNet hierarchy. Image similarity is represented using a Spatial Pyramid Match Kernel (SPM) (Lazebnik et al., 2006) with Locality-constrained Linear Coding (Wang et al., 2010) on shape based SIFT features (Lowe, 2004). 791 (a) (b) sentences (in thousands) 1200 400 800 0 0 1 2 3 sentences (in thousands) 400 800 600 200 0 0 1 2 3 4 5 6 7 Method-1 (M1) v.s. Method-2 (M2) M1 wins over M2 SALIENCY ORIG 76.34% VISUAL ORIG 81.75% VISUAL SALIENCY 72.48% VISUAL VISUAL W/O CONSTR 83.76% VISUAL NGRAM-ONLY 90.20% VISUAL HUMAN 19.00% Figure 2: Number of sentences (y-axis) for each average (x-axis in (a)) and maximum (x-axis in (b)) number of words with future dependencies Models are linear SVMs followed by a sigmoid to produce probability for each node.3 Content Selection – Salient Topics: We consider Tf.Idf driven sco</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision, 60:91–110, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="11474" citStr="Martins and Smith (2009)" startWordPosition="1766" endWordPosition="1769">this time, we leave this exploration as future research. Dynamic Programming with Dynamic Beam: The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). 3 Evaluation Since there is no existing benchmark data for image caption generalization, we crowdsource evaluation using Amazon Mechanical Turk (AMT). We empirically compare the following options: 4The required beam size at each step depends on how many words have dependency constraints involving any word following the current one – beam size is at most 2p, where P is the max number of words dependent on any future words. 792 Query Image Retrieved Images Big elm tree over Abandonned A woman paints a tree in Pillbox in field Flowering tree in The insulbrick matches the house is no houses in </context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andre Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 1–9, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<institution>Trento, Italy. The Association for Computer Linguistics.</institution>
<contexts>
<context position="18866" citStr="McDonald (2006)" startWordPosition="2982" endWordPosition="2983">once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concre</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan T. McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, April 3-7, 2006, Trento, Italy. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Chris Callison-Burch</author>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Paraphrastic sentence compression with a character-based metric: Tightening without deletion.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>84--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>Napoles, Callison-Burch, Ganitkevitch, Van Durme, 2011</marker>
<rawString>Courtney Napoles, Chris Callison-Burch, Juri Ganitkevitch, and Benjamin Van Durme. 2011. Paraphrastic sentence compression with a character-based metric: Tightening without deletion. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 84–90, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: a holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision.</journal>
<contexts>
<context position="15388" citStr="Oliva and Torralba, 2001" startWordPosition="2400" endWordPosition="2403">tions. Here the task is to produce, for a query image, a relevant description, i.e., a visually descriptive caption. Following Ordonez et al. (2011), we produce a caption for a query image by finding top k most similar images within the 1M image-text corpus (Ordonez et al., 2011) and then transferring their captions to the query image. To compute evaluation measures, we take the average scores of BLEU(1) and F-score (unigrambased with respect to content-words) over k = 5 candidate captions. Image similarity is computed using two global (whole) image descriptors. The first is the GIST feature (Oliva and Torralba, 2001), an image descriptor related to perceptual characteristics of scenes – naturalness, roughness, openness, etc. The second descriptor is also a global image descriptor, computed by resizing the image into a “tiny image” (Torralba et al., 2008), which is effective in matching the structure and overall color of images. To find visually relevant images, we compute the similarity of the query image to im793 Huge wall of glass A view of the post office My footprint in a James the cat is This little boy was so Cell phone shot of at the Conference building in Manila from sand box dreaming of running c</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: a holistic representation of the spatial envelope. International Journal of Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2330" citStr="Ordonez et al. (2011)" startWordPosition="345" endWordPosition="348"> images with similar content and then transferring Circumstantial Visually relevant, Visually truthful, information that is not but with overly but for an uncommon visually present extraneous details situation “I saw her in the light “Sections of the “A house being of her reading lamp bridge sitting in the pulled by a boat.” and sneaked back to Dyer Construction her door with the yard south of camera.” Cabelas Driver.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and </context>
<context position="5488" citStr="Ordonez et al., 2011" startWordPosition="802" endWordPosition="805">ause the depiction is unusual.1 This phenomenon of information gap between the visual content of the images and their corresponding narratives has been studied closely by Dodge et al. (2012). The content misalignment between images and text limits the extent to which visual detectors can learn meaningful mappings between images and text. To tackle this challenge, we introduce the new task of image caption generalization that rewrites captions to be more visually relevant and more readily applicable to other visually similar images. Our end goal is to convert noisy imagetext pairs in the wild (Ordonez et al., 2011) into pairs with tighter content alignment, resulting in new simplified captions over 1 million images. Evaluation results show both the intrinsic quality of the generalized captions and the extrinsic utility of the new image-text parallel corpus. The new parallel corpus will be made publicly available.2 2 Sentence Generalization as Constraint Optimization Casting the generalization task as visually-guided sentence compression with lightweight revisions, we formulate a constraint optimization problem that aims to maximize content selection and local linguistic fluency while satisfying constrai</context>
<context position="9208" citStr="Ordonez et al., 2011" startWordPosition="1409" endWordPosition="1413">re more likely to generalize across many different images. Additionally, we assign a very low content selection score (−oc) for proper nouns and numbers and a very high score (larger then maximum idf or visual score) for the 2k most frequent words in our corpus. Local Linguistic Fluency: We model linguistic fluency with 3-gram conditional probabilities: V(xl(yi), xl(yi−1), xl(yi−2)) (1) = p(xl(yi)|xl(yi−2), xl(yi−1)) We experiment with two different ngram statistics, one extracted from the Google Web 1T corpus (Brants and Franz., 2006), and the other computed from the 1M image-caption corpus (Ordonez et al., 2011). Dependency-driven Constraints: Table 1 defines the list of dependencies used as constraints driven from the typed dependencies (de Marneffe and Manning, 2009; de Marneffe et al., 2006). The direction of arrows indicate the direction of inclusion requirements. For example, dep(X +− Y ), denotes that “X” must be included whenever “Y ” is included. Similarly, dep(X +� Y ) denotes that “X” and “Y ” must either be included together or eliminated together. We determine the uni- or bi-directionality of these constraints by manually examining a few example sentences corresponding to each of these ty</context>
<context position="14911" citStr="Ordonez et al. (2011)" startWordPosition="2321" endWordPosition="2324">at is not visually present or verifiable, and other times humans overly compressed. To verify the utility of dependency-based constraints, we also compare two variations of VISUAL, with and without dependency-based constraints. As expected, the algorithm with constraints is preferred in the majority of cases. 3.2 Extrinsic Evaluation: Image-based Caption Retrieval We evaluate the usefulness of our new image-text parallel corpus for automatic generation of image descriptions. Here the task is to produce, for a query image, a relevant description, i.e., a visually descriptive caption. Following Ordonez et al. (2011), we produce a caption for a query image by finding top k most similar images within the 1M image-text corpus (Ordonez et al., 2011) and then transferring their captions to the query image. To compute evaluation measures, we take the average scores of BLEU(1) and F-score (unigrambased with respect to content-words) over k = 5 candidate captions. Image similarity is computed using two global (whole) image descriptors. The first is the GIST feature (Oliva and Torralba, 2001), an image descriptor related to perceptual characteristics of scenes – naturalness, roughness, openness, etc. The second d</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Torralba</author>
<author>Rob Fergus</author>
<author>William T Freeman</author>
</authors>
<title>80 million tiny images: a large dataset for non-parametric object and scene recognition.</title>
<date>2008</date>
<journal>Pattern Analysis and Machine Intelligence,</journal>
<volume>30</volume>
<contexts>
<context position="15630" citStr="Torralba et al., 2008" startWordPosition="2438" endWordPosition="2441">-text corpus (Ordonez et al., 2011) and then transferring their captions to the query image. To compute evaluation measures, we take the average scores of BLEU(1) and F-score (unigrambased with respect to content-words) over k = 5 candidate captions. Image similarity is computed using two global (whole) image descriptors. The first is the GIST feature (Oliva and Torralba, 2001), an image descriptor related to perceptual characteristics of scenes – naturalness, roughness, openness, etc. The second descriptor is also a global image descriptor, computed by resizing the image into a “tiny image” (Torralba et al., 2008), which is effective in matching the structure and overall color of images. To find visually relevant images, we compute the similarity of the query image to im793 Huge wall of glass A view of the post office My footprint in a James the cat is This little boy was so Cell phone shot of at the Conference building in Manila from sand box dreaming of running cute. He was flying his a hat stall in the Centre in the other side of the in a wide green spiderman kite all by Northeast Market, Yohohama Japan. Pasig River valley himself on top of Max Baltimore, MD. Patch 4 Wall of glass 4 A view of the po</context>
</contexts>
<marker>Torralba, Fergus, Freeman, 2008</marker>
<rawString>Antonio Torralba, Rob Fergus, and William T. Freeman. 2008. 80 million tiny images: a large dataset for non-parametric object and scene recognition. Pattern Analysis and Machine Intelligence, 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="18600" citStr="Turner and Charniak (2005)" startWordPosition="2940" endWordPosition="2943">o revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel cor</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 290–297, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinjun Wang</author>
<author>Jianchao Yang</author>
<author>Kai Yu</author>
<author>Fengjun Lv</author>
<author>T Huang</author>
<author>Yihong Gong</author>
</authors>
<title>Localityconstrained linear coding for image classification.</title>
<date>2010</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="7906" citStr="Wang et al., 2010" startWordPosition="1191" endWordPosition="1194">x) = E ψ(yi, ..., yi−K) i E= ψ(xl(yi), ..., xl(yi−x)) i where K is the size of local context. Content Selection – Visual Estimates: The computer vision system used consists of 7404 visual classifiers for recognizing leaf level WordNet synsets (Fellbaum, 1998). Each classifier is trained using labeled images from the ImageNet dataset (Deng et al., 2009) – an image database of over 14 million hand labeled images organized according to the WordNet hierarchy. Image similarity is represented using a Spatial Pyramid Match Kernel (SPM) (Lazebnik et al., 2006) with Locality-constrained Linear Coding (Wang et al., 2010) on shape based SIFT features (Lowe, 2004). 791 (a) (b) sentences (in thousands) 1200 400 800 0 0 1 2 3 sentences (in thousands) 400 800 600 200 0 0 1 2 3 4 5 6 7 Method-1 (M1) v.s. Method-2 (M2) M1 wins over M2 SALIENCY ORIG 76.34% VISUAL ORIG 81.75% VISUAL SALIENCY 72.48% VISUAL VISUAL W/O CONSTR 83.76% VISUAL NGRAM-ONLY 90.20% VISUAL HUMAN 19.00% Figure 2: Number of sentences (y-axis) for each average (x-axis in (a)) and maximum (x-axis in (b)) number of words with future dependencies Models are linear SVMs followed by a sigmoid to produce probability for each node.3 Content Selection – Sal</context>
</contexts>
<marker>Wang, Yang, Yu, Lv, Huang, Gong, 2010</marker>
<rawString>Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, T. Huang, and Yihong Gong. 2010. Localityconstrained linear coding for image classification. In Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18085" citStr="Wu and Palmer, 1994" startWordPosition="2860" endWordPosition="2863"> those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to l</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Ching Teo</author>
<author>Hal Daume</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Corpus-guided sentence generation of natural images.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="17854" citStr="Yang et al. (2011)" startWordPosition="2819" endWordPosition="2822">matching, which gives partial credits to word pairs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into 5We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before. Any word matched once (even with a partial credit) will be removed from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prio</context>
</contexts>
<marker>Yang, Teo, Daume, Aloimonos, 2011</marker>
<rawString>Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1353--1361</pages>
<location>Beijing, China,</location>
<contexts>
<context position="18912" citStr="Zhu et al. (2010)" startWordPosition="2988" endWordPosition="2991">moved from consideration when matching next words. descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Acknowledgments This research </context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>