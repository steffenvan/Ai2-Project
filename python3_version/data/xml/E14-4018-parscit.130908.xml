<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006127">
<title confidence="0.998005">
Inference of Phrase-Based Translation Models
via Minimum Description Length
</title>
<author confidence="0.970401">
Jes´us Gonz´alez-Rubio and Francisco Casacuberta
</author>
<affiliation confidence="0.794383">
Departamento de Sistemas Inform´aticos y Computaci´on
Universitat Polit`ecnica de Val`encia, Camino de Vera s/n, 46021 Valencia (Spain)
</affiliation>
<email confidence="0.98272">
{jegonzalez, fcn}@dsic.upv.es
</email>
<sectionHeader confidence="0.997118" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949846153846">
We present an unsupervised inference pro-
cedure for phrase-based translation mod-
els based on the minimum description
length principle. In comparison to cur-
rent inference techniques that rely on
long pipelines of training heuristics, this
procedure represents a theoretically well-
founded approach to directly infer phrase
lexicons. Empirical results show that the
proposed inference procedure has the po-
tential to overcome many of the prob-
lems inherent to the current inference ap-
proaches for phrase-based models.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999515673469387">
Since their introduction at the beginning of the
twenty-first century, phrase-based (PB) transla-
tion models (Koehn et al., 2003) have become the
state-of-the-art for statistical machine translation
(SMT). PB model provide a big leap in translation
quality with respect to the previous word-based
translation models (Brown et al., 1990; Vogel et
al., 1996). However, despite their empirical suc-
cess, inference procedures for PB models rely on
a long pipeline of heuristics (Och and Ney, 2003)
and mismatched learning models, such as the long
outperformed word-based models. Latter stages
of the pipeline cannot recover mistakes or omis-
sions made in earlier stages which forces the indi-
vidual stages to massively overgenerate hypothe-
ses. This manifests as a huge redundancy in the
inferred phrase lexicons, which in turn largely pe-
nalizes the efficiency of PB systems at run-time.
The fact that PB models usually cannot generate
the sentence pairs in which they have been trained
in, or that it is even possible to improve the perfor-
mance of a PB system by discarding most of the
learned phrases are clear indicators of these defi-
ciencies (Sanchis-Trilles et al., 2011).
We introduce an unsupervised procedure to in-
fer PB models based on the minimum descrip-
tion length (MDL) principle (Solomonoff, 1964;
Rissanen, 1978). MDL, formally described in
Section 2, is a general inference procedure that
“learns” by “finding data regularities”. MDL takes
its name from the fact that regularities allow to
compress the data, i.e. to describe it using fewer
symbols than those required to describe the data
literally. As such, MDL embodies a form of Oc-
cam’s Razor in which the best model for a given
data is the one that provides a better trade-off be-
tween goodness-of-fit on the data and “complex-
ity” or “richness” of the model.
MDL has been previously used to infer mono-
lingual grammars (Gr¨unwald, 1996) and inversion
transduction grammars (Saers et al., 2013). Here,
we adapt the basic principles described in the lat-
ter article to the inference of PB models. The
MDL inference procedure, described in Section 3,
learns PB models by iteratively generalizing an
initial model that perfectly overfits training data.
An MDL objective is used to guide this process.
MDL inference has the following desirable prop-
erties:
</bodyText>
<listItem confidence="0.984584111111111">
• Training and testing are optimized upon the
same model; a basic principle of machine learn-
ing largely ignored in PB models.
• It provides a joint estimation of the structure
(set of bilingual phrases) and the parameters
(phrase probabilities) of PB models.
• It automatically protects against overfitting by
implementing a trade-off between the expres-
siveness of the model and training data fitting.
</listItem>
<bodyText confidence="0.99972625">
The empirical evaluation described in Section 4
focuses on understanding the behavior of MDL-
based PB models and their specific traits. That
is, in contrast to a typical PB system building pa-
per, we are not exclusively focused on a short
term boost in translation quality. Instead, we aim
at studying the adequacy and future potential of
MDL as inference procedure for PB models.
</bodyText>
<page confidence="0.98473">
90
</page>
<note confidence="0.8976295">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 90–94,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.986078" genericHeader="method">
2 The MDL Principle
</sectionHeader>
<bodyText confidence="0.999939333333334">
Given a set of data D, the MDL principle aims at
obtaining the simplest possible model Φ that de-
scribes D as well as possible (Solomonoff, 1964;
Rissanen, 1978). Central to MDL is the one-
to-one correspondence between description length
functions and probability distributions that follows
from the Kraft-McMillan inequality (McMillan,
1956). For any probability distribution Pr(·), it
is possible to construct a coding scheme such that
the length (in bits) of the encoded data is mini-
mum and equal to −log2(Pr(D)). In other words,
searching for a minimum description length re-
duces to searching for a good probability distribu-
tion, and vice versa. Taking these considerations
into account, MDL inference is formalized as:
</bodyText>
<equation confidence="0.99468025">
Φ� = argmin DL(Φ, D) (1)
p
= argmin DL(Φ) + DL(D  |Φ) (2)
p
</equation>
<bodyText confidence="0.9999844">
where DL(Φ) denotes the description length of
the model, and DL(D  |Φ) denotes the descrip-
tion length of the data given the model. A com-
plete introductory tutorial of the MDL principle
and methods can be found in (Gr¨unwald, 2004).
</bodyText>
<sectionHeader confidence="0.981251" genericHeader="method">
3 MDL Phrase-Based Models
</sectionHeader>
<subsectionHeader confidence="0.998964">
3.1 Description Length Functions
</subsectionHeader>
<bodyText confidence="0.999979833333333">
We start by defining how to compute DL(Φ) and
DL(D  |Φ) for any PB model and data set.
Let Prp(D) be the probability of data set
D according to PB model Φ. We follow the
Kraft-McMillan inequality and define the de-
scription length of the data given the model as
DL(D  |Φ) = −log2(Prp(D)), which it is the
lower bound for the description length of the data.
Regarding the description length of the PB
model, DL(Φ), we compute it by serializing Φ
into a sequence of symbols and then computing
the length of the optimal encoding of such se-
quence. To do that, we need one symbol for each
word in the source and target languages, another
symbol to separate the source and target sides in
a phrase pair, and one additional symbol to dis-
tinguish between the different pairs in the phrase
lexicon. For example, the following toy PB model
</bodyText>
<subsectionHeader confidence="0.621098">
La The casa house azul blue
</subsectionHeader>
<bodyText confidence="0.999592875">
is serialized as La|The•casa|house•azul|blue,
where symbol • separates the phrase pairs, and |
separates the two sides of each pair. Assuming a
uniform distribution over the K different symbols,
each symbol would require −log2(1K ) bits to en-
code. We will thus require 3 bits to encode each
of the 8 symbols in the example, and 33 bits to en-
code the whole serialized PB model (11 symbols).
</bodyText>
<subsectionHeader confidence="0.99512">
3.2 Inference Procedure
</subsectionHeader>
<bodyText confidence="0.982333838709677">
We now describe how to perform the maximiza-
tion in Equation (2). In the case of PB models,
this reduces to a search for the optimal phrase lex-
icon. Obviously, an exhaustive search over all pos-
sible sets of phrase pairs in the data is unfeasible
in practice. Following the ideas in (Vilar and Vi-
dal, 2005), we implement a search procedure that
iteratively generalizes an initial PB model that per-
fectly fits the data. Let D = {fn, en}Nn=1 be a
data set with N sentence pairs, where fn are sen-
tences in the source language and en are their cor-
responding translation in the target language. Our
initial PB model will be as follows:
where the probability of each pair is given by the
number of occurrences of the pair in the data di-
vided by the number of occurrences of the source
(or target) language sentence.
To generalize this initial PB model, we need
to identify parts of the existing phrase pairs that
could be validly used in isolation. As a result, the
PB model will be able to generate new transla-
tions different from the ones in the training data.
From a probabilistic point of view, this process
moves some of the probability mass which is con-
centrated in the training data out to other data still
unseen; the very definition of generalization. Con-
sider a PB model such as:
La casa azul The blue house
Esta casa azul This blue house
Esta casa verde This green house
It can be segmented to obtain a new PB model:
</bodyText>
<subsectionHeader confidence="0.503835">
La The casa azul blue house
</subsectionHeader>
<bodyText confidence="0.951545272727273">
Esta This casa verde green house
which is able to generate one new sentence pair
(La casa verde→The green house) and has a
shorter description length (19 symbols) in compar-
ison to the original model (23 symbols). We only
consider segmentations that bisect the source and
target phrases. More sophisticated segmentation
approaches are beyond the scope of this article.
Algorithm 1 describes the proposed PB infer-
ence by iterative generalization. First, we col-
lect the potential segmentations of the current PB
</bodyText>
<page confidence="0.994438">
91
</page>
<table confidence="0.994344">
Algorithm 1: Iterative inference procedure.
input : Φ (initial PB model)
output : Φ� (generalized PB model)
auxiliary : collect(Φ) (Returns the set of possible
segmentations of model Φ)
ADL(s, Φ) (Returns variation in DL when
segmenting Φ according to s)
sort(S) (Sorts segmentation set S by
variation in DL)
commit(S, Φ) (Apply segmentations in S
to Φ, returns variation in DL)
EuTransI (Sp / En)
train tune test
#Sentences 10k 2k 1k
#Words 97k / 99k 23k / 24k 12k / 12k
Vocabulary 687 / 513 510 / 382 571 / 435
OOV – / – 0 / 0 0 / 0
Perplexity – / – 8.4 / 3.4 8.1 / 3.3
News Commentary (Sp / En)
train tune test
#Sentences 51k 2k 1k
#Words 1.4M / 1.2M 56k / 50k 30k / 26k
Vocabulary 47k / 35k 5k / 5k 8k / 7k
OOV – / – 390 / 325 832 / 538
Perplexity – / – 136.2 / 197.9 144.2 / 206.0
</table>
<tableCaption confidence="0.94340275">
Table 1: Main figures of the experimental corpora.
M and k stand for millions and thousands of ele-
ments respectively. Perplexity was calculated us-
ing 5-gram language models.
</tableCaption>
<figure confidence="0.9878675">
1 begin
repeat
S ← collect(Φ);
candidates ← [];
for s E S do
A0 ← ADL(s, Φ);
if A0 ≤ 0 then
candidates .append({A0, s});
sort(candidates);
A ← commit(candidates, Φ);
until A &gt; 0 ;
return Φ;
2
3
4
5
6
7
8
9
10
11
12
13 end
</figure>
<bodyText confidence="0.99968585">
model (line 3). Then, we estimate the variation in
description length due to the application of each
segmentation (lines 4 to 8). Finally, we sort the
segmentations by variation in description length
(line 9) and commit to the best of them (line 10).
Specifically, given that different segmentations
may modify the same phrase pair, we apply each
segmentation only if it only affect phrase pairs
unaffected by previous segmentations in S. The
algorithm stops when none of the segmentations
lead to a reduction in description length. Saers
et al., (2013) follow a similar greedy algorithm to
generalize inversion transduction grammars.
The key component of Algorithm 1 is function
ADL(s, Φ) that evaluates the impact of a candi-
date segmentation s on the description length of
PB model Φ. That is, ADL(s, Φ) computes the
difference in description length between the cur-
rent model Φ and the model Φ0 that would result
from committing to s:
</bodyText>
<equation confidence="0.989007">
ADL(s, Φ) = DL(Φ0) − DL(Φ)
+ DL(D  |Φ0) − DL(D  |Φ) (3)
</equation>
<bodyText confidence="0.999908260869565">
The length difference between the phrase lexi-
cons (DL(Φ0)−DL(Φ)) is trivial. We merely have
to compute the difference between the lengths of
the phrase pairs added and removed. The differ-
ence for the data is given by −log2 ( Pr b �D� )
where PrΦ0(D) and PrΦ(D) are the probability
of D according to Φ0 and Φ respectively. These
probabilities can be computed by translating the
training data. However, this is a very expensive
process that we cannot afford to perform for each
candidate segmentation. Instead, we estimate the
description length of the data in closed form based
on the probabilities of the phrase pairs involved.
The probability of a phrase pair {˜f, ˜e} is computed
as the the number of occurrences of the pair di-
vided by the number of occurrences of the source
(or target) phrase. We thus estimate the probabil-
ities in the segmented model Φ0 by counting the
occurrences of the replaced phrase pairs as occur-
rences of the segmented pairs. Let { ˜f0, ˜e0} be
the phrase pair we are splitting into { ˜f1, ˜e1} and
{˜f2, ˜e2}. The direct phrase probabilities in Φ0 will
be identical to those in Φ except that:
</bodyText>
<equation confidence="0.999746">
˜f1,˜e1}) + NΦ({
˜f0, ˜e0})
˜f1) + NΦ({ ˜f0, ˜e0})
˜f2, ˜e2}) + NΦ({
˜f0, ˜e0})
˜f2) + NΦ({ ˜f0,˜e0})
</equation>
<bodyText confidence="0.999559666666667">
where NΦ(·) are counts in Φ. Inverse probabilities
are computed accordingly. Finally, we compute
the variation in data description length using:
</bodyText>
<equation confidence="0.999117928571429">
PΦ0(˜e1  |˜f1) · PΦ0(˜e2  |˜f2)
PΦ(˜e0  |˜f0)
˜f1  |˜e1) · PΦ0(˜f2  |˜e2)
PΦ(
PΦ0(˜e0  |˜f0) = 0
PΦ0(˜e1  |˜f1) = NΦ({ NΦ(
PΦ0(˜e2  |˜f2) =
NΦ({ NΦ(
PrΦ0(D) ≈
PrΦ(D)
PΦ0(
·
(4)
˜f0  |˜e0)
</equation>
<page confidence="0.976923">
92
</page>
<table confidence="0.958947666666667">
Relative frequency [%]
EUtransI News Commentary
BLEU [%] BLEU [%]
(tune/test) Size (tune/test) Size
SotA 91.6 / 90.9 39.1k 31.4 / 30.7 2.2M
MDL 88.7 / 88.0 2.7k 24.8 / 24.6 79.1k
</table>
<tableCaption confidence="0.81371975">
Table 2: Size (number of phrase pairs) of the
MDL-based PB models, and quality of the gener-
ated translations. We compare against a state-of-
the-art PB inference pipeline (SotA).
</tableCaption>
<bodyText confidence="0.983256333333333">
For a segmentation set, we first estimate the new
model Φ� to reflect all the applied segmentations,
and then sum the differences in description length.
</bodyText>
<sectionHeader confidence="0.992687" genericHeader="method">
4 Empirical Results
</sectionHeader>
<bodyText confidence="0.999936583333333">
We evaluated the proposed inference procedure
on the EuTransI (Amengual et al., 2000) and the
News Commentary (Callison-Burch et al., 2007)
corpora. Table 1 shows their main figures.
We inferred PB models (set of phrase pairs and
their corresponding probabilities) with the training
partitions as described in Section 3.2. Then, we
included these MDL-based PB models in a con-
ventional log-linear model optimized with the tun-
ing partitions (Och, 2003). Finally, we generated
translations for the test partitions using a conven-
tional PB decoder (Koehn et al., 2007).
Table 2 shows size (number of phrase pairs) of
the inferred MDL-based PB models, and BLEU
score (Papineni et al., 2002) of their translations of
the tune and test partitions. As a comparison, we
display results for a state-of-the-art (SotA) PB sys-
tem (Koehn et al., 2007). These results show that
MDL inference obtained much more concise mod-
els (less than one tenth the number of phrases) than
the standard inference pipeline. Additionally, the
translations of the simple EuTransI corpus were of
a similar quality as the ones obtained by the SotA
system. In contrast, the quality of the translations
for News Commentary was significantly lower.
To better understand these results, Figure 1 dis-
plays the histogram of phrase lengths (number of
source words plus target words) of the SotA model
and the MDL-based model for the News Commen-
taries corpus. We first observed that the length of
the phrase pairs followed a completely different
distribution depending on the inference procedure.
Most of the phrase pairs of the MDL-based model
translated one source word by one target word
with an exponential decay in frequency for longer
phrase pairs; a typical distribution of events in nat-
</bodyText>
<figure confidence="0.7001075">
0 10 20 30 40 50 60 70 80
Length of the phrase pair (words)
</figure>
<figureCaption confidence="0.778082">
Figure 1: Histogram of lengths (source plus target
words) for the phrase pairs in the inferred models.
</figureCaption>
<bodyText confidence="0.9997817">
ural language (Zipf, 1935). Longer phrase pairs,
about 45% of the total, contain sequences of words
that only appear once in the corpus, and thus, they
cannot be segmented in any way that leads to a re-
duction in description length. Although formally
correct, long phrase pairs generalize poorly which
explains the comparatively poor performance of
MDL inference for the News Commentaries cor-
pus. This problem was largely attenuated for Eu-
TransI due to its simplicity.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Conclusions and Future Developments
</sectionHeader>
<bodyText confidence="0.99995275">
We have described a simple, unsupervised infer-
ence procedure for PB models that learns phrase
lexicons by iteratively splitting existing phrases
into smaller phrase pairs using a theoretically
well-founded minimum description length objec-
tive. Empirical results have shown that the in-
ferred PB models, far from the artificial redun-
dancy of the conventional PB inference pipeline,
are very parsimonious and provide competitive
translations for simple translation tasks.
The proposed methodology provides a solid
foundation from where to develop new PB infer-
ence approaches that overcome the problems in-
herent to the long pipeline of heuristics that nowa-
days constitute the state-of-the-art. Future devel-
opments in this direction will include:
</bodyText>
<listItem confidence="0.990215375">
• A more sophisticated segmentation procedure
that allow to divide the phrases into more that
two segments.
• A hybrid approach where the long phrase pairs
remaining after the MDL inference are further
segmented, e.g., according to a word lexicon.
• The inclusion of lexical models in the definition
of the PB model.
</listItem>
<figure confidence="0.979260166666667">
30
20
10
0
SotA
MDL
</figure>
<page confidence="0.995328">
93
</page>
<sectionHeader confidence="0.999184" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992397">
Work supported by the European Union 7th
Framework Program (FP7/2007-2013) under the
CasMaCat project (grans agreement no 287576),
by Spanish MICINN under grant TIN2012-31723,
and by the Generalitat Valenciana under grant
ALMPR (Prometeo/2009/014).
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893950617284">
Juan-Carlos Amengual, M. Asunci´on Casta˜no, Anto-
nio Castellanos, V´ıctor M. Jim´enez, David Llorens,
Andr´es Marzal, Federico Prat, Juan Miguel Vilar,
Jos´e-Miguel Bened´ı, Francisco Casacuberta, Mois´es
Pastor, and Enrique Vidal. 2000. The eutrans spo-
ken language translation system. Machine Transla-
tion, 15(1-2):75–103.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16:79–85.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Workshop on Statistical Machine
Translation, pages 136–158.
Peter Gr¨unwald. 1996. A minimum description length
approach to grammar inference. Connectionist, Sta-
tistical, and Symbolic Approaches to Learning for
Natural Language Processing, pages 203–216.
Peter Gr¨unwald. 2004. A tutorial introduc-
tion to the minimum description length principle.
http://arxiv.org/abs/math/0406077.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the Association for Computational
Linguistics, demonstration session, June.
Brockway McMillan. 1956. Two inequalities implied
by unique decipherability. IRE Transactions on In-
formation Theory, 2(4):115–116.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Meeting on Association for Computational Linguis-
tics, pages 160–167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
Meeting on Association for Computational Linguis-
tics, pages 311–318. Association for Computational
Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465 – 471.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Iterative rule segmentation under minimum descrip-
tion length for unsupervised transduction grammar
induction. In Statistical Language and Speech Pro-
cessing, volume 7978 of Lecture Notes in Computer
Science, pages 224–235. Springer.
Germ´an Sanchis-Trilles, Daniel Ortiz-Mart´ınez, Jes´us
Gonz´alez-Rubio, Jorge Gonz´alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in statistical machine transla-
tion. In Proceedings of the 15th Conference of the
European Association for Machine Translation.
Ray Solomonoff. 1964. A formal theory of inductive
inference, parts 1 and 2. Information and Control,
7:1–22, 224–254.
Juan Miguel Vilar and Enrique Vidal. 2005. A recur-
sive statistical translation model. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 199–207.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836–841.
George Kingsley Zipf. 1935. The Psychobiology of
Language. Houghton-Mifflin.
</reference>
<page confidence="0.99955">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397373">
<title confidence="0.7731935">Inference of Phrase-Based Translation via Minimum Description Length</title>
<author confidence="0.656838">Gonz´alez-Rubio</author>
<affiliation confidence="0.9378015">Departamento de Sistemas Inform´aticos y Universitat Polit`ecnica de Val`encia, Camino de Vera s/n, 46021 Valencia</affiliation>
<abstract confidence="0.996459642857143">We present an unsupervised inference procedure for phrase-based translation models based on the minimum description length principle. In comparison to current inference techniques that rely on long pipelines of training heuristics, this procedure represents a theoretically wellfounded approach to directly infer phrase lexicons. Empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Juan-Carlos Amengual</author>
<author>M Asunci´on Casta˜no</author>
<author>Antonio Castellanos</author>
<author>V´ıctor M Jim´enez</author>
<author>David Llorens</author>
<author>Andr´es Marzal</author>
<author>Federico Prat</author>
<author>Juan Miguel Vilar</author>
<author>Jos´e-Miguel Bened´ı</author>
<author>Francisco Casacuberta</author>
<author>Mois´es Pastor</author>
<author>Enrique Vidal</author>
</authors>
<title>The eutrans spoken language translation system.</title>
<date>2000</date>
<booktitle>Machine Translation,</booktitle>
<pages>15--1</pages>
<marker>Amengual, Casta˜no, Castellanos, Jim´enez, Llorens, Marzal, Prat, Vilar, Bened´ı, Casacuberta, Pastor, Vidal, 2000</marker>
<rawString>Juan-Carlos Amengual, M. Asunci´on Casta˜no, Antonio Castellanos, V´ıctor M. Jim´enez, David Llorens, Andr´es Marzal, Federico Prat, Juan Miguel Vilar, Jos´e-Miguel Bened´ı, Francisco Casacuberta, Mois´es Pastor, and Enrique Vidal. 2000. The eutrans spoken language translation system. Machine Translation, 15(1-2):75–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--79</pages>
<contexts>
<context position="1161" citStr="Brown et al., 1990" startWordPosition="157" endWordPosition="160">ure represents a theoretically wellfounded approach to directly infer phrase lexicons. Empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models. 1 Introduction Since their introduction at the beginning of the twenty-first century, phrase-based (PB) translation models (Koehn et al., 2003) have become the state-of-the-art for statistical machine translation (SMT). PB model provide a big leap in translation quality with respect to the previous word-based translation models (Brown et al., 1990; Vogel et al., 1996). However, despite their empirical success, inference procedures for PB models rely on a long pipeline of heuristics (Och and Ney, 2003) and mismatched learning models, such as the long outperformed word-based models. Latter stages of the pipeline cannot recover mistakes or omissions made in earlier stages which forces the individual stages to massively overgenerate hypotheses. This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time. The fact that PB models usually cannot generate the sen</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16:79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<contexts>
<context position="12842" citStr="Callison-Burch et al., 2007" startWordPosition="2194" endWordPosition="2197">mentary BLEU [%] BLEU [%] (tune/test) Size (tune/test) Size SotA 91.6 / 90.9 39.1k 31.4 / 30.7 2.2M MDL 88.7 / 88.0 2.7k 24.8 / 24.6 79.1k Table 2: Size (number of phrase pairs) of the MDL-based PB models, and quality of the generated translations. We compare against a state-ofthe-art PB inference pipeline (SotA). For a segmentation set, we first estimate the new model Φ� to reflect all the applied segmentations, and then sum the differences in description length. 4 Empirical Results We evaluated the proposed inference procedure on the EuTransI (Amengual et al., 2000) and the News Commentary (Callison-Burch et al., 2007) corpora. Table 1 shows their main figures. We inferred PB models (set of phrase pairs and their corresponding probabilities) with the training partitions as described in Section 3.2. Then, we included these MDL-based PB models in a conventional log-linear model optimized with the tuning partitions (Och, 2003). Finally, we generated translations for the test partitions using a conventional PB decoder (Koehn et al., 2007). Table 2 shows size (number of phrase pairs) of the inferred MDL-based PB models, and BLEU score (Papineni et al., 2002) of their translations of the tune and test partitions.</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gr¨unwald</author>
</authors>
<title>A minimum description length approach to grammar inference. Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing,</title>
<date>1996</date>
<pages>203--216</pages>
<marker>Gr¨unwald, 1996</marker>
<rawString>Peter Gr¨unwald. 1996. A minimum description length approach to grammar inference. Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, pages 203–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gr¨unwald</author>
</authors>
<title>A tutorial introduction to the minimum description length principle.</title>
<date>2004</date>
<note>http://arxiv.org/abs/math/0406077.</note>
<marker>Gr¨unwald, 2004</marker>
<rawString>Peter Gr¨unwald. 2004. A tutorial introduction to the minimum description length principle. http://arxiv.org/abs/math/0406077.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="955" citStr="Koehn et al., 2003" startWordPosition="127" endWordPosition="130"> procedure for phrase-based translation models based on the minimum description length principle. In comparison to current inference techniques that rely on long pipelines of training heuristics, this procedure represents a theoretically wellfounded approach to directly infer phrase lexicons. Empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models. 1 Introduction Since their introduction at the beginning of the twenty-first century, phrase-based (PB) translation models (Koehn et al., 2003) have become the state-of-the-art for statistical machine translation (SMT). PB model provide a big leap in translation quality with respect to the previous word-based translation models (Brown et al., 1990; Vogel et al., 1996). However, despite their empirical success, inference procedures for PB models rely on a long pipeline of heuristics (Och and Ney, 2003) and mismatched learning models, such as the long outperformed word-based models. Latter stages of the pipeline cannot recover mistakes or omissions made in earlier stages which forces the individual stages to massively overgenerate hypo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics, demonstration session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="13266" citStr="Koehn et al., 2007" startWordPosition="2261" endWordPosition="2264"> differences in description length. 4 Empirical Results We evaluated the proposed inference procedure on the EuTransI (Amengual et al., 2000) and the News Commentary (Callison-Burch et al., 2007) corpora. Table 1 shows their main figures. We inferred PB models (set of phrase pairs and their corresponding probabilities) with the training partitions as described in Section 3.2. Then, we included these MDL-based PB models in a conventional log-linear model optimized with the tuning partitions (Och, 2003). Finally, we generated translations for the test partitions using a conventional PB decoder (Koehn et al., 2007). Table 2 shows size (number of phrase pairs) of the inferred MDL-based PB models, and BLEU score (Papineni et al., 2002) of their translations of the tune and test partitions. As a comparison, we display results for a state-of-the-art (SotA) PB system (Koehn et al., 2007). These results show that MDL inference obtained much more concise models (less than one tenth the number of phrases) than the standard inference pipeline. Additionally, the translations of the simple EuTransI corpus were of a similar quality as the ones obtained by the SotA system. In contrast, the quality of the translation</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, demonstration session, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brockway McMillan</author>
</authors>
<title>Two inequalities implied by unique decipherability.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="4489" citStr="McMillan, 1956" startWordPosition="696" endWordPosition="697"> MDL as inference procedure for PB models. 90 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 90–94, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 The MDL Principle Given a set of data D, the MDL principle aims at obtaining the simplest possible model Φ that describes D as well as possible (Solomonoff, 1964; Rissanen, 1978). Central to MDL is the oneto-one correspondence between description length functions and probability distributions that follows from the Kraft-McMillan inequality (McMillan, 1956). For any probability distribution Pr(·), it is possible to construct a coding scheme such that the length (in bits) of the encoded data is minimum and equal to −log2(Pr(D)). In other words, searching for a minimum description length reduces to searching for a good probability distribution, and vice versa. Taking these considerations into account, MDL inference is formalized as: Φ� = argmin DL(Φ, D) (1) p = argmin DL(Φ) + DL(D |Φ) (2) p where DL(Φ) denotes the description length of the model, and DL(D |Φ) denotes the description length of the data given the model. A complete introductory tutor</context>
</contexts>
<marker>McMillan, 1956</marker>
<rawString>Brockway McMillan. 1956. Two inequalities implied by unique decipherability. IRE Transactions on Information Theory, 2(4):115–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1318" citStr="Och and Ney, 2003" startWordPosition="183" endWordPosition="186">otential to overcome many of the problems inherent to the current inference approaches for phrase-based models. 1 Introduction Since their introduction at the beginning of the twenty-first century, phrase-based (PB) translation models (Koehn et al., 2003) have become the state-of-the-art for statistical machine translation (SMT). PB model provide a big leap in translation quality with respect to the previous word-based translation models (Brown et al., 1990; Vogel et al., 1996). However, despite their empirical success, inference procedures for PB models rely on a long pipeline of heuristics (Och and Ney, 2003) and mismatched learning models, such as the long outperformed word-based models. Latter stages of the pipeline cannot recover mistakes or omissions made in earlier stages which forces the individual stages to massively overgenerate hypotheses. This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time. The fact that PB models usually cannot generate the sentence pairs in which they have been trained in, or that it is even possible to improve the performance of a PB system by discarding most of the learned phras</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13153" citStr="Och, 2003" startWordPosition="2245" endWordPosition="2246">ation set, we first estimate the new model Φ� to reflect all the applied segmentations, and then sum the differences in description length. 4 Empirical Results We evaluated the proposed inference procedure on the EuTransI (Amengual et al., 2000) and the News Commentary (Callison-Burch et al., 2007) corpora. Table 1 shows their main figures. We inferred PB models (set of phrase pairs and their corresponding probabilities) with the training partitions as described in Section 3.2. Then, we included these MDL-based PB models in a conventional log-linear model optimized with the tuning partitions (Och, 2003). Finally, we generated translations for the test partitions using a conventional PB decoder (Koehn et al., 2007). Table 2 shows size (number of phrase pairs) of the inferred MDL-based PB models, and BLEU score (Papineni et al., 2002) of their translations of the tune and test partitions. As a comparison, we display results for a state-of-the-art (SotA) PB system (Koehn et al., 2007). These results show that MDL inference obtained much more concise models (less than one tenth the number of phrases) than the standard inference pipeline. Additionally, the translations of the simple EuTransI corp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Meeting on Association for Computational Linguistics, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13387" citStr="Papineni et al., 2002" startWordPosition="2282" endWordPosition="2285">(Amengual et al., 2000) and the News Commentary (Callison-Burch et al., 2007) corpora. Table 1 shows their main figures. We inferred PB models (set of phrase pairs and their corresponding probabilities) with the training partitions as described in Section 3.2. Then, we included these MDL-based PB models in a conventional log-linear model optimized with the tuning partitions (Och, 2003). Finally, we generated translations for the test partitions using a conventional PB decoder (Koehn et al., 2007). Table 2 shows size (number of phrase pairs) of the inferred MDL-based PB models, and BLEU score (Papineni et al., 2002) of their translations of the tune and test partitions. As a comparison, we display results for a state-of-the-art (SotA) PB system (Koehn et al., 2007). These results show that MDL inference obtained much more concise models (less than one tenth the number of phrases) than the standard inference pipeline. Additionally, the translations of the simple EuTransI corpus were of a similar quality as the ones obtained by the SotA system. In contrast, the quality of the translations for News Commentary was significantly lower. To better understand these results, Figure 1 displays the histogram of phr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the Meeting on Association for Computational Linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<issue>5</issue>
<pages>471</pages>
<contexts>
<context position="2144" citStr="Rissanen, 1978" startWordPosition="320" endWordPosition="321">assively overgenerate hypotheses. This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time. The fact that PB models usually cannot generate the sentence pairs in which they have been trained in, or that it is even possible to improve the performance of a PB system by discarding most of the learned phrases are clear indicators of these deficiencies (Sanchis-Trilles et al., 2011). We introduce an unsupervised procedure to infer PB models based on the minimum description length (MDL) principle (Solomonoff, 1964; Rissanen, 1978). MDL, formally described in Section 2, is a general inference procedure that “learns” by “finding data regularities”. MDL takes its name from the fact that regularities allow to compress the data, i.e. to describe it using fewer symbols than those required to describe the data literally. As such, MDL embodies a form of Occam’s Razor in which the best model for a given data is the one that provides a better trade-off between goodness-of-fit on the data and “complexity” or “richness” of the model. MDL has been previously used to infer monolingual grammars (Gr¨unwald, 1996) and inversion transdu</context>
<context position="4309" citStr="Rissanen, 1978" startWordPosition="673" endWordPosition="674">to a typical PB system building paper, we are not exclusively focused on a short term boost in translation quality. Instead, we aim at studying the adequacy and future potential of MDL as inference procedure for PB models. 90 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 90–94, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 The MDL Principle Given a set of data D, the MDL principle aims at obtaining the simplest possible model Φ that describes D as well as possible (Solomonoff, 1964; Rissanen, 1978). Central to MDL is the oneto-one correspondence between description length functions and probability distributions that follows from the Kraft-McMillan inequality (McMillan, 1956). For any probability distribution Pr(·), it is possible to construct a coding scheme such that the length (in bits) of the encoded data is minimum and equal to −log2(Pr(D)). In other words, searching for a minimum description length reduces to searching for a good probability distribution, and vice versa. Taking these considerations into account, MDL inference is formalized as: Φ� = argmin DL(Φ, D) (1) p = argmin DL</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14(5):465 – 471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Karteek Addanki</author>
<author>Dekai Wu</author>
</authors>
<title>Iterative rule segmentation under minimum description length for unsupervised transduction grammar induction.</title>
<date>2013</date>
<booktitle>In Statistical Language and Speech Processing,</booktitle>
<volume>7978</volume>
<pages>224--235</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2779" citStr="Saers et al., 2013" startWordPosition="425" endWordPosition="428">described in Section 2, is a general inference procedure that “learns” by “finding data regularities”. MDL takes its name from the fact that regularities allow to compress the data, i.e. to describe it using fewer symbols than those required to describe the data literally. As such, MDL embodies a form of Occam’s Razor in which the best model for a given data is the one that provides a better trade-off between goodness-of-fit on the data and “complexity” or “richness” of the model. MDL has been previously used to infer monolingual grammars (Gr¨unwald, 1996) and inversion transduction grammars (Saers et al., 2013). Here, we adapt the basic principles described in the latter article to the inference of PB models. The MDL inference procedure, described in Section 3, learns PB models by iteratively generalizing an initial model that perfectly overfits training data. An MDL objective is used to guide this process. MDL inference has the following desirable properties: • Training and testing are optimized upon the same model; a basic principle of machine learning largely ignored in PB models. • It provides a joint estimation of the structure (set of bilingual phrases) and the parameters (phrase probabilities</context>
<context position="10184" citStr="Saers et al., (2013)" startWordPosition="1731" endWordPosition="1734">dates, Φ); until A &gt; 0 ; return Φ; 2 3 4 5 6 7 8 9 10 11 12 13 end model (line 3). Then, we estimate the variation in description length due to the application of each segmentation (lines 4 to 8). Finally, we sort the segmentations by variation in description length (line 9) and commit to the best of them (line 10). Specifically, given that different segmentations may modify the same phrase pair, we apply each segmentation only if it only affect phrase pairs unaffected by previous segmentations in S. The algorithm stops when none of the segmentations lead to a reduction in description length. Saers et al., (2013) follow a similar greedy algorithm to generalize inversion transduction grammars. The key component of Algorithm 1 is function ADL(s, Φ) that evaluates the impact of a candidate segmentation s on the description length of PB model Φ. That is, ADL(s, Φ) computes the difference in description length between the current model Φ and the model Φ0 that would result from committing to s: ADL(s, Φ) = DL(Φ0) − DL(Φ) + DL(D |Φ0) − DL(D |Φ) (3) The length difference between the phrase lexicons (DL(Φ0)−DL(Φ)) is trivial. We merely have to compute the difference between the lengths of the phrase pairs adde</context>
</contexts>
<marker>Saers, Addanki, Wu, 2013</marker>
<rawString>Markus Saers, Karteek Addanki, and Dekai Wu. 2013. Iterative rule segmentation under minimum description length for unsupervised transduction grammar induction. In Statistical Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science, pages 224–235. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an Sanchis-Trilles</author>
<author>Daniel Ortiz-Mart´ınez</author>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Jorge Gonz´alez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Bilingual segmentation for phrasetable pruning in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference of the European Association for Machine Translation.</booktitle>
<marker>Sanchis-Trilles, Ortiz-Mart´ınez, Gonz´alez-Rubio, Gonz´alez, Casacuberta, 2011</marker>
<rawString>Germ´an Sanchis-Trilles, Daniel Ortiz-Mart´ınez, Jes´us Gonz´alez-Rubio, Jorge Gonz´alez, and Francisco Casacuberta. 2011. Bilingual segmentation for phrasetable pruning in statistical machine translation. In Proceedings of the 15th Conference of the European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Solomonoff</author>
</authors>
<title>A formal theory of inductive inference, parts 1 and 2.</title>
<date>1964</date>
<journal>Information and Control,</journal>
<volume>7</volume>
<pages>224--254</pages>
<contexts>
<context position="2127" citStr="Solomonoff, 1964" startWordPosition="318" endWordPosition="319">vidual stages to massively overgenerate hypotheses. This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time. The fact that PB models usually cannot generate the sentence pairs in which they have been trained in, or that it is even possible to improve the performance of a PB system by discarding most of the learned phrases are clear indicators of these deficiencies (Sanchis-Trilles et al., 2011). We introduce an unsupervised procedure to infer PB models based on the minimum description length (MDL) principle (Solomonoff, 1964; Rissanen, 1978). MDL, formally described in Section 2, is a general inference procedure that “learns” by “finding data regularities”. MDL takes its name from the fact that regularities allow to compress the data, i.e. to describe it using fewer symbols than those required to describe the data literally. As such, MDL embodies a form of Occam’s Razor in which the best model for a given data is the one that provides a better trade-off between goodness-of-fit on the data and “complexity” or “richness” of the model. MDL has been previously used to infer monolingual grammars (Gr¨unwald, 1996) and </context>
<context position="4292" citStr="Solomonoff, 1964" startWordPosition="671" endWordPosition="672">t is, in contrast to a typical PB system building paper, we are not exclusively focused on a short term boost in translation quality. Instead, we aim at studying the adequacy and future potential of MDL as inference procedure for PB models. 90 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 90–94, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 The MDL Principle Given a set of data D, the MDL principle aims at obtaining the simplest possible model Φ that describes D as well as possible (Solomonoff, 1964; Rissanen, 1978). Central to MDL is the oneto-one correspondence between description length functions and probability distributions that follows from the Kraft-McMillan inequality (McMillan, 1956). For any probability distribution Pr(·), it is possible to construct a coding scheme such that the length (in bits) of the encoded data is minimum and equal to −log2(Pr(D)). In other words, searching for a minimum description length reduces to searching for a good probability distribution, and vice versa. Taking these considerations into account, MDL inference is formalized as: Φ� = argmin DL(Φ, D) </context>
</contexts>
<marker>Solomonoff, 1964</marker>
<rawString>Ray Solomonoff. 1964. A formal theory of inductive inference, parts 1 and 2. Information and Control, 7:1–22, 224–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Miguel Vilar</author>
<author>Enrique Vidal</author>
</authors>
<title>A recursive statistical translation model.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="6793" citStr="Vilar and Vidal, 2005" startWordPosition="1103" endWordPosition="1107">eparates the two sides of each pair. Assuming a uniform distribution over the K different symbols, each symbol would require −log2(1K ) bits to encode. We will thus require 3 bits to encode each of the 8 symbols in the example, and 33 bits to encode the whole serialized PB model (11 symbols). 3.2 Inference Procedure We now describe how to perform the maximization in Equation (2). In the case of PB models, this reduces to a search for the optimal phrase lexicon. Obviously, an exhaustive search over all possible sets of phrase pairs in the data is unfeasible in practice. Following the ideas in (Vilar and Vidal, 2005), we implement a search procedure that iteratively generalizes an initial PB model that perfectly fits the data. Let D = {fn, en}Nn=1 be a data set with N sentence pairs, where fn are sentences in the source language and en are their corresponding translation in the target language. Our initial PB model will be as follows: where the probability of each pair is given by the number of occurrences of the pair in the data divided by the number of occurrences of the source (or target) language sentence. To generalize this initial PB model, we need to identify parts of the existing phrase pairs that</context>
</contexts>
<marker>Vilar, Vidal, 2005</marker>
<rawString>Juan Miguel Vilar and Enrique Vidal. 2005. A recursive statistical translation model. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1182" citStr="Vogel et al., 1996" startWordPosition="161" endWordPosition="164">oretically wellfounded approach to directly infer phrase lexicons. Empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models. 1 Introduction Since their introduction at the beginning of the twenty-first century, phrase-based (PB) translation models (Koehn et al., 2003) have become the state-of-the-art for statistical machine translation (SMT). PB model provide a big leap in translation quality with respect to the previous word-based translation models (Brown et al., 1990; Vogel et al., 1996). However, despite their empirical success, inference procedures for PB models rely on a long pipeline of heuristics (Och and Ney, 2003) and mismatched learning models, such as the long outperformed word-based models. Latter stages of the pipeline cannot recover mistakes or omissions made in earlier stages which forces the individual stages to massively overgenerate hypotheses. This manifests as a huge redundancy in the inferred phrase lexicons, which in turn largely penalizes the efficiency of PB systems at run-time. The fact that PB models usually cannot generate the sentence pairs in which </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kingsley Zipf</author>
</authors>
<date>1935</date>
<note>The Psychobiology of</note>
<contexts>
<context position="14641" citStr="Zipf, 1935" startWordPosition="2494" endWordPosition="2495">rget words) of the SotA model and the MDL-based model for the News Commentaries corpus. We first observed that the length of the phrase pairs followed a completely different distribution depending on the inference procedure. Most of the phrase pairs of the MDL-based model translated one source word by one target word with an exponential decay in frequency for longer phrase pairs; a typical distribution of events in nat0 10 20 30 40 50 60 70 80 Length of the phrase pair (words) Figure 1: Histogram of lengths (source plus target words) for the phrase pairs in the inferred models. ural language (Zipf, 1935). Longer phrase pairs, about 45% of the total, contain sequences of words that only appear once in the corpus, and thus, they cannot be segmented in any way that leads to a reduction in description length. Although formally correct, long phrase pairs generalize poorly which explains the comparatively poor performance of MDL inference for the News Commentaries corpus. This problem was largely attenuated for EuTransI due to its simplicity. 5 Conclusions and Future Developments We have described a simple, unsupervised inference procedure for PB models that learns phrase lexicons by iteratively sp</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George Kingsley Zipf. 1935. The Psychobiology of</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>