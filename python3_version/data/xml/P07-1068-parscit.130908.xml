<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000487">
<title confidence="0.974503">
Semantic Class Induction and Coreference Resolution
</title>
<author confidence="0.989341">
Vincent Ng
</author>
<affiliation confidence="0.970543">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.89471">
Richardson, TX 75083-0688
</address>
<email confidence="0.998981">
vince@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943357142857">
This paper examines whether a learning-
based coreference resolver can be improved
using semantic class knowledge that is au-
tomatically acquired from a version of the
Penn Treebank in which the noun phrases
are labeled with their semantic classes. Ex-
periments on the ACE test data show that a
resolver that employs such induced semantic
class knowledge yields a statistically signif-
icant improvement of 2% in F-measure over
one that exploits heuristically computed se-
mantic class knowledge. In addition, the in-
duced knowledge improves the accuracy of
common noun resolution by 2-6%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99979716">
In the past decade, knowledge-lean approaches have
significantly influenced research in noun phrase
(NP) coreference resolution — the problem of deter-
mining which NPs refer to the same real-world en-
tity in a document. In knowledge-lean approaches,
coreference resolvers employ only morpho-syntactic
cues as knowledge sources in the resolution process
(e.g., Mitkov (1998), Tetreault (2001)). While these
approaches have been reasonably successful (see
Mitkov (2002)), Kehler et al. (2004) speculate that
deeper linguistic knowledge needs to be made avail-
able to resolvers in order to reach the next level of
performance. In fact, semantics plays a crucially im-
portant role in the resolution of common NPs, allow-
ing us to identify the coreference relation between
two lexically dissimilar common nouns (e.g., talks
and negotiations) and to eliminate George W. Bush
from the list of candidate antecedents of the city, for
instance. As a result, researchers have re-adopted
the once-popular knowledge-rich approach, investi-
gating a variety of semantic knowledge sources for
common noun resolution, such as the semantic rela-
tions between two NPs (e.g., Ji et al. (2005)), their
semantic similarity as computed using WordNet
(e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto
and Strube, 2006), and the contextual role played by
an NP (see Bean and Riloff (2004)).
Another type of semantic knowledge that has
been employed by coreference resolvers is the se-
mantic class (SC) of an NP, which can be used to dis-
allow coreference between semantically incompat-
ible NPs. However, learning-based resolvers have
not been able to benefit from having an SC agree-
ment feature, presumably because the method used
to compute the SC of an NP is too simplistic: while
the SC of a proper name is computed fairly accu-
rately using a named entity (NE) recognizer, many
resolvers simply assign to a common noun the first
(i.e., most frequent) WordNet sense as its SC (e.g.,
Soon et al. (2001), Markert and Nissim (2005)). It
is not easy to measure the accuracy of this heuristic,
but the fact that the SC agreement feature is not used
by Soon et al.’s decision tree coreference classifier
seems to suggest that the SC values of the NPs are
not computed accurately by this first-sense heuristic.
Motivated in part by this observation, we exam-
ine whether automatically induced semantic class
knowledge can improve the performance of a
learning-based coreference resolver, reporting eval-
uation results on the commonly-used ACE corefer-
</bodyText>
<page confidence="0.963298">
536
</page>
<note confidence="0.9258965">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536–543,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999109040816327">
ence corpus. Our investigation proceeds as follows. by the baseline resolver, improves the accuracy of
Train a classifier for labeling the SC of an NP. common noun resolution by about 5-8%.
In ACE, we are primarily concerned with classify- 2 Related Work
ing an NP as belonging to one of the ACE seman- Mention detection. Many ACE participants have
tic classes. For instance, part of the ACE Phase 2 also adopted a corpus-based approach to SC deter-
evaluation involves classifying an NP as PERSON, mination that is investigated as part of the mention
ORGANIZATION, GPE (a geographical-political re- detection (MD) task (e.g., Florian et al. (2006)).
gion), FACILITY, LOCATION, or OTHERS. We adopt Briefly, the goal of MD is to identify the boundary
a corpus-based approach to SC determination, re- of a mention, its mention type (e.g., pronoun, name),
casting the problem as a six-class classification task. and its semantic type (e.g., person, location). Un-
Derive two knowledge sources for coreference like them, (1) we do not perform the full MD task,
resolution from the induced SCs. The first as our goal is to investigate the role of SC knowl-
knowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not
(SCA). Following Soon et al. (2001), we represent use the ACE training data for acquiring our SC clas-
SCA as a binary value that indicates whether the in- sifier; instead, we use the BBN Entity Type Corpus
duced SCs of the two NPs involved are the same or (Weischedel and Brunstein, 2005), which consists of
not. The second KS is mention, which is represented all the Penn Treebank Wall Street Journal articles
as a binary value that indicates whether an NP be- with the ACE mentions manually identified and an-
longs to one of the five ACE SCs mentioned above. notated with their SCs. This provides us with a train-
Hence, the mention value of an NP can be readily ing set that is approximately five times bigger than
derived from its induced SC: the value is NO if its that of ACE. More importantly, the ACE participants
SC is OTHERS, and YES otherwise. This KS could do not evaluate the role of induced SC knowledge
be useful for ACE coreference, since ACE is con- in coreference resolution: many of them evaluate
cerned with resolving only NPs that are mentions. coreference performance on perfect mentions (e.g.,
Incorporate the two knowledge sources in a Luo et al. (2004)); and for those that do report per-
coreference resolver. Next, we investigate whether formance on automatically extracted mentions, they
these two KSs can improve a learning-based base- do not explain whether or how the induced SC infor-
line resolver that employs a fairly standard feature mation is used in their coreference algorithms.
set. Since (1) the two KSs can each be repre- Joint probabilistic models of coreference. Re-
sented in the resolver as a constraint (for filtering cently, there has been a surge of interest in im-
non-mentions or disallowing coreference between proving coreference resolution by jointly modeling
semantically incompatible NPs) or as a feature, and coreference with a related task such as MD (e.g.,
(2) they can be applied to the resolver in isolation or Daum´e and Marcu (2005)). However, joint models
in combination, we have eight ways of incorporating typically need to be trained on data that is simulta-
these KSs into the baseline resolver. neously annotated with information required by all
In our experiments on the ACE Phase 2 coref- of the underlying models. For instance, Daum´e and
erence corpus, we found that (1) our SC induc- Marcu’s model assumes as input a corpus annotated
tion method yields a significant improvement of 2% with both MD and coreference information. On the
in accuracy over Soon et al.’s first-sense heuristic other hand, we tackle coreference and SC induction
method as described above; (2) the coreference re- separately (rather than jointly), since we train our SC
solver that incorporates our induced SC knowledge determination model on the BBN Entity Type Cor-
by means of the two KSs mentioned above yields pus, where coreference information is absent.
a significant improvement of 2% in F-measure over 3 Semantic Class Induction
the resolver that exploits the SC knowledge com- This section describes how we train and evaluate a
puted by Soon et al.’s method; (3) the mention KS, classifier for determining the SC of an NP.
when used in the baseline resolver as a constraint,
improves the resolver by approximately 5-7% in F-
measure; and (4) SCA, when employed as a feature
537
</bodyText>
<subsectionHeader confidence="0.999767">
3.1 Training the Classifier
</subsectionHeader>
<bodyText confidence="0.999683173913043">
Training corpus. As mentioned before, we use
the BBN Entity Type Corpus for training the SC
classifier. This corpus was originally developed to
support the ACE and AQUAINT programs and con-
sists of annotations of 12 named entity types and
nine nominal entity types. Nevertheless, we will
only make use of the annotations of the five ACE
semantic types that are present in our ACE Phase 2
coreference corpus, namely, PERSON, ORGANIZA-
TION, GPE, FACILITY, and LOCATION.
Training instance creation. We create one train-
ing instance for each proper or common NP (ex-
tracted using an NP chunker and an NE recognizer)
in each training text. Each instance is represented
by a set of lexical, syntactic, and semantic features,
as described below. If the NP under consideration is
annotated as one of the five ACE SCs in the corpus,
then the classification of the associated training in-
stance is simply the ACE SC value of the NP. Other-
wise, the instance is labeled as OTHERS. This results
in 310063 instances in the training set.
Features. We represent the training instance for a
noun phrase, NPZ, using seven types of features:
</bodyText>
<listItem confidence="0.992193">
(1) WORD: For each word w in NPZ, we create a
WORD feature whose value is equal to w. No fea-
tures are created from stopwords, however.
(2) SUBJ VERB: If NPZ is involved in a subject-
verb relation, we create a SUBJ VERB feature whose
value is the verb participating in the relation. We
use Lin’s (1998b) MINIPAR dependency parser to
extract grammatical relations. Our motivation here
is to coarsely model subcategorization.
(3) VERB OBJ: A VERB OBJ feature is created in
a similar fashion as SUBJ VERB if NPZ participates
in a verb-object relation. Again, this represents our
attempt to coarsely model subcategorization.
(4) NE: We use BBN’s IdentiFinder (Bikel et al.,
1999), a MUC-style NE recognizer to determine the
NE type of NPZ. If NPZ is determined to be a PERSON
or ORGANIZATION, we create an NE feature whose
</listItem>
<bodyText confidence="0.760861166666667">
value is simply its MUC NE type. However, if NPZ
is determined to be a LOCATION, we create a feature
with value GPE (because most of the MUC LOCA-
TION NEs are ACE GPE NEs). Otherwise, no NE
feature will be created (because we are not interested
in the other MUC NE types).
</bodyText>
<table confidence="0.996560111111111">
ACE SC Keywords
PERSON person
ORGANIZATION social group
FACILITY establishment, construction, building, facil-
ity, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOCATION dry land, region, landmass, body of water,
geographical area, geological formation
</table>
<tableCaption confidence="0.770047">
Table 1: List of keywords used in WordNet search
for generating WN CLASS features.
</tableCaption>
<bodyText confidence="0.834378210526316">
(5) WN CLASS: For each keyword w shown in the
right column of Table 1, we determine whether the
head noun of NPZ is a hyponym of w in WordNet,
using only the first WordNet sense of NPZ.1 If so,
we create a WN CLASS feature with w as its value.
These keywords are potentially useful features be-
cause some of them are subclasses of the ACE SCs
shown in the left column of Table 1, while others
appear to be correlated with these ACE SCs.2
(6) INDUCED CLASS: Since the first-sense heuris-
tic used in the previous feature may not be accurate
in capturing the SC of an NP, we employ a corpus-
based method for inducing SCs that is motivated by
research in lexical semantics (e.g., Hearst (1992)).
Given a large, unannotated corpus3, we use Identi-
Finder to label each NE with its NE type and MINI-
PAR to extract all the appositive relations. An ex-
ample extraction would be &lt;Eastern Airlines, the
carrier&gt;, where the first entry is a proper noun la-
beled with either one of the seven MUC-style NE
types4 or OTHERS5 and the second entry is a com-
mon noun. We then infer the SC of a common
noun as follows: (1) we compute the probability
that the common noun co-occurs with each of the
eight NE types6 based on the extracted appositive
relations, and (2) if the most likely NE type has a
co-occurrence probability above a certain threshold
(we set it to 0.7), we create a INDUCED CLASS fea-
1This is motivated by Lin’s (1998c) observation that a coref-
erence resolver that employs only the first WordNet sense per-
forms slightly better than one that employs more than one sense.
2The keywords are obtained via our experimentation with
WordNet and the ACE SCs of the NPs in the ACE training data.
3We used (1) the BLLIP corpus (30M words), which con-
sists of WSJ articles from 1987 to 1989, and (2) the Reuters
Corpus (3.7GB data), which has 806,791 Reuters articles.
4Person, organization, location, date, time, money, percent.
TThis indicates the proper noun is not a MUC NE.
</bodyText>
<footnote confidence="0.799469">
6For simplicity, OTHERS is viewed as an NE type here.
</footnote>
<page confidence="0.993938">
538
</page>
<bodyText confidence="0.999886953488372">
ture for NPi whose value is the most likely NE type.
(7) NEIGHBOR: Research in lexical semantics sug-
gests that the SC of an NP can be inferred from its
distributionally similar NPs (see Lin (1998a)). Mo-
tivated by this observation, we create for each of
NPi’s ten most semantically similar NPs a NEIGH-
BOR feature whose value is the surface string of
the NP. To determine the ten nearest neighbors, we
use the semantic similarity values provided by Lin’s
dependency-based thesaurus, which is constructed
using a distributional approach combined with an
information-theoretic definition of similarity.
Learning algorithms. We experiment with four
learners commonly employed in language learning:
Decision List (DL): We use the DL learner as de-
scribed in Collins and Singer (1999), motivated by
its success in the related tasks of word sense dis-
ambiguation (Yarowsky, 1995) and NE classifica-
tion (Collins and Singer, 1999). We apply add-one
smoothing to smooth the class posteriors.
1-Nearest Neighbor (1-NN): We use the 1-NN clas-
sifier as implemented in TiMBL (Daelemans et al.,
2004), employing dot product as the similarity func-
tion (which defines similarity as the number of com-
mon feature-value pairs between two instances). All
other parameters are set to their default values.
Maximum Entropy (ME): We employ Lin’s ME
implementation7, using a Gaussian prior for smooth-
ing and running the algorithm until convergence.
Naive Bayes (NB): We use an in-house implementa-
tion of NB, using add-one smoothing to smooth the
class priors and the class-conditional probabilities.
In addition, we train an SVM classifier for SC
determination by combining the output of five clas-
sification methods: DL, 1-NN, ME, NB, and Soon
et al.’s method as described in the introduction,8
with the goal of examining whether SC classifica-
tion accuracy can be improved by combining the
output of individual classifiers in a supervised man-
ner. Specifically, we (1) use 80% of the instances
generated from the BBN Entity Type Corpus to train
the four classifiers; (2) apply the four classifiers and
Soon et al.’s method to independently make predic-
</bodyText>
<footnote confidence="0.9911692">
7See http://www.cs.ualberta.ca/∼lindek/downloads.htm
8In our implementation of Soon’s method, we label an in-
stance as OTHERS if no NE or WN CLASS feature is generated;
otherwise its label is the value of the NE feature or the ACE SC
that has the WN CLASS features as its keywords (see Table 1).
</footnote>
<table confidence="0.997432">
PER ORG GPE FAC LOC OTH
Training 19.8 9.6 11.4 1.6 1.2 56.3
Test 19.5 9.0 9.6 1.8 1.1 59.0
</table>
<tableCaption confidence="0.99697">
Table 2: Distribution of SCs in the ACE corpus.
</tableCaption>
<bodyText confidence="0.999669125">
tions for the remaining 20% of the instances; and (3)
train an SVM classifier (using the LIBSVM pack-
age (Chang and Lin, 2001)) on these 20% of the in-
stances, where each instance, i, is represented by a
set of 31 binary features. More specifically, let Li =
141, 42, 43, 44, 451 be the set of predictions that we
obtained for i in step (2). To represent i, we generate
one feature from each non-empty subset of Li.
</bodyText>
<subsectionHeader confidence="0.999635">
3.2 Evaluating the Classifiers
</subsectionHeader>
<bodyText confidence="0.974648454545455">
For evaluation, we use the ACE Phase 2 coreference
corpus, which comprises 422 training texts and 97
test texts. Each text has its mentions annotated with
their ACE SCs. We create our test instances from
the ACE texts in the same way as the training in-
stances described in Section 3.1. Table 2 shows the
percentages of instances corresponding to each SC.
Table 3 shows the accuracy of each classifier (see
row 1) for the ACE training set (54641 NPs, with
16414 proper NPs and 38227 common NPs) and the
ACE test set (13444 NPs, with 3713 proper NPs and
9731 common NPs), as well as their performance on
the proper NPs (row 2) and the common NPs (row
3). We employ as our baseline system the Soon et al.
method (see Footnote 8), whose accuracy is shown
under the Soon column. As we can see, DL, 1-NN,
and SVM show a statistically significant improve-
ment over the baseline for both data sets, whereas
ME and NB perform significantly worse.9 Addi-
tional experiments are needed to determine the rea-
son for ME and NB’s poor performance.
In an attempt to gain additional insight into the
performance contribution of each type of features,
we conduct feature ablation experiments using the
DL classifier (DL is chosen simply because it is the
best performer on the ACE training set). Results are
shown in Table 4, where each row shows the accu-
racy of the DL trained on all types of features except
for the one shown in that row (All), as well as accu-
racies on the proper NPs (PN) and the common NPs
(CN). For easy reference, the accuracy of the DL
9We use Noreen’s (1989) Approximate Randomization test
for significance testing, with P set to .05 unless otherwise stated.
</bodyText>
<page confidence="0.943094">
539
</page>
<table confidence="0.977563875">
Training Set Test Set
Soon DL 1-NN ME NB SVM Soon DL 1-NN ME NB SVM
Overall 83.1 85.0 84.0 54.5 71.3 84.2 81.1 82.9 83.1 53.0 70.3 83.3
Proper NPs 83.1 84.1 81.0 54.2 65.5 82.2 79.6 82.0 79.8 55.8 64.4 80.4
Common NPs 83.1 85.4 85.2 54.6 73.8 85.1 81.6 83.3 84.3 51.9 72.6 84.4
1
2
3
</table>
<tableCaption confidence="0.963893">
Table 3: SC classification accuracies of different methods for the ACE training set and test set.
</tableCaption>
<table confidence="0.9999442">
Feature Type Training Set PN Test Set
PN CN All CN All
All features 84.1 85.4 85.0 82.0 83.3 82.9
- WORD 84.2 85.4 85.0 82.0 83.1 82.8
- SUBJ VERB 84.1 85.4 85.0 82.0 83.3 82.9
-VERB OBJ 84.1 85.4 85.0 82.0 83.3 82.9
- NE 72.9 85.3 81.6 74.1 83.2 80.7
- WN CLASS 84.1 85.9 85.3 81.9 84.1 83.5
- INDUCED C 84.0 85.6 85.1 82.0 83.6 83.2
- NEIGHBOR 82.8 84.9 84.3 80.2 82.9 82.1
</table>
<tableCaption confidence="0.958955">
Table 4: Results for feature ablation experiments.
</tableCaption>
<table confidence="0.999873222222222">
Feature Type Training Set PN Test Set
PN CN All CN All
WORD 64.0 83.9 77.9 66.5 82.4 78.0
SUBJ VERB 24.0 70.2 56.3 28.8 70.5 59.0
VERB OBJ 24.0 70.2 56.3 28.8 70.5 59.0
NE 81.1 72.1 74.8 78.4 71.4 73.3
WN CLASS 25.6 78.8 62.8 30.4 78.9 65.5
INDUCED C 25.8 81.1 64.5 30.0 80.3 66.3
NEIGHBOR 67.7 85.8 80.4 68.0 84.4 79.8
</table>
<tableCaption confidence="0.99966">
Table 5: Accuracies of single-feature classifiers.
</tableCaption>
<bodyText confidence="0.99995044">
classifier trained on all types of features is shown
in row 1 of the table. As we can see, accuracy drops
significantly with the removal of NE and NEIGHBOR.
As expected, removing NE precipitates a large drop
in proper NP accuracy; somewhat surprisingly, re-
moving NEIGHBOR also causes proper NP accuracy
to drop significantly. To our knowledge, there are no
prior results on using distributionally similar neigh-
bors as features for supervised SC induction.
Note, however, that these results do not imply
that the remaining feature types are not useful for
SC classification; they simply suggest, for instance,
that WORD is not important in the presence of other
feature types. To get a better idea of the utility of
each feature type, we conduct another experiment in
which we train seven classifiers, each of which em-
ploys exactly one type of features. The accuracies
of these classifiers are shown in Table 5. As we can
see, NEIGHBOR has the largest contribution. This
again demonstrates the effectiveness of a distribu-
tional approach to semantic similarity. Its superior
performance to WORD, the second largest contribu-
tor, could be attributed to its ability to combat data
sparseness. The NE feature, as expected, is crucial
to the classification of proper NPs.
</bodyText>
<sectionHeader confidence="0.971938" genericHeader="method">
4 Application to Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999463285714286">
We can now derive from the induced SC informa-
tion two KSs — semantic class agreement and men-
tion — and incorporate them into our learning-based
coreference resolver in eight different ways, as de-
scribed in the introduction. This section examines
whether our coreference resolver can benefit from
any of the eight ways of incorporating these KSs.
</bodyText>
<subsectionHeader confidence="0.975564">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999926">
As in SC induction, we use the ACE Phase 2 coref-
erence corpus for evaluation purposes, acquiring the
coreference classifiers on the 422 training texts and
evaluating their output on the 97 test texts. We re-
port performance in terms of two metrics: (1) the F-
measure score as computed by the commonly-used
MUC scorer (Vilain et al., 1995), and (2) the accu-
racy on the anaphoric references, computed as the
fraction of anaphoric references correctly resolved.
Following Ponzetto and Strube (2006), we consider
an anaphoric reference, NPi, correctly resolved if NPi
and its closest antecedent are in the same corefer-
ence chain in the resulting partition. In all of our
experiments, we use NPs automatically extracted by
an in-house NP chunker and IdentiFinder.
</bodyText>
<subsectionHeader confidence="0.998042">
4.2 The Baseline Coreference System
</subsectionHeader>
<bodyText confidence="0.999614545454545">
Our baseline coreference system uses the C4.5 deci-
sion tree learner (Quinlan, 1993) to acquire a classi-
fier on the training texts for determining whether two
NPs are coreferent. Following previous work (e.g.,
Soon et al. (2001) and Ponzetto and Strube (2006)),
we generate training instances as follows: a positive
instance is created for each anaphoric NP, NPj, and
its closest antecedent, NPi; and a negative instance is
created for NPj paired with each of the intervening
NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is rep-
resented by 33 lexical, grammatical, semantic, and
</bodyText>
<page confidence="0.891534">
540
</page>
<bodyText confidence="0.99979187755102">
positional features that have been employed by high- we can see, the baseline achieves an F-measure of
performing resolvers such as Ng and Cardie (2002) 57.0 and a resolution accuracy of 48.4.
and Yang et al. (2003), as described below. To get a better sense of how strong our baseline
Lexical features. Nine features allow different is, we re-implement the Soon et al. (2001) corefer-
types of string matching operations to be performed ence resolver. This simply amounts to replacing the
on the given pair of NPs, NP,, and NPy10, including 33 features in the baseline resolver with the 12 fea-
(1) exact string match for pronouns, proper nouns, tures employed by Soon et al.’s system. Results of
and non-pronominal NPs (both before and after de- our Duplicated Soon et al. system are shown in row
terminers are removed); (2) substring match for 2 of Tables 6 and 7. In comparison to our baseline,
proper nouns and non-pronominal NPs; and (3) head the Duplicated Soon et al. system performs worse
noun match. In addition, one feature tests whether according to both metrics, and although the drop in
all the words that appear in one NP also appear in F-measure seems moderate, the performance differ-
the other NP. Finally, a nationality matching feature ence is in fact highly significant (p=0.002).11
is used to match, for instance, British with Britain. 4.3 Coreference with Induced SC Knowledge
Grammatical features. 22 features test the gram- Recall from the introduction that our investigation of
matical properties of one or both of the NPs. These the role of induced SC knowledge in learning-based
include ten features that test whether each of the two coreference resolution proceeds in three steps:
NPs is a pronoun, a definite NP, an indefinite NP, a Label the SC of each NP in each ACE document.
nested NP, and a clausal subject. A similar set of If a noun phrase, NPZ, is a proper or common NP,
five features is used to test whether both NPs are then its SC value is determined using an SC classi-
pronouns, definite NPs, nested NPs, proper nouns, fier that we acquired in Section 3. On the other hand,
and clausal subjects. In addition, five features deter- if NPZ is a pronoun, then we will be conservative and
mine whether the two NPs are compatible with re- posit its SC value as UNCONSTRAINED (i.e., it is se-
spect to gender, number, animacy, and grammatical mantically compatible with all other NPs).12
role. Furthermore, two features test whether the two Derive two KSs from the induced SCs. Recall that
NPs are in apposition or participate in a predicate our first KS, Mention, is defined on an NP; its value
nominal construction (i.e., the IS-A relation). is YES if the induced SC of the NP is not OTHERS,
Semantic features. Motivated by Soon et al. and NO otherwise. On the other hand, our second
(2001), we have a semantic feature that tests whether KS, SCA, is defined on a pair of NPs; its value is
one NP is a name alias or acronym of the other. YES if the two NPs have the same induced SC that
Positional feature. We have a feature that com- is not OTHERS, and NO otherwise.
putes the distance between the two NPs in sentences. Incorporate the two KSs into the baseline re-
After training, the decision tree classifier is used solver. Recall that there are eight ways of incor-
to select an antecedent for each NP in a test text. porating these two KSs into our resolver: they can
Following Soon et al. (2001), we select as the an- each be represented as a constraint or as a feature,
tecedent of each NP, NPS, the closest preceding NP and they can be applied to the resolver in isolation
that is classified as coreferent with NPS. If no such and in combination. Constraints are applied dur-
NP exists, no antecedent is selected for NPS. ing the antecedent selection step. Specifically, when
Row 1 of Table 6 and Table 7 shows the results employed as a constraint, the Mention KS disallows
of the baseline system in terms of F-measure (F) coreference between two NPs if at least one of them
and accuracy in resolving 4599 anaphoric references has a Mention value of NO, whereas the SCA KS dis-
(All), respectively. For further analysis, we also re- allows coreference if the SCA value of the two NPs
port the corresponding recall (R) and precision (P) involved is NO. When encoded as a feature for the
in Table 6, as well as the accuracies of the system in resolver, the Mention feature for an NP pair has the
resolving 1769 pronouns (PRO), 1675 proper NPs
(PN), and 1155 common NPs (CN) in Table 7. As
11Again, we use Approximate Randomization with P=.05.
12The only exception is pronouns whose SC value can be eas-
ily determined to be PERSON (e.g., he, him, his, himself).
</bodyText>
<table confidence="0.947791">
10We assume that NP. precedes NP, in the associated text.
541
System Variation R P F R P F R P F R P F
Baseline system 60.9 53.6 57.0 – – – – – – – – –
Duplicated Soon et al. 56.1 54.4 55.3 – – – – – – – – –
Add to the Baseline Soon’s SC Method Decision List SVM Perfect Information
Mention(C) only 56.9 69.7 62.6 59.5 70.6 64.6 59.5 70.7 64.6 61.2 83.1 70.5
Mention(F) only 60.9 54.0 57.2 61.2 52.9 56.7 60.9 53.6 57.0 62.3 33.7 43.8
SCA(C) only 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.7 64.3 61.3 86.1 71.6
SCA(F) only 62.0 52.8 57.0 62.5 53.5 57.6 63.0 53.3 57.7 71.1 33.0 45.1
Mention(C) + SCA(C) 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.8 64.3 61.3 86.1 71.6
Mention(C) + SCA(F) 58.2 66.4 62.0 60.9 66.8 63.7 61.4 66.5 63.8 71.1 76.7 73.8
Mention(F) + SCA(C) 56.4 69.8 62.4 57.7 71.3 63.8 58.9 70.6 64.3 62.7 85.3 72.3
Mention(F) + SCA(F) 62.0 52.7 57.0 62.6 52.8 57.3 63.2 52.6 57.4 71.8 30.3 42.6
</table>
<figure confidence="0.9003246">
1
2
3
4
5
6
7
8
9
10
</figure>
<tableCaption confidence="0.992881">
Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.
</tableCaption>
<table confidence="0.962167833333333">
System Variation PRO PN CN All PRO PN CN All PRO PN CN All
Baseline system 59.2 54.8 22.5 48.4 – – – – – – – –
Duplicated Soon et al. 53.4 45.7 16.9 41.4 – – – – – – – –
Add to the Baseline Soon’s SC Method Decision List SVM
Mention(C) only 58.5 51.3 16.5 45.3 59.1 54.1 20.2 47.5 59.1 53.9 20.6 47.5
Mention(F) only 59.2 55.0 22.5 48.5 59.2 56.1 22.4 48.8 59.4 55.2 22.6 48.6
SCA(C) only 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 52.0 19.6 46.3
SCA(F) only 59.2 54.9 27.8 49.7 60.4 56.7 30.1 51.5 60.8 56.4 29.4 51.3
Mention(C) + SCA(C) 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 51.9 19.5 46.3
Mention(C) + SCA(F) 58.9 52.0 22.3 47.2 60.2 55.9 28.1 50.6 60.7 55.3 27.4 50.4
Mention(F) + SCA(C) 58.1 50.3 16.3 44.8 58.1 52.4 16.7 45.6 58.6 52.4 19.7 46.6
Mention(F) + SCA(F) 59.2 55.0 27.6 49.7 60.4 56.8 30.1 51.5 60.8 56.5 29.5 51.4
</table>
<figure confidence="0.9256688">
1
2
3
4
5
6
7
8
9
10
</figure>
<tableCaption confidence="0.982057">
Table 7: Resolution accuracies for the ACE test set.
</tableCaption>
<bodyText confidence="0.99232986">
value YES if and only if the Mention value for both
NPs is YES, whereas the SCA feature for an NP pair
has its value taken from the SCA KS.
Now, we can evaluate the impact of the two KSs
on the performance of our baseline resolver. Specifi-
cally, rows 3-6 of Tables 6 and 7 show the F-measure
and the resolution accuracy, respectively, when ex-
actly one of the two KSs is employed by the baseline
as either a constraint (C) or a feature (F), and rows
7-10 of the two tables show the results when both
KSs are applied to the baseline. Furthermore, each
row of Table 6 contains four sets of results, each of
which corresponds to a different method for deter-
mining the SC value of an NP. For instance, the first
set is obtained by using Soon et al.’s method as de-
scribed in Footnote 8 to compute SC values, serving
as sort of a baseline for our results using induced SC
values. The second and third sets are obtained based
on the SC values computed by the DL and the SVM
classifier, respectively.13 The last set corresponds to
an oracle experiment in which the resolver has ac-
cess to perfect SC information. Rows 3-10 of Table
13Results using other learners are not shown due to space lim-
itations. DL and SVM are chosen simply because they achieve
the highest SC classification accuracies on the ACE training set.
7 can be interpreted in a similar manner.
From Table 6, we can see that (1) in comparison to
the baseline, F-measure increases significantly in the
five cases where at least one of the KSs is employed
as a constraint by the resolver, and such improve-
ments stem mainly from significant gains in preci-
sion; (2) in these five cases, the resolvers that use
SCs induced by DL and SVM achieve significantly
higher F-measure scores than their counterparts that
rely on Soon’s method for SC determination; and (3)
none of the resolvers appears to benefit from SCA in-
formation whenever mention is used as a constraint.
Moreover, note that even with perfectly computed
SC information, the performance of the baseline sys-
tem does not improve when neither MD nor SCA is
employed as a constraint. These results provide fur-
ther evidence that the decision tree learner is not ex-
ploiting these two semantic KSs in an optimal man-
ner, whether they are computed automatically or per-
fectly. Hence, in machine learning for coreference
resolution, it is important to determine not only what
linguistic KSs to use, but also how to use them.
While the coreference results in Table 6 seem to
suggest that SCA and mention should be employed
as constraints, the resolution results in Table 7 sug-
</bodyText>
<page confidence="0.989489">
542
</page>
<bodyText confidence="0.999982666666667">
gest that SCA is better encoded as a feature. Specifi-
cally, (1) in comparison to the baseline, the accuracy
of common NP resolution improves by about 5-8%
when SCA is encoded as a feature; and (2) whenever
SCA is employed as a feature, the overall resolution
accuracy is significantly higher for resolvers that use
SCs induced by DL and SVM than those that rely on
Soon’s method for SC determination, with improve-
ments in resolution observed on all three NP types.
Overall, these results provide suggestive evidence
that both KSs are useful for learning-based corefer-
ence resolution. In particular, mention should be em-
ployed as a constraint, whereas SCA should be used
as a feature. Interestingly, this is consistent with the
results that we obtained when the resolver has access
to perfect SC information (see Table 6), where the
highest F-measure is achieved by employing men-
tion as a constraint and SCA as a feature.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999990357142857">
We have shown that (1) both mention and SCA can
be usefully employed to improve the performance
of a learning-based coreference system, and (2) em-
ploying SC knowledge induced in a supervised man-
ner enables a resolver to achieve better performance
than employing SC knowledge computed by Soon
et al.’s simple method. In addition, we found that
the MUC scoring program is unable to reveal the
usefulness of the SCA KS, which, when encoded
as a feature, substantially improves the accuracy of
common NP resolution. This underscores the im-
portance of reporting both resolution accuracy and
clustering-level accuracy when analyzing the perfor-
mance of a coreference resolver.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999567985714286">
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proc. of
HLT/NAACL, pages 297–304.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what’s in a name. Machine Learning
34(1–3):211–231.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP/VLC.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2004. TiMBL: Tilburg Memory Based Learner, ver-
sion 5.1, Reference Guide. ILK Technical Report.
H. Daum´e III and D. Marcu. 2005. A large-scale exploration
of effective global features for a joint entity detection and
tracking model. In Proc. of HLT/EMNLP, pages 97–104.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Fac-
torizing complex models: A case study in mention detection.
In Proc. of COLING/ACL, pages 473–480.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proc. of COLING.
H. Ji, D. Westbrook, and R. Grishman. 2005. Using seman-
tic relations to refine coreference decisions. In Proc. of
HLT/EMNLP, pages 17–24.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proc. ofNAACL, pages 289–296.
D. Lin. 1998a. Automatic retrieval and clustering of similar
words. In Proc. of COLING/ACL, pages 768–774.
D. Lin. 1998b. Dependency-based evaluation of MINIPAR. In
Proc. of the LREC Workshop on the Evaluation of Parsing
Systems, pages 48–56.
D. Lin. 1998c. Using collocation statistics in information ex-
traction. In Proc. ofMUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.
2004. A mention-synchronous coreference resolution algo-
rithm based on the Bell tree. In Proc. of the ACL.
K. Markert and M. Nissim. 2005. Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367–402.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proc. of COLING/ACL, pages 869–875.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proc. of the ACL.
E. W. Noreen. 1989. Computer Intensive Methods for Testing
Hypothesis: An Introduction. John Wiley &amp; Sons.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. of HLT/NAACL, pages 192–199.
J. R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Compu-
tational Linguistics, 27(4):521–544.
J. Tetreault. 2001. A corpus-based evaluation of centering and
pronoun resolution. Computational Linguistics, 27(4).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proc. ofMUC-6, pages 45–52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun corefer-
ence and entity type corpus. Linguistica Data Consortium.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
resolution using competitive learning approach. In Proc. of
the ACL, pages 176–183.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of the ACL.
</reference>
<page confidence="0.998916">
543
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808150">
<title confidence="0.999612">Semantic Class Induction and Coreference Resolution</title>
<author confidence="0.99959">Vincent Ng</author>
<affiliation confidence="0.9996535">Human Language Technology Research Institute University of Texas at Dallas</affiliation>
<address confidence="0.948131">Richardson, TX 75083-0688</address>
<email confidence="0.999669">vince@hlt.utdallas.edu</email>
<abstract confidence="0.987093533333333">This paper examines whether a learningbased coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the Penn Treebank in which the noun phrases are labeled with their semantic classes. Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bean</author>
<author>E Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="2150" citStr="Bean and Riloff (2004)" startWordPosition="321" endWordPosition="324">reference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>D. Bean and E. Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<pages>34--1</pages>
<contexts>
<context position="9875" citStr="Bikel et al., 1999" startWordPosition="1610" endWordPosition="1613">ture whose value is equal to w. No features are created from stopwords, however. (2) SUBJ VERB: If NPZ is involved in a subjectverb relation, we create a SUBJ VERB feature whose value is the verb participating in the relation. We use Lin’s (1998b) MINIPAR dependency parser to extract grammatical relations. Our motivation here is to coarsely model subcategorization. (3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPZ participates in a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN’s IdentiFinder (Bikel et al., 1999), a MUC-style NE recognizer to determine the NE type of NPZ. If NPZ is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type. However, if NPZ is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCATION NEs are ACE GPE NEs). Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types). ACE SC Keywords PERSON person ORGANIZATION social group FACILITY establishment, construction, building, facility, workplace GPE country, province, government, town, city, administratio</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning 34(1–3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="15375" citStr="Chang and Lin, 2001" startWordPosition="2550" endWordPosition="2553">sifiers and Soon et al.’s method to independently make predic7See http://www.cs.ualberta.ca/∼lindek/downloads.htm 8In our implementation of Soon’s method, we label an instance as OTHERS if no NE or WN CLASS feature is generated; otherwise its label is the value of the NE feature or the ACE SC that has the WN CLASS features as its keywords (see Table 1). PER ORG GPE FAC LOC OTH Training 19.8 9.6 11.4 1.6 1.2 56.3 Test 19.5 9.0 9.6 1.8 1.1 59.0 Table 2: Distribution of SCs in the ACE corpus. tions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package (Chang and Lin, 2001)) on these 20% of the instances, where each instance, i, is represented by a set of 31 binary features. More specifically, let Li = 141, 42, 43, 44, 451 be the set of predictions that we obtained for i in step (2). To represent i, we generate one feature from each non-empty subset of Li. 3.2 Evaluating the Classifiers For evaluation, we use the ACE Phase 2 coreference corpus, which comprises 422 training texts and 97 test texts. Each text has its mentions annotated with their ACE SCs. We create our test instances from the ACE texts in the same way as the training instances described in Section</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP/VLC.</booktitle>
<contexts>
<context position="13471" citStr="Collins and Singer (1999)" startWordPosition="2232" endWordPosition="2235"> distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi’s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin’s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999). We apply add-one smoothing to smooth the class posteriors. 1-Nearest Neighbor (1-NN): We use the 1-NN classifier as implemented in TiMBL (Daelemans et al., 2004), employing dot product as the similarity function (which defines similarity as the number of common feature-value pairs between two instances). All other parameters are set to their default values. Maximum Entropy (ME): We employ Lin’s ME implementation7, using a Gaussian prior for smoothing a</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. of EMNLP/VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide.</title>
<date>2004</date>
<tech>ILK Technical Report.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. ILK Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>97--104</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>H. Daum´e III and D. Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proc. of HLT/EMNLP, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>I Zitouni</author>
</authors>
<title>Factorizing complex models: A case study in mention detection.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>473--480</pages>
<contexts>
<context position="4137" citStr="Florian et al. (2006)" startWordPosition="643" endWordPosition="646">ation proceeds as follows. by the baseline resolver, improves the accuracy of Train a classifier for labeling the SC of an NP. common noun resolution by about 5-8%. In ACE, we are primarily concerned with classify- 2 Related Work ing an NP as belonging to one of the ACE seman- Mention detection. Many ACE participants have tic classes. For instance, part of the ACE Phase 2 also adopted a corpus-based approach to SC deterevaluation involves classifying an NP as PERSON, mination that is investigated as part of the mention ORGANIZATION, GPE (a geographical-political re- detection (MD) task (e.g., Florian et al. (2006)). gion), FACILITY, LOCATION, or OTHERS. We adopt Briefly, the goal of MD is to identify the boundary a corpus-based approach to SC determination, re- of a mention, its mention type (e.g., pronoun, name), casting the problem as a six-class classification task. and its semantic type (e.g., person, location). UnDerive two knowledge sources for coreference like them, (1) we do not perform the full MD task, resolution from the induced SCs. The first as our goal is to investigate the role of SC knowlknowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not </context>
</contexts>
<marker>Florian, Jing, Kambhatla, Zitouni, 2006</marker>
<rawString>R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006. Factorizing complex models: A case study in mention detection. In Proc. of COLING/ACL, pages 473–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="11366" citStr="Hearst (1992)" startWordPosition="1873" endWordPosition="1874"> determine whether the head noun of NPZ is a hyponym of w in WordNet, using only the first WordNet sense of NPZ.1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs.2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)). Given a large, unannotated corpus3, we use IdentiFinder to label each NE with its NE type and MINIPAR to extract all the appositive relations. An example extraction would be &lt;Eastern Airlines, the carrier&gt;, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types4 or OTHERS5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>D Westbrook</author>
<author>R Grishman</author>
</authors>
<title>Using semantic relations to refine coreference decisions.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1957" citStr="Ji et al. (2005)" startWordPosition="290" endWordPosition="293">available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a pro</context>
</contexts>
<marker>Ji, Westbrook, Grishman, 2005</marker>
<rawString>H. Ji, D. Westbrook, and R. Grishman. 2005. Using semantic relations to refine coreference decisions. In Proc. of HLT/EMNLP, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="1280" citStr="Kehler et al. (2004)" startWordPosition="182" endWordPosition="185">emantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution — the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resol</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proc. ofNAACL, pages 289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="12890" citStr="Lin (1998" startWordPosition="2146" endWordPosition="2147">mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. TThis indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi’s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin’s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retrieval and clustering of similar words. In Proc. of COLING/ACL, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proc. of the LREC Workshop on the Evaluation of Parsing Systems,</booktitle>
<pages>48--56</pages>
<contexts>
<context position="12890" citStr="Lin (1998" startWordPosition="2146" endWordPosition="2147">mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. TThis indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi’s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin’s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Dependency-based evaluation of MINIPAR. In Proc. of the LREC Workshop on the Evaluation of Parsing Systems, pages 48–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using collocation statistics in information extraction.</title>
<date>1998</date>
<booktitle>In Proc. ofMUC-7.</booktitle>
<contexts>
<context position="12890" citStr="Lin (1998" startWordPosition="2146" endWordPosition="2147">mentation with WordNet and the ACE SCs of the NPs in the ACE training data. 3We used (1) the BLLIP corpus (30M words), which consists of WSJ articles from 1987 to 1989, and (2) the Reuters Corpus (3.7GB data), which has 806,791 Reuters articles. 4Person, organization, location, date, time, money, percent. TThis indicates the proper noun is not a MUC NE. 6For simplicity, OTHERS is viewed as an NE type here. 538 ture for NPi whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)). Motivated by this observation, we create for each of NPi’s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin’s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998c. Using collocation statistics in information extraction. In Proc. ofMUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="5910" citStr="Luo et al. (2004)" startWordPosition="950" endWordPosition="953">e. notated with their SCs. This provides us with a trainHence, the mention value of an NP can be readily ing set that is approximately five times bigger than derived from its induced SC: the value is NO if its that of ACE. More importantly, the ACE participants SC is OTHERS, and YES otherwise. This KS could do not evaluate the role of induced SC knowledge be useful for ACE coreference, since ACE is con- in coreference resolution: many of them evaluate cerned with resolving only NPs that are mentions. coreference performance on perfect mentions (e.g., Incorporate the two knowledge sources in a Luo et al. (2004)); and for those that do report percoreference resolver. Next, we investigate whether formance on automatically extracted mentions, they these two KSs can improve a learning-based base- do not explain whether or how the induced SC inforline resolver that employs a fairly standard feature mation is used in their coreference algorithms. set. Since (1) the two KSs can each be repre- Joint probabilistic models of coreference. Resented in the resolver as a constraint (for filtering cently, there has been a surge of interest in imnon-mentions or disallowing coreference between proving coreference re</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Comparing knowledge sources for nominal anaphora resolution.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="2788" citStr="Markert and Nissim (2005)" startWordPosition="431" endWordPosition="434">pe of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al. (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.’s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE corefer536 Proceedings of the 45th Annual Meeting of the Association of Computational Linguisti</context>
</contexts>
<marker>Markert, Nissim, 2005</marker>
<rawString>K. Markert and M. Nissim. 2005. Comparing knowledge sources for nominal anaphora resolution. Computational Linguistics, 31(3):367–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
</authors>
<title>Anaphora Resolution.</title>
<date>2002</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="1257" citStr="Mitkov (2002)" startWordPosition="180" endWordPosition="181">cally computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution — the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge source</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>R. Mitkov. 2002. Anaphora Resolution. Longman. R. Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Proc. of COLING/ACL, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="21742" citStr="Ng and Cardie (2002)" startWordPosition="3662" endWordPosition="3665">e training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high- we can see, the baseline achieves an F-measure of performing resolvers such as Ng and Cardie (2002) 57.0 and a resolution accuracy of 48.4. and Yang et al. (2003), as described below. To get a better sense of how strong our baseline Lexical features. Nine features allow different is, we re-implement the Soon et al. (2001) corefertypes of string matching operations to be performed ence resolver. This simply amounts to replacing the on the given pair of NPs, NP,, and NPy10, including 33 features in the baseline resolver with the 12 fea(1) exact string match for pronouns, proper nouns, tures employed by Soon et al.’s system. Results of and non-pronominal NPs (both before and after de- our Dupl</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypothesis: An Introduction.</title>
<date>1989</date>
<booktitle>In Proc. of the ACL.</booktitle>
<publisher>John Wiley</publisher>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer Intensive Methods for Testing Hypothesis: An Introduction. John Wiley &amp; Sons. M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004. Learning to resolve bridging references. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="2081" citStr="Ponzetto and Strube, 2006" startWordPosition="308" endWordPosition="311">tant role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun t</context>
<context position="20709" citStr="Ponzetto and Strube (2006)" startWordPosition="3496" endWordPosition="3499"> examines whether our coreference resolver can benefit from any of the eight ways of incorporating these KSs. 4.1 Experimental Setup As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follo</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. of HLT/NAACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="21092" citStr="Quinlan, 1993" startWordPosition="3558" endWordPosition="3559">re score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high- we can see, the baseline achieves an F-measure of</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2761" citStr="Soon et al. (2001)" startWordPosition="427" endWordPosition="430"> (2004)). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al. (2001), Markert and Nissim (2005)). It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.’s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic. Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE corefer536 Proceedings of the 45th Annual Meeting of the Association</context>
<context position="4772" citStr="Soon et al. (2001)" startWordPosition="749" endWordPosition="752">Y, LOCATION, or OTHERS. We adopt Briefly, the goal of MD is to identify the boundary a corpus-based approach to SC determination, re- of a mention, its mention type (e.g., pronoun, name), casting the problem as a six-class classification task. and its semantic type (e.g., person, location). UnDerive two knowledge sources for coreference like them, (1) we do not perform the full MD task, resolution from the induced SCs. The first as our goal is to investigate the role of SC knowlknowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not (SCA). Following Soon et al. (2001), we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the in- sifier; instead, we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or (Weischedel and Brunstein, 2005), which consists of not. The second KS is mention, which is represented all the Penn Treebank Wall Street Journal articles as a binary value that indicates whether an NP be- with the ACE mentions manually identified and anlongs to one of the five ACE SCs mentioned above. notated with their SCs. This provides us with a trainHence, the mention valu</context>
<context position="21236" citStr="Soon et al. (2001)" startWordPosition="3580" endWordPosition="3583">e fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high- we can see, the baseline achieves an F-measure of performing resolvers such as Ng and Cardie (2002) 57.0 and a resolution accuracy of 48.4. and Yang et al. (2003), as described below. To get a </context>
<context position="25015" citStr="Soon et al. (2001)" startWordPosition="4238" endWordPosition="4241">cond (2001), we have a semantic feature that tests whether KS, SCA, is defined on a pair of NPs; its value is one NP is a name alias or acronym of the other. YES if the two NPs have the same induced SC that Positional feature. We have a feature that com- is not OTHERS, and NO otherwise. putes the distance between the two NPs in sentences. Incorporate the two KSs into the baseline reAfter training, the decision tree classifier is used solver. Recall that there are eight ways of incorto select an antecedent for each NP in a test text. porating these two KSs into our resolver: they can Following Soon et al. (2001), we select as the an- each be represented as a constraint or as a feature, tecedent of each NP, NPS, the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS. If no such and in combination. Constraints are applied durNP exists, no antecedent is selected for NPS. ing the antecedent selection step. Specifically, when Row 1 of Table 6 and Table 7 shows the results employed as a constraint, the Mention KS disallows of the baseline system in terms of F-measure (F) coreference between two NPs if at least one of them and accuracy in reso</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
</authors>
<title>A corpus-based evaluation of centering and pronoun resolution.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1181" citStr="Tetreault (2001)" startWordPosition="170" endWordPosition="171">ally significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge. In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%. 1 Introduction In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution — the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular k</context>
</contexts>
<marker>Tetreault, 2001</marker>
<rawString>J. Tetreault. 2001. A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics, 27(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proc. ofMUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="20552" citStr="Vilain et al., 1995" startWordPosition="3473" endWordPosition="3476">and mention — and incorporate them into our learning-based coreference resolver in eight different ways, as described in the introduction. This section examines whether our coreference resolver can benefit from any of the eight ways of incorporating these KSs. 4.1 Experimental Setup As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder. 4.2 The Baseline Coreference System Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determini</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc. ofMUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>A Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus. Linguistica Data Consortium.</title>
<date>2005</date>
<contexts>
<context position="5024" citStr="Weischedel and Brunstein, 2005" startWordPosition="795" endWordPosition="798">ask. and its semantic type (e.g., person, location). UnDerive two knowledge sources for coreference like them, (1) we do not perform the full MD task, resolution from the induced SCs. The first as our goal is to investigate the role of SC knowlknowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not (SCA). Following Soon et al. (2001), we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the in- sifier; instead, we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or (Weischedel and Brunstein, 2005), which consists of not. The second KS is mention, which is represented all the Penn Treebank Wall Street Journal articles as a binary value that indicates whether an NP be- with the ACE mentions manually identified and anlongs to one of the five ACE SCs mentioned above. notated with their SCs. This provides us with a trainHence, the mention value of an NP can be readily ing set that is approximately five times bigger than derived from its induced SC: the value is NO if its that of ACE. More importantly, the ACE participants SC is OTHERS, and YES otherwise. This KS could do not evaluate the ro</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>R. Weischedel and A. Brunstein. 2005. BBN pronoun coreference and entity type corpus. Linguistica Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Coreference resolution using competitive learning approach.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="21805" citStr="Yang et al. (2003)" startWordPosition="3674" endWordPosition="3677">Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high- we can see, the baseline achieves an F-measure of performing resolvers such as Ng and Cardie (2002) 57.0 and a resolution accuracy of 48.4. and Yang et al. (2003), as described below. To get a better sense of how strong our baseline Lexical features. Nine features allow different is, we re-implement the Soon et al. (2001) corefertypes of string matching operations to be performed ence resolver. This simply amounts to replacing the on the given pair of NPs, NP,, and NPy10, including 33 features in the baseline resolver with the 12 fea(1) exact string match for pronouns, proper nouns, tures employed by Soon et al.’s system. Results of and non-pronominal NPs (both before and after de- our Duplicated Soon et al. system are shown in row terminers are remove</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference resolution using competitive learning approach. In Proc. of the ACL, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="13564" citStr="Yarowsky, 1995" startWordPosition="2249" endWordPosition="2250">Pi’s ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin’s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in Collins and Singer (1999), motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999). We apply add-one smoothing to smooth the class posteriors. 1-Nearest Neighbor (1-NN): We use the 1-NN classifier as implemented in TiMBL (Daelemans et al., 2004), employing dot product as the similarity function (which defines similarity as the number of common feature-value pairs between two instances). All other parameters are set to their default values. Maximum Entropy (ME): We employ Lin’s ME implementation7, using a Gaussian prior for smoothing and running the algorithm until convergence. Naive Bayes (NB): We use an in-house implementati</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>