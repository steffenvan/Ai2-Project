<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006031">
<title confidence="0.972474">
Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History
</title>
<author confidence="0.892625">
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych
</author>
<affiliation confidence="0.8447325">
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universit¨at Darmstadt
</affiliation>
<address confidence="0.873286">
Hochschulstrasse 10, D-64289 Darmstadt, Germany
</address>
<email confidence="0.931368">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852681818182">
We present an open-source toolkit which
allows (i) to reconstruct past states of
Wikipedia, and (ii) to efficiently access the
edit history of Wikipedia articles. Recon-
structing past states of Wikipedia is a pre-
requisite for reproducing previous experimen-
tal work based on Wikipedia. Beyond that,
the edit history of Wikipedia articles has been
shown to be a valuable knowledge source for
NLP, but access is severely impeded by the
lack of efficient tools for managing the huge
amount of provided data. By using a dedi-
cated storage format, our toolkit massively de-
creases the data volume to less than 2% of
the original size, and at the same time pro-
vides an easy-to-use interface to access the re-
vision data. The language-independent design
allows to process any language represented in
Wikipedia. We expect this work to consolidate
NLP research using Wikipedia in general, and
to foster research making use of the knowl-
edge encoded in Wikipedia’s edit history.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9707866">
In the last decade, the free encyclopedia Wikipedia
has become one of the most valuable and com-
prehensive knowledge sources in Natural Language
Processing. It has been used for numerous NLP
tasks, e.g. word sense disambiguation, semantic re-
latedness measures, or text categorization. A de-
tailed survey on usages of Wikipedia in NLP can be
found in (Medelyan et al., 2009).
The majority of Wikipedia-based NLP algorithms
works on single snapshots of Wikipedia, which are
</bodyText>
<page confidence="0.994289">
97
</page>
<bodyText confidence="0.999825242424243">
published by the Wikimedia Foundation as XML
dumps at irregular intervals.1 Such a snapshot only
represents the state of Wikipedia at a certain fixed
point in time, while Wikipedia actually is a dynamic
resource that is constantly changed by its millions of
editors. This rapid change is bound to have an influ-
ence on the performance of NLP algorithms using
Wikipedia data. However, the exact consequences
are largely unknown, as only very few papers have
systematically analyzed this influence (Zesch and
Gurevych, 2010). This is mainly due to older snap-
shots becoming unavailable, as there is no official
backup server. As a consequence, older experimen-
tal results cannot be reproduced anymore.
In this paper, we present a toolkit that solves
both issues by reconstructing a certain past state of
Wikipedia from its edit history, which is offered by
the Wikimedia Foundation in form of a database
dump. Section 3 gives a more detailed overview of
the reconstruction process.
Besides reconstructing past states of Wikipedia,
the revision history data also constitutes a novel
knowledge source for NLP algorithms. The se-
quence of article edits can be used as training data
for data-driven NLP algorithms, such as vandalism
detection (Chin et al., 2010), text summarization
(Nelken and Yamangil, 2008), sentence compres-
sion (Yamangil and Nelken, 2008), unsupervised
extraction of lexical simplifications (Yatskar et al.,
2010), the expansion of textual entailment corpora
(Zanzotto and Pennacchiotti, 2010), or assesing the
trustworthiness of Wikipedia articles (Zeng et al.,
2006).
</bodyText>
<footnote confidence="0.585699666666667">
1http://download.wikimedia.org/
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102,
Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics
</footnote>
<bodyText confidence="0.99998175">
However, efficient access to this new resource
has been limited by the immense size of the data.
The revisions for all articles in the current English
Wikipedia sum up to over 5 terabytes of text. Con-
sequently, most of the above mentioned previous
work only regarded small samples of the available
data. However, using more data usually leads to bet-
ter results, or how Church and Mercer (1993) put
it “more data are better data”. Thus, in Section 4,
we present a tool to efficiently access Wikipedia’s
edit history. It provides an easy-to-use API for pro-
grammatically accessing the revision data and re-
duces the required storage space to less than 2% of
its original size. Both tools are publicly available
on Google Code (http://jwpl.googlecode.
com) as open source software under the LGPL v3.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999778321428572">
To our knowledge, there are currently only two alter-
natives to programmatically access Wikipedia’s re-
vision history.
One possibility is to manually parse the original
XML revision dump. However, due to the huge size
of these dumps, efficient, random access is infeasi-
ble with this approach.
Another possibility is using the MediaWiki API2,
a web service which directly accesses live data from
the Wikipedia website. However, using a web ser-
vice entails that the desired revision for every single
article has to be requested from the service, trans-
ferred over the Internet and then stored locally in
an appropriate format. Access to all revisions of
all Wikipedia articles for a large-scale analysis is
infeasible with this method because it is strongly
constricted by the data transfer speed over the In-
ternet. Even though it is possible to bypass this bot-
tleneck by setting up a local Wikipedia mirror, the
MediaWiki API can only provide full text revisions,
which results in very large amounts of data to be
transferred.
Better suited for tasks of this kind are APIs
that utilize databases for storing and accessing the
Wikipedia data. However, current database-driven
Wikipedia APIs do not support access to article re-
visions. That is why we decided to extend an es-
tablished API with the ability to efficiently access
</bodyText>
<footnote confidence="0.701348">
2http://www.mediawiki.org/wiki/API
</footnote>
<bodyText confidence="0.99982146875">
Wikipedia’s edit history. Two established Wikipedia
APIs have been considered for this purpose.
Wikipedia Miner3 (Milne and Witten, 2009) is
an open source toolkit which provides access to
Wikipedia with the help of a preprocessed database.
It represents articles, categories and redirects as Java
classes and provides access to the article content ei-
ther as MediaWiki markup or as plain text. The
toolkit mainly focuses on Wikipedia’s structure, the
contained concepts, and semantic relations, but it
makes little use of the textual content within the ar-
ticles. Even though it was developed to work lan-
guage independently, it focuses mainly on the En-
glish Wikipedia.
Another open source API for accessing Wikipedia
data from a preprocessed database is JWPL4 (Zesch
et al., 2008). Like Wikipedia Miner, it also rep-
resents the content and structure of Wikipedia as
Java objects. In addition to that, JWPL contains a
MediaWiki markup parser to further analyze the ar-
ticle contents to make available fine-grained infor-
mation like e.g. article sections, info-boxes, or first
paragraphs. Furthermore, it was explicitly designed
to work with all language versions of Wikipedia.
We have chosen to extend JWPL with our revi-
sion toolkit, as it has better support for accessing ar-
ticle contents, natively supports multiple languages,
and seems to have a larger and more active developer
community. In the following section, we present the
parts of the toolkit which reconstruct past states of
Wikipedia, while in section 4, we describe tools al-
lowing to efficiently access Wikipedia’s edit history.
</bodyText>
<sectionHeader confidence="0.88644" genericHeader="method">
3 Reconstructing Past States of Wikipedia
</sectionHeader>
<bodyText confidence="0.999842090909091">
Access to arbitrary past states of Wikipedia is re-
quired to (i) evaluate the performance of Wikipedia-
based NLP algorithms over time, and (ii) to repro-
duce Wikipedia-based research results. For this rea-
son, we have developed a tool called TimeMachine,
which addresses both of these issues by making use
of the revision dump provided by the Wikimedia
Foundation. By iterating over all articles in the re-
vision dump and extracting the desired revision of
each article, it is possible to recover the state of
Wikipedia at an earlier point in time.
</bodyText>
<footnote confidence="0.999927">
3http://wikipedia-miner.sourceforge.net
4http://jwpl.googlecode.com
</footnote>
<page confidence="0.974315">
98
</page>
<figureCaption confidence="0.965127176470588">
Property Description Example Value
language The Wikipedia language version english
mainCategory Title of the main category of the Categories
Wikipedia language version used
disambiguationCategory Title of the disambiguation category of Disambiguation
the Wikipedia language version used
fromTimestamp Timestamp of the first snapshot to be 20090101130000
extracted
toTimestamp Timestamp of the last snapshot to be ex- 20091231130000
tracted
each Interval between snapshots in days 30
removeInputFilesAfterProcessing Remove source files [true/false] false
metaHistoryFile Path to the revision dump PATH/pages-meta-history.xml.bz2
pageLinksFile Path to the page-to-page link records PATH/pagelinks.sql.gz
categoryLinksFile Path to the category membership PATH/categorylinks.sql.gz
records
outputDirectory Output directory PATH/outdir/
</figureCaption>
<tableCaption confidence="0.982323">
Table 1: Configuration of the TimeMachine
</tableCaption>
<bodyText confidence="0.9999696">
The TimeMachine is controlled by a single con-
figuration file, which allows (i) to restore individual
Wikipedia snapshots or (ii) to generate whole snap-
shot series. Table 1 gives an overview of the con-
figuration parameters. The first three properties set
the environment for the specific language version of
Wikipedia. The two timestamps define the start and
end time of the snapshot series, while the interval
between the snapshots in the series is set by the pa-
rameter each. In the example, the TimeMachine re-
covers 13 snapshots between Jan 01, 2009 at 01.00
p.m and and Dec 31, 2009 at 01.00 p.m at an inter-
val of 30 days. In order to recover a single snap-
shot, the two timestamps have simply to be set to
the same value, while the parameter ‘each’ has no
effect. The option removeInputFilesAfterProcessing
specifies whether to delete the source files after pro-
cessing has finished. The final four properties define
the paths to the source files and the output directory.
The output of the TimeMachine is a set of eleven
text files for each snapshot, which can directly be
imported into an empty JWPL database. It can be
accessed with the JWPL API in the same way as
snapshots created using JWPL itself.
Issue of Deleted Articles The past snapshot of
Wikipedia created by our toolkit is identical to the
state of Wikipedia at that time with the exception of
articles that have been deleted meanwhile. Articles
might be deleted only by Wikipedia administrators
if they are subject to copyright violations, vandal-
ism, spam or other conditions that violate Wikipedia
policies. As a consequence, they are removed from
the public view along with all their revision infor-
mation, which makes it impossible to recover them
from any future publicly available dump.5 Even
though about five thousand pages are deleted every
day, only a small percentage of those pages actually
corresponds to meaningful articles. Most of the af-
fected pages are newly created duplicates of already
existing articles or spam articles.
</bodyText>
<sectionHeader confidence="0.994674" genericHeader="method">
4 Efficient Access to Revisions
</sectionHeader>
<bodyText confidence="0.999988533333333">
Even though article revisions are available from the
official Wikipedia revision dumps, accessing this in-
formation on a large scale is still a difficult task.
This is due to two main problems. First, the revi-
sion dump contains all revisions as full text. This
results in a massive amount of data and makes struc-
tured access very hard. Second, there is no efficient
API available so far for accessing article revisions
on a large scale.
Thus, we have developed a tool called
RevisionMachine, which solves these issues.
First, we describe our solution to the storage prob-
lem. Second, we present several use cases of the
RevisionMachine, and show how the API simplifies
experimental setups.
</bodyText>
<footnote confidence="0.9855645">
5http://en.wikipedia.org/wiki/Wikipedia:
DEL
</footnote>
<page confidence="0.994719">
99
</page>
<subsectionHeader confidence="0.974016">
4.1 Revision Storage
</subsectionHeader>
<bodyText confidence="0.999894176470589">
As each revision of a Wikipedia article stores the
full article text, the revision history obviously con-
tains a lot of redundant data. The RevisionMachine
makes use of this fact and utilizes a dedicated stor-
age format which stores a revision only by means
of the changes that have been made to the previous
revision. For this purpose, we have tested existing
diff libraries, like Javaxdelta6 or java-diff7, which
calculate the differences between two texts. How-
ever, both their runtime and the size of the result-
ing output was not feasible for the given size of the
data. Therefore, we have developed our own diff
algorithm, which is based on a longest common sub-
string search and constitutes the foundation for our
revision storage format.
The processing of two subsequent revisions can
be divided into four steps:
</bodyText>
<listItem confidence="0.968243176470588">
• First, the RevisionMachine searches for all
common substrings with a user-defined mini-
mal length.
• Then, the revisions are divided into blocks of
equal length. Corresponding blocks of both
revisions are then compared. If a block is
contained in one of the common substrings,
it can be marked as unchanged. Otherwise,
we have to categorize the kind of change
that occurred in this block. We differenti-
ate between five possible actions: Insert,
Delete, Replace, Cut and Paste8. This
information is stored in each block and is later
on used to encode the revision.
• In the next step, the current revision is repre-
sented by means of a sequence of actions per-
formed on the previous revision.
</listItem>
<equation confidence="0.9611935">
For example, in the adjacent revision pair
r1 : This is the very first sentence!
r2 : This is the second sentence
r2 can be encoded as
REPLACE 12 10 ’second’
DELETE 31 1
</equation>
<footnote confidence="0.939042333333333">
6http://javaxdelta.sourceforge.net/
7http://www.incava.org/projects/java/
java-diff
8Cut and Paste operations always occur pairwise. In ad-
dition to the other operations, they can make use of an additional
temporary storage register to save the text that is being moved.
</footnote>
<bodyText confidence="0.974233875">
• Finally, the string representation of this ac-
tion sequence is compressed and stored in the
database.
With this approach, we achieve to reduce the de-
mand for disk space for a recent English Wikipedia
dump containing all article revisions from 5470 GB
to only 96 GB, i.e. by 98%. The compressed data is
stored in a MySQL database, which provides sophis-
ticated indexing mechanisms for high-performance
access to the data.
Obviously, storing only the changes instead of
the full text of each revision trades in speed for
space. Accessing a certain revision now requires re-
constructing the text of the revision from a list of
changes. As articles often have several thousand re-
visions, this might take too long. Thus, in order to
speed up the recovery of the revision text, every n-th
revision is stored as a full revision. A low value of
n decreases the time needed to access a certain re-
vision, but increases the demand for storage space.
We have found n = 1000 to yield a good trade-off9.
This parameter, among a few other possibilities to
fine-tune the process, can be set in a graphical user
interface provided with the RevisionMachine.
</bodyText>
<subsectionHeader confidence="0.989464">
4.2 Revision Access
</subsectionHeader>
<bodyText confidence="0.999978352941177">
After the converted revisions have been stored in
the revision database, it can either be used stand-
alone or combined with the JWPL data and ac-
cessed via the standard JWPL API. The latter op-
tion makes it possible to combine the possibilities
of the RevisionMachine with other components like
the JWPL parser for the MediaWiki syntax.
In order to set up the RevisionMachine, it is only
necessary to provide the configuration details for the
database connection (see Listing 1). Upon first ac-
cess, the database user has to have write permission
on the database, as indexes have to be created. For
later use, read permission is sufficient. Access to the
RevisionMachine is achieved via two API objects.
The RevisionIterator allows to iterate over all revi-
sions in Wikipedia. The RevisionAPI grants access
to the revisions of individual articles. In addition to
</bodyText>
<footnote confidence="0.968865">
9If hard disk space is no limiting factor, the parameter can be
set to 1 to avoid the compression of the revisions and maximize
the performance.
</footnote>
<page confidence="0.820012">
100
</page>
<table confidence="0.984734090909091">
/ / Set up database connection
DatabaseConfiguration db = new DatabaseConfiguration();
db . setDatabase( ”dbname” ) ;
db . setHost( ” hostname ” ) ;
db . setUser( ” username ” ) ;
db . setPassword( ”pwd” ) ;
db.setLanguage(Language.english);
//Create API objects
Wikipedia wiki = WikiConnectionUtils.getWikipediaConnection(db);
RevisionIterator revIt = new RevisionIterator(db);
RevisionApi revApi = new RevisionApi(db);
</table>
<tableCaption confidence="0.760587">
Listing 1: Setting up the RevisionMachine
</tableCaption>
<bodyText confidence="0.990143558823529">
that, the Wikipedia object provides access to JWPL
functionalities.
In the following, we describe three use cases of
the RevisionMachine API, which demonstrate how
it is easily integrated into experimental setups.
Processing all article revisions in Wikipedia
The first use case focuses on the utilization of the
complete set of article revisions in a Wikipedia snap-
shot. Listing 2 shows how to iterate over all revi-
sions. Thereby, the iterator ensures that successive
revisions always correspond to adjacent revisions of
a single article in chronological order. The start of
a new article can easily be detected by checking the
timestamp and the article id. This approach is es-
pecially useful for applications in statistical natural
language processing, where large amounts of train-
ing data are a vital asset.
Processing revisions of individual articles The
second use case shows how the RevisionMachine
can be used to access the edit history of a specific
article. The example in Listing 3 illustrates how all
revisions for the article Automobile can be retrieved
by first performing a page query with the JWPL API
and then retrieving all revision timestamps for this
page, which can finally be used to access the revi-
sion objects.
Accessing the meta data of a revision The third
use case illustrates the access to the meta data of in-
dividual revisions. The meta data includes the name
or IP of the contributor, the additional user comment
for the revision and a flag that identifies a revision as
minor or major. Listing 4 shows how the number of
edits and unique contributors can be used to indicate
the level of edit activity for an article.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999954115384616">
In this paper, we presented an open-source toolkit
which extends JWPL, an API for accessing
Wikipedia, with the ability to reconstruct past states
of Wikipedia, and to efficiently access the edit his-
tory of Wikipedia articles.
Reconstructing past states of Wikipedia is a
prerequisite for reproducing previous experimen-
tal work based on Wikipedia, and is also a re-
quirement for the creation of time-based series of
Wikipedia snapshots and for assessing the influence
of Wikipedia growth on NLP algorithms. Further-
more, Wikipedia’s edit history has been shown to be
a valuable knowledge source for NLP, which is hard
to access because of the lack of efficient tools for
managing the huge amount of revision data. By uti-
lizing a dedicated storage format for the revisions,
our toolkit massively decreases the amount of data
to be stored. At the same time, it provides an easy-
to-use interface to access the revision data.
We expect this work to consolidate NLP re-
search using Wikipedia in general, and to foster
research making use of the knowledge encoded in
Wikipedia’s edit history. The toolkit will be made
available as part of JWPL, and can be obtained from
the project’s website at Google Code. (http://
jwpl.googlecode.com)
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999594375">
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Hessian research
excellence program “Landes-Offensive zur Entwicklung
Wissenschaftlich-¨okonomischer Exzellenz” (LOEWE) as
part of the research center ”Digital Humanities”. We
would also like to thank Simon Kulessa for designing and
implementing the foundations of the RevisionMachine.
</bodyText>
<page confidence="0.988449">
101
</page>
<figure confidence="0.988500461538461">
//Iterate over all revisions of all articles
while (revIt . hasNext ( ) ) {
Revision rev = revIt.next()
rev.getTimestamp();
rev . getArticleID ( ) ;
//process revision . . .
}
Listing 2: Iteration over all revisions of all articles
/ / Get article with title ” Automobile ”
Page article = wiki. getPage( ” Automobile ” ) ;
int id = article . getPageId ( ) ;
/ / Get all revisions for the article
CollectionGTimestamp&gt; revisionTimeStamps = revApi.getRevisionTimestamps(id);
for (Timestamp t:revisionTimeStamps) {
Revision rev = revApi.getRevision(id, t) ;
//process revision . . .
}
Listing 3: Accessing the revisions of a specific article
//Meta data provided by the RevisionAPI
StringBuffer s = new StringBuffer();
s . append( ”The article has ”+revApi . getNumberOfRevisions (pageId)+” r e v i s i o n s .\ n” ) ;
s . append( ” I t has ”+revApi . getNumberOfUniqueContributors(pageId)+” unique c o n t r i b u t o r s .\ n” ) ;
s . append (revApi . getNumberOfUniqueContributors(pageId , true )+ ” are registered users .\ n” ) ;
//Meta data provided by the Revision object
s . append ( ( rev . isMinor ( ) ?”Minor” : ” Major ” )+” revision by : ”+rev . getContributorID ( ) ) ;
s . append( ”\nComment : ”+rev . getComment ( ) ) ;
</figure>
<figureCaption confidence="0.700677">
Listing 4: Accessing the meta data of a revision
</figureCaption>
<sectionHeader confidence="0.987652" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999580274509804">
Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and
David Eichmann. 2010. Detecting wikipedia vandal-
ism with active learning and statistical language mod-
els. In Proceedings of the 4th workshop on Informa-
tion credibility, WICOW ’10, pages 3–10.
Kenneth W. Church and Robert L. Mercer. 1993. Intro-
duction to the special issue on computational linguis-
tics using large corpora. Computational Linguistics,
19(1):1–24.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from wikipedia.
Int. J. Hum.-Comput. Stud., 67:716–754, September.
D. Milne and I. H. Witten. 2009. An open-source toolkit
for mining Wikipedia. In Proc. New Zealand Com-
puter Science Research Student Conf., volume 9.
Rani Nelken and Elif Yamangil. 2008. Mining
wikipedia’s article revision history for training com-
putational linguistics algorithms. In Proceedings of
the AAAI Workshop on Wikipedia and Artificial Intel-
ligence: An Evolving Synergy (WikiAI), WikiAI08.
Elif Yamangil and Rani Nelken. 2008. Mining wikipedia
revision histories for improving sentence compres-
sion. In Proceedings of ACL-08: HLT, Short Papers,
pages 137–140, Columbus, Ohio, June. Association
for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ’10, pages 365–368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
wikipedia using co-training. In Proceedings of the
COLING-Workshop on The People’s Web Meets NLP:
Collaboratively Constructed Semantic Resources.
Honglei Zeng, Maher Alhossaini, Li Ding, Richard Fikes,
and Deborah L. McGuinness. 2006. Computing trust
from revision history. In Proceedings of the 2006 In-
ternational Conference on Privacy, Security and Trust.
Torsten Zesch and Iryna Gurevych. 2010. The more the
better? Assessing the influence of wikipedia’s growth
on semantic relatedness measures. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC), Valletta, Malta.
Torsten Zesch, Christof Mueller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC).
</reference>
<page confidence="0.998599">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150200">
<title confidence="0.99331">Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History</title>
<author confidence="0.502266">Oliver Ferschke</author>
<author confidence="0.502266">Torsten Zesch</author>
<author confidence="0.502266">Iryna</author>
<affiliation confidence="0.5983735">Ubiquitous Knowledge Processing Computer Science Department, Technische Universit¨at</affiliation>
<address confidence="0.534989">Hochschulstrasse 10, D-64289 Darmstadt,</address>
<web confidence="0.729999">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.995895869565217">We present an open-source toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efficiently access the edit history of Wikipedia articles. Reconstructing past states of Wikipedia is a prerequisite for reproducing previous experimental work based on Wikipedia. Beyond that, the edit history of Wikipedia articles has been shown to be a valuable knowledge source for NLP, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data. By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2% of the original size, and at the same time provides an easy-to-use interface to access the revision data. The language-independent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia’s edit history.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Si-Chi Chin</author>
<author>W Nick Street</author>
<author>Padmini Srinivasan</author>
<author>David Eichmann</author>
</authors>
<title>Detecting wikipedia vandalism with active learning and statistical language models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th workshop on Information credibility, WICOW ’10,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="3016" citStr="Chin et al., 2010" startWordPosition="466" endWordPosition="469">ence, older experimental results cannot be reproduced anymore. In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size</context>
</contexts>
<marker>Chin, Street, Srinivasan, Eichmann, 2010</marker>
<rawString>Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and David Eichmann. 2010. Detecting wikipedia vandalism with active learning and statistical language models. In Proceedings of the 4th workshop on Information credibility, WICOW ’10, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Robert L Mercer</author>
</authors>
<title>Introduction to the special issue on computational linguistics using large corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3926" citStr="Church and Mercer (1993)" startWordPosition="598" endWordPosition="601"> of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work only regarded small samples of the available data. However, using more data usually leads to better results, or how Church and Mercer (1993) put it “more data are better data”. Thus, in Section 4, we present a tool to efficiently access Wikipedia’s edit history. It provides an easy-to-use API for programmatically accessing the revision data and reduces the required storage space to less than 2% of its original size. Both tools are publicly available on Google Code (http://jwpl.googlecode. com) as open source software under the LGPL v3. 2 Related Work To our knowledge, there are currently only two alternatives to programmatically access Wikipedia’s revision history. One possibility is to manually parse the original XML revision dum</context>
</contexts>
<marker>Church, Mercer, 1993</marker>
<rawString>Kenneth W. Church and Robert L. Mercer. 1993. Introduction to the special issue on computational linguistics using large corpora. Computational Linguistics, 19(1):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>David Milne</author>
<author>Catherine Legg</author>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from wikipedia.</title>
<date>2009</date>
<journal>Int. J. Hum.-Comput. Stud.,</journal>
<pages>67--716</pages>
<contexts>
<context position="1662" citStr="Medelyan et al., 2009" startWordPosition="250" endWordPosition="253">pendent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia’s edit history. 1 Introduction In the last decade, the free encyclopedia Wikipedia has become one of the most valuable and comprehensive knowledge sources in Natural Language Processing. It has been used for numerous NLP tasks, e.g. word sense disambiguation, semantic relatedness measures, or text categorization. A detailed survey on usages of Wikipedia in NLP can be found in (Medelyan et al., 2009). The majority of Wikipedia-based NLP algorithms works on single snapshots of Wikipedia, which are 97 published by the Wikimedia Foundation as XML dumps at irregular intervals.1 Such a snapshot only represents the state of Wikipedia at a certain fixed point in time, while Wikipedia actually is a dynamic resource that is constantly changed by its millions of editors. This rapid change is bound to have an influence on the performance of NLP algorithms using Wikipedia data. However, the exact consequences are largely unknown, as only very few papers have systematically analyzed this influence (Ze</context>
</contexts>
<marker>Medelyan, Milne, Legg, Witten, 2009</marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and Ian H. Witten. 2009. Mining meaning from wikipedia. Int. J. Hum.-Comput. Stud., 67:716–754, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>An open-source toolkit for mining Wikipedia.</title>
<date>2009</date>
<booktitle>In Proc. New Zealand Computer Science Research Student Conf.,</booktitle>
<volume>9</volume>
<contexts>
<context position="5836" citStr="Milne and Witten, 2009" startWordPosition="904" endWordPosition="907">ting up a local Wikipedia mirror, the MediaWiki API can only provide full text revisions, which results in very large amounts of data to be transferred. Better suited for tasks of this kind are APIs that utilize databases for storing and accessing the Wikipedia data. However, current database-driven Wikipedia APIs do not support access to article revisions. That is why we decided to extend an established API with the ability to efficiently access 2http://www.mediawiki.org/wiki/API Wikipedia’s edit history. Two established Wikipedia APIs have been considered for this purpose. Wikipedia Miner3 (Milne and Witten, 2009) is an open source toolkit which provides access to Wikipedia with the help of a preprocessed database. It represents articles, categories and redirects as Java classes and provides access to the article content either as MediaWiki markup or as plain text. The toolkit mainly focuses on Wikipedia’s structure, the contained concepts, and semantic relations, but it makes little use of the textual content within the articles. Even though it was developed to work language independently, it focuses mainly on the English Wikipedia. Another open source API for accessing Wikipedia data from a preproces</context>
</contexts>
<marker>Milne, Witten, 2009</marker>
<rawString>D. Milne and I. H. Witten. 2009. An open-source toolkit for mining Wikipedia. In Proc. New Zealand Computer Science Research Student Conf., volume 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Elif Yamangil</author>
</authors>
<title>Mining wikipedia’s article revision history for training computational linguistics algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI), WikiAI08.</booktitle>
<contexts>
<context position="3064" citStr="Nelken and Yamangil, 2008" startWordPosition="472" endWordPosition="475">be reproduced anymore. In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in </context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>Rani Nelken and Elif Yamangil. 2008. Mining wikipedia’s article revision history for training computational linguistics algorithms. In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI), WikiAI08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining wikipedia revision histories for improving sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3114" citStr="Yamangil and Nelken, 2008" startWordPosition="479" endWordPosition="482"> toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 ter</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining wikipedia revision histories for improving sentence compression. In Proceedings of ACL-08: HLT, Short Papers, pages 137–140, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>365--368</pages>
<contexts>
<context position="3189" citStr="Yatskar et al., 2010" startWordPosition="488" endWordPosition="491">edia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work onl</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 365–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from wikipedia using co-training.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING-Workshop on The People’s</booktitle>
<contexts>
<context position="3269" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="498" endWordPosition="501">tion in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work only regarded small samples of the available data. However, using more data usually</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from wikipedia using co-training. In Proceedings of the COLING-Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Zeng</author>
<author>Maher Alhossaini</author>
<author>Li Ding</author>
<author>Richard Fikes</author>
<author>Deborah L McGuinness</author>
</authors>
<title>Computing trust from revision history.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 International Conference on Privacy, Security and Trust.</booktitle>
<contexts>
<context position="3344" citStr="Zeng et al., 2006" startWordPosition="509" endWordPosition="512">ction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1http://download.wikimedia.org/ Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work only regarded small samples of the available data. However, using more data usually leads to better results, or how Church and Mercer (1993) put it “more data</context>
</contexts>
<marker>Zeng, Alhossaini, Ding, Fikes, McGuinness, 2006</marker>
<rawString>Honglei Zeng, Maher Alhossaini, Li Ding, Richard Fikes, and Deborah L. McGuinness. 2006. Computing trust from revision history. In Proceedings of the 2006 International Conference on Privacy, Security and Trust.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>The more the better? Assessing the influence of wikipedia’s growth on semantic relatedness measures.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="2285" citStr="Zesch and Gurevych, 2010" startWordPosition="348" endWordPosition="351">9). The majority of Wikipedia-based NLP algorithms works on single snapshots of Wikipedia, which are 97 published by the Wikimedia Foundation as XML dumps at irregular intervals.1 Such a snapshot only represents the state of Wikipedia at a certain fixed point in time, while Wikipedia actually is a dynamic resource that is constantly changed by its millions of editors. This rapid change is bound to have an influence on the performance of NLP algorithms using Wikipedia data. However, the exact consequences are largely unknown, as only very few papers have systematically analyzed this influence (Zesch and Gurevych, 2010). This is mainly due to older snapshots becoming unavailable, as there is no official backup server. As a consequence, older experimental results cannot be reproduced anymore. In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The seq</context>
</contexts>
<marker>Zesch, Gurevych, 2010</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2010. The more the better? Assessing the influence of wikipedia’s growth on semantic relatedness measures. In Proceedings of the Conference on Language Resources and Evaluation (LREC), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof Mueller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="6478" citStr="Zesch et al., 2008" startWordPosition="1008" endWordPosition="1011">kit which provides access to Wikipedia with the help of a preprocessed database. It represents articles, categories and redirects as Java classes and provides access to the article content either as MediaWiki markup or as plain text. The toolkit mainly focuses on Wikipedia’s structure, the contained concepts, and semantic relations, but it makes little use of the textual content within the articles. Even though it was developed to work language independently, it focuses mainly on the English Wikipedia. Another open source API for accessing Wikipedia data from a preprocessed database is JWPL4 (Zesch et al., 2008). Like Wikipedia Miner, it also represents the content and structure of Wikipedia as Java objects. In addition to that, JWPL contains a MediaWiki markup parser to further analyze the article contents to make available fine-grained information like e.g. article sections, info-boxes, or first paragraphs. Furthermore, it was explicitly designed to work with all language versions of Wikipedia. We have chosen to extend JWPL with our revision toolkit, as it has better support for accessing article contents, natively supports multiple languages, and seems to have a larger and more active developer co</context>
</contexts>
<marker>Zesch, Mueller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof Mueller, and Iryna Gurevych. 2008. Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary. In Proceedings of the Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>