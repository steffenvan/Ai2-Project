<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9992475">
A Tree Kernel-based Unified Framework
for Chinese Zero Anaphora Resolution
</title>
<author confidence="0.961923">
Fang Kong Guodong Zhou*
</author>
<affiliation confidence="0.914021">
JiangSu Provincial Key Lab for Computer Information Processing Technology
School of Computer Science and Technology Soochow University
</affiliation>
<email confidence="0.992353">
{kongfang, gdzhou}@suda.edu.cn
</email>
<sectionHeader confidence="0.99382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998096043478261">
This paper proposes a unified framework for
zero anaphora resolution, which can be di-
vided into three sub-tasks: zero anaphor detec-
tion, anaphoricity determination and
antecedent identification. In particular, all the
three sub-tasks are addressed using tree ker-
nel-based methods with appropriate syntactic
parse tree structures. Experimental results on a
Chinese zero anaphora corpus show that the
proposed tree kernel-based methods signifi-
cantly outperform the feature-based ones. This
indicates the critical role of the structural in-
formation in zero anaphora resolution and the
necessity of tree kernel-based methods in
modeling such structural information. To our
best knowledge, this is the first systematic
work dealing with all the three sub-tasks in
Chinese zero anaphora resolution via a unified
framework. Moreover, we release a Chinese
zero anaphora corpus of 100 documents,
which adds a layer of annotation to the manu-
ally-parsed sentences in the Chinese Treebank
(CTB) 6.0.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874111111111">
As one of the most important techniques in dis-
course analysis, anaphora resolution has been a
focus of research in Natural Language Processing
(NLP) for decades and achieved much success in
English recently (e.g. Soon et al. 2001; Ng and
Cardie 2002; Yang et al. 2003, 2008; Kong et al.
2009).
However, there is little work on anaphora reso-
lution in Chinese. A major reason for this phe-
</bodyText>
<note confidence="0.476601">
* Corresponding author
</note>
<bodyText confidence="0.999903583333334">
nomenon is that Chinese, unlike English, is a pro-
drop language, whereas in English, definite noun
phrases (e.g. the company) and overt pronouns (e.g.
he) are frequently employed as referring expres-
sions, which refer to preceding entities. Kim (2000)
compared the use of overt subjects in English and
Chinese. He found that overt subjects occupy over
96% in English, while this percentage drops to
only 64% in Chinese. This indicates the prevalence
of zero anaphors in Chinese and the necessity of
zero anaphora resolution in Chinese anaphora reso-
lution. Since zero anaphors give little hints (e.g.
number or gender) about their possible antecedents,
zero anaphora resolution is much more challenging
than traditional anaphora resolution.
Although Chinese zero anaphora has been
widely studied in the linguistics research (Li and
Thompson 1979; Li 2004), only a small body of
prior work in computational linguistics deals with
Chinese zero anaphora resolution (Converse 2006;
Zhao and Ng 2007). Moreover, zero anaphor de-
tection, as a critical component for real applica-
tions of zero anaphora resolution, has been largely
ignored.
This paper proposes a unified framework for
Chinese zero anaphora resolution, which can be
divided into three sub-tasks: zero anaphor detec-
tion, which detects zero anaphors from a text, ana-
phoricity determination, which determines whether
a zero anaphor is anaphoric or not, and antecedent
identification, which finds the antecedent for an
anaphoric zero anaphor. To our best knowledge,
this is the first systematic work dealing with all the
three sub-tasks via a unified framework. Moreover,
we release a Chinese zero anaphora corpus of 100
documents, which adds a layer of annotation to the
</bodyText>
<page confidence="0.960774">
882
</page>
<note confidence="0.817796">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882–891,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999833421052632">
manually-parsed sentences in the Chinese Tree-
bank (CTB) 6.0. This is done by assigning ana-
phoric/non-anaphoric zero anaphora labels to the
null constituents in a parse tree. Finally, this paper
illustrates the critical role of the structural informa-
tion in zero anaphora resolution and the necessity
of tree kernel-based methods in modeling such
structural information.
The rest of this paper is organized as follows.
Section 2 briefly describes the related work on
both zero anaphora resolution and tree kernel-
based anaphora resolution. Section 3 introduces the
overwhelming problem of zero anaphora in Chi-
nese and our developed Chinese zero anaphora
corpus, which is available for research purpose.
Section 4 presents our tree kernel-based unified
framework in zero anaphora resolution. Section 5
reports the experimental results. Finally, we con-
clude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999525666666667">
This section briefly overviews the related work on
both zero anaphora resolution and tree kernel-
based anaphora resolution.
</bodyText>
<subsectionHeader confidence="0.988225">
2.1 Zero anaphora resolution
</subsectionHeader>
<bodyText confidence="0.988275515151515">
Although zero anaphors are prevalent in many lan-
guages, such as Chinese, Japanese and Spanish,
there only have a few works on zero anaphora
resolution.
Zero anaphora resolution in Chinese
Converse (2006) developed a Chinese zero anaph-
ora corpus which only deals with zero anaphora
category “-NONE- *pro*” for dropped sub-
jects/objects and ignores other categories, such as
“-NONE- *PRO*” for non-overt subjects in non-
finite clauses. Besides, Converse (2006) proposed
a rule-based method to resolve the anaphoric zero
anaphors only. The method did not consider zero
anaphor detection and anaphoric identification, and
performed zero anaphora resolution using the
Hobbs algorithm (Hobbs, 1978), assuming the
availability of golden anaphoric zero anaphors and
golden parse trees.
Instead, Zhao and Ng (2007) proposed feature-
based methods to zero anaphora resolution on the
same corpus from Convese (2006). However, they
only considered zero anaphors with explicit noun
phrase referents and discarded those with split an-
tecedents or referring to events. Moreover, they
focused on the sub-tasks of anaphoricity determi-
nation and antecedent identification. For zero ana-
phor detection, a simple heuristic rule was
employed. Although this rule can recover almost
all the zero anaphors, it suffers from very low pre-
cision by introducing too many false zero anaphors
and thus leads to low performance in anaphoricity
determination, much due to the imbalance between
positive and negative training examples.
</bodyText>
<subsectionHeader confidence="0.448288">
Zero anaphora resolution in Japanese
</subsectionHeader>
<bodyText confidence="0.984628717948719">
Seki et al. (2002) proposed a probabilistic model
for the sub-tasks of anaphoric identification and
antecedent identification with the help of a verb
dictionary. They did not perform zero anaphor de-
tection, assuming the availability of golden zero
anaphors. Besides, their model needed a large-
scale corpus to estimate the probabilities to prevent
them from the data sparseness problem.
Isozaki and Hirao (2003) explored some ranking
rules and a machine learning method on zero
anaphora resolution. However, they assumed that
zero anaphors were already detected and each zero
anaphor’s grammatical case was already deter-
mined by a zero anaphor detector.
Iida et al. (2006) explored a machine learning
method for the sub-task of antecedent identifica-
tion using rich syntactic pattern features, assuming
the availability of golden anaphoric zero anaphors.
Sasano et al. (2008) proposed a fully-lexicalized
probabilistic model for zero anaphora resolution,
which estimated case assignments for the overt
case components and the antecedents of zero ana-
phors simultaneously. However, this model needed
case frames to detect zero anaphors and a large-
scale corpus to construct these case frames auto-
matically.
For Japanese zero anaphora, we do not see any
reports about zero anaphora categories. Moreover,
all the above related works we can find on Japa-
nese zero anaphora resolution ignore zero anaphor
detection, focusing on either anaphoricity determi-
nation or antecedent identification. Maybe, it is
easy to detect zero anaphors in Japanese. However,
it is out of the scope of our knowledge and this
paper.
Zero anaphora resolution in Spanish
As the only work we can find, Ferrandez and Peral
(2000) proposed a hand-engineered rule-based
method for both anaphoricity determination and
</bodyText>
<page confidence="0.997971">
883
</page>
<bodyText confidence="0.999932714285714">
necessary structural information in the parse tree
for anaphora resolution of pronouns and the con-
text-sensitive convolution tree kernel much outper-
formed other tree kernels.
antecedent identification. That is, they ignored zero
anaphor detection. Besides, they only dealt with
zero anaphors that were in the subject position.
</bodyText>
<subsectionHeader confidence="0.999024">
2.2 Tree kernel-based anaphora resolution
</subsectionHeader>
<bodyText confidence="0.999976977272727">
Although there is no research on tree kernel-based
zero anaphora resolution in the literature, tree ker-
nel-based methods have been explored in tradi-
tional anaphora resolution to certain extent and
achieved comparable performance with the domi-
nated feature-based ones. One main advantage of
kernel-based methods is that they are very effec-
tive at reducing the burden of feature engineering
for structured objects. Indeed, the kernel-based
methods have been successfully applied to mine
structural information in various NLP techniques
and applications, such as syntactic parsing (Collins
and Duffy 2001; Moschitti 2004), semantic rela-
tion extraction (Zelenko et al. 2003; Zhao and
Grishman 2005; Zhou et al. 2007; Qian et al. 2008),
and semantic role labeling (Moschitti 2004).
Representative works in tree kernel-based
anaphora resolution include Yang et al. (2006) and
Zhou et al (2008). Yang et al. (2006) employed a
convolution tree kernel on anaphora resolution of
pronouns. In particular, a document-level syntactic
parse tree for an entire text was constructed by at-
taching the parse trees of all its sentences to a new-
added upper node. Examination of three parse tree
structures using different construction schemes
(Min-Expansion, Simple-Expansion and Full-
Expansion) on the ACE 2003 corpus showed
promising results. However, among the three con-
structed parse tree structures, there exists no obvi-
ous overwhelming one, which can well cover
structured syntactic information. One problem with
this tree kernel-based method is that all the con-
structed parse tree structures are context-free and
do not consider the information outside the sub-
trees. To overcome this problem, Zhou et al. (2008)
proposed a dynamic-expansion scheme to auto-
matically construct a proper parse tree structure for
anaphora resolution of pronouns by taking predi-
cate- and antecedent competitor-related informa-
tion into consideration. Besides, they proposed a
context-sensitive convolution tree kernel to com-
pute the similarity between the parse tree structures.
Evaluation on the ACE 2003 corpus showed that
the dynamic-expansion scheme can well cover
</bodyText>
<sectionHeader confidence="0.986739" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.999405666666667">
This section introduces the phenomenon of zero
anaphora in Chinese and our developed Chinese
zero anaphora corpus.
</bodyText>
<subsectionHeader confidence="0.999882">
3.1 Zero anaphora in Chinese
</subsectionHeader>
<bodyText confidence="0.999990485714286">
A zero anaphor is a gap in a sentence, which refers
to an entity that supplies the necessary information
for interpreting the gap. Figure 1 illustrates an ex-
ample sentence from Chinese TreeBank (CTB) 6.0
(File ID=001, Sentence ID=8). In this example,
there are four zero anaphors denoted as (Di (i=1,
2, ...4). Generally, zero anaphors can be under-
stood from the context and do not need to be speci-
fied.
A zero anaphor can be classified into either ana-
phoric or non-anaphoric, depending on whether it
has an antecedent in the discourse. Typically, a
zero anaphor is non-anaphoric when it refers to an
extra linguistic entity (e.g. the first or second per-
son in a conversion) or its referent is unspecified in
the context. Among the four anaphors in Figure 1,
zero anaphors (D 1 and (D 4 are non-anaphoric
while zero anaphors (D2 and (D3 are anaphoric,
referring to noun phrase “39Xff为/building ac-
tion” and noun phrase “*K%�/]—_1Z/new district
managing committee” respectively.
Chinese zero anaphora resolution is very diffi-
cult due to following reasons: 1) Zero anaphors
give little hints (e.g. number or gender) about their
possible antecedents. This makes antecedent iden-
tification much more difficult than traditional
anaphora resolution. 2) A zero anaphor can be ei-
ther anaphoric or non-anaphoric. In our corpus de-
scribed in Section 3.2, about 60% of zero anaphors
are non-anaphoric. This indicates the importance
of anaphoricity determination. 3) Zero anaphors
are not explicitly marked in a text. This indicates
the necessity of zero anaphor detection, which has
been largely ignored in previous research and has
proved to be difficult in our later experiments.
</bodyText>
<page confidence="0.997519">
884
</page>
<figureCaption confidence="0.99966">
Figure 1: An example sentence from CTB 6.0, which contains four zero anaphors
</figureCaption>
<bodyText confidence="0.9948145">
(the example is : 为规范建筑行为,防止出现无序现象,新区管委会根据国家和上海市的有关规定,结合浦东开发实际,及时出台了一系列规范建设市场的文件/ In order to standardize the building action and prevent the
inorder phenomenon, the standing committee of new zone annouced a series of files to standardize building market
based on the related provisions of China and Shanghai in time, and the realities of the development of Pudong are
considered. )
</bodyText>
<subsectionHeader confidence="0.999865">
3.2 Zero anaphora corpus in Chinese
</subsectionHeader>
<bodyText confidence="0.999870571428571">
Due to lack of an available zero anaphora corpus
for research purpose, we develop a Chinese zero
anaphora corpus of 100 documents from CTB 6.0,
which adds a layer of annotation to the manually-
parsed sentences. Hoping the public availability of
this corpus can push the research of zero anaphora
resolution in Chinese and other languages.
</bodyText>
<figureCaption confidence="0.989863">
Figure 2: An example sentence annotated in CTB 6.0
</figureCaption>
<table confidence="0.999504894736842">
ID Cate- Description AZ ZAs
gory As
1 -NONE- Used in topicalization and 6 742
*T* object preposing con-
structions
2 -NONE- Used in raising and pas- 1 2
* sive constructions
3 -NONE- Used in control structures. 219 399
*PRO* The *PRO* cannot be
substituted by an overt
constituent.
4 -NONE- for dropped subject or 394 449
*pro* object.
5 -NONE- Used for right node rais- 0 36
*RNR* ing (Cataphora)
6 Others 92 92
Other unknown empty
categories
Total (100 documents, 35089 words) 712 1720
</table>
<tableCaption confidence="0.972192666666667">
Table 1: Statistics on different categories of zero
anaphora (AZA and ZA indicates anaphoric zero ana-
phor and zero anaphor respectively)
</tableCaption>
<page confidence="0.99696">
885
</page>
<bodyText confidence="0.999380666666667">
Figure 2 illustrates an example sentence anno-
tated in CTB 6.0, where the special tag “-NONE-”
represents a null constituent and thus the occur-
rence of a zero anaphor. In our developed corpus,
we need to annotate anaphoric zero anaphors using
those null constituents with the special tag of “-
NONE-”.
Table 1 gives the statistics on all the six catego-
ries of zero anaphora. Since we do not consider
zero cataphora in the current version, we simply
redeem them non-anaphoric. It shows that among
1720 zero anaphors, only 712 (about 40%) are
anaphoric. This suggests the importance of ana-
phoricity determination in zero anaphora resolution.
Table 3 further shows that, among 712 anaphoric
zero anaphors, 598 (84%) are intra-sentential and
no anaphoric zero anaphors have their antecedents
occurring two sentences before.
</bodyText>
<table confidence="0.98879375">
Sentence distance AZAs
0 598
1 114
&gt;=2 0
</table>
<tableCaption confidence="0.9940415">
Table 3 Distribution of anaphoric zero anaphors over
sentence distances
</tableCaption>
<bodyText confidence="0.999409583333334">
Figure 3 shows an example in our corpus corre-
sponding to Figure 2. For a non-anaphoric zero
anaphor, we replace the null constituent with “E-i
NZA”, where i indicates the category of zero
anaphora, with “1” referring to “-NONE *T*”
etc. For an anaphoric zero anaphor, we replace it
with “E-x-y-z-i AZA”, where x indicates the sen-
tence id of its antecedent, y indicates the position
of the first word of its antecedent in the sentence, z
indicates the position of the last word of its antece-
dent in the sentence, and i indicates the category id
of the null constituent.
</bodyText>
<figureCaption confidence="0.987503">
Figure 3: an example sentence annotated in our corpus
</figureCaption>
<sectionHeader confidence="0.921332" genericHeader="method">
4 Tree Kernel-based Framework
</sectionHeader>
<bodyText confidence="0.99999005">
This section presents the tree kernel-based unified
framework for all the three sub-tasks in zero
anaphora resolution. For each sub-task, different
parse tree structures are constructed. In particular,
the context-sensitive convolution tree kernel, as
proposed in Zhou et al. (2008), is employed to
compute the similarity between two parse trees via
the SVM toolkit SVMLight.
In the tree kernel-based framework, we perform
the three sub-tasks, zero anaphor detection, ana-
phoricity determination and antecedent identifica-
tion in a pipeline manner. That is, given a zero
anaphor candidate Z, the zero anaphor detector is
first called to determine whether Z is a zero ana-
phor or not. If yes, the anaphoricity determiner is
then invoked to determine whether Z is an ana-
phoric zero anaphor. If yes, the antecedent identi-
fier is finally awaked to determine its antecedent.
In the future work, we will explore better ways of
integrating the three sub-tasks (e.g. joint learning).
</bodyText>
<subsectionHeader confidence="0.997072">
4.1 Zero anaphor detection
</subsectionHeader>
<bodyText confidence="0.99239926923077">
At the first glance, it seems that a zero anaphor can
occur between any two constituents in a parse tree.
Fortunately, an exploration of our corpus shows
that a zero anaphor always occurs just before a
predicate1 phrase node (e.g. VP). This phenome-
non has also been employed in Zhao and Ng (2007)
in generating zero anaphor candidates. In particular,
if the predicate phrase node occurs in a coordinate
structure or is modified by an adverbial node, we
only need to consider its parent. As shown in Fig-
ure 1, zero anaphors may occur immediately to the
left ofd E/guide, P-LL/avoid, ,`f4fl /appear, &amp;9
/according to, M A /combine, ,`f4 R /promulgate,
which cover the four true zero anaphors. Therefore,
it is simple but reliable in applying above heuristic
rules to generate zero anaphor candidates.
Given a zero anaphor candidate, it is critical to
construct a proper parse tree structure for tree ker-
nel-based zero anaphor detection. The intuition
behind our parser tree structure for zero anaphor
detection is to keep the competitive information
1 The predicate in Chinese can be categorized into verb predi-
cate, noun predicate and preposition predicate. In our corpus,
about 93% of the zero anaphors are driven by verb predicates.
In this paper, we only explore zero anaphors driven by verb
predicates.
</bodyText>
<page confidence="0.992919">
886
</page>
<bodyText confidence="0.999916034482759">
about the predicate phrase node and the zero ana-
phor candidate as much as possible. In particular,
the parse tree structure is constructed by first keep-
ing the path from the root node to the predicate
phrase node and then attaching all the immediate
verbal phrase nodes and nominal phrase nodes.
Besides, for the sub-tree rooted by the predicate
phrase node, we only keep those paths ended with
verbal leaf nodes and the immediate verbal and
nominal nodes attached to these paths. Figure 4
shows an example of the parse tree structure corre-
sponding to Figure 1 with the zero anaphor candi-
date Φ2 in consideration.
During training, if a zero anaphor candidate has
a counterpart in the same position in the golden
standard corpus (either anaphoric or non-
anaphoric), a positive instance is generated. Oth-
erwise, a negative instance is generated. During
testing, each zero anaphor candidate is presented to
the learned zero anaphor detector to determine
whether it is a zero anaphor or not. Besides, since a
zero anaphor candidate is generated when a predi-
cate phrase node appears, there may be two or
more zero anaphor candidates in the same position.
However, there is normally one zero anaphor in the
same position. Therefore, we just select the one
with maximal confidence as the zero anaphor in
the position and ignore others, if multiple zero
anaphor candidates occur in the same position.
</bodyText>
<figureCaption confidence="0.913189666666667">
Figure 4: An example parse tree structure for zero ana-
phor detection with the predicate phrase node and the
zero anaphor candidate (D2 in black
</figureCaption>
<subsectionHeader confidence="0.994561">
4.2 Anaphoricity determination
</subsectionHeader>
<bodyText confidence="0.997843333333333">
To determine whether a zero anaphor is anaphoric
or not, we limit the parse tree structure between the
previous predicate phrase node and the following
predicate phrase node. Besides, we only keep those
verbal phrase nodes and nominal phrase nodes.
Figure 5 illustrates an example of the parse tree
structure for anaphoricity determination, corre-
sponding to Figure 1 with the zero anaphor Φ2 in
consideration.
</bodyText>
<figure confidence="0.897709">
VP
A*
phenomenon
</figure>
<figureCaption confidence="0.992453">
Figure 5: An example parse tree structure for anaphoric-
ity determination with the zero anaphor (D2 in consid-
</figureCaption>
<bodyText confidence="0.47305">
eration
</bodyText>
<subsectionHeader confidence="0.997473">
4.3 Antecedent identification
</subsectionHeader>
<bodyText confidence="0.9990758">
To identify an antecedent for an anaphoric zero
anaphor, we adopt the Dynamic Expansion Tree,
as proposed in Zhou et al. (2008), which takes
predicate- and antecedent competitor-related in-
formation into consideration. Figure 6 illustrates an
example parse tree structure for antecedent identi-
fication, corresponding to Figure 1 with the ana-
phoric zero anaphor Φ 2 and the antecedent
candidate “建筑行)1/building action” in consid-
eration.
</bodyText>
<figureCaption confidence="0.9151135">
Figure 6: An example parse tree structure for antecedent
identification with the anaphoric zero anaphor (D2 and
the antecedent candidate “&amp;RJT)1/building action” in
consideration
</figureCaption>
<bodyText confidence="0.953597">
In this paper, we adopt a similar procedure as
Soon et al. (2001) in antecedent identification. Be-
</bodyText>
<figure confidence="0.9983737">
VV
IP
fA NP
appear NN
prevent
MIL
NP-SBJ
VV
VP
NP-OBJ
</figure>
<page confidence="0.991827">
887
</page>
<bodyText confidence="0.999981125">
sides, since all the anaphoric zero anaphors have
their antecedents at most one sentence away, we
only consider antecedent candidates which are at
most one sentence away. In particular, a document-
level parse tree for an entire document is con-
structed by attaching the parse trees of all its sen-
tences to a new-added upper node, as done in Yang
et al. (2006), to deal with inter-sentential ones.
</bodyText>
<sectionHeader confidence="0.985687" genericHeader="evaluation">
5 Experimentation and Discussion
</sectionHeader>
<bodyText confidence="0.999955125">
We have systematically evaluated our tree kernel-
based unified framework on our developed Chi-
nese zero anaphora corpus, as described in Section
3.2. Besides, in order to focus on zero anaphor
resolution itself and compare with related work, all
the experiments are done on golden parse trees
provided by CTB 6.0. Finally, all the performances
are achieved using 5-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.935368">
5.1 Experimental results
</subsectionHeader>
<bodyText confidence="0.983726117647059">
Zero anaphor detection
Table 4 gives the performance of zero anaphor de-
tection, which achieves 70.05%, 83.24% and 76.08
in precision, recall and F-measure, respectively.
Here, the lower precision is much due to the simple
heuristic rules used to generate zero anaphors can-
didates. In fact, the ratio of positive and negative
instances reaches about 1:12. However, this ratio is
much better than that (1:30) using the heuristic rule
as described in Zhao and Ng (2007). It is also
worth to point out that lower precision higher re-
call is much beneficial than higher precision lower
recall as higher recall means less filtering of true
zero anaphors and we can still rely on anaphoricity
determination to filter out those false zero ana-
phors introduced by lower precision in zero ana-
phor detectio
</bodyText>
<tableCaption confidence="0.993953">
Table 4: Performance of zero anaphor detection
</tableCaption>
<subsectionHeader confidence="0.37915">
Anaphoricity determination
</subsectionHeader>
<bodyText confidence="0.999686263157895">
Table 5 gives the performance of anaphoricity de-
termination. It shows that anaphoricity determina-
tion on golden zero anaphors achieves very good
performance of 89.83%, 84.21% and 86.93 in pre-
cision, recall and F-measure, respectively, although
useful information, such as gender and number, is
not available in anaphoricity determination. This
indicates the critical role of the structural informa-
tion in anaphoricity determination of zero anaphors.
It also shows that anaphoricity determination on
automatic zero anaphor detection achieves 77.96%,
53.97% and 63.78 in precision, recall and F-
measure, respectively. In comparison with ana-
phoricity determination on golden zero anaphors,
anaphoricity determination on automatic zero ana-
phor detection lowers the performance by about 23
in F-measure. This indicates the importance and
the necessity for further research in zero anaphor
detection.
</bodyText>
<table confidence="0.997819666666667">
P% R% F
golden zero anaphors 89.83 84.21 86.93
zero anaphor detection 77.96 53.97 63.78
</table>
<tableCaption confidence="0.999709">
Table 5: Performance of anaphoricity determination
</tableCaption>
<subsectionHeader confidence="0.835034">
Antecedent identification
</subsectionHeader>
<bodyText confidence="0.999137214285714">
Table 6 gives the performance of antecedent iden-
tification given golden zero anaphors. It shows that
antecedent identification on golden anaphoric zero
anaphors achieves 88.93%, 68.36% and 77.29 in
precision, recall and F-measure, respectively. It
also shows that antecedent identification on auto-
matic anaphoricity determination achieves 80.38%,
47.28% and 59.24 in precision, recall and F-
measure, respectively, with a decrease of about 8%
in precision, about 21% in recall and about 18% in
F-measure, in comparison with antecedent identifi-
cation on golden anaphoric zero anaphors. This
indicates the critical role of anaphoricity determi-
nation in antecedent identification.
</bodyText>
<table confidence="0.972023">
P% R% F
golden anaphoric zero ana- 88.90 68.36 77.29
phors
anaphoricity determination 80.38 47.28 59.54
</table>
<tableCaption confidence="0.969134">
Table 6: Performance of antecedent identification given
golden zero anaphors
</tableCaption>
<subsubsectionHeader confidence="0.199724">
Overall: zero anaphora resolution
</subsubsectionHeader>
<bodyText confidence="0.979067727272727">
Table 7 gives the performance of overall zero
anaphora resolution with automatic zero anaphor
detection, anaphoricity determination and antece-
dent identification. It shows that our tree kernel-
based framework achieves 77.66%, 31.74% and
45.06 in precision, recall and F-measure. In com-
parison with Table 6, it shows that the errors
caused by automatic zero anaphor detection de-
crease the performance of overall zero anaphora
resolution by about 14 in F-measure, in compari-
son with golden zero anaphors.
</bodyText>
<table confidence="0.93467">
P% R% F
70.05 83.24 76.08
888
P% R% F
77.66 31.74 45.06
</table>
<tableCaption confidence="0.999426">
Table 7: Performance of zero anaphora resolution
</tableCaption>
<bodyText confidence="0.9305728">
Figure 7 shows the learning curve of zero
anaphora resolution with the increase of the num-
ber of the documents in experimentation, with the
horizontal axis the number of the documents used
and the vertical axis the F-measure. It shows that
the F-measure is about 42.5 when 20 documents
are used in experimentation. This figure increases
very fast to about 45 when 50 documents are used
while further increase of documents only slightly
improves the performance.
</bodyText>
<figureCaption confidence="0.9966715">
Figure 7: Learning curve of zero anaphora resolution
over the number of the documents in experimentation
</figureCaption>
<bodyText confidence="0.99988575">
Table 8 shows the detailed performance of zero
anaphora resolution over different sentence dis-
tance between a zero anaphor and its antecedent. It
is expected that both the precision and the recall of
intra-sentential resolution are much higher than
those of inter-sentential resolution, largely due to
the much more dependency of intra-sentential an-
tecedent identification on the parse tree structures.
</bodyText>
<table confidence="0.995941">
Sentence distance P% R% F
0 85.12 33.28 47.85
1 46.55 23.64 31.36
2 - - -
</table>
<tableCaption confidence="0.993095">
Table 8: Performance of zero anaphora resolution over
sentence distances
</tableCaption>
<bodyText confidence="0.995670142857143">
Table 9 shows the detailed performance of zero
anaphora resolution over the two major zero
anaphora categories, “-NONE- *PRO*” and “-
NONE- *pro*”. It shows that our tree kernel-based
framework achieves comparable performance on
them, both with high precision and low recall. This
is in agreement with the overall performance.
</bodyText>
<table confidence="0.976755666666667">
ID Category P% R% F
3 -NONE- *PRO* 79.37 34.23 47.83
4 -NONE- *pro* 77.03 30.82 44.03
</table>
<tableCaption confidence="0.9794015">
Table 9: Performance of zero anaphora resolution over
major zero anaphora categories
</tableCaption>
<subsectionHeader confidence="0.999612">
5.2 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999839">
As a representative in Chinese zero anaphora reso-
lution, Zhao and Ng (2007) focused on anaphoric-
ity determination and antecedent identification
using feature-based methods. In this subsection, we
will compare our tree kernel-based framework with
theirs in details.
</bodyText>
<subsectionHeader confidence="0.462661">
Corpus
</subsectionHeader>
<bodyText confidence="0.999891866666667">
Zhao and Ng (2007) used a private corpus from
Converse (2006). Although their corpus contains
205 documents from CBT 3.0, it only deals with
the zero anaphors under the zero anaphora cate-
gory of “-NONE- *pro*” for dropped sub-
jects/objects. Furthermore, Zhao and Ng (2007)
only considered zero anaphors with explicit noun
phrase referents and discarded zero anaphors with
split antecedents (i.e. split into two separate noun
phrases) or referring to entities. As a result, their
corpus is only about half of our corpus in the num-
ber of zero anaphors and anaphoric zero anaphors.
Besides, our corpus deals with all the types of zero
anaphors and all the categories of zero anaphora
except zero cataphora.
</bodyText>
<subsectionHeader confidence="0.561003">
Method
</subsectionHeader>
<bodyText confidence="0.999952">
Zhao and Ng (2007) applied feature-based methods
on anaphoricity determination and antecedent iden-
tification with most of features structural in nature.
For zero anaphor detection, they used a very sim-
ple heuristic rule to generate zero anaphor candi-
dates. Although this rule can recover almost all the
zero anaphors, it suffers from very low precision
by introducing too many false zero anaphors and
thus may lead to low performance in anaphoricity
determination, much due to the imbalance between
positive and negative training examples with the
ratio up to about 1:30.
In comparison, we propose a tree kernel-based
unified framework for all the three sub-tasks in
zero anaphora resolution. In particular, different
parse tree structures are constructed for different
sub-tasks. Besides, a context sensitive convolution
tree kernel is employed to directly compute the
similarity between the parse trees.
For fair comparison with Zhao and Ng (2007),
we duplicate their system and evaluate it on our
developed Chinese zero anaphora corpus, using the
same J48 decision tree learning algorithm in Weka
and the same feature sets for anaphoricity determi-
nation and antecedent identification.
</bodyText>
<figure confidence="0.501125125">
auto ZA and AZA
46
45
44
43
42
41
20 30 40 50 60 70 80 90 100
</figure>
<page confidence="0.995054">
889
</page>
<bodyText confidence="0.998701076923077">
Table 10 gives the performance of the feature-
based method, as described in Zhao and Ng (2007),
in anaphoricity determination on our developed
corpus. In comparison with the tree kernel-based
method in this paper, the feature-based method
performs about 16 lower in F-measure, largely due
to the difference in precision (63.61% vs 89.83%),
when golden zero anaphors are given. It also
shows that, when our tree kernel-based zero ana-
phor detector is employed 2 , the feature-based
method gets much lower precision with a gap of
about 31%, although it achieves slightly higher
recall.
</bodyText>
<table confidence="0.997406666666667">
P% R% F
golden zero anaphors 63.61 79.71 70.76
zero anaphor detection 46.17 57.69 51.29
</table>
<tableCaption confidence="0.934887666666667">
Table 10: Performance of the feature-based method
(Zhao and Ng 2007) in anaphoricity determination on
our developed corpus
</tableCaption>
<table confidence="0.9998753">
P% R% F
golden anaphoric zero ana- 77.45 51.97 62.20
phors
golden zero anaphpors and 75.17 29.69 42.57
feature-based anaphoricity
determination
overall: tree kernel-based 70.67 23.64 35.43
zero anaphor detection and
feature-based anaphoricity
determination
</table>
<tableCaption confidence="0.995407333333333">
Table 11: Performance of the feature-based method
(Zhao and Ng 2007) in antecedent identification on our
developed corpus
</tableCaption>
<bodyText confidence="0.983440933333333">
Table 11 gives the performance of the feature-
based method, as described in Zhao and Ng (2007),
in antecedent identification on our developed cor-
pus. In comparison with our tree kernel-based
method, it shows that 1) when using golden ana-
phoric zero anaphors, the feature-based method
performs about 11%, 17% and 15 lower in preci-
sion, recall and F-measure, respectively; 2) when
golden zero anaphors are given and feature-based
anaphoricity determination is applied, the feature-
based method performs about 5%, 18% and 17
lower in precision, recall and F-measure, respec-
tively; and 3) when tree kernel-based zero anaphor
detection and feature-based anaphoricity determi-
nation are applied, the feature-based method per-
</bodyText>
<footnote confidence="0.63090525">
2 We do not apply the simple heuristic rule, as adopted in Zhao
and Ng (2007), in zero anaphor detection, due to its much
lower performance, for fair comparison on the other two sub-
tsaks..
</footnote>
<bodyText confidence="0.999926363636364">
forms about 7%, 8% and 10 lower in precision,
recall and F-measure, respectively.
In summary, above comparison indicates the
critical role of the structural information in zero
anaphora resolution, given the fact that most of
features in the feature-based methods in Zhao and
Ng (2007) are also structural, and the necessity of
tree kernel methods in modeling such structural
information, even if more feature engineering in
the feature-based methods may improve the per-
formance to a certain extent.
</bodyText>
<sectionHeader confidence="0.993264" genericHeader="conclusions">
6 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.999988945945946">
This paper proposes a tree kernel-based unified
framework for zero anaphora resolution, which can
be divided into three sub-tasks: zero anaphor de-
tection, anaphoricity determination and antecedent
identification.
The major contributions of this paper include: 1)
We release a wide-coverage Chinese zero anaphora
corpus of 100 documents, which adds a layer of
annotation to the manually-parsed sentences in the
Chinese Treebank (CTB) 6.0. 2) To our best
knowledge, this is the first systematic work dealing
with all the three sub-tasks in Chinese zero anaph-
ora resolution via a unified framework. 3) Em-
ployment of tree kernel-based methods indicates
the critical role of the structural information in zero
anaphora resolution and the necessity of tree kernel
methods in modeling such structural information.
In the future work, we will systematically evalu-
ate our framework on automatically-generated
parse trees, construct more effective parse tree
structures for different sub-tasks of zero anaphora
resolution, and explore joint learning among the
three sub-tasks.
Besides, we only consider zero anaphors driven
by a verb predicate phrase node in this paper. In
the future work, we will consider other situations.
Actually, among the remaining 7% zero anaphors,
about 5% are driven by a preposition phrase (PP)
node, and 2% are driven by a noun phrase (NP)
node. However, our preliminary experiments show
that simple inclusion of those PP-driven and NP-
driven zero anaphors will largely increase the im-
balance between positive and negative instances,
which significantly decrease the performance.
Finally, we will devote more on further develop-
ing our corpus, with the ultimate mission of anno-
tating all the documents in CBT 6.0.
</bodyText>
<page confidence="0.995206">
890
</page>
<sectionHeader confidence="0.998808" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990646">
This research was supported by Projects 60873150,
90920004 and 61003153 under the National Natu-
ral Science Foundation of China.
</bodyText>
<sectionHeader confidence="0.995137" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999635971830986">
S. Converse. 2006. Pronominal Anaphora Resolution in
Chinese. Ph.D. Thesis, Department of Computer and
Information Science. University of Pennsylvania.
M. Collins and N. Duffy. 2001. Convolution kernels for
natural language. NIPS’2001:625-632.
A. Ferrandez and J. Peral. 2000. A computational ap-
proach to zero-pronouns in Spanish. ACL&apos;2000:166-
172.
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting
syntactic patterns as clues in zero-anaphora resolu-
tion. COLING-ACL&apos;2006:625-632
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun
resolution based on ranking rules and machine
learning. EMNLP&apos;2003:184-191
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employing the
Centering Theory in Pronoun Resolution from the
Semantic Perspective. EMNLP’2009: 987-996
C. N. Li and S. A. Thompson. 1979. Third-person pro-
nouns and zero-anaphora in Chinese discourse. Syn-
tax and Semantics, 12:311-335.
W. Li. 2004. Topic chains in Chinese discourse. Dis-
course Processes, 37(1):25-45.
A. Moschitti. 2004. A Study on Convolution Kernels for
Shallow Semantic Parsing, ACL’2004.
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D.
Qian. 2008. Exploiting constituent dependencies for
tree kernel-based semantic relation extraction.
COLING’2008:697-704
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing Japanese anaphora intergrat-
ing zero pronoun detection and resolution.
COLING&apos;2002:911-917
R. Sasano. D. Kawahara and S. Kurohashi. 2008. A
fully-lexicalized probabilistic model for Japanese
zero anaphora resolution. COLING&apos;2008:769-776
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrase. Computational Linguistics, 2001, 27(4):521-
544.
V. Ng and C. Cardie 2002. Improving machine learning
approaches to coreference resolution. ACL’2002:
104-111
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003.
Coreference Resolution Using Competition Learning
Approach. ACL’2003:177-184
X.F. Yang, J. Su and C.L. Tan 2008. A Twin-Candidate
Model for Learning-Based Anaphora Resolution.
Computational Linguistics 34(3):327-356
N. Xue, F. Xia, F.D. Chiou and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207-238.
X.F. Yang, J. Su and C.L. Tan. 2006. Kernel-based
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL&apos;2006:41-48.
D. Zelenko, A. Chinatsu and R. Anthony. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3(2003):1083-1106
S. Zhao and H.T. Ng. 2007. Identification and Resolu-
tion of Chinese Zero Pronouns: A Machine Learning
Approach. EMNLP-CoNLL&apos;2007:541-550.
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods.
ACL’2005:419-426
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Context-
sensitive convolution tree kernel for pronoun resolu-
tion. IJCNLP&apos;2008:25-31
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007.
Tree kernel-based relation extraction with context-
sensitive structured parse tree information. EMNLP-
CoNLL’2007:728-736
</reference>
<page confidence="0.998493">
891
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434727">
<title confidence="0.9983305">A Tree Kernel-based Unified for Chinese Zero Anaphora Resolution</title>
<author confidence="0.99237">Kong Guodong</author>
<affiliation confidence="0.9864115">JiangSu Provincial Key Lab for Computer Information Processing School of Computer Science and Technology Soochow University</affiliation>
<email confidence="0.884289">kongfang@suda.edu.cn</email>
<email confidence="0.884289">gdzhou@suda.edu.cn</email>
<abstract confidence="0.975576375">This paper proposes a unified framework for zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, anaphoricity determination and antecedent identification. In particular, all the three sub-tasks are addressed using tree kernel-based methods with appropriate syntactic parse tree structures. Experimental results on a Chinese zero anaphora corpus show that the proposed tree kernel-based methods significantly outperform the feature-based ones. This indicates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel-based methods in modeling such structural information. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Converse</author>
</authors>
<title>Pronominal Anaphora Resolution in Chinese.</title>
<date>2006</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Computer and Information Science. University of Pennsylvania.</institution>
<contexts>
<context position="2659" citStr="Converse 2006" startWordPosition="401" endWordPosition="402">lish, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the necessity of zero anaphora resolution in Chinese anaphora resolution. Since zero anaphors give little hints (e.g. number or gender) about their possible antecedents, zero anaphora resolution is much more challenging than traditional anaphora resolution. Although Chinese zero anaphora has been widely studied in the linguistics research (Li and Thompson 1979; Li 2004), only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution (Converse 2006; Zhao and Ng 2007). Moreover, zero anaphor detection, as a critical component for real applications of zero anaphora resolution, has been largely ignored. This paper proposes a unified framework for Chinese zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, which detects zero anaphors from a text, anaphoricity determination, which determines whether a zero anaphor is anaphoric or not, and antecedent identification, which finds the antecedent for an anaphoric zero anaphor. To our best knowledge, this is the first systematic work dealing with all the th</context>
<context position="4855" citStr="Converse (2006)" startWordPosition="736" endWordPosition="737">inese zero anaphora corpus, which is available for research purpose. Section 4 presents our tree kernel-based unified framework in zero anaphora resolution. Section 5 reports the experimental results. Finally, we conclude our work in Section 6. 2 Related Work This section briefly overviews the related work on both zero anaphora resolution and tree kernelbased anaphora resolution. 2.1 Zero anaphora resolution Although zero anaphors are prevalent in many languages, such as Chinese, Japanese and Spanish, there only have a few works on zero anaphora resolution. Zero anaphora resolution in Chinese Converse (2006) developed a Chinese zero anaphora corpus which only deals with zero anaphora category “-NONE- *pro*” for dropped subjects/objects and ignores other categories, such as “-NONE- *PRO*” for non-overt subjects in nonfinite clauses. Besides, Converse (2006) proposed a rule-based method to resolve the anaphoric zero anaphors only. The method did not consider zero anaphor detection and anaphoric identification, and performed zero anaphora resolution using the Hobbs algorithm (Hobbs, 1978), assuming the availability of golden anaphoric zero anaphors and golden parse trees. Instead, Zhao and Ng (2007)</context>
<context position="26868" citStr="Converse (2006)" startWordPosition="4198" endWordPosition="4199">precision and low recall. This is in agreement with the overall performance. ID Category P% R% F 3 -NONE- *PRO* 79.37 34.23 47.83 4 -NONE- *pro* 77.03 30.82 44.03 Table 9: Performance of zero anaphora resolution over major zero anaphora categories 5.2 Comparison with previous work As a representative in Chinese zero anaphora resolution, Zhao and Ng (2007) focused on anaphoricity determination and antecedent identification using feature-based methods. In this subsection, we will compare our tree kernel-based framework with theirs in details. Corpus Zhao and Ng (2007) used a private corpus from Converse (2006). Although their corpus contains 205 documents from CBT 3.0, it only deals with the zero anaphors under the zero anaphora category of “-NONE- *pro*” for dropped subjects/objects. Furthermore, Zhao and Ng (2007) only considered zero anaphors with explicit noun phrase referents and discarded zero anaphors with split antecedents (i.e. split into two separate noun phrases) or referring to entities. As a result, their corpus is only about half of our corpus in the number of zero anaphors and anaphoric zero anaphors. Besides, our corpus deals with all the types of zero anaphors and all the categorie</context>
</contexts>
<marker>Converse, 2006</marker>
<rawString>S. Converse. 2006. Pronominal Anaphora Resolution in Chinese. Ph.D. Thesis, Department of Computer and Information Science. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<pages>2001--625</pages>
<contexts>
<context position="8939" citStr="Collins and Duffy 2001" startWordPosition="1345" endWordPosition="1348"> anaphora resolution Although there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution kernels for natural language. NIPS’2001:625-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferrandez</author>
<author>J Peral</author>
</authors>
<title>A computational approach to zero-pronouns in Spanish.</title>
<date>2000</date>
<pages>2000--166</pages>
<contexts>
<context position="7879" citStr="Ferrandez and Peral (2000)" startWordPosition="1194" endWordPosition="1197"> However, this model needed case frames to detect zero anaphors and a largescale corpus to construct these case frames automatically. For Japanese zero anaphora, we do not see any reports about zero anaphora categories. Moreover, all the above related works we can find on Japanese zero anaphora resolution ignore zero anaphor detection, focusing on either anaphoricity determination or antecedent identification. Maybe, it is easy to detect zero anaphors in Japanese. However, it is out of the scope of our knowledge and this paper. Zero anaphora resolution in Spanish As the only work we can find, Ferrandez and Peral (2000) proposed a hand-engineered rule-based method for both anaphoricity determination and 883 necessary structural information in the parse tree for anaphora resolution of pronouns and the context-sensitive convolution tree kernel much outperformed other tree kernels. antecedent identification. That is, they ignored zero anaphor detection. Besides, they only dealt with zero anaphors that were in the subject position. 2.2 Tree kernel-based anaphora resolution Although there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored i</context>
</contexts>
<marker>Ferrandez, Peral, 2000</marker>
<rawString>A. Ferrandez and J. Peral. 2000. A computational approach to zero-pronouns in Spanish. ACL&apos;2000:166-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Exploiting syntactic patterns as clues in zero-anaphora resolution.</title>
<date>2006</date>
<pages>2006--625</pages>
<contexts>
<context position="6856" citStr="Iida et al. (2006)" startWordPosition="1036" endWordPosition="1039"> the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem. Isozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor’s grammatical case was already determined by a zero anaphor detector. Iida et al. (2006) explored a machine learning method for the sub-task of antecedent identification using rich syntactic pattern features, assuming the availability of golden anaphoric zero anaphors. Sasano et al. (2008) proposed a fully-lexicalized probabilistic model for zero anaphora resolution, which estimated case assignments for the overt case components and the antecedents of zero anaphors simultaneously. However, this model needed case frames to detect zero anaphors and a largescale corpus to construct these case frames automatically. For Japanese zero anaphora, we do not see any reports about zero anap</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2006</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntactic patterns as clues in zero-anaphora resolution. COLING-ACL&apos;2006:625-632</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
</authors>
<title>Japanese zero pronoun resolution based on ranking rules and machine learning.</title>
<date>2003</date>
<pages>2003--184</pages>
<contexts>
<context position="6595" citStr="Isozaki and Hirao (2003)" startWordPosition="995" endWordPosition="998">oducing too many false zero anaphors and thus leads to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples. Zero anaphora resolution in Japanese Seki et al. (2002) proposed a probabilistic model for the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem. Isozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor’s grammatical case was already determined by a zero anaphor detector. Iida et al. (2006) explored a machine learning method for the sub-task of antecedent identification using rich syntactic pattern features, assuming the availability of golden anaphoric zero anaphors. Sasano et al. (2008) proposed a fully-lexicalized probabilistic model for zero anaphora resolution, which estimated case assignments for the overt case compo</context>
</contexts>
<marker>Isozaki, Hirao, 2003</marker>
<rawString>H. Isozaki and T. Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. EMNLP&apos;2003:184-191</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Kong</author>
<author>G D Zhou</author>
<author>Q M Zhu</author>
</authors>
<date>2009</date>
<booktitle>Employing the Centering Theory in Pronoun Resolution from the Semantic Perspective. EMNLP’2009:</booktitle>
<pages>987--996</pages>
<contexts>
<context position="1569" citStr="Kong et al. 2009" startWordPosition="229" endWordPosition="232">ledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 1 Introduction As one of the most important techniques in discourse analysis, anaphora resolution has been a focus of research in Natural Language Processing (NLP) for decades and achieved much success in English recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Yang et al. 2003, 2008; Kong et al. 2009). However, there is little work on anaphora resolution in Chinese. A major reason for this phe* Corresponding author nomenon is that Chinese, unlike English, is a prodrop language, whereas in English, definite noun phrases (e.g. the company) and overt pronouns (e.g. he) are frequently employed as referring expressions, which refer to preceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the</context>
</contexts>
<marker>Kong, Zhou, Zhu, 2009</marker>
<rawString>F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employing the Centering Theory in Pronoun Resolution from the Semantic Perspective. EMNLP’2009: 987-996</rawString>
</citation>
<citation valid="true">
<authors>
<author>C N Li</author>
<author>S A Thompson</author>
</authors>
<title>Third-person pronouns and zero-anaphora</title>
<date>1979</date>
<booktitle>in Chinese discourse. Syntax and Semantics,</booktitle>
<pages>12--311</pages>
<contexts>
<context position="2528" citStr="Li and Thompson 1979" startWordPosition="379" endWordPosition="382">ceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the necessity of zero anaphora resolution in Chinese anaphora resolution. Since zero anaphors give little hints (e.g. number or gender) about their possible antecedents, zero anaphora resolution is much more challenging than traditional anaphora resolution. Although Chinese zero anaphora has been widely studied in the linguistics research (Li and Thompson 1979; Li 2004), only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution (Converse 2006; Zhao and Ng 2007). Moreover, zero anaphor detection, as a critical component for real applications of zero anaphora resolution, has been largely ignored. This paper proposes a unified framework for Chinese zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, which detects zero anaphors from a text, anaphoricity determination, which determines whether a zero anaphor is anaphoric or not, and antecedent identification, which f</context>
</contexts>
<marker>Li, Thompson, 1979</marker>
<rawString>C. N. Li and S. A. Thompson. 1979. Third-person pronouns and zero-anaphora in Chinese discourse. Syntax and Semantics, 12:311-335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
</authors>
<title>Topic chains in Chinese discourse.</title>
<date>2004</date>
<booktitle>Discourse Processes,</booktitle>
<pages>37--1</pages>
<contexts>
<context position="2538" citStr="Li 2004" startWordPosition="383" endWordPosition="384">2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the necessity of zero anaphora resolution in Chinese anaphora resolution. Since zero anaphors give little hints (e.g. number or gender) about their possible antecedents, zero anaphora resolution is much more challenging than traditional anaphora resolution. Although Chinese zero anaphora has been widely studied in the linguistics research (Li and Thompson 1979; Li 2004), only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution (Converse 2006; Zhao and Ng 2007). Moreover, zero anaphor detection, as a critical component for real applications of zero anaphora resolution, has been largely ignored. This paper proposes a unified framework for Chinese zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, which detects zero anaphors from a text, anaphoricity determination, which determines whether a zero anaphor is anaphoric or not, and antecedent identification, which finds the a</context>
</contexts>
<marker>Li, 2004</marker>
<rawString>W. Li. 2004. Topic chains in Chinese discourse. Discourse Processes, 37(1):25-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing,</title>
<date>2004</date>
<contexts>
<context position="8956" citStr="Moschitti 2004" startWordPosition="1349" endWordPosition="1350">hough there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction sch</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>A. Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing, ACL’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Qian</author>
<author>G D Zhou</author>
<author>F Kong</author>
<author>Q M Zhu</author>
<author>P D Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<pages>2008--697</pages>
<contexts>
<context position="9068" citStr="Qian et al. 2008" startWordPosition="1367" endWordPosition="1370">methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. Howeve</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. COLING’2008:697-704</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seki</author>
<author>A Fujii</author>
<author>T Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing Japanese anaphora intergrating zero pronoun detection and resolution.</title>
<date>2002</date>
<pages>2002--911</pages>
<contexts>
<context position="6203" citStr="Seki et al. (2002)" startWordPosition="936" endWordPosition="939">ero anaphors with explicit noun phrase referents and discarded those with split antecedents or referring to events. Moreover, they focused on the sub-tasks of anaphoricity determination and antecedent identification. For zero anaphor detection, a simple heuristic rule was employed. Although this rule can recover almost all the zero anaphors, it suffers from very low precision by introducing too many false zero anaphors and thus leads to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples. Zero anaphora resolution in Japanese Seki et al. (2002) proposed a probabilistic model for the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem. Isozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor’s grammatical case was already dete</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic method for analyzing Japanese anaphora intergrating zero pronoun detection and resolution. COLING&apos;2002:911-917</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kawahara</author>
<author>S Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese zero anaphora resolution.</title>
<date>2008</date>
<pages>2008--769</pages>
<marker>Kawahara, Kurohashi, 2008</marker>
<rawString>R. Sasano. D. Kawahara and S. Kurohashi. 2008. A fully-lexicalized probabilistic model for Japanese zero anaphora resolution. COLING&apos;2008:769-776</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrase. Computational Linguistics,</title>
<date>2001</date>
<pages>27--4</pages>
<contexts>
<context position="1506" citStr="Soon et al. 2001" startWordPosition="216" endWordPosition="219">hods in modeling such structural information. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 1 Introduction As one of the most important techniques in discourse analysis, anaphora resolution has been a focus of research in Natural Language Processing (NLP) for decades and achieved much success in English recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Yang et al. 2003, 2008; Kong et al. 2009). However, there is little work on anaphora resolution in Chinese. A major reason for this phe* Corresponding author nomenon is that Chinese, unlike English, is a prodrop language, whereas in English, definite noun phrases (e.g. the company) and overt pronouns (e.g. he) are frequently employed as referring expressions, which refer to preceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. Th</context>
<context position="20531" citStr="Soon et al. (2001)" startWordPosition="3214" endWordPosition="3217">dopt the Dynamic Expansion Tree, as proposed in Zhou et al. (2008), which takes predicate- and antecedent competitor-related information into consideration. Figure 6 illustrates an example parse tree structure for antecedent identification, corresponding to Figure 1 with the anaphoric zero anaphor Φ 2 and the antecedent candidate “建筑行)1/building action” in consideration. Figure 6: An example parse tree structure for antecedent identification with the anaphoric zero anaphor (D2 and the antecedent candidate “&amp;RJT)1/building action” in consideration In this paper, we adopt a similar procedure as Soon et al. (2001) in antecedent identification. BeVV IP fA NP appear NN prevent MIL NP-SBJ VV VP NP-OBJ 887 sides, since all the anaphoric zero anaphors have their antecedents at most one sentence away, we only consider antecedent candidates which are at most one sentence away. In particular, a documentlevel parse tree for an entire document is constructed by attaching the parse trees of all its sentences to a new-added upper node, as done in Yang et al. (2006), to deal with inter-sentential ones. 5 Experimentation and Discussion We have systematically evaluated our tree kernelbased unified framework on our de</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W.M. Soon, H.T. Ng and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrase. Computational Linguistics, 2001, 27(4):521-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<volume>2002</volume>
<pages>104--111</pages>
<contexts>
<context position="1526" citStr="Ng and Cardie 2002" startWordPosition="220" endWordPosition="223">uch structural information. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 1 Introduction As one of the most important techniques in discourse analysis, anaphora resolution has been a focus of research in Natural Language Processing (NLP) for decades and achieved much success in English recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Yang et al. 2003, 2008; Kong et al. 2009). However, there is little work on anaphora resolution in Chinese. A major reason for this phe* Corresponding author nomenon is that Chinese, unlike English, is a prodrop language, whereas in English, definite noun phrases (e.g. the company) and overt pronouns (e.g. he) are frequently employed as referring expressions, which refer to preceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the pre</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie 2002. Improving machine learning approaches to coreference resolution. ACL’2002: 104-111</rawString>
</citation>
<citation valid="true">
<authors>
<author>X F Yang</author>
<author>G D Zhou</author>
<author>J Su</author>
<author>C L Chew</author>
</authors>
<title>Coreference Resolution Using Competition Learning Approach.</title>
<date>2003</date>
<pages>2003--177</pages>
<contexts>
<context position="1544" citStr="Yang et al. 2003" startWordPosition="224" endWordPosition="227">mation. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 1 Introduction As one of the most important techniques in discourse analysis, anaphora resolution has been a focus of research in Natural Language Processing (NLP) for decades and achieved much success in English recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Yang et al. 2003, 2008; Kong et al. 2009). However, there is little work on anaphora resolution in Chinese. A major reason for this phe* Corresponding author nomenon is that Chinese, unlike English, is a prodrop language, whereas in English, definite noun phrases (e.g. the company) and overt pronouns (e.g. he) are frequently employed as referring expressions, which refer to preceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero an</context>
</contexts>
<marker>Yang, Zhou, Su, Chew, 2003</marker>
<rawString>X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. Coreference Resolution Using Competition Learning Approach. ACL’2003:177-184</rawString>
</citation>
<citation valid="true">
<authors>
<author>X F Yang</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>A Twin-Candidate Model for Learning-Based Anaphora Resolution.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<pages>34--3</pages>
<marker>Yang, Su, Tan, 2008</marker>
<rawString>X.F. Yang, J. Su and C.L. Tan 2008. A Twin-Candidate Model for Learning-Based Anaphora Resolution. Computational Linguistics 34(3):327-356</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>11--2</pages>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F.D. Chiou and M. Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X F Yang</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Kernel-based pronoun resolution with structured syntactic knowledge.</title>
<date>2006</date>
<pages>2006--41</pages>
<contexts>
<context position="9203" citStr="Yang et al. (2006)" startWordPosition="1385" endWordPosition="1388"> feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. However, among the three constructed parse tree structures, there exists no obvious overwhelming one, which can well cover structured syntact</context>
<context position="20979" citStr="Yang et al. (2006)" startWordPosition="3294" endWordPosition="3297">th the anaphoric zero anaphor (D2 and the antecedent candidate “&amp;RJT)1/building action” in consideration In this paper, we adopt a similar procedure as Soon et al. (2001) in antecedent identification. BeVV IP fA NP appear NN prevent MIL NP-SBJ VV VP NP-OBJ 887 sides, since all the anaphoric zero anaphors have their antecedents at most one sentence away, we only consider antecedent candidates which are at most one sentence away. In particular, a documentlevel parse tree for an entire document is constructed by attaching the parse trees of all its sentences to a new-added upper node, as done in Yang et al. (2006), to deal with inter-sentential ones. 5 Experimentation and Discussion We have systematically evaluated our tree kernelbased unified framework on our developed Chinese zero anaphora corpus, as described in Section 3.2. Besides, in order to focus on zero anaphor resolution itself and compare with related work, all the experiments are done on golden parse trees provided by CTB 6.0. Finally, all the performances are achieved using 5-fold cross validation. 5.1 Experimental results Zero anaphor detection Table 4 gives the performance of zero anaphor detection, which achieves 70.05%, 83.24% and 76.0</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>X.F. Yang, J. Su and C.L. Tan. 2006. Kernel-based pronoun resolution with structured syntactic knowledge. COLING-ACL&apos;2006:41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>A Chinatsu</author>
<author>R Anthony</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--2003</pages>
<contexts>
<context position="9007" citStr="Zelenko et al. 2003" startWordPosition="1355" endWordPosition="1358"> zero anaphora resolution in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpan</context>
</contexts>
<marker>Zelenko, Chinatsu, Anthony, 2003</marker>
<rawString>D. Zelenko, A. Chinatsu and R. Anthony. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3(2003):1083-1106</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>H T Ng</author>
</authors>
<title>Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach.</title>
<date>2007</date>
<pages>2007--541</pages>
<contexts>
<context position="2678" citStr="Zhao and Ng 2007" startWordPosition="403" endWordPosition="406">s percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the necessity of zero anaphora resolution in Chinese anaphora resolution. Since zero anaphors give little hints (e.g. number or gender) about their possible antecedents, zero anaphora resolution is much more challenging than traditional anaphora resolution. Although Chinese zero anaphora has been widely studied in the linguistics research (Li and Thompson 1979; Li 2004), only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution (Converse 2006; Zhao and Ng 2007). Moreover, zero anaphor detection, as a critical component for real applications of zero anaphora resolution, has been largely ignored. This paper proposes a unified framework for Chinese zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, which detects zero anaphors from a text, anaphoricity determination, which determines whether a zero anaphor is anaphoric or not, and antecedent identification, which finds the antecedent for an anaphoric zero anaphor. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks via a</context>
<context position="5455" citStr="Zhao and Ng (2007)" startWordPosition="822" endWordPosition="825">se Converse (2006) developed a Chinese zero anaphora corpus which only deals with zero anaphora category “-NONE- *pro*” for dropped subjects/objects and ignores other categories, such as “-NONE- *PRO*” for non-overt subjects in nonfinite clauses. Besides, Converse (2006) proposed a rule-based method to resolve the anaphoric zero anaphors only. The method did not consider zero anaphor detection and anaphoric identification, and performed zero anaphora resolution using the Hobbs algorithm (Hobbs, 1978), assuming the availability of golden anaphoric zero anaphors and golden parse trees. Instead, Zhao and Ng (2007) proposed featurebased methods to zero anaphora resolution on the same corpus from Convese (2006). However, they only considered zero anaphors with explicit noun phrase referents and discarded those with split antecedents or referring to events. Moreover, they focused on the sub-tasks of anaphoricity determination and antecedent identification. For zero anaphor detection, a simple heuristic rule was employed. Although this rule can recover almost all the zero anaphors, it suffers from very low precision by introducing too many false zero anaphors and thus leads to low performance in anaphorici</context>
<context position="16698" citStr="Zhao and Ng (2007)" startWordPosition="2595" endWordPosition="2598">t. If yes, the anaphoricity determiner is then invoked to determine whether Z is an anaphoric zero anaphor. If yes, the antecedent identifier is finally awaked to determine its antecedent. In the future work, we will explore better ways of integrating the three sub-tasks (e.g. joint learning). 4.1 Zero anaphor detection At the first glance, it seems that a zero anaphor can occur between any two constituents in a parse tree. Fortunately, an exploration of our corpus shows that a zero anaphor always occurs just before a predicate1 phrase node (e.g. VP). This phenomenon has also been employed in Zhao and Ng (2007) in generating zero anaphor candidates. In particular, if the predicate phrase node occurs in a coordinate structure or is modified by an adverbial node, we only need to consider its parent. As shown in Figure 1, zero anaphors may occur immediately to the left ofd E/guide, P-LL/avoid, ,`f4fl /appear, &amp;9 /according to, M A /combine, ,`f4 R /promulgate, which cover the four true zero anaphors. Therefore, it is simple but reliable in applying above heuristic rules to generate zero anaphor candidates. Given a zero anaphor candidate, it is critical to construct a proper parse tree structure for tre</context>
<context position="21927" citStr="Zhao and Ng (2007)" startWordPosition="3443" endWordPosition="3446"> are done on golden parse trees provided by CTB 6.0. Finally, all the performances are achieved using 5-fold cross validation. 5.1 Experimental results Zero anaphor detection Table 4 gives the performance of zero anaphor detection, which achieves 70.05%, 83.24% and 76.08 in precision, recall and F-measure, respectively. Here, the lower precision is much due to the simple heuristic rules used to generate zero anaphors candidates. In fact, the ratio of positive and negative instances reaches about 1:12. However, this ratio is much better than that (1:30) using the heuristic rule as described in Zhao and Ng (2007). It is also worth to point out that lower precision higher recall is much beneficial than higher precision lower recall as higher recall means less filtering of true zero anaphors and we can still rely on anaphoricity determination to filter out those false zero anaphors introduced by lower precision in zero anaphor detectio Table 4: Performance of zero anaphor detection Anaphoricity determination Table 5 gives the performance of anaphoricity determination. It shows that anaphoricity determination on golden zero anaphors achieves very good performance of 89.83%, 84.21% and 86.93 in precision,</context>
<context position="26610" citStr="Zhao and Ng (2007)" startWordPosition="4159" endWordPosition="4162"> distances Table 9 shows the detailed performance of zero anaphora resolution over the two major zero anaphora categories, “-NONE- *PRO*” and “- NONE- *pro*”. It shows that our tree kernel-based framework achieves comparable performance on them, both with high precision and low recall. This is in agreement with the overall performance. ID Category P% R% F 3 -NONE- *PRO* 79.37 34.23 47.83 4 -NONE- *pro* 77.03 30.82 44.03 Table 9: Performance of zero anaphora resolution over major zero anaphora categories 5.2 Comparison with previous work As a representative in Chinese zero anaphora resolution, Zhao and Ng (2007) focused on anaphoricity determination and antecedent identification using feature-based methods. In this subsection, we will compare our tree kernel-based framework with theirs in details. Corpus Zhao and Ng (2007) used a private corpus from Converse (2006). Although their corpus contains 205 documents from CBT 3.0, it only deals with the zero anaphors under the zero anaphora category of “-NONE- *pro*” for dropped subjects/objects. Furthermore, Zhao and Ng (2007) only considered zero anaphors with explicit noun phrase referents and discarded zero anaphors with split antecedents (i.e. split in</context>
<context position="28466" citStr="Zhao and Ng (2007)" startWordPosition="4447" endWordPosition="4450">om very low precision by introducing too many false zero anaphors and thus may lead to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples with the ratio up to about 1:30. In comparison, we propose a tree kernel-based unified framework for all the three sub-tasks in zero anaphora resolution. In particular, different parse tree structures are constructed for different sub-tasks. Besides, a context sensitive convolution tree kernel is employed to directly compute the similarity between the parse trees. For fair comparison with Zhao and Ng (2007), we duplicate their system and evaluate it on our developed Chinese zero anaphora corpus, using the same J48 decision tree learning algorithm in Weka and the same feature sets for anaphoricity determination and antecedent identification. auto ZA and AZA 46 45 44 43 42 41 20 30 40 50 60 70 80 90 100 889 Table 10 gives the performance of the featurebased method, as described in Zhao and Ng (2007), in anaphoricity determination on our developed corpus. In comparison with the tree kernel-based method in this paper, the feature-based method performs about 16 lower in F-measure, largely due to the </context>
<context position="29889" citStr="Zhao and Ng 2007" startWordPosition="4675" endWordPosition="4678"> precision with a gap of about 31%, although it achieves slightly higher recall. P% R% F golden zero anaphors 63.61 79.71 70.76 zero anaphor detection 46.17 57.69 51.29 Table 10: Performance of the feature-based method (Zhao and Ng 2007) in anaphoricity determination on our developed corpus P% R% F golden anaphoric zero ana- 77.45 51.97 62.20 phors golden zero anaphpors and 75.17 29.69 42.57 feature-based anaphoricity determination overall: tree kernel-based 70.67 23.64 35.43 zero anaphor detection and feature-based anaphoricity determination Table 11: Performance of the feature-based method (Zhao and Ng 2007) in antecedent identification on our developed corpus Table 11 gives the performance of the featurebased method, as described in Zhao and Ng (2007), in antecedent identification on our developed corpus. In comparison with our tree kernel-based method, it shows that 1) when using golden anaphoric zero anaphors, the feature-based method performs about 11%, 17% and 15 lower in precision, recall and F-measure, respectively; 2) when golden zero anaphors are given and feature-based anaphoricity determination is applied, the featurebased method performs about 5%, 18% and 17 lower in precision, recall</context>
<context position="31132" citStr="Zhao and Ng (2007)" startWordPosition="4872" endWordPosition="4875">ely; and 3) when tree kernel-based zero anaphor detection and feature-based anaphoricity determination are applied, the feature-based method per2 We do not apply the simple heuristic rule, as adopted in Zhao and Ng (2007), in zero anaphor detection, due to its much lower performance, for fair comparison on the other two subtsaks.. forms about 7%, 8% and 10 lower in precision, recall and F-measure, respectively. In summary, above comparison indicates the critical role of the structural information in zero anaphora resolution, given the fact that most of features in the feature-based methods in Zhao and Ng (2007) are also structural, and the necessity of tree kernel methods in modeling such structural information, even if more feature engineering in the feature-based methods may improve the performance to a certain extent. 6 Conclusion and Further Work This paper proposes a tree kernel-based unified framework for zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, anaphoricity determination and antecedent identification. The major contributions of this paper include: 1) We release a wide-coverage Chinese zero anaphora corpus of 100 documents, which adds a layer</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>S. Zhao and H.T. Ng. 2007. Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach. EMNLP-CoNLL&apos;2007:541-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>R Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<pages>2005--419</pages>
<contexts>
<context position="9031" citStr="Zhao and Grishman 2005" startWordPosition="1359" endWordPosition="1362">tion in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 co</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>S. Zhao and R. Grishman. 2005. Extracting relations with integrated information using kernel methods. ACL’2005:419-426</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>F Kong</author>
<author>Q M Zhu</author>
</authors>
<title>Contextsensitive convolution tree kernel for pronoun resolution.</title>
<date>2008</date>
<pages>2008--25</pages>
<contexts>
<context position="9225" citStr="Zhou et al (2008)" startWordPosition="1390" endWordPosition="1393">e main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. However, among the three constructed parse tree structures, there exists no obvious overwhelming one, which can well cover structured syntactic information. One pr</context>
<context position="15684" citStr="Zhou et al. (2008)" startWordPosition="2425" endWordPosition="2428">ates the sentence id of its antecedent, y indicates the position of the first word of its antecedent in the sentence, z indicates the position of the last word of its antecedent in the sentence, and i indicates the category id of the null constituent. Figure 3: an example sentence annotated in our corpus 4 Tree Kernel-based Framework This section presents the tree kernel-based unified framework for all the three sub-tasks in zero anaphora resolution. For each sub-task, different parse tree structures are constructed. In particular, the context-sensitive convolution tree kernel, as proposed in Zhou et al. (2008), is employed to compute the similarity between two parse trees via the SVM toolkit SVMLight. In the tree kernel-based framework, we perform the three sub-tasks, zero anaphor detection, anaphoricity determination and antecedent identification in a pipeline manner. That is, given a zero anaphor candidate Z, the zero anaphor detector is first called to determine whether Z is a zero anaphor or not. If yes, the anaphoricity determiner is then invoked to determine whether Z is an anaphoric zero anaphor. If yes, the antecedent identifier is finally awaked to determine its antecedent. In the future w</context>
<context position="19979" citStr="Zhou et al. (2008)" startWordPosition="3134" endWordPosition="3137">tructure between the previous predicate phrase node and the following predicate phrase node. Besides, we only keep those verbal phrase nodes and nominal phrase nodes. Figure 5 illustrates an example of the parse tree structure for anaphoricity determination, corresponding to Figure 1 with the zero anaphor Φ2 in consideration. VP A* phenomenon Figure 5: An example parse tree structure for anaphoricity determination with the zero anaphor (D2 in consideration 4.3 Antecedent identification To identify an antecedent for an anaphoric zero anaphor, we adopt the Dynamic Expansion Tree, as proposed in Zhou et al. (2008), which takes predicate- and antecedent competitor-related information into consideration. Figure 6 illustrates an example parse tree structure for antecedent identification, corresponding to Figure 1 with the anaphoric zero anaphor Φ 2 and the antecedent candidate “建筑行)1/building action” in consideration. Figure 6: An example parse tree structure for antecedent identification with the anaphoric zero anaphor (D2 and the antecedent candidate “&amp;RJT)1/building action” in consideration In this paper, we adopt a similar procedure as Soon et al. (2001) in antecedent identification. BeVV IP fA NP app</context>
</contexts>
<marker>Zhou, Kong, Zhu, 2008</marker>
<rawString>G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Contextsensitive convolution tree kernel for pronoun resolution. IJCNLP&apos;2008:25-31</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>M Zhang</author>
<author>D H Ji</author>
<author>Q M Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with contextsensitive structured parse tree information.</title>
<date>2007</date>
<pages>2007--728</pages>
<contexts>
<context position="9049" citStr="Zhou et al. 2007" startWordPosition="1363" endWordPosition="1366">tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promis</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. Tree kernel-based relation extraction with contextsensitive structured parse tree information. EMNLPCoNLL’2007:728-736</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>