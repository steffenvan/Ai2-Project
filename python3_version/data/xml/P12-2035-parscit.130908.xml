<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.138686">
<title confidence="0.992316">
Transforming Standard Arabic to Colloquial Arabic
</title>
<author confidence="0.992421">
Emad Mohamed, Behrang Mohit and Kemal Oflazer
</author>
<affiliation confidence="0.843623">
Carnegie Mellon University - Qatar
Doha, Qatar
</affiliation>
<email confidence="0.986903">
emohamed@qatar.cmu.edu, behrang@cmu.edu, ko@cs.cmu.edu
</email>
<sectionHeader confidence="0.993828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999115428571429">
We present a method for generating Colloquial
Egyptian Arabic (CEA) from morphologically dis-
ambiguated Modern Standard Arabic (MSA).
When used in POS tagging, this process improves
the accuracy from 73.24% to 86.84% on unseen
CEA text, and reduces the percentage of out-of-
vocabulary words from 28.98% to 16.66%. The
process holds promise for any NLP task targeting
the dialectal varieties of Arabic; e.g., this approach
may provide a cheap way to leverage MSA data
and morphological resources to create resources
for colloquial Arabic to English machine transla-
tion. It can also considerably speed up the annota-
tion of Arabic dialects.
</bodyText>
<sectionHeader confidence="0.997724" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999872657142857">
Most of the research on Arabic is focused on Mod-
ern Standard Arabic. Dialectal varieties have not
received much attention due to the lack of dialectal
tools and annotated texts (Duh and Kirchoff,
2005). In this paper, we present a rule-based me-
thod to generate Colloquial Egyptian Arabic (CEA)
from Modern Standard Arabic (MSA), relying on
segment-based part-of-speech tags. The transfor-
mation process relies on the observation that di-
alectal varieties of Arabic differ mainly in the use
of affixes and function words while the word stem
mostly remains unchanged. For example, given the
Buckwalter-encoded MSA sentence “AlAxwAn
Almslmwn lm yfwzwA fy AlAntxbAt” the rules pro-
duce “AlAxwAn Almslmyn mfAzw$ f AlAntxAbAt”
(تاباختولاا ف شوزافم هيملسملا ناىخلاا, The Muslim Bro-
therhood did not win the elections). The availabili-
ty of segment-based part-of-speech tags is essential
since many of the affixes in MSA are ambiguous.
For example, lm could be either a negative particle
or a question work, and the word AlAxwAn could
be either made of two segments (Al+&lt;xwAn, the
brothers), or three segments (Al+&gt;xw+An, the two
brothers).
We first introduce the transformation rules, and
show that in many cases it is feasible to transform
MSA to CEA, although there are cases that require
much more than POS tags. We then provide a typ-
ical case in which we utilize the transformed text
of the Arabic Treebank (Bies and Maamouri, 2003)
to build a part-of-speech tagger for CEA. The tag-
ger improves the accuracy of POS tagging on au-
thentic Egyptian Arabic by 13% absolute (from
73.24% to 86.84%) and reduces the percentage of
out-of-vocabulary words from 28.98% to 16.66%.
</bodyText>
<sectionHeader confidence="0.82875" genericHeader="method">
2. MSA to CEA Conversion Rules
</sectionHeader>
<bodyText confidence="0.998691666666667">
Table 1 shows a sentence in MSA and its CEA
counterpart. Both can be translated into: “We did
not write it for them.” MSA has three words while
CEA is more synthetic as the preposition and the
negative particle turn into clitics. Table 1 illu-
strates the end product of one of the Imperfect
transformation rules, namely the case where the
Imperfect Verb is preceded by the negative particle
lm.
</bodyText>
<table confidence="0.99609">
Arabic Buckwalter
MSA ههل اهبتكو مل lm nktbhA lhn
CEA صمهلهىبتكم mktbnhlhm$
English We did not write it for them
</table>
<tableCaption confidence="0.99994">
Table 1: a sentence in MSA and CEA
</tableCaption>
<bodyText confidence="0.998701666666667">
Our 103 rules cover nominals (number and case
affixes), verbs (tense, number, gender, and modali-
ty), pronouns (number and gender), and demon-
strative pronouns (number and gender).
The rules also cover certain lexical items as 400
words in MSA have been converted to their com-
</bodyText>
<page confidence="0.982689">
176
</page>
<note confidence="0.684053">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 176–180,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998858172413793">
mon CEA counterparts. Examples of lexical con-
versions include ZlAm and Dlmp (darkness), rjl
and rAjl (man), rjAl and rjAlp (men), and kvyr and
ktyr (many), where the first word is the MSA ver-
sion and the second is the CEA version.
Many of the lexical mappings are ambiguous.
For example, the word rjl can either mean man or
leg. When it means man, the CEA form is rAjl, but
the word for leg is the same in both MSA and
CEA. While they have different vowel patterns
(rajul and rijol respectively), the vowel informa-
tion is harder to get correctly than POS tags. The
problem may arise especially when dealing with
raw data for which we need to provide POS tags
(and vowels) so we may be able to convert it to the
colloquial form. Below, we provide two sample
rules:
The imperfect verb is used, inter alia, to express
the negated past, for which CEA uses the perfect
verb. What makes things more complicated is that
CEA treats negative particles and prepositional
phrases as clitics. An example of this is the word
mktbthlhm$ (I did not write it for them) in Table 1
above. It is made of the negative particle m, the
stem ktb (to write), the object pronoun h, the pre-
position l, the pronoun hm (them) and the negative
particle $. Figure 1, and the following steps show
the conversions of lm nktbhA lhm to
mktbnhAlhm$:
</bodyText>
<listItem confidence="0.927772928571429">
1. Replace the negative word lm with one of
the prefixes m, mA or the word mA.
2. Replace the Imperfect Verb prefix with its
Perfect Verb suffix counterpart. For exam-
ple, the IV first person singular subject pre-
fix &gt; turns into t in the PV.
3. If the verb is followed by a prepositional
phrase headed by the preposition l that con-
tains a pronominal object, convert the pre-
position to a prepositional clitic.
4. Transform the dual to plural and the plural
feminine to plural masculine.
5. Add the negative suffix $ (or the variant $y,
which is less probable)
</listItem>
<bodyText confidence="0.998179125">
As alluded to in 1) above, given that colloquial
orthography is not standardized, many affixes and
clitics can be written in different ways. For exam-
ple, the word mktbnhlhm$, can be written in 24
ways. All these forms are legal and possible, as
attested by their existence in a CEA corpus (the
Arabic Online Commentary Dataset v1.1), which
we also use for building a language model later.
</bodyText>
<figureCaption confidence="0.9977075">
Figure 1: One negated IV form in MSA can generate 24
(3x2x2x2) possible forms in CEA
</figureCaption>
<bodyText confidence="0.999936538461539">
MSA possessive pronouns inflect for gender, num-
ber (singular, dual, and plural), and person. In
CEA, there is no distinction between the dual and
the plural, and a single pronoun is used for the
plural feminine and masculine. The three MSA
forms ktAbhm, ktAbhmA and ktAbhn (their book
for the masculine plural, the dual, and the feminine
plural respectively) all collapse to ktAbhm.
Table 2 has examples of some other rules we have
applied. We note that the stem, in bold, hardly
changes, and that the changes mainly affect func-
tion segments. The last example is a lexical rule in
which the stem has to change.
</bodyText>
<table confidence="0.996439571428571">
Rule MSA CEA
Future swf yktb Hyktb/hyktb
Future_NEG ln &gt;ktb m$ hktb/ m$ Hktb
IV yktbwn byktbw/ bktbw/ bktbwA
Passive ktb Anktb/ Atktb
NEG_PREP lys mnhn mmnhm$
Lexical trkhmA sAbhm
</table>
<tableCaption confidence="0.998198">
Table 2: Examples of Conversion Rules.
</tableCaption>
<sectionHeader confidence="0.966407" genericHeader="method">
3. POS Tagging Egyptian Arabic
</sectionHeader>
<bodyText confidence="0.999808538461538">
We use the conversion above to build a POS tagger
for Egyptian Arabic. We follow Mohamed and
Kuebler (2010) in using whole word tagging, i.e.,
without any word segmentation. We use the Co-
lumbia Arabic Treebank 6-tag tag set: PRT (Par-
ticle), NOM (Nouns, Adjectives, and Adverbs),
PROP (Proper Nouns), VRB (Verb), VRB-pass
(Passive Verb), and PNX (Punctuation) (Habash
and Roth, 2009). For example, the word
wHnktblhm (and we will write to them, MIPS -9)
receives the tag PRT+PRT+VRB+PRT+NOM.
This results in 58 composite tags, 9 of which occur
5 times or less in the converted ECA training set.
</bodyText>
<page confidence="0.989637">
177
</page>
<bodyText confidence="0.9999926875">
We converted two sections of the Arabic Tree-
bank (ATB): p2v3 and p3v2. For all the POS tag-
ging experiments, we use the memory-based POS
tagger (MBT) (Daelemans et al., 1996) The best
results, tuned on a dev set, were obtained, in non-
exhaustive search, with the Modified Value Dif-
ference Metric as a distance metric and with k (the
number of nearest neighbors) = 25. For known
words, we use the IGTree algorithm and 2 words to
the left, their POS tags, the focus word and its list
of possible tags, 1 right context word and its list of
possible tags as features. For unknown words, we
use the IB1 algorithm and the word itself, its first 5
and last 3 characters, 1 left context word and its
POS tag, and 1 right context word and its list of
possible tags as features.
</bodyText>
<subsectionHeader confidence="0.969775">
3.1. Development and Test Data
</subsectionHeader>
<bodyText confidence="0.99999425">
As a development set, we use 100 user-contributed
comments (2757 words) from the website ma-
srawy.com, which were judged to be highly collo-
quial. The test set contains 192 comments (7092
words) from the same website with the same crite-
rion. The development and test sets were hand-
annotated with composite tags as illustrated above
by two native Arabic-speaking students.
The test and development sets contained spel-
ling errors (mostly run-on words). The most com-
mon of these is the vocative particle yA, which is
usually attached to following word (e.g. yArAjl,
(you man, LW_)1 )). It is not clear whether it should
be treated as a proclitic, since it also occurs as a
separate word, which is the standard way of writ-
ing. The same holds true for the variation between
the letters * and z, (� and -3 in Arabic) which are
pronounced exactly the same way in CEA to the
extent that the substitution may not be considered a
spelling error.
</bodyText>
<subsectionHeader confidence="0.967086">
3.2. Experiments and Results
</subsectionHeader>
<bodyText confidence="0.989205444444444">
We ran five experiments to test the effect of MSA
to CEA conversion on POS tagging: (a) Standard,
where we train the tagger on the ATB MSA data,
(b) 3-gram LM, where for each MSA sentence we
generate all transformed sentences (see Section 2.1
and Figure 1) and pick the most probable sentence
according to a trigram language model built from
an 11.5 million words of user contributed
comments.1 This corpus is highly dialectal
</bodyText>
<footnote confidence="0.981263">
1Available from http://www.cs.jhu.edu/~ozaidan/AOC
</footnote>
<bodyText confidence="0.99935480952381">
Egyptian Arabic, but like all similar collections, it
is diglossic and demonstrates a high degree of
code-switching between MSA and CEA. We use
the SRILM toolkit (Stolcke, 2002) for language
modeling and sentence scoring, (c) Random,
where we choose a random sentence from all the
correct sentences generated for each MSA
sentence, (d) Hybrid, where we combine the data
in a) with the best settings (as measured on the dev
set) using the converted colloquial data (namely
experiment c). Hybridization is necessary since
most Arabic data in blogs and comments are a mix
of MSA and CEA, and (e) Hybrid + dev, where
we enrich the Hybrid training set with the dev data.
We use the following metrics for evaluation:
KWA: Known Word Accuracy (%), UWA:
Unknown Word Accuracy (%), TA: Total Accuracy
(%), and UW: unknown words (%) in the
respective set in the respective experiment. Table
3(a) presents the results on the development set
while Table 3(b) the results on the test set.
</bodyText>
<figure confidence="0.991945">
Experiment KWA UWA TA UW
(a) Standard 92.75 39.68 75.77 31.99
(b) 3-gram LM 89.12 43.46 76.21 28.29
(c) Random 92.36 43.51 79.25 26.84
(d) Hybrid 94.13 52.22 84.87 22.09
</figure>
<tableCaption confidence="0.976546">
Table 3(a): POS results on the development set.
</tableCaption>
<bodyText confidence="0.999918157894737">
We notice that randomly selecting a sentence from
the correct generated sentences yields better results
than choosing the most probable sentence accord-
ing to a language model. The reason for this may
be that randomization guarantees more coverage of
the various forms. We have found that the vocabu-
lary size (the number of unique word types) for the
training set generated for the Random experiment
is considerably larger than the vocabulary size for
the 3-gram LM experiment (55367 unique word
types in Random versus 51306 in 3-gram LM),
which results in a drop of 4.6% absolute in the per-
centage of unknown words: 27.31% versus
22.30%). This drop in the percentage of unknown
words may indicate that generating all possible
variations of CEA may be more useful than using a
language model in general. Even in a CEA corpus
of 35 million words, one third of the words gener-
ated by the rules are not in the corpus, while many
</bodyText>
<page confidence="0.996175">
178
</page>
<bodyText confidence="0.971945">
of these are in both the test set and the develop-
ment set.
</bodyText>
<figure confidence="0.79871">
Experiment KWA UWA TA UW
(a) Standard 89.03 40.67 73.24 28.98
(b) 3-gram LM 84.33 47.70 74.32 27.31
(c) Random 90.24 48.90 79.67 22.70
(d) Hybrid 92.22 53.92 83.81 19.45
(e) Hybrid+dev 94.87 56.46 86.84 16.66
</figure>
<tableCaption confidence="0.985123">
Table 3(b): POS results on the test set
</tableCaption>
<bodyText confidence="0.999899923076923">
We also notice that the conversion alone im-
proves tagging accuracy from 75.77% to 79.25%
on the development set, and from 73.24% to
79.67% on the test set. Combining the original
MSA and the best scoring converted data (Ran-
dom) raises the accuracies to 84.87% and 83.81%
respectively. The percentage of unknown words
drops from 29.98% to 19.45% in the test set when
we used the hybrid data. The fact that the percen-
tage of unknown words drops further to 16.66% in
the Hybrid+dev experiment points out the authen-
tic colloquial data contains elements that have not
been captured using conversion alone.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="related work">
4. Related Work
</sectionHeader>
<bodyText confidence="0.999991730769231">
To the best of our knowledge, ours is the first work
that generates CEA automatically from morpholog-
ically disambiguated MSA, but Habash et al.
(2005) discussed root and pattern morphological
analysis and generation of Arabic dialects within
the MAGED morphological analyzer. MAGED
incorporates the morphology, phonology, and or-
thography of several Arabic dialects. Diab et al.
(2010) worked on the annotation of dialectal Arab-
ic through the COLABA project, and they used the
(manually) annotated resources to facilitate the
incorporation of the dialects in Arabic information
retrieval.
Duh and Kirchhoff (2005) successfully designed
a POS tagger for CEA that used an MSA morpho-
logical analyzer and information gleaned from the
intersection of several Arabic dialects. This is dif-
ferent from our approach for which POS tagging is
only an application. Our focus is to use any exist-
ing MSA data to generate colloquial Arabic re-
sources that can be used in virtually any NLP task.
At a higher level, our work resembles that of
Kundu and Roth (2011), in which they chose to
adapt the text rather than the model. While they
adapted the test set, we do so at the training set
level.
</bodyText>
<sectionHeader confidence="0.990062" genericHeader="conclusions">
5. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999996103448276">
We have a presented a method to convert Modern
Standard Arabic to Egyptian Colloquial Arabic
with an example application to the POS tagging
task. This approach may provide a cheap way to
leverage MSA data and morphological resources to
create resources for colloquial Arabic to English
machine translation, for example.
While the rules of conversion were mainly
morphological in nature, they have proved useful
in handling colloquial data. However, morphology
alone is not enough for handling key points of dif-
ference between CEA and MSA. While CEA is
mainly an SVO language, MSA is mainly VSO,
and while demonstratives are pre-nominal in MSA,
they are post-nominal in CEA. These phenomena
can be handled only through syntactic conversion.
We expect that converting a dependency-based
treebank to CEA can account for many of the phe-
nomena part-of-speech tags alone cannot handle
We are planning to extend the rules to other lin-
guistic phenomena and dialects, with possible ap-
plications to various NLP tasks for which MSA
annotated data exist. When no gold standard seg-
ment-based POS tags are available, tools that pro-
duce segment-based annotation can be used, e.g.
segment-based POS tagging (Mohamed and Kueb-
ler, 2010) or MADA (Habash et al, 2009), although
these are not expected to yield the same results as
gold standard part-of-speech tags.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999115">
This publication was made possible by a NPRP
grant (NPRP 09-1140-1-177) from the Qatar Na-
tional Research Fund (a member of The Qatar
Foundation). The statements made herein are sole-
ly the responsibility of the authors.
We thank the two native speaker annotators and
the anonymous reviewers for their instructive and
enriching feedback.
</bodyText>
<page confidence="0.998327">
179
</page>
<sectionHeader confidence="0.990342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942326530613">
Bies, Ann and Maamouri, Mohamed (2003). Penn
Arabic Treebank guidelines. Technical report, LDC,
University of Pennsylvania.
Buckwalter, T. (2002). Arabic Morphological Analyz-
er (AraMorph). Version 1.0. Linguistic Data Consor-
tium, catalog number LDC2002L49 and ISBN 1-58563-
257- 0
Daelemans, Walter and van den Bosch, Antal ( 2005).
Memory Based Language Processing. Cambridge Uni-
versity Press.
Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and
Steven Gillis (1996). MBT: A memory-based part of
speech tagger-generator. In Eva Ejerhed and Ido Dagan,
editors, Proceedings of the 4th Workshop on Very Large
Corpora, pages 14–27, Copenhagen, Denmark.
Diab, Mona; Habash, Nizar; Rambow, Owen; Altan-
tawy, Mohamed, and Benajiba, Yassine. COLABA:
Arabic Dialect Annotation and Processing. LREC 2010.
Duh, K. and Kirchhoff, K. (2005). POS Tagging of
Dialectal Arabic: A Minimally Supervised Approach.
Proceedings of the ACL Workshop on Computational
Approaches to Semitic Languages, Ann Arbor, June
2005.
Habash, Nizar; Rambow, Own and Kiraz, George
(2005). Morphological analysis and generation for
Arabic dialects. Proceedings of the ACL Workshop on
Computational Approaches to Semitic Languages, pages
17–24, Ann Arbor, June 2005
Habash, Nizar and Roth, Ryan. CATiB: The Colum-
bia Arabic Treebank. Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 221–224, Singa-
pore, 4 August 2009. c 2009 ACL and AFNLP
Habash, Nizar, Owen Rambow and Ryan Roth. MA-
DA+TOKAN: A Toolkit for Arabic Tokenization, Dia-
critization, Morphological Disambiguation, POS Tag-
ging, Stemming and Lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR), Cairo, Egypt, 2009
Kundu, Gourab abd Roth, Don (2011). Adapting Text
instead of the Model: An Open Domain Approach. Pro-
ceedings of the Fifteenth Conference on Computational
Natural Language Learning, pages 229–237,Portland,
Oregon, USA, 23–24 June 2011
Mohamed, Emad. and Kuebler, Sandra (2010). Is
Arabic Part of Speech Tagging Feasible Without Word
Segmentation? Proceedings of HLT-NAACL 2010, Los
Angeles, CA.
Stolcke, A. (2002). SRILM - an extensible language
modeling toolkit. In Proc. of ICSLP, Denver, Colorado
</reference>
<page confidence="0.997762">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000101">
<title confidence="0.999495">Transforming Standard Arabic to Colloquial Arabic</title>
<author confidence="0.991884">Emad Mohamed</author>
<author confidence="0.991884">Behrang Mohit</author>
<author confidence="0.991884">Kemal Oflazer</author>
<affiliation confidence="0.999875">Carnegie Mellon University -</affiliation>
<address confidence="0.958233">Doha, Qatar</address>
<email confidence="0.99988">emohamed@qatar.cmu.edu,behrang@cmu.edu,ko@cs.cmu.edu</email>
<abstract confidence="0.982458564179105">We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects. Most of the research on Arabic is focused on Modern Standard Arabic. Dialectal varieties have not received much attention due to the lack of dialectal tools and annotated texts (Duh and Kirchoff, 2005). In this paper, we present a rule-based method to generate Colloquial Egyptian Arabic (CEA) from Modern Standard Arabic (MSA), relying on segment-based part-of-speech tags. The transformation process relies on the observation that dialectal varieties of Arabic differ mainly in the use of affixes and function words while the word stem remains unchanged. For the sentence lm yfwzwA fy rules promfAzw$ f ف شوزافم هيملسملا The Muslim Brotherhood did not win the elections). The availability of segment-based part-of-speech tags is essential since many of the affixes in MSA are ambiguous. example, be either a negative particle a question work, and the word either made of two segments the or three segments the two brothers). We first introduce the transformation rules, and show that in many cases it is feasible to transform MSA to CEA, although there are cases that require much more than POS tags. We then provide a typical case in which we utilize the transformed text of the Arabic Treebank (Bies and Maamouri, 2003) to build a part-of-speech tagger for CEA. The tagger improves the accuracy of POS tagging on authentic Egyptian Arabic by 13% absolute (from 73.24% to 86.84%) and reduces the percentage of out-of-vocabulary words from 28.98% to 16.66%. to CEA Conversion Rules Table 1 shows a sentence in MSA and its CEA Both can be translated into: did write it for MSA has three words while CEA is more synthetic as the preposition and the negative particle turn into clitics. Table 1 illustrates the end product of one of the Imperfect transformation rules, namely the case where the Imperfect Verb is preceded by the negative particle Arabic Buckwalter MSA ههل اهبتكو مل lm nktbhA lhn CEA صمهلهىبتكم mktbnhlhm$ English We did not write it for them Table 1: a sentence in MSA and CEA Our 103 rules cover nominals (number and case affixes), verbs (tense, number, gender, and modality), pronouns (number and gender), and demonstrative pronouns (number and gender). The rules also cover certain lexical items as 400 in MSA have been converted to their com- 176 of the 50th Annual Meeting of the Association for Computational pages 176–180, Republic of Korea, 8-14 July 2012. Association for Computational Linguistics mon CEA counterparts. Examples of lexical coninclude and where the first word is the MSA version and the second is the CEA version. Many of the lexical mappings are ambiguous. example, the word either mean When it means the CEA form is but word for the same in both MSA and CEA. While they have different vowel patterns the vowel information is harder to get correctly than POS tags. The problem may arise especially when dealing with raw data for which we need to provide POS tags (and vowels) so we may be able to convert it to the colloquial form. Below, we provide two sample rules: The imperfect verb is used, inter alia, to express the negated past, for which CEA uses the perfect verb. What makes things more complicated is that CEA treats negative particles and prepositional phrases as clitics. An example of this is the word did not write it for them) in Table 1 It is made of the negative particle the write), the object pronoun prethe pronoun and the negative 1, and the following steps show conversions of nktbhA lhm mktbnhAlhm$: Replace the negative word one of prefixes the word 2. Replace the Imperfect Verb prefix with its Perfect Verb suffix counterpart. For example, the IV first person singular subject preinto the PV. 3. If the verb is followed by a prepositional headed by the preposition contains a pronominal object, convert the preposition to a prepositional clitic. 4. Transform the dual to plural and the plural feminine to plural masculine. Add the negative suffix the variant which is less probable) As alluded to in 1) above, given that colloquial orthography is not standardized, many affixes and clitics can be written in different ways. For examthe word be written in 24 ways. All these forms are legal and possible, as attested by their existence in a CEA corpus (the Arabic Online Commentary Dataset v1.1), which we also use for building a language model later. Figure 1: One negated IV form in MSA can generate 24 (3x2x2x2) possible forms in CEA MSA possessive pronouns inflect for gender, number (singular, dual, and plural), and person. In CEA, there is no distinction between the dual and the plural, and a single pronoun is used for the plural feminine and masculine. The three MSA for the masculine plural, the dual, and the feminine respectively) all collapse to Table 2 has examples of some other rules we have applied. We note that the stem, in bold, hardly changes, and that the changes mainly affect function segments. The last example is a lexical rule in which the stem has to change. Rule MSA CEA Future Future_NEG IV Passive ktb NEG_PREP Lexical Table 2: Examples of Conversion Rules. Tagging Egyptian Arabic We use the conversion above to build a POS tagger for Egyptian Arabic. We follow Mohamed and Kuebler (2010) in using whole word tagging, i.e., without any word segmentation. We use the Columbia Arabic Treebank 6-tag tag set: PRT (Particle), NOM (Nouns, Adjectives, and Adverbs), PROP (Proper Nouns), VRB (Verb), VRB-pass (Passive Verb), and PNX (Punctuation) (Habash and Roth, 2009). For example, the word we will write to them, the tag This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set. 177 We converted two sections of the Arabic Treebank (ATB): p2v3 and p3v2. For all the POS tagging experiments, we use the memory-based POS (MBT) (Daelemans al., The best results, tuned on a dev set, were obtained, in nonexhaustive search, with the Modified Value Dif- Metric as a distance metric and with number of nearest neighbors) = 25. For known words, we use the IGTree algorithm and 2 words to the left, their POS tags, the focus word and its list of possible tags, 1 right context word and its list of possible tags as features. For unknown words, we use the IB1 algorithm and the word itself, its first 5 and last 3 characters, 1 left context word and its POS tag, and 1 right context word and its list of possible tags as features. and Test Data As a development set, we use 100 user-contributed (2757 words) from the website mawhich were judged to be highly colloquial. The test set contains 192 comments (7092 words) from the same website with the same criterion. The development and test sets were handannotated with composite tags as illustrated above by two native Arabic-speaking students. The test and development sets contained spelling errors (mostly run-on words). The most comof these is the vocative particle which is attached to following word (e.g. man, It is not clear whether it should be treated as a proclitic, since it also occurs as a separate word, which is the standard way of writing. The same holds true for the variation between letters Arabic) which are pronounced exactly the same way in CEA to the extent that the substitution may not be considered a spelling error. and Results We ran five experiments to test the effect of MSA CEA conversion on POS tagging: (a) where we train the tagger on the ATB MSA data, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed This corpus is highly dialectal fromhttp://www.cs.jhu.edu/~ozaidan/AOC Egyptian Arabic, but like all similar collections, it is diglossic and demonstrates a high degree of code-switching between MSA and CEA. We use the SRILM toolkit (Stolcke, 2002) for language and sentence scoring, (c) where we choose a random sentence from all the correct sentences generated for each MSA (d) where we combine the data in a) with the best settings (as measured on the dev set) using the converted colloquial data (namely experiment c). Hybridization is necessary since most Arabic data in blogs and comments are a mix MSA and CEA, and (e) + dev, we enrich the Hybrid training set with the dev data. We use the following metrics for evaluation: Known Word Accuracy (%), Word Accuracy (%), Total Accuracy and unknown words (%) in the respective set in the respective experiment. Table 3(a) presents the results on the development set while Table 3(b) the results on the test set. Experiment KWA UWA TA UW (a) Standard 92.75 39.68 75.77 31.99 (b) 3-gram LM 89.12 43.46 76.21 28.29 (c) Random 92.36 43.51 79.25 26.84 (d) Hybrid 94.13 52.22 84.87 22.09 Table 3(a): POS results on the development set. We notice that randomly selecting a sentence from the correct generated sentences yields better results than choosing the most probable sentence according to a language model. The reason for this may be that randomization guarantees more coverage of the various forms. We have found that the vocabulary size (the number of unique word types) for the set generated for the is considerably larger than the vocabulary size for the 3-gram LM experiment (55367 unique word in 51306 in which results in a drop of 4.6% absolute in the percentage of unknown words: 27.31% versus 22.30%). This drop in the percentage of unknown words may indicate that generating all possible variations of CEA may be more useful than using a language model in general. Even in a CEA corpus of 35 million words, one third of the words generated by the rules are not in the corpus, while many 178 of these are in both the test set and the development set. Experiment KWA UWA TA UW (a) Standard 89.03 40.67 73.24 28.98 (b) 3-gram LM 84.33 47.70 74.32 27.31 (c) Random 90.24 48.90 79.67 22.70 (d) Hybrid 92.22 53.92 83.81 19.45 (e) Hybrid+dev 94.87 56.46 86.84 16.66 Table 3(b): POS results on the test set We also notice that the conversion alone improves tagging accuracy from 75.77% to 79.25% on the development set, and from 73.24% to 79.67% on the test set. Combining the original MSA and the best scoring converted data (Random) raises the accuracies to 84.87% and 83.81% respectively. The percentage of unknown words drops from 29.98% to 19.45% in the test set when we used the hybrid data. The fact that the percentage of unknown words drops further to 16.66% in points out the authentic colloquial data contains elements that have not been captured using conversion alone. Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologdisambiguated MSA, but Habash et (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and orof several Arabic dialects. Diab (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of several Arabic dialects. This is different from our approach for which POS tagging is only an application. Our focus is to use any existing MSA data to generate colloquial Arabic resources that can be used in virtually any NLP task. At a higher level, our work resembles that of Kundu and Roth (2011), in which they chose to adapt the text rather than the model. While they adapted the test set, we do so at the training set level. and Future Work We have a presented a method to convert Modern Standard Arabic to Egyptian Colloquial Arabic with an example application to the POS tagging task. This approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation, for example. While the rules of conversion were mainly morphological in nature, they have proved useful in handling colloquial data. However, morphology alone is not enough for handling key points of difference between CEA and MSA. While CEA is mainly an SVO language, MSA is mainly VSO, and while demonstratives are pre-nominal in MSA, they are post-nominal in CEA. These phenomena can be handled only through syntactic conversion. We expect that converting a dependency-based treebank to CEA can account for many of the phenomena part-of-speech tags alone cannot handle We are planning to extend the rules to other linguistic phenomena and dialects, with possible applications to various NLP tasks for which MSA annotated data exist. When no gold standard segment-based POS tags are available, tools that produce segment-based annotation can be used, e.g. segment-based POS tagging (Mohamed and Kuebler, 2010) or MADA (Habash et al, 2009), although these are not expected to yield the same results as gold standard part-of-speech tags.</abstract>
<note confidence="0.73351225">Acknowledgements This publication was made possible by a NPRP grant (NPRP 09-1140-1-177) from the Qatar National Research Fund (a member of The Qatar</note>
<abstract confidence="0.9487486">Foundation). The statements made herein are solely the responsibility of the authors. We thank the two native speaker annotators and the anonymous reviewers for their instructive and enriching feedback.</abstract>
<note confidence="0.375759">179</note>
<title confidence="0.820624">References</title>
<author confidence="0.879485">Ann Bies</author>
<author confidence="0.879485">Mohamed Maamouri</author>
<affiliation confidence="0.9174165">Arabic Treebank guidelines. Technical report, LDC, University of Pennsylvania.</affiliation>
<address confidence="0.876105">Buckwalter, T. (2002). Arabic Morphological Analyz-</address>
<note confidence="0.938187333333333">er (AraMorph). Version 1.0. Linguistic Data Consortium, catalog number LDC2002L49 and ISBN 1-58563- 257- 0 Daelemans, Walter and van den Bosch, Antal ( 2005). Memory Based Language Processing. Cambridge University Press. Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and Steven Gillis (1996). MBT: A memory-based part of speech tagger-generator. In Eva Ejerhed and Ido Dagan, editors, Proceedings of the 4th Workshop on Very Large pages Copenhagen, Denmark. Diab, Mona; Habash, Nizar; Rambow, Owen; Altantawy, Mohamed, and Benajiba, Yassine. COLABA: Arabic Dialect Annotation and Processing. LREC 2010. Duh, K. and Kirchhoff, K. (2005). POS Tagging of Dialectal Arabic: A Minimally Supervised Approach. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Ann Arbor, June 2005. Habash, Nizar; Rambow, Own and Kiraz, George (2005). Morphological analysis and generation for Arabic dialects. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages Ann Arbor, June 2005</note>
<title confidence="0.4620285">Habash, Nizar and Roth, Ryan. CATiB: The Columbia Arabic Treebank. Proceedings of the ACL-IJCNLP</title>
<author confidence="0.298605">Conference Short Papers</author>
<author confidence="0.298605">pages Singa-</author>
<address confidence="0.651078">pore, 4 August 2009. c 2009 ACL and AFNLP</address>
<author confidence="0.853743">MA-</author>
<affiliation confidence="0.648392">DA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tag-</affiliation>
<address confidence="0.455948">ging, Stemming and Lemmatization. In Proceedings of</address>
<note confidence="0.951700071428571">the 2nd International Conference on Arabic Language and Tools Egypt, 2009 Kundu, Gourab abd Roth, Don (2011). Adapting Text instead of the Model: An Open Domain Approach. Proceedings of the Fifteenth Conference on Computational Language Learning, pages USA, June 2011 Mohamed, Emad. and Kuebler, Sandra (2010). Is Arabic Part of Speech Tagging Feasible Without Word Segmentation? Proceedings of HLT-NAACL 2010, Los Angeles, CA. Stolcke, A. (2002). SRILM an extensible language modeling toolkit. In Proc. of ICSLP, Denver, Colorado 180</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mohamed Maamouri</author>
</authors>
<title>Penn Arabic Treebank guidelines.</title>
<date>2003</date>
<tech>Technical report, LDC,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2283" citStr="Bies and Maamouri, 2003" startWordPosition="353" endWordPosition="356">tions). The availability of segment-based part-of-speech tags is essential since many of the affixes in MSA are ambiguous. For example, lm could be either a negative particle or a question work, and the word AlAxwAn could be either made of two segments (Al+&lt;xwAn, the brothers), or three segments (Al+&gt;xw+An, the two brothers). We first introduce the transformation rules, and show that in many cases it is feasible to transform MSA to CEA, although there are cases that require much more than POS tags. We then provide a typical case in which we utilize the transformed text of the Arabic Treebank (Bies and Maamouri, 2003) to build a part-of-speech tagger for CEA. The tagger improves the accuracy of POS tagging on authentic Egyptian Arabic by 13% absolute (from 73.24% to 86.84%) and reduces the percentage of out-of-vocabulary words from 28.98% to 16.66%. 2. MSA to CEA Conversion Rules Table 1 shows a sentence in MSA and its CEA counterpart. Both can be translated into: “We did not write it for them.” MSA has three words while CEA is more synthetic as the preposition and the negative particle turn into clitics. Table 1 illustrates the end product of one of the Imperfect transformation rules, namely the case wher</context>
</contexts>
<marker>Bies, Maamouri, 2003</marker>
<rawString>Bies, Ann and Maamouri, Mohamed (2003). Penn Arabic Treebank guidelines. Technical report, LDC, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Arabic Morphological Analyzer (AraMorph). Version 1.0.</title>
<date>2002</date>
<booktitle>Linguistic Data Consortium, catalog number LDC2002L49 and ISBN</booktitle>
<pages>1--58563</pages>
<marker>Buckwalter, 2002</marker>
<rawString>Buckwalter, T. (2002). Arabic Morphological Analyzer (AraMorph). Version 1.0. Linguistic Data Consortium, catalog number LDC2002L49 and ISBN 1-58563-257- 0</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>van den Bosch</author>
</authors>
<title>Memory Based Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<location>Antal (</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Daelemans, Walter and van den Bosch, Antal ( 2005). Memory Based Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Walter Daelemans</author>
<author>Zavrel</author>
</authors>
<location>Jakub; Berck, Peter, and</location>
<marker>Daelemans, Zavrel, </marker>
<rawString>Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Eva Ejerhed and Ido Dagan, editors, Proceedings of the 4th Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<location>Copenhagen, Denmark.</location>
<marker>Gillis, 1996</marker>
<rawString>Steven Gillis (1996). MBT: A memory-based part of speech tagger-generator. In Eva Ejerhed and Ido Dagan, editors, Proceedings of the 4th Workshop on Very Large Corpora, pages 14–27, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Mohamed Altantawy</author>
<author>Yassine Benajiba</author>
</authors>
<date>2010</date>
<booktitle>COLABA: Arabic Dialect Annotation and Processing. LREC</booktitle>
<contexts>
<context position="13042" citStr="Diab et al. (2010)" startWordPosition="2219" endWordPosition="2222">d data. The fact that the percentage of unknown words drops further to 16.66% in the Hybrid+dev experiment points out the authentic colloquial data contains elements that have not been captured using conversion alone. 4. Related Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologically disambiguated MSA, but Habash et al. (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and orthography of several Arabic dialects. Diab et al. (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of several Arabic dialects. This is different from our approach for which POS tagging is only an application. Our focus is to use any existing MSA data to generate colloquial Arabic resources that can be used in virtually any NLP task. At a h</context>
</contexts>
<marker>Diab, Habash, Rambow, Altantawy, Benajiba, 2010</marker>
<rawString>Diab, Mona; Habash, Nizar; Rambow, Owen; Altantawy, Mohamed, and Benajiba, Yassine. COLABA: Arabic Dialect Annotation and Processing. LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>POS Tagging of Dialectal Arabic: A Minimally Supervised Approach.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<location>Ann Arbor,</location>
<contexts>
<context position="13270" citStr="Duh and Kirchhoff (2005)" startWordPosition="2253" endWordPosition="2256">elated Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologically disambiguated MSA, but Habash et al. (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and orthography of several Arabic dialects. Diab et al. (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of several Arabic dialects. This is different from our approach for which POS tagging is only an application. Our focus is to use any existing MSA data to generate colloquial Arabic resources that can be used in virtually any NLP task. At a higher level, our work resembles that of Kundu and Roth (2011), in which they chose to adapt the text rather than the model. While they adapted the test set, we do so at the training set level. 5. Conclusions and Future Work We h</context>
</contexts>
<marker>Duh, Kirchhoff, 2005</marker>
<rawString>Duh, K. and Kirchhoff, K. (2005). POS Tagging of Dialectal Arabic: A Minimally Supervised Approach. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Ann Arbor, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Own Rambow</author>
<author>George Kiraz</author>
</authors>
<title>Morphological analysis and generation for Arabic dialects.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>17--24</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="12808" citStr="Habash et al. (2005)" startWordPosition="2187" endWordPosition="2190">est set. Combining the original MSA and the best scoring converted data (Random) raises the accuracies to 84.87% and 83.81% respectively. The percentage of unknown words drops from 29.98% to 19.45% in the test set when we used the hybrid data. The fact that the percentage of unknown words drops further to 16.66% in the Hybrid+dev experiment points out the authentic colloquial data contains elements that have not been captured using conversion alone. 4. Related Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologically disambiguated MSA, but Habash et al. (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and orthography of several Arabic dialects. Diab et al. (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of sever</context>
</contexts>
<marker>Habash, Rambow, Kiraz, 2005</marker>
<rawString>Habash, Nizar; Rambow, Own and Kiraz, George (2005). Morphological analysis and generation for Arabic dialects. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 17–24, Ann Arbor, June 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
</authors>
<title>CATiB: The Columbia Arabic Treebank.</title>
<date>2009</date>
<journal>ACL and AFNLP</journal>
<booktitle>Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>221--224</pages>
<contexts>
<context position="7155" citStr="Habash and Roth, 2009" startWordPosition="1204" endWordPosition="1207">EA Future swf yktb Hyktb/hyktb Future_NEG ln &gt;ktb m$ hktb/ m$ Hktb IV yktbwn byktbw/ bktbw/ bktbwA Passive ktb Anktb/ Atktb NEG_PREP lys mnhn mmnhm$ Lexical trkhmA sAbhm Table 2: Examples of Conversion Rules. 3. POS Tagging Egyptian Arabic We use the conversion above to build a POS tagger for Egyptian Arabic. We follow Mohamed and Kuebler (2010) in using whole word tagging, i.e., without any word segmentation. We use the Columbia Arabic Treebank 6-tag tag set: PRT (Particle), NOM (Nouns, Adjectives, and Adverbs), PROP (Proper Nouns), VRB (Verb), VRB-pass (Passive Verb), and PNX (Punctuation) (Habash and Roth, 2009). For example, the word wHnktblhm (and we will write to them, MIPS -9) receives the tag PRT+PRT+VRB+PRT+NOM. This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set. 177 We converted two sections of the Arabic Treebank (ATB): p2v3 and p3v2. For all the POS tagging experiments, we use the memory-based POS tagger (MBT) (Daelemans et al., 1996) The best results, tuned on a dev set, were obtained, in nonexhaustive search, with the Modified Value Difference Metric as a distance metric and with k (the number of nearest neighbors) = 25. For known words, w</context>
</contexts>
<marker>Habash, Roth, 2009</marker>
<rawString>Habash, Nizar and Roth, Ryan. CATiB: The Columbia Arabic Treebank. Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221–224, Singapore, 4 August 2009. c 2009 ACL and AFNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR),</booktitle>
<location>Cairo, Egypt,</location>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Habash, Nizar, Owen Rambow and Ryan Roth. MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR), Cairo, Egypt, 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gourab abd Roth Kundu</author>
<author>Don</author>
</authors>
<title>Adapting Text instead of the Model: An Open Domain Approach.</title>
<date>2011</date>
<booktitle>Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>229--237</pages>
<location>Oregon, USA, 23–24</location>
<marker>Kundu, Don, 2011</marker>
<rawString>Kundu, Gourab abd Roth, Don (2011). Adapting Text instead of the Model: An Open Domain Approach. Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 229–237,Portland, Oregon, USA, 23–24 June 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kuebler</author>
</authors>
<title>Is Arabic Part of Speech Tagging Feasible Without Word Segmentation?</title>
<date>2010</date>
<booktitle>Proceedings of HLT-NAACL 2010,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="6880" citStr="Kuebler (2010)" startWordPosition="1163" endWordPosition="1164">ely) all collapse to ktAbhm. Table 2 has examples of some other rules we have applied. We note that the stem, in bold, hardly changes, and that the changes mainly affect function segments. The last example is a lexical rule in which the stem has to change. Rule MSA CEA Future swf yktb Hyktb/hyktb Future_NEG ln &gt;ktb m$ hktb/ m$ Hktb IV yktbwn byktbw/ bktbw/ bktbwA Passive ktb Anktb/ Atktb NEG_PREP lys mnhn mmnhm$ Lexical trkhmA sAbhm Table 2: Examples of Conversion Rules. 3. POS Tagging Egyptian Arabic We use the conversion above to build a POS tagger for Egyptian Arabic. We follow Mohamed and Kuebler (2010) in using whole word tagging, i.e., without any word segmentation. We use the Columbia Arabic Treebank 6-tag tag set: PRT (Particle), NOM (Nouns, Adjectives, and Adverbs), PROP (Proper Nouns), VRB (Verb), VRB-pass (Passive Verb), and PNX (Punctuation) (Habash and Roth, 2009). For example, the word wHnktblhm (and we will write to them, MIPS -9) receives the tag PRT+PRT+VRB+PRT+NOM. This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set. 177 We converted two sections of the Arabic Treebank (ATB): p2v3 and p3v2. For all the POS tagging experiments, w</context>
</contexts>
<marker>Kuebler, 2010</marker>
<rawString>Mohamed, Emad. and Kuebler, Sandra (2010). Is Arabic Part of Speech Tagging Feasible Without Word Segmentation? Proceedings of HLT-NAACL 2010, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<location>Denver, Colorado</location>
<contexts>
<context position="9787" citStr="Stolcke, 2002" startWordPosition="1666" endWordPosition="1667">t of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments.1 This corpus is highly dialectal 1Available from http://www.cs.jhu.edu/~ozaidan/AOC Egyptian Arabic, but like all similar collections, it is diglossic and demonstrates a high degree of code-switching between MSA and CEA. We use the SRILM toolkit (Stolcke, 2002) for language modeling and sentence scoring, (c) Random, where we choose a random sentence from all the correct sentences generated for each MSA sentence, (d) Hybrid, where we combine the data in a) with the best settings (as measured on the dev set) using the converted colloquial data (namely experiment c). Hybridization is necessary since most Arabic data in blogs and comments are a mix of MSA and CEA, and (e) Hybrid + dev, where we enrich the Hybrid training set with the dev data. We use the following metrics for evaluation: KWA: Known Word Accuracy (%), UWA: Unknown Word Accuracy (%), TA: </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. (2002). SRILM - an extensible language modeling toolkit. In Proc. of ICSLP, Denver, Colorado</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>