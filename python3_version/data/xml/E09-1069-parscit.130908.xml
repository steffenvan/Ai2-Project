<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.999002">
Deterministic shift-reduce parsing for unification-based grammars by
using default unification
</title>
<author confidence="0.992221">
Takashi Ninomiya
</author>
<affiliation confidence="0.9976405">
Information Technology Center
University of Tokyo, Japan
</affiliation>
<email confidence="0.998673">
ninomi@r.dl.itc.u-tokyo.ac.jp
</email>
<author confidence="0.988262">
Nobuyuki Shimizu
</author>
<affiliation confidence="0.9975185">
Information Technology Center
University of Tokyo, Japan
</affiliation>
<email confidence="0.995696">
shimizu@r.dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938782608696">
Many parsing techniques including pa-
rameter estimation assume the use of a
packed parse forest for efficient and ac-
curate parsing. However, they have sev-
eral inherent problems deriving from the
restriction of locality in the packed parse
forest. Deterministic parsing is one of
solutions that can achieve simple and fast
parsing without the mechanisms of the
packed parse forest by accurately choos-
ing search paths. We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first
shift-reduce parsing with beam threshold-
ing for unification-based grammars. De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard
constraints. Therefore, it is developed by
using default unification, which almost
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982291285714286">
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively. Previous studies (Abney,
1997; Johnson et al., 1999; Kaplan et al., 2004;
Malouf and van Noord, 2004; Miyao and Tsujii,
2005; Riezler et al., 2000) defined a probabilistic
model of unification-based grammars, including
</bodyText>
<author confidence="0.698854">
Takuya Matsuzaki
</author>
<affiliation confidence="0.9989555">
Department of Computer Science
University of Tokyo, Japan
</affiliation>
<email confidence="0.997984">
matuzaki@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.92282">
Hiroshi Nakagawa
</author>
<affiliation confidence="0.9888275">
Information Technology Center
University of Tokyo, Japan
</affiliation>
<email confidence="0.965285">
nakagawa@dl.itc.u-tokyo.ac.jp
</email>
<bodyText confidence="0.999942194444445">
head-driven phrase structure grammar (HPSG),
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum
entropy model (Berger et al., 1996). Geman and
Johnson (Geman and Johnson, 2002) and Miyao
and Tsujii (Miyao and Tsujii, 2002) proposed a
feature forest, which is a dynamic programming
algorithm for estimating the probabilities of all
possible parse candidates. A feature forest can
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.
Feature forests have been used successfully
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and
Curran, 2004a; Ninomiya et al., 2007; Ninomiya
et al., 2006). Both estimation and parsing with
the packed parse forest, however, have several
inherent problems deriving from the restriction
of locality. First, feature functions can be de-
fined only for local structures, which limit the
parserâ€™s performance. This is because parsers
segment parse trees into constituents and factor
equivalent constituents into a single constituent
(edge) in a chart to avoid the same calculation.
This also means that the semantic structures must
be segmented. This is a crucial problem when
we think of designing semantic structures other
than predicate argument structures, e.g., syn-
chronous grammars for machine translation. The
size of the constituents will be exponential if the
semantic structures are not segmented. Lastly,
we need delayed evaluation for evaluating fea-
ture functions. The application of feature func-
tions must be delayed until all the values in the
</bodyText>
<note confidence="0.922974">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603â€“611,
Athens, Greece, 30 March â€“ 3 April 2009. cï¿½2009 Association for Computational Linguistics
</note>
<page confidence="0.998958">
603
</page>
<bodyText confidence="0.999943">
segmented constituents are instantiated. This is
because values in parse trees can propagate any-
where throughout the parse tree by unification.
For example, values may propagate from the root
node to terminal nodes, and the final form of the
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree. Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and
Matsumoto, 2003), search optimization learning
(DaumÃ© and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa,
2007), were studied.
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars
without the mechanisms of the packed parse for-
est. Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003),
through approaches based essentially on deter-
ministic parsing. These techniques, however,
cannot simply be applied to unification-based
grammar parsing because it can fail as a result of
its hard constraints in the grammar. Therefore,
in this study, we propose deterministic parsing
for unification-based grammars by using default
unification, which almost always succeeds in
unification by overwriting inconsistent con-
straints in the grammars. We further pursue
best-first shift-reduce parsing for unification-
based grammars.
Sections 2 and 3 explain unification-based
grammars and default unification, respectively.
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4. Section 5 dis-
cusses our experiments, and Section 6 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.986543" genericHeader="method">
2 Unification-based grammars
</sectionHeader>
<bodyText confidence="0.93485">
A unification-based grammar is defined as a pair
consisting of a set of lexical entries and a set of
phrase-structure rules. The lexical entries ex-
press word-specific characteristics, while the
phrase-structure rules describe constructions of
constituents in parse trees. Both the phrase-
structure rules and the lexical entries are
represented by feature structures (Carpenter,
1992), and constraints in the grammar are forced
by unification. Among the phrase-structure rules,
a binary rule is a partial function: T x T -+ T,
Spring has come
Spring has come
</bodyText>
<figureCaption confidence="0.999703">
Figure 1: Example of HPSG parsing.
</figureCaption>
<bodyText confidence="0.999883181818182">
where T is the set of all possible feature struc-
tures. The binary rule takes two partial parse
trees as daughters and returns a larger partial
parse tree that consists of the daughters and their
mother. A unary rule is a partial function:
T -+ T, which corresponds to a unary branch.
In the experiments, we used an HPSG (Pollard
and Sag, 1994), which is one of the sophisticated
unification-based grammars in linguistics. Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries. Figure 1 shows an example of HPSG pars-
ing of the sentence, â€œSpring has come.â€ The up-
per part of the figure shows a partial parse tree
for â€œhas come,â€ which is obtained by unifying
each of the lexical entries for â€œhasâ€ and â€œcomeâ€
with a daughter feature structure of the head-
complement rule. Larger partial parse trees are
obtained by repeatedly applying phrase-structure
rules to lexical/phrasal partial parse trees. Final-
ly, the parse result is output as a parse tree that
dominates the sentence.
</bodyText>
<sectionHeader confidence="0.991152" genericHeader="method">
3 Default unification
</sectionHeader>
<bodyText confidence="0.9995989">
Default unification was originally investigated in
a series of studies of lexical semantics, in order
to deal with default inheritance in a lexicon. It is
also desirable, however, for robust processing,
because (i) it almost always succeeds and (ii) a
feature structure is relaxed such that the amount
of information is maximized (Ninomiya et al.,
2002). In our experiments, we tested a simpli-
fied version of Copestakeâ€™s default unification.
Before explaining it, we first explain Carpenterâ€™s
</bodyText>
<figure confidence="0.998199325">
HEAD verb
HEAD noun
SUBJ &lt; SUBJ &lt;&gt; &gt;
1
COMPS &lt;&gt;
COMPS &lt;&gt;
HEAD noun
SUBJ &lt;&gt;
COMPS &lt;&gt;
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
head-comp
HEAD verb
SUBJ &lt; &gt;
2 1
COMPS &lt;&gt;
HEAD noun
1 SUBJ &lt;&gt;
COMPS &lt;&gt;
HEAD verb
SUBJ &lt;&gt;
COMPS &lt;&gt;
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
subject-head
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt;&gt;
head-comp
HEAD verb
SUBJ &lt; &gt;
2 1
COMPS &lt;&gt;
</figure>
<page confidence="0.996768">
604
</page>
<bodyText confidence="0.990519">
two definitions of default unification (Carpenter,
1993).
</bodyText>
<equation confidence="0.986344142857143">
(Credulous Default Unification)
à²¬G&apos; G&apos; âŠ‘ ğº is maximal such
ğ¹ âŠ”à¯– ğº= iFâŠ” ğº&apos;I }
l that ğ¹ âŠ” ğºï¿½is defined
(Skeptical Default Unification)
à²¬ à²¬
ğ¹ âŠ” à¯¦ ğº= â¨…(ğ¹ âŠ” à¯– ğº)
</equation>
<bodyText confidence="0.992276928571428">
ğ¹ is called a strict feature structure, whose in-
formation must not be lost, and ğº is called a de-
fault feature structure, whose information can be
lost but as little as possible so that ğ¹ and ğº can
be unified.
Credulous default unification is greedy, in that
it tries to maximize the amount of information
from the default feature structure, but it results in
a set of feature structures. Skeptical default un-
ification simply generalizes the set of feature
structures resulting from credulous default unifi-
cation. Skeptical default unification thus leads to
a unique result so that the default information
that can be found in every result of credulous
default unification remains. The following is an
example of skeptical default unification:
Copestake mentioned that the problem with
Carpenterâ€™s default unification is its time com-
plexity (Copestake, 1993). Carpenterâ€™s default
unification takes exponential time to find the op-
timal answer, because it requires checking the
unifiability of the power set of constraints in a
default feature structure. Copestake thus pro-
posed another definition of default unification, as
follows. Let ğ‘ƒğ‘‰ (ğº) be a function that returns a
set of path values in ğº, and let ğ‘ƒğ¸(ğº) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ğº.
</bodyText>
<equation confidence="0.88478975">
(Copestakeâ€™s default unification)
à¸­ğ¹ âˆˆ ğ‘ƒğ‘‰(ğº)and there is no Y âˆˆ ğ‘ƒğ‘‰(ğº)àµ¡,
ğ¹ Uà¯” ğº= ğ» âŠ” â¨† àµğ¹ such that ğ» âŠ” Yis defined and
ğ» âŠ” ğ¹ âŠ” ğ¹ï¿½is not defined
</equation>
<bodyText confidence="0.991622285714286">
where ğ» = ğ¹ âŠ” â¨† ğ‘ƒğ¸(ğº).
Copestakeâ€™s default unification works effi-
ciently because all path equations in the default
feature structure are unified with the strict fea-
ture structures, and because the unifiability of
path values is checked one by one for each node
in the result of unifying the path equations. The
</bodyText>
<equation confidence="0.984613725">
procedure forced_unification(p, q)
queue := {âŒ©p, qâŒª};
while( queue is not empty )
âŒ©p, qâŒª := shift(queue);
p := deref(p); q := deref(q);
if p â‰  q
Î¸(p) â‰” Î¸(p) âˆª Î¸(q);
Î¸(q) â‰” ptr(p);
forall f âˆˆ feat(p)â‹ƒ feat(q)
if f âˆˆ feat(p) âˆ§ f âˆˆ feat(q)
queue := queue âˆª âŒ©Î´(f, p), Î´(f, q)âŒª;
if f âˆ‰ feat(p) âˆ§ f âˆˆ feat(q)
Î´(f, p) â‰” Î´(f, q);
procedure mark(p, m)
p := deref(p);
if p has not been visited
Î¸(p) := {âŒ©Î¸(p), mâŒª};
forall f âˆˆ feat(p)
mark(Î´(f, p), m);
procedure collapse_defaults(p)
p := deref(p);
if p has not been visited
ts := âŠ¥; td := âŠ¥;
forall âŒ©t, ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡âŒª âˆˆ Î¸(p)
ts := ts âŠ” t;
forall âŒ©t, ğ‘‘ğ‘’ğ‘“ğ‘ğ‘¢ğ‘™ğ‘¡âŒª âˆˆ Î¸(p)
td := td âŠ” t;
if ts is not defined
return false;
if ts âŠ” td is defined
Î¸(p) := ts âŠ” td;
else
Î¸(p) := ts;
forall f âˆˆ feat(p)
collapse_defaults(Î´(f, p));
procedure default_unification(p, q)
mark(p, ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡);
mark(q, ğ‘‘ğ‘’ğ‘“ğ‘ğ‘¢ğ‘™ğ‘¡);
forced_unification(p, q);
collapse_defaults(p);
</equation>
<bodyText confidence="0.9628466">
Î¸(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of
types and markers in the feature structure node p.
A marker indicates that the types in a feature structure node
originally belong to the strict feature structures or the default
feature structures.
A pointer indicates that the node has been unified with other
nodes and it points the unified node. A function deref tra-
verses pointer nodes until it reaches to non-pointer node.
Î´(f, p) returns a feature structure node which is reached by
following a feature f from p.
</bodyText>
<figureCaption confidence="0.670932">
Figure 2: Algorithm for the simply typed ver-
sion of Corpestakeâ€™s default unification.
</figureCaption>
<bodyText confidence="0.999900076923077">
implementation is almost the same as that of
normal unification, but each node of a feature
structure has a set of values marked as â€œstrictâ€ or
â€œdefault.â€ When types are involved, however, it
is not easy to find unifiable path values in the
default feature structure. Therefore, we imple-
mented a more simply typed version of Corpes-
takeâ€™s default unification.
Figure 2 shows the algorithm by which we
implemented the simply typed version. First,
each node is marked as â€œstrictâ€ if it belongs to a
strict feature structure and as â€œdefaultâ€ otherwise.
The marked strict and default feature structures
</bodyText>
<equation confidence="0.78246775">
F: 1ğ› F: ğš F: 1 ğš F: ğš
à²¬
[F: ğš] âŠ”à­± á‰ G: 1 á‰ = â¨… á‰àµ¥G: ğ›àµ© , á‰ G: 1 á‰á‰‘ = àµ¥ G: âŠ¥ àµ©.
H:ğœ H: ğœ H:ğœ H: ğœ
</equation>
<page confidence="0.980373">
605
</page>
<bodyText confidence="0.719283966666666">
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i),
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1,
pi+2, pi+3
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr,
hwr, hpr, hlr
Unary reduce features: sy, hw, hp, hl
Sw(i) ... head word of i-th item from the top of the stack
Sp(i) ... head POS of i-th item from the top of the stack
Shw(i) ... head word of the head daughter of i-th item from the
top of the stack
Shp(i) ... head POS of the head daughter of i-th item from the
top of the stack
Snw(i) ... head word of the non-head daughter of i-th item
from the top of the stack
Snp(i) ... head POS of the non-head daughter of i-th item from
the top of the stack
Ssy(i) ... symbol of phrase category of the i-th item from the
top of the stack
Shsy(i) ... symbol of phrase category of the head daughter of
the i-th item from the top of the stack
Snsy(i) ... symbol of phrase category of the non-head daughter
of the i-th item from the top of the stack
d ... distance between head words of daughters
c ... whether a comma exists between daughters and/or inside
daughter phrases
sp ... the number of words dominated by the phrase
sy ... symbol of phrase category
hw ... head word
hp ... head POS
hl ... head lexical entry
</bodyText>
<figureCaption confidence="0.998011">
Figure 3: Feature templates.
</figureCaption>
<bodyText confidence="0.9999445">
are unified, whereas the types in the feature
structure nodes are not unified but merged as a
set of types. Then, all types marked as â€œstrictâ€
are unified into one type for each node. If this
fails, the default unification also returns unifica-
tion failure as its result. Finally, each node is
assigned a single type, which is the result of type
unification for all types marked as both â€œdefaultâ€
and â€œstrictâ€ if it succeeds or all types marked
only as â€œstrictâ€ otherwise.
</bodyText>
<sectionHeader confidence="0.891718" genericHeader="method">
4 Shift-reduce parsing for unification-
based grammars
</sectionHeader>
<bodyText confidence="0.999056866666667">
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).
Their algorithm works non-deterministically with
the mechanism of the packed parse forest, and
hence it has the problem of locality in the packed
parse forest. This section explains our shift-
reduce parsing algorithms, which are based on
deterministic shift-reduce CFG parsing (Sagae
and Lavie, 2005) and best-first shift-reduce CFG
parsing (Sagae and Lavie, 2006). Sagaeâ€™s parser
selects the most probable shift/reduce actions and
non-terminal symbols without assuming explicit
CFG rules. Therefore, his parser can proceed
deterministically without failure. However, in
</bodyText>
<table confidence="0.98737804">
Shift Features
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)]
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)]
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)]
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2]
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1,
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi]
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi,
pi+1] [pi+1, pi+2]
Binary Reduce Features
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)]
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)]
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)]
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2]
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d,
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c,
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d,
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy]
Unary Reduce Features
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)]
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)]
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)]
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2]
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl]
[sy, hw] [hp, hl] [hp] [hl] [sy]
</table>
<figureCaption confidence="0.999434">
Figure 4: Combinations of feature templates.
</figureCaption>
<bodyText confidence="0.999852833333333">
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar. We propose two new
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and
beam search. The major difference between our
algorithm and Sagaeâ€™s algorithm is that we use
default unification. First, we explain the deter-
ministic shift-reduce parsing algorithm, and then
we explain the shift-reduce parsing with back-
tracking and beam search.
</bodyText>
<subsectionHeader confidence="0.836111">
4.1 Deterministic shift-reduce parsing for
unification-based grammars
</subsectionHeader>
<bodyText confidence="0.999584444444445">
The deterministic shift-reduce parsing algorithm
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the
whole input sentence. Items in W are words and
POSs in the input sentence. The algorithm de-
fines two types of parser actions, shift and reduce,
as follows.
</bodyText>
<listItem confidence="0.849801833333333">
â€¢ Shift: A shift action removes the first item
(a word and a POS) from W. Then, one
lexical entry is selected from among the
candidate lexical entries for the item. Fi-
nally, the selected lexical entry is put on
the top of the stack.
</listItem>
<page confidence="0.90342">
606
</page>
<listItem confidence="0.979734545454545">
â€¢ Binary Reduce: A binary reduce action
removes two items from the top of the
stack. Then, partial parse trees are derived
by applying binary rules to the first re-
moved item and the second removed item
as a right daughter and left daughter, re-
spectively. Among the candidate partial
parse trees, one is selected and put on the
top of the stack.
â€¢ Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.
</listItem>
<bodyText confidence="0.983138396551724">
Then, partial parse trees are derived by
applying unary rules to the removed item.
Among the candidate partial parse trees,
one is selected and put on the top of the
stack.
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end). Parsing is considered suc-
cessfully finished when W is empty and S has
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty. Parsing is considered a
non-sentential success when W is empty and S
has only one item but it does not satisfy the sen-
tential condition.
In our experiments, we used a maximum en-
tropy classifier to choose the parserâ€™s action.
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates. Many of these features were tak-
en from those listed in (Ninomiya et al., 2007),
(Miyao and Tsujii, 2005) and (Sagae and Lavie,
2005), including global features defined over the
information in the stack, which cannot be used in
parsing with the packed parse forest. The fea-
tures for selecting shift actions are the same as
the features used in the supertagger (Ninomiya et
al., 2007). Our shift-reduce parsers can be re-
garded as an extension of the supertagger.
The deterministic parsing can fail because of
its grammarâ€™s hard constraints. So, we use de-
fault unification, which almost always succeeds
in unification. We assume that a head daughter
(or, an important daughter) is determined for
each binary rule in the unification-based gram-
mar. Default unification is used in the binary
rule application in the same way as used in Ni-
nomiyaâ€™s offline robust parsing (Ninomiya et al.,
2002), in which a binary rule unified with the
head daughter is the strict feature structure and
the non-head daughter is the default feature
ï¿½
structure, i.e., (R U H) U NH, where R is a bi-
head daughter. In the experiments, we used the
simply typed version of Copestakeâ€™s default un-
ification in the binary rule application1. Note
that default unification was always used instead
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation. Although Copestakeâ€™s default unification
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified
with the head daughter, or inconsistency is
caused by path equations in the default feature
structures. If the rule application fails for all the
binary rules, backtracking or beam search can be
used for its recovery as explained in Section 4.2.
In the experiments, we had no failure in the bi-
nary rule application with default unification.
</bodyText>
<subsectionHeader confidence="0.8498605">
4.2 Shift-reduce parsing by backtracking
and beam-search
</subsectionHeader>
<bodyText confidence="0.985854">
Another approach for recovering from the pars-
ing failure is backtracking. When parsing fails
or ends with non-sentential success, the parserâ€™s
state goes back to some old state (backtracking),
and it chooses the second best action and tries
parsing again. The old state is selected so as to
minimize the difference in the probabilities for
selecting the best candidate and the second best
candidate. We define a maximum number of
backtracking steps while parsing a sentence.
Backtracking repeats until parsing finishes with
sentential success or reaches the maximum num-
ber of backtracking steps. If parsing fails to find
a parse tree, the best continuous partial parse
trees are output for evaluation.
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first
search algorithms. Another possibility is to use
the best-first search algorithm. The best-first
parser has a state priority queue, and each state
consists of a tree stack and a word queue, which
are the same stack and queue explained in the
shift-reduce parsing algorithm. Parsing proceeds
by applying shift-reduce actions to the best state
in the state queue. First, the best state is re-
1 We also implemented Ninomiyaâ€™s default unification,
which can weaken path equation constraints. In the prelim-
inary experiments, we tested binary rule application given
</bodyText>
<equation confidence="0.9435922">
ï¿½
as (R U H) U NH with Copestakeâ€™s default unification,
ï¿½
(R U H) U NH with Ninomiyaâ€™s default unification, and
ï¿½
</equation>
<bodyText confidence="0.523170666666667">
(H U NH) U R with Ninomiyaâ€™s default unification. How-
ever, there was no significant difference of F-score among
these three methods. So, in the main experiments, we only
nary rule, H is a head daughter and NH is a non- ï¿½ NH with Copestakeâ€™s default unification
tested (R U H) U
because this method is simple and stable.
</bodyText>
<page confidence="0.953852">
607
</page>
<table confidence="0.8711095">
Previous
studies
(Miyao and Tsujii, 2005)
(Ninomiya et al., 2007)
Previous
studies
(Miyao and Tsujii, 2005)
(Ninomiya et al., 2007)
(Matsuzaki et al., 2007)
(Sagae et al., 2007)
</table>
<figure confidence="0.98486325">
Ours
back40
back10 + du
beam(7.4)
beam(20.1)+du
beam(403.4)
det
det+du
Ours
back40
back10 + du
beam(7.4)
beam(20.1)+du
beam(403.4)
det
det+du
</figure>
<table confidence="0.992549740740741">
Section 23 (Gold POS)
LP LR LF Avg. # of Avg. # # of # of non- # of
(%) (%) (%) Time backtrack of dead sentential sentential
(ms) states end success success
87.26 86.50 86.88 604 - - - - -
89.78 89.28 89.53 234 - - - - -
76.45 82.00 79.13 122 0 - 867 35 1514
87.78 87.45 87.61 256 0 - 0 117 2299
81.93 85.31 83.59 519 18986 - 386 23 2007
87.79 87.46 87.62 267 574 - 0 45 2371
86.17 87.77 86.96 510 - 226 369 30 2017
88.67 88.79 88.48 457 - 205 0 16 2400
89.98 89.92 89.95 10246 - 2822 71 14 2331
Section 23 (Auto POS) Avg. # of Avg. # # of # of non # of
LP LR LF Time backtrack of dead sentential sentential
(%) (%) (%) (ms) states end success success
84.96 84.25 84.60 674 - - - - -
87.28 87.05 87.17 260 - - - - -
86.93 86.47 86.70 30 - - - - -
88.50 88.00 88.20 - - - - - -
74.13 80.02 76.96 127 0 - 909 31 1476
85.93 85.72 85.82 252 0 - 0 124 2292
78.71 82.86 80.73 568 21068 - 438 27 1951
85.96 85.75 85.85 270 589 - 0 46 2370
83.84 85.82 84.82 544 - 234 421 33 1962
86.59 86.36 86.48 550 - 222 0 21 2395
87.70 87.86 87.78 16822 - 4553 89 16 2311
</table>
<tableCaption confidence="0.999902">
Table 1: Experimental results for Section 23.
</tableCaption>
<bodyText confidence="0.999948575757576">
moved from the state queue, and then shift-
reduce actions are applied to the state. The new-
ly generated states as results of the shift-reduce
actions are put on the queue. This process re-
peats until it generates a state satisfying the sen-
tential condition. We define the probability of a
parsing state as the product of the probabilities of
selecting actions that have been taken to reach
the state. We regard the state probability as the
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities
is always chosen in the algorithm. However, the
best-first algorithm with this objective function
searches like the breadth-first search, and hence,
parsing is very slow or cannot be processed in a
reasonable time. So, we introduce beam thre-
sholding to the best-first algorithm. The search
space is pruned by only adding a new state to the
state queue if its probability is greater than 1/b of
the probability of the best state in the states that
has had the same number of shift-reduce actions.
In what follows, we call this algorithm beam
search parsing.
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation. Note that, the beam search parsing for
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam
search. This is because we have to copy parse
trees, which consist of a large feature structures,
in every step of searching to keep many states on
the state queue. In the case of backtracking, co-
pying is not necessary.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999931266666667">
We evaluated the speed and accuracy of parsing
with Enju 2.30, an HPSG for English (Miyao and
Tsujii, 2005). The lexicon for the grammar was
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences). The grammar consisted
of 2,302 lexical entries for 11,187 words. Two
probabilistic classifiers for selecting shift-reduce
actions were trained using the same portion of
the treebank. One is trained using normal unifi-
cation, and the other is trained using default un-
ification.
We measured the accuracy of the predicate ar-
gument relation output of the parser. A predi-
cate-argument relation is defined as a tuple
(a, wh, a, w,), where a is the predicate type (e.g.,
</bodyText>
<page confidence="0.983585">
608
</page>
<figure confidence="0.990974066666667">
LF 90.00%
89.00%
88.00%
87.00%
86.00%
85.00%
84.00%
83.00%
82.00%
0 1 2 3 4 5 6 7 8
back
back+du
beam
beam+du
Avg. parsing time (s/sentence)
</figure>
<figureCaption confidence="0.999956">
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS).
</figureCaption>
<bodyText confidence="0.999665289473685">
adjective, intransitive verb), wh is the head word
of the predicate, a is the argument label (MOD-
ARG, ARG1, ..., ARG4), and wQ is the head
word of the argument. The labeled precision
(LP) / labeled recall (LR) is the ratio of tuples
correctly identified by the parser, and the labeled
F-score (LF) is the harmonic mean of the LP and
LR. This evaluation scheme was the same one
used in previous evaluations of lexicalized
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005). The expe-
riments were conducted on an Intel Xeon 5160
server with 3.0-GHz CPUs. Section 22 of the
Penn Treebank was used as the development set,
and the performance was evaluated using sen-
tences of &lt;_ 100 words in Section 23. The LP,
LR, and LF were evaluated for Section 23.
Table 1 lists the results of parsing for Section
23. In the table, â€œAvg. timeâ€ is the average pars-
ing time for the tested sentences. â€œ# of backtrackâ€
is the total number of backtracking steps that oc-
curred during parsing. â€œAvg. # of statesâ€ is the
average number of states for the tested sentences.
â€œ# of dead endâ€ is the number of sentences for
which parsing failed. â€œ# of non-sentential suc-
cessâ€ is the number of sentences for which pars-
ing succeeded but did not generate a parse tree
satisfying the sentential condition. â€œdetâ€ means
the deterministic shift-reduce parsing proposed
in this paper. â€œbacknâ€ means shift-reduce pars-
ing with backtracking at most n times for each
sentence. â€œduâ€ indicates that default unification
was used. â€œbeambâ€ means best-first shift-reduce
parsing with beam threshold b. The upper half
of the table gives the results obtained using gold
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger. The max-
imum number of backtracking steps and the
beam threshold were determined by observing
the performance for the development set (Section
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except
â€œbeam(403.4)â€). The performance of
â€œbeam(403.4)â€ was evaluated to see the limit of
the performance of the beam-search parsing.
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around
79.1% (Section 23, gold POS). With backtrack-
ing, the LF increased to 83.6%. Figure 5 shows
the relation between LF and parsing time for the
development set (Section 22, gold POS). As
seen in the figure, the LF increased as the parsing
time increased. The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.
Table 1 also shows that deterministic parsing
with default unification achieved higher accuracy,
with an LF of around 87.6% (Section 23, gold
POS), without backtracking. Default unification
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal
unification. The beam-search parsing without
default unification achieved high accuracy, with
an LF of around 87.0%, but is still worse than
deterministic parsing with default unification.
However, with default unification, it achieved
the best performance, with an LF of around
88.5%, in the settings of parsing time less than
500ms/sentence for Section 22.
For comparison with previous studies using
the packed parse forest, the performances of
Miyaoâ€™s parser, Ninomiyaâ€™s parser, Matsuzakiâ€™s
parser and Sagaeâ€™s parser are also listed in Table
1. Miyaoâ€™s parser is based on a probabilistic
model estimated only by a feature forest. Nino-
miyaâ€™s parser is a mixture of the feature forest
</bodyText>
<page confidence="0.996493">
609
</page>
<table confidence="0.99991784">
Path Strict Default Freq
type type
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: cons nil 101
HEAD:MOD:
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC: cons nil 96
hd:LOCAL:CAT:HEAD:MOD:
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
... ... ... ...
Total 10,598
</table>
<tableCaption confidence="0.998965">
Table 2: Path values overwritten by default unification in Section 22.
</tableCaption>
<bodyText confidence="0.999851516129032">
and an HPSG supertagger. Matsuzakiâ€™s parser
uses an HPSG supertagger and CFG filtering.
Sagaeâ€™s parser is a hybrid parser with a shallow
dependency parser. Though parsing without the
packed parse forest is disadvantageous to the
parsing with the packed parse forest in terms of
search space complexity, our model achieved
higher accuracy than Miyaoâ€™s parser.
â€œbeam(403.4)â€ in Table 1 and â€œbeamâ€ in Fig-
ure 5 show possibilities of beam-search parsing.
â€œbeam(403.4)â€ was very slow, but the accuracy
was higher than any other parsers except Sagaeâ€™s
parser.
Table 2 shows the behaviors of default unifi-
cation for â€œdet+du.â€ The table shows the 20
most frequent path values that were overwritten
by default unification in Section 22. In most of
the cases, the overwritten path values were in the
selection features, i.e., subcategorization frames
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee
specification (MOD:). The column of â€˜Default
typeâ€™ indicates the default types which were
overwritten by the strict types in the column of
â€˜Strict type,â€™ and the last column is the frequency
of overwriting. â€˜consâ€™ means a non-empty list,
and â€˜nilâ€™ means an empty list. In most of the
cases, modifiee and subcategorization frames
were changed from empty to non-empty and vice
versa. From the table, overwriting of head in-
formation was also observed, e.g., â€˜nounâ€™ was
changed to â€˜verb.â€™
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999995652173913">
We have presented shift-reduce parsing approach
for unification-based grammars, based on deter-
ministic shift-reduce parsing. First, we presented
deterministic parsing for unification-based
grammars. Deterministic parsing was difficult in
the framework of unification-based grammar
parsing, which often fails because of its hard
constraints. We introduced default unification to
avoid the parsing failure. Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification. The
experiments revealed that deterministic parsing
with default unification achieved high accuracy,
with a labeled F-score (LF) of 87.6% for Section
23 of the Penn Treebank with gold POSs.
Second, we also presented the best-first parsing
with beam search for unification-based gram-
mars. The best-first parsing with beam search
achieved the best accuracy, with an LF of 87.0%,
in the settings without default unification. De-
fault unification further increased LF from
87.0% to 88.5%. By widening the beam width,
the best-first parsing achieved an LF of 90.0%.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997380333333333">
Abney, Steven P. 1997. Stochastic Attribute-Value
Grammars. Computational Linguistics, 23(4), 597-
618.
</reference>
<page confidence="0.96782">
610
</page>
<reference confidence="0.999960190476191">
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71.
Briscoe, Ted and John Carroll. 1993. Generalized
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59.
Carpenter, Bob. 1992. The Logic of Typed Feature
Structures: Cambridge University Press.
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates
and Inheritance. In Inheritance, Defaults, and the
Lexicon. Cambridge: Cambridge University Press.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In proc. of ACL&apos;05, pp. 173-180.
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In proc. of COLING-04, pp. 282-288.
Clark, Stephen and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In proc.
of ACL&apos;04, pp. 104-111.
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon.
Cambridge: Cambridge University Press.
DaumÃ©, Hal III and Daniel Marcu. 2005. Learning as
Search Optimization: Approximate Large Margin
Methods for Structured Prediction. In proc. of
ICML 2005.
Geman, Stuart and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of
ACL&apos;02, pp. 279-286.
Hockenmaier, Julia. 2003. Parsing with Generative
Models of Predicate-Argument Structure. In proc.
of ACL&apos;03, pp. 359-366.
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ÌUnification-Based&amp;quot; Grammars. In proc. of
ACL &apos;99, pp. 535-541.
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In proc. of
HLT/NAACL&apos;04.
Malouf, Robert and Gertjan van Noord. 2004. Wide
Coverage Parsing with Stochastic Attribute Value
Grammars. In proc. of IJCNLP-04 Workshop
ÌBeyond Shallow Analyses&amp;quot;.
Matsuzaki, Takuya, Yusuke Miyao, and Jun&apos;ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp.
1671-1676.
Miyao, Yusuke and Jun&apos;ichi Tsujii. 2002. Maximum
Entropy Estimation for Feature Forests. In proc. of
HLT 2002, pp. 292-297.
Miyao, Yusuke and Jun&apos;ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG
parsing. In proc. of ACL&apos;05, pp. 83-90.
Nakagawa, Tetsuji. 2007. Multilingual dependency
parsing using global features. In proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pp. 915-932.
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke
Miyao, and Jun&apos;ichi Tsujii. 2007. A log-linear
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp.
60-68.
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa
Tsuruoka, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2006.
Extremely Lexicalized Models for Accurate and
Fast HPSG Parsing. In proc. of EMNLP 2006, pp.
155-163.
Ninomiya, Takashi, Yusuke Miyao, and Jun&apos;ichi Tsu-
jii. 2002. Lenient Default Unification for Robust
Processing within Unification Based Grammar
Formalisms. In proc. of COLING 2002, pp. 744-
750.
Nivre, Joakim and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In proc. of
COLING 2004, pp. 64-70.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar: University of Chicago
Press.
Ratnaparkhi, Adwait. 1997. A linear observed time
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP&apos;97.
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of
ACL&apos;00, pp. 480-487.
Sagae, Kenji and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In proc. of
IWPT 2005.
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp.
691-698.
Sagae, Kenji, Yusuke Miyao, and Jun&apos;ichi Tsujii.
2007. HPSG parsing with shallow dependency
constraints. In proc. of ACL 2007, pp. 624-631.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector
Machines. In proc. of IWPT-2003.
</reference>
<page confidence="0.997986">
611
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434485">
<title confidence="0.9583445">Deterministic shift-reduce parsing for unification-based grammars by using default unification</title>
<author confidence="0.991695">Takashi Ninomiya</author>
<affiliation confidence="0.9355195">Information Technology Center University of Tokyo, Japan</affiliation>
<email confidence="0.86181">ninomi@r.dl.itc.u-tokyo.ac.jp</email>
<author confidence="0.906216">Nobuyuki Shimizu</author>
<affiliation confidence="0.9346195">Information Technology Center University of Tokyo, Japan</affiliation>
<email confidence="0.973716">shimizu@r.dl.itc.u-tokyo.ac.jp</email>
<abstract confidence="0.993024875">Many parsing techniques including parameter estimation assume the use of a packed parse forest for efficient and accurate parsing. However, they have several inherent problems deriving from the restriction of locality in the packed parse forest. Deterministic parsing is one of solutions that can achieve simple and fast parsing without the mechanisms of the packed parse forest by accurately choosing search paths. We propose (i) deterministic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic Attribute-Value Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>597--618</pages>
<contexts>
<context position="1351" citStr="Abney, 1997" startWordPosition="182" endWordPosition="183">(i) deterministic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and John</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney, Steven P. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4), 597-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<contexts>
<context position="1916" citStr="Berger et al., 1996" startWordPosition="254" endWordPosition="257">investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1), 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR-Parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--59</pages>
<contexts>
<context position="14048" citStr="Briscoe and Carroll, 1993" startWordPosition="2329" endWordPosition="2332">e feature structure nodes are not unified but merged as a set of types. Then, all types marked as â€œstrictâ€ are unified into one type for each node. If this fails, the default unification also returns unification failure as its result. Finally, each node is assigned a single type, which is the result of type unification for all types marked as both â€œdefaultâ€ and â€œstrictâ€ if it succeeds or all types marked only as â€œstrictâ€ otherwise. 4 Shift-reduce parsing for unificationbased grammars Non-deterministic shift-reduce parsing for unification-based grammars has been studied by Briscoe and Carroll (Briscoe and Carroll, 1993). Their algorithm works non-deterministically with the mechanism of the packed parse forest, and hence it has the problem of locality in the packed parse forest. This section explains our shiftreduce parsing algorithms, which are based on deterministic shift-reduce CFG parsing (Sagae and Lavie, 2005) and best-first shift-reduce CFG parsing (Sagae and Lavie, 2006). Sagaeâ€™s parser selects the most probable shift/reduce actions and non-terminal symbols without assuming explicit CFG rules. Therefore, his parser can proceed deterministically without failure. However, in Shift Features [Sw(0)] [Sw(1</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, Ted and John Carroll. 1993. Generalized probabilistic LR-Parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1), 25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures:</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5841" citStr="Carpenter, 1992" startWordPosition="844" endWordPosition="845">based grammars and default unification, respectively. Shift-reduce parsing for unification-based grammars is presented in Section 4. Section 5 discusses our experiments, and Section 6 concludes the paper. 2 Unification-based grammars A unification-based grammar is defined as a pair consisting of a set of lexical entries and a set of phrase-structure rules. The lexical entries express word-specific characteristics, while the phrase-structure rules describe constructions of constituents in parse trees. Both the phrasestructure rules and the lexical entries are represented by feature structures (Carpenter, 1992), and constraints in the grammar are forced by unification. Among the phrase-structure rules, a binary rule is a partial function: T x T -+ T, Spring has come Spring has come Figure 1: Example of HPSG parsing. where T is the set of all possible feature structures. The binary rule takes two partial parse trees as daughters and returns a larger partial parse tree that consists of the daughters and their mother. A unary rule is a partial function: T -+ T, which corresponds to a unary branch. In the experiments, we used an HPSG (Pollard and Sag, 1994), which is one of the sophisticated unification</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, Bob. 1992. The Logic of Typed Feature Structures: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Skeptical and Credulous Default Unification with Applications to Templates and Inheritance. In Inheritance, Defaults, and the Lexicon. Cambridge:</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7981" citStr="Carpenter, 1993" startWordPosition="1228" endWordPosition="1229">is relaxed such that the amount of information is maximized (Ninomiya et al., 2002). In our experiments, we tested a simplified version of Copestakeâ€™s default unification. Before explaining it, we first explain Carpenterâ€™s HEAD verb HEAD noun SUBJ &lt; SUBJ &lt;&gt; &gt; 1 COMPS &lt;&gt; COMPS &lt;&gt; HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 head-comp HEAD verb SUBJ &lt; &gt; 2 1 COMPS &lt;&gt; HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp HEAD verb SUBJ &lt; &gt; 2 1 COMPS &lt;&gt; 604 two definitions of default unification (Carpenter, 1993). (Credulous Default Unification) à²¬G&apos; G&apos; âŠ‘ ğº is maximal such ğ¹ âŠ” ğº= iFâŠ” ğº&apos;I } l that ğ¹ âŠ” ğºï¿½is defined (Skeptical Default Unification) à²¬ à²¬ ğ¹ âŠ” à¯¦ ğº= â¨…(ğ¹ âŠ”  ğº) ğ¹ is called a strict feature structure, whose information must not be lost, and ğº is called a default feature structure, whose information can be lost but as little as possible so that ğ¹ and ğº can be unified. Credulous default unification is greedy, in that it tries to maximize the amount of information from the default feature structure, but it results in a set of feature structures. Skeptical default unification simply generalizes the </context>
</contexts>
<marker>Carpenter, 1993</marker>
<rawString>Carpenter, Bob. 1993. Skeptical and Credulous Default Unification with Applications to Templates and Inheritance. In Inheritance, Defaults, and the Lexicon. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In proc. of ACL&apos;05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="4152" citStr="Charniak and Johnson, 2005" startWordPosition="602" endWordPosition="606">â€“611, Athens, Greece, 30 March â€“ 3 April 2009. cï¿½2009 Association for Computational Linguistics 603 segmented constituents are instantiated. This is because values in parse trees can propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In proc. of ACL&apos;05, pp. 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In proc. of COLING-04,</booktitle>
<pages>282--288</pages>
<contexts>
<context position="2359" citStr="Clark and Curran, 2004" startWordPosition="323" endWordPosition="327">okyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parserâ€™s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same cal</context>
<context position="26449" citStr="Clark and Curran, 2004" startWordPosition="4451" endWordPosition="4454">82.00% 0 1 2 3 4 5 6 7 8 back back+du beam beam+du Avg. parsing time (s/sentence) Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wQ is the head word of the argument. The labeled precision (LP) / labeled recall (LR) is the ratio of tuples correctly identified by the parser, and the labeled F-score (LF) is the harmonic mean of the LP and LR. This evaluation scheme was the same one used in previous evaluations of lexicalized grammars (Clark and Curran, 2004b; Hockenmaier, 2003; Miyao and Tsujii, 2005). The experiments were conducted on an Intel Xeon 5160 server with 3.0-GHz CPUs. Section 22 of the Penn Treebank was used as the development set, and the performance was evaluated using sentences of &lt;_ 100 words in Section 23. The LP, LR, and LF were evaluated for Section 23. Table 1 lists the results of parsing for Section 23. In the table, â€œAvg. timeâ€ is the average parsing time for the tested sentences. â€œ# of backtrackâ€ is the total number of backtracking steps that occurred during parsing. â€œAvg. # of statesâ€ is the average number of states for t</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, Stephen and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In proc. of COLING-04, pp. 282-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In proc. of ACL&apos;04,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2359" citStr="Clark and Curran, 2004" startWordPosition="323" endWordPosition="327">okyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parserâ€™s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same cal</context>
<context position="26449" citStr="Clark and Curran, 2004" startWordPosition="4451" endWordPosition="4454">82.00% 0 1 2 3 4 5 6 7 8 back back+du beam beam+du Avg. parsing time (s/sentence) Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wQ is the head word of the argument. The labeled precision (LP) / labeled recall (LR) is the ratio of tuples correctly identified by the parser, and the labeled F-score (LF) is the harmonic mean of the LP and LR. This evaluation scheme was the same one used in previous evaluations of lexicalized grammars (Clark and Curran, 2004b; Hockenmaier, 2003; Miyao and Tsujii, 2005). The experiments were conducted on an Intel Xeon 5160 server with 3.0-GHz CPUs. Section 22 of the Penn Treebank was used as the development set, and the performance was evaluated using sentences of &lt;_ 100 words in Section 23. The LP, LR, and LF were evaluated for Section 23. Table 1 lists the results of parsing for Section 23. In the table, â€œAvg. timeâ€ is the average parsing time for the tested sentences. â€œ# of backtrackâ€ is the total number of backtracking steps that occurred during parsing. â€œAvg. # of statesâ€ is the average number of states for t</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, Stephen and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In proc. of ACL&apos;04, pp. 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Defaults in Lexical Representation. In Inheritance, Defaults, and the Lexicon. Cambridge:</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8997" citStr="Copestake, 1993" startWordPosition="1401" endWordPosition="1402"> greedy, in that it tries to maximize the amount of information from the default feature structure, but it results in a set of feature structures. Skeptical default unification simply generalizes the set of feature structures resulting from credulous default unification. Skeptical default unification thus leads to a unique result so that the default information that can be found in every result of credulous default unification remains. The following is an example of skeptical default unification: Copestake mentioned that the problem with Carpenterâ€™s default unification is its time complexity (Copestake, 1993). Carpenterâ€™s default unification takes exponential time to find the optimal answer, because it requires checking the unifiability of the power set of constraints in a default feature structure. Copestake thus proposed another definition of default unification, as follows. Let ğ‘ƒğ‘‰ (ğº) be a function that returns a set of path values in ğº, and let ğ‘ƒğ¸(ğº) be a function that returns a set of path equations, i.e., information about structure sharing in ğº. (Copestakeâ€™s default unification) à¸­ğ¹ âˆˆ ğ‘ƒğ‘‰(ğº)and there is no Y âˆˆ ğ‘ƒğ‘‰(ğº)àµ¡, ğ¹ U ğº= ğ» âŠ” â¨† ğ¹ such that ğ» âŠ” Yis defined and ğ» âŠ” ğ¹ âŠ” ğ¹ï¿½is not defined whe</context>
</contexts>
<marker>Copestake, 1993</marker>
<rawString>Copestake, Ann. 1993. Defaults in Lexical Representation. In Inheritance, Defaults, and the Lexicon. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal DaumÃ©</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as Search Optimization: Approximate Large Margin Methods for Structured Prediction. In</title>
<date>2005</date>
<booktitle>proc. of ICML</booktitle>
<contexts>
<context position="4257" citStr="DaumÃ© and Marcu, 2005" startWordPosition="616" endWordPosition="619">constituents are instantiated. This is because values in parse trees can propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard</context>
</contexts>
<marker>DaumÃ©, Marcu, 2005</marker>
<rawString>DaumÃ©, Hal III and Daniel Marcu. 2005. Learning as Search Optimization: Approximate Large Margin Methods for Structured Prediction. In proc. of ICML 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In proc. of ACL&apos;02,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="1961" citStr="Geman and Johnson, 2002" startWordPosition="261" endWordPosition="264">s (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estim</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Geman, Stuart and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In proc. of ACL&apos;02, pp. 279-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with Generative Models of Predicate-Argument Structure.</title>
<date>2003</date>
<booktitle>In proc. of ACL&apos;03,</booktitle>
<pages>359--366</pages>
<contexts>
<context position="26469" citStr="Hockenmaier, 2003" startWordPosition="4455" endWordPosition="4457">back back+du beam beam+du Avg. parsing time (s/sentence) Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wQ is the head word of the argument. The labeled precision (LP) / labeled recall (LR) is the ratio of tuples correctly identified by the parser, and the labeled F-score (LF) is the harmonic mean of the LP and LR. This evaluation scheme was the same one used in previous evaluations of lexicalized grammars (Clark and Curran, 2004b; Hockenmaier, 2003; Miyao and Tsujii, 2005). The experiments were conducted on an Intel Xeon 5160 server with 3.0-GHz CPUs. Section 22 of the Penn Treebank was used as the development set, and the performance was evaluated using sentences of &lt;_ 100 words in Section 23. The LP, LR, and LF were evaluated for Section 23. Table 1 lists the results of parsing for Section 23. In the table, â€œAvg. timeâ€ is the average parsing time for the tested sentences. â€œ# of backtrackâ€ is the total number of backtracking steps that occurred during parsing. â€œAvg. # of statesâ€ is the average number of states for the tested sentences.</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, Julia. 2003. Parsing with Generative Models of Predicate-Argument Structure. In proc. of ACL&apos;03, pp. 359-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for Stochastic ÌUnification-Based&amp;quot; Grammars.</title>
<date>1999</date>
<booktitle>In proc. of ACL &apos;99,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="1373" citStr="Johnson et al., 1999" startWordPosition="184" endWordPosition="187">stic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao a</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for Stochastic ÌUnification-Based&amp;quot; Grammars. In proc. of ACL &apos;99, pp. 535-541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell</author>
<author>A Vasserman</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In proc. of HLT/NAACL&apos;04.</booktitle>
<contexts>
<context position="1394" citStr="Kaplan et al., 2004" startWordPosition="188" endWordPosition="191">ing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and </context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, 2004</marker>
<rawString>Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell III, and A. Vasserman. 2004. Speed and accuracy in shallow and deep stochastic parsing. In proc. of HLT/NAACL&apos;04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide Coverage Parsing with Stochastic Attribute Value Grammars.</title>
<date>2004</date>
<booktitle>In proc. of IJCNLP-04 Workshop ÌBeyond Shallow Analyses&amp;quot;.</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Malouf, Robert and Gertjan van Noord. 2004. Wide Coverage Parsing with Stochastic Attribute Value Grammars. In proc. of IJCNLP-04 Workshop ÌBeyond Shallow Analyses&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Efficient HPSG Parsing with Supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>In proc. of IJCAI</booktitle>
<pages>1671--1676</pages>
<contexts>
<context position="22268" citStr="Matsuzaki et al., 2007" startWordPosition="3675" endWordPosition="3678">inary rule application given ï¿½ as (R U H) U NH with Copestakeâ€™s default unification, ï¿½ (R U H) U NH with Ninomiyaâ€™s default unification, and ï¿½ (H U NH) U R with Ninomiyaâ€™s default unification. However, there was no significant difference of F-score among these three methods. So, in the main experiments, we only nary rule, H is a head daughter and NH is a non- ï¿½ NH with Copestakeâ€™s default unification tested (R U H) U because this method is simple and stable. 607 Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) (Matsuzaki et al., 2007) (Sagae et al., 2007) Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Section 23 (Gold POS) LP LR LF Avg. # of Avg. # # of # of non- # of (%) (%) (%) Time backtrack of dead sentential sentential (ms) states end success success 87.26 86.50 86.88 604 - - - - - 89.78 89.28 89.53 234 - - - - - 76.45 82.00 79.13 122 0 - 867 35 1514 87.78 87.45 87.61 256 0 - 0 117 2299 81.93 85.31 83.59 519 18986 - 386 23 2007 87.79 87.46 87.62 267 574 - 0 45 2371 86.17 87.77 86.96 510 - 226 369 30 2017 88.67 88.79 88.48 45</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Matsuzaki, Takuya, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2007. Efficient HPSG Parsing with Supertagging and CFG-filtering. In proc. of IJCAI 2007, pp. 1671-1676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Maximum Entropy Estimation for Feature Forests. In</title>
<date>2002</date>
<booktitle>proc. of HLT</booktitle>
<pages>292--297</pages>
<contexts>
<context position="2007" citStr="Miyao and Tsujii, 2002" startWordPosition="269" endWordPosition="272"> al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Miyao, Yusuke and Jun&apos;ichi Tsujii. 2002. Maximum Entropy Estimation for Feature Forests. In proc. of HLT 2002, pp. 292-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In proc. of ACL&apos;05,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="1446" citStr="Miyao and Tsujii, 2005" startWordPosition="197" endWordPosition="200">-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a </context>
<context position="18518" citStr="Miyao and Tsujii, 2005" startWordPosition="3056" endWordPosition="3059"> considered successfully finished when W is empty and S has only one item which satisfies the sentential condition: the category is verb and the subcategorization frame is empty. Parsing is considered a non-sentential success when W is empty and S has only one item but it does not satisfy the sentential condition. In our experiments, we used a maximum entropy classifier to choose the parserâ€™s action. Figure 3 lists the feature templates for the classifier, and Figure 4 lists the combinations of feature templates. Many of these features were taken from those listed in (Ninomiya et al., 2007), (Miyao and Tsujii, 2005) and (Sagae and Lavie, 2005), including global features defined over the information in the stack, which cannot be used in parsing with the packed parse forest. The features for selecting shift actions are the same as the features used in the supertagger (Ninomiya et al., 2007). Our shift-reduce parsers can be regarded as an extension of the supertagger. The deterministic parsing can fail because of its grammarâ€™s hard constraints. So, we use default unification, which almost always succeeds in unification. We assume that a head daughter (or, an important daughter) is determined for each binary</context>
<context position="22153" citStr="Miyao and Tsujii, 2005" startWordPosition="3657" endWordPosition="3660">miyaâ€™s default unification, which can weaken path equation constraints. In the preliminary experiments, we tested binary rule application given ï¿½ as (R U H) U NH with Copestakeâ€™s default unification, ï¿½ (R U H) U NH with Ninomiyaâ€™s default unification, and ï¿½ (H U NH) U R with Ninomiyaâ€™s default unification. However, there was no significant difference of F-score among these three methods. So, in the main experiments, we only nary rule, H is a head daughter and NH is a non- ï¿½ NH with Copestakeâ€™s default unification tested (R U H) U because this method is simple and stable. 607 Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) (Matsuzaki et al., 2007) (Sagae et al., 2007) Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Section 23 (Gold POS) LP LR LF Avg. # of Avg. # # of # of non- # of (%) (%) (%) Time backtrack of dead sentential sentential (ms) states end success success 87.26 86.50 86.88 604 - - - - - 89.78 89.28 89.53 234 - - - - - 76.45 82.00 79.13 122 0 - 867 35 1514 87.78 87.45 87.61 256 0 - 0 117 2299 81.93 85.31 83.59 519 189</context>
<context position="25196" citStr="Miyao and Tsujii, 2005" startWordPosition="4235" endWordPosition="4238"> follows, we call this algorithm beam search parsing. In the experiments, we tested both backtracking and beam search with/without default unification. Note that, the beam search parsing for unification-based grammars is very slow compared to the shift-reduce CFG parsing with beam search. This is because we have to copy parse trees, which consist of a large feature structures, in every step of searching to keep many states on the state queue. In the case of backtracking, copying is not necessary. 5 Experiments We evaluated the speed and accuracy of parsing with Enju 2.30, an HPSG for English (Miyao and Tsujii, 2005). The lexicon for the grammar was extracted from Sections 02-21 of the Penn Treebank (39,832 sentences). The grammar consisted of 2,302 lexical entries for 11,187 words. Two probabilistic classifiers for selecting shift-reduce actions were trained using the same portion of the treebank. One is trained using normal unification, and the other is trained using default unification. We measured the accuracy of the predicate argument relation output of the parser. A predicate-argument relation is defined as a tuple (a, wh, a, w,), where a is the predicate type (e.g., 608 LF 90.00% 89.00% 88.00% 87.0</context>
<context position="26494" citStr="Miyao and Tsujii, 2005" startWordPosition="4458" endWordPosition="4461">eam+du Avg. parsing time (s/sentence) Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wQ is the head word of the argument. The labeled precision (LP) / labeled recall (LR) is the ratio of tuples correctly identified by the parser, and the labeled F-score (LF) is the harmonic mean of the LP and LR. This evaluation scheme was the same one used in previous evaluations of lexicalized grammars (Clark and Curran, 2004b; Hockenmaier, 2003; Miyao and Tsujii, 2005). The experiments were conducted on an Intel Xeon 5160 server with 3.0-GHz CPUs. Section 22 of the Penn Treebank was used as the development set, and the performance was evaluated using sentences of &lt;_ 100 words in Section 23. The LP, LR, and LF were evaluated for Section 23. Table 1 lists the results of parsing for Section 23. In the table, â€œAvg. timeâ€ is the average parsing time for the tested sentences. â€œ# of backtrackâ€ is the total number of backtracking steps that occurred during parsing. â€œAvg. # of statesâ€ is the average number of states for the tested sentences. â€œ# of dead endâ€ is the n</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Miyao, Yusuke and Jun&apos;ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In proc. of ACL&apos;05, pp. 83-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Multilingual dependency parsing using global features.</title>
<date>2007</date>
<booktitle>In proc. of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<pages>915--932</pages>
<contexts>
<context position="4323" citStr="Nakagawa, 2007" startWordPosition="629" endWordPosition="630">propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose </context>
</contexts>
<marker>Nakagawa, 2007</marker>
<rawString>Nakagawa, Tetsuji. 2007. Multilingual dependency parsing using global features. In proc. of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915-932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>A log-linear model with an n-gram reference distribution for accurate HPSG parsing.</title>
<date>2007</date>
<booktitle>In proc. of IWPT</booktitle>
<pages>60--68</pages>
<contexts>
<context position="2525" citStr="Ninomiya et al., 2007" startWordPosition="351" endWordPosition="354">t al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parserâ€™s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same calculation. This also means that the semantic structures must be segmented. This is a crucial problem when we think of designing semantic structures other than predicat</context>
<context position="18492" citStr="Ninomiya et al., 2007" startWordPosition="3052" endWordPosition="3055">, a dead end). Parsing is considered successfully finished when W is empty and S has only one item which satisfies the sentential condition: the category is verb and the subcategorization frame is empty. Parsing is considered a non-sentential success when W is empty and S has only one item but it does not satisfy the sentential condition. In our experiments, we used a maximum entropy classifier to choose the parserâ€™s action. Figure 3 lists the feature templates for the classifier, and Figure 4 lists the combinations of feature templates. Many of these features were taken from those listed in (Ninomiya et al., 2007), (Miyao and Tsujii, 2005) and (Sagae and Lavie, 2005), including global features defined over the information in the stack, which cannot be used in parsing with the packed parse forest. The features for selecting shift actions are the same as the features used in the supertagger (Ninomiya et al., 2007). Our shift-reduce parsers can be regarded as an extension of the supertagger. The deterministic parsing can fail because of its grammarâ€™s hard constraints. So, we use default unification, which almost always succeeds in unification. We assume that a head daughter (or, an important daughter) is </context>
<context position="22177" citStr="Ninomiya et al., 2007" startWordPosition="3661" endWordPosition="3664">n, which can weaken path equation constraints. In the preliminary experiments, we tested binary rule application given ï¿½ as (R U H) U NH with Copestakeâ€™s default unification, ï¿½ (R U H) U NH with Ninomiyaâ€™s default unification, and ï¿½ (H U NH) U R with Ninomiyaâ€™s default unification. However, there was no significant difference of F-score among these three methods. So, in the main experiments, we only nary rule, H is a head daughter and NH is a non- ï¿½ NH with Copestakeâ€™s default unification tested (R U H) U because this method is simple and stable. 607 Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) (Matsuzaki et al., 2007) (Sagae et al., 2007) Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Section 23 (Gold POS) LP LR LF Avg. # of Avg. # # of # of non- # of (%) (%) (%) Time backtrack of dead sentential sentential (ms) states end success success 87.26 86.50 86.88 604 - - - - - 89.78 89.28 89.53 234 - - - - - 76.45 82.00 79.13 122 0 - 867 35 1514 87.78 87.45 87.61 256 0 - 0 117 2299 81.93 85.31 83.59 519 18986 - 386 23 2007 87.79 8</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Ninomiya, Takashi, Takuya Matsuzaki, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2007. A log-linear model with an n-gram reference distribution for accurate HPSG parsing. In proc. of IWPT 2007, pp. 60-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Extremely Lexicalized Models for Accurate and Fast HPSG Parsing.</title>
<date>2006</date>
<booktitle>In proc. of EMNLP</booktitle>
<pages>155--163</pages>
<contexts>
<context position="2549" citStr="Ninomiya et al., 2006" startWordPosition="355" endWordPosition="358"> Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parserâ€™s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same calculation. This also means that the semantic structures must be segmented. This is a crucial problem when we think of designing semantic structures other than predicate argument structures, e</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2006. Extremely Lexicalized Models for Accurate and Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 155-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Lenient Default Unification for Robust Processing within Unification Based Grammar Formalisms.</title>
<date>2002</date>
<booktitle>In proc. of COLING</booktitle>
<pages>744--750</pages>
<contexts>
<context position="7448" citStr="Ninomiya et al., 2002" startWordPosition="1116" endWordPosition="1119">e structure of the headcomplement rule. Larger partial parse trees are obtained by repeatedly applying phrase-structure rules to lexical/phrasal partial parse trees. Finally, the parse result is output as a parse tree that dominates the sentence. 3 Default unification Default unification was originally investigated in a series of studies of lexical semantics, in order to deal with default inheritance in a lexicon. It is also desirable, however, for robust processing, because (i) it almost always succeeds and (ii) a feature structure is relaxed such that the amount of information is maximized (Ninomiya et al., 2002). In our experiments, we tested a simplified version of Copestakeâ€™s default unification. Before explaining it, we first explain Carpenterâ€™s HEAD verb HEAD noun SUBJ &lt; SUBJ &lt;&gt; &gt; 1 COMPS &lt;&gt; COMPS &lt;&gt; HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 head-comp HEAD verb SUBJ &lt; &gt; 2 1 COMPS &lt;&gt; HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp HEAD verb SUBJ &lt; &gt; 2 1 COMPS &lt;&gt; 604 two definitions of default unification (Carpenter, 1993). (Credulous Default Unification) à²¬G&apos; G&apos; âŠ‘ ğº is maximal such ğ¹ âŠ” ğº</context>
<context position="19301" citStr="Ninomiya et al., 2002" startWordPosition="3185" endWordPosition="3188">The features for selecting shift actions are the same as the features used in the supertagger (Ninomiya et al., 2007). Our shift-reduce parsers can be regarded as an extension of the supertagger. The deterministic parsing can fail because of its grammarâ€™s hard constraints. So, we use default unification, which almost always succeeds in unification. We assume that a head daughter (or, an important daughter) is determined for each binary rule in the unification-based grammar. Default unification is used in the binary rule application in the same way as used in Ninomiyaâ€™s offline robust parsing (Ninomiya et al., 2002), in which a binary rule unified with the head daughter is the strict feature structure and the non-head daughter is the default feature ï¿½ structure, i.e., (R U H) U NH, where R is a bihead daughter. In the experiments, we used the simply typed version of Copestakeâ€™s default unification in the binary rule application1. Note that default unification was always used instead of normal unification in both training and evaluation in the case of the parsers using default unification. Although Copestakeâ€™s default unification almost always succeeds, the binary rule application can fail if the binary r</context>
</contexts>
<marker>Ninomiya, Miyao, Tsujii, 2002</marker>
<rawString>Ninomiya, Takashi, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2002. Lenient Default Unification for Robust Processing within Unification Based Grammar Formalisms. In proc. of COLING 2002, pp. 744-750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In proc. of COLING</booktitle>
<pages>64--70</pages>
<contexts>
<context position="4583" citStr="Nivre and Scholz, 2004" startWordPosition="666" endWordPosition="669">ently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased gramm</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Nivre, Joakim and Mario Scholz. 2004. Deterministic dependency parsing of English text. In proc. of COLING 2004, pp. 64-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar:</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6394" citStr="Pollard and Sag, 1994" startWordPosition="942" endWordPosition="945">xical entries are represented by feature structures (Carpenter, 1992), and constraints in the grammar are forced by unification. Among the phrase-structure rules, a binary rule is a partial function: T x T -+ T, Spring has come Spring has come Figure 1: Example of HPSG parsing. where T is the set of all possible feature structures. The binary rule takes two partial parse trees as daughters and returns a larger partial parse tree that consists of the daughters and their mother. A unary rule is a partial function: T -+ T, which corresponds to a unary branch. In the experiments, we used an HPSG (Pollard and Sag, 1994), which is one of the sophisticated unification-based grammars in linguistics. Generally, an HPSG has a small number of phrasestructure rules and a large number of lexical entries. Figure 1 shows an example of HPSG parsing of the sentence, â€œSpring has come.â€ The upper part of the figure shows a partial parse tree for â€œhas come,â€ which is obtained by unifying each of the lexical entries for â€œhasâ€ and â€œcomeâ€ with a daughter feature structure of the headcomplement rule. Larger partial parse trees are obtained by repeatedly applying phrase-structure rules to lexical/phrasal partial parse trees. Fi</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In proc. of EMNLP&apos;97.</booktitle>
<contexts>
<context position="4602" citStr="Ratnaparkhi, 1997" startWordPosition="670" endWordPosition="671">mmars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased grammars. Sections 2 and</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi, Adwait. 1997. A linear observed time statistical parser based on maximum entropy models. In proc. of EMNLP&apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized Stochastic Modeling of Constraint-Based Grammars using LogLinear Measures and EM Training.</title>
<date>2000</date>
<booktitle>In proc. of ACL&apos;00,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="1469" citStr="Riezler et al., 2000" startWordPosition="201" endWordPosition="204">ing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including Takuya Matsuzaki Department of Computer Science University of Tokyo, Japan matuzaki@is.s.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming alg</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized Stochastic Modeling of Constraint-Based Grammars using LogLinear Measures and EM Training. In proc. of ACL&apos;00, pp. 480-487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In proc. of IWPT</booktitle>
<contexts>
<context position="4625" citStr="Sagae and Lavie, 2005" startWordPosition="672" endWordPosition="676">uctures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased grammars. Sections 2 and 3 explain unification-</context>
<context position="14349" citStr="Sagae and Lavie, 2005" startWordPosition="2374" endWordPosition="2377">ype unification for all types marked as both â€œdefaultâ€ and â€œstrictâ€ if it succeeds or all types marked only as â€œstrictâ€ otherwise. 4 Shift-reduce parsing for unificationbased grammars Non-deterministic shift-reduce parsing for unification-based grammars has been studied by Briscoe and Carroll (Briscoe and Carroll, 1993). Their algorithm works non-deterministically with the mechanism of the packed parse forest, and hence it has the problem of locality in the packed parse forest. This section explains our shiftreduce parsing algorithms, which are based on deterministic shift-reduce CFG parsing (Sagae and Lavie, 2005) and best-first shift-reduce CFG parsing (Sagae and Lavie, 2006). Sagaeâ€™s parser selects the most probable shift/reduce actions and non-terminal symbols without assuming explicit CFG rules. Therefore, his parser can proceed deterministically without failure. However, in Shift Features [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] [Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] [Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] [Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] [pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, wi] [pi, wi] [pi+1, wi] [pi,</context>
<context position="18546" citStr="Sagae and Lavie, 2005" startWordPosition="3061" endWordPosition="3064">shed when W is empty and S has only one item which satisfies the sentential condition: the category is verb and the subcategorization frame is empty. Parsing is considered a non-sentential success when W is empty and S has only one item but it does not satisfy the sentential condition. In our experiments, we used a maximum entropy classifier to choose the parserâ€™s action. Figure 3 lists the feature templates for the classifier, and Figure 4 lists the combinations of feature templates. Many of these features were taken from those listed in (Ninomiya et al., 2007), (Miyao and Tsujii, 2005) and (Sagae and Lavie, 2005), including global features defined over the information in the stack, which cannot be used in parsing with the packed parse forest. The features for selecting shift actions are the same as the features used in the supertagger (Ninomiya et al., 2007). Our shift-reduce parsers can be regarded as an extension of the supertagger. The deterministic parsing can fail because of its grammarâ€™s hard constraints. So, we use default unification, which almost always succeeds in unification. We assume that a head daughter (or, an important daughter) is determined for each binary rule in the unification-bas</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Sagae, Kenji and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In proc. of IWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In proc. of COLING/ACL on Main conference poster sessions,</booktitle>
<pages>691--698</pages>
<contexts>
<context position="14413" citStr="Sagae and Lavie, 2006" startWordPosition="2383" endWordPosition="2386">ctâ€ if it succeeds or all types marked only as â€œstrictâ€ otherwise. 4 Shift-reduce parsing for unificationbased grammars Non-deterministic shift-reduce parsing for unification-based grammars has been studied by Briscoe and Carroll (Briscoe and Carroll, 1993). Their algorithm works non-deterministically with the mechanism of the packed parse forest, and hence it has the problem of locality in the packed parse forest. This section explains our shiftreduce parsing algorithms, which are based on deterministic shift-reduce CFG parsing (Sagae and Lavie, 2005) and best-first shift-reduce CFG parsing (Sagae and Lavie, 2006). Sagaeâ€™s parser selects the most probable shift/reduce actions and non-terminal symbols without assuming explicit CFG rules. Therefore, his parser can proceed deterministically without failure. However, in Shift Features [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] [Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] [Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] [Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] [pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] [pi-1, pi, pi+1] [pi, pi+1, </context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Sagae, Kenji and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In proc. of COLING/ACL on Main conference poster sessions, pp. 691-698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>HPSG parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In proc. of ACL</booktitle>
<pages>624--631</pages>
<contexts>
<context position="22289" citStr="Sagae et al., 2007" startWordPosition="3679" endWordPosition="3682">ven ï¿½ as (R U H) U NH with Copestakeâ€™s default unification, ï¿½ (R U H) U NH with Ninomiyaâ€™s default unification, and ï¿½ (H U NH) U R with Ninomiyaâ€™s default unification. However, there was no significant difference of F-score among these three methods. So, in the main experiments, we only nary rule, H is a head daughter and NH is a non- ï¿½ NH with Copestakeâ€™s default unification tested (R U H) U because this method is simple and stable. 607 Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) Previous studies (Miyao and Tsujii, 2005) (Ninomiya et al., 2007) (Matsuzaki et al., 2007) (Sagae et al., 2007) Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Ours back40 back10 + du beam(7.4) beam(20.1)+du beam(403.4) det det+du Section 23 (Gold POS) LP LR LF Avg. # of Avg. # # of # of non- # of (%) (%) (%) Time backtrack of dead sentential sentential (ms) states end success success 87.26 86.50 86.88 604 - - - - - 89.78 89.28 89.53 234 - - - - - 76.45 82.00 79.13 122 0 - 867 35 1514 87.78 87.45 87.61 256 0 - 0 117 2299 81.93 85.31 83.59 519 18986 - 386 23 2007 87.79 87.46 87.62 267 574 - 0 45 2371 86.17 87.77 86.96 510 - 226 369 30 2017 88.67 88.79 88.48 457 - 205 0 16 2400 89.</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Sagae, Kenji, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2007. HPSG parsing with shallow dependency constraints. In proc. of ACL 2007, pp. 624-631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In proc. of IWPT-2003.</booktitle>
<contexts>
<context position="4203" citStr="Yamada and Matsumoto, 2003" startWordPosition="609" endWordPosition="612">09 Association for Computational Linguistics 603 segmented constituents are instantiated. This is because values in parse trees can propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (DaumÃ© and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based gramm</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In proc. of IWPT-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>