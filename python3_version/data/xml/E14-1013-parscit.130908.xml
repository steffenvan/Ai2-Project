<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001641">
<title confidence="0.998048">
Modelling the Lexicon in
Unsupervised Part of Speech Induction
</title>
<author confidence="0.999147">
Greg Dubbin Phil Blunsom
</author>
<affiliation confidence="0.999792">
Department of Computer Science Department of Computer Science
University of Oxford University of Oxford
</affiliation>
<address confidence="0.589871">
United Kingdom United Kingdom
</address>
<email confidence="0.967805">
Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.992531" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989524">
Automatically inducing the syntactic part-
of-speech categories for words in text
is a fundamental task in Computational
Linguistics. While the performance of
unsupervised tagging models has been
slowly improving, current state-of-the-art
systems make the obviously incorrect as-
sumption that all tokens of a given word
type must share a single part-of-speech
tag. This one-tag-per-type heuristic coun-
ters the tendency of Hidden Markov
Model based taggers to over generate tags
for a given word type. However, it is
clearly incompatible with basic syntactic
theory. In this paper we extend a state-of-
the-art Pitman-Yor Hidden Markov Model
tagger with an explicit model of the lexi-
con. In doing so we are able to incorpo-
rate a soft bias towards inducing few tags
per type. We develop a particle filter for
drawing samples from the posterior of our
model and present empirical results that
show that our model is competitive with
and faster than the state-of-the-art without
making any unrealistic restrictions.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999690695652174">
Research on the unsupervised induction of part-
of-speech (PoS) tags has the potential to im-
prove both our understanding of the plausibil-
ity of theories of first language acquisition, and
Natural Language Processing applications such
as Speech Recognition and Machine Transla-
tion. While there has been much prior work
on this task (Brown et al., 1992; Clark, 2003;
Christodoulopoulos et al., 2010; Toutanova and
Johnson, 2008; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011), a common thread in
many of these works is that models based on a
Hidden Markov Model (HMM) graphical struc-
ture suffer from a tendency to assign too many
different tags to the tokens of a given word type.
Models which restrict word types to only occur
with a single tag show a significant increase in
performance, even though this restriction is clearly
at odds with the gold standard labeling (Brown et
al., 1992; Clark, 2003; Blunsom and Cohn, 2011).
While the empirically observed expectation for the
number of tags per word type is close to one, there
are many exceptions, e.g. words that occur as both
nouns and verbs (opening, increase, related etc.).
In this paper we extend the Pitman-Yor HMM
tagger (Blunsom and Cohn, 2011) to explicitly in-
clude a model of the lexicon that encodes from
which tags a word type may be generated. For
each word type we draw an ambiguity class which
is the set of tags that it may occur with, captur-
ing the fact that words are often ambiguous be-
tween certain tags (e.g. Noun and Verb), while
rarely between others (e.g. Determiner and Verb).
We extend the type based Sequential Monte Carlo
(SMC) inference algorithm of Dubbin and Blun-
som (2012) to incorporate our model of the lexi-
con, removing the need for the heuristic inference
technique of Blunsom and Cohn (2011).
We start in Section 3 by introducing the origi-
nal PYP-HMM model and our extended model of
the lexicon. Section 4 introduces a Particle Gibbs
sampler for this model, a basic SMC method that
generates samples from the model’s posterior. We
evaluate these algorithms in Section 5, analyzing
their behavior in comparisons to previously pro-
posed state-of-the-art approaches.
</bodyText>
<page confidence="0.984182">
116
</page>
<note confidence="0.997327">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.967887" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999979753424658">
From the early work in the 1990’s, much of the
focus on unsupervised PoS induction has been
on hidden Markov Models (HMM) (Brown et al.,
1992; Kupiec, 1992; Merialdo, 1993). The HMM
has proven to be a powerful model of PoS tag as-
signment. Successful approaches generally build
upon the HMM model by expanding its context
and smoothing the sparse data. Constraints such
as tag dictionaries simplify inference by restricting
the number of tags to explore for each word (Gold-
water and Griffiths, 2007). Ganchev et al. (2010)
used posterior regularization to ensure that word
types have a sparse posterior distribution over tags.
A similar approach constrains inference to only
explore tag assignments such that all tokens of the
same word type are assigned the same tag. These
constraints reduce tag assignment ambiguity while
also providing a bias towards the natural spar-
sity of tag distributions in language (Clark, 2003).
However they do not provide a model based solu-
tion to tag ambiguity.
Recent work encodes similar sparsity infor-
mation with non-parametric priors, relying on
Bayesian inference to achieve strong results with-
out any tag dictionaries or constraints (Goldwater
and Griffiths, 2007; Johnson, 2007; Gao and John-
son, 2008). Liang et al. (2010) propose a type-
based approach to this Bayesian inference similar
to Brown et al. (1992), suggesting that there are
strong dependencies between tokens of the same
word-type. Lee et al. (2010) demonstrate strong
results with a similar model and the introduction
of a one-tag-per-type constraint on inference.
Blunsom and Cohn (2011) extend the Bayesian
inference approach with a hierarchical non-
parametric prior that expands the HMM con-
text to trigrams. However, the hierarchical non-
parametric model adds too many long-range de-
pendencies for the type-based inference proposed
earlier. The model produces state-of-the art re-
sults with a one-tag-per-type constraint, but even
with this constraint the tag assignments must be
roughly inferred from an approximation of the ex-
pectations.
Ambiguity classes representing the set of tags
each word-type can take aid inference by mak-
ing the sparsity between tags and words explicit.
Toutanova and Johnson (2008) showed that mod-
elling ambiguity classes can lead to positive re-
sults with a small tag-dictionary extracted from the
data. By including ambiguity classes in the model,
this approach is able to infer ambiguity classes of
unknown words.
Many improvements in part-of-speech induc-
tion over the last few years have come from the
use of semi-supervised approaches in the form of
projecting PoS constraints across languages with
parallel corpora (Das and Petrov, 2011) or extract-
ing them from the wiktionary (Li et al., 2012).
These semi-supervised methods ultimately rely on
a strong unsupervised model of PoS as their base.
Thus, further improvements in unsupervised mod-
els, especially in modelling tag constrains, should
lead to improvements in semi-supervised part-of-
speech induction.
We find that modelling the lexicon in part-of-
speech inference can lead to more efficient algo-
rithms that match the state-of-the-art unsupervised
performance. We also note that the lexicon model
relies heavily on morphological information, and
suffers without it on languages with flexible word
ordering. These results promise further improve-
ments with more advanced lexicon models.
</bodyText>
<sectionHeader confidence="0.930669" genericHeader="method">
3 The Pitman-Yor Lexicon Hidden
</sectionHeader>
<subsectionHeader confidence="0.644127">
Markov Model
</subsectionHeader>
<bodyText confidence="0.999965214285714">
This article proposes enhancing the standard Hid-
den Markov Model (HMM) by explicitly incorpo-
rating a model of the lexicon that consists of word
types and their associated tag ambiguity classes.
The ambiguity class of a word type is the set of
possible lexical categories to which tokens of that
type can be assigned. In this work we aim to
learn the ambiguity classes unsupervised rather
than have them specified in a tag dictionary.
The Lexicon HMM (Lex-HMM) extends the
Pitman-Yor HMM (PYP-HMM) described by
Blunsom and Cohn (2011). When the ambiguity
class of all of the word types in the lexicon is the
complete tagset, the two models are the same.
</bodyText>
<subsectionHeader confidence="0.998878">
3.1 PYP-HMM
</subsectionHeader>
<bodyText confidence="0.999926142857143">
The base of the model applies a hierarchical
Pitman-Yor process (PYP) prior to a trigram hid-
den Markov model to jointly model the distribu-
tion of a sequence of latent word tags, t, and
word tokens, w. The joint probability defined
by the transition, Pθ(tl|tn−1, tn−2), and emission,
Pθ(wn|tn), distributions of a trigram HMM is
</bodyText>
<equation confidence="0.7012365">
Pθ(t, w) = N+1H Pθ(tl|tn−1, tn−2)Pθ(wn|tn)
n=1
</equation>
<page confidence="0.990273">
117
</page>
<bodyText confidence="0.999947375">
where N = |t |= |w |and the special tag $
is added to denote the sentence boundaries. The
model defines a generative process in which the
tags are selected from a transition distribution,
tl|tl−1, tl−2, T, determined by the two previous
tags in their history, and the word tokens are se-
lected from the emission distribution, wl|tl, E, of
the latest tag.
</bodyText>
<equation confidence="0.938226">
tn|tn−1, tn−2,T — Ttn−1rtn−2
wn|tn, E — Etn
</equation>
<bodyText confidence="0.998670277777778">
The PYP-HMM draws the above multinomial dis-
tributions from a hierarchical Pitman-Yor Process
prior. The Pitman-Yor prior defines a smooth back
off probability from more complex to less com-
plex transition and emission distributions. In the
PYP-HMM trigram model, the transition distri-
butions form a hierarchy with trigram transition
distributions drawn from a PYP with the bigram
transitions as their base distribution, and the bi-
gram transitions similarly backing off to the uni-
gram transitions. The hierarchical prior can be in-
tuitively understood to smooth the trigram transi-
tion distributions with bigram and unigram distri-
butions in a similar manner to an ngram language
model (Teh, 2006). This back-off structure greatly
reduces sparsity in the trigram distributions and is
achieved by chaining together the PYPs through
their base distributions:
</bodyText>
<construct confidence="0.82733425">
Tij|aT , bT , Bi — PYP(aT , bT , Bi)
Bi|aB, bB, U — PYP(aB, bB, U)
U|aU, bU — PYP(aU, bU, Uniform).
Ei|aE, bE, C — PYP(aE, bE, Ci),
</construct>
<bodyText confidence="0.999270785714286">
where Tij, Bi, and U are trigram, bigram, and un-
igram transition distributions respectively, and Ci
is either a uniform distribution (PYP-HMM) or a
bigram character language model distribution to
model word morphology (PYP-HMM+LM).
Sampling from the posterior of the hierarchi-
cal PYP is calculated with a variant of the Chi-
nese Restaurant Process (CRP) called the Chinese
Restaurant Franchise (CRF) (Teh, 2006; Goldwa-
ter et al., 2006). In the CRP analogy, each latent
variable (tag) in a sequence is represented by a
customer entering a restaurant and sitting at one of
an infinite number of tables. A customer chooses
to sit at a table in a restaurant according to the
</bodyText>
<equation confidence="0.946450333333333">
probability
1 &lt; k
P(zn = k |z1:n−1) = 1 n−1+b &lt; K−
llx−a+b k = K− + 1
n−1+b
(1)
</equation>
<bodyText confidence="0.999960148148148">
where zn is the index of the table chosen by the
nth customer to the restaurant, z1:n−1 is the seat-
ing arrangement of the previous n — 1 customers
to enter, c−k is the count of the customers at table
k, and K− is the total number of tables chosen by
the previous n — 1 customers. All customers at a
table share the same dish, representing the value
assigned to the latent variables. When customers
sit at an empty table, a new dish is assigned to that
table according to the base distribution of the PYP.
To expand the CRP analogy to the CRF for hierar-
chical PYPs, when a customer sits at a new table,
a new customer enters the restaurant of the PYP of
the base distribution.
Blunsom and Cohn (2011) explored two Gibbs
sampling methods for inference with the PYP-
HMM model. The first individually samples tag
assignments for each token. The second employs
a tactic shown to be effective by earlier works by
constraining inference to only one tag per word
type (PYP-1HMM). However marginalizing over
all possible table assignments for more than a sin-
gle tag is intractable. Blunsom and Cohn (2011)
approximates the PYP-1HMM tag posteriors for a
particular sample according to heuristic fractional
table counts. This approximation is shown to be
particularly inaccurate for values of a close to one.
</bodyText>
<subsectionHeader confidence="0.999285">
3.2 The Lexicon HMM
</subsectionHeader>
<bodyText confidence="0.9999585">
We define the lexicon to be the set of all word
types (W) and a function (L) which maps each
word type (Wi E W) to an element in the power
set of possible tags T,
</bodyText>
<equation confidence="0.939102">
L : W —* P(T).
</equation>
<bodyText confidence="0.998829875">
The Lexicon HMM (Lex-HMM) generates the
lexicon with all of the word types and their ambi-
guity classes before generating the standard HMM
parameters. The set of tags associated with each
word type is referred to as its ambiguity class
si C T. The ambiguity classes are generated from
a multinomial distribution with a sparse, Pitman-
Yor Process prior,
</bodyText>
<equation confidence="0.571750333333333">
si|S — S
S|aS, bS — PY P(aS, bS, G)
c−k −a
</equation>
<page confidence="0.733382">
118
</page>
<figure confidence="0.999556708333333">
Frequency
...
Log-Log Ambiguity Class Frequency vs. Rank
105
Gold Lexicon
Predicted Lexicon
104
103
102
101
100
100 101 102 103
Rank
U
Bj Tij
t1
t2
t3
w3
Ej
w1
w2
S
si Wi
</figure>
<figureCaption confidence="0.99989">
Figure 1: Lex-HMM Structure: The graphical
</figureCaption>
<bodyText confidence="0.980802070175439">
structure of the Lex-HMM model. In addition to
the trigram transition (Tij) and emission (Ej), the
model includes an ambiguity class (si) for each
word type (Wi) drawn from a distribution 5 with
a PYP prior.
where 5 is the multinomial distribution over all
possible ambiguity classes. The base distribution
of the PYP, G, chooses the size of the ambiguity
class according to a geometric distribution (nor-
malized so that the size of the class is at most the
number of tags ITI). G assigns uniform probabil-
ity to all classes of the same size. A plate diagram
for this model is shown in Figure 1.
This model represents the observation that there
are relatively few distinct ambiguity classes over
all of the word types in a corpus. For example, the
full Penn-Treebank Wall Street Journal (WSJ) cor-
pus with 45 possible tags and 49,206 word types
has only 343 ambiguity classes. Figure 2 shows
that ambiguity classes in the WSJ have a power-
law distribution. Furthermore, these classes are
generally small; the average ambiguity class in the
WSJ corpus has 2.94 tags. The PYP prior favors
power-law distributions and the modified geomet-
ric base distribution favors smaller class sizes.
Once the lexicon is generated, the standard
HMM parameters can be generated as described
in section 3.1. The base emission probabilities C
are constrained to fit the generated lexicon. The
standard Lex-HMM model emission probabilities
for tag ti are uniform over all word types with ti
in their ambiguity class. The character language
model presents a challenge because it is non-trivial
to renormalise over words with ti in their ambigu-
ity class. In this case word types without ti in their
Figure 2: Ambiguity Class Distribution: Log-
log plot of ambiguity class frequency over rank
for the Penn-Treebank WSJ Gold Standard lexicon
highlighting a Zipfian distribution and the ambigu-
ity of classes extracted from the predicted tags.
ambiguity class are assigned an emission probabil-
ity of 0 and the model is left deficient.
Neither of the samplers proposed by Blunsom
and Cohn (2011) and briefly described in section
3.1 are well suited to inference with the lexicon.
Local Gibbs sampling of individual token-tag as-
signments would be very unlikely to explore a
range of confusion classes, while the type based
approximate sample relies on a one-tag-per-type
restriction. Thus in the next section we extend the
Particle Filtering solution presented in Dubbin and
Blunsom (2012) to the problem of simultaneous
resampling the ambiguity class as well as the tags
for all tokens of a given type. This sampler pro-
vides both a more attractive inference algorithm
for the original PYP-HMM and one adaptable to
our Lex-HMM.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999759384615385">
To perform inference with both the lexicon and
the tag assignments, we block sample the ambi-
guity class assignment as well as all tag assign-
ments for tokens of the same word type. It would
be intractable to exactly calculate the probabili-
ties to sample these blocks. Particle filters are an
example of a Sequential Monte Carlo technique
which generates unbiased samples from a distribu-
tion without summing over the intractable number
of possibilities.
The particle filter samples multiple independent
sequences of ambiguity classes and tag assign-
ments. Each sequence of samples, called a parti-
</bodyText>
<page confidence="0.995953">
119
</page>
<bodyText confidence="0.990564673469388">
cle, is generated incrementally. For each particle,
the particle filter first samples an ambiguity class,
and then samples each tag assignment in sequence
based only on the previous samples in the parti-
cle. The value of the next variable in a sequence
is sampled from a proposal distribution based only
on the earlier values in the sequence. Each particle
is assigned an importance weight such that a par-
ticle sampled proportional to its weight represents
an unbiased sample of the true distribution.
Each particle represents a specific sampling of
an ambiguity class, tag sequence, tW,p
1:n , and the
count deltas, zW,p
1:n . The term tW,p
1:n denotes the se-
quence of n tags generated for word-type W and
stored as part of particle p E [1, P]. The count
deltas store the differences in the seating arrange-
ment neccessary to calculate the posterior proba-
bilities according to the Chinese restaurant fran-
chise described in section 3.1. The table counts
from each particle are the only data necessary to
calculate the probabilities described in equation
(1).
The ambiguity class for a particle is proposed
by uniformly sampling one tag from the tagset to
add to or remove from the previous iteration’s am-
biguity class with the additional possibility of us-
ing the same ambiguity class. The particle weights
are then set to
P(sW,p|S−W )
Ht∈sW,p(et + 1)#(Et) Ht∈T−sW,p(et)#(Et)
where P(sW,p|S−W) is the probability of the am-
biguity class proposed for particle p for word type
W given the ambiguity classes for the rest of the
vocabulary, et is the number of word types with t
in their ambiguity class, and #(Et) is the number
of tables in the CRP for the emission distribution
of tag t. The last two terms of the equation cor-
rect for the difference in the base probabilities of
the words that have already been sampled with a
different lexicon.
At each token occurrence n, the next tag assign-
ment, tW,p
n for each particle p E [1, P] is deter-
mined by the seating decisions zW,p
n , which are
made according the proposal distribution:
</bodyText>
<equation confidence="0.952361928571429">
qW,p
n (zW,p
n |zW,p
1:n−1, z−W ) OC
P
(zW,p
n |c−2, c−1, zW,p
1:n−1, z−W)
XP(c+1|C 1 zWp ZW,p z−W)
n n n l:n−1,
( +2  |W,p +1 W,p −W )
XP Cn zn ,cn ,z1:n−1,z
XP(wWn |zW,p
n ,znn7! 1,z−W)
</equation>
<bodyText confidence="0.998887967741936">
In this case, c±k
n represents a tag in the context of
site tW n offset by k, while zW,p
1:n−1 and z−W rep-
resent the table counts from the seating decisions
previously chosen by particle p and the values at
all of the sites where a word token of type W
does not appear, respectively. This proposal dis-
tribution ignores changes to the seating arrange-
ment between the three transitions involving the
site n. The specific tag assignement, tW, pn, is
completely determined by the seating decisions
sampled according to this proposal distribution.
Once all of the particles have been sampled, one
of them is sampled with probability proportional
to its weight. This final sample is a sample from
the target distribution.
As the Particle Filter is embedded in a Gibbs
sampler which cycles over all word types this al-
gorithm is an instance of Particle Gibbs. Andrieu
et al. (2010) shows that to ensure the samples gen-
erated by SMC for a Gibbs sampler have the tar-
get distribution as the invariant density, the par-
ticle filter must be modified to perform a condi-
tional SMC update. This means that the particle
filter guarantees that one of the final particles is as-
signed the same values as the previous Gibbs iter-
ation. Therefore, a special 0th particle is automati-
cally assigned the value from the prior iteration of
the Gibbs sampler at each site n, though the pro-
posal probability qWn (tW,c
</bodyText>
<equation confidence="0.640138">
n |tW,p
1:n−1, zW,p
</equation>
<bodyText confidence="0.9786744">
1:n−1) still has
to be calculated to update the weight ωW,p
n prop-
erly. This ensures that the sampler has a chance of
reverting to the prior iteration’s sequence.
</bodyText>
<sectionHeader confidence="0.995915" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999990705882353">
We provide an empirical evaluation of our pro-
posed Lex-HMM in terms of the accuracy of
the taggings learned according to the most pop-
ular metric, and the distributions over ambiguity
classes. Our experimental evaluation considers the
impact of our improved Particle Gibbs inference
algorithm both for the original PYP-HMM and
when used for inference in our extended model.
We intend to learn whether the lexicon model
can match or exceed the performance of the other
models despite focusing on only a subset of the
possible tags each iteration. We hypothesize that
an accurate lexicon model and the sparsity it in-
duces over the number of tags per word-type will
improve the performance over the standard PYP-
HMM model while also decreasing training time.
Furthermore, our lexicon model is novel, and its
</bodyText>
<page confidence="0.756795">
.
120
</page>
<table confidence="0.9996081">
Sampler M-1 Accuracy Time (h)
Meta-Model (CGS10) 76.1 —
MEMM (BBDK10) 75.5 ∼40*
Lex-HMM 71.1 7.9
Type PYP-HMM 70.1 401.2
Local PYP-HMM 70.2 8.6
PYP-1HMM 75.6 20.6
Lex-HMM+LM 77.5 16.9
Type PYP-HMM+LM 73.5 446.0
PYP-1HMM+LM 77.5 34.9
</table>
<tableCaption confidence="0.999554">
Table 1: M-1 Accuracy on the WSJ Corpus:
</tableCaption>
<bodyText confidence="0.997079625">
Comparison of the accuracy of each of the sam-
plers with and without the language model emis-
sion prior on the English WSJ Corpus. The second
column reports run time in hours where available*.
Note the Lex-HMM+LM model matches the PYP-
1HMM+LM approximation despite finishing in
half the time. The abbreviations in parentheses
indicate that the results were reported in CGS10
(Christodoulopoulos et al., 2010) and BBDK10
(Berg-Kirkpatrick et al., 2010) *CGS10 reports
that the MEMM model takes approximately 40
hours on 16 cores.
accuracy in representing ambiguity classes is an
important aspect of its performance. The model
focuses inference on the most likely tag choices,
represented by ambiguity classes.
</bodyText>
<subsectionHeader confidence="0.983478">
5.1 Unsupervised Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999990402985075">
The most popular evaluation for unsupervised
part-of-speech taggers is to induce a tagging for
a corpus and compare the induced tags to those
annotated by a linguist. As the induced tags are
simply integer labels, we must employ a map-
ping between these and the more meaningful syn-
tactic categories of the gold standard. We re-
port results using the many-to-one (M-1) met-
ric considered most intuitive by the evaluation of
Christodoulopoulos et al. (2010). M-1 measures
the accuracy of the model after mapping each pre-
dicted class to its most frequent corresponding tag.
While Christodoulopoulos et al. (2010) found V-
measure to be more stable over the number of
parts-of-speech, this effect doesn’t appear when
the number of tags is constant, as in our case. For
experiments on English, we report results on the
entire Penn. Treebank (Marcus et al., 1993). For
other languages we use the corpora made avail-
able for the CoNLL-X Shared Task (Buchholz and
Marsi, 2006). All Lex-HMM results are reported
with 10 particles as no significant improvement
was found with 50 particles.
Table 1 compares the M-1 accuracies of both
the PYP-HMM and the Lex-HMM models on the
Penn. Treebank Wall Street Journal corpus. Blun-
som and Cohn (2011) found that the Local PYP-
HMM+LM sampler is unable to mix, achieving
accuracy below 50%, therefore it has been left
out of this analysis. The Lex-HMM+LM model
achieves the same accuracy as the state-of-the-
art PYP-1HMM+LM approximation. The Lex-
HMM+LM’s focus on only the most likely tags for
each word type allows it to finish training in half
the time as the PYP-1HMM+LM approximation
without any artificial restrictions on the number of
tags per type. This contrasts with other approaches
that eliminate the constraint at a much greater cost,
e.g. the Type PYP-HMM, the MEMM, and the
Meta-Model 1
The left side of table 2 compares the M-1 accu-
racies of the Lex-HMM model to the PYP-HMM
model. These models both ignore word morphol-
ogy and rely on word order. The 1HMM approxi-
mation achieves the highest average accuracy. The
Lex-HMM model matches or surpasses the type-
based PYP-HMM approach in six languages while
running much faster due to the particle filter con-
sidering a smaller set of parts-of-speech for each
particle. However, in the absence of morpho-
logical information, the Lex-HMM model has a
similar average accuracy to the local and type-
based PYP-HMM samplers. The especially low
performance on Hungarian, a language with free
word ordering and strong morphology, suggests
that the Lex-HMM model struggles to find ambi-
guity classes without morphology. The Lex-HMM
model has a higher average accuracy than the type-
based or local PYP-HMM samplers when Hungar-
ian is ignored.
The right side of table 2 compares the M-1 ac-
curacies of the Lex-HMM+LM model to the PYP-
HMM+LM. The language model leads to consis-
tently improved performance for each of the sam-
plers excepting the token sampler, which is un-
able to mix properly with the additional complex-
ity. The accuracies achieved by the 1HMM+LM
</bodyText>
<footnote confidence="0.9925558">
1While were unable to get an estimate on the runtime of
the Meta-Model, it uses a system similar to the feature-based
system of the MEMM with an additional feature derived from
the proposed class from the brown model. Therefore, it is
likely that this model has a similar runtime.
</footnote>
<page confidence="0.98632">
121
</page>
<table confidence="0.99996725">
Language Lex-HMM PYP-HMM Local 1HMM Lex-HMM+LM PYP-HMM+LM 1HMM+LM
WSJ 71.1 70.1 70.2 75.6 77.5 73.5 77.5
Arabic 57.2 57.6 56.2 61.9 62.1 62.7 62.0
Bulgarian 67.2 67.8 67.6 71.4 72.7 72.1 76.2
Czech 61.3 61.6 64.5 65.4 68.2 67.4 67.9
Danish 68.6 70.3 69.1 70.6 74.7 73.1 74.6
Dutch 70.3 71.6 64.1 73.2 71.7 71.8 72.9
Hungarian 57.9 61.8 64.8 69.6 64.4 69.9 73.2
Portuguese 69.5 71.1 68.1 72.0 76.3 73.9 77.1
Spanish 73.2 69.1 68.5 74.7 80.0 75.2 78.8
Swedish 66.3 63.5 67.6 67.2 70.4 67.6 68.6
Average 66.3 (67.2) 66.5 (67.0) 66.1 (66.2) 70.2 (70.3) 71.8 (72.6) 70.7 (70.8) 72.9 (72.9)
</table>
<tableCaption confidence="0.982467">
Table 2: M-1 Accuracy of Lex-HMM and PYP-HMM models: Comparison of M-1 accuracy for the
</tableCaption>
<bodyText confidence="0.994420975609756">
lexicon based model (Lex-HMM) and the PYP-HMM model on several languages. The Lex-HMM and
PYP-HMM columns indicate the results of word type based particle filtering with 10 and 100 particles,
respectively, while the Local and 1HMM columns use the token based sampler and the 1HMM approxi-
mation described by Blunsom and Cohn (2011). The token based sampler was run for 500 iterations and
the other samplers for 200. The percentages in brakets represent the average accuracy over all languages
except for Hungarian.
sampler represent the previous state-of-the-art.
These results show that the Lex-HMM+LM model
achieves state-of-the-art M-1 accuracies on sev-
eral datasets, including the English WSJ. The Lex-
HMM+LM model performs nearly as well as, and
often better than, the 1HMM+LM sampler without
any restrictions on tag assignments.
The drastic improvement in the performance
of the Lex-HMM model reinforces our hypothe-
sis that morphology is critical to the inference of
ambiguity classes. Without the language model
representing word morphology, the distinction be-
tween ambiguity classes is too ambiguous. This
leads the sampler to infer an excess of poor am-
biguity classes. For example, the tag assignments
from the Lex-PYP model on the WSJ dataset con-
sist of 660 distinct ambiguity classes, while the
Lex-PYP+LM tag assignments only have 182 dis-
tinct ambiguity classes.
Note that while the Lex-HMM and Lex-
HMM+LM samplers do not have any restrictions
on inference, they do not sacrifice time. The ad-
ditional samples generated by the particle filter
are mitigated by limiting the number of tags each
particle must consider. In practice, this results in
the Lex-HMM samplers with 10 particles running
in half time as the 1HMM samplers. The Lex-
HMM+LM sampler with 10 particles took 16.9
hours, while the 1HMM+LM sampler required
34.9 hours. Furthermore, the run time evaluation
does not take advantage of the inherent distributed
nature of particle filters. Each of the particles can
be sampled completely independentally from the
others, making it trivial to run each on a seperate
core.
</bodyText>
<subsectionHeader confidence="0.999842">
5.2 Lexicon Analysis
</subsectionHeader>
<bodyText confidence="0.999980896551724">
While section 5.1 demonstrates that the Lex-
HMM+LM sampler performs similarly to the
more restricted 1HMM+LM, we also seek to eval-
uate the accuracy of the lexicon model itself. We
compare the ambiguity classes extracted from the
gold standard and predicted tag assignments of the
WSJ corpus. We also explore the relationship be-
tween the actual and sampled ambiguity classes.
The solid curve in figure 2 shows the distribu-
tion of the number of word types assigned to each
ambiguity set extracted from the gold standard tag
assignments from the Penn Treebank Wall Street
Journal corpus. The straight line strongly indi-
cates that ambiguity classes follow a Zipfian dis-
tribution. Figure 2 also graphs the distribution of
the ambiguity classes extracted from the best tag-
assignment prediction from the model. The pre-
dicted graph has a similar shape to the gold stan-
dard but represents half as many distinct ambigu-
ity classes - 182 versus 343.
For a qualitative analysis of the generated lex-
icon, table 3 lists frequent ambiguity classes and
the most common words assigned to them. The 14
most frequent ambiguity classes contain only one
tag each, the top half of table 3 shows the 5 most
frequent. One third of the word-types in the first
five rows of the table are exactly matched with the
ambiguity classes from the gold standard. Most of
the remaining words in those rows are assigned to
</bodyText>
<page confidence="0.991619">
122
</page>
<table confidence="0.989990583333333">
Rank Gold Rank Tags Top Word Types
1 1 NNP Mr., Corp. (1), Inc. (.99), Co. (1), Exchange (.99)
2 2 NN % (1), company, stock (.99), -RRB- (0), years (0)
3 3 JJ new, other, first (.9), most (0), major (1)
4 5 NNS companies, prices (1), quarter (0), week (0), investors
5 4 CD $ (0), million (1), billion, 31, # (0)
15 303 NN, CD yen (.47, 0), dollar (1, 0), 150 (0, 1), 29 (0, 1), 33 (0, 1)
16 17 VB, NN plan (.03, .9), offer (.2, .74), issues (0, 0), increase (.34, .66), end (.18, .81)
17 115 DT, NNP As (0, 0), One (0,.01), First (0,.82), Big (0,.91), On (0,.01)
18 11 NN, JJ market (.99, 0), U.S. (0, 0), bank (1, 0), cash (.98, 0), high (.06, .9)
20 22 VBN, JJ estimated (.58, .15), lost (.43, .03), failed (.35, .04), related (.74, .23), re-
duced (.57, .12)
</table>
<tableCaption confidence="0.998668">
Table 3: Selection of Predicted Ambiguity Classes: Common ambiguity classes from the predicted
</tableCaption>
<bodyText confidence="0.994501">
part-of-speech assignments from the WSJ data set, and the five most common word types associated
with each ambiguity class. The sets are ranked according to the number of word types associated to
them. Words in bold are matched to exactly the same ambiguity set in the gold standard. The lower
five ambiguity classes are the most common with more than one part-of-speech. Numbers in parentheses
represent the proportion of tokens of that type assigned to each tag in the gold standard for that ambiguity
class.
a class representing almost all of the words’ occur-
rences in the gold standard, e.g., ‘Corp.’ is an NNP
in 1514 out of 1521 occurrences. Some words are
assigned to classes with similar parts of speech,
e.g. {NNS} rather than {NN} for week.
The lower half of table 3 shows the most fre-
quent ambiguity classes with more than a sin-
gle tag. The words assigned to the {NN,CD},
{DT,NNP}, and {NN,JJ} classes are not them-
selves ambiguous. Rather words that are unam-
biguously one of the two tags are often assigned
to an ambiguity class with both. The most com-
mon types in the {NN, CD} set are unambiguously
either NN or CD. In many cases the words are
merged into broader ambiguity classes because the
Lex-HMM+LM uses the language model to model
the morphology of words over individual parts-
of-speech, rather than entire ambiguity classes.
Therefore, a word-type is likely to be assigned
a given ambiguity class as long as at least one
part-of-speech in that ambiguity class is associ-
ated with morphologically similar words. These
results suggest modifying the Lex-HMM+LM to
model word morphology over ambiguity classes
rather than parts-of-speech.
The {VB,NN} and {VBN,JJ} are representative
of true ambiguity classes. Occurrences of words in
these classes are likely to be either of the possible
parts-of-speech. These results show that the Lex-
HMM is modelling ambiguity classes as intended.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997534375">
This paper described an extension to the PYP-
HMM part-of-speech model that incorporates a
sparse prior on the lexicon and an SMC based in-
ference algorithm. These contributions provide a
more plausible model of part-of-speech induction
which models the true ambiguity of tag to type as-
signments without the loss of performance of ear-
lier HMM models. Our empirical evaluation indi-
cates that this model is able to meet or exceed the
performance of the previous state-of-the-art across
a range of language families.
In addition to the promising empirical results,
our analysis indicates that the model learns ambi-
guity classes that are often quite similar to those
in the gold standard. We believe that further im-
provements in both the structure of the lexicon
prior and the inference algorithm will lead to addi-
tional performance gains. For example, the model
could be improved by better modelling the rela-
tionship between a word’s morphology and its am-
biguity class. We intend to apply our model to
recent semi-supervised approaches which induce
partial tag dictionaries from parallel language data
(Das and Petrov, 2011) or the Wiktionary (Li et
al., 2012). We hypothesize that the additional data
should improve the modelled lexicon and conse-
quently improve tag assignments.
The Lex-HMM models ambiguity classes to fo-
cus the sampler on the most likely parts-of-speech
for a given word-type. In doing so, it matches or
improves on the accuracy of other models while
running much faster.
</bodyText>
<page confidence="0.998529">
123
</page>
<sectionHeader confidence="0.989513" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908710280374">
Christophe Andrieu, Arnaud Doucet, and Roman
Holenstein. 2010. Particle markov chain monte
carlo methods. Journal Of The Royal Statistical So-
ciety Series B, 72(3):269–342.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582–590, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal Pitman-Yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
865–874, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18:467–479, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06,
pages 149–164, Morristown, NJ, USA. Association
for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575–584, Cambridge, MA, October. Association for
Computational Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting
of the European Association for Computational Lin-
guistics (EACL), pages 59–66.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11, pages 600–609, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gregory Dubbin and Phil Blunsom. 2012. Unsuper-
vised bayesian part of speech inference with particle
gibbs. In Peter A. Flach, Tijl De Bie, and Nello Cris-
tianini, editors, ECML/PKDD (1), volume 7523 of
Lecture Notes in Computer Science, pages 760–773.
Springer.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. Journal of Ma-
chine Learning Research, 99:2001–2049, August.
Jianfeng Gao and Mark Johnson. 2008. A compar-
ison of bayesian estimators for unsupervised hid-
den markov model pos taggers. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’08, pages 344–352,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proc. of the 45th Annual Meeting of
the ACL (ACL-2007), pages 744–751, Prague, Czech
Republic, June.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neu-
ral Information Processing Systems 18, pages 459–
466. MIT Press, Cambridge, MA.
Mark Johnson. 2007. Why doesnt EM find good
HMM POS-taggers? In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 296–305, Prague,
Czech Republic.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech
and Language, 6:225–242.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 853–861, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1389–1398, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-
based MCMC. In North American Association for
Computational Linguistics (NAACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Bernard Merialdo. 1993. Tagging english text with
a probabilistic model. Computational Linguistics,
20:155–171.
</reference>
<page confidence="0.982967">
124
</page>
<reference confidence="0.998414846153846">
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 985–992, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neu-
ral Information Processing Systems 20, pages 1521–
1528. MIT Press, Cambridge, MA.
</reference>
<page confidence="0.998498">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.268804">
<title confidence="0.999285">Modelling the Lexicon Unsupervised Part of Speech Induction</title>
<author confidence="0.999923">Greg Dubbin Phil Blunsom</author>
<affiliation confidence="0.999709">Department of Computer Science Department of Computer Science University of Oxford University of Oxford</affiliation>
<note confidence="0.31075">United Kingdom United Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk</note>
<abstract confidence="0.9996985">Automatically inducing the syntactic partof-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag. This one-tag-per-type heuristic counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type. However, it is clearly incompatible with basic syntactic theory. In this paper we extend a state-ofthe-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christophe Andrieu</author>
<author>Arnaud Doucet</author>
<author>Roman Holenstein</author>
</authors>
<title>Particle markov chain monte carlo methods.</title>
<date>2010</date>
<journal>Journal Of The Royal Statistical Society Series B,</journal>
<volume>72</volume>
<issue>3</issue>
<contexts>
<context position="18789" citStr="Andrieu et al. (2010)" startWordPosition="3129" endWordPosition="3132"> does not appear, respectively. This proposal distribution ignores changes to the seating arrangement between the three transitions involving the site n. The specific tag assignement, tW, pn, is completely determined by the seating decisions sampled according to this proposal distribution. Once all of the particles have been sampled, one of them is sampled with probability proportional to its weight. This final sample is a sample from the target distribution. As the Particle Filter is embedded in a Gibbs sampler which cycles over all word types this algorithm is an instance of Particle Gibbs. Andrieu et al. (2010) shows that to ensure the samples generated by SMC for a Gibbs sampler have the target distribution as the invariant density, the particle filter must be modified to perform a conditional SMC update. This means that the particle filter guarantees that one of the final particles is assigned the same values as the previous Gibbs iteration. Therefore, a special 0th particle is automatically assigned the value from the prior iteration of the Gibbs sampler at each site n, though the proposal probability qWn (tW,c n |tW,p 1:n−1, zW,p 1:n−1) still has to be calculated to update the weight ωW,p n prop</context>
</contexts>
<marker>Andrieu, Doucet, Holenstein, 2010</marker>
<rawString>Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. 2010. Particle markov chain monte carlo methods. Journal Of The Royal Statistical Society Series B, 72(3):269–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical Pitman-Yor process hmm for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>865--874</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1789" citStr="Blunsom and Cohn, 2011" startWordPosition="264" endWordPosition="267">s that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close to one, there are many exceptions, e.g. words that occur </context>
<context position="3097" citStr="Blunsom and Cohn (2011)" startWordPosition="494" endWordPosition="497">e Pitman-Yor HMM tagger (Blunsom and Cohn, 2011) to explicitly include a model of the lexicon that encodes from which tags a word type may be generated. For each word type we draw an ambiguity class which is the set of tags that it may occur with, capturing the fact that words are often ambiguous between certain tags (e.g. Noun and Verb), while rarely between others (e.g. Determiner and Verb). We extend the type based Sequential Monte Carlo (SMC) inference algorithm of Dubbin and Blunsom (2012) to incorporate our model of the lexicon, removing the need for the heuristic inference technique of Blunsom and Cohn (2011). We start in Section 3 by introducing the original PYP-HMM model and our extended model of the lexicon. Section 4 introduces a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Background </context>
<context position="5286" citStr="Blunsom and Cohn (2011)" startWordPosition="836" endWordPosition="839">olution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produces state-of-the art results with a one-tag-per-type constraint, but even with this constraint the tag assignments must be roughly inferred from an approximation of the expectations. Ambiguity classes representing the set of tags each word-type can take aid inference by making the sparsity between tags and words explicit. Toutanov</context>
<context position="7648" citStr="Blunsom and Cohn (2011)" startWordPosition="1202" endWordPosition="1205">rovements with more advanced lexicon models. 3 The Pitman-Yor Lexicon Hidden Markov Model This article proposes enhancing the standard Hidden Markov Model (HMM) by explicitly incorporating a model of the lexicon that consists of word types and their associated tag ambiguity classes. The ambiguity class of a word type is the set of possible lexical categories to which tokens of that type can be assigned. In this work we aim to learn the ambiguity classes unsupervised rather than have them specified in a tag dictionary. The Lexicon HMM (Lex-HMM) extends the Pitman-Yor HMM (PYP-HMM) described by Blunsom and Cohn (2011). When the ambiguity class of all of the word types in the lexicon is the complete tagset, the two models are the same. 3.1 PYP-HMM The base of the model applies a hierarchical Pitman-Yor process (PYP) prior to a trigram hidden Markov model to jointly model the distribution of a sequence of latent word tags, t, and word tokens, w. The joint probability defined by the transition, Pθ(tl|tn−1, tn−2), and emission, Pθ(wn|tn), distributions of a trigram HMM is Pθ(t, w) = N+1H Pθ(tl|tn−1, tn−2)Pθ(wn|tn) n=1 117 where N = |t |= |w |and the special tag $ is added to denote the sentence boundaries. The</context>
<context position="10990" citStr="Blunsom and Cohn (2011)" startWordPosition="1787" endWordPosition="1790">ant, z1:n−1 is the seating arrangement of the previous n — 1 customers to enter, c−k is the count of the customers at table k, and K− is the total number of tables chosen by the previous n — 1 customers. All customers at a table share the same dish, representing the value assigned to the latent variables. When customers sit at an empty table, a new dish is assigned to that table according to the base distribution of the PYP. To expand the CRP analogy to the CRF for hierarchical PYPs, when a customer sits at a new table, a new customer enters the restaurant of the PYP of the base distribution. Blunsom and Cohn (2011) explored two Gibbs sampling methods for inference with the PYPHMM model. The first individually samples tag assignments for each token. The second employs a tactic shown to be effective by earlier works by constraining inference to only one tag per word type (PYP-1HMM). However marginalizing over all possible table assignments for more than a single tag is intractable. Blunsom and Cohn (2011) approximates the PYP-1HMM tag posteriors for a particular sample according to heuristic fractional table counts. This approximation is shown to be particularly inaccurate for values of a close to one. 3.</context>
<context position="14459" citStr="Blunsom and Cohn (2011)" startWordPosition="2383" endWordPosition="2386">over all word types with ti in their ambiguity class. The character language model presents a challenge because it is non-trivial to renormalise over words with ti in their ambiguity class. In this case word types without ti in their Figure 2: Ambiguity Class Distribution: Loglog plot of ambiguity class frequency over rank for the Penn-Treebank WSJ Gold Standard lexicon highlighting a Zipfian distribution and the ambiguity of classes extracted from the predicted tags. ambiguity class are assigned an emission probability of 0 and the model is left deficient. Neither of the samplers proposed by Blunsom and Cohn (2011) and briefly described in section 3.1 are well suited to inference with the lexicon. Local Gibbs sampling of individual token-tag assignments would be very unlikely to explore a range of confusion classes, while the type based approximate sample relies on a one-tag-per-type restriction. Thus in the next section we extend the Particle Filtering solution presented in Dubbin and Blunsom (2012) to the problem of simultaneous resampling the ambiguity class as well as the tags for all tokens of a given type. This sampler provides both a more attractive inference algorithm for the original PYP-HMM an</context>
<context position="22567" citStr="Blunsom and Cohn (2011)" startWordPosition="3755" endWordPosition="3759">10) found Vmeasure to be more stable over the number of parts-of-speech, this effect doesn’t appear when the number of tags is constant, as in our case. For experiments on English, we report results on the entire Penn. Treebank (Marcus et al., 1993). For other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). All Lex-HMM results are reported with 10 particles as no significant improvement was found with 50 particles. Table 1 compares the M-1 accuracies of both the PYP-HMM and the Lex-HMM models on the Penn. Treebank Wall Street Journal corpus. Blunsom and Cohn (2011) found that the Local PYPHMM+LM sampler is unable to mix, achieving accuracy below 50%, therefore it has been left out of this analysis. The Lex-HMM+LM model achieves the same accuracy as the state-of-theart PYP-1HMM+LM approximation. The LexHMM+LM’s focus on only the most likely tags for each word type allows it to finish training in half the time as the PYP-1HMM+LM approximation without any artificial restrictions on the number of tags per type. This contrasts with other approaches that eliminate the constraint at a much greater cost, e.g. the Type PYP-HMM, the MEMM, and the Meta-Model 1 The</context>
<context position="25653" citStr="Blunsom and Cohn (2011)" startWordPosition="4278" endWordPosition="4281">1 68.1 72.0 76.3 73.9 77.1 Spanish 73.2 69.1 68.5 74.7 80.0 75.2 78.8 Swedish 66.3 63.5 67.6 67.2 70.4 67.6 68.6 Average 66.3 (67.2) 66.5 (67.0) 66.1 (66.2) 70.2 (70.3) 71.8 (72.6) 70.7 (70.8) 72.9 (72.9) Table 2: M-1 Accuracy of Lex-HMM and PYP-HMM models: Comparison of M-1 accuracy for the lexicon based model (Lex-HMM) and the PYP-HMM model on several languages. The Lex-HMM and PYP-HMM columns indicate the results of word type based particle filtering with 10 and 100 particles, respectively, while the Local and 1HMM columns use the token based sampler and the 1HMM approximation described by Blunsom and Cohn (2011). The token based sampler was run for 500 iterations and the other samplers for 200. The percentages in brakets represent the average accuracy over all languages except for Hungarian. sampler represent the previous state-of-the-art. These results show that the Lex-HMM+LM model achieves state-of-the-art M-1 accuracies on several datasets, including the English WSJ. The LexHMM+LM model performs nearly as well as, and often better than, the 1HMM+LM sampler without any restrictions on tag assignments. The drastic improvement in the performance of the Lex-HMM model reinforces our hypothesis that mo</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical Pitman-Yor process hmm for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865–874, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<pages>18--467</pages>
<contexts>
<context position="1658" citStr="Brown et al., 1992" startWordPosition="246" endWordPosition="249">w tags per type. We develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empir</context>
<context position="3838" citStr="Brown et al., 1992" startWordPosition="608" endWordPosition="611">es a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Background From the early work in the 1990’s, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992; Kupiec, 1992; Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Comput. Linguist., 18:467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22303" citStr="Buchholz and Marsi, 2006" startWordPosition="3712" endWordPosition="3715"> using the many-to-one (M-1) metric considered most intuitive by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag. While Christodoulopoulos et al. (2010) found Vmeasure to be more stable over the number of parts-of-speech, this effect doesn’t appear when the number of tags is constant, as in our case. For experiments on English, we report results on the entire Penn. Treebank (Marcus et al., 1993). For other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). All Lex-HMM results are reported with 10 particles as no significant improvement was found with 50 particles. Table 1 compares the M-1 accuracies of both the PYP-HMM and the Lex-HMM models on the Penn. Treebank Wall Street Journal corpus. Blunsom and Cohn (2011) found that the Local PYPHMM+LM sampler is unable to mix, achieving accuracy below 50%, therefore it has been left out of this analysis. The Lex-HMM+LM model achieves the same accuracy as the state-of-theart PYP-1HMM+LM approximation. The LexHMM+LM’s focus on only the most likely tags for each word type allows it to finish training in</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149–164, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>575--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1704" citStr="Christodoulopoulos et al., 2010" startWordPosition="252" endWordPosition="255">ticle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of </context>
<context position="21000" citStr="Christodoulopoulos et al., 2010" startWordPosition="3502" endWordPosition="3505">) 76.1 — MEMM (BBDK10) 75.5 ∼40* Lex-HMM 71.1 7.9 Type PYP-HMM 70.1 401.2 Local PYP-HMM 70.2 8.6 PYP-1HMM 75.6 20.6 Lex-HMM+LM 77.5 16.9 Type PYP-HMM+LM 73.5 446.0 PYP-1HMM+LM 77.5 34.9 Table 1: M-1 Accuracy on the WSJ Corpus: Comparison of the accuracy of each of the samplers with and without the language model emission prior on the English WSJ Corpus. The second column reports run time in hours where available*. Note the Lex-HMM+LM model matches the PYP1HMM+LM approximation despite finishing in half the time. The abbreviations in parentheses indicate that the results were reported in CGS10 (Christodoulopoulos et al., 2010) and BBDK10 (Berg-Kirkpatrick et al., 2010) *CGS10 reports that the MEMM model takes approximately 40 hours on 16 cores. accuracy in representing ambiguity classes is an important aspect of its performance. The model focuses inference on the most likely tag choices, represented by ambiguity classes. 5.1 Unsupervised Part-of-Speech Tagging The most popular evaluation for unsupervised part-of-speech taggers is to induce a tagging for a corpus and compare the induced tags to those annotated by a linguist. As the induced tags are simply integer labels, we must employ a mapping between these and th</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth Annual Meeting of the European Association for Computational Linguistics (EACL),</booktitle>
<pages>59--66</pages>
<contexts>
<context position="1671" citStr="Clark, 2003" startWordPosition="250" endWordPosition="251">develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observ</context>
<context position="4618" citStr="Clark, 2003" startWordPosition="734" endWordPosition="735">its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth Annual Meeting of the European Association for Computational Linguistics (EACL), pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6367" citStr="Das and Petrov, 2011" startWordPosition="1003" endWordPosition="1006"> classes representing the set of tags each word-type can take aid inference by making the sparsity between tags and words explicit. Toutanova and Johnson (2008) showed that modelling ambiguity classes can lead to positive results with a small tag-dictionary extracted from the data. By including ambiguity classes in the model, this approach is able to infer ambiguity classes of unknown words. Many improvements in part-of-speech induction over the last few years have come from the use of semi-supervised approaches in the form of projecting PoS constraints across languages with parallel corpora (Das and Petrov, 2011) or extracting them from the wiktionary (Li et al., 2012). These semi-supervised methods ultimately rely on a strong unsupervised model of PoS as their base. Thus, further improvements in unsupervised models, especially in modelling tag constrains, should lead to improvements in semi-supervised part-ofspeech induction. We find that modelling the lexicon in part-ofspeech inference can lead to more efficient algorithms that match the state-of-the-art unsupervised performance. We also note that the lexicon model relies heavily on morphological information, and suffers without it on languages with</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 600–609, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Dubbin</author>
<author>Phil Blunsom</author>
</authors>
<title>Unsupervised bayesian part of speech inference with particle gibbs.</title>
<date>2012</date>
<journal>ECML/PKDD</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>1</volume>
<pages>760--773</pages>
<editor>In Peter A. Flach, Tijl De Bie, and Nello Cristianini, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2973" citStr="Dubbin and Blunsom (2012)" startWordPosition="472" endWordPosition="476">e many exceptions, e.g. words that occur as both nouns and verbs (opening, increase, related etc.). In this paper we extend the Pitman-Yor HMM tagger (Blunsom and Cohn, 2011) to explicitly include a model of the lexicon that encodes from which tags a word type may be generated. For each word type we draw an ambiguity class which is the set of tags that it may occur with, capturing the fact that words are often ambiguous between certain tags (e.g. Noun and Verb), while rarely between others (e.g. Determiner and Verb). We extend the type based Sequential Monte Carlo (SMC) inference algorithm of Dubbin and Blunsom (2012) to incorporate our model of the lexicon, removing the need for the heuristic inference technique of Blunsom and Cohn (2011). We start in Section 3 by introducing the original PYP-HMM model and our extended model of the lexicon. Section 4 introduces a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Ling</context>
<context position="14852" citStr="Dubbin and Blunsom (2012)" startWordPosition="2444" endWordPosition="2447">fian distribution and the ambiguity of classes extracted from the predicted tags. ambiguity class are assigned an emission probability of 0 and the model is left deficient. Neither of the samplers proposed by Blunsom and Cohn (2011) and briefly described in section 3.1 are well suited to inference with the lexicon. Local Gibbs sampling of individual token-tag assignments would be very unlikely to explore a range of confusion classes, while the type based approximate sample relies on a one-tag-per-type restriction. Thus in the next section we extend the Particle Filtering solution presented in Dubbin and Blunsom (2012) to the problem of simultaneous resampling the ambiguity class as well as the tags for all tokens of a given type. This sampler provides both a more attractive inference algorithm for the original PYP-HMM and one adaptable to our Lex-HMM. 4 Inference To perform inference with both the lexicon and the tag assignments, we block sample the ambiguity class assignment as well as all tag assignments for tokens of the same word type. It would be intractable to exactly calculate the probabilities to sample these blocks. Particle filters are an example of a Sequential Monte Carlo technique which genera</context>
</contexts>
<marker>Dubbin, Blunsom, 2012</marker>
<rawString>Gregory Dubbin and Phil Blunsom. 2012. Unsupervised bayesian part of speech inference with particle gibbs. In Peter A. Flach, Tijl De Bie, and Nello Cristianini, editors, ECML/PKDD (1), volume 7523 of Lecture Notes in Computer Science, pages 760–773. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>99--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 99:2001–2049, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised hidden markov model pos taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>344--352</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4935" citStr="Gao and Johnson, 2008" startWordPosition="780" endWordPosition="784">ver tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earli</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 344–352, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007),</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1764" citStr="Goldwater and Griffiths, 2007" startWordPosition="260" endWordPosition="263">el and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close to one, there are many exception</context>
<context position="4195" citStr="Goldwater and Griffiths, 2007" startWordPosition="665" endWordPosition="669">nal Linguistics, pages 116–125, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Background From the early work in the 1990’s, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992; Kupiec, 1992; Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), pages 744–751, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9975" citStr="Goldwater et al., 2006" startWordPosition="1588" endWordPosition="1592">ning together the PYPs through their base distributions: Tij|aT , bT , Bi — PYP(aT , bT , Bi) Bi|aB, bB, U — PYP(aB, bB, U) U|aU, bU — PYP(aU, bU, Uniform). Ei|aE, bE, C — PYP(aE, bE, Ci), where Tij, Bi, and U are trigram, bigram, and unigram transition distributions respectively, and Ci is either a uniform distribution (PYP-HMM) or a bigram character language model distribution to model word morphology (PYP-HMM+LM). Sampling from the posterior of the hierarchical PYP is calculated with a variant of the Chinese Restaurant Process (CRP) called the Chinese Restaurant Franchise (CRF) (Teh, 2006; Goldwater et al., 2006). In the CRP analogy, each latent variable (tag) in a sequence is represented by a customer entering a restaurant and sitting at one of an infinite number of tables. A customer chooses to sit at a table in a restaurant according to the probability 1 &lt; k P(zn = k |z1:n−1) = 1 n−1+b &lt; K− llx−a+b k = K− + 1 n−1+b (1) where zn is the index of the table chosen by the nth customer to the restaurant, z1:n−1 is the seating arrangement of the previous n — 1 customers to enter, c−k is the count of the customers at table k, and K− is the total number of tables chosen by the previous n — 1 customers. All </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459– 466. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesnt EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007),</booktitle>
<pages>296--305</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4911" citStr="Johnson, 2007" startWordPosition="778" endWordPosition="779"> distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007), pages 296–305, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="3852" citStr="Kupiec, 1992" startWordPosition="612" endWordPosition="613">sampler for this model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Background From the early work in the 1990’s, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992; Kupiec, 1992; Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned t</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised pos tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>853--861</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5146" citStr="Lee et al. (2010)" startWordPosition="816" endWordPosition="819">viding a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produces state-of-the art results with a one-tag-per-type constraint, but even with this constraint the tag assignments must be roughly inferred from an approximation of the expectations. Ambiguity</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised pos tagging. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 853–861, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1389--1398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1389–1398, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Typebased MCMC.</title>
<date>2010</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="4956" citStr="Liang et al. (2010)" startWordPosition="785" endWordPosition="788">oach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produce</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2010. Typebased MCMC. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="22193" citStr="Marcus et al., 1993" startWordPosition="3693" endWordPosition="3696">apping between these and the more meaningful syntactic categories of the gold standard. We report results using the many-to-one (M-1) metric considered most intuitive by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag. While Christodoulopoulos et al. (2010) found Vmeasure to be more stable over the number of parts-of-speech, this effect doesn’t appear when the number of tags is constant, as in our case. For experiments on English, we report results on the entire Penn. Treebank (Marcus et al., 1993). For other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). All Lex-HMM results are reported with 10 particles as no significant improvement was found with 50 particles. Table 1 compares the M-1 accuracies of both the PYP-HMM and the Lex-HMM models on the Penn. Treebank Wall Street Journal corpus. Blunsom and Cohn (2011) found that the Local PYPHMM+LM sampler is unable to mix, achieving accuracy below 50%, therefore it has been left out of this analysis. The Lex-HMM+LM model achieves the same accuracy as the state-of-theart PYP-1HMM+LM approx</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>20--155</pages>
<contexts>
<context position="3869" citStr="Merialdo, 1993" startWordPosition="614" endWordPosition="615">is model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Background From the early work in the 1990’s, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992; Kupiec, 1992; Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. Thes</context>
</contexts>
<marker>Merialdo, 1993</marker>
<rawString>Bernard Merialdo. 1993. Tagging english text with a probabilistic model. Computational Linguistics, 20:155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9249" citStr="Teh, 2006" startWordPosition="1470" endWordPosition="1471">or Process prior. The Pitman-Yor prior defines a smooth back off probability from more complex to less complex transition and emission distributions. In the PYP-HMM trigram model, the transition distributions form a hierarchy with trigram transition distributions drawn from a PYP with the bigram transitions as their base distribution, and the bigram transitions similarly backing off to the unigram transitions. The hierarchical prior can be intuitively understood to smooth the trigram transition distributions with bigram and unigram distributions in a similar manner to an ngram language model (Teh, 2006). This back-off structure greatly reduces sparsity in the trigram distributions and is achieved by chaining together the PYPs through their base distributions: Tij|aT , bT , Bi — PYP(aT , bT , Bi) Bi|aB, bB, U — PYP(aB, bB, U) U|aU, bU — PYP(aU, bU, Uniform). Ei|aE, bE, C — PYP(aE, bE, Ci), where Tij, Bi, and U are trigram, bigram, and unigram transition distributions respectively, and Ci is either a uniform distribution (PYP-HMM) or a bigram character language model distribution to model word morphology (PYP-HMM+LM). Sampling from the posterior of the hierarchical PYP is calculated with a var</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 985–992, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1521--1528</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1733" citStr="Toutanova and Johnson, 2008" startWordPosition="256" endWordPosition="259">from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close t</context>
<context position="5906" citStr="Toutanova and Johnson (2008)" startWordPosition="930" endWordPosition="933">n (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produces state-of-the art results with a one-tag-per-type constraint, but even with this constraint the tag assignments must be roughly inferred from an approximation of the expectations. Ambiguity classes representing the set of tags each word-type can take aid inference by making the sparsity between tags and words explicit. Toutanova and Johnson (2008) showed that modelling ambiguity classes can lead to positive results with a small tag-dictionary extracted from the data. By including ambiguity classes in the model, this approach is able to infer ambiguity classes of unknown words. Many improvements in part-of-speech induction over the last few years have come from the use of semi-supervised approaches in the form of projecting PoS constraints across languages with parallel corpora (Das and Petrov, 2011) or extracting them from the wiktionary (Li et al., 2012). These semi-supervised methods ultimately rely on a strong unsupervised model of </context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1521– 1528. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>