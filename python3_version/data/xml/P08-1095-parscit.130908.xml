<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.952371">
You talking to me? A Corpus and Algorithm for Conversation
Disentanglement
</title>
<author confidence="0.89388">
Micha Elsner and Eugene Charniak
</author>
<affiliation confidence="0.907634">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.948971">
Providence, RI 02912
</address>
<email confidence="0.999785">
{melsner,ec}@@cs.brown.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999609">
When multiple conversations occur simultane-
ously, a listener must decide which conversa-
tion each utterance is part of in order to inter-
pret and respond to it appropriately. We refer
to this task as disentanglement. We present a
corpus of Internet Relay Chat (IRC) dialogue
in which the various conversations have been
manually disentangled, and evaluate annota-
tor reliability. This is, to our knowledge, the
first such corpus for internet chat. We pro-
pose a graph-theoretic model for disentangle-
ment, using discourse-based features which
have not been previously applied to this task.
The model’s predicted disentanglements are
highly correlated with manual annotations.
</bodyText>
<sectionHeader confidence="0.993113" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.994077933333333">
Simultaneous conversations seem to arise naturally
in both informal social interactions and multi-party
typed chat. Aoki et al. (2006)’s study of voice con-
versations among 8-10 people found an average of
1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is
even higher, at 2.75. The typical conversation, there-
fore, is one which is interrupted– frequently.
Disentanglement is the clustering task of dividing
a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level
dialogue analysis: for instance, consider the multi-
party exchange in figure 1.
Contextually, it is clear that this corresponds to
two conversations, and Felicia’s1 response “excel-
</bodyText>
<footnote confidence="0.633545">
1Real user nicknames are replaced with randomly selected
</footnote>
<listItem confidence="0.7630623">
(Chanel) Felicia: google works:)
(Gale) Arlie: you guys have never worked
in a factory before have you
(Gale) Arlie: there’s some real unethical
stuff that goes on
(Regine) hands Chanel a trophy
(Arlie) Gale, of course ... thats how they
make money
(Gale) and people lose limbs or get killed
(Felicia) excellent
</listItem>
<figureCaption confidence="0.999575">
Figure 1: Some (abridged) conversation from our corpus.
</figureCaption>
<bodyText confidence="0.99462545">
lent” is intended for Chanel and Regine. A straight-
forward reading of the transcript, however, might in-
terpret it as a response to Gale’s statement immedi-
ately preceding.
Humans are adept at disentanglement, even in
complicated environments like crowded cocktail
parties or chat rooms; in order to perform this task,
they must maintain a complex mental representation
of the ongoing discourse. Moreover, they adapt their
utterances to some degree to make the task easier
(O’Neill and Martin, 2003), which suggests that dis-
entanglement is in some sense a “difficult” discourse
task.
Disentanglement has two practical applications.
One is the analysis of pre-recorded transcripts in
order to extract some kind of information, such as
question-answer pairs or summaries. These tasks
should probably take as as input each separate con-
versation, rather than the entire transcript. Another
identifiers for ethical reasons.
</bodyText>
<page confidence="0.979469">
834
</page>
<note confidence="0.714492">
Proceedings of ACL-08: HLT, pages 834–842,
</note>
<page confidence="0.5381">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999646076923077">
application is as part of a user-interface system for
active participants in the chat, in which users target a
conversation of interest which is then highlighted for
them. Aoki et al. (2003) created such a system for
speech, which users generally preferred to a conven-
tional system– when the disentanglement worked!
Previous attempts to solve the problem (Aoki et
al., 2006; Aoki et al., 2003; Camtepe et al., 2005;
Acar et al., 2005) have several flaws. They clus-
ter speakers, not utterances, and so fail when speak-
ers move from one conversation to another. Their
features are mostly time gaps between one utterance
and another, without effective use of utterance con-
tent. Moreover, there is no framework for a prin-
cipled comparison of results: there are no reliable
annotation schemes, no standard corpora, and no
agreed-upon metrics.
We attempt to remedy these problems. We present
a new corpus of manually annotated chat room data
and evaluate annotator reliability. We give a set of
metrics describing structural similarity both locally
and globally. We propose a model which uses dis-
course structure and utterance contents in addition
to time gaps. It partitions a chat transcript into dis-
tinct conversations, and its output is highly corre-
lated with human annotations.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994352238806">
Two threads of research are direct attempts to solve
the disentanglement problem: Aoki et al. (2006),
Aoki et al. (2003) for speech and Camtepe et al.
(2005), Acar et al. (2005) for chat. We discuss
their approaches below. However, we should em-
phasize that we cannot compare our results directly
with theirs, because none of these studies publish re-
sults on human-annotated data. Although Aoki et al.
(2006) construct an annotated speech corpus, they
give no results for model performance, only user sat-
isfaction with their conversational system. Camtepe
et al. (2005) and Acar et al. (2005) do give perfor-
mance results, but only on synthetic data.
All of the previous approaches treat the problem
as one of clustering speakers, rather than utterances.
That is, they assume that during the window over
which the system operates, a particular speaker is
engaging in only one conversation. Camtepe et al.
(2005) assume this is true throughout the entire tran-
script; real speakers, by contrast, often participate
in many conversations, sequentially or sometimes
even simultaneously. Aoki et al. (2003) analyze each
thirty-second segment of the transcript separately.
This makes the single-conversation restriction some-
what less severe, but has the disadvantage of ignor-
ing all events which occur outside the segment.
Acar et al. (2005) attempt to deal with this prob-
lem by using a fuzzy algorithm to cluster speakers;
this assigns each speaker a distribution over conver-
sations rather than a hard assignment. However, the
algorithm still deals with speakers rather than utter-
ances, and cannot determine which conversation any
particular utterance is part of.
Another problem with these approaches is the in-
formation used for clustering. Aoki et al. (2003) and
Camtepe et al. (2005) detect the arrival times of mes-
sages, and use them to construct an affinity graph be-
tween participants by detecting turn-taking behavior
among pairs of speakers. (Turn-taking is typified by
short pauses between utterances; speakers aim nei-
ther to interrupt nor leave long gaps.) Aoki et al.
(2006) find that turn-taking on its own is inadequate.
They motivate a richer feature set, which, however,
does not yet appear to be implemented. Acar et
al. (2005) adds word repetition to their feature set.
However, their approach deals with all word repe-
titions on an equal basis, and so degrades quickly
in the presence of noise words (their term for words
which shared across conversations) to almost com-
plete failure when only 1/2 of the words are shared.
To motivate our own approach, we examine some
linguistic studies of discourse, especially analysis of
multi-party conversation. O’Neill and Martin (2003)
point out several ways in which multi-party text chat
differs from typical two-party conversation. One key
difference is the frequency with which participants
mention each others’ names. They hypothesize that
mentioning is a strategy which participants use to
make disentanglement easier, compensating for the
lack of cues normally present in face-to-face dia-
logue. Mentions (such as Gale’s comments to Ar-
lie in figure 1) are very common in our corpus, oc-
curring in 36% of comments, and provide a useful
feature.
Another key difference is that participants may
create a new conversation (floor) at any time, a pro-
cess which Sacks et al. (1974) calls schisming. Dur-
</bodyText>
<page confidence="0.997577">
835
</page>
<bodyText confidence="0.999809928571428">
ing a schism, a new conversation is formed, not
necessarily because of a shift in the topic, but be-
cause certain participants have refocused their atten-
tion onto each other, and away from whoever held
the floor in the parent conversation.
Despite these differences, there are still strong
similarities between chat and other conversations
such as meetings. Our feature set incorporates infor-
mation which has proven useful in meeting segmen-
tation (Galley et al., 2003) and the task of detect-
ing addressees of a specific utterance in a meeting
(Jovanovic et al., 2006). These include word rep-
etitions, utterance topic, and cue words which can
indicate the bounds of a segment.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999974529411765">
Our dataset is recorded from the IRC (Internet Re-
lay Chat) channel ##LINUX at freenode.net, using
the freely-available gaim client. ##LINUX is an un-
official tech support line for the Linux operating sys-
tem, selected because it is one of the most active chat
rooms on freenode, leading to many simultaneous
conversations, and because its content is typically
inoffensive. Although it is notionally intended only
for tech support, it includes large amounts of social
chat as well, such as the conversation about factory
work in the example above (figure 1).
The entire dataset contains 52:18 hours of chat,
but we devote most of our attention to three anno-
tated sections: development (706 utterances; 2:06
hr) and test (800 utts.; 1:39 hr) plus a short pilot sec-
tion on which we tested our annotation system (359
utts.; 0:58 hr).
</bodyText>
<subsectionHeader confidence="0.998811">
3.1 Annotation
</subsectionHeader>
<bodyText confidence="0.963059416666667">
Our annotators were seven university students with
at least some familiarity with the Linux OS, al-
though in some cases very slight. Annotation of the
test dataset typically took them about two hours. In
all, we produced six annotations of the test set2.
Our annotation scheme marks each utterance as
part of a single conversation. Annotators are in-
structed to create as many, or as few conversations as
they need to describe the data. Our instructions state
that a conversation can be between any number of
2One additional annotation was discarded because the anno-
tator misunderstood the task.
people, and that, “We mean conversation in the typ-
ical sense: a discussion in which the participants are
all reacting and paying attention to one another... it
should be clear that the comments inside a conver-
sation fit together.” The annotation system itself is a
simple Java program with a graphical interface, in-
tended to appear somewhat similar to a typical chat
client. Each speaker’s name is displayed in a differ-
ent color, and the system displays the elapsed time
between comments, marking especially long pauses
in red. Annotators group sentences into conversa-
tions by clicking and dragging them onto each other.
</bodyText>
<subsectionHeader confidence="0.997103">
3.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999988407407407">
Before discussing the annotations themselves, we
will describe the metrics we use to compare differ-
ent annotations; these measure both how much our
annotators agree with each other, and how well our
model and various baselines perform. Comparing
clusterings with different numbers of clusters is a
non-trivial task, and metrics for agreement on su-
pervised classification, such as the r. statistic, are not
applicable.
To measure global similarity between annota-
tions, we use one-to-one accuracy. This measure de-
scribes how well we can extract whole conversations
intact, as required for summarization or information
extraction. To compute it, we pair up conversations
from the two annotations to maximize the total over-
lap3, then report the percentage of overlap found.
If we intend to monitor or participate in the con-
versation as it occurs, we will care more about lo-
cal judgements. The local agreement metric counts
agreements and disagreements within a context k.
We consider a particular utterance: the previous
k utterances are each in either the same or a dif-
ferent conversation. The lock score between two
annotators is their average agreement on these k
same/different judgements, averaged over all utter-
ances. For example, loci counts pairs of adjacent
utterances for which two annotations agree.
</bodyText>
<page confidence="0.996027">
836
</page>
<table confidence="0.99133675">
Mean Max Min
Conversations 81.33 128 50
Avg. Conv. Length 10.6 16.0 6.2
Avg. Conv. Density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
1-to-1 52.98 63.50 35.63
loc 3 81.09 86.53 74.75
M-to-1 (by entropy) 86.70 94.13 75.50
</table>
<tableCaption confidence="0.995126">
Table 1: Statistics on 6 annotations of 800 lines of chat
transcript. Inter-annotator agreement metrics (below the
line) are calculated between distinct pairs of annotations.
</tableCaption>
<subsectionHeader confidence="0.971344">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999988866666667">
A statistical examination of our data (table 1) shows
that that it is eminently suitable for disentanglement:
the average number of conversations active at a time
is 2.75. Our annotators have high agreement on
the local metric (average of 81.1%). On the 1-to-
1 metric, they disagree more, with a mean overlap
of 53.0% and a maximum of only 63.5%. This level
of overlap does indicate a useful degree of reliabil-
ity, which cannot be achieved with naive heuristics
(see section 5). Thus measuring 1-to-1 overlap with
our annotations is a reasonable evaluation for com-
putational models. However, we feel that the major
source of disagreement is one that can be remedied
in future annotation schemes: the specificity of the
individual annotations.
To measure the level of detail in an annotation, we
use the information-theoretic entropy of the random
variable which indicates which conversation an ut-
terance is in. This quantity is non-negative, increas-
ing as the number of conversations grow and their
size becomes more balanced. It reaches its maxi-
mum, 9.64 bits for this dataset, when each utterance
is placed in a separate conversation. In our anno-
tations, it ranges from 3.0 to 6.2. This large vari-
ation shows that some annotators are more specific
than others, but does not indicate how much they
agree on the general structure. To measure this, we
introduce the many-to-one accuracy. This measure-
ment is asymmetrical, and maps each of the conver-
sations of the source annotation to the single con-
</bodyText>
<footnote confidence="0.96710475">
3This is an example of max-weight bipartite matching, and
can be computed optimally using, eg, max-flow. The widely
used greedy algorithm is a two-approximation, although we
have not found large differences in practice.
</footnote>
<bodyText confidence="0.9025385">
(Lai) need money
(Astrid) suggest a paypal fund or similar
(Lai) Azzie [sic; typo for Astrid?]: my
shack guy here said paypal too but i have
no local bank acct
(Felicia) second’s Azzie’s suggestion
(Gale) we should charge the noobs $1 per
question to [Lai’s] paypal
</bodyText>
<listItem confidence="0.893373125">
(Felicia) bingo!
(Gale) we’d have the money in 2 days max
(Azzie) Lai: hrm, Have you tried to set
one up?
(Arlie) the federal reserve system conspir-
acy is keeping you down man
(Felicia) Gale: all ubuntu users .. pay up!
(Gale) and susers pay double
</listItem>
<figure confidence="0.561621166666667">
(Azzie) I certainly would make suse users
pay.
(Hildegard) triple.
(Lai) Azzie: not since being offline
(Felicia) it doesn’t need to be “in state”
either
</figure>
<figureCaption confidence="0.633606166666667">
Figure 2: A schism occurring in our corpus (abridged):
not all annotators agree on where the thread about charg-
ing for answers to techical questions diverges from the
one about setting up Paypal accounts. Either Gale’s or
Azzie’s first comment seems to be the schism-inducing
utterance.
</figureCaption>
<bodyText confidence="0.998497555555556">
versation in the target with which it has the great-
est overlap, then counts the total percentage of over-
lap. This is not a statistic to be optimized (indeed,
optimization is trivial: simply make each utterance
in the source into its own conversation), but it can
give us some intuition about specificity. In partic-
ular, if one subdivides a coarse-grained annotation
to make a more specific variant, the many-to-one
accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones
(coarse), we find high many-to-one accuracy, with a
mean of 86%, which implies that the more specific
annotations have mostly the same large-scale bound-
aries as the coarser ones.
By examining the local metric, we can see even
more: local correlations are good, at an average of
81.1%. This means that, in the three-sentence win-
dow preceding each sentence, the annotators are of-
</bodyText>
<page confidence="0.987688">
837
</page>
<bodyText confidence="0.999864021276596">
ten in agreement. If they recognize subdivisions of
a large conversation, these subdivisions tend to be
contiguous, not mingled together, which is why they
have little impact on the local measure.
We find reasons for the annotators’ disagreement
about appropriate levels of detail in the linguistic
literature. As mentioned, new conversations of-
ten break off from old ones in schisms. Aoki et
al. (2006) discuss conversational features associated
with schisming and the related process of affiliation,
by which speakers attach themselves to a conversa-
tion. Schisms often branch off from asides or even
normal comments (toss-outs) within an existing con-
versation. This means that there is no clear begin-
ning to the new conversation– at the time when it
begins, it is not clear that there are two separate
floors, and this will not become clear until distinct
sets of speakers and patterns of turn-taking are es-
tablished. Speakers, meanwhile, take time to ori-
ent themselves to the new conversation. An example
schism is shown in Figure 2.
Our annotation scheme requires annotators to
mark each utterance as part of a single conversation,
and distinct conversations are not related in any way.
If a schism occurs, the annotator is faced with two
options: if it seems short, they may view it as a mere
digression and label it as part of the parent conver-
sation. If it seems to deserve a place of its own, they
will have to separate it from the parent, but this sev-
ers the initial comment (an otherwise unremarkable
aside) from its context. One or two of the annota-
tors actually remarked that this made the task con-
fusing. Our annotators seem to be either “splitters”
or “lumpers”– in other words, each annotator seems
to aim for a consistent level of detail, but each one
has their own idea of what this level should be.
As a final observation about the dataset, we test
the appropriateness of the assumption (used in pre-
vious work) that each speaker takes part in only one
conversation. In our data, the average speaker takes
part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a
speaker is, the more conversations they participate
in, as shown by a plot of conversations versus utter-
ances (Figure 3). The assumption is not very accu-
rate, especially for speakers with more than 10 utter-
ances.
</bodyText>
<figure confidence="0.983385416666667">
10
9
8
7
6
5
4
3
2
1
00 10 20 30 40 50 60
Utterances
</figure>
<figureCaption confidence="0.9949315">
Figure 3: Utterances versus conversations participated in
per speaker on development data.
</figureCaption>
<sectionHeader confidence="0.994308" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.99994425">
Our model for disentanglement fits into the general
class of graph partitioning algorithms (Roth and Yih,
2004) which have been used for a variety of tasks in
NLP, including the related task of meeting segmen-
tation (Malioutov and Barzilay, 2006). These algo-
rithms operate in two stages: first, a binary classifier
marks each pair of items as alike or different, and
second, a consistent partition is extracted.4
</bodyText>
<subsectionHeader confidence="0.974283">
4.1 Classification
</subsectionHeader>
<bodyText confidence="0.9994949">
We use a maximum-entropy classifier (Daum´e III,
2004) to decide whether a pair of utterances x and
y are in same or different conversations. The most
likely class is different, which occurs 57% of the
time in development data. We describe the classi-
fier’s performance in terms of raw accuracy (cor-
rect decisions / total), precision and recall of the
same class, and F-score, the harmonic mean of pre-
cision and recall. Our classifier uses several types
of features (table 2). The chat-specific features yield
the highest accuracy and precision. Discourse and
content-based features have poor accuracy on their
own (worse than the baseline), since they work best
on nearby pairs of utterances, and tend to fail on
more distant pairs. Paired with the time gap fea-
ture, however, they boost accuracy somewhat and
produce substantial gains in recall, encouraging the
model to group related utterances together.
The time gap, as discussed above, is the most
widely used feature in previous work. We exam-
</bodyText>
<footnote confidence="0.95923275">
4Our first attempt at this task used a Bayesian generative
model. However, we could not define a sharp enough posterior
over new sentences, which made the model unstable and overly
sensitive to its prior.
</footnote>
<note confidence="0.557355">
Threads
</note>
<page confidence="0.94663">
838
</page>
<subsectionHeader confidence="0.324087">
Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66)
</subsectionHeader>
<bodyText confidence="0.943739363636363">
Time The time between x and y in sec-
onds, bucketed logarithmically.
Speaker x and y have the same speaker.
Mention x mentions y (or vice versa),
both mention the same name, ei-
ther mentions any name.
Discourse (Acc 52: Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting
(“hello” &amp;c), an answer (“yes”,
“no” &amp;c), or thanks.
Question Either asks a question (explicitly
marked with “?”).
Long Either is long (&gt; 10 words).
Content (Acc 50: Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared
between x and y which have
unigram probability i, bucketed
logarithmically.
Tech Whether both x and y use tech-
nical jargon, neither do, or only
one does.
Combined (Acc 75: Prec: 73 Rec: 68 F: 71)
</bodyText>
<tableCaption confidence="0.981125">
Table 2: Feature functions with performance on develop-
ment data.
</tableCaption>
<bodyText confidence="0.99792475">
ine the distribution of pauses between utterances in
the same conversation. Our choice of a logarithmic
bucketing scheme is intended to capture two char-
acteristics of the distribution (figure 4). The curve
has its maximum at 1-3 seconds, and pauses shorter
than a second are less common. This reflects turn-
taking behavior among participants; participants in
the same conversation prefer to wait for each others’
responses before speaking again. On the other hand,
the curve is quite heavy-tailed to the right, leading
us to bucket long pauses fairly coarsely.
Our discourse-based features model some pair-
</bodyText>
<figure confidence="0.9907794">
Frequency 40
20
0
0 10 100 1000
seconds
</figure>
<figureCaption confidence="0.9965775">
Figure 4: Distribution of pause length (log-scaled) be-
tween utterances in the same conversation.
</figureCaption>
<bodyText confidence="0.999968853658537">
wise relationships: questions followed by answers,
short comments reacting to longer ones, greetings at
the beginning and thanks at the end.
Word repetition is a key feature in nearly every
model for segmentation or coherence, so it is no sur-
prise that it is useful here. We bucket repeated words
by their unigram probability5 (measured over the en-
tire 52 hours of transcript). The bucketing scheme
allows us to deal with “noise words” which are re-
peated coincidentally.
The point of the repetition feature is of course to
detect sentences with similar topics. We also find
that sentences with technical content are more likely
to be related than non-technical sentences. We label
an utterance as technical if it contains a web address,
a long string of digits, or a term present in a guide
for novice Linux users 6 but not in a large news cor-
pus (Graff, 1995)7. This is a light-weight way to
capture one “semantic dimension” or cluster of re-
lated words, in a corpus which is not amenable to
full LSA or similar techniques. LSA in text corpora
yields a better relatedness measure than simple rep-
etition (Foltz et al., 1998), but is ineffective in our
corpus because of its wide variety of topics and lack
of distinct document boundaries.
Pairs of utterances which are widely separated
in the discourse are unlikely to be directly related–
even if they are part of the same conversation, the
link between them is probably a long chain of in-
tervening utterances. Thus, if we run our classifier
on a pair of very distant utterances, we expect it to
default to the majority class, which in this case will
be different, and this will damage our performance
in case the two are really part of the same conver-
sation. To deal with this, we run our classifier only
on utterances separated by 129 seconds or less. This
is the last of our logarithmic buckets in which the
classifier has a significant advantage over the major-
ity baseline. For 99.9% of utterances in an ongoing
conversation, the previous utterance in that conver-
sation is within this gap, and so the system has a
</bodyText>
<footnote confidence="0.991032">
5We discard the 50 most frequent words entirely.
6“Introduction to Linux: A Hands-on Guide”. Machtelt
Garrels. Edition 1.25 from http://tldp.org/LDP/intro-
linux/html/intro-linux.html .
7Our data came from the LA times, 94-97– helpfully, it pre-
dates the current wide coverage of Linux in the mainstream
press.
</footnote>
<page confidence="0.997905">
839
</page>
<bodyText confidence="0.997687222222222">
chance of correctly linking the two.
On test data, the classifier has a mean accuracy of
68.2 (averaged over annotations). The mean preci-
sion of same conversation is 53.3 and the recall is
71.3, with mean F-score of 60. This error rate is
high, but the partitioning procedure allows us to re-
cover from some of the errors, since if nearby utter-
ances are grouped correctly, the bad decisions will
be outvoted by good ones.
</bodyText>
<subsectionHeader confidence="0.997656">
4.2 Partitioning
</subsectionHeader>
<bodyText confidence="0.963352307692308">
The next step in the process is to cluster the utter-
ances. We wish to find a set of clusters for which the
weighted accuracy of the classifier would be max-
imal; this is an example of correlation clustering
(Bansal et al., 2004), which is NP-complete8. Find-
ing an exact solution proves to be difficult; the prob-
lem has a quadratic number of variables (one for
each pair of utterances) and a cubic number of tri-
angle inequality constraints (three for each triplet).
With 800 utterances in our test set, even solving the
linear program with CPLEX (Ilog, Inc., 2003) is too
expensive to be practical.
Although there are a variety of approximations
and local searches, we do not wish to investigate
partitioning methods in this paper, so we simply
use a greedy search. In this algorithm, we as-
sign utterance j by examining all previous utter-
ances i within the classifier’s window, and treat-
ing the classifier’s judgement pi�j − .5 as a vote for
cluster(i). If the maximum vote is greater than 0,
we set cluster(j) = argmax, vote,. Otherwise j
is put in a new cluster. Greedy clustering makes at
least a reasonable starting point for further efforts,
since it is a natural online algorithm– it assigns each
utterance as it arrives, without reference to the fu-
ture.
At any rate, we should not take our objective func-
tion too seriously. Although it is roughly correlated
with performance, the high error rate of the classifier
makes it unlikely that small changes in objective will
mean much. In fact, the objective value of our output
solutions are generally higher than those for true so-
8We set up the problem by taking the weight of edge i, j as
the classifier’s decision pij − .5. Roth and Yih (2004) use log
probabilities as weights. Bansal et al. (2004) propose the log
odds ratio log(p/(1 − p)). We are unsure of the relative merit
of these approaches.
lutions, which implies we have already reached the
limits of what our classifier can tell us.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999634">
We annotate the 800 line test transcript using our
system. The annotation obtained has 63 conversa-
tions, with mean length 12.70. The average density
of conversations is 2.9, and the entropy is 3.79. This
places it within the bounds of our human annota-
tions (see table 1), toward the more general end of
the spectrum.
As a standard of comparison for our system, we
provide results for several baselines– trivial systems
which any useful annotation should outperform.
All different Each utterance is a separate conversa-
tion.
All same The whole transcript is a single conversa-
tion.
Blocks of k Each consecutive group of k utterances
is a conversation.
Pause of k Each pause of k seconds or more sepa-
rates two conversations.
Speaker Each speaker’s utterances are treated as a
monologue.
For each particular metric, we calculate the best
baseline result among all of these. To find the best
block size or pause length, we search over multiples
of 5 between 5 and 300. This makes these baselines
appear better than they really are, since their perfor-
mance is optimized with respect to the test data.
Our results, in table 3, are encouraging. On aver-
age, annotators agree more with each other than with
any artificial annotation, and more with our model
than with the baselines. For the 1-to-1 accuracy met-
ric, we cannot claim much beyond these general re-
sults. The range of human variation is quite wide,
and there are annotators who are closer to baselines
than to any other human annotator. As explained
earlier, this is because some human annotations are
much more specific than others. For very specific
annotations, the best baselines are short blocks or
pauses. For the most general, marking all utterances
the same does very well (although for all other an-
notations, it is extremely poor).
</bodyText>
<page confidence="0.98882">
840
</page>
<table confidence="0.997486142857143">
Other Annotators Model Best Baseline All Diff All Same
Mean 1-to-1 52.98 40.62 34.73 (Blocks of 40) 10.16 20.93
Max 1-to-1 63.50 51.12 56.00 (Pause of 65) 16.00 53.50
Min 1-to-1 35.63 33.63 28.62 (Pause of 25) 6.25 7.13
Mean loc 3 81.09 72.75 62.16 (Speaker) 52.93 47.07
Max loc 3 86.53 75.16 69.05 (Speaker) 62.15 57.47
Min loc 3 74.75 70.47 54.37 (Speaker) 42.53 37.85
</table>
<tableCaption confidence="0.983417">
Table 3: Metric values between proposed annotations and human annotations. Model scores typically fall between
inter-annotator agreement and baseline performance.
</tableCaption>
<bodyText confidence="0.999985217391304">
For the local metric, the results are much clearer.
There is no overlap in the ranges; for every test an-
notation, agreement is highest with other annota-
tor, then our model and finally the baselines. The
most competitive baseline is one conversation per
speaker, which makes sense, since if a speaker
makes two comments in a four-utterance window,
they are very likely to be related.
The name mention features are critical for our
model’s performance. Without this feature, the clas-
sifier’s development F-score drops from 71 to 56.
The disentanglement system’s test performance de-
creases proportionally; mean 1-to-1 falls to 36.08,
and mean loc 3 to 63.00, essentially baseline per-
formance. On the other hand, mentions are not
sufficient; with only name mention and time gap
features, mean 1-to-1 is 38.54 and loc 3 is 67.14.
For some utterances, of course, name mentions pro-
vide the only reasonable clue to the correct deci-
sion, which is why humans mention names in the
first place. But our system is probably overly depen-
dent on them, since they are very reliable compared
to our other features.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999978814814815">
Although our annotators are reasonably reliable, it
seems clear that they think of conversations as a hi-
erarchy, with digressions and schisms. We are in-
terested to see an annotation protocol which more
closely follows human intuition and explicitly in-
cludes these kinds of relationships.
We are also interested to see how well this feature
set performs on speech data, as in (Aoki et al., 2003).
Spoken conversation is more natural than text chat,
but when participants are not face-to-face, disentan-
glement remains a problem. On the other hand, spo-
ken dialogue contains new sources of information,
such as prosody. Turn-taking behavior is also more
distinct, which makes the task easier, but according
to (Aoki et al., 2006), it is certainly not sufficient.
Improving the current model will definitely re-
quire better features for the classifier. However, we
also left the issue of partitioning nearly completely
unexplored. If the classifier can indeed be improved,
we expect the impact of search errors to increase.
Another issue is that human users may prefer more
or less specific annotations than our model provides.
We have observed that we can produce lower or
higher-entropy annotations by changing the classi-
fier’s bias to label more edges same or different. But
we do not yet know whether this corresponds with
human judgements, or merely introduces errors.
</bodyText>
<sectionHeader confidence="0.9995" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999838">
This work provides a corpus of annotated data for
chat disentanglement, which, along with our pro-
posed metrics, should allow future researchers to
evaluate and compare their results quantitatively9.
Our annotations are consistent with one another, es-
pecially with respect to local agreement. We show
that features based on discourse patterns and the
content of utterances are helpful in disentanglement.
The model we present can outperform a variety of
baselines.
</bodyText>
<sectionHeader confidence="0.998722" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98974625">
Our thanks to Suman Karumuri, Steve Sloman, Matt
Lease, David McClosky, 7 test annotators, 3 pilot
annotators, 3 anonymous reviewers and the NSF
PIRE grant.
</bodyText>
<footnote confidence="0.9655175">
9Code and data for this project will be available at
http://cs.brown.edu/people/melsner.
</footnote>
<page confidence="0.997788">
841
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755735294117">
Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-
ishnamoorthy, and Blent Yener. 2005. Model-
ing and multiway analysis of chatroom tensors. In
Paul B. Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,
and Ralph C. Merkle, editors, ISI, volume 3495 of
Lecture Notes in Computer Science, pages 256–268.
Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter’s cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI ’03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425–432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where’s the “party” in “multi-
party”?: analyzing the structure of small-group socia-
ble talk. In CSCW ’06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393–402, New York, NY, USA.
ACM Press.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89–113.
Seyit Ahmet Camtepe, Mark K. Goldberg, Malik
Magdon-Ismail, and Mukkai Krishnamoorty. 2005.
Detecting conversing groups of chatters: a model, al-
gorithms, and tests. In IADIS AC, pages 89–96.
Hal Daum´e III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&amp;3):285–307.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In ACL ’03: Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 562–569, Morristown, NJ,
USA. Association for Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Ilog, Inc. 2003. Cplex solver.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006. Addressee identification in face-to-face
meetings. In EACL. The Association for Computer
Linguistics.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In ACL.
The Association for Computer Linguistics.
Jacki O’Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP ’03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40–49, New York, NY, USA. ACM
Press.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-2004, pages
1–8. Boston, MA, USA.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systematics for the organization of
turn-taking for conversation. Language, 50(4):696–
735.
</reference>
<page confidence="0.998011">
842
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990282">You talking to me? A Corpus and Algorithm for Conversation Disentanglement</title>
<author confidence="0.996388">Micha Elsner</author>
<author confidence="0.996388">Eugene Charniak</author>
<affiliation confidence="0.997416">Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University</affiliation>
<address confidence="0.999335">Providence, RI 02912</address>
<abstract confidence="0.997253460317461">When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model’s predicted disentanglements are highly correlated with manual annotations. 1 Motivation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al. (2006)’s study of voice conversations among 8-10 people found an average of 1.76 conversations (floors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, is one which is interrupted– frequently. Disentanglement is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis: for instance, consider the multiparty exchange in figure 1. Contextually, it is clear that this corresponds to conversations, and response “exceluser nicknames are replaced with randomly selected google works:) you guys have never worked in a factory before have you there’s some real unethical stuff that goes on Chanel a trophy of course ... thats how they make money people lose limbs or get killed Figure 1: Some (abridged) conversation from our corpus. lent” is intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. Humans are adept at disentanglement, even in complicated environments like crowded cocktail parties or chat rooms; in order to perform this task, they must maintain a complex mental representation of the ongoing discourse. Moreover, they adapt their utterances to some degree to make the task easier (O’Neill and Martin, 2003), which suggests that disentanglement is in some sense a “difficult” discourse task. Disentanglement has two practical applications. One is the analysis of pre-recorded transcripts in order to extract some kind of information, such as question-answer pairs or summaries. These tasks should probably take as as input each separate conversation, rather than the entire transcript. Another identifiers for ethical reasons.</abstract>
<note confidence="0.65088">834 of ACL-08: pages 834–842, Ohio, USA, June 2008. Association for Computational Linguistics</note>
<abstract confidence="0.999434133333334">application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system– when the disentanglement worked! Previous attempts to solve the problem (Aoki et al., 2006; Aoki et al., 2003; Camtepe et al., 2005; Acar et al., 2005) have several flaws. They cluster speakers, not utterances, and so fail when speakers move from one conversation to another. Their features are mostly time gaps between one utterance and another, without effective use of utterance content. Moreover, there is no framework for a principled comparison of results: there are no reliable annotation schemes, no standard corpora, and no agreed-upon metrics. We attempt to remedy these problems. We present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics describing structural similarity both locally and globally. We propose a model which uses discourse structure and utterance contents in addition to time gaps. It partitions a chat transcript into distinct conversations, and its output is highly correlated with human annotations. 2 Related Work Two threads of research are direct attempts to solve the disentanglement problem: Aoki et al. (2006), Aoki et al. (2003) for speech and Camtepe et al. (2005), Acar et al. (2005) for chat. We discuss their approaches below. However, we should emphasize that we cannot compare our results directly with theirs, because none of these studies publish results on human-annotated data. Although Aoki et al. (2006) construct an annotated speech corpus, they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. All of the previous approaches treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) assume this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each thirty-second segment of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) adds word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly the presence of words term for words which shared across conversations) to almost comfailure when only the words are shared. To motivate our own approach, we examine some linguistic studies of discourse, especially analysis of multi-party conversation. O’Neill and Martin (2003) point out several ways in which multi-party text chat differs from typical two-party conversation. One key difference is the frequency with which participants mention each others’ names. They hypothesize that mentioning is a strategy which participants use to make disentanglement easier, compensating for the lack of cues normally present in face-to-face dialogue. Mentions (such as Gale’s comments to Arlie in figure 1) are very common in our corpus, occurring in 36% of comments, and provide a useful feature. Another key difference is that participants may create a new conversation (floor) at any time, a prowhich Sacks et al. (1974) calls Dur- 835 ing a schism, a new conversation is formed, not necessarily because of a shift in the topic, but because certain participants have refocused their attention onto each other, and away from whoever held the floor in the parent conversation. Despite these differences, there are still strong similarities between chat and other conversations such as meetings. Our feature set incorporates information which has proven useful in meeting segmentation (Galley et al., 2003) and the task of detecting addressees of a specific utterance in a meeting (Jovanovic et al., 2006). These include word reputterance topic, and words can indicate the bounds of a segment. 3 Dataset Our dataset is recorded from the IRC (Internet Re- Chat) channel using freely-available an unofficial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factory work in the example above (figure 1). The entire dataset contains 52:18 hours of chat, but we devote most of our attention to three annotated sections: development (706 utterances; 2:06 hr) and test (800 utts.; 1:39 hr) plus a short pilot section on which we tested our annotation system (359 utts.; 0:58 hr). 3.1 Annotation Our annotators were seven university students with at least some familiarity with the Linux OS, although in some cases very slight. Annotation of the test dataset typically took them about two hours. In we produced six annotations of the test Our annotation scheme marks each utterance as part of a single conversation. Annotators are instructed to create as many, or as few conversations as they need to describe the data. Our instructions state that a conversation can be between any number of additional annotation was discarded because the annotator misunderstood the task. people, and that, “We mean conversation in the typical sense: a discussion in which the participants are all reacting and paying attention to one another... it should be clear that the comments inside a conversation fit together.” The annotation system itself is a simple Java program with a graphical interface, intended to appear somewhat similar to a typical chat client. Each speaker’s name is displayed in a different color, and the system displays the elapsed time between comments, marking especially long pauses in red. Annotators group sentences into conversations by clicking and dragging them onto each other. 3.2 Metrics Before discussing the annotations themselves, we will describe the metrics we use to compare different annotations; these measure both how much our annotators agree with each other, and how well our model and various baselines perform. Comparing clusterings with different numbers of clusters is a non-trivial task, and metrics for agreement on suclassification, such as the are not applicable. To measure global similarity between annotawe use This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overthen report the percentage of overlap found. If we intend to monitor or participate in the conversation as it occurs, we will care more about lojudgements. The agreement counts and disagreements within a context We consider a particular utterance: the previous are each in either the a dif- The between two is their average agreement on these same/different judgements, averaged over all utter- For example, counts pairs of adjacent utterances for which two annotations agree.</abstract>
<phone confidence="0.210368">836</phone>
<author confidence="0.880837">Mean Max Min</author>
<note confidence="0.735857875">Conversations 81.33 128 50 Avg. Conv. Length 10.6 16.0 6.2 Avg. Conv. Density 2.75 2.92 2.53 Entropy 4.83 6.18 3.00 1-to-1 52.98 63.50 35.63 81.09 86.53 74.75 M-to-1 (by entropy) 86.70 94.13 75.50 Table 1: Statistics on 6 annotations of 800 lines of chat</note>
<abstract confidence="0.999441023255814">transcript. Inter-annotator agreement metrics (below the line) are calculated between distinct pairs of annotations. 3.3 Discussion A statistical examination of our data (table 1) shows that that it is eminently suitable for disentanglement: the average number of conversations active at a time is 2.75. Our annotators have high agreement on the local metric (average of 81.1%). On the 1-to- 1 metric, they disagree more, with a mean overlap of 53.0% and a maximum of only 63.5%. This level of overlap does indicate a useful degree of reliability, which cannot be achieved with naive heuristics (see section 5). Thus measuring 1-to-1 overlap with our annotations is a reasonable evaluation for computational models. However, we feel that the major source of disagreement is one that can be remedied in future annotation schemes: the specificity of the individual annotations. To measure the level of detail in an annotation, we use the information-theoretic entropy of the random variable which indicates which conversation an utterance is in. This quantity is non-negative, increasing as the number of conversations grow and their size becomes more balanced. It reaches its maxifor this dataset, when each utterance is placed in a separate conversation. In our annotations, it ranges from 3.0 to 6.2. This large variation shows that some annotators are more specific than others, but does not indicate how much they agree on the general structure. To measure this, we introduce the many-to-one accuracy. This measurement is asymmetrical, and maps each of the converof the to the single conis an example of max-weight bipartite matching, and can be computed optimally using, eg, max-flow. The widely used greedy algorithm is a two-approximation, although we have not found large differences in practice. money a paypal fund or similar [sic; typo for Astrid?]: my shack guy here said paypal too but i have no local bank acct Azzie’s suggestion should charge the noobs $1 per question to [Lai’s] paypal have the money in 2 days max hrm, Have you tried to set one up? federal reserve system conspiracy is keeping you down man all ubuntu users .. pay up! susers pay double certainly would make suse users pay. not since being offline doesn’t need to be “in state” either Figure 2: A schism occurring in our corpus (abridged): not all annotators agree on where the thread about charging for answers to techical questions diverges from the one about setting up Paypal accounts. Either Gale’s or Azzie’s first comment seems to be the schism-inducing utterance. in the which it has the greatest overlap, then counts the total percentage of overlap. This is not a statistic to be optimized (indeed, optimization is trivial: simply make each utterance in the source into its own conversation), but it can give us some intuition about specificity. In particular, if one subdivides a coarse-grained annotation to make a more specific variant, the many-to-one accuracy from fine to coarse remains 1. When we map high-entropy annotations (fine) to lower ones (coarse), we find high many-to-one accuracy, with a mean of 86%, which implies that the more specific annotations have mostly the same large-scale boundaries as the coarser ones. By examining the local metric, we can see even more: local correlations are good, at an average of 81.1%. This means that, in the three-sentence winpreceding each sentence, the annotators are of- 837 ten in agreement. If they recognize subdivisions of a large conversation, these subdivisions tend to be contiguous, not mingled together, which is why they have little impact on the local measure. We find reasons for the annotators’ disagreement about appropriate levels of detail in the linguistic literature. As mentioned, new conversations often break off from old ones in schisms. Aoki et al. (2006) discuss conversational features associated schisming and the related process of by which speakers attach themselves to a conversation. Schisms often branch off from asides or even comments within an existing conversation. This means that there is no clear beginning to the new conversation– at the time when it begins, it is not clear that there are two separate floors, and this will not become clear until distinct sets of speakers and patterns of turn-taking are established. Speakers, meanwhile, take time to orient themselves to the new conversation. An example schism is shown in Figure 2. Our annotation scheme requires annotators to mark each utterance as part of a single conversation, and distinct conversations are not related in any way. If a schism occurs, the annotator is faced with two options: if it seems short, they may view it as a mere digression and label it as part of the parent conversation. If it seems to deserve a place of its own, they will have to separate it from the parent, but this severs the initial comment (an otherwise unremarkable aside) from its context. One or two of the annotators actually remarked that this made the task confusing. Our annotators seem to be either “splitters” or “lumpers”– in other words, each annotator seems to aim for a consistent level of detail, but each one has their own idea of what this level should be. As a final observation about the dataset, we test the appropriateness of the assumption (used in previous work) that each speaker takes part in only one conversation. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 3). The assumption is not very accurate, especially for speakers with more than 10 utterances.</abstract>
<note confidence="0.8852899">10 9 8 7 6 5 4 3 2 1</note>
<phone confidence="0.580851">00 10 20 30 40 50 60</phone>
<abstract confidence="0.980701792792793">Utterances Figure 3: Utterances versus conversations participated in per speaker on development data. 4 Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). These algorithms operate in two stages: first, a binary classifier marks each pair of items as alike or different, and a consistent partition is 4.1 Classification We use a maximum-entropy classifier (Daum´e III, to decide whether a pair of utterances in The most class is which occurs the time in development data. We describe the classifier’s performance in terms of raw accuracy (cordecisions precision and recall of the and F-score, the harmonic mean of precision and recall. Our classifier uses several types of features (table 2). The chat-specific features yield the highest accuracy and precision. Discourse and content-based features have poor accuracy on their own (worse than the baseline), since they work best on nearby pairs of utterances, and tend to fail on more distant pairs. Paired with the time gap feature, however, they boost accuracy somewhat and produce substantial gains in recall, encouraging the model to group related utterances together. The time gap, as discussed above, is the most used feature in previous work. We examfirst attempt at this task used a Bayesian generative model. However, we could not define a sharp enough posterior over new sentences, which made the model unstable and overly sensitive to its prior. Threads 838 Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66) The time between seconds, bucketed logarithmically. the same speaker. vice versa), both mention the same name, either mentions any name. Discourse (Acc 52: Prec: 47 Rec: 77 F: 58) words Either a greeting (“hello” &amp;c), an answer (“yes”, “no” &amp;c), or thanks. Question Either asks a question (explicitly marked with “?”). Either is long words). Content (Acc 50: Prec: 45 Rec: 74 F: 56) The number of words shared have probability bucketed logarithmically. Whether both nical jargon, neither do, or only one does. Combined (Acc 75: Prec: 73 Rec: 68 F: 71) Table 2: Feature functions with performance on development data. ine the distribution of pauses between utterances in the same conversation. Our choice of a logarithmic bucketing scheme is intended to capture two characteristics of the distribution (figure 4). The curve has its maximum at 1-3 seconds, and pauses shorter than a second are less common. This reflects turntaking behavior among participants; participants in the same conversation prefer to wait for each others’ responses before speaking again. On the other hand, the curve is quite heavy-tailed to the right, leading us to bucket long pauses fairly coarsely. discourse-based features model some pair- Frequency 40 20 0 0 10 100 1000 seconds Figure 4: Distribution of pause length (log-scaled) between utterances in the same conversation. wise relationships: questions followed by answers, short comments reacting to longer ones, greetings at the beginning and thanks at the end. Word repetition is a key feature in nearly every model for segmentation or coherence, so it is no surprise that it is useful here. We bucket repeated words their unigram (measured over the entire 52 hours of transcript). The bucketing scheme allows us to deal with “noise words” which are repeated coincidentally. The point of the repetition feature is of course to detect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a web address, a long string of digits, or a term present in a guide novice Linux users 6but not in a large news cor- (Graff, This is a light-weight way to capture one “semantic dimension” or cluster of related words, in a corpus which is not amenable to full LSA or similar techniques. LSA in text corpora yields a better relatedness measure than simple repetition (Foltz et al., 1998), but is ineffective in our corpus because of its wide variety of topics and lack of distinct document boundaries. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related– even if they are part of the same conversation, the link between them is probably a long chain of intervening utterances. Thus, if we run our classifier on a pair of very distant utterances, we expect it to default to the majority class, which in this case will and this will damage our performance in case the two are really part of the same conversation. To deal with this, we run our classifier only on utterances separated by 129 seconds or less. This is the last of our logarithmic buckets in which the classifier has a significant advantage over the majority baseline. For 99.9% of utterances in an ongoing conversation, the previous utterance in that conversation is within this gap, and so the system has a discard the 50 most frequent words entirely. to Linux: A Hands-on Guide”. Machtelt Edition 1.25 from linux/html/intro-linux.html . data came from the LA times, 94-97– helpfully, it predates the current wide coverage of Linux in the mainstream press. 839 chance of correctly linking the two. On test data, the classifier has a mean accuracy of over annotations). The mean preciof conversation the recall is with mean F-score of This error rate is high, but the partitioning procedure allows us to recover from some of the errors, since if nearby utterances are grouped correctly, the bad decisions will be outvoted by good ones. 4.2 Partitioning The next step in the process is to cluster the utterances. We wish to find a set of clusters for which the weighted accuracy of the classifier would be maxthis is an example of clustering et al., 2004), which is Finding an exact solution proves to be difficult; the problem has a quadratic number of variables (one for each pair of utterances) and a cubic number of triangle inequality constraints (three for each triplet). With 800 utterances in our test set, even solving the linear program with CPLEX (Ilog, Inc., 2003) is too expensive to be practical. Although there are a variety of approximations and local searches, we do not wish to investigate partitioning methods in this paper, so we simply use a greedy search. In this algorithm, we asutterance examining all previous utterthe classifier’s window, and treatthe classifier’s judgement a vote for If the maximum vote is greater than 0, set = Otherwise is put in a new cluster. Greedy clustering makes at least a reasonable starting point for further efforts, since it is a natural online algorithm– it assigns each utterance as it arrives, without reference to the future. At any rate, we should not take our objective function too seriously. Although it is roughly correlated with performance, the high error rate of the classifier makes it unlikely that small changes in objective will mean much. In fact, the objective value of our output are generally higher than those for true soset up the problem by taking the weight of edge j classifier’s decision Roth and Yih (2004) use log probabilities as weights. Bansal et al. (2004) propose the log ratio We are unsure of the relative merit of these approaches. lutions, which implies we have already reached the limits of what our classifier can tell us. 5 Experiments We annotate the 800 line test transcript using our system. The annotation obtained has 63 conversawith mean length The average density conversations is and the entropy is This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum. As a standard of comparison for our system, we provide results for several baselines– trivial systems which any useful annotation should outperform. different utterance is a separate conversation. same whole transcript is a single conversation. of consecutive group of is a conversation. of pause of or more separates two conversations. speaker’s utterances are treated as a monologue. For each particular metric, we calculate the best baseline result among all of these. To find the best block size or pause length, we search over multiples 5 and 300. This makes these baselines appear better than they really are, since their performance is optimized with respect to the test data. Our results, in table 3, are encouraging. On average, annotators agree more with each other than with any artificial annotation, and more with our model than with the baselines. For the 1-to-1 accuracy metric, we cannot claim much beyond these general results. The range of human variation is quite wide, and there are annotators who are closer to baselines than to any other human annotator. As explained earlier, this is because some human annotations are much more specific than others. For very specific annotations, the best baselines are short blocks or pauses. For the most general, marking all utterances the same does very well (although for all other annotations, it is extremely poor).</abstract>
<note confidence="0.8340148">840 Other Annotators Model Best Baseline All Diff All Same Mean 1-to-1 52.98 40.62 34.73 (Blocks of 40) 10.16 20.93 Max 1-to-1 63.50 51.12 56.00 (Pause of 65) 16.00 53.50 Min 1-to-1 35.63 33.63 28.62 (Pause of 25) 6.25 7.13</note>
<address confidence="0.484089666666667">81.09 72.75 62.16 (Speaker) 52.93 47.07 86.53 75.16 69.05 (Speaker) 62.15 57.47 74.75 70.47 54.37 (Speaker) 42.53 37.85</address>
<abstract confidence="0.960048830985916">Table 3: Metric values between proposed annotations and human annotations. Model scores typically fall between inter-annotator agreement and baseline performance. For the local metric, the results are much clearer. There is no overlap in the ranges; for every test annotation, agreement is highest with other annotator, then our model and finally the baselines. The most competitive baseline is one conversation per speaker, which makes sense, since if a speaker makes two comments in a four-utterance window, they are very likely to be related. The name mention features are critical for our model’s performance. Without this feature, the classifier’s development F-score drops from 71 to 56. The disentanglement system’s test performance decreases proportionally; mean 1-to-1 falls to 36.08, mean 3to 63.00, essentially baseline performance. On the other hand, mentions are not sufficient; with only name mention and time gap mean 1-to-1 is 38.54 and 3is 67.14. For some utterances, of course, name mentions provide the only reasonable clue to the correct decision, which is why humans mention names in the first place. But our system is probably overly dependent on them, since they are very reliable compared to our other features. 6 Future Work Although our annotators are reasonably reliable, it seems clear that they think of conversations as a hierarchy, with digressions and schisms. We are interested to see an annotation protocol which more closely follows human intuition and explicitly includes these kinds of relationships. We are also interested to see how well this feature set performs on speech data, as in (Aoki et al., 2003). Spoken conversation is more natural than text chat, but when participants are not face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody. Turn-taking behavior is also more distinct, which makes the task easier, but according to (Aoki et al., 2006), it is certainly not sufficient. Improving the current model will definitely require better features for the classifier. However, we also left the issue of partitioning nearly completely unexplored. If the classifier can indeed be improved, we expect the impact of search errors to increase. Another issue is that human users may prefer more or less specific annotations than our model provides. We have observed that we can produce lower or higher-entropy annotations by changing the classifier’s bias to label more edges same or different. But we do not yet know whether this corresponds with human judgements, or merely introduces errors. 7 Conclusion This work provides a corpus of annotated data for chat disentanglement, which, along with our proposed metrics, should allow future researchers to and compare their results Our annotations are consistent with one another, especially with respect to local agreement. We show that features based on discourse patterns and the content of utterances are helpful in disentanglement. The model we present can outperform a variety of baselines. Acknowledgements Our thanks to Suman Karumuri, Steve Sloman, Matt Lease, David McClosky, 7 test annotators, 3 pilot annotators, 3 anonymous reviewers and the NSF PIRE grant. and data for this project will be available at http://cs.brown.edu/people/melsner.</abstract>
<note confidence="0.854955">841 References Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Krishnamoorthy, and Blent Yener. 2005. Model-</note>
<title confidence="0.877756">ing and multiway analysis of chatroom tensors. In</title>
<author confidence="0.7850475">Paul B Kantor</author>
<author confidence="0.7850475">Gheorghe Muresan</author>
<author confidence="0.7850475">Fred Roberts</author>
<author confidence="0.7850475">Daniel Dajun Zeng</author>
<author confidence="0.7850475">Fei-Yue Wang</author>
<author confidence="0.7850475">Hsinchun Chen</author>
<note confidence="0.623401333333333">Ralph C. Merkle, editors, volume 3495 of Notes in Computer pages 256–268. Springer. M. Aoki, Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The mad hatter’s cocktail party: a</note>
<abstract confidence="0.911559818181818">social mobile audio space supporting multiple simulconversations. In ’03: Proceedings of the SIGCHI conference on Human factors in computing pages 425–432, New York, NY, USA. ACM Press. Paul M. Aoki, Margaret H. Szymanski, Luke D. Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. 2006. Where’s the “party” in “multiparty”?: analyzing the structure of small-group sociatalk. In ’06: Proceedings of the 2006 20th anniversary conference on Computer supported coop-</abstract>
<address confidence="0.683186">pages 393–402, New York, NY, USA.</address>
<note confidence="0.6150756875">ACM Press. Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. clustering. 56(1- 3):89–113. Seyit Ahmet Camtepe, Mark K. Goldberg, Malik Magdon-Ismail, and Mukkai Krishnamoorty. 2005. conversing groups of chatters: a model, aland tests. In pages 89–96. Hal Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/megam/, August. Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with semantic analysis. 25(2&amp;3):285–307.</note>
<author confidence="0.795108">Michel Galley</author>
<author confidence="0.795108">Kathleen McKeown</author>
<author confidence="0.795108">Eric Fosler-Lussier</author>
<note confidence="0.901962">and Hongyan Jing. 2003. Discourse segmentation of conversation. In ’03: Proceedings of the 41st Annual Meeting on Association for Compupages 562–569, Morristown, NJ, USA. Association for Computational Linguistics. Graff. 1995. American News Text Linguistic Data Consortium. LDC95T21. Ilog, Inc. 2003. Cplex solver. Jovanovic, Rieks op den Akker, and Anton Nijholt. 2006. Addressee identification in face-to-face In The Association for Computer Linguistics. Igor Malioutov and Regina Barzilay. 2006. Minimum model for spoken lecture segmentation. In The Association for Computer Linguistics. Jacki O’Neill and David Martin. 2003. Text chat in ac- In ’03: Proceedings of the 2003 international ACM SIGGROUP conference on Supporting pages 40–49, New York, NY, USA. ACM Press. Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural lantasks. In of pages 1–8. Boston, MA, USA. Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systematics for the organization of for conversation. 50(4):696– 735. 842</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evrim Acar</author>
<author>Seyit Ahmet Camtepe</author>
<author>Mukkai S Krishnamoorthy</author>
<author>Blent Yener</author>
</authors>
<title>Modeling and multiway analysis of chatroom tensors.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3495</volume>
<pages>256--268</pages>
<editor>In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI,</editor>
<contexts>
<context position="3583" citStr="Acar et al., 2005" startWordPosition="543" endWordPosition="546">re transcript. Another identifiers for ethical reasons. 834 Proceedings of ACL-08: HLT, pages 834–842, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system– when the disentanglement worked! Previous attempts to solve the problem (Aoki et al., 2006; Aoki et al., 2003; Camtepe et al., 2005; Acar et al., 2005) have several flaws. They cluster speakers, not utterances, and so fail when speakers move from one conversation to another. Their features are mostly time gaps between one utterance and another, without effective use of utterance content. Moreover, there is no framework for a principled comparison of results: there are no reliable annotation schemes, no standard corpora, and no agreed-upon metrics. We attempt to remedy these problems. We present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics describing structural similarity both </context>
<context position="5031" citStr="Acar et al. (2005)" startWordPosition="777" endWordPosition="780">nnotations. 2 Related Work Two threads of research are direct attempts to solve the disentanglement problem: Aoki et al. (2006), Aoki et al. (2003) for speech and Camtepe et al. (2005), Acar et al. (2005) for chat. We discuss their approaches below. However, we should emphasize that we cannot compare our results directly with theirs, because none of these studies publish results on human-annotated data. Although Aoki et al. (2006) construct an annotated speech corpus, they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. All of the previous approaches treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) assume this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each thirty-second segment of the transcript separately. This makes the single-conve</context>
<context position="6674" citStr="Acar et al. (2005)" startWordPosition="1038" endWordPosition="1041">on any particular utterance is part of. Another problem with these approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) adds word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of noise words (their term for words which shared across conversations) to almost complete failure when only 1/2 of the words are shared. To motivate our own approach, we examine some linguistic studies of discourse, especially analysis of multi-party conversation. O’Neill and Martin (2003) point out several ways in which multi-party text chat differs from typical two-party conversation. One key difference is the frequency with which par</context>
</contexts>
<marker>Acar, Camtepe, Krishnamoorthy, Yener, 2005</marker>
<rawString>Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Krishnamoorthy, and Blent Yener. 2005. Modeling and multiway analysis of chatroom tensors. In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI, volume 3495 of Lecture Notes in Computer Science, pages 256–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Matthew Romaine</author>
<author>Margaret H Szymanski</author>
<author>James D Thornton</author>
<author>Daniel Wilson</author>
<author>Allison Woodruff</author>
</authors>
<title>The mad hatter’s cocktail party: a social mobile audio space supporting multiple simultaneous conversations.</title>
<date>2003</date>
<booktitle>In CHI ’03: Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>425--432</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3339" citStr="Aoki et al. (2003)" startWordPosition="502" endWordPosition="505">pplications. One is the analysis of pre-recorded transcripts in order to extract some kind of information, such as question-answer pairs or summaries. These tasks should probably take as as input each separate conversation, rather than the entire transcript. Another identifiers for ethical reasons. 834 Proceedings of ACL-08: HLT, pages 834–842, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system– when the disentanglement worked! Previous attempts to solve the problem (Aoki et al., 2006; Aoki et al., 2003; Camtepe et al., 2005; Acar et al., 2005) have several flaws. They cluster speakers, not utterances, and so fail when speakers move from one conversation to another. Their features are mostly time gaps between one utterance and another, without effective use of utterance content. Moreover, there is no framework for a principled comparison of results: there are no reliable annotation schemes, no</context>
<context position="4560" citStr="Aoki et al. (2003)" startWordPosition="700" endWordPosition="703">andard corpora, and no agreed-upon metrics. We attempt to remedy these problems. We present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics describing structural similarity both locally and globally. We propose a model which uses discourse structure and utterance contents in addition to time gaps. It partitions a chat transcript into distinct conversations, and its output is highly correlated with human annotations. 2 Related Work Two threads of research are direct attempts to solve the disentanglement problem: Aoki et al. (2006), Aoki et al. (2003) for speech and Camtepe et al. (2005), Acar et al. (2005) for chat. We discuss their approaches below. However, we should emphasize that we cannot compare our results directly with theirs, because none of these studies publish results on human-annotated data. Although Aoki et al. (2006) construct an annotated speech corpus, they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. All of the previous approaches treat the problem as one of clustering s</context>
<context position="6192" citStr="Aoki et al. (2003)" startWordPosition="958" endWordPosition="961">of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) adds word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis,</context>
<context position="30135" citStr="Aoki et al., 2003" startWordPosition="4976" endWordPosition="4979"> reasonable clue to the correct decision, which is why humans mention names in the first place. But our system is probably overly dependent on them, since they are very reliable compared to our other features. 6 Future Work Although our annotators are reasonably reliable, it seems clear that they think of conversations as a hierarchy, with digressions and schisms. We are interested to see an annotation protocol which more closely follows human intuition and explicitly includes these kinds of relationships. We are also interested to see how well this feature set performs on speech data, as in (Aoki et al., 2003). Spoken conversation is more natural than text chat, but when participants are not face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody. Turn-taking behavior is also more distinct, which makes the task easier, but according to (Aoki et al., 2006), it is certainly not sufficient. Improving the current model will definitely require better features for the classifier. However, we also left the issue of partitioning nearly completely unexplored. If the classifier can indeed be improved, we expect the impact of sea</context>
</contexts>
<marker>Aoki, Romaine, Szymanski, Thornton, Wilson, Woodruff, 2003</marker>
<rawString>Springer. Paul M. Aoki, Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The mad hatter’s cocktail party: a social mobile audio space supporting multiple simultaneous conversations. In CHI ’03: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 425–432, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Margaret H Szymanski</author>
<author>Luke D Plurkowski</author>
<author>James D Thornton</author>
<author>Allison Woodruff</author>
<author>Weilie Yi</author>
</authors>
<title>Where’s the “party” in “multiparty”?: analyzing the structure of small-group sociable talk.</title>
<date>2006</date>
<booktitle>In CSCW ’06: Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work,</booktitle>
<pages>393--402</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1063" citStr="Aoki et al. (2006)" startWordPosition="149" endWordPosition="152">sentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model’s predicted disentanglements are highly correlated with manual annotations. 1 Motivation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al. (2006)’s study of voice conversations among 8-10 people found an average of 1.76 conversations (floors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, is one which is interrupted– frequently. Disentanglement is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis: for instance, consider the multiparty exchange in figure 1. Contextually, it is clear that this corresponds to two conversations, and Felicia’s1</context>
<context position="3522" citStr="Aoki et al., 2006" startWordPosition="531" endWordPosition="534">as as input each separate conversation, rather than the entire transcript. Another identifiers for ethical reasons. 834 Proceedings of ACL-08: HLT, pages 834–842, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system– when the disentanglement worked! Previous attempts to solve the problem (Aoki et al., 2006; Aoki et al., 2003; Camtepe et al., 2005; Acar et al., 2005) have several flaws. They cluster speakers, not utterances, and so fail when speakers move from one conversation to another. Their features are mostly time gaps between one utterance and another, without effective use of utterance content. Moreover, there is no framework for a principled comparison of results: there are no reliable annotation schemes, no standard corpora, and no agreed-upon metrics. We attempt to remedy these problems. We present a new corpus of manually annotated chat room data and evaluate annotator reliability. We</context>
<context position="4847" citStr="Aoki et al. (2006)" startWordPosition="748" endWordPosition="751"> uses discourse structure and utterance contents in addition to time gaps. It partitions a chat transcript into distinct conversations, and its output is highly correlated with human annotations. 2 Related Work Two threads of research are direct attempts to solve the disentanglement problem: Aoki et al. (2006), Aoki et al. (2003) for speech and Camtepe et al. (2005), Acar et al. (2005) for chat. We discuss their approaches below. However, we should emphasize that we cannot compare our results directly with theirs, because none of these studies publish results on human-annotated data. Although Aoki et al. (2006) construct an annotated speech corpus, they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. All of the previous approaches treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) assume this is true throughout the entire transcript; real speakers, by contrast, often participa</context>
<context position="6516" citStr="Aoki et al. (2006)" startWordPosition="1011" endWordPosition="1014">er conversations rather than a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) adds word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of noise words (their term for words which shared across conversations) to almost complete failure when only 1/2 of the words are shared. To motivate our own approach, we examine some linguistic studies of discourse, especially analysis of multi-party conversation. O’Neill and Marti</context>
<context position="16235" citStr="Aoki et al. (2006)" startWordPosition="2602" endWordPosition="2605">oarser ones. By examining the local metric, we can see even more: local correlations are good, at an average of 81.1%. This means that, in the three-sentence window preceding each sentence, the annotators are of837 ten in agreement. If they recognize subdivisions of a large conversation, these subdivisions tend to be contiguous, not mingled together, which is why they have little impact on the local measure. We find reasons for the annotators’ disagreement about appropriate levels of detail in the linguistic literature. As mentioned, new conversations often break off from old ones in schisms. Aoki et al. (2006) discuss conversational features associated with schisming and the related process of affiliation, by which speakers attach themselves to a conversation. Schisms often branch off from asides or even normal comments (toss-outs) within an existing conversation. This means that there is no clear beginning to the new conversation– at the time when it begins, it is not clear that there are two separate floors, and this will not become clear until distinct sets of speakers and patterns of turn-taking are established. Speakers, meanwhile, take time to orient themselves to the new conversation. An exa</context>
<context position="30466" citStr="Aoki et al., 2006" startWordPosition="5028" endWordPosition="5031">rchy, with digressions and schisms. We are interested to see an annotation protocol which more closely follows human intuition and explicitly includes these kinds of relationships. We are also interested to see how well this feature set performs on speech data, as in (Aoki et al., 2003). Spoken conversation is more natural than text chat, but when participants are not face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody. Turn-taking behavior is also more distinct, which makes the task easier, but according to (Aoki et al., 2006), it is certainly not sufficient. Improving the current model will definitely require better features for the classifier. However, we also left the issue of partitioning nearly completely unexplored. If the classifier can indeed be improved, we expect the impact of search errors to increase. Another issue is that human users may prefer more or less specific annotations than our model provides. We have observed that we can produce lower or higher-entropy annotations by changing the classifier’s bias to label more edges same or different. But we do not yet know whether this corresponds with huma</context>
</contexts>
<marker>Aoki, Szymanski, Plurkowski, Thornton, Woodruff, Yi, 2006</marker>
<rawString>Paul M. Aoki, Margaret H. Szymanski, Luke D. Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. 2006. Where’s the “party” in “multiparty”?: analyzing the structure of small-group sociable talk. In CSCW ’06: Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work, pages 393–402, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Bansal</author>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<date>2004</date>
<booktitle>Correlation clustering. Machine Learning,</booktitle>
<pages>56--1</pages>
<contexts>
<context position="24568" citStr="Bansal et al., 2004" startWordPosition="4031" endWordPosition="4034">data, the classifier has a mean accuracy of 68.2 (averaged over annotations). The mean precision of same conversation is 53.3 and the recall is 71.3, with mean F-score of 60. This error rate is high, but the partitioning procedure allows us to recover from some of the errors, since if nearby utterances are grouped correctly, the bad decisions will be outvoted by good ones. 4.2 Partitioning The next step in the process is to cluster the utterances. We wish to find a set of clusters for which the weighted accuracy of the classifier would be maximal; this is an example of correlation clustering (Bansal et al., 2004), which is NP-complete8. Finding an exact solution proves to be difficult; the problem has a quadratic number of variables (one for each pair of utterances) and a cubic number of triangle inequality constraints (three for each triplet). With 800 utterances in our test set, even solving the linear program with CPLEX (Ilog, Inc., 2003) is too expensive to be practical. Although there are a variety of approximations and local searches, we do not wish to investigate partitioning methods in this paper, so we simply use a greedy search. In this algorithm, we assign utterance j by examining all previ</context>
<context position="26092" citStr="Bansal et al. (2004)" startWordPosition="4297" endWordPosition="4300">ther efforts, since it is a natural online algorithm– it assigns each utterance as it arrives, without reference to the future. At any rate, we should not take our objective function too seriously. Although it is roughly correlated with performance, the high error rate of the classifier makes it unlikely that small changes in objective will mean much. In fact, the objective value of our output solutions are generally higher than those for true so8We set up the problem by taking the weight of edge i, j as the classifier’s decision pij − .5. Roth and Yih (2004) use log probabilities as weights. Bansal et al. (2004) propose the log odds ratio log(p/(1 − p)). We are unsure of the relative merit of these approaches. lutions, which implies we have already reached the limits of what our classifier can tell us. 5 Experiments We annotate the 800 line test transcript using our system. The annotation obtained has 63 conversations, with mean length 12.70. The average density of conversations is 2.9, and the entropy is 3.79. This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum. As a standard of comparison for our system, we provide results for several</context>
</contexts>
<marker>Bansal, Blum, Chawla, 2004</marker>
<rawString>Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Machine Learning, 56(1-3):89–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyit Ahmet Camtepe</author>
<author>Mark K Goldberg</author>
<author>Malik Magdon-Ismail</author>
<author>Mukkai Krishnamoorty</author>
</authors>
<title>Detecting conversing groups of chatters: a model, algorithms, and tests.</title>
<date>2005</date>
<booktitle>In IADIS AC,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="3563" citStr="Camtepe et al., 2005" startWordPosition="539" endWordPosition="542">, rather than the entire transcript. Another identifiers for ethical reasons. 834 Proceedings of ACL-08: HLT, pages 834–842, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system– when the disentanglement worked! Previous attempts to solve the problem (Aoki et al., 2006; Aoki et al., 2003; Camtepe et al., 2005; Acar et al., 2005) have several flaws. They cluster speakers, not utterances, and so fail when speakers move from one conversation to another. Their features are mostly time gaps between one utterance and another, without effective use of utterance content. Moreover, there is no framework for a principled comparison of results: there are no reliable annotation schemes, no standard corpora, and no agreed-upon metrics. We attempt to remedy these problems. We present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics describing structu</context>
<context position="5008" citStr="Camtepe et al. (2005)" startWordPosition="772" endWordPosition="775">ly correlated with human annotations. 2 Related Work Two threads of research are direct attempts to solve the disentanglement problem: Aoki et al. (2006), Aoki et al. (2003) for speech and Camtepe et al. (2005), Acar et al. (2005) for chat. We discuss their approaches below. However, we should emphasize that we cannot compare our results directly with theirs, because none of these studies publish results on human-annotated data. Although Aoki et al. (2006) construct an annotated speech corpus, they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. All of the previous approaches treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) assume this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each thirty-second segment of the transcript separately. This</context>
</contexts>
<marker>Camtepe, Goldberg, Magdon-Ismail, Krishnamoorty, 2005</marker>
<rawString>Seyit Ahmet Camtepe, Mark K. Goldberg, Malik Magdon-Ismail, and Mukkai Krishnamoorty. 2005. Detecting conversing groups of chatters: a model, algorithms, and tests. In IADIS AC, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/megam/,</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>Hal Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/megam/, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="22646" citStr="Foltz et al., 1998" startWordPosition="3699" endWordPosition="3702">re is of course to detect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a web address, a long string of digits, or a term present in a guide for novice Linux users 6 but not in a large news corpus (Graff, 1995)7. This is a light-weight way to capture one “semantic dimension” or cluster of related words, in a corpus which is not amenable to full LSA or similar techniques. LSA in text corpora yields a better relatedness measure than simple repetition (Foltz et al., 1998), but is ineffective in our corpus because of its wide variety of topics and lack of distinct document boundaries. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related– even if they are part of the same conversation, the link between them is probably a long chain of intervening utterances. Thus, if we run our classifier on a pair of very distant utterances, we expect it to default to the majority class, which in this case will be different, and this will damage our performance in case the two are really part of the same conversation. To deal with </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Fosler-Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8260" citStr="Galley et al., 2003" startWordPosition="1293" endWordPosition="1296">other key difference is that participants may create a new conversation (floor) at any time, a process which Sacks et al. (1974) calls schisming. Dur835 ing a schism, a new conversation is formed, not necessarily because of a shift in the topic, but because certain participants have refocused their attention onto each other, and away from whoever held the floor in the parent conversation. Despite these differences, there are still strong similarities between chat and other conversations such as meetings. Our feature set incorporates information which has proven useful in meeting segmentation (Galley et al., 2003) and the task of detecting addressees of a specific utterance in a meeting (Jovanovic et al., 2006). These include word repetitions, utterance topic, and cue words which can indicate the bounds of a segment. 3 Dataset Our dataset is recorded from the IRC (Internet Relay Chat) channel ##LINUX at freenode.net, using the freely-available gaim client. ##LINUX is an unofficial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 562–569, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<tech>LDC95T21.</tech>
<contexts>
<context position="22383" citStr="Graff, 1995" startWordPosition="3655" endWordPosition="3656">that it is useful here. We bucket repeated words by their unigram probability5 (measured over the entire 52 hours of transcript). The bucketing scheme allows us to deal with “noise words” which are repeated coincidentally. The point of the repetition feature is of course to detect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a web address, a long string of digits, or a term present in a guide for novice Linux users 6 but not in a large news corpus (Graff, 1995)7. This is a light-weight way to capture one “semantic dimension” or cluster of related words, in a corpus which is not amenable to full LSA or similar techniques. LSA in text corpora yields a better relatedness measure than simple repetition (Foltz et al., 1998), but is ineffective in our corpus because of its wide variety of topics and lack of distinct document boundaries. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related– even if they are part of the same conversation, the link between them is probably a long chain of intervening utterances.</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff. 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inc Ilog</author>
</authors>
<date>2003</date>
<note>Cplex solver.</note>
<marker>Ilog, 2003</marker>
<rawString>Ilog, Inc. 2003. Cplex solver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natasa Jovanovic</author>
</authors>
<title>Rieks op den Akker, and Anton Nijholt.</title>
<date>2006</date>
<booktitle>In EACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<marker>Jovanovic, 2006</marker>
<rawString>Natasa Jovanovic, Rieks op den Akker, and Anton Nijholt. 2006. Addressee identification in face-to-face meetings. In EACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In ACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="18565" citStr="Malioutov and Barzilay, 2006" startWordPosition="3007" endWordPosition="3010">e more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 3). The assumption is not very accurate, especially for speakers with more than 10 utterances. 10 9 8 7 6 5 4 3 2 1 00 10 20 30 40 50 60 Utterances Figure 3: Utterances versus conversations participated in per speaker on development data. 4 Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). These algorithms operate in two stages: first, a binary classifier marks each pair of items as alike or different, and second, a consistent partition is extracted.4 4.1 Classification We use a maximum-entropy classifier (Daum´e III, 2004) to decide whether a pair of utterances x and y are in same or different conversations. The most likely class is different, which occurs 57% of the time in development data. We describe the classifier’s performance in terms of raw accuracy (correct decisions / total), precision and recall of the same class, and F-score, the harmonic mean of precision and rec</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In ACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacki O’Neill</author>
<author>David Martin</author>
</authors>
<title>Text chat in action.</title>
<date>2003</date>
<booktitle>In GROUP ’03: Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work,</booktitle>
<pages>40--49</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<marker>O’Neill, Martin, 2003</marker>
<rawString>Jacki O’Neill and David Martin. 2003. Text chat in action. In GROUP ’03: Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work, pages 40–49, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>1--8</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="18431" citStr="Roth and Yih, 2004" startWordPosition="2984" endWordPosition="2987">on. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 3). The assumption is not very accurate, especially for speakers with more than 10 utterances. 10 9 8 7 6 5 4 3 2 1 00 10 20 30 40 50 60 Utterances Figure 3: Utterances versus conversations participated in per speaker on development data. 4 Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). These algorithms operate in two stages: first, a binary classifier marks each pair of items as alike or different, and second, a consistent partition is extracted.4 4.1 Classification We use a maximum-entropy classifier (Daum´e III, 2004) to decide whether a pair of utterances x and y are in same or different conversations. The most likely class is different, which occurs 57% of the time in development data. We describe the classifier’s performance in terms of</context>
<context position="26037" citStr="Roth and Yih (2004)" startWordPosition="4288" endWordPosition="4291">ing makes at least a reasonable starting point for further efforts, since it is a natural online algorithm– it assigns each utterance as it arrives, without reference to the future. At any rate, we should not take our objective function too seriously. Although it is roughly correlated with performance, the high error rate of the classifier makes it unlikely that small changes in objective will mean much. In fact, the objective value of our output solutions are generally higher than those for true so8We set up the problem by taking the weight of edge i, j as the classifier’s decision pij − .5. Roth and Yih (2004) use log probabilities as weights. Bansal et al. (2004) propose the log odds ratio log(p/(1 − p)). We are unsure of the relative merit of these approaches. lutions, which implies we have already reached the limits of what our classifier can tell us. 5 Experiments We annotate the 800 line test transcript using our system. The annotation obtained has 63 conversations, with mean length 12.70. The average density of conversations is 2.9, and the entropy is 3.79. This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum. As a standard of co</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of CoNLL-2004, pages 1–8. Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<pages>735</pages>
<contexts>
<context position="7768" citStr="Sacks et al. (1974)" startWordPosition="1214" endWordPosition="1217">n which multi-party text chat differs from typical two-party conversation. One key difference is the frequency with which participants mention each others’ names. They hypothesize that mentioning is a strategy which participants use to make disentanglement easier, compensating for the lack of cues normally present in face-to-face dialogue. Mentions (such as Gale’s comments to Arlie in figure 1) are very common in our corpus, occurring in 36% of comments, and provide a useful feature. Another key difference is that participants may create a new conversation (floor) at any time, a process which Sacks et al. (1974) calls schisming. Dur835 ing a schism, a new conversation is formed, not necessarily because of a shift in the topic, but because certain participants have refocused their attention onto each other, and away from whoever held the floor in the parent conversation. Despite these differences, there are still strong similarities between chat and other conversations such as meetings. Our feature set incorporates information which has proven useful in meeting segmentation (Galley et al., 2003) and the task of detecting addressees of a specific utterance in a meeting (Jovanovic et al., 2006). These i</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systematics for the organization of turn-taking for conversation. Language, 50(4):696– 735.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>