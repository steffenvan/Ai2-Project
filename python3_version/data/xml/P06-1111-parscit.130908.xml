<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000186">
<title confidence="0.992389">
Prototype-Driven Grammar Induction
</title>
<author confidence="0.994098">
Aria Haghighi
</author>
<affiliation confidence="0.9975835">
Computer Science Division
University of California Berkeley
</affiliation>
<email confidence="0.995387">
aria42@cs.berkeley.edu
</email>
<author confidence="0.998042">
Dan Klein
</author>
<affiliation confidence="0.9985355">
Computer Science Division
University of California Berkeley
</affiliation>
<email confidence="0.998446">
klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911">
We investigate prototype-driven learning for pri-
marily unsupervised grammar induction. Prior
knowledge is specified declaratively, by providing a
few canonical examples of each target phrase type.
This sparse prototype information is then propa-
gated across a corpus using distributional similar-
ity features, which augment an otherwise standard
PCFG model. We show that distributional features
are effective at distinguishing bracket labels, but not
determining bracket locations. To improve the qual-
ity of the induced trees, we combine our PCFG in-
duction with the CCM model of Klein and Manning
(2002), which has complementary stengths: it iden-
tifies brackets but does not label them. Using only
a handful of prototypes, we show substantial im-
provements over naive PCFG induction for English
and Chinese grammar induction.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965327868853">
There has been a great deal of work on unsuper-
vised grammar induction, with motivations rang-
ing from scientific interest in language acquisi-
tion to engineering interest in parser construc-
tion (Carroll and Charniak, 1992; Clark, 2001).
Recent work has successfully induced unlabeled
grammatical structure, but has not successfully
learned labeled tree structure (Klein and Manning,
2002; Klein and Manning, 2004; Smith and Eis-
ner, 2004) .
In this paper, our goal is to build a system capa-
ble of producing labeled parses in a target gram-
mar with as little total effort as possible. We in-
vestigate a prototype-driven approach to grammar
induction, in which one supplies canonical ex-
amples of each target concept. For example, we
might specify that we are interested in trees which
use the symbol NP and then list several examples
of prototypical NPs (determiner noun, pronouns,
etc., see figure 1 for a sample prototype list). This
prototype information is similar to specifying an
annotation scheme, which even human annotators
must be provided before they can begin the con-
struction of a treebank. In principle, prototype-
driven learning is just a kind of semi-supervised
learning. However, in practice, the information we
provide is on the order of dozens of total seed in-
stances, instead of a handful of fully parsed trees,
and is of a different nature.
The prototype-driven approach has three
strengths. First, since we provide a set of target
symbols, we can evaluate induced trees using
standard labeled parsing metrics, rather than the
far more forgiving unlabeled metrics described in,
for example, Klein and Manning (2004). Second,
knowledge is declaratively specified in an inter-
pretable way (see figure 1). If a user of the system
is unhappy with its systematic behavior, they can
alter it by altering the prototype information (see
section 7.1 for examples). Third, and related to
the first two, one does not confuse the ability of
the system to learn a consistent grammar with its
ability to learn the grammar a user has in mind.
In this paper, we present a series of experiments
in the induction of labeled context-free trees us-
ing a combination of unlabeled data and sparse
prototypes. We first affirm the well-known re-
sult that simple, unconstrained PCFG induction
produces grammars of poor quality as measured
against treebank structures. We then augment a
PCFG with prototype features, and show that these
features, when propagated to non-prototype se-
quences using distributional similarity, are effec-
tive at learning bracket labels on fixed unlabeled
trees, but are still not enough to learn good tree
structures without bracketing information. Finally,
we intersect the feature-augmented PCFG with the
CCM model of Klein and Manning (2002), a high-
quality bracketing model. The intersected model
is able to learn trees with higher unlabeled F1 than
those in Klein and Manning (2004). More impor-
</bodyText>
<page confidence="0.973592">
881
</page>
<note confidence="0.531721">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881–888,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999561">
tantly, its trees are labeled and can be evaluated
according to labeled metrics. Against the English
Penn Treebank, our final trees achieve a labeled F1
of 65.1 on short sentences, a 51.7% error reduction
over naive PCFG induction.
</bodyText>
<sectionHeader confidence="0.996505" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99989164">
The majority of our experiments induced tree
structures from the WSJ section of the English
Penn treebank (Marcus et al., 1994), though see
section 7.4 for an experiment on Chinese. To fa-
cilitate comparison with previous work, we ex-
tracted WSJ-10, the 7,422 sentences which con-
tain 10 or fewer words after the removal of punc-
tuation and null elements according to the scheme
detailed in Klein (2005). We learned models on all
or part of this data and compared their predictions
to the manually annotated treebank trees for the
sentences on which the model was trained. As in
previous work, we begin with the part-of-speech
(POS) tag sequences for each sentence rather than
lexical sequences (Carroll and Charniak, 1992;
Klein and Manning, 2002).
Following Klein and Manning (2004), we report
unlabeled bracket precision, recall, and F1. Note
that according to their metric, brackets of size 1
are omitted from the evaluation. Unlike that work,
all of our induction methods produce trees labeled
with symbols which are identified with treebank
categories. Therefore, we also report labeled pre-
cision, recall, and F1, still ignoring brackets of
size 1.1
</bodyText>
<sectionHeader confidence="0.997212" genericHeader="method">
3 Experiments in PCFG induction
</sectionHeader>
<bodyText confidence="0.998217714285714">
As an initial experiment, we used the inside-
outside algorithm to induce a PCFG in the
straightforward way (Lari and Young, 1990; Man-
ning and Sch¨utze, 1999). For all the experiments
in this paper, we considered binary PCFGs over
the nonterminals and terminals occuring in WSJ-
10. The PCFG rules were of the following forms:
</bodyText>
<listItem confidence="0.99835925">
• X —* Y Z, for nonterminal types X, Y, and
Z, with Y =� X or Z =� X
• X —* t Y , X —* Y t, for each terminal t
• X —* t t&apos;, for terminals t and t&apos;
</listItem>
<bodyText confidence="0.5091125">
For a given sentence S, our CFG generates la-
beled trees T over S.2 Each tree consists of binary
</bodyText>
<footnote confidence="0.99231275">
1In cases where multiple gold labels exist in the gold trees,
precision and recall were calculated as in Collins (1999).
2Restricting our CFG to a binary branching grammar re-
sults in an upper bound of 88.1% on unlabeled Fl.
</footnote>
<listItem confidence="0.62751175">
productions X(i, j) —* α over constituent spans
(i, j), where α is a pair of non-terminal and/or
terminal symbols in the grammar. The generative
probability of a tree T for S is:
</listItem>
<equation confidence="0.997754">
PCFG(T, S) = � P(α|X)
X(iJ)→αET
</equation>
<bodyText confidence="0.999978340909091">
In the inside-outside algorithm, we iteratively
compute posterior expectations over production
occurences at each training span, then use those
expectations to re-estimate production probabili-
ties. This process is guaranteed to converge to a
local extremum of the data likelihood, but initial
production probability estimates greatly influence
the final grammar (Carroll and Charniak, 1992). In
particular, uniform initial estimates are an (unsta-
ble) fixed point. The classic approach is to add a
small amount of random noise to the initial prob-
abilities in order to break the symmetry between
grammar symbols.
We randomly initialized 5 grammars using tree-
bank non-terminals and trained each to conver-
gence on the first 2000 sentences of WSJ-10.
Viterbi parses were extracted for each of these
2000 sentences according to each grammar. Of
course, the parses’ symbols have nothing to anchor
them to our intended treebank symbols. That is, an
NP in one of these grammars may correspond to
the target symbol VP, or may not correspond well
to any target symbol. To evaluate these learned
grammars, we must map the models’ phrase types
to target phrase types. For each grammar, we fol-
lowed the common approach of greedily mapping
model symbols to target symbols in the way which
maximizes the labeled F1. Note that this can, and
does, result in mapping multiple model symbols
to the most frequent target symbols. This experi-
ment, labeled PCFG x NONE in figure 4, resulted in
an average labeled F1 of 26.3 and an unlabeled F1
of 45.7. The unlabeled F1 is better than randomly
choosing a tree (34.7), but not better than always
choosing a right branching structure (61.7).
Klein and Manning (2002) suggest that the task
of labeling constituents is significantly easier than
identifying them. Perhaps it is too much to ask
a PCFG induction algorithm to perform both of
these tasks simultaneously. Along the lines of
Pereira and Schabes (1992), we reran the inside-
outside algorithm, but this time placed zero mass
on all trees which did not respect the bracketing
of the gold trees. This constraint does not fully
</bodyText>
<page confidence="0.997018">
882
</page>
<table confidence="0.999420642857143">
Phrase Prototypes Phrase Prototypes
NP DT NN VP VBN IN NN
JJ NNS VBD DT NN
NNP NNP MD VB CD
S PRP VBD DT NN QP CD CD
DT NN VBD IN DT NN RB CD
DT VBZ DT JJ NN DT CD CD
PP IN NN ADJP RB JJ
TO CD CD JJ
IN PRP JJ CC JJ
ADVP RB RB
RB CD
RB CC RB
VP-INF VB NN NP-INF NN POS
</table>
<figureCaption confidence="0.95230225">
Figure 1: English phrase type prototype list man-
ually specified (The entire supervision for our sys-
tem). The second part of the table is additional
prototypes discussed in section 7.1.
</figureCaption>
<bodyText confidence="0.9997504">
eliminate the structural uncertainty since we are
inducing binary trees and the gold trees are flat-
ter than binary in many cases. This approach of
course achieved the upper bound on unlabeled F1,
because of the gold bracket constraints. However,
it only resulted in an average labeled F1 of 52.6
(experiment PCFG x GOLD in figure 4). While this
labeled score is an improvement over the PCFG x
NONE experiment, it is still relatively disappoint-
ing.
</bodyText>
<subsectionHeader confidence="0.997867">
3.1 Encoding Prior Knowledge with
Prototypes
</subsectionHeader>
<bodyText confidence="0.957498353846154">
Clearly, we need to do something more than
adding structural bias (e.g. bracketing informa-
tion) if we are to learn a PCFG in which the sym-
bols have the meaning and behaviour we intend.
How might we encode information about our prior
knowledge or intentions?
Providing labeled trees is clearly an option. This
approach tells the learner how symbols should re-
cursively relate to each other. Another option is to
provide fully linearized yields as prototypes. We
take this approach here, manually creating a list
of POS sequences typical of the 7 most frequent
categories in the Penn Treebank (see figure 1).3
Our grammar is limited to these 7 phrase types
plus an additional type which has no prototypes
and is unconstrained.4 This list grounds each sym-
3A possible objection to this approach is the introduction
of improper reasearcher bias via specifying prototypes. See
section 7.3 for an experiment utilizing an automatically gen-
erated prototype list with comparable results.
4In our experiments we found that adding prototypes for
more categories did not improve performance and took more
bol in terms of an observable portion of the data,
rather than attempting to relate unknown symbols
to other unknown symbols.
Broadly, we would like to learn a grammar
which explains the observed data (EM’s objec-
tive) but also meets our prior expectations or re-
quirements of the target grammar. How might
we use such a list to constrain the learning of
a PCFG with the inside-outside algorithm? We
might require that all occurences of a prototype
sequence, say DT NN, be constituents of the cor-
responding type (NP). However, human-elicited
prototypes are not likely to have the property that,
when they occur, they are (nearly) always con-
stituents. For example, DT NN is a perfectly rea-
sonable example of a noun phrase, but is not a con-
stituent when it is part of a longer DT NN NN con-
stituent. Therefore, when summing over trees with
the inside-outside algorithm, we could require a
weaker property: whenever a prototype sequence
is a constituent it must be given the label specified
in the prototype file.5 This constraint is enough to
break the symmetry between the model labels, and
therefore requires neither random initialization for
training, nor post-hoc mapping of labels for eval-
uation. Adding prototypes in this way and keep-
ing the gold bracket constraint gave 59.9 labeled
F1. The labeled F1 measure is again an improve-
ment over naive PCFG induction, but is perhaps
less than we might expect given that the model has
been given bracketing information and has proto-
types as a form of supervision to direct it.
In response to a prototype, however, we may
wish to conclude something stronger than a con-
straint on that particular POS sequence. We might
hope that sequences which are similar to a proto-
type in some sense are generally given the same
label as that prototype. For example, DT NN is a
noun phrase prototype, the sequence DT JJ NN is
another good candidate for being a noun phrase.
This kind of propagation of constraints requires
that we have a good way of defining and detect-
ing similarity between POS sequences.
</bodyText>
<subsectionHeader confidence="0.998965">
3.2 Phrasal Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.845084375">
A central linguistic argument for constituent types
is substitutability: phrases of the same type appear
time. We note that we still evaluate against all phrase types
regardless of whether or not they are modeled by our gram-
mar.
5Even this property is likely too strong: prototypes may
have multiple possible labels, for example DT NN may also
be a QP in the English treebank.
</bodyText>
<page confidence="0.996914">
883
</page>
<table confidence="0.958391">
Yield Prototype Skew KL Phrase Type Skew KL
DT JJ NN DT NN 0.10 NP 0.39
IN DT VBG NN IN NN 0.24 PP 0.45
DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58
CC NN IN NN 0.43 PP 0.71
MD NNS PRP VBD DT NN 1.43 NONE -
</table>
<figureCaption confidence="0.662876">
Figure 2: Yields along with most similar proto-
types and phrase types, guessed according to (3).
</figureCaption>
<bodyText confidence="0.99981768">
in similar contexts and are mutually substitutable
(Harris, 1954; Radford, 1988). For instance, DT
JJ NN and DT NN occur in similar contexts, and
are indeed both common NPs. This idea has been
repeatedly and successfully operationalized using
various kinds of distributional clustering, where
we define a similarity measure between two items
on the basis of their immediate left and right con-
texts (Sch¨utze, 1995; Clark, 2000; Klein and Man-
ning, 2002).
As in Clark (2001), we characterize the distribu-
tion of a sequence by the distribution of POS tags
occurring to the left and right of that sequence in
a corpus. Each occurence of a POS sequence α
falls in a context x α y, where x and y are the ad-
jacent tags. The distribution over contexts x − y
for a given α is called its signature, and is denoted
by a(α). Note that a(α) is composed of context
counts from all occurences, constitiuent and dis-
tituent, of α. Let Q,(α) denote the context dis-
tribution for α where the context counts are taken
only from constituent occurences of α. For each
phrase type in our grammar, X, define a,(X) to be
the context distribution obtained from the counts
of all constituent occurences of type X:
</bodyText>
<equation confidence="0.98155">
U,(X) =F-p(αJX) U,(α) (1)
</equation>
<bodyText confidence="0.999848">
where p(α|X) is the distribution of yield types for
phrase type X. We compare context distributions
using the skewed KL divergence:
</bodyText>
<equation confidence="0.997912">
DSKL(p,q) = DKL(pk7p + (1 − y)q)
</equation>
<bodyText confidence="0.999953">
where -y controls how much of the source distribu-
tions is mixed in with the target distribution.
A reasonable baseline rule for classifying the
phrase type of a POS yield is to assign it to the
phrase from which it has minimal divergence:
</bodyText>
<equation confidence="0.99921">
type(α) = arg min
X DSKL(a,(α), a,(X)) (2)
</equation>
<bodyText confidence="0.999669333333333">
However, this rule is not always accurate, and,
moreover, we do not have access to Q,(α) or
Q,(X). We chose to approximate Q,(X) us-
ing the prototype yields for X as samples from
p(α|X). Letting proto(X) denote the (few) pro-
totype yields for phrase type X, we define Q(X):
</bodyText>
<equation confidence="0.99943325">
Q(X) = oto F-
1
( )I αEproto(X)
X
</equation>
<bodyText confidence="0.998489333333333">
Note Q(X) is an approximation to (1) in sev-
eral ways. We have replaced an expectation over
p(α|X) with a uniform weighting of proto(X),
and we have replaced Q,(α) with a(α) for each
term in that expectation. Because of this, we will
rely only on high confidence guesses, and allow
yields to be given a NONE type if their divergence
from each &amp;(X) exceeds a fixed threshold t. This
gives the following alternative to (2):
</bodyText>
<equation confidence="0.9882575">
type(α) = (3)
�
</equation>
<bodyText confidence="0.9975714">
NONE, if minX DSKL(Q(α), &amp;(X)) &lt; t
arg minX DSKL(u(α), Q(X)), otherwise
We built a distributional model implementing
the rule in (3) by constructing a(α) from context
counts in the WSJ portion of the Penn Treebank
as well as the BLIPP corpus. Each Q(X) was ap-
proximated by a uniform mixture of a(α) for each
of X’s prototypes α listed in figure 1.
This method of classifying constituents is very
precise if the threshold is chosen conservatively
enough. For instance, using a threshold of t =
0.75 and -y = 0.1, this rule correctly classifies the
majority label of a constituent-type with 83% pre-
cision, and has a recall of 23% over constituent
types. Figure 2 illustrates some sample yields, the
prototype sequence to which it is least divergent,
and the output of rule (3).
We incorporated this distributional information
into our PCFG induction scheme by adding a pro-
totype feature over each span (i, j) indicating the
output of (3) for the yield α in that span. Asso-
ciated with each sentence 5 is a feature map F
specifying, for each (i, j), a prototype feature pij.
These features are generated using an augmented
CFG model, CFG+, given by:6
</bodyText>
<equation confidence="0.99671725">
PCFG+(T, F) = H P(pij|X)P(α|X)
X(iJ)→αET
H= OCFG+(X → α,pij)
X(iJ)→αET
</equation>
<bodyText confidence="0.99434">
6Technically, all features in F must be generated for each
assignment to T, which means that there should be terms in
this equation for the prototype features on distituent spans.
However, we fixed the prototype distribution to be uniform
for distituent spans so that the equation is correct up to a con-
stant depending on F.
</bodyText>
<equation confidence="0.576841">
a(α)
</equation>
<page confidence="0.765546">
884
</page>
<figure confidence="0.900558">
in November
</figure>
<figureCaption confidence="0.997123">
Figure 3: Illustration of PCFG augmented with
prototype similarity features.
</figureCaption>
<bodyText confidence="0.94300975">
where OCFG+(X → α, pij) is the local factor for
placing X → α on a span with prototype feature
pij. An example is given in figure 3.
For our experiments, we fixed P(pij|X) to be:
</bodyText>
<equation confidence="0.927749">
�
0.60, if pij = X
P(pij|X) =
uniform, otherwise
</equation>
<bodyText confidence="0.999921230769231">
Modifying the model in this way, and keeping the
gold bracketing information, gave 71.1 labeled F1
(see experiment PROTO × GOLD in figure 4), a
40.3% error reduction over naive PCFG induction
in the presence of gold bracketing information.
We note that the our labeled F1 is upper-bounded
by 86.0 due to unary chains and more-than-binary
configurations in the treebank that cannot be ob-
tained from our binary grammar.
We conclude that in the presence of gold bracket
information, we can achieve high labeled accu-
racy by using a CFG augmented with distribu-
tional prototype features.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="method">
4 Constituent Context Model
</sectionHeader>
<bodyText confidence="0.999954555555556">
So far, we have shown that, given perfect per-
fect bracketing information, distributional proto-
type features allow us to learn tree structures with
fairly accurate labels. However, such bracketing
information is not available in the unsupervised
case.
Perhaps we don’t actually need bracketing con-
straints in the presence of prototypes and distri-
butional similarity features. However this exper-
iment, labeled PROTO × NONE in figure 4, gave
only 53.1 labeled F1 (61.1 unlabeled), suggesting
that some amount of bracketing constraint is nec-
essary to achieve high performance.
Fortunately, there are unsupervised systems
which can induce unlabeled bracketings with rea-
sonably high accuracy. One such model is
the constituent-context model (CCM) of Klein
and Manning (2002), a generative distributional
model. For a given sentence 5, the CCM generates
a bracket matrix, B, which for each span (i, j), in-
dicates whether or not it is a constituent (Bij = c)
or a distituent (Bij = d). In addition, it generates
a feature map F0, which for each span (i, j) in 5
specifies a pair of features, F0ij = (yij, cij), where
yij is the POS yield of the span, and cij is the con-
text of the span, i.e identity of the conjoined left
and right POS tags:
</bodyText>
<equation confidence="0.989049">
PCCM(B, F0) = P(B) 11 P(yij|Bij)P(cij|Bij)
(i�j)
</equation>
<bodyText confidence="0.999863555555556">
The distribution P(B) only places mass on brack-
etings which correspond to binary trees. We
can efficiently compute PCCM(B, F0) (up to
a constant) depending on F0 using local fac-
tors OCCM(yij, cij) which decomposes over con-
stituent spans:7
The CCM by itself yields an unlabeled F1 of 71.9
on WSJ-10, which is reasonably high, but does not
produce labeled trees.
</bodyText>
<sectionHeader confidence="0.980979" genericHeader="method">
5 Intersecting CCM and PCFG
</sectionHeader>
<bodyText confidence="0.9999731875">
The CCM and PCFG models provide complemen-
tary views of syntactic structure. The CCM explic-
itly learns the non-recursive contextual and yield
properties of constituents and distituents. The
PCFG model, on the other hand, does not explic-
itly model properties of distituents but instead fo-
cuses on modeling the hierarchical and recursive
properties of natural language syntax. One would
hope that modeling both of these aspects simulta-
neously would improve the overall quality of our
induced grammar.
We therefore combine the CCM with our feature-
augmented PCFG, denoted by PROTO in exper-
iment names. When we run EM on either of
the models alone, at each iteration and for each
training example, we calculate posteriors over that
</bodyText>
<equation confidence="0.9399800625">
7Klein (2005) gives a full presentation.
IN
NN
VP( P(VBD PP|VP)
P(l P=(VP|VP)
✦✦❛❛❛PP ( P(IN NN|PP)
Pl (P = PP|PP)
✦ ❛❛❛
✦ ✦
P(NN NNS|NP)
P(P = NP|NP)
1 NP
J�
P(S|ROOT) } ROOT
S P(NP VP|S)
{ P(P = NONE|S)
</equation>
<figure confidence="0.940353916666667">
✘✘✘✘✘ ❳❳❳❳❳
NN
NNN
VBD
fell
payrolls
Factory
11 P(yij|c)P(cij|c)
PCCM(B, F0) ∝ P(yij|d)P(cij|d)
(iJ):Bzj=c
11 = OCCM(yij, cij)
(iJ):Bzj=c
</figure>
<page confidence="0.996579">
885
</page>
<bodyText confidence="0.999745117647059">
model’s latent variables. For CCM, the latent vari-
able is a bracketing matrix B (equivalent to an un-
labeled binary tree), while for the CFG+ the latent
variable is a labeled tree T. While these latent
variables aren’t exactly the same, there is a close
relationship between them. A bracketing matrix
constrains possible labeled trees, and a given la-
beled tree determines a bracketing matrix. One
way to combine these models is to encourage both
models to prefer latent variables which are com-
patible with each other.
Similar to the approach of Klein and Manning
(2004) on a different model pair, we intersect CCM
and CFG+ by multiplying their scores for any la-
beled tree. For each possible labeled tree over a
sentence S, our generative model for a labeled tree
T is given as follows:
</bodyText>
<equation confidence="0.7570735">
P(T, F, F&apos;) = (4)
PCFG+(T, F)PCCM(B(T), F&apos;)
</equation>
<bodyText confidence="0.999967666666667">
where B(T) corresponds to the bracketing ma-
trix determined by T. The EM algorithm for the
product model will maximize:
</bodyText>
<equation confidence="0.99530175">
P(S,F, F&apos;) = 1: PCCM(B, F&apos;)PCFG+(T, F)
TET (S)
=1: PCCM(B, F&apos;) 1: PCFG+(T, F)
B TET(B,S)
</equation>
<bodyText confidence="0.9999874">
where T (S) is the set of labeled trees consistent
with the sentence S and T (B, S) is the set of la-
beled trees consistent with the bracketing matrix
B and the sentence S. Notice that this quantity in-
creases as the CCM and CFG+ models place proba-
bility mass on compatible latent structures, giving
an intuitive justification for the success of this ap-
proach.
We can compute posterior expectations over
(B, T) in the combined model (4) using a variant
of the inside-outside algorithm. The local factor
for a binary rule r = X -* Y Z, over span (i, j),
with CCM features F&apos;ij = (yij, cij) and prototype
feature pij, is given by the product of local factors
for the CCM and CFG+ models:
</bodyText>
<equation confidence="0.687167">
φ(r, (i, j)) = φCCM(yij,cij)φCFG+(r,pij)
</equation>
<bodyText confidence="0.9988435">
From these local factors, the inside-outside al-
gorithm produces expected counts for each binary
rule, r, over each span (i, j) and split point k, de-
noted by P(r, (i, j), k|S, F, F&apos;). These posteriors
are sufficient to re-estimate all of our model pa-
rameters.
</bodyText>
<table confidence="0.999319571428571">
Labeled Unlabeled
Setting Prec. Rec. F1 Prec. Rec. F1
No Brackets
PCFG x NONE 23.9 29.1 26.3 40.7 52.1 45.7
PROTO x NONE 51.8 62.9 56.8 59.6 76.2 66.9
Gold Brackets
PCFG x GOLD 47.0 57.2 51.6 78.8 100.0 88.1
PROTO x GOLD 64.8 78.7 71.1 78.8 100.0 88.1
CCM Brackets
CCM - - - 64.2 81.6 71.9
PCFG x CCM 32.3 38.9 35.3 64.1 81.4 71.8
PROTO x CCM 56.9 68.5 62.2 68.4 86.9 76.5
BEST 59.4 72.1 65.1 69.7 89.1 78.2
UBOUND 78.8 94.7 86.0 78.8 100.0 88.1
</table>
<figureCaption confidence="0.955541666666667">
Figure 4: English grammar induction results. The
upper bound on labeled recall is due to unary
chains.
</figureCaption>
<sectionHeader confidence="0.805278" genericHeader="method">
6 CCM as a Bracketer
</sectionHeader>
<bodyText confidence="0.997250434782609">
We tested the product model described in sec-
tion 5 on WSJ-10 under the same conditions as
in section 3. Our initial experiment utilizes no
protoype information, random initialization, and
greedy remapping of its labels. This experiment,
PCFG x CCM in figure 4, gave 35.3 labeled F1,
compared to the 51.6 labeled F1 with gold brack-
eting information (PCFG x GOLD in figure 4).
Next we added the manually specified proto-
types in figure 1, and constrained the model to give
these yields their labels if chosen as constituents.
This experiment gave 48.9 labeled F1 (73.3 unla-
beled). The error reduction is 21.0% labeled (5.3%
unlabeled) over PCFG x CCM.
We then experimented with adding distributional
prototype features as discussed in section 3.2 us-
ing a threshold of 0.75 and γ = 0.1. This experi-
ment, PROTO x CCM in figure 4, gave 62.2 labeled
F1 (76.5 unlabeled). The error reduction is 26.0%
labeled (12.0% unlabeled) over the experiment us-
ing prototypes without the similarity features. The
overall error reduction from PCFG x CCM is 41.6%
(16.7%) in labeled (unlabeled) F1.
</bodyText>
<sectionHeader confidence="0.997323" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999333875">
The most common type of error by our PROTO x
CCM system was due to the binary grammar re-
striction. For instance common NPs, such as DT JJ
NN, analyzed as [NP DT [NP JJ NN] ], which pro-
poses additional N constituents compared to the
flatter treebank analysis. This discrepancy greatly,
and perhaps unfairly, damages NP precision (see
figure 6). However, this is error is unavoidable
</bodyText>
<page confidence="0.993809">
886
</page>
<figure confidence="0.999907084033614">
boast
NP
� ���
� �
� PP PPP
� �
S
���� ����
NP VP
�� ��� �����
MD
NNP
VP
����� �� �������
NP
France
can
VB
��� � � �����
NN
POS
lion
’s
NP
� ���
� �
DT
the
IN
NN
JJ
share
NP
�� � ���
NNS
of
bottles
high-priced
France
PP
�� � � ����
VP
��� �� �����
the
lion
’s
share
S
�� ������� ���������
NNP VP
����� � �� ��������
can
boast
DT
NN
POS
NN
high-priced
bottles
IN
NP
� ���
� �
NP
�� � ���
VP
��
� �
MD
JJ
NNS
VB
of
NP
� �
PP
��
� �
NNP
VP
IN
MD
NP
�� � ���
VP
���� ����
NN
boast
bottles
NP
� ��
�
high-priced
DT
NP
� � ��
share
the
NN
POS
S
������� � ��������
JJ
NNS
can
VB
of
NP
��� � ����
��������� ���������
France
PP
��� � ����
VP
��� � ����
lion
lion ’s
’s
a) b) c)
</figure>
<figureCaption confidence="0.94048975">
Figure 5: Examples of corrections from adding VP-INF and NP-POS prototype categories. The tree in (a)
is the Treebank parse, (b) is the parse with PROTO x CCM model, and c) is the parse with the BEST model
(added prototype categories), which fixes the possesive NP and infinitival VP problems, but not the PP
attachment.
</figureCaption>
<bodyText confidence="0.9937965">
given our grammar restriction.
Figure 5(b) demonstrates three other errors. Pos-
sessive NPs are analyzed as [NP NN [PP POS NN ]
], with the POS element treated as a preposition
and the possessed NP as its complement. While
labeling the POS NN as a PP is clearly incorrect,
placing a constituent over these elements is not
unreasonable and in fact has been proposed by
some linguists (Abney, 1987). Another type of
error also reported by Klein and Manning (2002)
is MD VB groupings in infinitival VPs also some-
times argued by linguists (Halliday, 2004). More
seriously, prepositional phrases are almost always
attached “high” to the verb for longer NPs.
</bodyText>
<subsectionHeader confidence="0.992798">
7.1 Augmenting Prototypes
</subsectionHeader>
<bodyText confidence="0.9999986875">
One of the advantages of the prototype driven ap-
proach, over a fully unsupervised approach, is the
ability to refine or add to the annotation specifica-
tion if we are not happy with the output of our sys-
tem. We demonstrate this flexibility by augment-
ing the prototypes in figure 1 with two new cate-
gories NP-POS and VP-INF, meant to model pos-
sessive noun phrases and infinitival verb phrases,
which tend to have slightly different distributional
properties from normal NPs and VPs. These new
sub-categories are used during training and then
stripped in post-processing. This prototype list
gave 65.1 labeled F1 (78.2 unlabeled). This exper-
iment is labeled BEST in figure 4. Looking at the
CFG-learned rules in figure 7, we see that the basic
structure of the treebank grammar is captured.
</bodyText>
<subsectionHeader confidence="0.999547">
7.2 Parsing with only the PCFG
</subsectionHeader>
<bodyText confidence="0.999731">
In order to judge how well the PCFG component
of our model did in isolation, we experimented
with training our BEST model with the CCM com-
ponent, but dropping it at test time. This experi-
</bodyText>
<table confidence="0.999372875">
Label Prec. Rec. F1
S 79.3 80.0 79.7
NP 49.0 74.4 59.1
VP 80.4 73.3 76.7
PP 45.6 78.6 57.8
QP 36.2 78.8 49.6
ADJP 29.4 33.3 31.2
ADVP 25.0 12.2 16.4
</table>
<figureCaption confidence="0.725559">
Figure 6: Precision, recall, and F1 for individual
phrase types in the BEST model
</figureCaption>
<table confidence="0.998683625">
Rule Probability Rule Probability
S-NPVP 0.51 VP - VBZ NP 0.20
S - PRP VP 0.13 VP - VBD NP 0.15
S - NNP VP 0.06 VP - VBP NP 0.09
S - NNS VP 0.05 VP - VB NP 0.08
NP - DT NN 0.12 ROOT - S 0.95
NP - NP PP 0.09 ROOT - NP 0.05
NP - NNP NNP 0.09
NP - JJ NN 0.07
PP - IN NP 0.37 QP - CD CD 0.35
PP - CC NP 0.06 QP - CD NN 0.30
PP - TO VP 0.05 QP - QP PP 0.10
PP - TO QP 0.04 QP - QP NNS 0.05
ADJP - RB VBN 0.37 ADVP - RB RB 0.25
ADJP - RB JJ 0.31 ADVP - ADJP PRP 0.15
ADJP - RBR JJ 0.09 ADVP - RB CD 0.10
</table>
<figureCaption confidence="0.997698">
Figure 7: Top PCFG Rules learned by BEST model
</figureCaption>
<bodyText confidence="0.9113985">
ment gave 65.1 labeled F1 (76.8 unlabeled). This
demonstrates that while our PCFG performance
degrades without the CCM, it can be used on its
own with reasonable accuracy.
</bodyText>
<subsectionHeader confidence="0.99852">
7.3 Automatically Generated Prototypes
</subsectionHeader>
<bodyText confidence="0.9999865">
There are two types of bias which enter into the
creation of prototypes lists. One of them is the
bias to choose examples which reflect the annota-
tion semantics we wish our model to have. The
second is the iterative change of prototypes in or-
der to maximize F1. Whereas the first is appro-
</bodyText>
<page confidence="0.988782">
887
</page>
<bodyText confidence="0.999996090909091">
priate, indeed the point, the latter is not. In or-
der to guard against the second type of bias, we
experimented with automatically extracted gener-
ated prototype lists which would not be possible
without labeled data. For each phrase type cat-
egory, we extracted the three most common yield
associated with that category that differed in either
first or last POS tag. Repeating our PROTO x CCM
experiment with this list yielded 60.9 labeled F1
(76.5 unlabeled), comparable to the performance
of our manual prototype list.
</bodyText>
<subsectionHeader confidence="0.992368">
7.4 Chinese Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999995619047619">
In order to demonstrate that our system is some-
what language independent, we tested our model
on CTB-10, the 2,437 sentences of the Chinese
Treebank (Ircs, 2002) of length at most 10 af-
ter punctuation is stripped. Since the authors
have no expertise in Chinese, we automatically ex-
tracted prototypes in the same way described in
section 7.3. Since we did not have access to a large
auxiliary POS tagged Chinese corpus, our distri-
butional model was built only from the treebank
text, and the distributional similarities are presum-
ably degraded relative to the English. Our PCFG
x CCM experiment gave 18.0 labeled F1 (43.4 un-
labeled). The PROTO x CCM model gave 39.0 la-
beled F1 (53.2 unlabeled). Presumably with ac-
cess to more POS tagged data, and the expertise of
a Chinese speaker, our system would see increased
performance. It is worth noting that our unlabeled
F1 of 53.2 is the best reported from a primarily
unsupervised system, with the next highest figure
being 46.7 reported by Klein and Manning (2004).
</bodyText>
<sectionHeader confidence="0.998288" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999950538461538">
We have shown that distributional prototype fea-
tures can allow one to specify a target labeling
scheme in a compact and declarative way. These
features give substantial error reduction in labeled
F1 measure for English and Chinese grammar in-
duction. They also achieve the best reported un-
labeled F1 measure.8 Another positive property
of this approach is that it tries to reconcile the
success of distributional clustering approaches to
grammar induction (Clark, 2001; Klein and Man-
ning, 2002), with the CFG tree models in the su-
pervised literature (Collins, 1999). Most impor-
tantly, this is the first work, to the authors’ knowl-
</bodyText>
<footnote confidence="0.838874">
8The next highest results being 77.1 and 46.7 for English
and Chinese respectively from Klein and Manning (2004).
</footnote>
<bodyText confidence="0.999914125">
edge, which has learned CFGs in an unsupervised
or semi-supervised setting and can parse natural
language language text with any reasonable accu-
racy.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a Microsoft / CITRIS grant
and by an equipment donation from Intel.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999472510204081">
Stephen P. Abney. 1987. The English Noun Phrase in its
Sentential Aspect. Ph.D. thesis, MIT.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. Technical Report CS-92-16.
Alexander Clark. 2000. Inducing syntactic categories by con-
text distribution clustering. In CoNLL, pages 91–94, Lis-
bon, Portugal.
Alexander Clark. 2001. The unsupervised induction of
stochastic context-free grammars using distributional clus-
tering. In CoNLL.
Michael Collins. 1999. The Unsupervised learning ofNatural
Language Structure. Ph.D. thesis, University of Rochester.
M.A.K Halliday. 2004. An introduction to functional gram-
mar. Edward Arnold, 2nd edition.
Zellig Harris. 1954. Distributional Structure. University of
Chicago Press, Chicago.
Nianwen Xue Ircs. 2002. Building a large-scale annotated
chinese corpus.
Dan Klein and Christopher Manning. 2002. A generative
constituent-context model for improved grammar induc-
tion. In ACL.
Dan Klein and Christopher Manning. 2004. Corpus-based
induction of syntactic structure: Models of dependency and
constituency. In ACL.
Dan Klein. 2005. The unsupervised learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
Karim Lari and Steve Young. 1990. The estimation of
stochastic context-free grammars using the insideoutside
algorithm. Computer Speech and Language, 2(4):35–56.
Christopher D. Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313–330.
Fernando C. N. Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora. In
Meeting of the Association for Computational Linguistics,
pages 128–135.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging.
In EACL.
Noah A. Smith and Jason Eisner. 2004. Guiding unsuper-
vised grammar induction using contrastive estimation. In
Working notes of the IJCAI workshop on Grammatical In-
ference Applications.
</reference>
<page confidence="0.99756">
888
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965513">
<title confidence="0.999821">Prototype-Driven Grammar Induction</title>
<author confidence="0.999776">Aria Haghighi</author>
<affiliation confidence="0.9999635">Computer Science Division University of California Berkeley</affiliation>
<email confidence="0.998357">aria42@cs.berkeley.edu</email>
<author confidence="0.999971">Dan Klein</author>
<affiliation confidence="0.999913">Computer Science Division University of California Berkeley</affiliation>
<email confidence="0.999786">klein@cs.berkeley.edu</email>
<abstract confidence="0.998051333333333">investigate for primarily unsupervised grammar induction. Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features effective at distinguishing bracket but not bracket To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen P Abney</author>
</authors>
<title>The English Noun Phrase in its Sentential Aspect.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="26604" citStr="Abney, 1987" startWordPosition="4610" endWordPosition="4611"> the Treebank parse, (b) is the parse with PROTO x CCM model, and c) is the parse with the BEST model (added prototype categories), which fixes the possesive NP and infinitival VP problems, but not the PP attachment. given our grammar restriction. Figure 5(b) demonstrates three other errors. Possessive NPs are analyzed as [NP NN [PP POS NN ] ], with the POS element treated as a preposition and the possessed NP as its complement. While labeling the POS NN as a PP is clearly incorrect, placing a constituent over these elements is not unreasonable and in fact has been proposed by some linguists (Abney, 1987). Another type of error also reported by Klein and Manning (2002) is MD VB groupings in infinitival VPs also sometimes argued by linguists (Halliday, 2004). More seriously, prepositional phrases are almost always attached “high” to the verb for longer NPs. 7.1 Augmenting Prototypes One of the advantages of the prototype driven approach, over a fully unsupervised approach, is the ability to refine or add to the annotation specification if we are not happy with the output of our system. We demonstrate this flexibility by augmenting the prototypes in figure 1 with two new categories NP-POS and VP</context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>Stephen P. Abney. 1987. The English Noun Phrase in its Sentential Aspect. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical Report CS-92-16.</tech>
<contexts>
<context position="1290" citStr="Carroll and Charniak, 1992" startWordPosition="180" endWordPosition="183">t labels, but not determining bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (Klein and Manning, 2002; Klein and Manning, 2004; Smith and Eisner, 2004) . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible. We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept. For example, we might specify that we are interested in trees which use the symbol NP and then list several e</context>
<context position="5153" citStr="Carroll and Charniak, 1992" startWordPosition="803" endWordPosition="806">Penn treebank (Marcus et al., 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992; Klein and Manning, 2002). Following Klein and Manning (2004), we report unlabeled bracket precision, recall, and F1. Note that according to their metric, brackets of size 1 are omitted from the evaluation. Unlike that work, all of our induction methods produce trees labeled with symbols which are identified with treebank categories. Therefore, we also report labeled precision, recall, and F1, still ignoring brackets of size 1.1 3 Experiments in PCFG induction As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way (Lari and Young, 1990; Manni</context>
<context position="7011" citStr="Carroll and Charniak, 1992" startWordPosition="1124" endWordPosition="1127">of 88.1% on unlabeled Fl. productions X(i, j) —* α over constituent spans (i, j), where α is a pair of non-terminal and/or terminal symbols in the grammar. The generative probability of a tree T for S is: PCFG(T, S) = � P(α|X) X(iJ)→αET In the inside-outside algorithm, we iteratively compute posterior expectations over production occurences at each training span, then use those expectations to re-estimate production probabilities. This process is guaranteed to converge to a local extremum of the data likelihood, but initial production probability estimates greatly influence the final grammar (Carroll and Charniak, 1992). In particular, uniform initial estimates are an (unstable) fixed point. The classic approach is to add a small amount of random noise to the initial probabilities in order to break the symmetry between grammar symbols. We randomly initialized 5 grammars using treebank non-terminals and trained each to convergence on the first 2000 sentences of WSJ-10. Viterbi parses were extracted for each of these 2000 sentences according to each grammar. Of course, the parses’ symbols have nothing to anchor them to our intended treebank symbols. That is, an NP in one of these grammars may correspond to the</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical Report CS-92-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In CoNLL,</booktitle>
<pages>91--94</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="13950" citStr="Clark, 2000" startWordPosition="2338" endWordPosition="2339"> NNS PRP VBD DT NN 0.54 S 0.58 CC NN IN NN 0.43 PP 0.71 MD NNS PRP VBD DT NN 1.43 NONE - Figure 2: Yields along with most similar prototypes and phrase types, guessed according to (3). in similar contexts and are mutually substitutable (Harris, 1954; Radford, 1988). For instance, DT JJ NN and DT NN occur in similar contexts, and are indeed both common NPs. This idea has been repeatedly and successfully operationalized using various kinds of distributional clustering, where we define a similarity measure between two items on the basis of their immediate left and right contexts (Sch¨utze, 1995; Clark, 2000; Klein and Manning, 2002). As in Clark (2001), we characterize the distribution of a sequence by the distribution of POS tags occurring to the left and right of that sequence in a corpus. Each occurence of a POS sequence α falls in a context x α y, where x and y are the adjacent tags. The distribution over contexts x − y for a given α is called its signature, and is denoted by a(α). Note that a(α) is composed of context counts from all occurences, constitiuent and distituent, of α. Let Q,(α) denote the context distribution for α where the context counts are taken only from constituent occuren</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In CoNLL, pages 91–94, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>The unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1304" citStr="Clark, 2001" startWordPosition="184" endWordPosition="185">g bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (Klein and Manning, 2002; Klein and Manning, 2004; Smith and Eisner, 2004) . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible. We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept. For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of pro</context>
<context position="13996" citStr="Clark (2001)" startWordPosition="2347" endWordPosition="2348">43 PP 0.71 MD NNS PRP VBD DT NN 1.43 NONE - Figure 2: Yields along with most similar prototypes and phrase types, guessed according to (3). in similar contexts and are mutually substitutable (Harris, 1954; Radford, 1988). For instance, DT JJ NN and DT NN occur in similar contexts, and are indeed both common NPs. This idea has been repeatedly and successfully operationalized using various kinds of distributional clustering, where we define a similarity measure between two items on the basis of their immediate left and right contexts (Sch¨utze, 1995; Clark, 2000; Klein and Manning, 2002). As in Clark (2001), we characterize the distribution of a sequence by the distribution of POS tags occurring to the left and right of that sequence in a corpus. Each occurence of a POS sequence α falls in a context x α y, where x and y are the adjacent tags. The distribution over contexts x − y for a given α is called its signature, and is denoted by a(α). Note that a(α) is composed of context counts from all occurences, constitiuent and distituent, of α. Let Q,(α) denote the context distribution for α where the context counts are taken only from constituent occurences of α. For each phrase type in our grammar,</context>
<context position="31214" citStr="Clark, 2001" startWordPosition="5447" endWordPosition="5448"> best reported from a primarily unsupervised system, with the next highest figure being 46.7 reported by Klein and Manning (2004). 8 Conclusion We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way. These features give substantial error reduction in labeled F1 measure for English and Chinese grammar induction. They also achieve the best reported unlabeled F1 measure.8 Another positive property of this approach is that it tries to reconcile the success of distributional clustering approaches to grammar induction (Clark, 2001; Klein and Manning, 2002), with the CFG tree models in the supervised literature (Collins, 1999). Most importantly, this is the first work, to the authors’ knowl8The next highest results being 77.1 and 46.7 for English and Chinese respectively from Klein and Manning (2004). edge, which has learned CFGs in an unsupervised or semi-supervised setting and can parse natural language language text with any reasonable accuracy. Acknowledgments We would like to thank the anonymous reviewers for their comments. This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Inte</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. The unsupervised induction of stochastic context-free grammars using distributional clustering. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>The Unsupervised learning ofNatural Language Structure.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="6305" citStr="Collins (1999)" startWordPosition="1015" endWordPosition="1016">a PCFG in the straightforward way (Lari and Young, 1990; Manning and Sch¨utze, 1999). For all the experiments in this paper, we considered binary PCFGs over the nonterminals and terminals occuring in WSJ10. The PCFG rules were of the following forms: • X —* Y Z, for nonterminal types X, Y, and Z, with Y =� X or Z =� X • X —* t Y , X —* Y t, for each terminal t • X —* t t&apos;, for terminals t and t&apos; For a given sentence S, our CFG generates labeled trees T over S.2 Each tree consists of binary 1In cases where multiple gold labels exist in the gold trees, precision and recall were calculated as in Collins (1999). 2Restricting our CFG to a binary branching grammar results in an upper bound of 88.1% on unlabeled Fl. productions X(i, j) —* α over constituent spans (i, j), where α is a pair of non-terminal and/or terminal symbols in the grammar. The generative probability of a tree T for S is: PCFG(T, S) = � P(α|X) X(iJ)→αET In the inside-outside algorithm, we iteratively compute posterior expectations over production occurences at each training span, then use those expectations to re-estimate production probabilities. This process is guaranteed to converge to a local extremum of the data likelihood, but</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. The Unsupervised learning ofNatural Language Structure. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An introduction to functional grammar. Edward Arnold, 2nd edition.</title>
<date>2004</date>
<contexts>
<context position="26759" citStr="Halliday, 2004" startWordPosition="4636" endWordPosition="4637">esive NP and infinitival VP problems, but not the PP attachment. given our grammar restriction. Figure 5(b) demonstrates three other errors. Possessive NPs are analyzed as [NP NN [PP POS NN ] ], with the POS element treated as a preposition and the possessed NP as its complement. While labeling the POS NN as a PP is clearly incorrect, placing a constituent over these elements is not unreasonable and in fact has been proposed by some linguists (Abney, 1987). Another type of error also reported by Klein and Manning (2002) is MD VB groupings in infinitival VPs also sometimes argued by linguists (Halliday, 2004). More seriously, prepositional phrases are almost always attached “high” to the verb for longer NPs. 7.1 Augmenting Prototypes One of the advantages of the prototype driven approach, over a fully unsupervised approach, is the ability to refine or add to the annotation specification if we are not happy with the output of our system. We demonstrate this flexibility by augmenting the prototypes in figure 1 with two new categories NP-POS and VP-INF, meant to model possessive noun phrases and infinitival verb phrases, which tend to have slightly different distributional properties from normal NPs </context>
</contexts>
<marker>Halliday, 2004</marker>
<rawString>M.A.K Halliday. 2004. An introduction to functional grammar. Edward Arnold, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure.</title>
<date>1954</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="13588" citStr="Harris, 1954" startWordPosition="2280" endWordPosition="2281">luate against all phrase types regardless of whether or not they are modeled by our grammar. 5Even this property is likely too strong: prototypes may have multiple possible labels, for example DT NN may also be a QP in the English treebank. 883 Yield Prototype Skew KL Phrase Type Skew KL DT JJ NN DT NN 0.10 NP 0.39 IN DT VBG NN IN NN 0.24 PP 0.45 DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58 CC NN IN NN 0.43 PP 0.71 MD NNS PRP VBD DT NN 1.43 NONE - Figure 2: Yields along with most similar prototypes and phrase types, guessed according to (3). in similar contexts and are mutually substitutable (Harris, 1954; Radford, 1988). For instance, DT JJ NN and DT NN occur in similar contexts, and are indeed both common NPs. This idea has been repeatedly and successfully operationalized using various kinds of distributional clustering, where we define a similarity measure between two items on the basis of their immediate left and right contexts (Sch¨utze, 1995; Clark, 2000; Klein and Manning, 2002). As in Clark (2001), we characterize the distribution of a sequence by the distribution of POS tags occurring to the left and right of that sequence in a corpus. Each occurence of a POS sequence α falls in a con</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional Structure. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue Ircs</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<contexts>
<context position="29883" citStr="Ircs, 2002" startWordPosition="5226" endWordPosition="5227">xperimented with automatically extracted generated prototype lists which would not be possible without labeled data. For each phrase type category, we extracted the three most common yield associated with that category that differed in either first or last POS tag. Repeating our PROTO x CCM experiment with this list yielded 60.9 labeled F1 (76.5 unlabeled), comparable to the performance of our manual prototype list. 7.4 Chinese Grammar Induction In order to demonstrate that our system is somewhat language independent, we tested our model on CTB-10, the 2,437 sentences of the Chinese Treebank (Ircs, 2002) of length at most 10 after punctuation is stripped. Since the authors have no expertise in Chinese, we automatically extracted prototypes in the same way described in section 7.3. Since we did not have access to a large auxiliary POS tagged Chinese corpus, our distributional model was built only from the treebank text, and the distributional similarities are presumably degraded relative to the English. Our PCFG x CCM experiment gave 18.0 labeled F1 (43.4 unlabeled). The PROTO x CCM model gave 39.0 labeled F1 (53.2 unlabeled). Presumably with access to more POS tagged data, and the expertise o</context>
</contexts>
<marker>Ircs, 2002</marker>
<rawString>Nianwen Xue Ircs. 2002. Building a large-scale annotated chinese corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="834" citStr="Klein and Manning (2002)" startWordPosition="109" endWordPosition="112">.berkeley.edu Abstract We investigate prototype-driven learning for primarily unsupervised grammar induction. Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features are effective at distinguishing bracket labels, but not determining bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (K</context>
<context position="3827" citStr="Klein and Manning (2002)" startWordPosition="589" endWordPosition="592">rees using a combination of unlabeled data and sparse prototypes. We first affirm the well-known result that simple, unconstrained PCFG induction produces grammars of poor quality as measured against treebank structures. We then augment a PCFG with prototype features, and show that these features, when propagated to non-prototype sequences using distributional similarity, are effective at learning bracket labels on fixed unlabeled trees, but are still not enough to learn good tree structures without bracketing information. Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a highquality bracketing model. The intersected model is able to learn trees with higher unlabeled F1 than those in Klein and Manning (2004). More impor881 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881–888, Sydney, July 2006. c�2006 Association for Computational Linguistics tantly, its trees are labeled and can be evaluated according to labeled metrics. Against the English Penn Treebank, our final trees achieve a labeled F1 of 65.1 on short sentences, a 51.7% error reduction over naive PCFG induction. 2 Experimenta</context>
<context position="5179" citStr="Klein and Manning, 2002" startWordPosition="807" endWordPosition="810">, 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992; Klein and Manning, 2002). Following Klein and Manning (2004), we report unlabeled bracket precision, recall, and F1. Note that according to their metric, brackets of size 1 are omitted from the evaluation. Unlike that work, all of our induction methods produce trees labeled with symbols which are identified with treebank categories. Therefore, we also report labeled precision, recall, and F1, still ignoring brackets of size 1.1 3 Experiments in PCFG induction As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way (Lari and Young, 1990; Manning and Sch¨utze, 1999). Fo</context>
<context position="8311" citStr="Klein and Manning (2002)" startWordPosition="1346" endWordPosition="1349">ate these learned grammars, we must map the models’ phrase types to target phrase types. For each grammar, we followed the common approach of greedily mapping model symbols to target symbols in the way which maximizes the labeled F1. Note that this can, and does, result in mapping multiple model symbols to the most frequent target symbols. This experiment, labeled PCFG x NONE in figure 4, resulted in an average labeled F1 of 26.3 and an unlabeled F1 of 45.7. The unlabeled F1 is better than randomly choosing a tree (34.7), but not better than always choosing a right branching structure (61.7). Klein and Manning (2002) suggest that the task of labeling constituents is significantly easier than identifying them. Perhaps it is too much to ask a PCFG induction algorithm to perform both of these tasks simultaneously. Along the lines of Pereira and Schabes (1992), we reran the insideoutside algorithm, but this time placed zero mass on all trees which did not respect the bracketing of the gold trees. This constraint does not fully 882 Phrase Prototypes Phrase Prototypes NP DT NN VP VBN IN NN JJ NNS VBD DT NN NNP NNP MD VB CD S PRP VBD DT NN QP CD CD DT NN VBD IN DT NN RB CD DT VBZ DT JJ NN DT CD CD PP IN NN ADJP </context>
<context position="13976" citStr="Klein and Manning, 2002" startWordPosition="2340" endWordPosition="2344">DT NN 0.54 S 0.58 CC NN IN NN 0.43 PP 0.71 MD NNS PRP VBD DT NN 1.43 NONE - Figure 2: Yields along with most similar prototypes and phrase types, guessed according to (3). in similar contexts and are mutually substitutable (Harris, 1954; Radford, 1988). For instance, DT JJ NN and DT NN occur in similar contexts, and are indeed both common NPs. This idea has been repeatedly and successfully operationalized using various kinds of distributional clustering, where we define a similarity measure between two items on the basis of their immediate left and right contexts (Sch¨utze, 1995; Clark, 2000; Klein and Manning, 2002). As in Clark (2001), we characterize the distribution of a sequence by the distribution of POS tags occurring to the left and right of that sequence in a corpus. Each occurence of a POS sequence α falls in a context x α y, where x and y are the adjacent tags. The distribution over contexts x − y for a given α is called its signature, and is denoted by a(α). Note that a(α) is composed of context counts from all occurences, constitiuent and distituent, of α. Let Q,(α) denote the context distribution for α where the context counts are taken only from constituent occurences of α. For each phrase </context>
<context position="19171" citStr="Klein and Manning (2002)" startWordPosition="3240" endWordPosition="3243">th fairly accurate labels. However, such bracketing information is not available in the unsupervised case. Perhaps we don’t actually need bracketing constraints in the presence of prototypes and distributional similarity features. However this experiment, labeled PROTO × NONE in figure 4, gave only 53.1 labeled F1 (61.1 unlabeled), suggesting that some amount of bracketing constraint is necessary to achieve high performance. Fortunately, there are unsupervised systems which can induce unlabeled bracketings with reasonably high accuracy. One such model is the constituent-context model (CCM) of Klein and Manning (2002), a generative distributional model. For a given sentence 5, the CCM generates a bracket matrix, B, which for each span (i, j), indicates whether or not it is a constituent (Bij = c) or a distituent (Bij = d). In addition, it generates a feature map F0, which for each span (i, j) in 5 specifies a pair of features, F0ij = (yij, cij), where yij is the POS yield of the span, and cij is the context of the span, i.e identity of the conjoined left and right POS tags: PCCM(B, F0) = P(B) 11 P(yij|Bij)P(cij|Bij) (i�j) The distribution P(B) only places mass on bracketings which correspond to binary tree</context>
<context position="26669" citStr="Klein and Manning (2002)" startWordPosition="4619" endWordPosition="4622">M model, and c) is the parse with the BEST model (added prototype categories), which fixes the possesive NP and infinitival VP problems, but not the PP attachment. given our grammar restriction. Figure 5(b) demonstrates three other errors. Possessive NPs are analyzed as [NP NN [PP POS NN ] ], with the POS element treated as a preposition and the possessed NP as its complement. While labeling the POS NN as a PP is clearly incorrect, placing a constituent over these elements is not unreasonable and in fact has been proposed by some linguists (Abney, 1987). Another type of error also reported by Klein and Manning (2002) is MD VB groupings in infinitival VPs also sometimes argued by linguists (Halliday, 2004). More seriously, prepositional phrases are almost always attached “high” to the verb for longer NPs. 7.1 Augmenting Prototypes One of the advantages of the prototype driven approach, over a fully unsupervised approach, is the ability to refine or add to the annotation specification if we are not happy with the output of our system. We demonstrate this flexibility by augmenting the prototypes in figure 1 with two new categories NP-POS and VP-INF, meant to model possessive noun phrases and infinitival verb</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1481" citStr="Klein and Manning, 2004" startWordPosition="206" endWordPosition="209">y stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (Klein and Manning, 2002; Klein and Manning, 2004; Smith and Eisner, 2004) . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible. We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept. For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of prototypical NPs (determiner noun, pronouns, etc., see figure 1 for a sample prototype list). This prototype information is similar to specifying an annotation scheme, which even h</context>
<context position="3969" citStr="Klein and Manning (2004)" startWordPosition="613" endWordPosition="616">ion produces grammars of poor quality as measured against treebank structures. We then augment a PCFG with prototype features, and show that these features, when propagated to non-prototype sequences using distributional similarity, are effective at learning bracket labels on fixed unlabeled trees, but are still not enough to learn good tree structures without bracketing information. Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a highquality bracketing model. The intersected model is able to learn trees with higher unlabeled F1 than those in Klein and Manning (2004). More impor881 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881–888, Sydney, July 2006. c�2006 Association for Computational Linguistics tantly, its trees are labeled and can be evaluated according to labeled metrics. Against the English Penn Treebank, our final trees achieve a labeled F1 of 65.1 on short sentences, a 51.7% error reduction over naive PCFG induction. 2 Experimental Setup The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank (Marcus et al., 1994), thoug</context>
<context position="5215" citStr="Klein and Manning (2004)" startWordPosition="812" endWordPosition="815">an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992; Klein and Manning, 2002). Following Klein and Manning (2004), we report unlabeled bracket precision, recall, and F1. Note that according to their metric, brackets of size 1 are omitted from the evaluation. Unlike that work, all of our induction methods produce trees labeled with symbols which are identified with treebank categories. Therefore, we also report labeled precision, recall, and F1, still ignoring brackets of size 1.1 3 Experiments in PCFG induction As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way (Lari and Young, 1990; Manning and Sch¨utze, 1999). For all the experiments in this paper,</context>
<context position="21716" citStr="Klein and Manning (2004)" startWordPosition="3683" endWordPosition="3686">)P(cij|d) (iJ):Bzj=c 11 = OCCM(yij, cij) (iJ):Bzj=c 885 model’s latent variables. For CCM, the latent variable is a bracketing matrix B (equivalent to an unlabeled binary tree), while for the CFG+ the latent variable is a labeled tree T. While these latent variables aren’t exactly the same, there is a close relationship between them. A bracketing matrix constrains possible labeled trees, and a given labeled tree determines a bracketing matrix. One way to combine these models is to encourage both models to prefer latent variables which are compatible with each other. Similar to the approach of Klein and Manning (2004) on a different model pair, we intersect CCM and CFG+ by multiplying their scores for any labeled tree. For each possible labeled tree over a sentence S, our generative model for a labeled tree T is given as follows: P(T, F, F&apos;) = (4) PCFG+(T, F)PCCM(B(T), F&apos;) where B(T) corresponds to the bracketing matrix determined by T. The EM algorithm for the product model will maximize: P(S,F, F&apos;) = 1: PCCM(B, F&apos;)PCFG+(T, F) TET (S) =1: PCCM(B, F&apos;) 1: PCFG+(T, F) B TET(B,S) where T (S) is the set of labeled trees consistent with the sentence S and T (B, S) is the set of labeled trees consistent with the</context>
<context position="30732" citStr="Klein and Manning (2004)" startWordPosition="5370" endWordPosition="5373">liary POS tagged Chinese corpus, our distributional model was built only from the treebank text, and the distributional similarities are presumably degraded relative to the English. Our PCFG x CCM experiment gave 18.0 labeled F1 (43.4 unlabeled). The PROTO x CCM model gave 39.0 labeled F1 (53.2 unlabeled). Presumably with access to more POS tagged data, and the expertise of a Chinese speaker, our system would see increased performance. It is worth noting that our unlabeled F1 of 53.2 is the best reported from a primarily unsupervised system, with the next highest figure being 46.7 reported by Klein and Manning (2004). 8 Conclusion We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way. These features give substantial error reduction in labeled F1 measure for English and Chinese grammar induction. They also achieve the best reported unlabeled F1 measure.8 Another positive property of this approach is that it tries to reconcile the success of distributional clustering approaches to grammar induction (Clark, 2001; Klein and Manning, 2002), with the CFG tree models in the supervised literature (Collins, 1999). Most importantly, t</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The unsupervised learning of Natural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="4834" citStr="Klein (2005)" startWordPosition="753" endWordPosition="754"> evaluated according to labeled metrics. Against the English Penn Treebank, our final trees achieve a labeled F1 of 65.1 on short sentences, a 51.7% error reduction over naive PCFG induction. 2 Experimental Setup The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank (Marcus et al., 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992; Klein and Manning, 2002). Following Klein and Manning (2004), we report unlabeled bracket precision, recall, and F1. Note that according to their metric, brackets of size 1 are omitted from the evaluation. Unlike that work, all of our induction methods produce trees labeled with</context>
<context position="20814" citStr="Klein (2005)" startWordPosition="3527" endWordPosition="3528">ual and yield properties of constituents and distituents. The PCFG model, on the other hand, does not explicitly model properties of distituents but instead focuses on modeling the hierarchical and recursive properties of natural language syntax. One would hope that modeling both of these aspects simultaneously would improve the overall quality of our induced grammar. We therefore combine the CCM with our featureaugmented PCFG, denoted by PROTO in experiment names. When we run EM on either of the models alone, at each iteration and for each training example, we calculate posteriors over that 7Klein (2005) gives a full presentation. IN NN VP( P(VBD PP|VP) P(l P=(VP|VP) ✦✦❛❛❛PP ( P(IN NN|PP) Pl (P = PP|PP) ✦ ❛❛❛ ✦ ✦ P(NN NNS|NP) P(P = NP|NP) 1 NP J� P(S|ROOT) } ROOT S P(NP VP|S) { P(P = NONE|S) ✘✘✘✘✘ ❳❳❳❳❳ NN NNN VBD fell payrolls Factory 11 P(yij|c)P(cij|c) PCCM(B, F0) ∝ P(yij|d)P(cij|d) (iJ):Bzj=c 11 = OCCM(yij, cij) (iJ):Bzj=c 885 model’s latent variables. For CCM, the latent variable is a bracketing matrix B (equivalent to an unlabeled binary tree), while for the CFG+ the latent variable is a labeled tree T. While these latent variables aren’t exactly the same, there is a close relationship </context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein. 2005. The unsupervised learning of Natural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="5746" citStr="Lari and Young, 1990" startWordPosition="897" endWordPosition="900">(Carroll and Charniak, 1992; Klein and Manning, 2002). Following Klein and Manning (2004), we report unlabeled bracket precision, recall, and F1. Note that according to their metric, brackets of size 1 are omitted from the evaluation. Unlike that work, all of our induction methods produce trees labeled with symbols which are identified with treebank categories. Therefore, we also report labeled precision, recall, and F1, still ignoring brackets of size 1.1 3 Experiments in PCFG induction As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way (Lari and Young, 1990; Manning and Sch¨utze, 1999). For all the experiments in this paper, we considered binary PCFGs over the nonterminals and terminals occuring in WSJ10. The PCFG rules were of the following forms: • X —* Y Z, for nonterminal types X, Y, and Z, with Y =� X or Z =� X • X —* t Y , X —* Y t, for each terminal t • X —* t t&apos;, for terminals t and t&apos; For a given sentence S, our CFG generates labeled trees T over S.2 Each tree consists of binary 1In cases where multiple gold labels exist in the gold trees, precision and recall were calculated as in Collins (1999). 2Restricting our CFG to a binary branch</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 2(4):35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4562" citStr="Marcus et al., 1994" startWordPosition="704" endWordPosition="707">in Klein and Manning (2004). More impor881 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881–888, Sydney, July 2006. c�2006 Association for Computational Linguistics tantly, its trees are labeled and can be evaluated according to labeled metrics. Against the English Penn Treebank, our final trees achieve a labeled F1 of 65.1 on short sentences, a 51.7% error reduction over naive PCFG induction. 2 Experimental Setup The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank (Marcus et al., 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992; Klein a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="8555" citStr="Pereira and Schabes (1992)" startWordPosition="1385" endWordPosition="1388">t this can, and does, result in mapping multiple model symbols to the most frequent target symbols. This experiment, labeled PCFG x NONE in figure 4, resulted in an average labeled F1 of 26.3 and an unlabeled F1 of 45.7. The unlabeled F1 is better than randomly choosing a tree (34.7), but not better than always choosing a right branching structure (61.7). Klein and Manning (2002) suggest that the task of labeling constituents is significantly easier than identifying them. Perhaps it is too much to ask a PCFG induction algorithm to perform both of these tasks simultaneously. Along the lines of Pereira and Schabes (1992), we reran the insideoutside algorithm, but this time placed zero mass on all trees which did not respect the bracketing of the gold trees. This constraint does not fully 882 Phrase Prototypes Phrase Prototypes NP DT NN VP VBN IN NN JJ NNS VBD DT NN NNP NNP MD VB CD S PRP VBD DT NN QP CD CD DT NN VBD IN DT NN RB CD DT VBZ DT JJ NN DT CD CD PP IN NN ADJP RB JJ TO CD CD JJ IN PRP JJ CC JJ ADVP RB RB RB CD RB CC RB VP-INF VB NN NP-INF NN POS Figure 1: English phrase type prototype list manually specified (The entire supervision for our system). The second part of the table is additional prototype</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando C. N. Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Meeting of the Association for Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Radford</author>
</authors>
<title>Transformational Grammar.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="13604" citStr="Radford, 1988" startWordPosition="2282" endWordPosition="2283">all phrase types regardless of whether or not they are modeled by our grammar. 5Even this property is likely too strong: prototypes may have multiple possible labels, for example DT NN may also be a QP in the English treebank. 883 Yield Prototype Skew KL Phrase Type Skew KL DT JJ NN DT NN 0.10 NP 0.39 IN DT VBG NN IN NN 0.24 PP 0.45 DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58 CC NN IN NN 0.43 PP 0.71 MD NNS PRP VBD DT NN 1.43 NONE - Figure 2: Yields along with most similar prototypes and phrase types, guessed according to (3). in similar contexts and are mutually substitutable (Harris, 1954; Radford, 1988). For instance, DT JJ NN and DT NN occur in similar contexts, and are indeed both common NPs. This idea has been repeatedly and successfully operationalized using various kinds of distributional clustering, where we define a similarity measure between two items on the basis of their immediate left and right contexts (Sch¨utze, 1995; Clark, 2000; Klein and Manning, 2002). As in Clark (2001), we characterize the distribution of a sequence by the distribution of POS tags occurring to the left and right of that sequence in a corpus. Each occurence of a POS sequence α falls in a context x α y, wher</context>
</contexts>
<marker>Radford, 1988</marker>
<rawString>Andrew Radford. 1988. Transformational Grammar. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In EACL.</booktitle>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2004</date>
<booktitle>In Working notes of the IJCAI workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="1506" citStr="Smith and Eisner, 2004" startWordPosition="210" endWordPosition="214"> brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (Klein and Manning, 2002; Klein and Manning, 2004; Smith and Eisner, 2004) . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible. We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept. For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of prototypical NPs (determiner noun, pronouns, etc., see figure 1 for a sample prototype list). This prototype information is similar to specifying an annotation scheme, which even human annotators must be p</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>Noah A. Smith and Jason Eisner. 2004. Guiding unsupervised grammar induction using contrastive estimation. In Working notes of the IJCAI workshop on Grammatical Inference Applications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>