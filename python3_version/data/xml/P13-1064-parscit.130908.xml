<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000309">
<title confidence="0.996875">
Bridging Languages through Etymology:
The case of cross language text categorization
</title>
<author confidence="0.984348">
Vivi Nastase and Carlo Strapparava
</author>
<affiliation confidence="0.7647305">
Human Language Technologies, Fondazione Bruno Kessler
Trento, Italy
</affiliation>
<email confidence="0.995017">
{nastase, strappa}@fbk.eu
</email>
<sectionHeader confidence="0.993754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846266666667">
We propose the hypothesis that word ety-
mology is useful for NLP applications as
a bridge between languages. We support
this hypothesis with experiments in cross-
language (English-Italian) document cat-
egorization. In a straightforward bag-of-
words experimental set-up we add etymo-
logical ancestors of the words in the docu-
ments, and investigate the performance of
a model built on English data, on Italian
test data (and viceversa). The results show
not only statistically significant, but a large
improvement – a jump of almost 40 points
in F1-score – over the raw (vanilla bag-of-
words) representation.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980996875">
When exposed to a document in a language he
does not know, a reader might be able to glean
some meaning from words that are the same (e.g.
names) or similar to those in a language he knows.
As an example, let us say that an Italian speaker
is reading an English text that contains the word
expense, which he does not know. He may be re-
minded however of the Latin word expensa which
is also the etymological root of the Italian word
spesa, which usually means “cost”/”shopping”,
and may thus infer that the English word refers
to the cost of things. In the experiments presented
here we investigate whether an automatic text cat-
egorization system could benefit from knowledge
about the etymological roots of words. The cross
language text categorization (CLTC) task consists
of categorizing documents in a target language Lt
using a model built from labeled examples in a
source language Ls. The task becomes more diffi-
cult when the data consists of comparable corpora
in the two languages – documents on the same top-
ics (e.g. sports, economy) – instead of parallel cor-
pora – there exists a one-to-one correspondence
between documents in the corpora for the two lan-
guages, one document being the translation of the
other.
To test the usefulness of etymological in-
formation we work with comparable collec-
tions of news articles in English and Ital-
ian, whose articles are assigned one of four
categories: culture and school, tourism, qual-
ity of life, made in Italy. We perform a progres-
sion of experiments, which embed etymological
information deeper and deeper into the model. We
start with the basic set-up, representing the doc-
uments as bag-of-words, where we train a model
on the English training data, and use this model
to categorize documents from the Italian test data
(and viceversa). The results are better than ran-
dom, but quite low. We then add the etymological
roots of the words in the data to the bag-of-words,
and notice a large – 21 points – increase in per-
formance in terms of F1-score. We then use the
bag-of-words representation of the training data to
build a semantic space using LSA, and use the
generated word vectors to represent the training
and test data. The improvement is an additional
16 points in F1-score.
Compared to related work, presented in Sec-
tion 3, where cross language text categorization
is approached through translation or mapping of
features (i.e. words) from the source to the target
language, word etymologies are a novel source of
cross-lingual knowledge. Instead of mapping fea-
tures between languages, we introduce new fea-
tures which are shared, and thus do not need trans-
lation or other forms of mapping.
The experiments presented show unequivocally
that word etymology is a useful addition to com-
putational models, just as they are to readers
who have such knowledge. This is an interest-
ing and useful result, especially in the current
research landscape where using and exploiting
multi-linguality is a desired requirement.
</bodyText>
<page confidence="0.980576">
651
</page>
<note confidence="0.939001">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–659,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.977585631578947">
morpheme relation related morpheme
eng: ex- rel:etymological origin of eng: excentric English: muscle
eng: expense rel:etymology lat: expensa ↓
eng: -ly rel:etymological origin of eng: absurdly French: muscle
eng: -ly rel:etymological origin of eng: admirably ↓
... Latin: musculus
↓
Latin: mus
ita: spesa rel:etymology lat: expensa
ita: spesa rel:has derived form ita: spese
...
ita: spesare rel:etymologically related ita: spesa
... ↓
Proto Indo-European: muh2s
lat: expensa rel:etymological origin of eng: expense
lat: expensa rel:etymological origin of ita: spesa
...
lat: expensa rel:is derived from lat: expensus
...
</table>
<figureCaption confidence="0.982331">
Figure 1: Sample entries from the Etymological WordNet, and a few etymological layers
</figureCaption>
<sectionHeader confidence="0.856441" genericHeader="method">
2 Word Etymology
</sectionHeader>
<bodyText confidence="0.999916072727273">
Word etymology gives us a glimpse into the evo-
lution of words in a language. Words may be
adopted from a language because of cultural,
scientific, economic, political or other reasons
(Hitchings, 2009). In time these words “adjust” to
the language that adopted them – their sense may
change to various degrees – but they are still se-
mantically related to their etymological roots. To
illustrate the point, we show an example that the
reader, too, may find amusing: on the ticket vali-
dation machine on Italian buses, by way of instruc-
tion, it is written Per obliterare il biglietto .... A
native/frequent English speaker would most prob-
ably key in on, and be puzzled by, the word oblit-
erare, very similar to the English obliterate, whose
most used sense is to destroy completely/ cause to
physically disappear . The Italian obliterare has
the “milder” sense of cancellare – cancel (which
is also shared by the English obliterate, but is less
frequent according to Merriam-Webster), and both
come from the Latin obliterare – erase, efface,
cause to disappear. While there has been some
sense migration – in English the more (physically)
destructive sense of the word has higher promi-
nence, while in Italian the word is closer in mean-
ing to its etymological root – the Italian and the
English words are still semantically related.
Dictionaries customarily include etymologi-
cal information for their entries, and recently,
Wikipedia’s Wiktionary has joined this trend. The
etymological information can, and indeed has
been extracted and prepared for machine con-
sumption (de Melo and Weikum, 2010): Etymo-
logical WordNet1 contains 6,031,431 entries for
2,877,036 words (actually, morphemes) in 397
languages. A few sample entries from this re-
source are shown in Figure 1.
The information in Etymological WordNet is
organized around 5 relations: etymology with
its inverse etymological origin of; is derived from
with its inverse has derived form; and the sym-
metrical etymologically related. The etymology
relation links a word with its etymological ances-
tors, and it is the relation used in the experiments
presented here. Prefixes and suffixes – such as ex-
and -ly shown in Figure 1 – are filtered out, as
they bring in much noise by relating words that
merely share such a morpheme (e.g. absurdly and
admirably) but are otherwise semantically distant.
has derived form is also used, to capture morpho-
logical variations.
The depth of the etymological hierarchy (con-
sidering the etymology relations) is 10. Figure 1
shows an example of a word with several levels of
etymological ancestry.
</bodyText>
<footnote confidence="0.993206">
1http://www1.icsi.berkeley.edu/
˜demelo/etymwn/
</footnote>
<page confidence="0.996873">
652
</page>
<figure confidence="0.93516225">
...
...
⎤
English texts Italian texts
</figure>
<equation confidence="0.955609363636364">
ti 1 ti 2 · · · ti m−1 ti ⎥
m ⎥
0 0 ··· ⎥ ⎥
..⎥
0 . ⎥ ⎥ ⎥ ⎥
...⎥. .
0 . ⎥ ⎥
.⎥
.. ⎥
0 ⎥
··· 0 0 ⎥ ⎥ ⎥
0 0 ··· 0 1 ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎥ ⎥
0 0 ··· 1 0 ⎥ ⎥ ⎥ ⎥ ⎥
⎥ ⎥
0 1 ··· 1 1 ⎥ ⎥ ⎥
1 1 · · · 0 1 ⎥ ⎥ ⎥ ⎥
..⎥ ⎥
0 . ⎥ ⎥
..⎦
. 0 0 1 ··· 0 1
0 1 ··· 1 0
</equation>
<figure confidence="0.989570370370371">
⎡ English we1
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Lexicon we2
shared ...
names and we
words p−1
common wep
etymology we/i
Italian 1
Lexicon ...
wetym
1
...
wi1
wi2
...
wi
q−1
wiq
te 1 te 2 · · · te n−1 te n
0 1 · · · 0 1
1 1 · · · 1 0
0 1 · · · 0 0
0 1 · · · 0 0
1 0 · · · 0 0
0 1 · · · 0 0
0 0 · · ·
· · · 0
</figure>
<figureCaption confidence="0.999651">
Figure 2: Multilingual word-by-document matrix
</figureCaption>
<bodyText confidence="0.998496208333334">
Text categorization (also text classification),
task of automatically sorting a set of documents
into categories (or classes or topics) from a prede-
(Sebastiani, 2005), allows for the quick
selection of documents from the same domain, or
the same topic. It is a very well research area, dat-
ing back to the 60s (Borko and Bernick, 1962).
The most frequently, and successfully, used docu-
ment representation is the
Results using this representation achieve accuracy
in the 90%s. Most variations include feature filter-
ing or weighing, and variations in learning algo-
rithms (Sebastiani, 2005).
Within the area of cross-language text catego-
rization (CLTC) several methods have been ex-
plored for producing the model for a target lan-
guage Lt using information and data from the
source language L3. In a precursor task to CLTC,
cross language information retrieval (CLIR), Du-
mais et al. (1997) find semantic correspondences
in parallel (different language) corpora through la-
tent semantic analysis (LSA). Most CLTC meth-
ods rely heavily on machine translation (MT). MT
has been used: to cast the cross-lan
</bodyText>
<figure confidence="0.863822714285714">
“the
set”
bag-of-words(BoWs).
guage text
0
3 Cross Language Text Categorization
fined
</figure>
<bodyText confidence="0.9699648">
categorization problem to the monolingual setting
(Fortuna and Shawe-Taylor, 2005); to cast the
cross-language text categorization problem into
two monolingual settings for active learning (Liu
et al., 2012); to translate and adapt a model built
on language L3 to language Lt (Rigutini et al.,
2005), (Shi et al., 2010); to produce parallel
corpora for multi-view learning (Guo and Xiao,
2012). Wan et al. (2011) also use machine trans-
lation, but enhance the processing through domain
adaptation by feature weighing, assuming that the
training data in one language and the test data in
the other come from different domains, or can ex-
hibit different linguistic phenomena due to linguis-
tic and cultural differences. Prettenhofer and Stein
(2010) use a word translation oracle to produce
pivots
of semantically similar words
use the data partitions induced by these words to
find cross language structural correspondences.
In a computationally lighter framework, not de-
pendent on MT, Gliozzo and Strapparava (2006)
and Wu et al. (2008) use bilingual lexicons and
aligned WordNet synsets to obtain shared features
between the training data in language L3 and the
testing data in language Lt. Gliozzo an
– pairs
– and
d Strap-
parava (2005), the first to use comparable as op-
</bodyText>
<page confidence="0.996894">
653
</page>
<bodyText confidence="0.99950216">
posed to parallel corpora for CLTC, use LSA to
build multilingual domain models.
The bag-of-word document representation
maps a document di from a corpus D into a k-
dimensional space Rk, where k is the dimension
of the (possibly filtered) vocabulary of the corpus:
W = {wi, ..., wk}. Position j in the vector
representation of di corresponds to word wj, and
it may have different values, among the most
commonly used being: binary values – wj appears
(1) or not (0) in di; frequency of occurrence of wj
in di, absolute or normalized (relative to the size
of the document or the size of the vocabulary); the
tf ∗ idf(wj, di, D).
For the task of cross language text categoriza-
tion, the problem of sharing a model across lan-
guages is that the dimensions, a.k.a the vocabu-
lary, of the two languages are largely different.
Limited overlap can be achieved through shared
names and words. As we have seen in the lit-
erature review, machine translation and bilingual
dictionaries can be used to cast these dimensions
from the source language Ls to the target language
Lt. In this work we explore expanding the shared
dimensions through word etymologies. Figure 2
shows schematically the binary k dimensional rep-
resentation for English and Italian data, and shared
dimensions.
Cross language text categorization could be
used to obtain comparable corpora for building
translation models. In such a situation, relying on
a framework that itself relies on machine transla-
tion is not helpful. Bilingual lexicons are available
for frequently studied languages, but less so for
those poorer in resources. Considering such short-
comings, we look into additional linguistic infor-
mation, in particular word etymology. This infor-
mation impacts the data representation, by intro-
ducing new shared features between the different
language corpora without the need for translation
or other forms of mapping. The newly produced
representation can be used in conjunction with any
of the previously proposed algorithms.
Word etymologies are a novel source of linguis-
tic information in NLP, possibly because resources
that capture this information in a machine readable
format are also novel. Fang et al. (2009) used lim-
ited etymological information extracted from the
Collins English Dictionary (CED) for text catego-
rization on the British National Corpus (BNC): in-
formation on the provenance of words (ranges of
probability distribution of etymologies in different
versions of Latin – New Latin, Late Latin, Me-
dieval Latin) was used in a “home-made” range
classifier.
The experiments presented in this paper use the
bag-of-word document representation with abso-
lute frequency values. To this basic representation
we add word etymological ancestors and run clas-
sification experiments. We then use LSA – previ-
ously shown by (Dumais et al., 1997) and (Gliozzo
and Strapparava, 2005) to be useful for this task –
to induce the latent semantic dimensions of docu-
ments and words respectively, hypothesizing that
word etymological ancestors will lead to semantic
dimensions that transcend language boundaries.
The vectors obtained through LSA (on the training
data only) for words that are shared by the English
training data and the Italian test data (names, and
most importantly, etymological ancestors of words
in the original documents) are then used for re-
representing the training and test data. The same
process is applied for Italian training and English
test data. Classification is done using support vec-
tor machines (SVMs).
</bodyText>
<subsectionHeader confidence="0.991764">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999936857142857">
The data we work with consists of compara-
ble corpora of news articles in English and Ital-
ian. Each news article is annotated with one of
the four categories: culture and school, tourism,
quality of life, made in Italy. Table 1 shows the
dataset statistics. The average document length is
approximately 300 words.
</bodyText>
<subsectionHeader confidence="0.999458">
3.2 Raw cross-lingual text categorization
</subsectionHeader>
<bodyText confidence="0.9999195">
As is commonly done in text categorization (Se-
bastiani, 2005), the documents in our data are
represented as bag-of-words, and classification is
done using support vector machines (SVMs).
One experimental run consists of 4 binary ex-
periments – one class versus the rest, for each of
the 4 classes. The results are reported through
micro-averaged precision, recall and F1-score for
the targeted class, as well as overall accuracy. The
high results, on a par with text categorization ex-
periments in the field, validates our experimental
set-up.
For the cross language categorization experi-
ments described in this paper, we use the data
described above, and train on one language (En-
glish/Italian), and test on the other, using the same
</bodyText>
<page confidence="0.99371">
654
</page>
<table confidence="0.999632923076923">
Training English Total Training Italian Total
Test Test
5759 1989 7748 5781 1901 7682
5711 1864 7575 6111 2068 8179
5731 1857 7588 6090 2015 8105
3665 1245 4910 6284 2104 8388
20866 6955 27821 24266 8088 32354
Categories
quality of life
made in Italy
tourism
culture and school
Total
</table>
<tableCaption confidence="0.977731">
Table 1: Dataset statistics
</tableCaption>
<table confidence="0.9595205">
monolingual BoW categorization
Prec Rec F1 Acc
Train EN / Test EN 0.92 0.92 0.92 0.96
Train IT / Test IT 0.94 0.94 0.94 0.97
</table>
<tableCaption confidence="0.8892045">
Table 2: Performance for monolingual raw text
categorization
</tableCaption>
<bodyText confidence="0.997852625">
experimental set-up as for the monolingual sce-
nario (4 binary problems). The categorization
baseline (BoW baseline in Figure 4) was obtained
in this set-up. This baseline is higher than the ran-
dom baseline or the positive class baseline2 (all in-
stances are assigned the target class in each of the
4 binary classification experiments) due to shared
words and names between the two languages.
</bodyText>
<subsectionHeader confidence="0.984379">
3.3 Enriching the bag-of-word
</subsectionHeader>
<bodyText confidence="0.999599428571428">
representation with word etymology
As personal experience has shown us that etymo-
logical information is useful for comprehending
a text in a different language, we set out to test
whether this information can be useful in an auto-
matic processing setting. We first verified whether
the vocabularies of our two corpora, English and
Italian, have shared word etymologies. Relying
on word etymologies from the Etymological dic-
tionary, we found that from our data’s vocabulary,
518 English terms and 543 Italian terms shared
490 direct etymological ancestors. Etymological
ancestors also help cluster related terms within one
language – 887 etymological ancestors for 4727
English and 864 ancestors for 5167 Italian terms.
This overlap further increases when adding de-
rived forms (through the has derived form rela-
tion). The fact that this overlap exists strengthens
the motivation to try using etymological ancestors
for the task of text categorization.
In this first step of integrating word etymology
</bodyText>
<footnote confidence="0.947308">
2In this situation the random and positive class baseline
are the same: 25% F1 score.
</footnote>
<bodyText confidence="0.999272875">
into the experiment, we extract for each word in
each document in the dataset its ancestors from
the Etymological dictionary. Because each word
wj in a document di has associated an absolute
frequency value fij (the number of occurrences of
wj in di), for the added etymological ancestors ek
in document Di we associate as value the sum of
frequencies of their etymological children in di:
</bodyText>
<equation confidence="0.994593">
�fiek = fij
wj∈di
wjetymology ek
</equation>
<bodyText confidence="0.999553">
We make the depth of extraction a parameter,
and generate data representation when consider-
ing only direct etymological antecedents (depth 1)
and then up to a distance of N. For our dataset we
noticed that the representation does not change af-
ter N=4, so this is the maximum depth we con-
sider. The bag-of-words representation for each
document is expanded with the corresponding et-
ymological features.
</bodyText>
<table confidence="0.790530928571429">
expansion training data vo- vocabulary over-
cabulary size lap with testing
Train EN /Test IT
raw 71122 14207 (19.9%)
depth 1 78936 18275 (23.1%)
depth 2 79068 18359 (23.2%)
depth 3 79100 18380 (23.2%)
depth 4 79103 18382 (23.2%)
Train IT /Test EN
raw 78750 14110 (17.9%)
depth 1 83656 18682 (22.3%)
depth 2 83746 18785 (22.4%)
depth 3 83769 18812 (22.5%)
depth 4 83771 18814 (22.5%)
</table>
<tableCaption confidence="0.997982">
Table 3: Feature expansion with word etymologies
</tableCaption>
<bodyText confidence="0.992436">
Table 3 shows the training data vocabulary size
and increase in the overlap between the training
and test data with the addition of etymological fea-
</bodyText>
<page confidence="0.998358">
655
</page>
<bodyText confidence="0.999980111111111">
tures. The increase is largest when introducing
the immediate etymological ancestors, of approx-
imately 4000 new (overlapping) features for both
combinations of training and testing. Without ety-
mological features the overlap was approximately
14000 for both configurations. The results ob-
tained with this enriched BoW representation for
etymological ancestor depth 1, 2 and 3 are pre-
sented in Figure 4.
</bodyText>
<subsectionHeader confidence="0.9544095">
3.4 Cross-lingual text categorization in a
latent semantic space adding etymology
</subsectionHeader>
<bodyText confidence="0.999885714285714">
Shared word etymologies can serve as a bridge be-
tween two languages as we have seen in the pre-
vious configuration. When using shared word et-
ymologies in the bag-of-words representation, we
only take advantage of the shallow association be-
tween these new features and the classes within
which they appear. But through the co-occurrence
of the etymological features and other words in
different documents in the training data, we can
induce a deeper representation for the words in
a document, that captures better the relationship
between the features (words) and the classes to
which the documents belong. We use latent se-
mantic analysis (LSA) (Deerwester et al., 1990)
to perform this representational transformation.
The process relies on the assumption that word
co-occurrences across different documents are the
surface manifestation of shared semantic dimen-
sions. Mathematically, the (word x document)
matrix D is expressed as a product of three ma-
trices:
</bodyText>
<sectionHeader confidence="0.26336" genericHeader="method">
D=VEUT
</sectionHeader>
<bodyText confidence="0.998768882352941">
by performing singular value decomposition
(SVD). V would correspond roughly to a (word
x latent semantic dimension) matrix, UT is the
transposed of a (document x latent semantic
dimension) matrix, and E is a diagonal matrix
whose values are indicative of the “strength” of the
semantic dimensions. By reducing the size of E,
for example by selecting the dimensions with the
top K values, we can obtain an approximation of
the original matrix D Pz DK = VKEKUTK, where
we restrict the latent semantic dimensions taken
into account to the K chosen ones. Figure 3 shows
schematically the process.
We perform this decomposition and dimension
reduction step on the (word x document) ma-
trix built from the training data only, and using
K=400. Both the training and test data are then
</bodyText>
<figureCaption confidence="0.994442">
Figure 3: Schematic view of LSA
</figureCaption>
<bodyText confidence="0.999370222222222">
re-represented through the new word vectors from
matrix VK. Because the LSA space was built only
from the training data, only the shared words and
shared etymological ancestors are used to produce
representations of the test data. The categorization
is done again with SVM. The results of this exper-
iment are shown in Figure 4, together with an LSA
baseline – using the raw data and relying on shared
words and names as overlap.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9499005625">
The experiments whose results we present here
were produced using unfiltered data – all words in
the datasets, all etymological ancestors up to the
desired depth, no filtering based on frequency of
occurrence. Feature filtering is commonly done in
machine learning when the data has many features,
and in text categorization when using the bag-of-
words representation in particular. We chose not to
perform this step for two main reasons: (i) filter-
ing is sensitive to the chosen threshold; (ii) LSA
thrives on word co-occurrences, which would be
drastically reduced by word removal. The point
that etymology information is a useful addition to
the task of cross-language text categorization can
be made without finding the optimal filtering set-
up.
The baseline experiments show that despite
the relatively large word overlap (approx. 14000
terms), cross-language text categorization gives
low results. Adding a first batch of etymological
information – approximately 4000 shared immedi-
ate ancestors – leads to an increase of 18 points in
terms of F1-score on the BoW experimental set-up
for English training/Italian testing, and 21 points
for Italian training/English testing. Further addi-
tions of etymological ancestors at depths 2 and
3 results in an increase of 21 points in terms of
F1-score for English training/Italian testing, and
27 points for Italian training/English testing. The
higher increase in performance on this experimen-
tal configuration for Italian training/English test-
ing is explained by the higher term overlap be-
</bodyText>
<figure confidence="0.929606726027397">
dimension
latent semantic latent semantic
documents dimension dimension documents
ic
ic
x
latent semanti
latentsemanti
x
V x D
V x K
words
K x K
words
SVD
and
dimension
reduction
K x D
dimension
656
Italian training, English testing
English training, Italian testing
0.65
0.43
0.64
0.80
F1−score 1
0.9
0.8
0.7
0.6
0.5
0.4
F1−score 1
0.9
0.8
0.7
0.6
0.5
0.4
0.74
0.69
0.54
0.42
BoW-etym LSA-etym BoW-etym LSA-etym
1
0.9
Accuracy
0.8
0.7
0.6
0.5
0.4
1
0.9
Accuracy
0.8
0.7
0.6
0.5
0.4
Italian training, English testing
English training, Italian testing
0.79
0.71
0.84 0.87
0.83
0.72
0.82
0.89
BoW-etym LSA-etym BoW-etym LSA-etym
depth=1 depth=2 depth=3 BoW-baseline LSA-baseline
</figure>
<figureCaption confidence="0.999887">
Figure 4: CLTC results with etymological features
</figureCaption>
<bodyText confidence="0.998973290322581">
tween the training and test data, as evidenced by
the statistics in Table 3.
The next processing step induced a represen-
tation of the shared words that encodes deeper
level dependencies between words and documents
based on word co-occurrences in documents. The
LSA space built on the training data leads to a
vector representation of the shared words, includ-
ing the shared etymological ancestors, that cap-
tures more than the obvious word-document co-
occurrences. Using this representation leads to a
further increase of 15 points in F1-score for En-
glish training/Italian testing set-up over the BoW
representation, and 14 points over the baseline
LSA-based categorization. The increase for the
Italian training/English testing is 5 points over the
BoW representation, but 20 points over the base-
line LSA. We saw that the high performance BoW
on Italian training/English testing is due to the
high term overlap. The clue to why the increase
when using LSA is lower than for English train-
ing/Italian testing is in the way LSA operates – it
relies heavily on word co-occurrences in finding
the latent semantic dimensions of documents and
words. We expect then that in the Italian training
collection, words are “less shared” among docu-
ments, which means a lower average document
frequency. Figure 5 shows the changes in aver-
age document frequency for the two training col-
lections, starting with the raw data (depth 0), and
with additional etymological features.
</bodyText>
<figure confidence="0.930447">
Average document frequency for words in the training data
0 1 2 3 4
Etymology depth
</figure>
<figureCaption confidence="0.977361">
Figure 5: Document frequency changes with the
</figureCaption>
<bodyText confidence="0.9500675">
addition of etymological features
The shape of the document frequency curves
mirror the LSA results – the largest increase is the
effect of adding the set of direct etymological an-
cestors, and additions of further, more distant, an-
cestors lead to smaller improvements.
</bodyText>
<figure confidence="0.997225538461539">
Average DF
140
130
120
110
100
90
70
80
60
50
EN
IT
</figure>
<page confidence="0.9873">
657
</page>
<bodyText confidence="0.999967285714286">
We have performed the experiments described
above on two releases of the Etymological dictio-
nary. The results described in the paper were ob-
tained on the latest release (February 2013). The
difference in results on the two dictionary versions
was significant: a 4 and 5 points increase respec-
tively in micro-averaged F1-score in the bag-of-
words setting for English training/Italian testing
and Italian training/English testing, and a 2 and
6 points increase in the LSA setting. This indi-
cates that more etymological information is better,
and the dynamic nature of Wikipedia and the Wik-
tionary could lead to an ever increasing and better
etymological resource for NLP applications.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988636363636">
The motivation for this work was to test the hy-
pothesis that information about word etymology is
useful for computational approaches to language,
in particular for text classification. Cross-language
text classification can be used to build compara-
ble corpora in different languages, using a single
language starting point, preferably one with more
resources, that can thus spill over to other lan-
guages. The experiments presented have shown
clearly that etymological ancestors can be used
to provide the necessary bridge between the lan-
guages we considered – English and Italian. Mod-
els produced on English data when using etymo-
logical information perform with high accuracy
(89%) and high F1-score (80) on Italian test data,
with an increase of almost 40 points over a simple
bag-of-words model, which, for crossing language
boundaries, relies exclusively on shared names
and words. Training on Italian data and testing on
English data performed almost as well (87% accu-
racy, 75 F1-score). We plan to expand our experi-
ments to more languages with shared etymologies,
and investigate what characteristics of languages
and data indicate that etymological information is
beneficial for the task at hand.
We also plan to explore further uses for this lan-
guage bridge, at a finer semantic level. Monolin-
gual and cross-lingual textual entailment in par-
ticular would be interesting applications, because
they require finding shared meaning on two text
fragments. Word etymologies would allow recog-
nizing words with shared ancestors, and thus with
shared meaning, both within and across languages.
</bodyText>
<sectionHeader confidence="0.992143" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998835">
We thank the reviewers for the helpful comments.
This work was financially supported by the EC-
funded project EXCITEMENT – EXploring Cus-
tomer Interactions through Textual EntailMENT
FP7 ICT-287923. Carlo Strapparava was partially
supported by the PerTe project (Trento RISE).
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994169142857143">
Harold Borko and Myrna Bernick. 1962. Auto-
matic Document Classification. System Develop-
ment Corporation, Santa Monica, CA.
Gerard de Melo and Gerhard Weikum. 2010. Towards
universal multilingual knowledge bases. In Prin-
ciples, Construction, and Applications of Multilin-
gual Wordnets. Proceedings of the 5th Global Word-
Net Conference (GWC 2010), pages 149–156, New
Delhi, India.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the American Socienty for Information Science,
41(6):391–407.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent semantic
indexing. In AAAI Symposium on CrossLanguage
Text and Speech Retrieval.
Alex Chengyu Fang, Wanyin Li, and Nancy Ide. 2009.
Latin etymologies as features on BNC text cate-
gorization. In 23rd Pacific Asia Conference on
Language, Information and Computation (PACLIC
2009), pages 662–669.
Blaz Fortuna and John Shawe-Taylor. 2005. The use of
machine translation tools for cross-lingual text min-
ing. In Learning with multiple views – Workshop
at the 22nd International Conference on Machine
Learning (ICML 2005).
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts.
Alfio Gliozzo and Carlo Strapparava. 2006. Ex-
ploiting comparable corpora and bilingual dictionar-
ies for cross-language text categorization. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING-ACL 2006), pages 553–560, Sydney,
Australia.
</reference>
<page confidence="0.982243">
658
</page>
<reference confidence="0.999515244444445">
Yuhong Guo and Min Xiao. 2012. Cross language
text classification via subspace co-regularized multi-
view learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML
2012), Edinburgh, Scotland, UK.
Henry Hitchings. 2009. The Secret Life of Words: How
English Became English. John Murray Publishers.
Yue Liu, Lin Dai, Weitao Zhou, and Heyan Huang.
2012. Active learning for cross language text cat-
egorization. In Proceedings of the 16th Pacific-Asia
conference on Advances in Knowledge Discovery
and Data Mining (PAKDD 2012), pages 195–206,
Kuala Lumpur, Malaysia.
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural corre-
spondence learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1118–1127, Uppsala,
Sweden.
Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005. An EM based training algorithm for cross-
language text categorization. In Proceedings of the
International Conference on Web Intelligence (WI
2005), pages 200–206, Compiegne, France.
Fabrizio Sebastiani. 2005. Text categorization. In
Alessandro Zanasi, editor, Text Mining and its Ap-
plications, pages 109–129. WIT Press, Southamp-
ton, UK.
Lei Shi, Rada Mihalcea, and Minhgjun Tian. 2010.
Cross language text classification by model trans-
lation and semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1057–1067, Uppsala, Sweden.
Chang Wan, Rong Pan, and Jifei Li. 2011. Bi-
weighting domain adaptation for cross-language text
classification. In Proceedings of the 22nd Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2011), pages 1535–1540, Barcelona, Catalonia,
Spain.
Ke Wu, Xiaolin Wang, and Bao-Liang Lu. 2008.
Cross language text categorization using a bilingual
lexicon. In Third International Joint Conference
on Natural Language Processing (IJCNLP 2008),
pages 165–172, Hyderabad, India.
</reference>
<page confidence="0.998961">
659
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.301984">
<title confidence="0.951465666666667">Bridging Languages through The case of cross language text categorization Nastase</title>
<author confidence="0.573584">Human Language Technologies</author>
<author confidence="0.573584">Fondazione Bruno</author>
<affiliation confidence="0.523553">Trento,</affiliation>
<abstract confidence="0.984408625">We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement – a jump of almost 40 points in F1-score – over the raw (vanilla bag-ofwords) representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harold Borko</author>
<author>Myrna Bernick</author>
</authors>
<title>Automatic Document Classification. System Development Corporation,</title>
<date>1962</date>
<location>Santa Monica, CA.</location>
<contexts>
<context position="8390" citStr="Borko and Bernick, 1962" startWordPosition="1488" endWordPosition="1491">ed ... names and we words p−1 common wep etymology we/i Italian 1 Lexicon ... wetym 1 ... wi1 wi2 ... wi q−1 wiq te 1 te 2 · · · te n−1 te n 0 1 · · · 0 1 1 1 · · · 1 0 0 1 · · · 0 0 0 1 · · · 0 0 1 0 · · · 0 0 0 1 · · · 0 0 0 0 · · · · · · 0 Figure 2: Multilingual word-by-document matrix Text categorization (also text classification), task of automatically sorting a set of documents into categories (or classes or topics) from a prede(Sebastiani, 2005), allows for the quick selection of documents from the same domain, or the same topic. It is a very well research area, dating back to the 60s (Borko and Bernick, 1962). The most frequently, and successfully, used document representation is the Results using this representation achieve accuracy in the 90%s. Most variations include feature filtering or weighing, and variations in learning algorithms (Sebastiani, 2005). Within the area of cross-language text categorization (CLTC) several methods have been explored for producing the model for a target language Lt using information and data from the source language L3. In a precursor task to CLTC, cross language information retrieval (CLIR), Dumais et al. (1997) find semantic correspondences in parallel (differe</context>
</contexts>
<marker>Borko, Bernick, 1962</marker>
<rawString>Harold Borko and Myrna Bernick. 1962. Automatic Document Classification. System Development Corporation, Santa Monica, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Towards universal multilingual knowledge bases.</title>
<date>2010</date>
<booktitle>In Principles, Construction, and Applications of Multilingual Wordnets. Proceedings of the 5th Global WordNet Conference (GWC 2010),</booktitle>
<pages>149--156</pages>
<location>New Delhi, India.</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2010. Towards universal multilingual knowledge bases. In Principles, Construction, and Applications of Multilingual Wordnets. Proceedings of the 5th Global WordNet Conference (GWC 2010), pages 149–156, New Delhi, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Socienty for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="19677" citStr="Deerwester et al., 1990" startWordPosition="3304" endWordPosition="3307"> between two languages as we have seen in the previous configuration. When using shared word etymologies in the bag-of-words representation, we only take advantage of the shallow association between these new features and the classes within which they appear. But through the co-occurrence of the etymological features and other words in different documents in the training data, we can induce a deeper representation for the words in a document, that captures better the relationship between the features (words) and the classes to which the documents belong. We use latent semantic analysis (LSA) (Deerwester et al., 1990) to perform this representational transformation. The process relies on the assumption that word co-occurrences across different documents are the surface manifestation of shared semantic dimensions. Mathematically, the (word x document) matrix D is expressed as a product of three matrices: D=VEUT by performing singular value decomposition (SVD). V would correspond roughly to a (word x latent semantic dimension) matrix, UT is the transposed of a (document x latent semantic dimension) matrix, and E is a diagonal matrix whose values are indicative of the “strength” of the semantic dimensions. By</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Socienty for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Todd A Letsche</author>
<author>Michael L Littman</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic cross-language retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In AAAI Symposium on CrossLanguage Text and Speech Retrieval.</booktitle>
<contexts>
<context position="8939" citStr="Dumais et al. (1997)" startWordPosition="1573" endWordPosition="1577">ry well research area, dating back to the 60s (Borko and Bernick, 1962). The most frequently, and successfully, used document representation is the Results using this representation achieve accuracy in the 90%s. Most variations include feature filtering or weighing, and variations in learning algorithms (Sebastiani, 2005). Within the area of cross-language text categorization (CLTC) several methods have been explored for producing the model for a target language Lt using information and data from the source language L3. In a precursor task to CLTC, cross language information retrieval (CLIR), Dumais et al. (1997) find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (</context>
<context position="13325" citStr="Dumais et al., 1997" startWordPosition="2276" endWordPosition="2279">etymological information extracted from the Collins English Dictionary (CED) for text categorization on the British National Corpus (BNC): information on the provenance of words (ranges of probability distribution of etymologies in different versions of Latin – New Latin, Late Latin, Medieval Latin) was used in a “home-made” range classifier. The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. To this basic representation we add word etymological ancestors and run classification experiments. We then use LSA – previously shown by (Dumais et al., 1997) and (Gliozzo and Strapparava, 2005) to be useful for this task – to induce the latent semantic dimensions of documents and words respectively, hypothesizing that word etymological ancestors will lead to semantic dimensions that transcend language boundaries. The vectors obtained through LSA (on the training data only) for words that are shared by the English training data and the Italian test data (names, and most importantly, etymological ancestors of words in the original documents) are then used for rerepresenting the training and test data. The same process is applied for Italian training</context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>Susan T. Dumais, Todd A. Letsche, Michael L. Littman, and Thomas K. Landauer. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI Symposium on CrossLanguage Text and Speech Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Chengyu Fang</author>
<author>Wanyin Li</author>
<author>Nancy Ide</author>
</authors>
<title>Latin etymologies as features on BNC text categorization.</title>
<date>2009</date>
<booktitle>In 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC</booktitle>
<pages>662--669</pages>
<contexts>
<context position="12691" citStr="Fang et al. (2009)" startWordPosition="2178" endWordPosition="2181">hose poorer in resources. Considering such shortcomings, we look into additional linguistic information, in particular word etymology. This information impacts the data representation, by introducing new shared features between the different language corpora without the need for translation or other forms of mapping. The newly produced representation can be used in conjunction with any of the previously proposed algorithms. Word etymologies are a novel source of linguistic information in NLP, possibly because resources that capture this information in a machine readable format are also novel. Fang et al. (2009) used limited etymological information extracted from the Collins English Dictionary (CED) for text categorization on the British National Corpus (BNC): information on the provenance of words (ranges of probability distribution of etymologies in different versions of Latin – New Latin, Late Latin, Medieval Latin) was used in a “home-made” range classifier. The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. To this basic representation we add word etymological ancestors and run classification experiments. We then use LSA – previou</context>
</contexts>
<marker>Fang, Li, Ide, 2009</marker>
<rawString>Alex Chengyu Fang, Wanyin Li, and Nancy Ide. 2009. Latin etymologies as features on BNC text categorization. In 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC 2009), pages 662–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaz Fortuna</author>
<author>John Shawe-Taylor</author>
</authors>
<title>The use of machine translation tools for cross-lingual text mining.</title>
<date>2005</date>
<booktitle>In Learning with multiple views – Workshop at the 22nd International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="9319" citStr="Fortuna and Shawe-Taylor, 2005" startWordPosition="1628" endWordPosition="1631">orization (CLTC) several methods have been explored for producing the model for a target language Lt using information and data from the source language L3. In a precursor task to CLTC, cross language information retrieval (CLIR), Dumais et al. (1997) find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to ling</context>
</contexts>
<marker>Fortuna, Shawe-Taylor, 2005</marker>
<rawString>Blaz Fortuna and John Shawe-Taylor. 2005. The use of machine translation tools for cross-lingual text mining. In Learning with multiple views – Workshop at the 22nd International Conference on Machine Learning (ICML 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Cross language text categorization by acquiring multilingual domain models from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="13361" citStr="Gliozzo and Strapparava, 2005" startWordPosition="2281" endWordPosition="2284">xtracted from the Collins English Dictionary (CED) for text categorization on the British National Corpus (BNC): information on the provenance of words (ranges of probability distribution of etymologies in different versions of Latin – New Latin, Late Latin, Medieval Latin) was used in a “home-made” range classifier. The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. To this basic representation we add word etymological ancestors and run classification experiments. We then use LSA – previously shown by (Dumais et al., 1997) and (Gliozzo and Strapparava, 2005) to be useful for this task – to induce the latent semantic dimensions of documents and words respectively, hypothesizing that word etymological ancestors will lead to semantic dimensions that transcend language boundaries. The vectors obtained through LSA (on the training data only) for words that are shared by the English training data and the Italian test data (names, and most importantly, etymological ancestors of words in the original documents) are then used for rerepresenting the training and test data. The same process is applied for Italian training and English test data. Classificati</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>553--560</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="10249" citStr="Gliozzo and Strapparava (2006)" startWordPosition="1774" endWordPosition="1777">Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences. In a computationally lighter framework, not dependent on MT, Gliozzo and Strapparava (2006) and Wu et al. (2008) use bilingual lexicons and aligned WordNet synsets to obtain shared features between the training data in language L3 and the testing data in language Lt. Gliozzo an – pairs – and d Strapparava (2005), the first to use comparable as op653 posed to parallel corpora for CLTC, use LSA to build multilingual domain models. The bag-of-word document representation maps a document di from a corpus D into a kdimensional space Rk, where k is the dimension of the (possibly filtered) vocabulary of the corpus: W = {wi, ..., wk}. Position j in the vector representation of di correspond</context>
</contexts>
<marker>Gliozzo, Strapparava, 2006</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2006. Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 553–560, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhong Guo</author>
<author>Min Xiao</author>
</authors>
<title>Cross language text classification via subspace co-regularized multiview learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML 2012),</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="9630" citStr="Guo and Xiao, 2012" startWordPosition="1678" endWordPosition="1681">through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences. In a computationally lighter framework, not dependent on MT, Gliozzo and</context>
</contexts>
<marker>Guo, Xiao, 2012</marker>
<rawString>Yuhong Guo and Min Xiao. 2012. Cross language text classification via subspace co-regularized multiview learning. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012), Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Hitchings</author>
</authors>
<title>The Secret Life of Words: How English Became English.</title>
<date>2009</date>
<publisher>John Murray Publishers.</publisher>
<contexts>
<context position="4929" citStr="Hitchings, 2009" startWordPosition="787" endWordPosition="788">tymology lat: expensa ita: spesa rel:has derived form ita: spese ... ita: spesare rel:etymologically related ita: spesa ... ↓ Proto Indo-European: muh2s lat: expensa rel:etymological origin of eng: expense lat: expensa rel:etymological origin of ita: spesa ... lat: expensa rel:is derived from lat: expensus ... Figure 1: Sample entries from the Etymological WordNet, and a few etymological layers 2 Word Etymology Word etymology gives us a glimpse into the evolution of words in a language. Words may be adopted from a language because of cultural, scientific, economic, political or other reasons (Hitchings, 2009). In time these words “adjust” to the language that adopted them – their sense may change to various degrees – but they are still semantically related to their etymological roots. To illustrate the point, we show an example that the reader, too, may find amusing: on the ticket validation machine on Italian buses, by way of instruction, it is written Per obliterare il biglietto .... A native/frequent English speaker would most probably key in on, and be puzzled by, the word obliterare, very similar to the English obliterate, whose most used sense is to destroy completely/ cause to physically di</context>
</contexts>
<marker>Hitchings, 2009</marker>
<rawString>Henry Hitchings. 2009. The Secret Life of Words: How English Became English. John Murray Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Liu</author>
<author>Lin Dai</author>
<author>Weitao Zhou</author>
<author>Heyan Huang</author>
</authors>
<title>Active learning for cross language text categorization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining (PAKDD 2012),</booktitle>
<pages>195--206</pages>
<location>Kuala Lumpur, Malaysia.</location>
<contexts>
<context position="9444" citStr="Liu et al., 2012" startWordPosition="1646" endWordPosition="1649">urce language L3. In a precursor task to CLTC, cross language information retrieval (CLIR), Dumais et al. (1997) find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semanticall</context>
</contexts>
<marker>Liu, Dai, Zhou, Huang, 2012</marker>
<rawString>Yue Liu, Lin Dai, Weitao Zhou, and Heyan Huang. 2012. Active learning for cross language text categorization. In Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining (PAKDD 2012), pages 195–206, Kuala Lumpur, Malaysia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Crosslanguage text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<pages>1118--1127</pages>
<location>Uppsala,</location>
<contexts>
<context position="9981" citStr="Prettenhofer and Stein (2010)" startWordPosition="1734" endWordPosition="1737"> categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences. In a computationally lighter framework, not dependent on MT, Gliozzo and Strapparava (2006) and Wu et al. (2008) use bilingual lexicons and aligned WordNet synsets to obtain shared features between the training data in language L3 and the testing data in language Lt. Gliozzo an – pairs – and d Strapparava (2005), the first to use comparable as op653 posed to parallel corpora for CLTC, use LSA to build multilingual domai</context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1118–1127, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Rigutini</author>
<author>Marco Maggini</author>
<author>Bing Liu</author>
</authors>
<title>An EM based training algorithm for crosslanguage text categorization.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Web Intelligence (WI</booktitle>
<pages>200--206</pages>
<location>Compiegne, France.</location>
<contexts>
<context position="9536" citStr="Rigutini et al., 2005" startWordPosition="1663" endWordPosition="1666">IR), Dumais et al. (1997) find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language struct</context>
</contexts>
<marker>Rigutini, Maggini, Liu, 2005</marker>
<rawString>Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005. An EM based training algorithm for crosslanguage text categorization. In Proceedings of the International Conference on Web Intelligence (WI 2005), pages 200–206, Compiegne, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Text categorization.</title>
<date>2005</date>
<booktitle>Text Mining and its Applications,</booktitle>
<pages>109--129</pages>
<editor>In Alessandro Zanasi, editor,</editor>
<publisher>WIT Press,</publisher>
<location>Southampton, UK.</location>
<contexts>
<context position="8222" citStr="Sebastiani, 2005" startWordPosition="1457" endWordPosition="1459"> . ⎥ ⎥ ..⎦ . 0 0 1 ··· 0 1 0 1 ··· 1 0 ⎡ English we1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ Lexicon we2 shared ... names and we words p−1 common wep etymology we/i Italian 1 Lexicon ... wetym 1 ... wi1 wi2 ... wi q−1 wiq te 1 te 2 · · · te n−1 te n 0 1 · · · 0 1 1 1 · · · 1 0 0 1 · · · 0 0 0 1 · · · 0 0 1 0 · · · 0 0 0 1 · · · 0 0 0 0 · · · · · · 0 Figure 2: Multilingual word-by-document matrix Text categorization (also text classification), task of automatically sorting a set of documents into categories (or classes or topics) from a prede(Sebastiani, 2005), allows for the quick selection of documents from the same domain, or the same topic. It is a very well research area, dating back to the 60s (Borko and Bernick, 1962). The most frequently, and successfully, used document representation is the Results using this representation achieve accuracy in the 90%s. Most variations include feature filtering or weighing, and variations in learning algorithms (Sebastiani, 2005). Within the area of cross-language text categorization (CLTC) several methods have been explored for producing the model for a target language Lt using information and data from t</context>
<context position="14435" citStr="Sebastiani, 2005" startWordPosition="2454" endWordPosition="2456">e then used for rerepresenting the training and test data. The same process is applied for Italian training and English test data. Classification is done using support vector machines (SVMs). 3.1 Data The data we work with consists of comparable corpora of news articles in English and Italian. Each news article is annotated with one of the four categories: culture and school, tourism, quality of life, made in Italy. Table 1 shows the dataset statistics. The average document length is approximately 300 words. 3.2 Raw cross-lingual text categorization As is commonly done in text categorization (Sebastiani, 2005), the documents in our data are represented as bag-of-words, and classification is done using support vector machines (SVMs). One experimental run consists of 4 binary experiments – one class versus the rest, for each of the 4 classes. The results are reported through micro-averaged precision, recall and F1-score for the targeted class, as well as overall accuracy. The high results, on a par with text categorization experiments in the field, validates our experimental set-up. For the cross language categorization experiments described in this paper, we use the data described above, and train o</context>
</contexts>
<marker>Sebastiani, 2005</marker>
<rawString>Fabrizio Sebastiani. 2005. Text categorization. In Alessandro Zanasi, editor, Text Mining and its Applications, pages 109–129. WIT Press, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
<author>Minhgjun Tian</author>
</authors>
<title>Cross language text classification by model translation and semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<pages>1057--1067</pages>
<location>Uppsala,</location>
<contexts>
<context position="9556" citStr="Shi et al., 2010" startWordPosition="1667" endWordPosition="1670"> find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences</context>
</contexts>
<marker>Shi, Mihalcea, Tian, 2010</marker>
<rawString>Lei Shi, Rada Mihalcea, and Minhgjun Tian. 2010. Cross language text classification by model translation and semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1057–1067, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wan</author>
<author>Rong Pan</author>
<author>Jifei Li</author>
</authors>
<title>Biweighting domain adaptation for cross-language text classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>1535--1540</pages>
<location>Barcelona, Catalonia,</location>
<contexts>
<context position="9649" citStr="Wan et al. (2011)" startWordPosition="1682" endWordPosition="1685">ic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-lan “the set” bag-of-words(BoWs). guage text 0 3 Cross Language Text Categorization fined categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al., 2012); to translate and adapt a model built on language L3 to language Lt (Rigutini et al., 2005), (Shi et al., 2010); to produce parallel corpora for multi-view learning (Guo and Xiao, 2012). Wan et al. (2011) also use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences. In a computationally lighter framework, not dependent on MT, Gliozzo and Strapparava (2006)</context>
</contexts>
<marker>Wan, Pan, Li, 2011</marker>
<rawString>Chang Wan, Rong Pan, and Jifei Li. 2011. Biweighting domain adaptation for cross-language text classification. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011), pages 1535–1540, Barcelona, Catalonia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke Wu</author>
<author>Xiaolin Wang</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Cross language text categorization using a bilingual lexicon.</title>
<date>2008</date>
<booktitle>In Third International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>165--172</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="10270" citStr="Wu et al. (2008)" startWordPosition="1779" endWordPosition="1782"> use machine translation, but enhance the processing through domain adaptation by feature weighing, assuming that the training data in one language and the test data in the other come from different domains, or can exhibit different linguistic phenomena due to linguistic and cultural differences. Prettenhofer and Stein (2010) use a word translation oracle to produce pivots of semantically similar words use the data partitions induced by these words to find cross language structural correspondences. In a computationally lighter framework, not dependent on MT, Gliozzo and Strapparava (2006) and Wu et al. (2008) use bilingual lexicons and aligned WordNet synsets to obtain shared features between the training data in language L3 and the testing data in language Lt. Gliozzo an – pairs – and d Strapparava (2005), the first to use comparable as op653 posed to parallel corpora for CLTC, use LSA to build multilingual domain models. The bag-of-word document representation maps a document di from a corpus D into a kdimensional space Rk, where k is the dimension of the (possibly filtered) vocabulary of the corpus: W = {wi, ..., wk}. Position j in the vector representation of di corresponds to word wj, and it </context>
</contexts>
<marker>Wu, Wang, Lu, 2008</marker>
<rawString>Ke Wu, Xiaolin Wang, and Bao-Liang Lu. 2008. Cross language text categorization using a bilingual lexicon. In Third International Joint Conference on Natural Language Processing (IJCNLP 2008), pages 165–172, Hyderabad, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>