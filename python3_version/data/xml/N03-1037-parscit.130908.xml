<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000151">
<note confidence="0.959692333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 205-211
Edmonton, May-June 2003
</note>
<title confidence="0.99614">
A Web-Trained Extraction Summarization System
</title>
<author confidence="0.887412">
Liang Zhou and Eduard Hovy
</author>
<affiliation confidence="0.72988">
USC Information Sciences Institute
</affiliation>
<address confidence="0.8168985">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.998883">
{liangz, hovy}@isi.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989">
A serious bottleneck in the development of
trainable text summarization systems is the
shortage of training data. Constructing such
data is a very tedious task, especially because
there are in general many different correct
ways to summarize a text. Fortunately we can
utilize the Internet as a source of suitable
training data. In this paper, we present a
summarization system that uses the web as the
source of training data. The procedure involves
structuring the articles downloaded from
various websites, building adequate corpora of
(summary, text) and (extract, text) pairs,
training on positive and negative data, and
automatically learning to perform the task of
extraction-based summarization at a level
comparable to the best DUC systems.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990036238095238">
The task of an extraction-based text summarizer is to
select from a text the most important sentences that are
in size a small percentage of the original text yet still as
informative as the full text (Kupiec et al., 1995).
Typically, trainable summarization systems characterize
each sentence according to a set of predefined features
and then learn from training material which feature
combinations are indicative of good extract sentences.
In order to learn the characteristics of indicative
summarizing sentences, a large enough collection of
(summary, text) pairs must be provided to the system.
Research in automated text summarization is
constantly troubled by the difficulty of finding or
constructing large collections of (extract, text) pairs.
Usually, (abstract, text) pairs are available and can be
easily obtained (though not in sufficient quantity to
support fully automated learning for large domains). But
abstract sentences are not identical to summary
sentences and hence make direct comparison difficult.
Therefore, some algorithms have been introduced to
generate (extract, text) pairs expanded from (abstract,
text) inputs (Marcu, 1999).
The explosion of the World Wide Web has made
accessible billions of documents and newspaper articles.
If one could automatically find short forms of longer
documents, one could build large training sets over
time. However, one cannot today retrieve short and long
texts on the same topic directly.
News published on the Internet is an exception.
Although it is not ideally organized, the topic
orientation and temporal nature of news makes it
possible to impose an organization and thereby obtain a
training corpus on the same topic. We hypothesize that
weekly articles are sophisticated summaries of daily
ones, and monthly articles are summaries of weekly
ones, as shown in Figure 1. Under this hypothesis, how
accurate an extract summarizer can one train? In this
paper we first describe the corpus reorganization, then
in Section 3 the training data formulation and the
system, the system evaluation in Section 4, and finally
future work in Section 5.
daily
</bodyText>
<figureCaption confidence="0.997484">
Figure 1. Corpus structure.
</figureCaption>
<bodyText confidence="0.4188315">
monthly
weekly
</bodyText>
<sectionHeader confidence="0.948417" genericHeader="method">
2 Corpus Construction
</sectionHeader>
<subsectionHeader confidence="0.978001">
2.1 Download Initial Collection
</subsectionHeader>
<bodyText confidence="0.999846692307692">
The Yahoo Full Coverage Collection (YFCC) was
downloaded from http://fullcoverage.yahoo.com during
December 2001. The full coverage texts were
downloaded based on a snapshot of the links contained
in Yahoo Full Coverage at that time. A spider crawled
the top eight categories: U.S., World, Business,
Technology, Science, Health, Entertainment, and
Sports. All news links in each category were saved in an
index page that contained the headline and its full text
URL. A page fetcher then downloaded all the pages
listed in the snapshot index file.
Under the eight categories, there are 463
subcategories, 216590 news articles.
</bodyText>
<subsectionHeader confidence="0.997829">
2.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999956866666667">
All the articles in the YFCC are preprocessed as
following. Each article is in the original raw html form
with actual contents buried in layers of irrelevant tags
and markings. Identifying the text body is a challenging
process (Finn et al., 2001). The system identifies the
main body of the article using a set of retrieval
templates, and then further eliminates useless
information embedded in the main body by considering
each opening and closing tag set. For example, if the tag
name indicates the contents between the opening and
closing tags are images or just meta-info, the contents is
discarded.
The clean texts are then processed by a sentence
breaker, Lovin’s stemmer, a part-of-speech tagger, and
converted into standard XML form.
</bodyText>
<subsectionHeader confidence="0.999361">
2.3 Chronological Reorganization
</subsectionHeader>
<bodyText confidence="0.99997576">
The news articles posted under Yahoo Full Coverage
are from 125 different web publishers. Except for some
well-known sites, the publishing frequencies for the rest
of the sites are not known. But Yahoo tends to use those
publishers over and over again, leaving for each
publisher a trail of publishing habit. Our system records
the publishing date for each article from each publisher
chronologically, and then calculates the publishing
frequency for each publisher. Over all the articles from
a publisher, the system computes the minimum
publishing gap (MPG) between two articles. If the MPG
is less than 3 days or the MPG is unknown in the case of
publishers seen only once in the YFCC, then this
publisher is labeled as a daily publisher. If the MPG is
greater than 3 days but less than 15, it is labeled as a
weekly publisher. Publishers with all other MPG values
are labeled as monthly publishers.
For each article in the collection, the system relabels
it as a daily, weekly, or monthly publication. Each
domain under each category in the collection is then
restructured into a hierarchy by year, months within the
year, weeks of each month, and finally days of each
week. The visualization of an example of the
hierarchical structure of the domain Africa under
category World is shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.997258" genericHeader="method">
3 System
</sectionHeader>
<bodyText confidence="0.9998222">
Recognizing (summary, text) pairs automatically
from the web repository is the key to overcoming the
constant shortage of summarization training data. After
taking a closer examination of the reorganized YFCC,
one notices that for each day, there are a number of
</bodyText>
<figure confidence="0.92358075">
Africa
1999 2000 2001
3 6 7 8 10 1 2 3 ... 1 2 3 ...
Year
Month
1 2 3 4 Week
Day
Articles
</figure>
<figureCaption confidence="0.999919">
Figure 2: The hierarchical structure for domain Africa.
</figureCaption>
<bodyText confidence="0.999883333333333">
articles published that update the progress of a particular
news topic. Daily articles are published by identified
daily publishers. Then at the end of each week, there are
several weekly articles published by weekly publishers
on the same topic. At the end of each month, again there
are articles on the same topic posted by publishers
labeled as monthly publishers. There is a common
thematic connection between the daily articles and the
weekly articles, and between the weekly articles and the
monthly articles. The daily articles on a particular event
are more detailed, and are written step-by-step as it was
happening. The weekly articles review the daily articles
and recite important snippets from the daily news. The
monthly articles are written in a more condensed
fashion quoting from the weeklies.
Instead of asking human judges to identify
informative sentences in documents, and since
beautifully written “summaries” are already available,
we need to align the sentences from the daily articles
with each weekly article sentence, and align weekly
article sentences with each monthly article sentence, in
order to collect the (summary, text) pairs and eventually
generate (extract, text) pairs. The pairs are constructed
at both sentence and document levels.
</bodyText>
<subsectionHeader confidence="0.998289">
3.1 Alignment
</subsectionHeader>
<bodyText confidence="0.9999275">
In our system, three methods for sentence-level and
document-level alignment are investigated:
</bodyText>
<listItem confidence="0.843533">
• extraction-based: Marcu (1999) introduces an
algorithm that produces corresponding extracts
</listItem>
<bodyText confidence="0.956242135135135">
given (abstract, text) tuples with maximal
semantic similarity. We duplicated this algorithm
but replaced inputs with (summary, text), parents
and their respective children in the hierarchical
domain tree. Thus for example, the summary is a
monthly article when the text is a weekly article or
a weekly article when the text is a daily one. We
start with the cosine-similarity metric stated in
(Marcu 1999) and keep deleting sentences that are
not related to the summary document until any
more deletion would result in a drop in similarity
with the summary. The resulting set of sentences is
the extract concerning the topic discussed in the
summary. It forms the pair (extract, text). If there
is more than one summary for a particular text
(nonsummary article), the resulting extracts will
vary if the summary articles are written on the
same event, but are focused on different
perspectives. Thus, a summary article may be
aligned with several extracts and extracts
generated from a single text may align with many
summaries. The relationship amongst summaries,
extracts, and texts forms a network topology.
To generate sentence level alignment, we
replaced the input with (summary sentence, text)
pairs. Starting with a nonsummary text,
the sentences that are irrelevant to the summary
sentence are deleted repeatedly, resulting in the
preservation of sentences similar in meaning to the
summary sentence. For each sentence in the
summary, it is aligned with a number of
nonsummary sentences to form (summary
sentence, nonsummary sentences) pairs. This
alignment is done for each sentence of the
summary articles. Finally for each nonsummary
we group together all the aligned sentences to form
the pair (extract, text).
</bodyText>
<listItem confidence="0.926702">
• similarity-based: inspired by sentence alignment
</listItem>
<bodyText confidence="0.90086232">
for multilingual parallel corpora in Machine
Translation (Church, 1993; Fung and Church,
1994; Melamed, 1999), we view the alignment
between sentences from summaries and sentences
from nonsummaries as the alignment of
monolingual parallel texts at the sentence level. In
every domain of the YFCC, each article is
represented as a vector in a vector space where
each dimension is a distinct non-stop word
appearing in this domain. Measuring the cosine-
similarity between two articles, we can decide
whether they are close semantically. This method
has been widely used in Information Retrieval
(Salton, 1975). To extend this idea, we measure
the cosine-similarity between two sentences, one
from a summary (weekly or monthly article) and
the other one from a nonsummary (daily or weekly
article). If the similarity score between the two
crosses a predetermined threshold, the two
sentences are aligned to form the pair (summary
sentence, text sentence). The relationship between
sentences is many-to-many. With any particular
nonsummary article, sentences that are aligned
with summary sentences form the extract and the
pair (extract, text).
</bodyText>
<listItem confidence="0.907334">
• summary-based: concerned with the noise that
may accompany similarity calculations from
extraction-based and similarity-based alignments,
we align an entire summary article with all its
nonsummary articles published in the same time
period, as determined from the previously
described chronological reorganization. The
alignment results are pairs of the format (summary,
texts). One summary can only be aligned with a
certain group of nonsummaries. Each nonsummary
can be aligned with many summaries. No sentence
level alignment is done with this method.
</listItem>
<subsectionHeader confidence="0.999287">
3.2 Training Data
</subsectionHeader>
<bodyText confidence="0.997467695652174">
The main goal of a leaning-based extraction
summarization system is to learn the ability to judge
whether a particular sentence in a text appear in the
extract or not. Therefore, two sets of training data are
needed, one indicative enough for the system to select a
sentence to be in the extract (labeled as positive data),
the other indicative enough for the system to keep the
sentence from being added to the extract (labeled as
negative data). For each of the alignment methods, we
produce summary training data and nonsummary
training data for each domain in the YFCC.
From extraction-based and similarity-based alignment
methods, for each nonsummary article, there are two
sets of sentences, the set of sentences that compose the
extract with the respect to some summary article or
align with summary sentences, and the rest of the
sentences that are not related to the summary or aligned.
The two sets of sentences over all articles in the domain
form the positive and negative training data sets.
Using summary-based alignment, all the summary
articles are in the positive training set, and all the
nonsummary material is in the negative set. Full texts
are used.
</bodyText>
<subsectionHeader confidence="0.99675">
3.3 Bigram Estimates Extract Desirability
</subsectionHeader>
<bodyText confidence="0.99955375">
We treat each domain independently. Using a bigram
model, we estimate the desirability of a sentence
appearing in the extract P(S) from the summary training
data as:
</bodyText>
<equation confidence="0.992651">
P(S) = P(w1  |start) P(w2  |w1)...P(wn  |wn-1)
</equation>
<bodyText confidence="0.999670333333333">
We estimate the desirability of a sentence not
appearing in the extract P’(S) from the nonsummary
training data as:
</bodyText>
<equation confidence="0.986492">
P’(S) = P’(w1  |start) P’(w2  |w1)...P’(wn  |wn-1)
</equation>
<bodyText confidence="0.990194">
For each domain in the YFCC, a summary bigram
table and a nonsummary bigram table are created.
</bodyText>
<subsectionHeader confidence="0.674463">
3.4 Extraction Process
</subsectionHeader>
<bodyText confidence="0.999796111111111">
Zajic et al. (2002) used a Hidden Markov Model as
part of their headline generation system. In our system,
we started with a similar idea of a lattice for summary
extraction. In Figure 3, E states emit sentences that are
going to be in the extract, and N states emit all other
sentences. Given an input sentence, if P(S) is greater
than P’(S), it means that the sentence has a higher
desirability of being an extraction sentence; otherwise,
the sentence will not be included in the resulting extract.
</bodyText>
<figureCaption confidence="0.992403">
Figure 3. Lattice.
</figureCaption>
<bodyText confidence="0.998033461538461">
After reading in the last sentence from the input, the
extract is created by traversing the path from start state
to end state and only outputting the sentences emitted
by the E states.
The extracts generated are in size shorter than the
original texts. However, the number of sentences that E
states emit cannot be predetermined. This results in
unpredictable extract length. Most frequently, longer
extracts are produced. The system needs more control
over how long extracts will be in order for meaningful
evaluation to be conducted.
To follow up on the lattice idea, we used the
following scoring mechanism:
</bodyText>
<equation confidence="0.997112">
R = P(S) / P’(S)
</equation>
<bodyText confidence="0.997670666666667">
R indicates the desirability ratio of the sentence being in
the extract over it being left out. For each sentence from
the input, it is assigned an R score. Then all the
sentences with their R scores are sorted in descending
order. With respect to the length restriction, we choose
only the top n R-scored sentences.
</bodyText>
<subsectionHeader confidence="0.984969">
3.5 Selecting the Training Domain
</subsectionHeader>
<bodyText confidence="0.981352142857143">
There are 463 domains under the 8 categories of
YFCC, meaning 463 paired summary-bigram and
nonsummary-bigram tables. On average for each
domain, the summary-bigram table contains 20000
entries; the nonsummary-bigram table contains 173000
entries. When an unknown text or a set of unknown
texts come in to be summarized, the system needs to
select the most appropriate pair of bigram tables to
create the extract. The most desirable domain for an
unknown text or texts contains articles focusing on the
same issues as the unknown ones. Two methods are
used:
• topic signature (Lin and Hovy, 2000): a topic
signature is a family of related terms {topic,
signature}, where topic is the target concept and
signature is a vector of related terms. The topic in
the formula is assigned with the domain name. To
construct the set of related words, we consider
only nouns because we are only interested in the
major issues discussed in the domain, not in how
those issues evolved. Each noun in the domain
</bodyText>
<equation confidence="0.9690435">
s
N1
E1
N2 N3
E2 E3
e
</equation>
<bodyText confidence="0.995368214285714">
receives a tf.idf score. 30 top-scoring nouns are
selected to be the signature representing the
domain. For each test text, its signature is
computed with the same tf.idf method against each
domain. The domain that has the highest number
of overlaps in signature words is selected and its
bigram tables are used to construct the extract of
the test text. The following table illustrates. Inputs
are three sets of 10 documents each from the
DUC01 training corpus concerning the topics on
Africa, earthquake, and Iraq, respectively. The
scores are the total overlaps between a domain and
each individual test set. The Three sets are all
correctly classified.
</bodyText>
<table confidence="0.98459875">
domain/input Africa Earthquake Iraq
Africa 24 10 9
Earthquake 7 20 8
Iraq 48 8 97
</table>
<bodyText confidence="0.977186382352941">
• hierarchical signature: each domain is given a
name when it was downloaded. The name gives a
description of the domain at the highest level.
Since the name is the most informative word, if we
gather the words that most frequently co-occur
within the sentence(s) that contain the name itself,
a list of less informative but still important words
can become part of the domain signature. Using
this list of words, we find another list of words that
most frequently co-occur with each of them
individually. Therefore, a three-layer hierarchical
domain signature can be created: level one, the
domain name; level two, 10 words with the highest
co-occurrences with the domain name; level three,
10 words that most frequently co-occur with level
two signatures. Again only nouns are considered.
For example, for domain on Iraq, the level one
signature is “Iraq”; level two signatures are
“Saddam”, “sanction”, “weapon”, “Baghdad”, and
etc.; third level signatures are “Gulf”, “UN”,
“Arab”, “security”, etc. The document signature
for the test text is computed the same way as in the
topic signature method. Overlap between the
domain signature and the document signature is
computed with a different scoring system, in
which the weights are chosen by hand. If level one
is matched, add 10 points; for each match at level
two, add 2 points; for each match at level three,
add 1 point. The domain that receives the highest
points will be selected. A much deeper signature
hierarchy can be created recursively. Through
experiment, we see that a three-level signature
suffices. The following table shows the effects of
this method:
</bodyText>
<table confidence="0.982933">
domain/input Africa Earthquake Iraq
Africa 86 7 41
Earthquake 7 74 0
Iraq 15 26 202
</table>
<bodyText confidence="0.578931666666667">
Since it worked well for our test domains, we
employed the topic-signature method in selecting
training domains.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999726">
4.1 Alignment Choice
</subsectionHeader>
<bodyText confidence="0.976807318181818">
To determine which of the alignment methods of
Section 3.1 is best, we need true summaries, not
monthly or weekly articles from the web. We tested the
equivalencies of the three methods on three sets of
articles from the DUC01 training corpus, which
includes human-generated “gold standard” summaries.
They are on the topics of Africa, earthquake, and Iraq.
The following table shows the results of this
experiment. Each entry demonstrates the cosine
similarity, using the tf.idf score, of the extracts
generated by the system using training data created from
the alignment method in the column, compare to the
summaries generated by human.
extraction similarity summary
Africa 0.273 0.304 0.293
Earthquake 0.318 0.332 0.342
Iraq 0.234 0.246 0.247
We see that all three methods produce roughly equal
extracts, when compared with the gold standard
summaries. The summary-based alignment method is
the least time consuming and the most straightforward
method to use in practice.
</bodyText>
<subsectionHeader confidence="0.998836">
4.2 System Performance
</subsectionHeader>
<bodyText confidence="0.999780103448276">
There are 30 directories in the DUC01 testing corpus.
All articles in each directory are used to make the
selection of its corresponding training domain, as
described in Section 3.5. Even if no domain completely
covers the event, the best one is selected by the system.
To evaluate system performance on summary
creation, we randomly selected one article from each
directory from the DUC01 testing corpus, for each
article, there are three human produced summaries. Our
system summarizes each article three times with the
length restriction respectively set to the lengths of the
three human summaries. We also evaluated the DUC01
single-document summarization baseline system results
(first 100 words from each document) to set a lower
bound. To see the upper bound, each human generated
summary is judged against the other two human
summaries on the same article. DUC01 top performer,
system from SMU, in single-document summarization,
was also evaluated. In all, 30 * 3! human summary
judgments, 30 * 3 baseline summary judgments, 30
SMU system judgments, and 30 * 3 system summary
judgments are made. The following table is the
evaluation results using the SEE system version 1.0 (Lin
2002), with visualization in Figure 4. Summary model
units are graded as full, partial, or none in completeness
in coverage with the peer model units. And Figure 5
shows an example of the comparison between the
human-created summary and the system-generated
extract.
</bodyText>
<table confidence="0.9800126">
SRECALL SPRECISON LRECALL LPRECISION
Baseline 0.246 0.306 0.301 0.396
System 0.452 0.341 0.577 0.509
SMU 0.499 0.482 0.583 0.672
Human 0.542 0.500 0.611 0.585
</table>
<bodyText confidence="0.999901818181818">
desirability, sentences at the end of the extract are less
desirable and can be removed. This needs further
investigation. Clearly there is the need to reduce the size
of the generated summaries. In order to produce simple
and concise extracts, sentence compression needs to be
performed (Knight and Marcu, 2000).
Despite the problems, however, our system’s
performance places it at equal level to the top-scoring
systems in DUC01. Now that the DUC02 material is
also available, we will compare our results to their top-
scoring system as well.
</bodyText>
<subsectionHeader confidence="0.973761">
4.3 Conclusion
</subsectionHeader>
<bodyText confidence="0.9999955">
One important stage in developing a learning-based
extraction summarization system is to find sufficient
and relevant collections of (extract, text) pairs. This task
is also the most difficult one since resources of
constructing the pairs are scarce. To solve this
bottleneck, one wonders whether the web can be seen
as a vast repository that is waiting to be tailored in order
to fulfill our quest in finding summarization training
data. We have discovered a way to find short forms of
longer documents and have built an extraction-based
summarizer learning from reorganizing news articles
from the World Wide Web and performing at a level
comparable to DUC01 systems. We are excited about
the power of how reorganization of the web news
articles has brought us and will explore this idea in other
tasks of natural language processing.
</bodyText>
<sectionHeader confidence="0.96741" genericHeader="method">
5 Future Work
</sectionHeader>
<figure confidence="0.9950122">
0.6
0.8
0.7
0.5
0.4
0.3
0.2
0.1
0
SRECALL SPRECI LRECALL LPRECI
Performance Evaluation
baseline
system
SMU
human
</figure>
<figureCaption confidence="0.999924">
Figure 4. System performance. Multi-document summarization naturally comes
</figureCaption>
<bodyText confidence="0.99882921875">
into picture for future development. Our corpus
organization itself is in the form of multiple articles
being summarized into one (monthly or weekly). How
do we learn and use this structure to summarize a new
set of articles?
Headline generation is another task that we can
approach equipped with our large restructured web
The performance is reported on four metrics. Recall
measures how well a summarizer retains original
content. Precision measures how well a system
generates summaries. SRECALL and SPRECISION are
the strict recall and strict precision that take into
consideration only units with full completeness in unit
coverage. LRECALL and LPRECISION are the lenient
recall and lenient precision that count units with partial
and full completeness in unit coverage. Extract
summaries that are produced by our system has
comparable performance in recall with SMU, meaning
that the coverage of important information is good.
But our system shows weakness in precision due to
the fact that each sentence in the system-generated
extract is not compressed in any way. Each sentence in
the extract has high coverage over the human summary.
But sentences that have no value have also been
included in the result. This causes long extracts on
average, hence, the low average in precision measure.
Since our sentence ranking mechanism is based on
corpus.
We believe that the answers to these questions are
embedded in the characteristics of the corpus, namely
the WWW, and are eager to discover them in the near
future.
</bodyText>
<sectionHeader confidence="0.98311" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9823295">
We want to thank Dr. Chin-Yew Lin for making the
Yahoo Full Coverage Collection download.
Human-produced summary System-produced summary
A major earthquake registering 7.2 on the Richter A major earthquake registering 7.2 on the Richter scale
scale shook the Solomon Islands in the South shook the Solomon Islands in the South Pacific today,
Pacific today . the U.S. Geological Survey says.
It was the largest earthquake in the Solomons since The preliminary reading of 7.2 is slightly stronger than
a 7.4 quake on Nov . 5 , 1978 and the strongest in the 7.1 magnitude earthquake that hit the San Francisco
the world in the five months . Bay area Oct. 17.
An 8.3 quake hit the Macquarie Islands south of It was the largest earthquake in the Solomons since a 7.4
Australia on May 23 . quake on Nov. 5, 1978.
The preliminary reading of 7.2 is slightly stronger There were no immediate reports of injury or damage.
than the 7.1 magnitude earthquake that hit the San An 8.3 quake hit the Macquarie Islands south of
Francisco Bay area Oct . 17 . Australia on May 23.
Major earthquakes in the Solomons usually don&apos;t The Richter scale is a measure of ground motion as
cause much damage or many casualties because recorded on seismographs.
the area is sparsely populated and not extensively Thus a reading of 7.5 reflects an earthquake 10 times
developed . stronger than one of 6.5.
</bodyText>
<figureCaption confidence="0.977">
Figure 5. Comparison of summaries generated by human and system.
</figureCaption>
<sectionHeader confidence="0.996456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99911424">
Kenneth W. Church. 1993. Char align: A program for
aligning parallel texts at the character level. In
Proceedings of the Workshop on Very Large
Corpora: Academic and Industrial Perspectives,
ACL. Association for Computational Linguistics,
1993.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2001. Fact or fiction: Content classification for
digital libraries. In Proceedings of NSF/DELOS
Workshop on Personalization and Recommender.
Pascale Fung and Kenneth W. Church. 1994. K-vec: A
new approach for aligning parallel texts. In
Proceedings from the 15th International Conference
on Computational Linguistics, Kyoto.
Kevin Knight and Daniel Marcu (2000). Statistics-
Based Summarization--Step One: Sentence
Compression. The 17th National Conference of the
American Association for Artificial Intelligence
AAAI&apos;2000, Outstanding Paper Award, Austin,
Texas, July 30-August 3, 2000.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ’95,
Proceedings of the 18th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval. Seattle, Washington, USA.,
pages 68-73. ACM Press.
Chin-Yew Lin. 2001. Summary evaluation environment.
http://www.isi.edu/~cyl/SEE.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text
summarization. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING 2000), Saarbrücken, Germany,
July 31- August 4, 2000.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. The
22nd International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR&apos;99), pages 137-144, Berkeley, CA, August
1999.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics,
25(1):107-130.
Gerard Salton. 1975. A vector space model for
information retrieval. Communications of the ACM,
18(11):613-620, November 1975.
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002.
Automatic headline generation for newspaper
stories. In Proceedings of the ACL-2002 Workshop
on Text Summarization, Philadelphia, PA, 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.366414">
<note confidence="0.842610333333333">Proceedings of HLT-NAACL 2003 Main Papers , pp. 205-211 Edmonton, May-June 2003</note>
<title confidence="0.995037">A Web-Trained Extraction Summarization System</title>
<author confidence="0.982806">Liang Zhou</author>
<author confidence="0.982806">Eduard</author>
<affiliation confidence="0.993519">USC Information Sciences</affiliation>
<address confidence="0.98761">4676 Admiralty</address>
<author confidence="0.692078">Marina del Rey</author>
<author confidence="0.692078">CA</author>
<email confidence="0.999373">liangz@isi.edu</email>
<email confidence="0.999373">hovy@isi.edu</email>
<abstract confidence="0.999403">A serious bottleneck in the development of trainable text summarization systems is the shortage of training data. Constructing such data is a very tedious task, especially because there are in general many different correct ways to summarize a text. Fortunately we can utilize the Internet as a source of suitable training data. In this paper, we present a summarization system that uses the web as the source of training data. The procedure involves structuring the articles downloaded from various websites, building adequate corpora of (summary, text) and (extract, text) pairs, training on positive and negative data, and automatically learning to perform the task of extraction-based summarization at a level comparable to the best DUC systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Char align: A program for aligning parallel texts at the character level.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, ACL. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="9717" citStr="Church, 1993" startWordPosition="1524" endWordPosition="1525">y text, the sentences that are irrelevant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). • similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between </context>
</contexts>
<marker>Church, 1993</marker>
<rawString>Kenneth W. Church. 1993. Char align: A program for aligning parallel texts at the character level. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, ACL. Association for Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
<author>Barry Smyth</author>
</authors>
<title>Fact or fiction: Content classification for digital libraries.</title>
<date>2001</date>
<booktitle>In Proceedings of NSF/DELOS Workshop on Personalization and Recommender.</booktitle>
<contexts>
<context position="4143" citStr="Finn et al., 2001" startWordPosition="630" endWordPosition="633">es: U.S., World, Business, Technology, Science, Health, Entertainment, and Sports. All news links in each category were saved in an index page that contained the headline and its full text URL. A page fetcher then downloaded all the pages listed in the snapshot index file. Under the eight categories, there are 463 subcategories, 216590 news articles. 2.2 Preprocessing All the articles in the YFCC are preprocessed as following. Each article is in the original raw html form with actual contents buried in layers of irrelevant tags and markings. Identifying the text body is a challenging process (Finn et al., 2001). The system identifies the main body of the article using a set of retrieval templates, and then further eliminates useless information embedded in the main body by considering each opening and closing tag set. For example, if the tag name indicates the contents between the opening and closing tags are images or just meta-info, the contents is discarded. The clean texts are then processed by a sentence breaker, Lovin’s stemmer, a part-of-speech tagger, and converted into standard XML form. 2.3 Chronological Reorganization The news articles posted under Yahoo Full Coverage are from 125 differe</context>
</contexts>
<marker>Finn, Kushmerick, Smyth, 2001</marker>
<rawString>Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 2001. Fact or fiction: Content classification for digital libraries. In Proceedings of NSF/DELOS Workshop on Personalization and Recommender.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kenneth W Church</author>
</authors>
<title>K-vec: A new approach for aligning parallel texts.</title>
<date>1994</date>
<booktitle>In Proceedings from the 15th International Conference on Computational Linguistics, Kyoto.</booktitle>
<contexts>
<context position="9740" citStr="Fung and Church, 1994" startWordPosition="1526" endWordPosition="1529">ntences that are irrelevant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). • similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between two sentences, one from</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>Pascale Fung and Kenneth W. Church. 1994. K-vec: A new approach for aligning parallel texts. In Proceedings from the 15th International Conference on Computational Linguistics, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>StatisticsBased Summarization--Step One: Sentence Compression.</title>
<date>2000</date>
<booktitle>The 17th National Conference of the American Association for Artificial Intelligence AAAI&apos;2000, Outstanding Paper Award,</booktitle>
<volume>30</volume>
<location>Austin, Texas,</location>
<contexts>
<context position="21109" citStr="Knight and Marcu, 2000" startWordPosition="3370" endWordPosition="3373">coverage with the peer model units. And Figure 5 shows an example of the comparison between the human-created summary and the system-generated extract. SRECALL SPRECISON LRECALL LPRECISION Baseline 0.246 0.306 0.301 0.396 System 0.452 0.341 0.577 0.509 SMU 0.499 0.482 0.583 0.672 Human 0.542 0.500 0.611 0.585 desirability, sentences at the end of the extract are less desirable and can be removed. This needs further investigation. Clearly there is the need to reduce the size of the generated summaries. In order to produce simple and concise extracts, sentence compression needs to be performed (Knight and Marcu, 2000). Despite the problems, however, our system’s performance places it at equal level to the top-scoring systems in DUC01. Now that the DUC02 material is also available, we will compare our results to their topscoring system as well. 4.3 Conclusion One important stage in developing a learning-based extraction summarization system is to find sufficient and relevant collections of (extract, text) pairs. This task is also the most difficult one since resources of constructing the pairs are scarce. To solve this bottleneck, one wonders whether the web can be seen as a vast repository that is waiting </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu (2000). StatisticsBased Summarization--Step One: Sentence Compression. The 17th National Conference of the American Association for Artificial Intelligence AAAI&apos;2000, Outstanding Paper Award, Austin, Texas, July 30-August 3, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In SIGIR ’95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<pages>68--73</pages>
<publisher>ACM Press.</publisher>
<location>Seattle, Washington, USA.,</location>
<contexts>
<context position="1256" citStr="Kupiec et al., 1995" startWordPosition="190" endWordPosition="193">ion system that uses the web as the source of training data. The procedure involves structuring the articles downloaded from various websites, building adequate corpora of (summary, text) and (extract, text) pairs, training on positive and negative data, and automatically learning to perform the task of extraction-based summarization at a level comparable to the best DUC systems. 1 Introduction The task of an extraction-based text summarizer is to select from a text the most important sentences that are in size a small percentage of the original text yet still as informative as the full text (Kupiec et al., 1995). Typically, trainable summarization systems characterize each sentence according to a set of predefined features and then learn from training material which feature combinations are indicative of good extract sentences. In order to learn the characteristics of indicative summarizing sentences, a large enough collection of (summary, text) pairs must be provided to the system. Research in automated text summarization is constantly troubled by the difficulty of finding or constructing large collections of (extract, text) pairs. Usually, (abstract, text) pairs are available and can be easily obta</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In SIGIR ’95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Seattle, Washington, USA., pages 68-73. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<date>2001</date>
<note>Summary evaluation environment. http://www.isi.edu/~cyl/SEE.</note>
<marker>Lin, 2001</marker>
<rawString>Chin-Yew Lin. 2001. Summary evaluation environment. http://www.isi.edu/~cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000),</booktitle>
<location>Saarbrücken, Germany,</location>
<contexts>
<context position="15165" citStr="Lin and Hovy, 2000" startWordPosition="2403" endWordPosition="2406">ng the Training Domain There are 463 domains under the 8 categories of YFCC, meaning 463 paired summary-bigram and nonsummary-bigram tables. On average for each domain, the summary-bigram table contains 20000 entries; the nonsummary-bigram table contains 173000 entries. When an unknown text or a set of unknown texts come in to be summarized, the system needs to select the most appropriate pair of bigram tables to create the extract. The most desirable domain for an unknown text or texts contains articles focusing on the same issues as the unknown ones. Two methods are used: • topic signature (Lin and Hovy, 2000): a topic signature is a family of related terms {topic, signature}, where topic is the target concept and signature is a vector of related terms. The topic in the formula is assigned with the domain name. To construct the set of related words, we consider only nouns because we are only interested in the major issues discussed in the domain, not in how those issues evolved. Each noun in the domain s N1 E1 N2 N3 E2 E3 e receives a tf.idf score. 30 top-scoring nouns are selected to be the signature representing the domain. For each test text, its signature is computed with the same tf.idf method</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), Saarbrücken, Germany, July 31- August 4, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>The 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99),</booktitle>
<pages>137--144</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="2191" citStr="Marcu, 1999" startWordPosition="323" endWordPosition="324">f (summary, text) pairs must be provided to the system. Research in automated text summarization is constantly troubled by the difficulty of finding or constructing large collections of (extract, text) pairs. Usually, (abstract, text) pairs are available and can be easily obtained (though not in sufficient quantity to support fully automated learning for large domains). But abstract sentences are not identical to summary sentences and hence make direct comparison difficult. Therefore, some algorithms have been introduced to generate (extract, text) pairs expanded from (abstract, text) inputs (Marcu, 1999). The explosion of the World Wide Web has made accessible billions of documents and newspaper articles. If one could automatically find short forms of longer documents, one could build large training sets over time. However, one cannot today retrieve short and long texts on the same topic directly. News published on the Internet is an exception. Although it is not ideally organized, the topic orientation and temporal nature of news makes it possible to impose an organization and thereby obtain a training corpus on the same topic. We hypothesize that weekly articles are sophisticated summaries </context>
<context position="7809" citStr="Marcu (1999)" startWordPosition="1229" endWordPosition="1230">eeklies. Instead of asking human judges to identify informative sentences in documents, and since beautifully written “summaries” are already available, we need to align the sentences from the daily articles with each weekly article sentence, and align weekly article sentences with each monthly article sentence, in order to collect the (summary, text) pairs and eventually generate (extract, text) pairs. The pairs are constructed at both sentence and document levels. 3.1 Alignment In our system, three methods for sentence-level and document-level alignment are investigated: • extraction-based: Marcu (1999) introduces an algorithm that produces corresponding extracts given (abstract, text) tuples with maximal semantic similarity. We duplicated this algorithm but replaced inputs with (summary, text), parents and their respective children in the hierarchical domain tree. Thus for example, the summary is a monthly article when the text is a weekly article or a weekly article when the text is a daily one. We start with the cosine-similarity metric stated in (Marcu 1999) and keep deleting sentences that are not related to the summary document until any more deletion would result in a drop in similari</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. The 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99), pages 137-144, Berkeley, CA, August 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--1</pages>
<contexts>
<context position="9756" citStr="Melamed, 1999" startWordPosition="1530" endWordPosition="1531">vant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). • similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between two sentences, one from a summary (week</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>A vector space model for information retrieval.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<pages>18--11</pages>
<contexts>
<context position="10253" citStr="Salton, 1975" startWordPosition="1608" endWordPosition="1609">gnment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between two sentences, one from a summary (weekly or monthly article) and the other one from a nonsummary (daily or weekly article). If the similarity score between the two crosses a predetermined threshold, the two sentences are aligned to form the pair (summary sentence, text sentence). The relationship between sentences is many-to-many. With any particular nonsummary article, sentences that are aligned with summary sentences form the extract and the pair (extract, text). • summary-based: concerned with the noise that may accompany simi</context>
</contexts>
<marker>Salton, 1975</marker>
<rawString>Gerard Salton. 1975. A vector space model for information retrieval. Communications of the ACM, 18(11):613-620, November 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Automatic headline generation for newspaper stories.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-2002 Workshop on Text Summarization,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="13088" citStr="Zajic et al. (2002)" startWordPosition="2053" endWordPosition="2056"> nonsummary material is in the negative set. Full texts are used. 3.3 Bigram Estimates Extract Desirability We treat each domain independently. Using a bigram model, we estimate the desirability of a sentence appearing in the extract P(S) from the summary training data as: P(S) = P(w1 |start) P(w2 |w1)...P(wn |wn-1) We estimate the desirability of a sentence not appearing in the extract P’(S) from the nonsummary training data as: P’(S) = P’(w1 |start) P’(w2 |w1)...P’(wn |wn-1) For each domain in the YFCC, a summary bigram table and a nonsummary bigram table are created. 3.4 Extraction Process Zajic et al. (2002) used a Hidden Markov Model as part of their headline generation system. In our system, we started with a similar idea of a lattice for summary extraction. In Figure 3, E states emit sentences that are going to be in the extract, and N states emit all other sentences. Given an input sentence, if P(S) is greater than P’(S), it means that the sentence has a higher desirability of being an extraction sentence; otherwise, the sentence will not be included in the resulting extract. Figure 3. Lattice. After reading in the last sentence from the input, the extract is created by traversing the path fr</context>
</contexts>
<marker>Zajic, Dorr, Schwartz, 2002</marker>
<rawString>David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. Automatic headline generation for newspaper stories. In Proceedings of the ACL-2002 Workshop on Text Summarization, Philadelphia, PA, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>