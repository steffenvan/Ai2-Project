<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994108">
Hierarchical Phrase-Based Translation
</title>
<author confidence="0.999171">
David Chiang*
</author>
<affiliation confidence="0.9979245">
Information Sciences Institute
University of Southern California
</affiliation>
<bodyText confidence="0.972245625">
We present a statistical machine translation model that uses hierarchical phrases—phrases
that contain subphrases. The model is formally a synchronous context-free grammar but is
learned from a parallel text without any syntactic annotations. Thus it can be seen as combining
fundamental ideas from both syntax-based translation and phrase-based translation. We describe
our system’s training and decoding methods in detail, and evaluate it for translation speed and
translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system
performs significantly better than the Alignment Template System, a state-of-the-art phrase-
based system.
</bodyText>
<sectionHeader confidence="0.99075" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.996960090909091">
The alignment template translation model (Och and Ney 2004) and related phrase-based
models advanced the state of the art in machine translation by expanding the basic
unit of translation from words to phrases, that is, substrings of potentially unlimited
size (but not necessarily phrases in any syntactic theory). These phrases allow a model
to learn local reorderings, translations of multiword expressions, or insertions and
deletions that are sensitive to local context. This makes them a simple and powerful
mechanism for translation.
The basic phrase-based model is an instance of the noisy-channel approach (Brown
et al. 1993). Following convention, we call the source language “French” and the target
language “English”; the translation of a French sentence f into an English sentence e is
modeled as:
</bodyText>
<equation confidence="0.80097275">
arg max P(e  |f ) = arg max P(e,f ) (1)
e e
= arg max (P(e) x P( f  |e)) (2)
e
</equation>
<bodyText confidence="0.57398">
The phrase-based translation model P( f  |e) “encodes” e into f by the following steps:
</bodyText>
<listItem confidence="0.348218">
1. segment e into phrases ¯e1· · · ¯eI, typically with a uniform distribution over
</listItem>
<note confidence="0.832580285714286">
segmentations;
* 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292, USA. E-mail: chiang@isi.edu. Much of the
research presented here was carried out while the author was at the University of Maryland Institute for
Advanced Computer Studies.
Submission received:1 May 2006; accepted for publication: 3 October 2006.
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
</note>
<listItem confidence="0.984440666666667">
2. reorder the ¯ei according to some distortion model;
3. translate each of the ¯ei into French phrases according to a model P(f¯  |¯e)
estimated from the training data.
</listItem>
<bodyText confidence="0.938377585365854">
Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002)
or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002). But
the basic architecture of phrase segmentation (or generation), phrase reordering, and
phrase translation remains the same.
Phrase-based models can robustly perform translations that are localized to sub-
strings that are common enough to have been observed in training. But Koehn, Och, and
Marcu (2003) find that phrases longer than three words improve performance little for
training corpora of up to 20 million words, suggesting that the data may be too sparse
to learn longer phrases. Above the phrase level, some models perform no reordering
(Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion
model that reorders phrases independently of their content (Koehn, Och, and Marcu
2003; Och and Ney 2004), and some, for example, the Alignment Template System
(Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system
(Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add
some lexical sensitivity. But, as an illustration of the limitations of phrase reordering,
consider the following Mandarin example and its English translation:
M1)11&apos;1 � Æ JLF01 P �� n �� 0* �-
Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi
Australia is with North Korea have dipl. rels. that few countries one of
.
.
Australia is one of the few countries that have diplomatic relations with North Korea.
If we count zhiyi (literally, ‘of-one’) as a single token, then translating this sentence
correctly into English requires identifying a sequence of five word groups that need
to be reversed. When we run a phrase-based system, ATS, on this sentence (using the
experimental setup described herein), we get the following phrases with translations:
[Aozhou] [shi]1 [yu Beihan]2 [you] [bangjiao] [de shaoshu guojia zhiyi] [.]
[Australia] [has] [dipl. rels.] [with North Korea]2 [is]1 [one of the few countries] [.]
where we have used subscripts to indicate the reordering of phrases. The phrase-based
model is able to order “has diplomatic relations with North Korea” correctly (using
phrase reordering) and “is one of the few countries” correctly (using a combination of
phrase translation and phrase reordering), but does not invert these two groups as it
should.
We propose a solution to these problems that does not interfere with the strengths
of the phrase-based approach, but rather capitalizes on them: Because phrases are good
for learning reorderings of words, we can use them to learn reorderings of phrases as
well. In order to do this we need hierarchical phrases that can contain other phrases.
For example, a hierarchical phrase pair that might help with the above example is
(yu 1 you 2 , have 2 with 1 ) (3)
where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture
the fact that Chinese prepositional phrases almost always modify verb phrases on the
</bodyText>
<page confidence="0.520361">
202
</page>
<note confidence="0.636152">
Chiang Hierarchical Phrase-Based Translation
</note>
<bodyText confidence="0.9881794375">
left, whereas English prepositional phrases usually modify verb phrases on the right.
Because it generalizes over possible prepositional objects and direct objects, it acts both
as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably
more powerful than a conventional phrase pair.
Similarly, the hierarchical phrase pair
( 1 de 2 , the 2 that 1 ) (4)
would capture the fact that Chinese relative clauses modify NPs on the left, whereas
English relative clauses modify on the right; and the pair
( 1 zhiyi, one of 1 ) (5)
would render the construction zhiyi in English word order. These three rules, along with
some conventional phrase pairs, suffice to translate the sentence correctly:
[Aozhou] [shi] [[[yu [Beihan]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi]
[Australia] [is] [one of [the [few countries]3 that [have [dipl. rels.]2 with [N. Korea]1]]]
The system we describe in this article uses rules like (3), (4), and (5), which we formalize
in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover,
the system is able to learn them automatically from a parallel text without syntactic
annotation.
Because our system uses a synchronous CFG, it could be thought of as an example
of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997;
Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful
but has not previously produced systems that can compete with phrase-based systems
in large-scale translation tasks such as the evaluations held by NIST. Our approach
differs from early syntax-based statistical translation models in combining the idea of
hierarchical structure with key insights from phrase-based MT: Crucially, by incorpo-
rating the use of elementary structures with possibly many words, we hope to inherit
phrase-based MT’s capacity for memorizing translations from parallel data. Other in-
sights borrowed from the current state of the art include minimum-error-rate training of
log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model.
The conjunction of these various elements presents a considerable challenge for
implementation, which we discuss in detail in this article. The result is the first system
employing a grammar (to our knowledge) to perform better than phrase-based systems
in large-scale evaluations.2
</bodyText>
<note confidence="0.4827264">
1 The actual derivation used varies in practice. A previous version of the model selected precisely the
derivation shown in the text, although the version described in this article happens to select a less
intuitive one:
[Aozhou shi] [[[yu]1 Beihan [you [bangjiao]2 de [shaoshu]3 guojia]] zhiyi .]
[Australia is] [one of the [[[few]3 countries having [diplomatic relations]2] [with]1 North Korea] .]
</note>
<footnote confidence="0.628565666666667">
2 An earlier version of the system described in this article was entered by the University of Maryland
as its primary system in the 2005 NIST MT Evaluation. The results can be found at
http://www.nist.gov/speech/tests/mt/mt05eval official results release 20050801 v3.html.
</footnote>
<page confidence="0.716783">
203
</page>
<note confidence="0.802778">
Computational Linguistics Volume 33, Number 2
</note>
<sectionHeader confidence="0.998198" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999744166666667">
Approaches to syntax-based statistical MT have varied in their reliance on syntactic
theories, or annotations made according to syntactic theories. At one extreme are
those, exemplified by that of Wu (1997), that have no dependence on syntactic the-
ory beyond the idea that natural language is hierarchical. If these methods distin-
guish between different categories, they typically do not distinguish very many. Our
approach, as presented here, falls squarely into this family. By contrast, other ap-
proaches, exemplified by that of Yamada and Knight (2001), do make use of parallel
data with syntactic annotations, either in the form of phrase-structure trees or de-
pendency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because
syntactically annotated corpora are comparatively small, obtaining parsed parallel text
in quantity usually entails running an automatic parser on a parallel corpus to produce
noisy annotations.
Both of these strands of research have recently begun to explore extraction of
larger rules, guided by word alignments. The extraction method we use, which is a
straightforward generalization of phrase extraction from word-aligned parallel text, has
been independently proposed before in various settings. The method of Block (2000) is
the earliest instance we are aware of, though it is restricted to rules with one variable.
The same method has also been used by Probst et al. (2002) and Xia and McCord (2004)
in conjunction with syntactic annotations to extract rules that are used for reordering
prior to translation. Finally, Galley et al. (2004) use the same method to extract a very
large grammar from syntactically annotated data. The discontinuous phrases used by
Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have
variables that stand for single words rather than subderivations, and they can interleave
in non-hierarchical ways.
</bodyText>
<sectionHeader confidence="0.910933" genericHeader="method">
3. Grammar
</sectionHeader>
<bodyText confidence="0.999899666666667">
The model is based on a synchronous CFG, elsewhere known as a syntax-directed
transduction grammar (Lewis and Stearns 1968). We give here an informal definition
and then describe in detail how we build a synchronous CFG for our model.
</bodyText>
<subsectionHeader confidence="0.998331">
3.1 Synchronous CFG
</subsectionHeader>
<bodyText confidence="0.9858835">
In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of
right-hand sides:
</bodyText>
<equation confidence="0.998328">
X — (γ, α, —)
</equation>
<bodyText confidence="0.89327675">
where X is a nonterminal, γ and α are both strings of terminals and nonterminals,
and — is a one-to-one correspondence between nonterminal occurrences in γ and
nonterminal occurrences in α. For example, the hierarchical phrase pairs (3), (4), and
(5) previously presented could be formalized in a synchronous CFG as:
</bodyText>
<equation confidence="0.9668734">
X �yu X 1 you X 2 , have X 2 with X 1) (6)
X �X 1 de X 2 , the X 2 that X 1 ) (7)
204
Chiang Hierarchical Phrase-Based Translation
X -4 (X 1 zhiyi, one of X 1 ) (8)
</equation>
<bodyText confidence="0.984267">
where we have used boxed indices to indicate which nonterminal occurrences are linked
by —. The conventional phrase pairs would be formalized as:
</bodyText>
<table confidence="0.995872125">
X -4 (Aozhou, Australia)
X -4 (Beihan, North Korea)
X -4 (shi, is)
X -4 (bangjiao, diplomatic relations)
X -4 (shaoshu guojia, few countries)
Two more rules complete our example:
S -4 (S 1 X 2 ,S 1 X 2)
S -4 (X 1 , X 1 )
</table>
<bodyText confidence="0.892363">
A synchronous CFG derivation begins with a pair of linked start symbols. At each step,
two linked nonterminals are rewritten using the two components of a single rule. When
denoting links with boxed indices, we must consistently reindex the newly introduced
symbols apart from the symbols already present. For an example using these rules, see
Figure 1.
</bodyText>
<subsectionHeader confidence="0.999729">
3.2 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.984358846153846">
The bulk of the grammar consists of automatically extracted rules. The extraction
process begins with a word-aligned corpus: a set of triples (f, e, —), where f is a French
sentence, e is an English sentence, and — is a (many-to-many) binary relation between
positions off and positions of e. The word alignments are obtained by running GIZA++
(Och and Ney 2000) on the corpus in both directions, and forming the union of the two
sets of word alignments.
We then extract from each word-aligned sentence pair a set of rules that are
consistent with the word alignments. This can be thought of in two steps. First, we
identify initial phrase pairs using the same criterion as most phrase-based systems
(Och and Ney 2004), namely, there must be at least one word inside one phrase
aligned to a word inside the other, but no word inside one phrase can be aligned to
a word outside the other phrase. For example, suppose our training data contained the
fragment
</bodyText>
<figure confidence="0.997293533333333">
30 ��F-*
30 duonianlai
30 plus-years-past
�:f
youhao
friendly
[7
hezou
cooperation
n
de
of
friendly cooperation over the last 30 years
205
Computational Linguistics Volume 33, Number 2
</figure>
<figureCaption confidence="0.980222">
Figure 1
</figureCaption>
<bodyText confidence="0.816667428571429">
Example derivation of a synchronous CFG. Numbers above arrows are rules used at each step.
with word alignments as shown in Figure 2a. The initial phrases that would be extracted
are shown in Figure 2b. More formally:
Definition 1
Given a word-aligned sentence pair ( f, e, —), let fji stand for the substring of f from
position i to position j inclusive, and similarly for eji. Then a rule ( fj, ele ) is an initial
phrase pair of ( f, e, —) iff:
</bodyText>
<listItem confidence="0.999237666666667">
1. fk — eke for some k E [i, j] and k&apos; E [i&apos;, j&apos;];
2. fk —6 eke for all k E [i, j] and k&apos; E/ [i&apos;, j&apos;];
3. fk —6 eke for all k E/ [i, j] and k&apos; E [i&apos;, j&apos;].
</listItem>
<bodyText confidence="0.992610333333333">
Second, in order to obtain rules from the phrases, we look for phrases that contain
other phrases and replace the subphrases with nonterminal symbols. For example,
given the initial phrases shown in Figure 2b, we could form the rule
</bodyText>
<equation confidence="0.766583">
X -4 (X 1 duonianlai de X 2 , X 2 over the last X 1 years) (16)
</equation>
<page confidence="0.600112">
206
</page>
<figure confidence="0.725616666666667">
Chiang Hierarchical Phrase-Based Translation
Figure 2
Grammar extraction example: (a) Input word alignment. (b) Initial phrases. (c) Example rule.
207
Computational Linguistics Volume 33, Number 2
as shown in Figure 2c. More formally:
Definition 2
The set of rules of (f, e, —) is the smallest set satisfying the following:
1. If (fji , ej�
</figure>
<equation confidence="0.969520222222222">
i~ ) is an initial phrase pair, then
X -4 ( fj i , ej�
i~ )
is a rule of (f, e, —).
2. If (X -+ (γ, α)) is a rule of (f, e, —) and (fji , ej�
i~ ) is an initial phrase pair such
that γ = γ1 fj i γ2 and α = α1ej�
i, α2, then
X -4 (γ1X k γ2, α1X k α2)
</equation>
<bodyText confidence="0.998750111111111">
where k is an index not used in γ and α, is a rule of (f, e, —).
This scheme generates a very large number of rules, which is undesirable not only
because it makes training and decoding very slow, but also because it creates spurious
ambiguity—a situation where the decoder produces many derivations that are distinct
yet have the same model feature vectors and give the same translation. This can result in
k-best lists with very few different translations or feature vectors, which is problematic
for the minimum-error-rate training algorithm (see Section 4.3). To avoid this, we filter
our grammar according to the following constraints, chosen to balance grammar size
and performance on our development set:
</bodyText>
<listItem confidence="0.994311153846154">
1. If there are multiple initial phrase pairs containing the same set of
alignments, only the smallest is kept. That is, unaligned words are not
allowed at the edges of phrases.
2. Initial phrases are limited to a length of 10 words on either side.
3. Rules are limited to five nonterminals plus terminals on the French side.
4. Rules can have at most two nonterminals, which simplifies the decoder
implementation. This also makes our grammar weakly equivalent to an
inversion transduction grammar (Wu 1997), although the conversion
would create a very large number of new nonterminal symbols.
5. It is prohibited for nonterminals to be adjacent on the French side, a major
cause of spurious ambiguity.
6. A rule must have at least one pair of aligned words, so that translation
decisions are always based on some lexical evidence.
</listItem>
<figure confidence="0.301955">
Variations of constraints (1) and (2) are also commonly used in phrase-based systems.
208
Chiang Hierarchical Phrase-Based Translation
</figure>
<subsectionHeader confidence="0.994876">
3.3 Other Rules
</subsectionHeader>
<bodyText confidence="0.999846166666667">
Glue rules. Having extracted rules from the training data, we could let X be the gram-
mar’s start symbol and translate new sentences using only the extracted rules. But
for robustness and for continuity with phrase-based translation models, we allow the
grammar to divide a French sentence into a sequence of chunks and translate one chunk
at a time. We formalize this inside a synchronous CFG using the rules (14) and (15),
which we call the glue rules, repeated here:
</bodyText>
<equation confidence="0.99919">
S → (S 1 X 2 ,S 1 X 2) (14)
S → �X 1 , X 1 ) (15)
</equation>
<bodyText confidence="0.9998846">
These rules analyze an S (the start symbol) as a sequence of Xs which are translated
without reordering. Note that if we restricted our grammar to comprise only the glue
rules and conventional phrase pairs (that is, rules without nonterminal symbols on the
right-hand side), the model would reduce to a phrase-based model with monotone
translation (no phrase reordering).
Entity rules. Finally, for each sentence to be translated, we run some specialized transla-
tion modules to translate the numbers, dates, numbers, and bylines in the sentence, and
insert these translations into the grammar as new rules.3 Such modules are often used
by phrase-based systems as well, but here their translations can plug into hierarchical
phrases, for example, into the rule
</bodyText>
<equation confidence="0.966148">
X → �X 1 duonianlai, over the last X 1 years) (17)
</equation>
<bodyText confidence="0.642179">
allowing it to generalize over numbers of years.
</bodyText>
<sectionHeader confidence="0.778323" genericHeader="method">
4. Model
</sectionHeader>
<bodyText confidence="0.992275">
Given a French sentence f, a synchronous CFG will have, in general, many derivations
that yield f on the French side, and therefore (in general) many possible translations e.
We now define a model over derivations D to predict which translations are more likely
than others.
</bodyText>
<subsectionHeader confidence="0.98512">
4.1 Definition
</subsectionHeader>
<bodyText confidence="0.998468">
Following Och and Ney (2002), we depart from the traditional noisy-channel approach
and use a more general log-linear model over derivations D:
</bodyText>
<equation confidence="0.969551">
P(D) a 11 φi(D)λi (18)
i
</equation>
<bodyText confidence="0.826520875">
3 These modules are due to U. Germann and F. J. Och. In a previous paper (Chiang et al. 2005) we reported
on translation modules for numbers and names. The present modules are not the same as those, though
the mechanism for integrating them is identical.
209
Computational Linguistics Volume 33, Number 2
where the φi are features defined on derivations and the λi are feature weights. One of
the features is an m-gram language model PLM(e); the remainder of the features we will
define as products of functions on the rules used in a derivation:
</bodyText>
<equation confidence="0.9884558">
φi(D) = rl φi(X - (γ, α)) (19)
(X-.(y,α))ED
Thus we can rewrite P(D) as
P(D) a PLM(e)7LM x rl rl φi(X - (γ, α))7i (20)
i#LM (X-.(y,α))ED
</equation>
<bodyText confidence="0.9962075">
The factors other than the language model factor can be put into a particularly con-
venient form. A weighted synchronous CFG is a synchronous CFG together with a
function w that assigns weights to rules. This function induces a weight function over
derivations:
</bodyText>
<equation confidence="0.947732285714286">
w(D) = rl w(X -+ (γ, α)) (21)
(X-.(y,α))ED
If we define
w(X - (γ,α)) = rl φi(X - (γ, α))7i (22)
i#LM
then the probability model becomes
P(D) a PLM(e)7LM x w(D) (23)
</equation>
<bodyText confidence="0.99987275">
It is easy to write dynamic-programming algorithms to find the highest-weight transla-
tion or k-best translations with a weighted synchronous CFG. Therefore it is problematic
that w(D) does not include the language model, which is extremely important for
translation quality. We return to this challenge in Section 5.
</bodyText>
<subsectionHeader confidence="0.885422">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999449666666667">
For our experiments, we use a feature set analogous to the default feature set of
Pharaoh (Koehn, Och, and Marcu 2003). The rules extracted from the training bitext
have the following features:
</bodyText>
<listItem confidence="0.90584">
• P(γ  |α) and P(α  |γ), the latter of which is not found in the noisy-channel
model, but has been previously found to be a helpful feature (Och and
Ney 2002);
210
Chiang Hierarchical Phrase-Based Translation
• the lexical weights Pw(γ  |α) and Pw(α  |γ), which estimate how well the
words in α translate the words in γ (Koehn, Och, and Marcu 2003);4
• a penalty exp(−1) for extracted rules, analogous to Koehn’s phrase
penalty (Koehn 2003), which allows the model to learn a preference for
longer or shorter derivations.
Next, there are penalties exp(−1) for various other classes of rules:
• for the glue rule (14), so that the model can learn a preference for
hierarchical phrases over a serial combination of phrases,
• for each of the four types of rules (numbers, dates, names, bylines)
inserted by the specialized translation modules, so that the model can
learn how much to rely on each of them.
</listItem>
<bodyText confidence="0.982775">
Finally, for all the rules, there is a word penalty exp(−#T(α)), where #T just counts
terminal symbols. This allows the model to learn a general preference for shorter or
longer outputs.
</bodyText>
<subsectionHeader confidence="0.997571">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.995568791666667">
In order to estimate the parameters of the phrase translation and lexical-weighting
features, we need counts for the extracted rules. For each sentence pair in the training
data, there is in general more than one derivation of the sentence pair using the rules
extracted from it. Because we have observed the sentence pair but have not observed
the derivations, we do not know how many times each derivation has been seen, and
therefore we do not actually know how many times each rule has been seen.
Following Och and others, we use heuristics to hypothesize a distribution of possi-
ble rules as though we observed them in the training data, a distribution that does not
necessarily maximize the likelihood of the training data.5 Och’s method gives a count
of one to each extracted phrase pair occurrence. We likewise give a count of one to
each initial phrase pair occurrence, then distribute its weight equally among the rules
obtained by subtracting subphrases from it. Treating this distribution as our observed
data, we use relative-frequency estimation to obtain P(γ  |α) and P(α  |γ).
Finally, the parameters λi of the log-linear model (18) are learned by minimum-
error-rate training (Och 2003), which tries to set the parameters so as to maximize
the BLEU score (Papineni et al. 2002) of a development set. This gives a weighted
synchronous CFG according to (22) that is ready to be used by the decoder.
4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in
training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical
weight; we take a weighted average.
5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its
successors, which use heuristics to hypothesize an augmented version of the training data, but it is
especially reminiscent of the Data Oriented Parsing method (Bod 1992), which hypothesizes a
distribution over many possible derivations of each training example from subtrees of varying sizes.
</bodyText>
<figure confidence="0.425687">
211
Computational Linguistics Volume 33, Number 2
</figure>
<sectionHeader confidence="0.84286" genericHeader="method">
5. Decoding
</sectionHeader>
<bodyText confidence="0.954580818181818">
In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search to-
gether with a postprocessor for mapping French derivations to English derivations.
Given a French sentence f, it finds the English yield of the single best derivation that
has French yield f:
�eˆ = e arg max P(D) (24)
Ds.t.f(D)=f
Note that this is not necessarily the highest-probability English string, which would
require a more expensive summation over derivations.
We now discuss the details of the decoder, focusing attention on efficiently calculat-
ing English language-model probabilities for possible translations, which is the primary
technical challenge.
</bodyText>
<subsectionHeader confidence="0.996594">
5.1 Basic Algorithm
</subsectionHeader>
<bodyText confidence="0.99998775">
In the following we present several parsers as deductive proof systems (Shieber,
Schabes, and Pereira 1995; Goodman 1999). A parser in this notation defines a space
of weighted items, in which some items are designated axioms and some items are
designated goals (the items to be proven), and a set of inference rules of the form
</bodyText>
<equation confidence="0.9986085">
I1 : w1 ··· Ik : wk φ
I : w
</equation>
<bodyText confidence="0.999532857142857">
which means that if all the items Ii (called the antecedents) are provable, with weight wi,
then I (called the consequent) is provable, with weight w, provided the side condition φ
holds. The parsing process grows a set of provable items: It starts with the axioms,
and proceeds by applying inference rules to prove more and more items until a goal is
proven.
For example, the well-known CKY algorithm for CFGs in Chomsky normal form
can be thought of as a deductive proof system whose items can take one of two forms:
</bodyText>
<listItem confidence="0.994646">
• [X, i, j], indicating that a subtree rooted in X has been recognized spanning
from i to j (that is, spanning fji+1), or
• (X -+ γ), if a rule X -4 γ belongs to the grammar.6
</listItem>
<bodyText confidence="0.671127">
The axioms would be
</bodyText>
<equation confidence="0.997841">
X -+ γ : w
(X -w4 γ) E G
</equation>
<bodyText confidence="0.9466495">
6 Treating grammar rules as axioms is not standard practice, but advocated by Goodman (1999). Here, it
has the benefit of simplifying the presentation in Section 5.3.4.
</bodyText>
<page confidence="0.553078">
212
</page>
<subsectionHeader confidence="0.228817">
Chiang Hierarchical Phrase-Based Translation
</subsectionHeader>
<bodyText confidence="0.926457">
and the inference rules would be
</bodyText>
<equation confidence="0.99602225">
Z -+ fi+1 : w
[Z,i,i+1]:w
Z -+ XY : w [X, i, k] : w1 [Y, k, j] : w2
[Z, i,j] : w1w2w
</equation>
<bodyText confidence="0.999431">
and the goal would be [S, 0, n], where S is the start symbol of the grammar and n is the
length of the input string f.
Given a synchronous CFG, we could convert its French-side grammar into
Chomsky normal form, and then for each sentence, we could find the best parse using
CKY. Then it would be a straightforward matter to revert the best parse from Chomsky
normal form into the original form and map it into its corresponding English tree, whose
yield is the output translation. However, because we have already restricted the number
of nonterminal symbols in our rules to two, it is more convenient to use a modified CKY
algorithm that operates on our grammar directly, without any conversion to Chomsky
normal form. The axioms, inference rules, and goals for the basic decoder are shown in
Figure 3. Its time complexity is O(n3), just as CKY’s is. Because this algorithm does not
yet incorporate a language model, let us call it the −LM parser.
The actual search procedure is given by the pseudocode in Figure 4. It organizes the
proved items into an array chart whose cells chart[X, i, j] are sets of items. The cells are
ordered such that every item comes after its possible antecedents: smaller spans before
larger spans, and X items before S items (because of the unary rule S → �X 1 , X 1 )).
Then the parser can proceed by visiting the chart cells in order and trying to prove
all the items for each cell. Whenever it proves a new item, it adds the item to the
</bodyText>
<figure confidence="0.89820125">
Figure 3
Inference rules for the −LM parser.
213
Computational Linguistics Volume 33, Number 2
</figure>
<figureCaption confidence="0.992127">
Figure 4
</figureCaption>
<bodyText confidence="0.999319882352941">
Search procedure for the −LM parser.
appropriate chart cell; in order to reconstruct the derivations later, it must also store,
with each item, a tuple of back-pointers to the antecedents from which the item was
deduced (for axioms, an empty tuple is used). If two items are added to a cell that are
equivalent except for their weights or back-pointers, then they are merged (in the MT
decoding literature, this is also known as hypothesis recombination), with the merged
item taking its weight and back-pointers from the better of the two equivalent items.
(However, if we are interested in finding the k-best derivations, the merged item gets
the multiset of all the tuples of back-pointers from the equivalent items. These back-
pointers are used below in Section 5.2.)
The algorithm in Figure 4 does not completely search the space of proofs, because
it has a constraint that prohibits any X from spanning a substring longer than a fixed
limit Λ on the French side, corresponding to the maximum length constraint on initial
rules during training. This gives the decoding algorithm an asymptotic time complexity
of O(n). In principle Λ should match the initial phrase length limit used in training (as
it does in our experiments), but in practice it can be adjusted separately to maximize
accuracy or speed.
</bodyText>
<subsectionHeader confidence="0.999074">
5.2 Generating k-best Lists
</subsectionHeader>
<bodyText confidence="0.9991653">
We often want to find not only the best derivation for a French sentence but a list of the
k-best derivations. These are used for minimum-error-rate training and for rescoring
with a language model (Section 5.3.1). We describe here how to do this using the lazy
algorithm of Huang and Chiang (2005). Part of this method will also be reused in our
algorithm for fast parsing with a language model (Section 5.3.4).
If we conceive of lists as functions from indices to values, we may create a virtual
list, a function that computes member values on demand instead of storing all the
values statically. The heart of the k-best algorithm is a function MERGEPRODUCTS,
which takes a set G of tuples of (virtual) lists with an operator ⊗ and returns a
virtual list:
</bodyText>
<equation confidence="0.8337885">
MERGEPRODUCTS(G, ⊗) = U ~~xi xi ∈ Li (25)
(L1,...Ln)∈G i
</equation>
<page confidence="0.483446">
214
</page>
<figure confidence="0.523439">
Chiang Hierarchical Phrase-Based Translation
</figure>
<figureCaption confidence="0.673215">
Figure 5
</figureCaption>
<bodyText confidence="0.963653466666667">
Example illustrating MERGEPRODUCTS, where L1 = {1, 2,6, 10} and L2 = {1, 4, 7}. Numbers are
negative log-probabilities.
It assumes that the input lists are sorted and returns a sorted list. A naive im-
plementation of MERGEPRODUCTS would simply calculate all possible products and
sort; however, if we are only interested in the top part of the result, we can implement
MERGEPRODUCTS so that the output values are computed lazily and the input lists are
accessed only as needed. To do this, we must assume that the multiplication operator ®
is monotonic in each of its arguments. By way of motivation, consider the simple case
G = {(L1,L2)}. The full set of possible products can be arranged in a two-dimensional
grid (see Figure 5a), which we could then sort to obtain MERGEPRODUCTS(G). But
because of our assumptions, we know that the first element of MERGEPRODUCTS(G)
must be L1[1] ® L2[1]. Moreover, we know that the second element must be either
L1[1] ® L2[2] or L1[2] ® L2[1]. In general (see Figure 5b), if some of the cells have been
previously enumerated, the next cell must be one of the cells (shaded gray) adjacent to
the previously enumerated ones and we need not consider the others (shaded white).
In this way, if we only want to compute the first few elements of MERGEPRODUCTS(G),
we can do so by performing a small number of products and discarding the rest of
the grid.
Figure 6 shows the pseudocode for MERGEPRODUCTS.7 In lines 2–5, a priority
queue is initialized with the best element from each L E G, where L ranges over tuples of
lists, and 1 stands for a vector whose elements all have the value 1 (the dimensionality of
the vector should be evident from the context). The rest of the function creates the virtual
list: To enumerate the next element of the list, we first insert the elements adjacent to
the previously enumerated element, if any (lines 9–13, where bi stands for the vector
whose ith element is 1 and is zero elsewhere), and then enumerate the best element
in the priority queue, if any (lines 14–18). We assume standard implementations of
7 This version corrects the behavior of the previously published version in some boundary conditions.
Thanks to D. Smith and J. May for pointing those cases out. In the actual implementation, an earlier
version is used which has the correct behavior but not for cyclic forests (which the parser never
produces).
</bodyText>
<figure confidence="0.7663255">
215
Computational Linguistics Volume 33, Number 2
</figure>
<figureCaption confidence="0.955039">
Figure 6
</figureCaption>
<bodyText confidence="0.989394636363636">
Function for computing the union of products of sorted lists (Huang and Chiang 2005).
the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al.
2001).
The k-best list generator is then easy to define (Figure 7). First, we generate a parse
forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using
memoization to ensure that we generate only one k-best list for each item in the forest.
The pseudocode in Figure 7 will find only the weights for the k-best derivations; extend-
ing it to output the translations as well is a matter of modifying line 5 to package the
English sides of rules together with the weights w, and replacing the real multiplication
operator × in line 9 with one that not only multiplies weights but also builds partial
translations out of subtranslations.
</bodyText>
<figureCaption confidence="0.976987">
Figure 7
</figureCaption>
<table confidence="0.497296">
Algorithm for computing k-best lists (Huang and Chiang 2005).
216
Chiang Hierarchical Phrase-Based Translation
</table>
<subsectionHeader confidence="0.993909">
5.3 Adding the Language Model
</subsectionHeader>
<bodyText confidence="0.993253678571428">
We now turn to the problem of incorporating the language model (LM), describing
three methods: first, using the −LM parser to obtain a k-best list of translations and
rescoring it with the LM; second, incorporating the LM directly into the grammar in
a construction reminiscent of the intersection of a CFG with a finite-state automaton;
third, a hybrid method which we call cube pruning.
5.3.1 Rescoring. One easy way to incorporate the LM into the model would be to decode
first using the −LM parser to produce a k-best list of translations, then to rescore the
k-best list using the LM. This method has the potential to be very fast: linear in k.
However, because the number of possible translations is exponential in n, we may have
to set k extremely high in order to find the true best translation (taking the LM into
account) or something acceptably close to it.
5.3.2 Intersection. A more principled solution would be to calculate the LM probabilities
online. To do this, we view an m-gram LM as a weighted finite state machine M in
which each state corresponds to a sequence of (m − 1) English terminal symbols. We
can then intersect the English side of our weighted CFG G with this finite-state machine
to produce a new weighted CFG that incorporates M. Thus PLM would be part of the
rule weights (22) just like the other features. (For notational consistency, however, we
write the LM probabilities separately from the rule weights.) In principle this method
should admit no search errors, though in practice the blow-up in the effective size of the
grammar necessitates pruning of the search space, which can cause search errors.
The classic construction for intersecting a (non-synchronous) CFG with a finite-
state machine is due to Bar-Hillel, Perles, and Shamir (1964), but we use a slightly
different construction proposed by Wu (1996) for inversion transduction grammar and
bigram LMs. We present an adaptation of his algorithm to synchronous CFGs with two
nonterminals per right-hand side and general m-gram LMs. First, assume that the LM
expects a whole sentence to be preceded by (m − 1) start-of-sentence symbols (s) and
followed by a single end-of-sentence symbol (/s). The grammar can be made to do this
simply by adding a rule
</bodyText>
<equation confidence="0.847373">
S’ -4 (S 1 , (s)m−1 S 1 (/s)) (26)
</equation>
<bodyText confidence="0.997428">
and making S’ the new start symbol.
First, we define two functions p and q which operate on strings over T U {*}, where
T is the English terminal alphabet, and * is a special placeholder symbol that stands for
an elided part of an English string.
</bodyText>
<equation confidence="0.998336714285714">
p(a1 ··· at) = PLM(ai  |ai−m+1 ··· ai−1) (27)
m&lt;i&lt;Y
*/�{ai_m+1,...,ai}
�
a1 ··· am−1 *at−m+2 ··· at if f &gt; m
q(a1 · · · at) = (28)
a1 · · · at otherwise
</equation>
<page confidence="0.70628">
217
</page>
<table confidence="0.480162">
Computational Linguistics Volume 33, Number 2
</table>
<tableCaption confidence="0.994789">
Table 1
</tableCaption>
<bodyText confidence="0.242951">
Values of p and q in the “cgisf” example.
</bodyText>
<equation confidence="0.974185">
a1 ··· at p(a1 ··· at) q(a1 ··· at)
sf 1 sf
cg*gisf PLM(s  |gi) x PLM(f  |is) cg*sf
(s) (s) cg*sf (/s) PLM(c  |(s) (s)) x PLM(g  |(s) c) x PLM((/s)  |sf) (s) (s) *f (/s)
</equation>
<bodyText confidence="0.9555940625">
The function p calculates LM probabilities for all the complete m-grams in a string;
the function q elides symbols when all their m-grams have been accounted for. These
functions let us correctly calculate the LM score of a sentence piecemeal. For example,
let m = 3 and “c g i s f” stand for “colorless green ideas sleep furiously.” Then Table 1
shows some values of p and q.
Then we may extend the −LM parser as shown in Figure 8 to use p and q to calculate
LM probabilities. We call this parser the +LM parser. The items are of the form [X, i, j; e],
signifying that a subtree rooted in X has been recognized spanning from i to j on the
French side, and its English translation (possibly with parts elided) is e.
The theoretical running time of this algorithm is O(n3|T|4(m−1)), because a deduc-
tion can combine up to two starred strings, which each have up to 2(m − 1) terminal
symbols. This is far too slow to use in practice, so we must use beam-search to prune
the search space down to a reasonable size.
5.3.3 Pruning. The chart is organized into cells, each of which contains all the items
standing for X spanning fji+1. The rule items are also organized into cells, each of which
contains all the rules with the same French side and left-hand side. From here on, let us
</bodyText>
<equation confidence="0.617819">
(  |)
LM
cgi P
i cg cg*gi
</equation>
<figureCaption confidence="0.828267">
Figure 8
</figureCaption>
<bodyText confidence="0.9674215">
Inference rules for the +LM parser. Here w[x/X] means the string w with the string x substituted
for the symbol X. The function q is defined in the text.
</bodyText>
<page confidence="0.782292">
218
</page>
<note confidence="0.318014">
Chiang Hierarchical Phrase-Based Translation
</note>
<bodyText confidence="0.984594">
consider the item scores as costs, that is, negative log (base-10) probabilities. Then, for
each cell, we throw out any item that has a score worse than:
</bodyText>
<listItem confidence="0.9983415">
• β plus the best score in the same cell, or
• the score of the bth best item in the same cell.
</listItem>
<bodyText confidence="0.998954">
In the +LM parser, the score of an item [X, i, j; e] in the chart does not reflect the
LM probability of generating the first (m − 1) words of e. Thus two items [X, i, j; e]
and [X, i, j; e&apos;] are not directly comparable. To enable more meaningful comparisons, we
define a heuristic
</bodyText>
<equation confidence="0.9968422">
h([X,i,j;e]) = −log10 PLM(e1 · · ·ef) where f = min{m − 1, JeJ� (29)
Similarly for rules,
r
h(X -4 (γ, w0X1w1 ··· wr−1Xrwr)) = − log10 11 PLM(wi) (30)
i=0
</equation>
<bodyText confidence="0.997835034482759">
When comparing items for pruning (and only for pruning), we add this heuristic
function to the score of each item.
5.3.4 Cube Pruning. Now we can develop a compromise between the rescoring and
intersection methods. Consider Figure 9a. To the left of the grid we have four rules with
the same French side, and above we have three items with the same category and span,
that is, they belong to the same chart cell. Any of the twelve combinations of these rules
and items can be used to deduce a new item (whose scores are shown in the grid), and
all these new items will go into the same chart cell (partially listed on the right). The
intersection method would compute all twelve items and add them to the new chart
cell, where most of them will likely be pruned away. In actuality, the grid may be a
cube (one dimension for rules and two dimensions for two nonterminals) with up to
b3 elements, whereas the target chart cell can hold at most b items (where b is the limit
on the size of the cell imposed during pruning). Thus the vast majority of computed
items are pruned. But it is possible to compute only a small corner of the cube and
preemptively prune the rest of the items without computing them, a method we refer
to as cube pruning.
The situation pictured in Figure 9a is very similar to k-best list generation. The four
rules to the left of the grid can be thought of like a 4-best list for a single −LM rule item
(X --� cong X); the three items above the grid, like a 3-best list for the single −LM item
[X, 6,8]; and the new items to be deduced, like a k-best list for [X, 5, 8], except that we
don’t know what k is in advance. If we could use MERGEPRODUCTS to enumerate the
new items best-first, then we could enumerate them until one of them was pruned from
the new cell; then the rest of items, which would have a worse score than the pruned
item, could be preemptively pruned.
MERGEPRODUCTS expects its input lists to be sorted best-first, and the ® operator
to be monotonic in each of its arguments. For cube pruning, we sort items (both in
the inputs to MERGEPRODUCTS and in the priority queue inside MERGEPRODUCTS)
according to their +LM score, including the heuristic function h. The ® operator we
use takes one or more antecedent items and forms their consequent item according to
</bodyText>
<figure confidence="0.6813815">
219
Computational Linguistics Volume 33, Number 2
</figure>
<figureCaption confidence="0.85666">
Figure 9
</figureCaption>
<bodyText confidence="0.435723">
Example illustrating hybrid method for incorporating the LM. Numbers are negative
</bodyText>
<equation confidence="0.567193">
log-probabilities. (a) k-best list generation. (b) Cube pruning.
</equation>
<bodyText confidence="0.999597545454545">
the +LM parser. Note that the LM makes this ⊗ only approximately monotonic. This
means that the enumeration of new items will not necessarily be best-first. To alleviate
this problem, we stop the enumeration not as soon as an item falls outside the beam, but
as soon as an item falls outside the beam by a margin of e. This quantity e expresses our
guess as to how much the scores of the enumerated items can fluctuate because of the
LM. A simpler approach, and probably better in practice, would be simply to set e = 0,
that is, to ignore any fluctuation, but increase R and b to compensate.
See Figure 9b for an example of cube pruning. The upper-left grid cell is enumerated
first, as in the k-best example in Section 5.2, but the choice of the second is different, be-
cause of the added LM costs. Then, the third item is enumerated and merged with
the first (unlike in the k-best algorithm). Supposing a threshold beam of R = 5 and
</bodyText>
<page confidence="0.552058">
220
</page>
<note confidence="0.247092">
Chiang Hierarchical Phrase-Based Translation
</note>
<bodyText confidence="0.98163">
a margin of e = 0.5, we quit upon considering the next item, because, with a score of
7.7, it falls outside the beam by more than e. The rest of the grid is then discarded.
The pseudocode is given in Figure 10. The function INFER+LM is used as the ® op-
erator; it takes a tuple of antecedent +LM items and returns a consequent +LM item
according to the inference rules in Figure 8. The procedure REPARSE+LM takes
a −LM chart chart as input and produces a +LM chart chart&apos;. The variables u, v stand for
items in −LM and u&apos;, v&apos;, for items in +LM, and the relation v �i v&apos; is defined as follows:
</bodyText>
<equation confidence="0.9832615">
[X, i,j] �i [X, i, j; e] (31)
(X — 1&apos;) �] (X — (1&apos;, α)) (32)
</equation>
<bodyText confidence="0.993776">
For each cell in the input chart, it takes the single item from the cell and constructs the
virtual list L of all of its +LM counterparts (lines 9–15). Then, it adds the top items of L
to the target cell until the cell is judged to be full (lines 16–20).
</bodyText>
<sectionHeader confidence="0.997556" genericHeader="evaluation">
6. Experiments
</sectionHeader>
<bodyText confidence="0.999739428571429">
The implementation of our system, named Hiero, is in Python, a bytecode-interpreted
language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex,
a Python-like compiled language, with C++ code from the SRI Language Modeling
Toolkit (Stolcke 2002). In this section we report on experiments with Mandarin-to-
English translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al.
2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference
sentence length for the brevity penalty.
</bodyText>
<figure confidence="0.80605275">
Figure 10
Algorithm for faster integrated calculation of LM probabilities.
221
Computational Linguistics Volume 33, Number 2
</figure>
<subsectionHeader confidence="0.97558">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999128">
We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2
with the exception of the United Nations data, for a total of 28 million words (English
side).8 We then filtered this grammar for our development set, which was the 2002 NIST
MT evaluation dry-run data, and our test sets, which were the data from the 2003–2005
NIST MT evaluations. Some example rules are shown in Table 3, and the sizes of the
filtered grammars are shown in Table 4.
We also used the SRI Language Modeling Toolkit to train two trigram language
models with modified Kneser–Ney smoothing (Kneser and Ney 1995; Chen and
Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the
other on the English side of the parallel text (28 million words).
</bodyText>
<subsectionHeader confidence="0.99952">
6.2 Evaluating Translation Speed
</subsectionHeader>
<bodyText confidence="0.9999893">
Table 5 shows the average decoding time on part of the development set for the three
LM-incorporation methods described in Section 5.3, on a single processor of a dual
3 GHz Xeon machine. For these experiments, only the Gigaword language model was
used. We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules
except where noted in Table 5. Note that values for R and e are only meaningful relative
to the scale of the feature weights; here, the language model weight was 0.06. The
feature weights were obtained by minimum-error-rate training using the cube-pruning
(e = 0.1) decoder. For the LM rescoring decoder, parsing and k-best list generation used
feature weights optimized for the −LM model, but rescoring used the same weights as
the other experiments.
We tested the rescoring method (k = 103 and 104), the intersection method, and the
cube-pruning method (e = 0, 0.1, and 0.2). The LM rescoring decoder (k = 104) is the
fastest but has the poorest BLEU score. Identifying and rescoring the k-best derivations
is very quick; the execution time is dominated by reconstructing the output strings
for the k-best derivations, so it is possible that further optimization could reduce these
times. The intersecting decoder has the best score but runs very slowly. Finally, the cube-
pruning decoder runs almost as fast as the rescoring decoder and translates almost as
well as the intersecting decoder. Among these tests, e = 0.1 gives the best results, but in
general the optimal setting will depend on the other beam settings and the scale of the
feature weights.
</bodyText>
<subsectionHeader confidence="0.999244">
6.3 Evaluating Translation Accuracy
</subsectionHeader>
<bodyText confidence="0.999427428571429">
We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS
(Och et al. 2004; Thayer et al. 2004), and Hiero itself run as a conventional phrase-based
system with monotone translation (no phrase reordering).
The ATS baseline was trained on all the parallel data listed in Table 1, for a total
of 159 million words (English side). The second language model was also trained on
the English side of the whole bitext. Phrases of up to 10 in length on the French side
were extracted from the parallel text, and minimum-error-rate training (Och 2003) was
</bodyText>
<table confidence="0.8289764">
8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the
United Nations data. For example, extracting only rules with no variables up to length 5 yields a
grammar of 5.8M rules (filtered for the development set), which fits into memory easily.
222
Chiang Hierarchical Phrase-Based Translation
</table>
<tableCaption confidence="0.984531">
Table 2
</tableCaption>
<table confidence="0.984286230769231">
Corpora used in training data. Sizes are approximate and in millions of words.
Corpus LDC catalog Size
United Nations LDC2004E12 112
Hong Kong Hansards LDC2004T08 12
FBIS LDC2003E14 10
Xinhua LDC2002E18 4
Sinorama LDC2005E47 3
Named entity list LDC2003E01 1
Multiple Translation Chinese LDC2002T01, LDC2003T17, LDC2004T07 0.8
Chinese Treebank LDC2002E17, LDC2003E07 0.2
Translation lexicon LDC2002L27 0.1
Chinese News Translation LDC2005T06 0.1
English Gigaword (2nd ed.) LDC2005T12 2800
</table>
<bodyText confidence="0.998479105263158">
performed on the development set for 17 features, the same as used in the NIST 2004
and 2005 evaluations.9 These features are similar to the features used for our system,
but also include features for phrase-reordering (which are not applicable to our system),
IBM Model 1 in both directions, a missing word penalty, and a feature that controls a
fallback lexicon.
The other baseline, which we call Hiero Monotone, is the same as Hiero except with
the limitation that extracted rules cannot have any nonterminal symbols on their right-
hand sides. In other words, only conventional phrases can be extracted, of length up to
5. These phrases are combined using the glue rules only, which makes the grammar
equivalent to a conventional phrase-based model with monotone translation. Thus
this system represents the nearest phrase-based equivalent to our model, to provide
a controlled test of the effect of hierarchical phrases.
We performed minimum-error-rate training separately on Hiero and Hiero
Monotone to maximize their BLEU scores on the development set; the feature weights
for Hiero are shown in Table 6. The beam settings used for both decoders were
R = 30, b = 30 for X cells, R = 30, b = 15 for S cells, b = 100 for rules, and e = 3. On the
test set, we found that Hiero improves over both baselines in all three tests (see Table 7).
All improvements are statistically significant (p &lt; 0.01) using the sign test as described
by Collins, Koehn, and Kuˇcerov´a (2005).
</bodyText>
<sectionHeader confidence="0.685235" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999838222222222">
Syntax-based statistical machine translation is a twofold challenge. It is a modeling
challenge, in part because of the difficulty of coordinating syntactic structures with
potentially messy parallel corpora; it is an implementation challenge, because of the
added complexity introduced by hierarchical structures. Here we have addressed the
modeling challenge by taking only the fundamental idea from syntax, that language is
hierarchically structured, and integrating it conservatively into a phrase-based model
typical of the current state of the art. This fusion does no violence to the latter; indeed,
we have presented our approach as a logical outgrowth of the phrase-based approach.
Moreover, hierarchical structure improves translation accuracy significantly.
</bodyText>
<note confidence="0.3880635">
9 The definition of BLEU used in this training was the original IBM definition (Papineni et al. 2002), which
defines the effective reference length as the reference length that is closest to the test sentence length.
</note>
<page confidence="0.572484">
223
</page>
<figure confidence="0.8585135">
Computational Linguistics Volume 33, Number 2
Table 3
A selection of extracted rules, with ranks after filtering for the development set. All have X for
their left-hand sides.
Rank Chinese English
1 0 .
2 ,
3 de the
4 , ,
5 he and
6 X 1 . X 1 .
7 ,
8 zai in
9 , X 1 , X 1
10 X 1 X 1 ,
11 de of
12 .
13 &amp;quot;
14 &amp;quot;
15 and
</figure>
<table confidence="0.97503452173913">
63 zai X 1 inX1
394 X 1 de X 2 the X 2 ofX1
756 X 1 de X 2 the X 2 X 1
1061 X 1 deX2 the X 2 of the X 1
3752 X 1 hou after X 1
4399 jingtian X 1 X 1 this year
5232 X 1 yuan
$ X 1
5506 zhongguo X 1 X 1 of china
6030 X 1 duo more than X 1
7947 X 1 zhongguo X 2 X 2 X 1 china
8119 X 1 zai X 2 inX2 X 1
11052 zai X 1 zhong X 2 in X 1 X 2
11996 zai X 1 wenti shang on the X 1 issue
12068 zai X 1 de wenti shang on the basis of X 1
13237 zai X 1 hou after X 1
14125 zai X 1 zhiqian before X 1
14249 X 1 nian X 2 X 2 X 1 years
16871 zai X 1 de X 2 X 2 in X 1
28145 X 1 guanxi de X 2 the X 2 of X 1 relations
34294 zai X 1 de X 2 xia under the X 2 of X 1
224
Chiang Hierarchical Phrase-Based Translation
</table>
<tableCaption confidence="0.997701">
Table 4
</tableCaption>
<table confidence="0.946999571428571">
Test set sizes, with grammar sizes for two systems.
Test set Sentences Phrases/rules (thousands)
Hiero Monotone Hiero
development 993 448 3712
MT03 919 417 3389
MT04 1788 643 5556
MT05 1082 455 3646
</table>
<tableCaption confidence="0.994493">
Table 5
</tableCaption>
<table confidence="0.910373363636364">
Comparison of three methods for decoding with a language model. Time = mean per-sentence
user+system time, in seconds. BLEU = case-insensitive BLEU-4. All tests were on the first 400
sentences of the development set.
Method Settings Time BLEU
rescore k = 104 16 33.31
rescore k = 105 139 33.33
intersect∗ 1455 37.09
cube prune ε = 0 23 36.14
cube prune ε = 0.1 35 36.77
cube prune ε = 0.2 111 36.91
∗Rules were pruned using b = 30, β = 1.
</table>
<tableCaption confidence="0.995894">
Table 6
</tableCaption>
<bodyText confidence="0.741769">
Feature weights obtained by minimum-error-rate training.
</bodyText>
<subsectionHeader confidence="0.680377">
Feature Weight
</subsectionHeader>
<bodyText confidence="0.741239">
language model (large) 1.00
language model (bitext) 1.03
</bodyText>
<equation confidence="0.7554355">
P(γ  |α) 0.155
P(α  |γ) 1.23
P.(γ  |α) 1.61
P.(α  |γ) 0.494
</equation>
<table confidence="0.619985428571429">
numbers 0.364
dates 6.67
names 2.89
bylines −952
extracted rules 4.32
glue rule −0.281
word penalty −4.12
</table>
<bodyText confidence="0.999744285714286">
The choice to use hierarchical structures that are more complex than flat structures,
as well as rules that contain multiple lexical items instead of one, an m-gram model
whose structure cuts across the structure of context-free derivations, and large amounts
of training data for meaningful comparison with modern systems—these all threaten
to make training a synchronous grammar and translating with it intractable. We have
shown how, through training with simple methods inspired by phrase-based models,
and translating using a modified CKY with cube pruning, this challenge can be met.
</bodyText>
<page confidence="0.643337">
225
</page>
<note confidence="0.345563">
Computational Linguistics Volume 33, Number 2
</note>
<tableCaption confidence="0.880437">
Table 7
Results on baseline systems and hierarchical system. Also shown are the 95% confidence
intervals, obtained using bootstrap resampling.
</tableCaption>
<table confidence="0.997909">
System MT03 MT04 MT05
Hiero Monotone 28.27 f 1.03 28.83 f 0.74 26.35 f 0.92
ATS 30.84 f 0.99 31.74 f 0.73 30.50 f 0.95
Hiero 33.72 f 1.12 34.57 f 0.82 31.79 f 0.91
</table>
<bodyText confidence="0.998880714285714">
Clearly, however, we have only scratched the surface of the modeling challenge. The
fact that moving from flat structures to hierarchical structures significantly improves
translation quality suggests that more specific ideas from syntax may be valuable as
well. There are many possibilities for enriching the simple framework that the present
model provides. But the course taken here is one of organic development of an approach
known to work well at large-scale tasks, and we plan to stay this course in future work
towards more syntactically informed statistical machine translation.
</bodyText>
<sectionHeader confidence="0.994257" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.912877363636364">
I would like to thank Liang Huang, Philipp
Koehn, Adam Lopez, Nitin Madnani, Daniel
Marcu, Christof Monz, Dragos Munteanu,
Philip Resnik, Michael Subotin, Wei Wang,
and the anonymous reviewers. This work
was partially supported by ONR MURI
contract FCPO.810548265, by Department
of Defense contract RD-02-5700, and
under the GALE program of the Defense
Advanced Research Projects Agency,
contract HR 0011-06-C-0022. S. D. G.
</bodyText>
<sectionHeader confidence="0.982022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998895180327869">
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 2000. Learning
dependency translation models as
collections of finite-state head transducers.
Computational Linguistics, 26:45–60.
Bar-Hillel, Yehoshua, M. Perles, and
E. Shamir. 1964. On formal properties of
simple phrase structure grammars. In
Yehoshua Bar-Hillel, editor, Language
and Information: Selected Essays on their
Theory and Application. Addison-Wesley,
Reading, MA, pages 116–150.
Block, Hans Ulrich. 2000. Example-based
incremental synchronous interpretation.
In Wolfgang Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer-Verlag, Berlin, pages 411–417.
Bod, Rens. 1992. A computational model
of language performance: Data Oriented
Parsing. In Proceedings of the Fourteenth
International Conference on Computational
Linguistics (COLING), pages 855–859,
Nantes, France.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19:263–311.
Chen, Stanley F. and Joshua Goodman.
1998. An empirical study of smoothing
techniques for language modeling.
Technical Report TR-10-98, Harvard
University Center for Research in
Computing Technology.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical
machine translation. In Proceedings
of the 43rd Annual Meeting of the ACL,
pages 263–270, Ann Arbor, MI.
Chiang, David, Adam Lopez, Nitin Madnani,
Christof Monz, Philip Resnik, and Michael
Subotin. 2005. The Hiero machine
translation system: Extensions, evaluation,
and analysis. In Proceedings of HLT/EMNLP
2005, pages 779–786, Vancouver, Canada.
Collins, Michael, Philipp Koehn, and Ivona
Kuˇcerov´a. 2005. Clause restructuring
for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of
the ACL, pages 531–540, Ann Arbor, MI.
Cormen, Thomas H., Charles E. Leiserson,
Ronald L. Rivest, and Clifford Stein. 2001.
Introduction to Algorithms. MIT Press,
second edition, Cambridge, MA.
Ding, Yuan and Martha Palmer. 2005.
Machine translation using probabilistic
synchronous dependency insertion
grammars. In Proceedings of the 43rd
Annual Meeting of the ACL, pages 541–548,
Ann Arbor, MI.
</reference>
<page confidence="0.725079">
226
</page>
<note confidence="0.387916">
Chiang Hierarchical Phrase-Based Translation
</note>
<reference confidence="0.994647318181819">
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What’s in a
translation rule? In Proceedings of
HLT-NAACL 2004, pages 273–280,
Boston, MA.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25:573–605.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
Ninth International Workshop on Parsing
Technologies (IWPT), pages 53–64,
Vancouver, Canada.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for M-gram
language modeling. In Proceedings
of the IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 181–184, Detroit, MI.
Koehn, Philipp. 2003. Noun Phrase
Translation. Ph.D. thesis, University of
Southern California, Los Angeles, CA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL
2003, pages 127–133, Edmonton, Canada.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted
finite state transducer translation
template model for statistical machine
translation. Natural Language Engineering,
12:35–75.
Lewis, P. M., II and R. E. Stearns.1968.
Syntax-directed transduction. Journal
of the ACM, 15:465–488.
Magerman, David M. 1995. Statistical
decision-tree models for parsing. In
Proceedings of the 33rd Annual Meeting of the
ACL, pages 276–283, Cambridge, MA.
Marcu, Daniel and William Wong. 2002.
A phrase-based, joint probability model
for statistical machine translation. In
Proceedings of EMNLP 2002, pages 133–139,
Philadelphia, PA.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the ACL, pages 160–167, Sapporo, Japan.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In Proceedings of the 38th Annual Meeting
of the ACL, pages 440–447, Hong Kong.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295–302,
Philadelphia, PA.
Och, Franz Josef and Hermann Ney. 2004.
The alignment template approach to
statistical machine translation.
Computational Linguistics, 30:417–449.
Och, Franz Josef, Ignacio Thayer, Daniel
Marcu, Kevin Knight, Dragos Stefan
Munteanu, Quamrul Tipu, Michel Galley,
and Mark Hopkins. 2004. Arabic and
Chinese MT at USC/ISI. Presentation
given at NIST Machine Translation
Evaluation Workshop, Alexandria, VA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of
machine translation. In Proceedings of
the 40th Annual Meeting of the ACL,
pages 311–318, Philadelphia, PA.
Probst, Katharina, Lori Levin, Erik Peterson,
Alon Lavie, and Jaime Carbonell. 2002.
MT for minority languages using
elicitation-based learning of syntactic
transfer rules. Machine Translation,
17:245–270.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In Proceedings of the
43rd Annual Meeting of the ACL,
pages 271–279, Ann Arbor, MI.
Rigo, Armin. 2004. Representation-based
just-in-time specialization and the Psyco
prototype for Python. In Nevin Heintze
and Peter Sestoft, editors, Proceedings of the
2004 ACM SIGPLAN Workshop on Partial
Evaluation and Semantics-based Program
Manipulation, pages 15–26, Verona, Italy.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive
parsing. Journal of Logic Programming,
24:3–36.
Simard, Michel, Nicola Cancedda, Bruno
Cavestro, Marc Dymetman, Eric Gaussier,
Cyril Goutte, Kenji Yamada, Philippe
Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases.
In Proceedings of HLT/EMNLP 2005,
pages 755–762, Ann Arbor, MI.
Stolcke, Andreas. 2002. SRILM – an
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, volume 2,
pages 901–904, Denver, CO.
Thayer, Ignacio, Emil Ettelaie, Kevin Knight,
Daniel Marcu, Dragos Stefan Munteanu,
Franz Joseph Och, and Quamrul Tipu.
2004. The ISI/USC MT system. In
Proceedings of IWSLT 2004, pages 59–60,
Kyoto, Japan.
Tillmann, Christoph. 2004. A unigram
orientation model for statistical machine
227
Computational Linguistics Volume 33, Number 2
translation. In Proceedings of HLT-NAACL
2004, pages 101–104. Companion volume,
Boston, MA.
Tillmann, Christoph and Tong Zhang.
2005. A localized preduction model
for statistical machine translation. In
Proceedings of the 43rd Annual Meeting
of the ACL, pages 557–564, Ann Arbor, MI.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Meeting of the ACL, pages 152–158,
Santa Cruz, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23:377–404.
Xia, Fei and Michael McCord. 2004.
Improving a statistical MT system
with automatically learned rewrite
patterns. In Proceedings of the Twentienth
International Conference on Computational
Linguistics (COLING), pages 508–514,
Geneva, Switzerland.
Yamada, Kenji and Kevin Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th Annual
Meeting of the ACL, pages 523–530,
Toulouse, France.
Zens, Richard and Hermann Ney. 2004.
Improvements in phrase-based statistical
machine translation. In Proceedings
of HLT-NAACL 2004, pages 257–264,
Boston, MA.
</reference>
<page confidence="0.929804">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957551">
<title confidence="0.997365">Hierarchical Phrase-Based Translation</title>
<affiliation confidence="0.990369">Information Sciences Institute University of Southern California</affiliation>
<abstract confidence="0.9973015">present a statistical machine translation model that uses that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite-state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--45</pages>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26:45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>In Yehoshua Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application.</booktitle>
<pages>116--150</pages>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA,</location>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Bar-Hillel, Yehoshua, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Yehoshua Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application. Addison-Wesley, Reading, MA, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Ulrich Block</author>
</authors>
<title>Example-based incremental synchronous interpretation.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation.</booktitle>
<pages>411--417</pages>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<contexts>
<context position="9909" citStr="Block (2000)" startWordPosition="1540" endWordPosition="1541">or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and t</context>
</contexts>
<marker>Block, 2000</marker>
<rawString>Block, Hans Ulrich. 2000. Example-based incremental synchronous interpretation. In Wolfgang Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation. Springer-Verlag, Berlin, pages 411–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>A computational model of language performance: Data Oriented Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING),</booktitle>
<pages>855--859</pages>
<location>Nantes, France.</location>
<contexts>
<context position="23006" citStr="Bod 1992" startWordPosition="3841" endWordPosition="3842">his gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder. 4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average. 5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesize an augmented version of the training data, but it is especially reminiscent of the Data Oriented Parsing method (Bod 1992), which hypothesizes a distribution over many possible derivations of each training example from subtrees of varying sizes. 211 Computational Linguistics Volume 33, Number 2 5. Decoding In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search together with a postprocessor for mapping French derivations to English derivations. Given a French sentence f, it finds the English yield of the single best derivation that has French yield f: �eˆ = e arg max P(D) (24) Ds.t.f(D)=f Note that this is not necessarily the highest-probability English string, which would require a more exp</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>Bod, Rens. 1992. A computational model of language performance: Data Oriented Parsing. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING), pages 855–859, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1433" citStr="Brown et al. 1993" startWordPosition="200" endWordPosition="203">mplate translation model (Och and Ney 2004) and related phrase-based models advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory). These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. This makes them a simple and powerful mechanism for translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al. 1993). Following convention, we call the source language “French” and the target language “English”; the translation of a French sentence f into an English sentence e is modeled as: arg max P(e |f ) = arg max P(e,f ) (1) e e = arg max (P(e) x P( f |e)) (2) e The phrase-based translation model P( f |e) “encodes” e into f by the following steps: 1. segment e into phrases ¯e1· · · ¯eI, typically with a uniform distribution over segmentations; * 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292, USA. E-mail: chiang@isi.edu. Much of the research presented here was carried out while the author was</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="43388" citStr="Chen and Goodman 1998" startWordPosition="7456" endWordPosition="7459">grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side).8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the 2003–2005 NIST MT evaluations. Some example rules are shown in Table 3, and the sizes of the filtered grammars are shown in Table 4. We also used the SRI Language Modeling Toolkit to train two trigram language models with modified Kneser–Ney smoothing (Kneser and Ney 1995; Chen and Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words). 6.2 Evaluating Translation Speed Table 5 shows the average decoding time on part of the development set for the three LM-incorporation methods described in Section 5.3, on a single processor of a dual 3 GHz Xeon machine. For these experiments, only the Gigaword language model was used. We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules except where noted in Table 5. Note that values for R and e are only meaningful relative to th</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Chen, Stanley F. and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5360" citStr="Chiang 2005" startWordPosition="848" endWordPosition="849">phrase translation and phrase reordering), but does not invert these two groups as it should. We propose a solution to these problems that does not interfere with the strengths of the phrase-based approach, but rather capitalizes on them: Because phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that can contain other phrases. For example, a hierarchical phrase pair that might help with the above example is (yu 1 you 2 , have 2 with 1 ) (3) where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture the fact that Chinese prepositional phrases almost always modify verb phrases on the 202 Chiang Hierarchical Phrase-Based Translation left, whereas English prepositional phrases usually modify verb phrases on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, the hierarchical phrase pair ( 1 de 2 , the 2 that 1 ) (4) would capture the fact that Chinese relative clauses modify NP</context>
<context position="28499" citStr="Chiang (2005)" startWordPosition="4801" endWordPosition="4802">ial rules during training. This gives the decoding algorithm an asymptotic time complexity of O(n). In principle Λ should match the initial phrase length limit used in training (as it does in our experiments), but in practice it can be adjusted separately to maximize accuracy or speed. 5.2 Generating k-best Lists We often want to find not only the best derivation for a French sentence but a list of the k-best derivations. These are used for minimum-error-rate training and for rescoring with a language model (Section 5.3.1). We describe here how to do this using the lazy algorithm of Huang and Chiang (2005). Part of this method will also be reused in our algorithm for fast parsing with a language model (Section 5.3.4). If we conceive of lists as functions from indices to values, we may create a virtual list, a function that computes member values on demand instead of storing all the values statically. The heart of the k-best algorithm is a function MERGEPRODUCTS, which takes a set G of tuples of (virtual) lists with an operator ⊗ and returns a virtual list: MERGEPRODUCTS(G, ⊗) = U ~~xi xi ∈ Li (25) (L1,...Ln)∈G i 214 Chiang Hierarchical Phrase-Based Translation Figure 5 Example illustrating MERG</context>
<context position="31607" citStr="Chiang 2005" startWordPosition="5336" endWordPosition="5337"> element is 1 and is zero elsewhere), and then enumerate the best element in the priority queue, if any (lines 14–18). We assume standard implementations of 7 This version corrects the behavior of the previously published version in some boundary conditions. Thanks to D. Smith and J. May for pointing those cases out. In the actual implementation, an earlier version is used which has the correct behavior but not for cyclic forests (which the parser never produces). 215 Computational Linguistics Volume 33, Number 2 Figure 6 Function for computing the union of products of sorted lists (Huang and Chiang 2005). the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al. 2001). The k-best list generator is then easy to define (Figure 7). First, we generate a parse forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k-best list for each item in the forest. The pseudocode in Figure 7 will find only the weights for the k-best derivations; extending it to output the translations as well is a matter of modifying line 5 to package the English sides of rules together with the weights w, and replacing the real m</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Adam Lopez</author>
<author>Nitin Madnani</author>
<author>Christof Monz</author>
<author>Philip Resnik</author>
<author>Michael Subotin</author>
</authors>
<title>The Hiero machine translation system: Extensions, evaluation, and analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>779--786</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="18443" citStr="Chiang et al. 2005" startWordPosition="3064" endWordPosition="3067">rs) (17) allowing it to generalize over numbers of years. 4. Model Given a French sentence f, a synchronous CFG will have, in general, many derivations that yield f on the French side, and therefore (in general) many possible translations e. We now define a model over derivations D to predict which translations are more likely than others. 4.1 Definition Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model over derivations D: P(D) a 11 φi(D)λi (18) i 3 These modules are due to U. Germann and F. J. Och. In a previous paper (Chiang et al. 2005) we reported on translation modules for numbers and names. The present modules are not the same as those, though the mechanism for integrating them is identical. 209 Computational Linguistics Volume 33, Number 2 where the φi are features defined on derivations and the λi are feature weights. One of the features is an m-gram language model PLM(e); the remainder of the features we will define as products of functions on the rules used in a derivation: φi(D) = rl φi(X - (γ, α)) (19) (X-.(y,α))ED Thus we can rewrite P(D) as P(D) a PLM(e)7LM x rl rl φi(X - (γ, α))7i (20) i#LM (X-.(y,α))ED The facto</context>
</contexts>
<marker>Chiang, Lopez, Madnani, Monz, Resnik, Subotin, 2005</marker>
<rawString>Chiang, David, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, and Michael Subotin. 2005. The Hiero machine translation system: Extensions, evaluation, and analysis. In Proceedings of HLT/EMNLP 2005, pages 779–786, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, MI.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Collins, Michael, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the ACL, pages 531–540, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<note>second edition,</note>
<contexts>
<context position="31693" citStr="Cormen et al. 2001" startWordPosition="5346" endWordPosition="5349">e priority queue, if any (lines 14–18). We assume standard implementations of 7 This version corrects the behavior of the previously published version in some boundary conditions. Thanks to D. Smith and J. May for pointing those cases out. In the actual implementation, an earlier version is used which has the correct behavior but not for cyclic forests (which the parser never produces). 215 Computational Linguistics Volume 33, Number 2 Figure 6 Function for computing the union of products of sorted lists (Huang and Chiang 2005). the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al. 2001). The k-best list generator is then easy to define (Figure 7). First, we generate a parse forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k-best list for each item in the forest. The pseudocode in Figure 7 will find only the weights for the k-best derivations; extending it to output the translations as well is a matter of modifying line 5 to package the English sides of rules together with the weights w, and replacing the real multiplication operator × in line 9 with one that not only multiplies weights but also </context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. MIT Press, second edition, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>541--548</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="9338" citStr="Ding and Palmer 2005" startWordPosition="1457" endWordPosition="1460">ctic theories, or annotations made according to syntactic theories. At one extreme are those, exemplified by that of Wu (1997), that have no dependence on syntactic theory beyond the idea that natural language is hierarchical. If these methods distinguish between different categories, they typically do not distinguish very many. Our approach, as presented here, falls squarely into this family. By contrast, other approaches, exemplified by that of Yamada and Knight (2001), do make use of parallel data with syntactic annotations, either in the form of phrase-structure trees or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Ding, Yuan and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<pages>273--280</pages>
<location>Boston, MA.</location>
<contexts>
<context position="10228" citStr="Galley et al. (2004)" startWordPosition="1593" endWordPosition="1596">ch have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. 3. Grammar The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968). We give here an informal definition and then describe in detail how we build a synchronous CFG for our model. 3.1 Synchronous CFG In a syn</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL 2004, pages 273–280, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<date>1999</date>
<booktitle>Semiring parsing. Computational Linguistics,</booktitle>
<pages>25--573</pages>
<contexts>
<context position="23977" citStr="Goodman 1999" startWordPosition="3985" endWordPosition="3986">rench sentence f, it finds the English yield of the single best derivation that has French yield f: �eˆ = e arg max P(D) (24) Ds.t.f(D)=f Note that this is not necessarily the highest-probability English string, which would require a more expensive summation over derivations. We now discuss the details of the decoder, focusing attention on efficiently calculating English language-model probabilities for possible translations, which is the primary technical challenge. 5.1 Basic Algorithm In the following we present several parsers as deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999). A parser in this notation defines a space of weighted items, in which some items are designated axioms and some items are designated goals (the items to be proven), and a set of inference rules of the form I1 : w1 ··· Ik : wk φ I : w which means that if all the items Ii (called the antecedents) are provable, with weight wi, then I (called the consequent) is provable, with weight w, provided the side condition φ holds. The parsing process grows a set of provable items: It starts with the axioms, and proceeds by applying inference rules to prove more and more items until a goal is proven. For </context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25:573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>53--64</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="28499" citStr="Huang and Chiang (2005)" startWordPosition="4799" endWordPosition="4802">nt on initial rules during training. This gives the decoding algorithm an asymptotic time complexity of O(n). In principle Λ should match the initial phrase length limit used in training (as it does in our experiments), but in practice it can be adjusted separately to maximize accuracy or speed. 5.2 Generating k-best Lists We often want to find not only the best derivation for a French sentence but a list of the k-best derivations. These are used for minimum-error-rate training and for rescoring with a language model (Section 5.3.1). We describe here how to do this using the lazy algorithm of Huang and Chiang (2005). Part of this method will also be reused in our algorithm for fast parsing with a language model (Section 5.3.4). If we conceive of lists as functions from indices to values, we may create a virtual list, a function that computes member values on demand instead of storing all the values statically. The heart of the k-best algorithm is a function MERGEPRODUCTS, which takes a set G of tuples of (virtual) lists with an operator ⊗ and returns a virtual list: MERGEPRODUCTS(G, ⊗) = U ~~xi xi ∈ Li (25) (L1,...Ln)∈G i 214 Chiang Hierarchical Phrase-Based Translation Figure 5 Example illustrating MERG</context>
<context position="31607" citStr="Huang and Chiang 2005" startWordPosition="5334" endWordPosition="5337"> whose ith element is 1 and is zero elsewhere), and then enumerate the best element in the priority queue, if any (lines 14–18). We assume standard implementations of 7 This version corrects the behavior of the previously published version in some boundary conditions. Thanks to D. Smith and J. May for pointing those cases out. In the actual implementation, an earlier version is used which has the correct behavior but not for cyclic forests (which the parser never produces). 215 Computational Linguistics Volume 33, Number 2 Figure 6 Function for computing the union of products of sorted lists (Huang and Chiang 2005). the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al. 2001). The k-best list generator is then easy to define (Figure 7). First, we generate a parse forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k-best list for each item in the forest. The pseudocode in Figure 7 will find only the weights for the k-best derivations; extending it to output the translations as well is a matter of modifying line 5 to package the English sides of rules together with the weights w, and replacing the real m</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Huang, Liang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 53–64, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="43364" citStr="Kneser and Ney 1995" startWordPosition="7452" endWordPosition="7455">tal Setup We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side).8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the 2003–2005 NIST MT evaluations. Some example rules are shown in Table 3, and the sizes of the filtered grammars are shown in Table 4. We also used the SRI Language Modeling Toolkit to train two trigram language models with modified Kneser–Ney smoothing (Kneser and Ney 1995; Chen and Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words). 6.2 Evaluating Translation Speed Table 5 shows the average decoding time on part of the development set for the three LM-incorporation methods described in Section 5.3, on a single processor of a dual 3 GHz Xeon machine. For these experiments, only the Gigaword language model was used. We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules except where noted in Table 5. Note that values for R and e are only m</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Kneser, Reinhard and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 181–184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Noun Phrase Translation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles, CA.</location>
<contexts>
<context position="20420" citStr="Koehn 2003" startWordPosition="3406" endWordPosition="3407">, we use a feature set analogous to the default feature set of Pharaoh (Koehn, Och, and Marcu 2003). The rules extracted from the training bitext have the following features: • P(γ |α) and P(α |γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney 2002); 210 Chiang Hierarchical Phrase-Based Translation • the lexical weights Pw(γ |α) and Pw(α |γ), which estimate how well the words in α translate the words in γ (Koehn, Och, and Marcu 2003);4 • a penalty exp(−1) for extracted rules, analogous to Koehn’s phrase penalty (Koehn 2003), which allows the model to learn a preference for longer or shorter derivations. Next, there are penalties exp(−1) for various other classes of rules: • for the glue rule (14), so that the model can learn a preference for hierarchical phrases over a serial combination of phrases, • for each of the four types of rules (numbers, dates, names, bylines) inserted by the specialized translation modules, so that the model can learn how much to rely on each of them. Finally, for all the rules, there is a word penalty exp(−#T(α)), where #T just counts terminal symbols. This allows the model to learn a</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Koehn, Philipp. 2003. Noun Phrase Translation. Ph.D. thesis, University of Southern California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<pages>12--35</pages>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>Kumar, Shankar, Yonggang Deng, and William Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering, 12:35–75.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P M Lewis</author>
<author>R E Stearns 1968</author>
</authors>
<title>Syntax-directed transduction.</title>
<journal>Journal of the ACM,</journal>
<pages>15--465</pages>
<marker>Lewis, 1968, </marker>
<rawString>Lewis, P. M., II and R. E. Stearns.1968. Syntax-directed transduction. Journal of the ACM, 15:465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>276--283</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="22827" citStr="Magerman 1995" startWordPosition="3813" endWordPosition="3814">inear model (18) are learned by minimumerror-rate training (Och 2003), which tries to set the parameters so as to maximize the BLEU score (Papineni et al. 2002) of a development set. This gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder. 4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average. 5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesize an augmented version of the training data, but it is especially reminiscent of the Data Oriented Parsing method (Bod 1992), which hypothesizes a distribution over many possible derivations of each training example from subtrees of varying sizes. 211 Computational Linguistics Volume 33, Number 2 5. Decoding In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search together with a postprocessor for mapping French derivations to English derivations. Given a French sentence f, it finds the English yield of the single best</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, David M. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the ACL, pages 276–283, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2002,</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2527" citStr="Marcu and Wong 2002" startWordPosition="383" endWordPosition="386">arina del Rey, CA 90292, USA. E-mail: chiang@isi.edu. Much of the research presented here was carried out while the author was at the University of Maryland Institute for Advanced Computer Studies. Submission received:1 May 2006; accepted for publication: 3 October 2006. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 2. reorder the ¯ei according to some distortion model; 3. translate each of the ¯ei into French phrases according to a model P(f¯ |¯e) estimated from the training data. Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, som</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, Daniel and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of EMNLP 2002, pages 133–139, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7578" citStr="Och 2003" startWordPosition="1191" endWordPosition="1192">ystems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST. Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT’s capacity for memorizing translations from parallel data. Other insights borrowed from the current state of the art include minimum-error-rate training of log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model. The conjunction of these various elements presents a considerable challenge for implementation, which we discuss in detail in this article. The result is the first system employing a grammar (to our knowledge) to perform better than phrase-based systems in large-scale evaluations.2 1 The actual derivation used varies in practice. A previous version of the model selected precisely the derivation shown in the text, although the version described in this article happens to select a less intuitive one: [Aozhou shi] [[[yu]1 Beihan [you [bangjiao]2 de [shaoshu]3</context>
<context position="22282" citStr="Och 2003" startWordPosition="3717" endWordPosition="3718">les as though we observed them in the training data, a distribution that does not necessarily maximize the likelihood of the training data.5 Och’s method gives a count of one to each extracted phrase pair occurrence. We likewise give a count of one to each initial phrase pair occurrence, then distribute its weight equally among the rules obtained by subtracting subphrases from it. Treating this distribution as our observed data, we use relative-frequency estimation to obtain P(γ |α) and P(α |γ). Finally, the parameters λi of the log-linear model (18) are learned by minimumerror-rate training (Och 2003), which tries to set the parameters so as to maximize the BLEU score (Papineni et al. 2002) of a development set. This gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder. 4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average. 5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesiz</context>
<context position="45757" citStr="Och 2003" startWordPosition="7863" endWordPosition="7864">s. 6.3 Evaluating Translation Accuracy We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al. 2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering). The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side). The second language model was also trained on the English side of the whole bitext. Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data. For example, extracting only rules with no variables up to length 5 yields a grammar of 5.8M rules (filtered for the development set), which fits into memory easily. 222 Chiang Hierarchical Phrase-Based Translation Table 2 Corpora used in training data. Sizes are approximate and in millions of words. Corpus LDC catalog Size United Nations LDC2004E12 112 Hong Kong Hansards LDC2004T08 12 FBIS LDC2003E14 10 Xinhua LDC2002E18 4 Sinorama LDC2005E47 3 Named entity lis</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<location>Hong Kong.</location>
<contexts>
<context position="12524" citStr="Och and Ney 2000" startWordPosition="1998" endWordPosition="2001">itten using the two components of a single rule. When denoting links with boxed indices, we must consistently reindex the newly introduced symbols apart from the symbols already present. For an example using these rules, see Figure 1. 3.2 Rule Extraction The bulk of the grammar consists of automatically extracted rules. The extraction process begins with a word-aligned corpus: a set of triples (f, e, —), where f is a French sentence, e is an English sentence, and — is a (many-to-many) binary relation between positions off and positions of e. The word alignments are obtained by running GIZA++ (Och and Ney 2000) on the corpus in both directions, and forming the union of the two sets of word alignments. We then extract from each word-aligned sentence pair a set of rules that are consistent with the word alignments. This can be thought of in two steps. First, we identify initial phrase pairs using the same criterion as most phrase-based systems (Och and Ney 2004), namely, there must be at least one word inside one phrase aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase. For example, suppose our training data contained the fragment 30 ��</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the ACL, pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2608" citStr="Och and Ney 2002" startWordPosition="400" endWordPosition="403"> here was carried out while the author was at the University of Maryland Institute for Advanced Computer Studies. Submission received:1 May 2006; accepted for publication: 3 October 2006. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 2. reorder the ¯ei according to some distortion model; 3. translate each of the ¯ei into French phrases according to a model P(f¯ |¯e) estimated from the training data. Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), </context>
<context position="7567" citStr="Och and Ney 2002" startWordPosition="1187" endWordPosition="1190">viously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST. Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT’s capacity for memorizing translations from parallel data. Other insights borrowed from the current state of the art include minimum-error-rate training of log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model. The conjunction of these various elements presents a considerable challenge for implementation, which we discuss in detail in this article. The result is the first system employing a grammar (to our knowledge) to perform better than phrase-based systems in large-scale evaluations.2 1 The actual derivation used varies in practice. A previous version of the model selected precisely the derivation shown in the text, although the version described in this article happens to select a less intuitive one: [Aozhou shi] [[[yu]1 Beihan [you [bangjiao]2 de</context>
<context position="18209" citStr="Och and Ney (2002)" startWordPosition="3021" endWordPosition="3024">ations into the grammar as new rules.3 Such modules are often used by phrase-based systems as well, but here their translations can plug into hierarchical phrases, for example, into the rule X → �X 1 duonianlai, over the last X 1 years) (17) allowing it to generalize over numbers of years. 4. Model Given a French sentence f, a synchronous CFG will have, in general, many derivations that yield f on the French side, and therefore (in general) many possible translations e. We now define a model over derivations D to predict which translations are more likely than others. 4.1 Definition Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model over derivations D: P(D) a 11 φi(D)λi (18) i 3 These modules are due to U. Germann and F. J. Och. In a previous paper (Chiang et al. 2005) we reported on translation modules for numbers and names. The present modules are not the same as those, though the mechanism for integrating them is identical. 209 Computational Linguistics Volume 33, Number 2 where the φi are features defined on derivations and the λi are feature weights. One of the features is an m-gram language model PLM(e); the remainder of </context>
<context position="20140" citStr="Och and Ney 2002" startWordPosition="3358" endWordPosition="3361">highest-weight translation or k-best translations with a weighted synchronous CFG. Therefore it is problematic that w(D) does not include the language model, which is extremely important for translation quality. We return to this challenge in Section 5. 4.2 Features For our experiments, we use a feature set analogous to the default feature set of Pharaoh (Koehn, Och, and Marcu 2003). The rules extracted from the training bitext have the following features: • P(γ |α) and P(α |γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney 2002); 210 Chiang Hierarchical Phrase-Based Translation • the lexical weights Pw(γ |α) and Pw(α |γ), which estimate how well the words in α translate the words in γ (Koehn, Och, and Marcu 2003);4 • a penalty exp(−1) for extracted rules, analogous to Koehn’s phrase penalty (Koehn 2003), which allows the model to learn a preference for longer or shorter derivations. Next, there are penalties exp(−1) for various other classes of rules: • for the glue rule (14), so that the model can learn a preference for hierarchical phrases over a serial combination of phrases, • for each of the four types of rules </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 295–302, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="858" citStr="Och and Ney 2004" startWordPosition="113" endWordPosition="116"> is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system. 1. Introduction The alignment template translation model (Och and Ney 2004) and related phrase-based models advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory). These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. This makes them a simple and powerful mechanism for translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al. 1993). Following convention, w</context>
<context position="3343" citStr="Och and Ney 2004" startWordPosition="516" endWordPosition="519"> the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have d</context>
<context position="12880" citStr="Och and Ney 2004" startWordPosition="2060" endWordPosition="2063"> a word-aligned corpus: a set of triples (f, e, —), where f is a French sentence, e is an English sentence, and — is a (many-to-many) binary relation between positions off and positions of e. The word alignments are obtained by running GIZA++ (Och and Ney 2000) on the corpus in both directions, and forming the union of the two sets of word alignments. We then extract from each word-aligned sentence pair a set of rules that are consistent with the word alignments. This can be thought of in two steps. First, we identify initial phrase pairs using the same criterion as most phrase-based systems (Och and Ney 2004), namely, there must be at least one word inside one phrase aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase. For example, suppose our training data contained the fragment 30 ��F-* 30 duonianlai 30 plus-years-past �:f youhao friendly [7 hezou cooperation n de of friendly cooperation over the last 30 years 205 Computational Linguistics Volume 33, Number 2 Figure 1 Example derivation of a synchronous CFG. Numbers above arrows are rules used at each step. with word alignments as shown in Figure 2a. The initial phrases that would b</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
<author>Dragos Stefan Munteanu</author>
<author>Quamrul Tipu</author>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
</authors>
<date>2004</date>
<booktitle>Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop,</booktitle>
<location>Alexandria, VA.</location>
<contexts>
<context position="3414" citStr="Och et al. 2004" startWordPosition="528" endWordPosition="531"> localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have diplomatic relations with North Korea. If we count zhiyi (literally, ‘of</context>
<context position="45289" citStr="Och et al. 2004" startWordPosition="7782" endWordPosition="7785">tput strings for the k-best derivations, so it is possible that further optimization could reduce these times. The intersecting decoder has the best score but runs very slowly. Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder. Among these tests, e = 0.1 gives the best results, but in general the optimal setting will depend on the other beam settings and the scale of the feature weights. 6.3 Evaluating Translation Accuracy We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al. 2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering). The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side). The second language model was also trained on the English side of the whole bitext. Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data</context>
</contexts>
<marker>Och, Thayer, Marcu, Knight, Munteanu, Tipu, Galley, Hopkins, 2004</marker>
<rawString>Och, Franz Josef, Ignacio Thayer, Daniel Marcu, Kevin Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel Galley, and Mark Hopkins. 2004. Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop, Alexandria, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="22373" citStr="Papineni et al. 2002" startWordPosition="3732" endWordPosition="3735">ot necessarily maximize the likelihood of the training data.5 Och’s method gives a count of one to each extracted phrase pair occurrence. We likewise give a count of one to each initial phrase pair occurrence, then distribute its weight equally among the rules obtained by subtracting subphrases from it. Treating this distribution as our observed data, we use relative-frequency estimation to obtain P(γ |α) and P(α |γ). Finally, the parameters λi of the log-linear model (18) are learned by minimumerror-rate training (Och 2003), which tries to set the parameters so as to maximize the BLEU score (Papineni et al. 2002) of a development set. This gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder. 4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average. 5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesize an augmented version of the training data, but it is especially reminiscent of the Data O</context>
<context position="42482" citStr="Papineni et al. 2002" startWordPosition="7305" endWordPosition="7308">ell and constructs the virtual list L of all of its +LM counterparts (lines 9–15). Then, it adds the top items of L to the target cell until the cell is judged to be full (lines 16–20). 6. Experiments The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit (Stolcke 2002). In this section we report on experiments with Mandarin-toEnglish translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty. Figure 10 Algorithm for faster integrated calculation of LM probabilities. 221 Computational Linguistics Volume 33, Number 2 6.1 Experimental Setup We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side).8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data</context>
<context position="48961" citStr="Papineni et al. 2002" startWordPosition="8366" endWordPosition="8369">se of the added complexity introduced by hierarchical structures. Here we have addressed the modeling challenge by taking only the fundamental idea from syntax, that language is hierarchically structured, and integrating it conservatively into a phrase-based model typical of the current state of the art. This fusion does no violence to the latter; indeed, we have presented our approach as a logical outgrowth of the phrase-based approach. Moreover, hierarchical structure improves translation accuracy significantly. 9 The definition of BLEU used in this training was the original IBM definition (Papineni et al. 2002), which defines the effective reference length as the reference length that is closest to the test sentence length. 223 Computational Linguistics Volume 33, Number 2 Table 3 A selection of extracted rules, with ranks after filtering for the development set. All have X for their left-hand sides. Rank Chinese English 1 0 . 2 , 3 de the 4 , , 5 he and 6 X 1 . X 1 . 7 , 8 zai in 9 , X 1 , X 1 10 X 1 X 1 , 11 de of 12 . 13 &amp;quot; 14 &amp;quot; 15 and 63 zai X 1 inX1 394 X 1 de X 2 the X 2 ofX1 756 X 1 de X 2 the X 2 X 1 1061 X 1 deX2 the X 2 of the X 1 3752 X 1 hou after X 1 4399 jingtian X 1 X 1 this year 5232 </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Probst</author>
<author>Lori Levin</author>
<author>Erik Peterson</author>
<author>Alon Lavie</author>
<author>Jaime Carbonell</author>
</authors>
<title>MT for minority languages using elicitation-based learning of syntactic transfer rules.</title>
<date>2002</date>
<booktitle>Machine Translation,</booktitle>
<pages>17--245</pages>
<contexts>
<context position="10062" citStr="Probst et al. (2002)" startWordPosition="1567" endWordPosition="1570">aining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. 3. Grammar The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction gramma</context>
</contexts>
<marker>Probst, Levin, Peterson, Lavie, Carbonell, 2002</marker>
<rawString>Probst, Katharina, Lori Levin, Erik Peterson, Alon Lavie, and Jaime Carbonell. 2002. MT for minority languages using elicitation-based learning of syntactic transfer rules. Machine Translation, 17:245–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, MI.</location>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the ACL, pages 271–279, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armin Rigo</author>
</authors>
<title>Representation-based just-in-time specialization and the Psyco prototype for Python.</title>
<date>2004</date>
<booktitle>Proceedings of the 2004 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-based Program Manipulation,</booktitle>
<pages>15--26</pages>
<editor>In Nevin Heintze and Peter Sestoft, editors,</editor>
<location>Verona, Italy.</location>
<contexts>
<context position="42218" citStr="Rigo 2004" startWordPosition="7269" endWordPosition="7270">&apos;. The variables u, v stand for items in −LM and u&apos;, v&apos;, for items in +LM, and the relation v �i v&apos; is defined as follows: [X, i,j] �i [X, i, j; e] (31) (X — 1&apos;) �] (X — (1&apos;, α)) (32) For each cell in the input chart, it takes the single item from the cell and constructs the virtual list L of all of its +LM counterparts (lines 9–15). Then, it adds the top items of L to the target cell until the cell is judged to be full (lines 16–20). 6. Experiments The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit (Stolcke 2002). In this section we report on experiments with Mandarin-toEnglish translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty. Figure 10 Algorithm for faster integrated calculation of LM probabilities. 221 Computational Linguistics Volume 33, Number 2 6.1 Experimental Setup We ran the grammar extractor of Section 3.2 on the parallel cor</context>
</contexts>
<marker>Rigo, 2004</marker>
<rawString>Rigo, Armin. 2004. Representation-based just-in-time specialization and the Psyco prototype for Python. In Nevin Heintze and Peter Sestoft, editors, Proceedings of the 2004 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-based Program Manipulation, pages 15–26, Verona, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
</authors>
<title>Eric Gaussier, Cyril Goutte,</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>755--762</pages>
<location>Kenji Yamada, Philippe</location>
<contexts>
<context position="10370" citStr="Simard et al. (2005)" startWordPosition="1616" endWordPosition="1619">ard generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. 3. Grammar The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968). We give here an informal definition and then describe in detail how we build a synchronous CFG for our model. 3.1 Synchronous CFG In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: X — (γ, α, —) where X is a nonterminal, γ and</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, 2005</marker>
<rawString>Simard, Michel, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada, Philippe Langlais, and Arne Mauser. 2005. Translating with non-contiguous phrases. In Proceedings of HLT/EMNLP 2005, pages 755–762, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="42331" citStr="Stolcke 2002" startWordPosition="7286" endWordPosition="7287"> as follows: [X, i,j] �i [X, i, j; e] (31) (X — 1&apos;) �] (X — (1&apos;, α)) (32) For each cell in the input chart, it takes the single item from the cell and constructs the virtual list L of all of its +LM counterparts (lines 9–15). Then, it adds the top items of L to the target cell until the cell is judged to be full (lines 16–20). 6. Experiments The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit (Stolcke 2002). In this section we report on experiments with Mandarin-toEnglish translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty. Figure 10 Algorithm for faster integrated calculation of LM probabilities. 221 Computational Linguistics Volume 33, Number 2 6.1 Experimental Setup We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English si</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ignacio Thayer</author>
<author>Emil Ettelaie</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Dragos Stefan Munteanu</author>
<author>Franz Joseph Och</author>
<author>Quamrul Tipu</author>
</authors>
<title>The ISI/USC MT system.</title>
<date>2004</date>
<booktitle>In Proceedings of IWSLT 2004,</booktitle>
<pages>59--60</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="3435" citStr="Thayer et al. 2004" startWordPosition="532" endWordPosition="535">strings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have diplomatic relations with North Korea. If we count zhiyi (literally, ‘of-one’) as a single to</context>
<context position="45310" citStr="Thayer et al. 2004" startWordPosition="7786" endWordPosition="7789">the k-best derivations, so it is possible that further optimization could reduce these times. The intersecting decoder has the best score but runs very slowly. Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder. Among these tests, e = 0.1 gives the best results, but in general the optimal setting will depend on the other beam settings and the scale of the feature weights. 6.3 Evaluating Translation Accuracy We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al. 2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering). The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side). The second language model was also trained on the English side of the whole bitext. Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data. For example, extrac</context>
</contexts>
<marker>Thayer, Ettelaie, Knight, Marcu, Munteanu, Och, Tipu, 2004</marker>
<rawString>Thayer, Ignacio, Emil Ettelaie, Kevin Knight, Daniel Marcu, Dragos Stefan Munteanu, Franz Joseph Och, and Quamrul Tipu. 2004. The ISI/USC MT system. In Proceedings of IWSLT 2004, pages 59–60, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine 227</title>
<date>2004</date>
<contexts>
<context position="3498" citStr="Tillmann 2004" startWordPosition="543" endWordPosition="544"> Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have diplomatic relations with North Korea. If we count zhiyi (literally, ‘of-one’) as a single token, then translating this sentence correctly into English requ</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Tillmann, Christoph. 2004. A unigram orientation model for statistical machine 227</rawString>
</citation>
<citation valid="false">
<title>Computational Linguistics Volume 33, Number 2 translation.</title>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<pages>101--104</pages>
<location>Boston, MA.</location>
<marker></marker>
<rawString>Computational Linguistics Volume 33, Number 2 translation. In Proceedings of HLT-NAACL 2004, pages 101–104. Companion volume, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A localized preduction model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>557--564</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="3524" citStr="Tillmann and Zhang 2005" startWordPosition="545" endWordPosition="548">d Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have diplomatic relations with North Korea. If we count zhiyi (literally, ‘of-one’) as a single token, then translating this sentence correctly into English requires identifying a sequenc</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Tillmann, Christoph and Tong Zhang. 2005. A localized preduction model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the ACL, pages 557–564, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="34344" citStr="Wu (1996)" startWordPosition="5793" endWordPosition="5794">G that incorporates M. Thus PLM would be part of the rule weights (22) just like the other features. (For notational consistency, however, we write the LM probabilities separately from the rule weights.) In principle this method should admit no search errors, though in practice the blow-up in the effective size of the grammar necessitates pruning of the search space, which can cause search errors. The classic construction for intersecting a (non-synchronous) CFG with a finitestate machine is due to Bar-Hillel, Perles, and Shamir (1964), but we use a slightly different construction proposed by Wu (1996) for inversion transduction grammar and bigram LMs. We present an adaptation of his algorithm to synchronous CFGs with two nonterminals per right-hand side and general m-gram LMs. First, assume that the LM expects a whole sentence to be preceded by (m − 1) start-of-sentence symbols (s) and followed by a single end-of-sentence symbol (/s). The grammar can be made to do this simply by adding a rule S’ -4 (S 1 , (s)m−1 S 1 (/s)) (26) and making S’ the new start symbol. First, we define two functions p and q which operate on strings over T U {*}, where T is the English terminal alphabet, and * is </context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Wu, Dekai. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the 34th Annual Meeting of the ACL, pages 152–158, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="6849" citStr="Wu 1997" startWordPosition="1084" endWordPosition="1085"> [shi] [[[yu [Beihan]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi] [Australia] [is] [one of [the [few countries]3 that [have [dipl. rels.]2 with [N. Korea]1]]] The system we describe in this article uses rules like (3), (4), and (5), which we formalize in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover, the system is able to learn them automatically from a parallel text without syntactic annotation. Because our system uses a synchronous CFG, it could be thought of as an example of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST. Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT’s capacity for memorizing translations from parallel data. Other insights</context>
<context position="8844" citStr="Wu (1997)" startWordPosition="1381" endWordPosition="1382">untries having [diplomatic relations]2] [with]1 North Korea] .] 2 An earlier version of the system described in this article was entered by the University of Maryland as its primary system in the 2005 NIST MT Evaluation. The results can be found at http://www.nist.gov/speech/tests/mt/mt05eval official results release 20050801 v3.html. 203 Computational Linguistics Volume 33, Number 2 2. Related Work Approaches to syntax-based statistical MT have varied in their reliance on syntactic theories, or annotations made according to syntactic theories. At one extreme are those, exemplified by that of Wu (1997), that have no dependence on syntactic theory beyond the idea that natural language is hierarchical. If these methods distinguish between different categories, they typically do not distinguish very many. Our approach, as presented here, falls squarely into this family. By contrast, other approaches, exemplified by that of Yamada and Knight (2001), do make use of parallel data with syntactic annotations, either in the form of phrase-structure trees or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because syntactically annotated corpora are comparatively small, obtai</context>
<context position="16027" citStr="Wu 1997" startWordPosition="2652" endWordPosition="2653">to the following constraints, chosen to balance grammar size and performance on our development set: 1. If there are multiple initial phrase pairs containing the same set of alignments, only the smallest is kept. That is, unaligned words are not allowed at the edges of phrases. 2. Initial phrases are limited to a length of 10 words on either side. 3. Rules are limited to five nonterminals plus terminals on the French side. 4. Rules can have at most two nonterminals, which simplifies the decoder implementation. This also makes our grammar weakly equivalent to an inversion transduction grammar (Wu 1997), although the conversion would create a very large number of new nonterminal symbols. 5. It is prohibited for nonterminals to be adjacent on the French side, a major cause of spurious ambiguity. 6. A rule must have at least one pair of aligned words, so that translation decisions are always based on some lexical evidence. Variations of constraints (1) and (2) are also commonly used in phrase-based systems. 208 Chiang Hierarchical Phrase-Based Translation 3.3 Other Rules Glue rules. Having extracted rules from the training data, we could let X be the grammar’s start symbol and translate new se</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, Dekai. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the Twentienth International Conference on Computational Linguistics (COLING),</booktitle>
<pages>508--514</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="10088" citStr="Xia and McCord (2004)" startWordPosition="1572" endWordPosition="1575">xt in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. 3. Grammar The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968)</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Xia, Fei and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of the Twentienth International Conference on Computational Linguistics (COLING), pages 508–514, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>523--530</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="6912" citStr="Yamada and Knight 2001" startWordPosition="1091" endWordPosition="1094">oshu guojia]3] zhiyi] [Australia] [is] [one of [the [few countries]3 that [have [dipl. rels.]2 with [N. Korea]1]]] The system we describe in this article uses rules like (3), (4), and (5), which we formalize in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover, the system is able to learn them automatically from a parallel text without syntactic annotation. Because our system uses a synchronous CFG, it could be thought of as an example of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST. Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT’s capacity for memorizing translations from parallel data. Other insights borrowed from the current state of the art include minimum-err</context>
<context position="9193" citStr="Yamada and Knight (2001)" startWordPosition="1433" endWordPosition="1436">l. 203 Computational Linguistics Volume 33, Number 2 2. Related Work Approaches to syntax-based statistical MT have varied in their reliance on syntactic theories, or annotations made according to syntactic theories. At one extreme are those, exemplified by that of Wu (1997), that have no dependence on syntactic theory beyond the idea that natural language is hierarchical. If these methods distinguish between different categories, they typically do not distinguish very many. Our approach, as presented here, falls squarely into this family. By contrast, other approaches, exemplified by that of Yamada and Knight (2001), do make use of parallel data with syntactic annotations, either in the form of phrase-structure trees or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction fro</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the ACL, pages 523–530, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<pages>257--264</pages>
<location>Boston, MA.</location>
<contexts>
<context position="3176" citStr="Zens and Ney 2004" startWordPosition="489" endWordPosition="492">features of a log-linear model (Och and Ney 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11&apos;1 � Æ JLF01 P �� n �� 0* �- Aozhou shi yu Bei</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Zens, Richard and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In Proceedings of HLT-NAACL 2004, pages 257–264, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>