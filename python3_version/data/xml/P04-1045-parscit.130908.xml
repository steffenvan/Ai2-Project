<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983057">
Predicting Student Emotions in Computer-Human Tutoring Dialogues
</title>
<author confidence="0.996882">
Diane J. Litman Kate Forbes-Riley
</author>
<affiliation confidence="0.806612666666667">
University of Pittsburgh University of Pittsburgh
Department of Computer Science Learning Research and Development Center
Learning Research and Development Center Pittsburgh PA, 15260, USA
</affiliation>
<address confidence="0.43636">
Pittsburgh PA, 15260, USA forbesk@pitt.edu
</address>
<email confidence="0.999084">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975357142857">
We examine the utility of speech and lexical fea-
tures for predicting student emotions in computer-
human spoken tutoring dialogues. We first anno-
tate student turns for negative, neutral, positive and
mixed emotions. We then extract acoustic-prosodic
features from the speech signal, and lexical items
from the transcribed or recognized speech. We com-
pare the results of machine learning experiments us-
ing these features alone or in combination to pre-
dict various categorizations of the annotated student
emotions. Our best results yield a 19-36% relative
improvement in error reduction over a baseline. Fi-
nally, we compare our results with emotion predic-
tion in human-human tutoring dialogues.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999744870967742">
This paper explores the feasibility of automatically
predicting student emotional states in a corpus of
computer-human spoken tutoring dialogues. Intel-
ligent tutoring dialogue systems have become more
prevalent in recent years (Aleven and Rose, 2003),
as one method of improving the performance gap
between computer and human tutors; recent exper-
iments with such systems (e.g., (Graesser et al.,
2002)) are starting to yield promising empirical
results. Another method for closing this perfor-
mance gap has been to incorporate affective reason-
ing into computer tutoring systems, independently
of whether or not the tutor is dialogue-based (Conati
et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For
example, (Aist et al., 2002) have shown that adding
human-provided emotional scaffolding to an auto-
mated reading tutor increases student persistence.
Our long-term goal is to merge these lines of dia-
logue and affective tutoring research, by enhancing
our intelligent tutoring spoken dialogue system to
automatically predict and adapt to student emotions,
and to investigate whether this improves learning
and other measures of performance.
Previous spoken dialogue research has shown
that predictive models of emotion distinctions (e.g.,
emotional vs. non-emotional, negative vs. non-
negative) can be developed using features typically
available to a spoken dialogue system in real-time
(e.g, acoustic-prosodic, lexical, dialogue, and/or
contextual) (Batliner et al., 2000; Lee et al., 2001;
Lee et al., 2002; Ang et al., 2002; Batliner et al.,
2003; Shafran et al., 2003). In prior work we
built on and generalized such research, by defin-
ing a three-way distinction between negative, neu-
tral, and positive student emotional states that could
be reliably annotated and accurately predicted in
human-human spoken tutoring dialogues (Forbes-
Riley and Litman, 2004; Litman and Forbes-Riley,
2004). Like the non-tutoring studies, our results
showed that combining feature types yielded the
highest predictive accuracy.
In this paper we investigate the application of
our approach to a comparable corpus of computer-
human tutoring dialogues, which displays many dif-
ferent characteristics, such as shorter utterances, lit-
tle student initiative, and non-overlapping speech.
We investigate whether we can annotate and predict
student emotions as accurately and whether the rel-
ative utility of speech and lexical features as pre-
dictors is the same, especially when the output of
the speech recognizer is used (rather than a human
transcription of the student speech). Our best mod-
els for predicting three different types of emotion
classifications achieve accuracies of 66-73%, repre-
senting relative improvements of 19-36% over ma-
jority class baseline errors. Our computer-human
results also show interesting differences compared
with comparable analyses of human-human data.
Our results provide an empirical basis for enhanc-
ing our spoken dialogue tutoring system to automat-
ically predict and adapt to a student model that in-
cludes emotional states.
</bodyText>
<sectionHeader confidence="0.963724" genericHeader="introduction">
2 Computer-Human Dialogue Data
</sectionHeader>
<bodyText confidence="0.999844942307692">
Our data consists of student dialogues with IT-
SPOKE (Intelligent Tutoring SPOKEn dialogue
system) (Litman and Silliman, 2004), a spoken dia-
logue tutor built on top of the Why2-Atlas concep-
tual physics text-based tutoring system (VanLehn et
al., 2002). In ITSPOKE, a student first types an
essay answering a qualitative physics problem. IT-
SPOKE then analyzes the essay and engages the stu-
dent in spoken dialogue to correct misconceptions
and to elicit complete explanations.
First, the Why2-Atlas back-end parses the student
essay into propositional representations, in order to
find useful dialogue topics. It uses 3 different ap-
proaches (symbolic, statistical and hybrid) compet-
itively to create a representation for each sentence,
then resolves temporal and nominal anaphora and
constructs proofs using abductive reasoning (Jor-
dan et al., 2004). During the dialogue, student
speech is digitized from microphone input and sent
to the Sphinx2 recognizer, whose stochastic lan-
guage models have a vocabulary of 1240 words and
are trained with 7720 student utterances from eval-
uations of Why2-Atlas and from pilot studies of IT-
SPOKE. Sphinx2’s best “transcription” (recognition
output) is then sent to the Why2-Atlas back-end for
syntactic, semantic and dialogue analysis. Finally,
the text response produced by Why2-Atlas is sent
to the Cepstral text-to-speech system and played to
the student. After the dialogue, the student revises
the essay, thereby ending the tutoring or causing an-
other round of tutoring/essay revision.
Our corpus of dialogues with ITSPOKE was col-
lected from November 2003 - April 2004, as part
of an evaluation comparing ITSPOKE, Why2-Atlas,
and human tutoring (Litman et al., 2004). Sub-
jects are University of Pittsburgh students who have
never taken college physics, and who are native En-
glish speakers. Subjects first read a small document
of background physics material, then work through
5 problems (dialogues) with ITSPOKE. The corpus
contains 100 dialogues (physics problems) from 20
subjects, with a total of 2445 student turns and 398
unique words. 15 dialogues have been annotated for
emotion as described in Section 3. On average, our
dialogues last 19.4 minutes and contain 25 student
turns. While ITSPOKE’s word error rate on this
corpus is 31.2%, semantic accuracy is more useful
for dialogue evaluation as it does not penalize for
unimportant word errors. Semantic analysis based
on speech recognition is the same as based on per-
fect transcription 92.4% of the time. An emotion-
annotated corpus example is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.969141" genericHeader="method">
3 Annotating Student Turns
</sectionHeader>
<bodyText confidence="0.872333642857143">
In our data, student “emotions”1 can only be iden-
tified indirectly: via what is said and/or how it is
... dialogue excerpt at 18.3 min. into session...
ITSPOKE : What is the magnitude of the accel-
eration of the packet in the horizontal direction?
STUDENT : i don’t know (NEGATIVE)
ITSPOKE : Ok, let’s consider how forces in the
vertical direction affect the motion of a body in a
horizontal direction in a different scenario. Say an
apple falls from a tree. What force(s) are acting on
the apple as it falls?
STUDENT : gravity (NEGATIVE)
ITSPOKE : Yeah. Are there any other forces act-
ing on the apple as it falls?
</bodyText>
<equation confidence="0.645507">
STUDENT : no (NEGATIVE)
</equation>
<bodyText confidence="0.6672325">
ITSPOKE : Fine. The only force acting on the
apple as it falls to the earth is the earth’s gravita-
tional force. In which direction does gravitational
force act?
STUDENT : down (POSITIVE)
ITSPOKE : Yep. Therefore, what is the magni-
tude of this gravitational force in the horizontal di-
rection?
STUDENT : in the direction of the airplane
(NEUTRAL)
</bodyText>
<figureCaption confidence="0.99882">
Figure 1: Annotated Spoken Dialogue Excerpt
</figureCaption>
<bodyText confidence="0.999493291666667">
said. In (Litman and Forbes-Riley, 2004), we dis-
cuss a scheme for manually annotating student turns
in a human-human tutoring dialogue corpus for in-
tuitively perceived emotions.2 These emotions are
viewed along a linear scale, shown and defined as
follows: negative neutral positive.
Negative: a student turn that expresses emotions
such as confused, bored, irritated. Evidence of a
negative emotion can come from many knowledge
sources such as lexical items (e.g., “I don’t know”
in student in Figure 1), and/or acoustic-prosodic
features (e.g., prior-turn pausing in student ).
Positive: a student turn expressing emotions such
as confident, enthusiastic. An example is student ,
which displays louder speech and faster tempo.
Neutral: a student turn not expressing a nega-
tive or positive emotion. An example is student ,
where evidence comes from moderate loudness,
pitch and tempo.
We also distinguish Mixed: a student turn ex-
pressing both positive and negative emotions.
To avoid influencing the annotator’s intuitive un-
derstanding of emotion expression, and because
particular emotional cues are not used consistently
</bodyText>
<footnote confidence="0.950940666666667">
1We use the term “emotion” loosely to cover both affects
and attitudes that can impact student learning.
2Weak and strong expressions of emotions are annotated.
</footnote>
<bodyText confidence="0.978732895522388">
or unambiguously across speakers, our annotation
manual does not associate particular cues with par-
ticular emotion labels. Instead, it contains examples
of labeled dialogue excerpts (as in Figure 1, except
on human-human data) with links to corresponding
audio files. The cues mentioned in the discussion of
Figure 1 above were elicited during post-annotation
discussion of the emotions, and are presented here
for expository use only. (Litman and Forbes-Riley,
2004) further details our annotation scheme and dis-
cusses how it builds on related work.
To analyze the reliability of the scheme on our
new computer-human data, we selected 15 tran-
scribed dialogues from the corpus described in Sec-
tion 2, yielding a dataset of 333 student turns, where
approximately 30 turns came from each of 10 sub-
jects. The 333 turns were separately annotated by
two annotators following the emotion annotation
scheme described above.
We focus here on three analyses of this data, item-
ized below. While the first analysis provides the
most fine-grained distinctions for triggering system
adaptation, the second and third (simplified) analy-
ses correspond to those used in (Lee et al., 2001)
and (Batliner et al., 2000), respectively. These
represent alternative potentially useful triggering
mechanisms, and are worth exploring as they might
be easier to annotate and/or predict.
Negative, Neutral, Positive (NPN): mixeds
are conflated with neutrals.
Negative, Non-Negative (NnN): positives,
mixeds, neutrals are conflated as non-
negatives.
Emotional, Non-Emotional (EnE): nega-
tives, positives, mixeds are conflated as Emo-
tional; neutrals are Non-Emotional.
Tables 1-3 provide a confusion matrix for each
analysis summarizing inter-annotator agreement.
The rows correspond to the labels assigned by an-
notator 1, and the columns correspond to the labels
assigned by annotator 2. For example, the annota-
tors agreed on 89 negatives in Table 1.
In the NnN analysis, the two annotators agreed on
the annotations of 259/333 turns achieving 77.8%
agreement, with Kappa = 0.5. In the EnE analy-
sis, the two annotators agreed on the annotations
of 220/333 turns achieving 66.1% agreement, with
Kappa = 0.3. In the NPN analysis, the two anno-
tators agreed on the annotations of 202/333 turns
achieving 60.7% agreement, with Kappa = 0.4. This
inter-annotator agreement is on par with that of
prior studies of emotion annotation in naturally oc-
curring computer-human dialogues (e.g., agreement
of 71% and Kappa of 0.47 in (Ang et al., 2002),
Kappa of 0.45 and 0.48 in (Narayanan, 2002), and
Kappa ranging between 0.32 and 0.42 in (Shafran
et al., 2003)). A number of researchers have ac-
commodated for this low agreement by exploring
ways of achieving consensus between disagreed an-
notations, to yield 100% agreement (e.g (Ang et al.,
2002; Devillers et al., 2003)). As in (Ang et al.,
2002), we will experiment below with predicting
emotions using both our agreed data and consensus-
labeled data.
negative non-negative
negative 89 36
non-negative 38 170
</bodyText>
<tableCaption confidence="0.99853">
Table 1: NnN Analysis Confusion Matrix
</tableCaption>
<table confidence="0.972615">
emotional non-emotional
emotional 129 43
non-emotional 70 91
</table>
<tableCaption confidence="0.985595">
Table 2: EnE Analysis Confusion Matrix
</tableCaption>
<table confidence="0.60486025">
negative neutral positive
negative 89 30 6
neutral 32 94 38
positive 6 19 19
</table>
<tableCaption confidence="0.993273">
Table 3: NPN Analysis Confusion Matrix
</tableCaption>
<sectionHeader confidence="0.920056" genericHeader="method">
4 Extracting Features from Turns
</sectionHeader>
<bodyText confidence="0.999552789473684">
For each of the 333 student turns described above,
we next extracted the set of features itemized in Fig-
ure 2, for use in the machine learning experiments
described in Section 5.
Motivated by previous studies of emotion predic-
tion in spontaneous dialogues (Ang et al., 2002; Lee
et al., 2001; Batliner et al., 2003), our acoustic-
prosodic features represent knowledge of pitch, en-
ergy, duration, tempo and pausing. We further re-
strict our features to those that can be computed
automatically and in real-time, since our goal is to
use such features to trigger online adaptation in IT-
SPOKE based on predicted student emotions. F0
and RMS values, representing measures of pitch and
loudness, respectively, are computed using Entropic
Research Laboratory’s pitch tracker, get f0, with no
post-correction. Amount of Silence is approximated
as the proportion of zero f0 frames for the turn. Turn
Duration and Prior Pause Duration are computed
</bodyText>
<sectionHeader confidence="0.690689" genericHeader="method">
Acoustic-Prosodic Features
</sectionHeader>
<bodyText confidence="0.619101909090909">
4 fundamental frequency (f0): max, min,
mean, standard deviation
4 energy (RMS): max, min, mean, standard de-
viation
4 temporal: amount of silence in turn, turn du-
ration, duration of pause prior to turn, speaking
rate
Lexical Features
human-transcribed lexical items in the turn
ITSPOKE-recognized lexical items in the turn
Identifier Features: subject, gender, problem
</bodyText>
<figureCaption confidence="0.993436">
Figure 2: Features Per Student Turn
</figureCaption>
<bodyText confidence="0.999904615384615">
automatically via the start and end turn boundaries
in ITSPOKE logs. Speaking Rate is automatically
calculated as #syllables per second in the turn.
While acoustic-prosodic features address how
something is said, lexical features representing what
is said have also been shown to be useful for predict-
ing emotion in spontaneous dialogues (Lee et al.,
2002; Ang et al., 2002; Batliner et al., 2003; Dev-
illers et al., 2003; Shafran et al., 2003). Our first set
of lexical features represents the human transcrip-
tion of each student turn as a word occurrence vec-
tor (indicating the lexical items that are present in
the turn). This feature represents the “ideal” perfor-
mance of ITSPOKE with respect to speech recogni-
tion. The second set represents ITSPOKE’s actual
best speech recognition hypothesis of what is said in
each student turn, again as a word occurrence vec-
tor.
Finally, we recorded for each turn the 3 “iden-
tifier” features shown last in Figure 2. Prior stud-
ies (Oudeyer, 2002; Lee et al., 2002) have shown
that “subject” and “gender” can play an important
role in emotion recognition. “Subject” and “prob-
lem” are particularly important in our tutoring do-
main because students will use our system repeat-
edly, and problems are repeated across students.
</bodyText>
<sectionHeader confidence="0.984811" genericHeader="method">
5 Predicting Student Emotions
</sectionHeader>
<subsectionHeader confidence="0.990469">
5.1 Feature Sets and Method
</subsectionHeader>
<bodyText confidence="0.998768555555556">
We next created the 10 feature sets in Figure 3,
to study the effects that various feature combina-
tions had on predicting emotion. We compare
an acoustic-prosodic feature set (“sp”), a human-
transcribed lexical items feature set (“lex”) and
an ITSPOKE-recognized lexical items feature set
(“asr”). We further compare feature sets combin-
ing acoustic-prosodic and either transcribed or rec-
ognized lexical items (“sp+lex”, “sp+asr”). Finally,
we compare each of these 5 feature sets with an
identical set supplemented with our 3 identifier fea-
tures (“+id”).
sp: 12 acoustic-prosodic features
lex: human-transcribed lexical items
asr: ITSPOKE recognized lexical items
sp+lex: combined sp and lex features
sp+asr: combined sp and asr features
+id: each above set + 3 identifier features
</bodyText>
<figureCaption confidence="0.993412">
Figure 3: Feature Sets for Machine Learning
</figureCaption>
<bodyText confidence="0.999973571428571">
We use the Weka machine learning soft-
ware (Witten and Frank, 1999) to automatically
learn our emotion prediction models. In our human-
human dialogue studies (Litman and Forbes, 2003),
the use of boosted decision trees yielded the most
robust performance across feature sets so we will
continue their use here.
</bodyText>
<subsectionHeader confidence="0.999333">
5.2 Predicting Agreed Turns
</subsectionHeader>
<bodyText confidence="0.92261">
As in (Shafran et al., 2003; Lee et al., 2001), our
first study looks at the clearer cases of emotional
turns, i.e. only those student turns where the two
annotators agreed on an emotion label.
Tables 4-6 show, for each emotion classification,
the mean accuracy (%correct) and standard error
(SE) for our 10 feature sets (Figure 3), computed
across 10 runs of 10-fold cross-validation.&apos; For
comparison, the accuracy of a standard baseline al-
gorithm (MAJ), which always predicts the major-
ity class, is shown in each caption. For example,
Table 4’s caption shows that for NnN, always pre-
dicting the majority class of non-negative yields an
accuracy of 65.65%. In each table, the accuracies
are labeled for how they compare statistically to the
relevant baseline accuracy ( = worse, = same,
= better), as automatically computed in Weka using
a two-tailed t-test (p .05).
First note that almost every feature set signif-
icantly outperforms the majority class baseline,
across all emotion classifications; the only excep-
tions are the speech-only feature sets without iden-
tifier features (“sp-id”) in the NnN and EnE tables,
which perform the same as the baseline. These re-
sults suggest that without any subject or task spe-
cific information, acoustic-prosodic features alone
3For each cross-validation, the training and test data are
drawn from utterances produced by the same set of speakers.
A separate experiment showed that testing on one speaker and
training on the others, averaged across all speakers, does not
significantly change the results.
are not useful predictors for our two binary classi-
fication tasks, at least in our computer-human dia-
logue corpus. As will be discussed in Section 6,
however, “sp-id” feature sets are useful predictors
in human-human tutoring dialogues.
Feat. Set -id SE +id SE
sp 64.10 0.80 70.66 0.76
lex 68.20 0.41 72.74 0.58
asr 72.30 0.58 70.51 0.59
sp+lex 71.78 0.77 72.43 0.87
sp+asr 69.90 0.57 71.44b 0.68
</bodyText>
<tableCaption confidence="0.8557745">
Table 4: %Correct, NnN Agreed, MAJ (non-
negative) = 65.65%
</tableCaption>
<bodyText confidence="0.845030166666667">
Feat. Set -id SE +id SE
sp 59.18 0.75 70.68 0.89
lex 63.18 0.82 75.64 0.37
asr 66.36 0.54 72.91 0.35
sp+lex 63.86 0.97 69.59 0.48
sp+asr 65.14 0.82 69.64 0.57
</bodyText>
<tableCaption confidence="0.677688">
Table 5: %Correct, EnE Agreed, MAJ (emotional)
= 58.64%
</tableCaption>
<bodyText confidence="0.973412872340426">
Feat. Set -id SE +id SE
sp 55.49 1.01 62.03 0.91
lex 52.66 0.62 67.84 0.66
asr 57.95 0.67 65.70 0.50
sp+lex 62.08 0.56 63.52 0.48
sp+asr 61.22 1.20 62.23 0.86
Table 6: %Correct, NPN Agreed, MAJ (neutral) =
46.52%
Further note that adding identifier features to the
“-id” feature sets almost always improves perfor-
mance, although this difference is not always sig-
nificant4; across tables the “+id” feature sets out-
perform their “-id” counterparts across all feature
sets and emotion classifications except one (NnN
“asr”). Surprisingly, while (Lee et al., 2002) found
it useful to develop separate gender-based emotion
prediction models, in our experiment, gender is the
only identifier that does not appear in any learned
model. Also note that with the addition of identifier
features, the speech-only feature sets (sp+id) now
do outperform the majority class baselines for all
three emotion classifications.
4For any feature set, the mean +/- 2*SE = the 95% con-
fidence interval. If the confidence intervals for two feature
sets are non-overlapping, then their mean accuracies are sig-
nificantly different with 95% confidence.
With respect to the relative utility of lexical ver-
sus acoustic-prosodic features, without identifier
features, using only lexical features (“lex” or “asr”)
almost always produces statistically better perfor-
mance than using only speech features (“sp”); the
only exception is NPN “lex”, which performs sta-
tistically the same as NPN “sp”. This is consistent
with others’ findings, e.g., (Lee et al., 2002; Shafran
et al., 2003). When identifier features are added
to both, the lexical sets don’t always significantly
outperform the speech set; only in NPN and EnE
“lex+id” is this the case. For NnN, just as using
“sp+id” rather than “sp-id” improved performance
when compared to the majority baseline, the addi-
tion of the identifier features also improves the util-
ity of the speech features when compared to the lex-
ical features.
Interestingly, although we hypothesized that the
“lex” feature sets would present an upper bound on
the performance of the “asr” sets, because the hu-
man transcription is more accurate than the speech
recognizer, we see that this is not consistently the
case. In fact, in the “-id” sets, “asr” always signifi-
cantly outperforms “lex”. A comparison of the de-
cision trees produced in either case, however, does
not reveal why this is the case; words chosen as pre-
dictors are not very intuitive in either case (e.g., for
NnN, an example path through the learned “lex” de-
cision tree says predict negative if the utterance con-
tains the word will but does not contain the word
decrease). Understanding this result is an area for
future research. Within the “+id” sets, we see that
“lex” and “asr” perform the same in the NnN and
NPN classifications; in EnE “lex+id” significantly
outperforms “asr+id”. The utility of the “lex” fea-
tures compared to “asr” also increases when com-
bined with the “sp” features (with and without iden-
tifiers), for both NnN and NPN.
Moreover, based on results in (Lee et al., 2002;
Ang et al., 2002; Forbes-Riley and Litman, 2004),
we hypothesized that combining speech and lexical
features would result in better performance than ei-
ther feature set alone. We instead found that the rel-
ative performance of these sets depends both on the
emotion classification being predicted and the pres-
ence or absence of “id” features. Although consis-
tently with prior research we find that the combined
feature sets usually outperform the speech-only fea-
ture sets, the combined feature sets frequently per-
form worse than the lexical-only feature sets. How-
ever, we will see in Section 6 that combining knowl-
edge sources does improve prediction performance
in human-human dialogues.
Finally, the bolded accuracies in each table sum-
marize the best-performing feature sets with and
without identifiers, with respect to both the %Corr
figures shown in the tables, as well as to relative
improvement in error reduction over the baseline
(MAJ) error5, after excluding all the feature sets
containing “lex” features. In this way we give a
better estimate of the best performance our system
could accomplish, given the features it can currently
access from among those discussed. These best-
performing feature sets yield relative improvements
over their majority baseline errors ranging from 19-
36%. Moreover, although the NPN classification
yields the lowest raw accuracies, it yields the high-
est relative improvement over its baseline.
</bodyText>
<subsectionHeader confidence="0.998791">
5.3 Predicting Consensus Turns
</subsectionHeader>
<bodyText confidence="0.989416909090909">
Following (Ang et al., 2002; Devillers et al., 2003),
we also explored consensus labeling, both with the
goal of increasing our usable data set for predic-
tion, and to include the more difficult annotation
cases. For our consensus labeling, the original an-
notators revisited each originally disagreed case,
and through discussion, sought a consensus label.
Due to consensus labeling, agreement rose across
all three emotion classifications to 100%. Tables 7-
9 show, for each emotion classification, the mean
accuracy (%correct) and standard error (SE) for our
</bodyText>
<sectionHeader confidence="0.620858" genericHeader="method">
10 feature sets.
</sectionHeader>
<bodyText confidence="0.980803333333333">
Feat. Set -id SE +id SE
sp 59.10 0.57 64.20 0.52
lex 63.70 0.47 68.64 0.41
asr 66.26 0.71 68.13 0.56
sp+lex 64.69 0.61 65.40 0.63
sp+asr 65.99 0.51 67.55 0.48
</bodyText>
<tableCaption confidence="0.95163">
Table 7: %Corr., NnN Consensus, MAJ=62.47%
</tableCaption>
<bodyText confidence="0.870399833333333">
Feat. Set -id SE +id SE
sp 56.13 0.94 59.30 0.48
lex 52.07 0.34 65.37 0.47
asr 53.78 0.66 64.13 0.51
sp+lex 60.96 0.76 63.01 0.62
sp+asr 57.84 0.73 60.89 0.38
</bodyText>
<tableCaption confidence="0.963195">
Table 8: %Corr., EnE Consensus, MAJ=55.86%
</tableCaption>
<bodyText confidence="0.9991036">
A comparison with Tables 4-6 shows that overall,
using consensus-labeled data decreased the perfor-
mance across all feature sets and emotion classifi-
cations. This was also found in (Ang et al., 2002).
Moreover, it is no longer the case that every feature
</bodyText>
<subsectionHeader confidence="0.733392">
5Relative improvement over the baseline (MAJ) error for
</subsectionHeader>
<bodyText confidence="0.99165275">
feature set x = , where error(x) is 100
minus the %Corr(x) value shown in Tables 4-6.
Feat. Set -id SE +id SE
sp 48.97 0.66 51.90 0.40
lex 47.86 0.54 57.28 0.44
asr 51.09 0.66 53.41 0.66
sp+lex 53.41 0.62 54.20 0.86
sp+asr 52.50 0.42 53.84 0.42
</bodyText>
<tableCaption confidence="0.974402">
Table 9: %Corr., NPN Consensus, MAJ=48.35%
</tableCaption>
<bodyText confidence="0.999163148148148">
set performs as well as or better than their base-
lines6; within the “-id” sets, NnN “sp” and EnE
“lex” perform significantly worse than their base-
lines. However, again we see that the “+id” sets do
consistently better than the “-id” sets and moreover
always outperform the baselines.
We also see again that using only lexical features
almost always yields better performance than us-
ing only speech features. In addition, we again see
that the “lex” feature sets perform comparably to the
“asr” feature sets, rather than outperforming them as
we first hypothesized. And finally, we see again that
while in most cases combining speech and lexical
features yields better performance than using only
speech features, the combined feature sets in most
cases perform the same or worse than the lexical
feature sets. As above, the bolded accuracies sum-
marize the best-performing feature sets from each
emotion classification, after excluding all the fea-
ture sets containing “lex” to give a better estimate
of actual system performance. The best-performing
feature sets in the consensus data yield an 11%-19%
relative improvement in error reduction compared to
the majority class prediction, which is a lower error
reduction than seen for agreed data. Moreover, the
NPN classification yields the lowest accuracies and
the lowest improvements over its baseline.
</bodyText>
<sectionHeader confidence="0.742714" genericHeader="method">
6 Comparison with Human Tutoring
</sectionHeader>
<bodyText confidence="0.998924384615385">
While building ITSPOKE, we collected a corre-
sponding corpus of spoken human tutoring dia-
logues, using the same experimental methodology
as for our computer tutoring corpus (e.g. same sub-
ject pool, physics problems, web and audio inter-
face, etc); the only difference between the two cor-
pora is whether the tutor is human or computer.
As discussed in (Forbes-Riley and Litman, 2004),
two annotators had previously labeled 453 turns in
this corpus with the emotion annotation scheme dis-
cussed in Section 3, and performed a preliminary
set of machine learning experiments (different from
those reported above). Here, we perform the exper-
</bodyText>
<footnote confidence="0.963313">
6The majority class for EnE Consensus is non-emotional;
all others are unchanged.
</footnote>
<table confidence="0.9991396">
NnN EnE NPN
FS -id SE +id SE -id SE +id SE -id SE +id SE
sp 77.46 0.42 77.56 0.30 84.71 0.39 84.66 0.40 73.09 0.68 74.18 0.40
lex 80.74 0.42 80.60 0.34 88.86 0.26 86.23 0.34 78.56 0.45 77.18 0.43
sp+lex 81.37 0.33 80.79 0.41 87.74 0.36 88.31 0.29 79.06 0.38 78.03 0.33
</table>
<tableCaption confidence="0.999339">
Table 10: Human-Human %Correct, NnN MAJ=72.21%; EnE MAJ=50.86%; NPN MAJ=53.24%
</tableCaption>
<bodyText confidence="0.999980869565218">
iments from Section 5.2 on this annotated human
tutoring data, as a step towards understand the dif-
ferences between annotating and predicting emotion
in human versus computer tutoring dialogues.
With respect to inter-annotator agreement, in
the NnN analysis, the two annotators had 88.96%
agreement (Kappa = 0.74). In the EnE analysis, the
annotators had 77.26% agreement (Kappa = 0.55).
In the NPN analysis, the annotators had 75.06%
agreement (Kappa = 0.60). A comparison with the
results in Section 3 shows that all of these figures are
higher than their computer tutoring counterparts.
With respect to predictive accuracy, Table 10
shows our results for the agreed data. A compari-
son with Tables 4-6 shows that overall, the human-
human data yields increased performance across all
feature sets and emotion classifications, although it
should be noted that the human-human corpus is
over 100 turns larger than the computer-human cor-
pus. Every feature set performs significantly better
than their baselines. However, unlike the computer-
human data, we don’t see the “+id” sets perform-
ing better than the “-id” sets; rather, both sets per-
form about the same. We do see again the “lex”
sets yielding better performance than the “sp” sets.
However, we now see that in 5 out of 6 cases, com-
bining speech and lexical features yields better per-
formance than using either “sp” or “lex” alone. Fi-
nally, these feature sets yield a relative error re-
duction of 42.45%-77.33% compared to the major-
ity class predictions, which is far better than in our
computer tutoring experiments. Moreover, the EnE
classification yields the highest raw accuracies and
relative improvements over baseline error.
We hypothesize that such differences arise in part
due to differences between the two corpora: 1) stu-
dent turns with the computer tutor are much shorter
than with the human tutor (and thus contain less
emotional content - making both annotation and
prediction more difficult), 2) students respond to
the computer tutor differently and perhaps more id-
iosyncratically than to the human tutor, 3) the com-
puter tutor is less “flexible” than the human tutor
(allowing little student initiative, questions, ground-
ings, contextual references, etc.), which also effects
student emotional response and its expression.
</bodyText>
<sectionHeader confidence="0.996792" genericHeader="conclusions">
7 Conclusions and Current Directions
</sectionHeader>
<bodyText confidence="0.99998795">
Our results show that acoustic-prosodic and lexical
features can be used to automatically predict student
emotion in computer-human tutoring dialogues.
We examined emotion prediction using a classi-
fication scheme developed for our prior human-
human tutoring studies (negative/positive/neutral),
as well as using two simpler schemes proposed by
other dialogue researchers (negative/non-negative,
emotional/non-emotional). We used machine learn-
ing to examine the impact of different feature sets
on prediction accuracy. Across schemes, our fea-
ture sets outperform a majority baseline, and lexi-
cal features outperform acoustic-prosodic features.
While adding identifier features typically also im-
proves performance, combining lexical and speech
features does not. Our analyses also suggest that
prediction in consensus-labeled turns is harder than
in agreed turns, and that prediction in our computer-
human corpus is harder and based on somewhat dif-
ferent features than in our human-human corpus.
Our continuing work extends this methodology
with the goal of enhancing ITSPOKE to predict and
adapt to student emotions. We continue to manu-
ally annotate ITSPOKE data, and are exploring par-
tial automation via semi-supervised machine learn-
ing (Maeireizo-Tokeshi et al., 2004). Further man-
ual annotation might also improve reliability, as un-
derstanding systematic disagreements can lead to
coding manual revisions. We are also expanding our
feature set to include features suggested in prior di-
alogue research, tutoring-dependent features (e.g.,
pedagogical goal), and other features available in
our logs (e.g., semantic analysis). Finally, we will
explore how the recognized emotions can be used
to improve system performance. First, we will label
human tutor adaptations to emotional student turns
in our human tutoring corpus; this labeling will be
used to formulate adaptive strategies for ITSPOKE,
and to determine which of our three prediction tasks
best triggers adaptation.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998613333333333">
This research is supported by NSF Grants 9720359
&amp; 0328431. Thanks to the Why2-Atlas team and S.
Silliman for system design and data collection.
</bodyText>
<sectionHeader confidence="0.989553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999579895238095">
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intel-
ligent tutoring system with human-supplied ca-
pabilities: Adding Human-Provided Emotional
Scaffolding to an Automated Reading Tutor that
Listens. In Proc. Intelligent Tutoring Systems.
V. Aleven and C. P. Rose, editors. 2003. Proc. AI in
Education Workshop on Tutorial Dialogue Sys-
tems: With a View toward the Classroom.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and
A. Stolcke. 2002. Prosody-based automatic de-
tection of annoyance and frustration in human-
computer dialog. In Proc. International Conf. on
Spoken Language Processing (ICSLP).
A. Batliner, K. Fischer, R. Huber, J. Spilker, and
E. N¨oth. 2000. Desperately seeking emotions:
Actors, wizards, and human beings. In Proc.
ISCA Workshop on Speech and Emotion.
A. Batliner, K. Fischer, R. Huber, J. Spilker, and
E. Noth. 2003. How to find trouble in communi-
cation. Speech Communication, 40:117–143.
K. Bhatt, M. Evens, and S. Argamon. 2004.
Hedged responses and expressions of affect in hu-
man/human and human/computer tutorial inter-
actions. In Proc. Cognitive Science.
C. Conati, R. Chabbal, and H. Maclaren. 2003.
A study on using biometric sensors for moni-
toring user emotions in educational games. In
Proc. User Modeling Workshop on Assessing and
Adapting to User Attitudes and Effect: Why,
When, and How?
L. Devillers, L. Lamel, and I. Vasilescu. 2003.
Emotion detection in task-oriented spoken di-
alogs. In Proc. IEEE International Conference
on Multimedia &amp; Expo (ICME).
K. Forbes-Riley and D. Litman. 2004. Predict-
ing emotion in spoken dialogue from multi-
ple knowledge sources. In Proc. Human Lan-
guage Technology Conf. of the North American
Chap. of the Assoc. for Computational Linguis-
tics (HLT/NAACL).
A. Graesser, K. VanLehn, C. Rose, P. Jordan, and
D. Harter. 2002. Intelligent tutoring systems
with conversational dialogue. AI Magazine.
P. W. Jordan, M. Makatchev, and K. VanLehn.
2004. Combining competing language under-
standing approaches in an intelligent tutoring sys-
tem. In Proc. Intelligent Tutoring Systems.
B. Kort, R. Reilly, and R. W. Picard. 2001. An af-
fective model of interplay between emotions and
learning: Reengineering educational pedagogy -
building a learning companion. In International
Conf. on Advanced Learning Technologies.
C.M. Lee, S. Narayanan, and R. Pieraccini. 2001.
Recognition of negative emotions from the
speech signal. In Proc. IEEE Automatic Speech
Recognition and Understanding Workshop.
C.M. Lee, S. Narayanan, and R. Pieraccini. 2002.
Combining acoustic and language information
for emotion recognition. In International Conf.
on Spoken Language Processing (ICSLP).
D. Litman and K. Forbes-Riley. 2004. Annotating
student emotional states in spoken tutoring dia-
logues. In Proc. 5th SIGdial Workshop on Dis-
course and Dialogue.
D. Litman and K. Forbes. 2003. Recognizing emo-
tion from student speech in tutoring dialogues.
In Proc. IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU).
D. Litman and S. Silliman. 2004. ITSPOKE:
An intelligent tutoring spoken dialogue sys-
tem. In Companion Proc. of the Human Lan-
guage Technology Conf. of the North American
Chap. of the Assoc. for Computational Linguis-
tics (HLT/NAACL).
D. J. Litman, C. P. Rose, K. Forbes-Riley, K. Van-
Lehn, D. Bhembe, and S. Silliman. 2004. Spo-
ken versus typed human and computer dialogue
tutoring. In Proc. Intelligent Tutoring Systems.
B. Maeireizo-Tokeshi, D. Litman, and R. Hwa.
2004. Co-training for predicting emotions with
spoken dialogue data. In Companion Proc. As-
soc. for Computational Linguistics (ACL).
S. Narayanan. 2002. Towards modeling user be-
havior in human-machine interaction: Effect of
errors and emotions. In Proc. ISLE Workshop on
Dialogue Tagging for Multi-modal Human Com-
puter Interaction.
P-Y. Oudeyer. 2002. The production and recog-
nition of emotions in speech: Features and Al-
gorithms. International Journal of Human Com-
puter Studies, 59(1-2):157–183.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice
signatures. In Proc. IEEE Automatic Speech
Recognition and Understanding Workshop.
K. VanLehn, P. W. Jordan, C. P. Ros´e, D. Bhembe,
M. B¨ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler,
R. Srivastava, and R. Wilson. 2002. The archi-
tecture of Why2-Atlas: A coach for qualitative
physics essay writing. In Proc. Intelligent Tutor-
ing Systems.
I. H. Witten and E. Frank. 1999. Data Min-
ing: Practical Machine Learning Tools and Tech-
niques with Java implementations.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.809757">
<title confidence="0.999978">Predicting Student Emotions in Computer-Human Tutoring Dialogues</title>
<author confidence="0.999702">Diane J Litman Kate Forbes-Riley</author>
<affiliation confidence="0.9998365">University of Pittsburgh University of Pittsburgh Department of Computer Science Learning Research and Development Center</affiliation>
<address confidence="0.935337">Learning Research and Development Center Pittsburgh PA, 15260, USA</address>
<email confidence="0.9382065">PA,15260,USAforbesk@pitt.edulitman@cs.pitt.edu</email>
<abstract confidence="0.9978502">We examine the utility of speech and lexical features for predicting student emotions in computerhuman spoken tutoring dialogues. We first annotate student turns for negative, neutral, positive and mixed emotions. We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech. We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions. Our best results yield a 19-36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human-human tutoring dialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>B Kort</author>
<author>R Reilly</author>
<author>J Mostow</author>
<author>R Picard</author>
</authors>
<title>Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens.</title>
<date>2002</date>
<booktitle>In Proc. Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="1798" citStr="Aist et al., 2002" startWordPosition="259" endWordPosition="262">uman spoken tutoring dialogues. Intelligent tutoring dialogue systems have become more prevalent in recent years (Aleven and Rose, 2003), as one method of improving the performance gap between computer and human tutors; recent experiments with such systems (e.g., (Graesser et al., 2002)) are starting to yield promising empirical results. Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems, independently of whether or not the tutor is dialogue-based (Conati et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For example, (Aist et al., 2002) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence. Our long-term goal is to merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typ</context>
</contexts>
<marker>Aist, Kort, Reilly, Mostow, Picard, 2002</marker>
<rawString>G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Picard. 2002. Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>C P Rose</author>
<author>editors</author>
</authors>
<date>2003</date>
<booktitle>Proc. AI in Education Workshop on Tutorial Dialogue Systems: With a View toward the Classroom.</booktitle>
<marker>Aleven, Rose, editors, 2003</marker>
<rawString>V. Aleven and C. P. Rose, editors. 2003. Proc. AI in Education Workshop on Tutorial Dialogue Systems: With a View toward the Classroom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in humancomputer dialog.</title>
<date>2002</date>
<booktitle>In Proc. International Conf. on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="2595" citStr="Ang et al., 2002" startWordPosition="374" endWordPosition="377">affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of computerhuman tutoring di</context>
<context position="11547" citStr="Ang et al., 2002" startWordPosition="1774" endWordPosition="1777">eed on 89 negatives in Table 1. In the NnN analysis, the two annotators agreed on the annotations of 259/333 turns achieving 77.8% agreement, with Kappa = 0.5. In the EnE analysis, the two annotators agreed on the annotations of 220/333 turns achieving 66.1% agreement, with Kappa = 0.3. In the NPN analysis, the two annotators agreed on the annotations of 202/333 turns achieving 60.7% agreement, with Kappa = 0.4. This inter-annotator agreement is on par with that of prior studies of emotion annotation in naturally occurring computer-human dialogues (e.g., agreement of 71% and Kappa of 0.47 in (Ang et al., 2002), Kappa of 0.45 and 0.48 in (Narayanan, 2002), and Kappa ranging between 0.32 and 0.42 in (Shafran et al., 2003)). A number of researchers have accommodated for this low agreement by exploring ways of achieving consensus between disagreed annotations, to yield 100% agreement (e.g (Ang et al., 2002; Devillers et al., 2003)). As in (Ang et al., 2002), we will experiment below with predicting emotions using both our agreed data and consensuslabeled data. negative non-negative negative 89 36 non-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emo</context>
<context position="14090" citStr="Ang et al., 2002" startWordPosition="2178" endWordPosition="2181">ration, duration of pause prior to turn, speaking rate Lexical Features human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features: subject, gender, problem Figure 2: Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs. Speaking Rate is automatically calculated as #syllables per second in the turn. While acoustic-prosodic features address how something is said, lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2. Prior studies (O</context>
<context position="21485" citStr="Ang et al., 2002" startWordPosition="3385" endWordPosition="3388">ase (e.g., for NnN, an example path through the learned “lex” decision tree says predict negative if the utterance contains the word will but does not contain the word decrease). Understanding this result is an area for future research. Within the “+id” sets, we see that “lex” and “asr” perform the same in the NnN and NPN classifications; in EnE “lex+id” significantly outperforms “asr+id”. The utility of the “lex” features compared to “asr” also increases when combined with the “sp” features (with and without identifiers), for both NnN and NPN. Moreover, based on results in (Lee et al., 2002; Ang et al., 2002; Forbes-Riley and Litman, 2004), we hypothesized that combining speech and lexical features would result in better performance than either feature set alone. We instead found that the relative performance of these sets depends both on the emotion classification being predicted and the presence or absence of “id” features. Although consistently with prior research we find that the combined feature sets usually outperform the speech-only feature sets, the combined feature sets frequently perform worse than the lexical-only feature sets. However, we will see in Section 6 that combining knowledge</context>
<context position="22962" citStr="Ang et al., 2002" startWordPosition="3613" endWordPosition="3616">o relative improvement in error reduction over the baseline (MAJ) error5, after excluding all the feature sets containing “lex” features. In this way we give a better estimate of the best performance our system could accomplish, given the features it can currently access from among those discussed. These bestperforming feature sets yield relative improvements over their majority baseline errors ranging from 19- 36%. Moreover, although the NPN classification yields the lowest raw accuracies, it yields the highest relative improvement over its baseline. 5.3 Predicting Consensus Turns Following (Ang et al., 2002; Devillers et al., 2003), we also explored consensus labeling, both with the goal of increasing our usable data set for prediction, and to include the more difficult annotation cases. For our consensus labeling, the original annotators revisited each originally disagreed case, and through discussion, sought a consensus label. Due to consensus labeling, agreement rose across all three emotion classifications to 100%. Tables 7- 9 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets. Feat. Set -id SE +id SE sp 59.10 0.57 64.20 0.52 l</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in humancomputer dialog. In Proc. International Conf. on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>K Fischer</author>
<author>R Huber</author>
<author>J Spilker</author>
<author>E N¨oth</author>
</authors>
<title>Desperately seeking emotions: Actors, wizards, and human beings.</title>
<date>2000</date>
<booktitle>In Proc. ISCA Workshop on Speech and Emotion.</booktitle>
<marker>Batliner, Fischer, Huber, Spilker, N¨oth, 2000</marker>
<rawString>A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. N¨oth. 2000. Desperately seeking emotions: Actors, wizards, and human beings. In Proc. ISCA Workshop on Speech and Emotion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>K Fischer</author>
<author>R Huber</author>
<author>J Spilker</author>
<author>E Noth</author>
</authors>
<title>How to find trouble in communication.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>40--117</pages>
<contexts>
<context position="2618" citStr="Batliner et al., 2003" startWordPosition="378" endWordPosition="381"> research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of computerhuman tutoring dialogues, which displays</context>
<context position="12663" citStr="Batliner et al., 2003" startWordPosition="1959" endWordPosition="1962">on-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2: EnE Analysis Confusion Matrix negative neutral positive negative 89 30 6 neutral 32 94 38 positive 6 19 19 Table 3: NPN Analysis Confusion Matrix 4 Extracting Features from Turns For each of the 333 student turns described above, we next extracted the set of features itemized in Figure 2, for use in the machine learning experiments described in Section 5. Motivated by previous studies of emotion prediction in spontaneous dialogues (Ang et al., 2002; Lee et al., 2001; Batliner et al., 2003), our acousticprosodic features represent knowledge of pitch, energy, duration, tempo and pausing. We further restrict our features to those that can be computed automatically and in real-time, since our goal is to use such features to trigger online adaptation in ITSPOKE based on predicted student emotions. F0 and RMS values, representing measures of pitch and loudness, respectively, are computed using Entropic Research Laboratory’s pitch tracker, get f0, with no post-correction. Amount of Silence is approximated as the proportion of zero f0 frames for the turn. Turn Duration and Prior Pause </context>
<context position="14113" citStr="Batliner et al., 2003" startWordPosition="2182" endWordPosition="2185">f pause prior to turn, speaking rate Lexical Features human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features: subject, gender, problem Figure 2: Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs. Speaking Rate is automatically calculated as #syllables per second in the turn. While acoustic-prosodic features address how something is said, lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2. Prior studies (Oudeyer, 2002; Lee et al</context>
</contexts>
<marker>Batliner, Fischer, Huber, Spilker, Noth, 2003</marker>
<rawString>A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth. 2003. How to find trouble in communication. Speech Communication, 40:117–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bhatt</author>
<author>M Evens</author>
<author>S Argamon</author>
</authors>
<title>Hedged responses and expressions of affect in human/human and human/computer tutorial interactions.</title>
<date>2004</date>
<booktitle>In Proc. Cognitive Science.</booktitle>
<contexts>
<context position="1764" citStr="Bhatt et al., 2004" startWordPosition="253" endWordPosition="256">al states in a corpus of computer-human spoken tutoring dialogues. Intelligent tutoring dialogue systems have become more prevalent in recent years (Aleven and Rose, 2003), as one method of improving the performance gap between computer and human tutors; recent experiments with such systems (e.g., (Graesser et al., 2002)) are starting to yield promising empirical results. Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems, independently of whether or not the tutor is dialogue-based (Conati et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For example, (Aist et al., 2002) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence. Our long-term goal is to merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) c</context>
</contexts>
<marker>Bhatt, Evens, Argamon, 2004</marker>
<rawString>K. Bhatt, M. Evens, and S. Argamon. 2004. Hedged responses and expressions of affect in human/human and human/computer tutorial interactions. In Proc. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Conati</author>
<author>R Chabbal</author>
<author>H Maclaren</author>
</authors>
<title>A study on using biometric sensors for monitoring user emotions in educational games.</title>
<date>2003</date>
<booktitle>In Proc. User Modeling Workshop on Assessing and Adapting to User Attitudes and Effect: Why,</booktitle>
<location>When, and How?</location>
<contexts>
<context position="1724" citStr="Conati et al., 2003" startWordPosition="245" endWordPosition="248">automatically predicting student emotional states in a corpus of computer-human spoken tutoring dialogues. Intelligent tutoring dialogue systems have become more prevalent in recent years (Aleven and Rose, 2003), as one method of improving the performance gap between computer and human tutors; recent experiments with such systems (e.g., (Graesser et al., 2002)) are starting to yield promising empirical results. Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems, independently of whether or not the tutor is dialogue-based (Conati et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For example, (Aist et al., 2002) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence. Our long-term goal is to merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. no</context>
</contexts>
<marker>Conati, Chabbal, Maclaren, 2003</marker>
<rawString>C. Conati, R. Chabbal, and H. Maclaren. 2003. A study on using biometric sensors for monitoring user emotions in educational games. In Proc. User Modeling Workshop on Assessing and Adapting to User Attitudes and Effect: Why, When, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>L Lamel</author>
<author>I Vasilescu</author>
</authors>
<title>Emotion detection in task-oriented spoken dialogs.</title>
<date>2003</date>
<booktitle>In Proc. IEEE International Conference on Multimedia &amp; Expo (ICME).</booktitle>
<contexts>
<context position="11870" citStr="Devillers et al., 2003" startWordPosition="1828" endWordPosition="1831">annotators agreed on the annotations of 202/333 turns achieving 60.7% agreement, with Kappa = 0.4. This inter-annotator agreement is on par with that of prior studies of emotion annotation in naturally occurring computer-human dialogues (e.g., agreement of 71% and Kappa of 0.47 in (Ang et al., 2002), Kappa of 0.45 and 0.48 in (Narayanan, 2002), and Kappa ranging between 0.32 and 0.42 in (Shafran et al., 2003)). A number of researchers have accommodated for this low agreement by exploring ways of achieving consensus between disagreed annotations, to yield 100% agreement (e.g (Ang et al., 2002; Devillers et al., 2003)). As in (Ang et al., 2002), we will experiment below with predicting emotions using both our agreed data and consensuslabeled data. negative non-negative negative 89 36 non-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2: EnE Analysis Confusion Matrix negative neutral positive negative 89 30 6 neutral 32 94 38 positive 6 19 19 Table 3: NPN Analysis Confusion Matrix 4 Extracting Features from Turns For each of the 333 student turns described above, we next extracted the set of features itemized in Figure 2, for use in </context>
<context position="14137" citStr="Devillers et al., 2003" startWordPosition="2186" endWordPosition="2190">speaking rate Lexical Features human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features: subject, gender, problem Figure 2: Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs. Speaking Rate is automatically calculated as #syllables per second in the turn. While acoustic-prosodic features address how something is said, lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2. Prior studies (Oudeyer, 2002; Lee et al., 2002) have shown that</context>
<context position="22987" citStr="Devillers et al., 2003" startWordPosition="3617" endWordPosition="3620">ment in error reduction over the baseline (MAJ) error5, after excluding all the feature sets containing “lex” features. In this way we give a better estimate of the best performance our system could accomplish, given the features it can currently access from among those discussed. These bestperforming feature sets yield relative improvements over their majority baseline errors ranging from 19- 36%. Moreover, although the NPN classification yields the lowest raw accuracies, it yields the highest relative improvement over its baseline. 5.3 Predicting Consensus Turns Following (Ang et al., 2002; Devillers et al., 2003), we also explored consensus labeling, both with the goal of increasing our usable data set for prediction, and to include the more difficult annotation cases. For our consensus labeling, the original annotators revisited each originally disagreed case, and through discussion, sought a consensus label. Due to consensus labeling, agreement rose across all three emotion classifications to 100%. Tables 7- 9 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets. Feat. Set -id SE +id SE sp 59.10 0.57 64.20 0.52 lex 63.70 0.47 68.64 0.41 </context>
</contexts>
<marker>Devillers, Lamel, Vasilescu, 2003</marker>
<rawString>L. Devillers, L. Lamel, and I. Vasilescu. 2003. Emotion detection in task-oriented spoken dialogs. In Proc. IEEE International Conference on Multimedia &amp; Expo (ICME).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Predicting emotion in spoken dialogue from multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In Proc. Human Language Technology Conf. of the North American Chap. of the Assoc. for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="21517" citStr="Forbes-Riley and Litman, 2004" startWordPosition="3389" endWordPosition="3392">, an example path through the learned “lex” decision tree says predict negative if the utterance contains the word will but does not contain the word decrease). Understanding this result is an area for future research. Within the “+id” sets, we see that “lex” and “asr” perform the same in the NnN and NPN classifications; in EnE “lex+id” significantly outperforms “asr+id”. The utility of the “lex” features compared to “asr” also increases when combined with the “sp” features (with and without identifiers), for both NnN and NPN. Moreover, based on results in (Lee et al., 2002; Ang et al., 2002; Forbes-Riley and Litman, 2004), we hypothesized that combining speech and lexical features would result in better performance than either feature set alone. We instead found that the relative performance of these sets depends both on the emotion classification being predicted and the presence or absence of “id” features. Although consistently with prior research we find that the combined feature sets usually outperform the speech-only feature sets, the combined feature sets frequently perform worse than the lexical-only feature sets. However, we will see in Section 6 that combining knowledge sources does improve prediction</context>
<context position="26280" citStr="Forbes-Riley and Litman, 2004" startWordPosition="4156" endWordPosition="4159">tion compared to the majority class prediction, which is a lower error reduction than seen for agreed data. Moreover, the NPN classification yields the lowest accuracies and the lowest improvements over its baseline. 6 Comparison with Human Tutoring While building ITSPOKE, we collected a corresponding corpus of spoken human tutoring dialogues, using the same experimental methodology as for our computer tutoring corpus (e.g. same subject pool, physics problems, web and audio interface, etc); the only difference between the two corpora is whether the tutor is human or computer. As discussed in (Forbes-Riley and Litman, 2004), two annotators had previously labeled 453 turns in this corpus with the emotion annotation scheme discussed in Section 3, and performed a preliminary set of machine learning experiments (different from those reported above). Here, we perform the exper6The majority class for EnE Consensus is non-emotional; all others are unchanged. NnN EnE NPN FS -id SE +id SE -id SE +id SE -id SE +id SE sp 77.46 0.42 77.56 0.30 84.71 0.39 84.66 0.40 73.09 0.68 74.18 0.40 lex 80.74 0.42 80.60 0.34 88.86 0.26 86.23 0.34 78.56 0.45 77.18 0.43 sp+lex 81.37 0.33 80.79 0.41 87.74 0.36 88.31 0.29 79.06 0.38 78.03 0</context>
</contexts>
<marker>Forbes-Riley, Litman, 2004</marker>
<rawString>K. Forbes-Riley and D. Litman. 2004. Predicting emotion in spoken dialogue from multiple knowledge sources. In Proc. Human Language Technology Conf. of the North American Chap. of the Assoc. for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
<author>K VanLehn</author>
<author>C Rose</author>
<author>P Jordan</author>
<author>D Harter</author>
</authors>
<title>Intelligent tutoring systems with conversational dialogue.</title>
<date>2002</date>
<journal>AI Magazine.</journal>
<contexts>
<context position="1467" citStr="Graesser et al., 2002" startWordPosition="206" endWordPosition="209">notated student emotions. Our best results yield a 19-36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human-human tutoring dialogues. 1 Introduction This paper explores the feasibility of automatically predicting student emotional states in a corpus of computer-human spoken tutoring dialogues. Intelligent tutoring dialogue systems have become more prevalent in recent years (Aleven and Rose, 2003), as one method of improving the performance gap between computer and human tutors; recent experiments with such systems (e.g., (Graesser et al., 2002)) are starting to yield promising empirical results. Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems, independently of whether or not the tutor is dialogue-based (Conati et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For example, (Aist et al., 2002) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence. Our long-term goal is to merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue syste</context>
</contexts>
<marker>Graesser, VanLehn, Rose, Jordan, Harter, 2002</marker>
<rawString>A. Graesser, K. VanLehn, C. Rose, P. Jordan, and D. Harter. 2002. Intelligent tutoring systems with conversational dialogue. AI Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jordan</author>
<author>M Makatchev</author>
<author>K VanLehn</author>
</authors>
<title>Combining competing language understanding approaches in an intelligent tutoring system.</title>
<date>2004</date>
<booktitle>In Proc. Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="4986" citStr="Jordan et al., 2004" startWordPosition="731" endWordPosition="735">m (VanLehn et al., 2002). In ITSPOKE, a student first types an essay answering a qualitative physics problem. ITSPOKE then analyzes the essay and engages the student in spoken dialogue to correct misconceptions and to elicit complete explanations. First, the Why2-Atlas back-end parses the student essay into propositional representations, in order to find useful dialogue topics. It uses 3 different approaches (symbolic, statistical and hybrid) competitively to create a representation for each sentence, then resolves temporal and nominal anaphora and constructs proofs using abductive reasoning (Jordan et al., 2004). During the dialogue, student speech is digitized from microphone input and sent to the Sphinx2 recognizer, whose stochastic language models have a vocabulary of 1240 words and are trained with 7720 student utterances from evaluations of Why2-Atlas and from pilot studies of ITSPOKE. Sphinx2’s best “transcription” (recognition output) is then sent to the Why2-Atlas back-end for syntactic, semantic and dialogue analysis. Finally, the text response produced by Why2-Atlas is sent to the Cepstral text-to-speech system and played to the student. After the dialogue, the student revises the essay, th</context>
</contexts>
<marker>Jordan, Makatchev, VanLehn, 2004</marker>
<rawString>P. W. Jordan, M. Makatchev, and K. VanLehn. 2004. Combining competing language understanding approaches in an intelligent tutoring system. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kort</author>
<author>R Reilly</author>
<author>R W Picard</author>
</authors>
<title>An affective model of interplay between emotions and learning: Reengineering educational pedagogy -building a learning companion.</title>
<date>2001</date>
<booktitle>In International Conf. on Advanced Learning Technologies.</booktitle>
<contexts>
<context position="1743" citStr="Kort et al., 2001" startWordPosition="249" endWordPosition="252">ing student emotional states in a corpus of computer-human spoken tutoring dialogues. Intelligent tutoring dialogue systems have become more prevalent in recent years (Aleven and Rose, 2003), as one method of improving the performance gap between computer and human tutors; recent experiments with such systems (e.g., (Graesser et al., 2002)) are starting to yield promising empirical results. Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems, independently of whether or not the tutor is dialogue-based (Conati et al., 2003; Kort et al., 2001; Bhatt et al., 2004). For example, (Aist et al., 2002) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence. Our long-term goal is to merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negati</context>
</contexts>
<marker>Kort, Reilly, Picard, 2001</marker>
<rawString>B. Kort, R. Reilly, and R. W. Picard. 2001. An affective model of interplay between emotions and learning: Reengineering educational pedagogy -building a learning companion. In International Conf. on Advanced Learning Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
</authors>
<title>Recognition of negative emotions from the speech signal.</title>
<date>2001</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="2559" citStr="Lee et al., 2001" startWordPosition="366" endWordPosition="369">o merge these lines of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable</context>
<context position="10212" citStr="Lee et al., 2001" startWordPosition="1569" endWordPosition="1572">analyze the reliability of the scheme on our new computer-human data, we selected 15 transcribed dialogues from the corpus described in Section 2, yielding a dataset of 333 student turns, where approximately 30 turns came from each of 10 subjects. The 333 turns were separately annotated by two annotators following the emotion annotation scheme described above. We focus here on three analyses of this data, itemized below. While the first analysis provides the most fine-grained distinctions for triggering system adaptation, the second and third (simplified) analyses correspond to those used in (Lee et al., 2001) and (Batliner et al., 2000), respectively. These represent alternative potentially useful triggering mechanisms, and are worth exploring as they might be easier to annotate and/or predict. Negative, Neutral, Positive (NPN): mixeds are conflated with neutrals. Negative, Non-Negative (NnN): positives, mixeds, neutrals are conflated as nonnegatives. Emotional, Non-Emotional (EnE): negatives, positives, mixeds are conflated as Emotional; neutrals are Non-Emotional. Tables 1-3 provide a confusion matrix for each analysis summarizing inter-annotator agreement. The rows correspond to the labels assi</context>
<context position="12639" citStr="Lee et al., 2001" startWordPosition="1955" endWordPosition="1958">e negative 89 36 non-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2: EnE Analysis Confusion Matrix negative neutral positive negative 89 30 6 neutral 32 94 38 positive 6 19 19 Table 3: NPN Analysis Confusion Matrix 4 Extracting Features from Turns For each of the 333 student turns described above, we next extracted the set of features itemized in Figure 2, for use in the machine learning experiments described in Section 5. Motivated by previous studies of emotion prediction in spontaneous dialogues (Ang et al., 2002; Lee et al., 2001; Batliner et al., 2003), our acousticprosodic features represent knowledge of pitch, energy, duration, tempo and pausing. We further restrict our features to those that can be computed automatically and in real-time, since our goal is to use such features to trigger online adaptation in ITSPOKE based on predicted student emotions. F0 and RMS values, representing measures of pitch and loudness, respectively, are computed using Entropic Research Laboratory’s pitch tracker, get f0, with no post-correction. Amount of Silence is approximated as the proportion of zero f0 frames for the turn. Turn D</context>
<context position="16244" citStr="Lee et al., 2001" startWordPosition="2525" endWordPosition="2528">: human-transcribed lexical items asr: ITSPOKE recognized lexical items sp+lex: combined sp and lex features sp+asr: combined sp and asr features +id: each above set + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software (Witten and Frank, 1999) to automatically learn our emotion prediction models. In our humanhuman dialogue studies (Litman and Forbes, 2003), the use of boosted decision trees yielded the most robust performance across feature sets so we will continue their use here. 5.2 Predicting Agreed Turns As in (Shafran et al., 2003; Lee et al., 2001), our first study looks at the clearer cases of emotional turns, i.e. only those student turns where the two annotators agreed on an emotion label. Tables 4-6 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets (Figure 3), computed across 10 runs of 10-fold cross-validation.&apos; For comparison, the accuracy of a standard baseline algorithm (MAJ), which always predicts the majority class, is shown in each caption. For example, Table 4’s caption shows that for NnN, always predicting the majority class of non-negative yields an accuracy</context>
</contexts>
<marker>Lee, Narayanan, Pieraccini, 2001</marker>
<rawString>C.M. Lee, S. Narayanan, and R. Pieraccini. 2001. Recognition of negative emotions from the speech signal. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
</authors>
<title>Combining acoustic and language information for emotion recognition.</title>
<date>2002</date>
<booktitle>In International Conf. on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="2577" citStr="Lee et al., 2002" startWordPosition="370" endWordPosition="373">s of dialogue and affective tutoring research, by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of compute</context>
<context position="14072" citStr="Lee et al., 2002" startWordPosition="2174" endWordPosition="2177">e in turn, turn duration, duration of pause prior to turn, speaking rate Lexical Features human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features: subject, gender, problem Figure 2: Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs. Speaking Rate is automatically calculated as #syllables per second in the turn. While acoustic-prosodic features address how something is said, lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2</context>
<context position="18973" citStr="Lee et al., 2002" startWordPosition="2973" endWordPosition="2976"> Table 5: %Correct, EnE Agreed, MAJ (emotional) = 58.64% Feat. Set -id SE +id SE sp 55.49 1.01 62.03 0.91 lex 52.66 0.62 67.84 0.66 asr 57.95 0.67 65.70 0.50 sp+lex 62.08 0.56 63.52 0.48 sp+asr 61.22 1.20 62.23 0.86 Table 6: %Correct, NPN Agreed, MAJ (neutral) = 46.52% Further note that adding identifier features to the “-id” feature sets almost always improves performance, although this difference is not always significant4; across tables the “+id” feature sets outperform their “-id” counterparts across all feature sets and emotion classifications except one (NnN “asr”). Surprisingly, while (Lee et al., 2002) found it useful to develop separate gender-based emotion prediction models, in our experiment, gender is the only identifier that does not appear in any learned model. Also note that with the addition of identifier features, the speech-only feature sets (sp+id) now do outperform the majority class baselines for all three emotion classifications. 4For any feature set, the mean +/- 2*SE = the 95% confidence interval. If the confidence intervals for two feature sets are non-overlapping, then their mean accuracies are significantly different with 95% confidence. With respect to the relative utili</context>
<context position="21467" citStr="Lee et al., 2002" startWordPosition="3381" endWordPosition="3384">uitive in either case (e.g., for NnN, an example path through the learned “lex” decision tree says predict negative if the utterance contains the word will but does not contain the word decrease). Understanding this result is an area for future research. Within the “+id” sets, we see that “lex” and “asr” perform the same in the NnN and NPN classifications; in EnE “lex+id” significantly outperforms “asr+id”. The utility of the “lex” features compared to “asr” also increases when combined with the “sp” features (with and without identifiers), for both NnN and NPN. Moreover, based on results in (Lee et al., 2002; Ang et al., 2002; Forbes-Riley and Litman, 2004), we hypothesized that combining speech and lexical features would result in better performance than either feature set alone. We instead found that the relative performance of these sets depends both on the emotion classification being predicted and the presence or absence of “id” features. Although consistently with prior research we find that the combined feature sets usually outperform the speech-only feature sets, the combined feature sets frequently perform worse than the lexical-only feature sets. However, we will see in Section 6 that c</context>
</contexts>
<marker>Lee, Narayanan, Pieraccini, 2002</marker>
<rawString>C.M. Lee, S. Narayanan, and R. Pieraccini. 2002. Combining acoustic and language information for emotion recognition. In International Conf. on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Annotating student emotional states in spoken tutoring dialogues.</title>
<date>2004</date>
<booktitle>In Proc. 5th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2961" citStr="Litman and Forbes-Riley, 2004" startWordPosition="429" endWordPosition="432">onal, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of computerhuman tutoring dialogues, which displays many different characteristics, such as shorter utterances, little student initiative, and non-overlapping speech. We investigate whether we can annotate and predict student emotions as accurately and whether the relative utility of speech and lexical features as predictors is the same, especially when the output of the speech recognizer is</context>
<context position="7796" citStr="Litman and Forbes-Riley, 2004" startWordPosition="1196" endWordPosition="1199">e falls from a tree. What force(s) are acting on the apple as it falls? STUDENT : gravity (NEGATIVE) ITSPOKE : Yeah. Are there any other forces acting on the apple as it falls? STUDENT : no (NEGATIVE) ITSPOKE : Fine. The only force acting on the apple as it falls to the earth is the earth’s gravitational force. In which direction does gravitational force act? STUDENT : down (POSITIVE) ITSPOKE : Yep. Therefore, what is the magnitude of this gravitational force in the horizontal direction? STUDENT : in the direction of the airplane (NEUTRAL) Figure 1: Annotated Spoken Dialogue Excerpt said. In (Litman and Forbes-Riley, 2004), we discuss a scheme for manually annotating student turns in a human-human tutoring dialogue corpus for intuitively perceived emotions.2 These emotions are viewed along a linear scale, shown and defined as follows: negative neutral positive. Negative: a student turn that expresses emotions such as confused, bored, irritated. Evidence of a negative emotion can come from many knowledge sources such as lexical items (e.g., “I don’t know” in student in Figure 1), and/or acoustic-prosodic features (e.g., prior-turn pausing in student ). Positive: a student turn expressing emotions such as confide</context>
<context position="9508" citStr="Litman and Forbes-Riley, 2004" startWordPosition="1454" endWordPosition="1457">y 1We use the term “emotion” loosely to cover both affects and attitudes that can impact student learning. 2Weak and strong expressions of emotions are annotated. or unambiguously across speakers, our annotation manual does not associate particular cues with particular emotion labels. Instead, it contains examples of labeled dialogue excerpts (as in Figure 1, except on human-human data) with links to corresponding audio files. The cues mentioned in the discussion of Figure 1 above were elicited during post-annotation discussion of the emotions, and are presented here for expository use only. (Litman and Forbes-Riley, 2004) further details our annotation scheme and discusses how it builds on related work. To analyze the reliability of the scheme on our new computer-human data, we selected 15 transcribed dialogues from the corpus described in Section 2, yielding a dataset of 333 student turns, where approximately 30 turns came from each of 10 subjects. The 333 turns were separately annotated by two annotators following the emotion annotation scheme described above. We focus here on three analyses of this data, itemized below. While the first analysis provides the most fine-grained distinctions for triggering syst</context>
</contexts>
<marker>Litman, Forbes-Riley, 2004</marker>
<rawString>D. Litman and K. Forbes-Riley. 2004. Annotating student emotional states in spoken tutoring dialogues. In Proc. 5th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes</author>
</authors>
<title>Recognizing emotion from student speech in tutoring dialogues.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</booktitle>
<contexts>
<context position="16042" citStr="Litman and Forbes, 2003" startWordPosition="2490" endWordPosition="2493"> recognized lexical items (“sp+lex”, “sp+asr”). Finally, we compare each of these 5 feature sets with an identical set supplemented with our 3 identifier features (“+id”). sp: 12 acoustic-prosodic features lex: human-transcribed lexical items asr: ITSPOKE recognized lexical items sp+lex: combined sp and lex features sp+asr: combined sp and asr features +id: each above set + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software (Witten and Frank, 1999) to automatically learn our emotion prediction models. In our humanhuman dialogue studies (Litman and Forbes, 2003), the use of boosted decision trees yielded the most robust performance across feature sets so we will continue their use here. 5.2 Predicting Agreed Turns As in (Shafran et al., 2003; Lee et al., 2001), our first study looks at the clearer cases of emotional turns, i.e. only those student turns where the two annotators agreed on an emotion label. Tables 4-6 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets (Figure 3), computed across 10 runs of 10-fold cross-validation.&apos; For comparison, the accuracy of a standard baseline algor</context>
</contexts>
<marker>Litman, Forbes, 2003</marker>
<rawString>D. Litman and K. Forbes. 2003. Recognizing emotion from student speech in tutoring dialogues. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>S Silliman</author>
</authors>
<title>ITSPOKE: An intelligent tutoring spoken dialogue system.</title>
<date>2004</date>
<booktitle>In Companion Proc. of the Human Language Technology Conf. of the North American Chap. of the Assoc. for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="4265" citStr="Litman and Silliman, 2004" startWordPosition="622" endWordPosition="625">ls for predicting three different types of emotion classifications achieve accuracies of 66-73%, representing relative improvements of 19-36% over majority class baseline errors. Our computer-human results also show interesting differences compared with comparable analyses of human-human data. Our results provide an empirical basis for enhancing our spoken dialogue tutoring system to automatically predict and adapt to a student model that includes emotional states. 2 Computer-Human Dialogue Data Our data consists of student dialogues with ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) (Litman and Silliman, 2004), a spoken dialogue tutor built on top of the Why2-Atlas conceptual physics text-based tutoring system (VanLehn et al., 2002). In ITSPOKE, a student first types an essay answering a qualitative physics problem. ITSPOKE then analyzes the essay and engages the student in spoken dialogue to correct misconceptions and to elicit complete explanations. First, the Why2-Atlas back-end parses the student essay into propositional representations, in order to find useful dialogue topics. It uses 3 different approaches (symbolic, statistical and hybrid) competitively to create a representation for each se</context>
</contexts>
<marker>Litman, Silliman, 2004</marker>
<rawString>D. Litman and S. Silliman. 2004. ITSPOKE: An intelligent tutoring spoken dialogue system. In Companion Proc. of the Human Language Technology Conf. of the North American Chap. of the Assoc. for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>C P Rose</author>
<author>K Forbes-Riley</author>
<author>K VanLehn</author>
<author>D Bhembe</author>
<author>S Silliman</author>
</authors>
<title>Spoken versus typed human and computer dialogue tutoring.</title>
<date>2004</date>
<booktitle>In Proc. Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="5845" citStr="Litman et al., 2004" startWordPosition="865" endWordPosition="868">Why2-Atlas and from pilot studies of ITSPOKE. Sphinx2’s best “transcription” (recognition output) is then sent to the Why2-Atlas back-end for syntactic, semantic and dialogue analysis. Finally, the text response produced by Why2-Atlas is sent to the Cepstral text-to-speech system and played to the student. After the dialogue, the student revises the essay, thereby ending the tutoring or causing another round of tutoring/essay revision. Our corpus of dialogues with ITSPOKE was collected from November 2003 - April 2004, as part of an evaluation comparing ITSPOKE, Why2-Atlas, and human tutoring (Litman et al., 2004). Subjects are University of Pittsburgh students who have never taken college physics, and who are native English speakers. Subjects first read a small document of background physics material, then work through 5 problems (dialogues) with ITSPOKE. The corpus contains 100 dialogues (physics problems) from 20 subjects, with a total of 2445 student turns and 398 unique words. 15 dialogues have been annotated for emotion as described in Section 3. On average, our dialogues last 19.4 minutes and contain 25 student turns. While ITSPOKE’s word error rate on this corpus is 31.2%, semantic accuracy is </context>
</contexts>
<marker>Litman, Rose, Forbes-Riley, VanLehn, Bhembe, Silliman, 2004</marker>
<rawString>D. J. Litman, C. P. Rose, K. Forbes-Riley, K. VanLehn, D. Bhembe, and S. Silliman. 2004. Spoken versus typed human and computer dialogue tutoring. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Maeireizo-Tokeshi</author>
<author>D Litman</author>
<author>R Hwa</author>
</authors>
<title>Co-training for predicting emotions with spoken dialogue data.</title>
<date>2004</date>
<booktitle>In Companion Proc. Assoc. for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="30561" citStr="Maeireizo-Tokeshi et al., 2004" startWordPosition="4822" endWordPosition="4825">ures. While adding identifier features typically also improves performance, combining lexical and speech features does not. Our analyses also suggest that prediction in consensus-labeled turns is harder than in agreed turns, and that prediction in our computerhuman corpus is harder and based on somewhat different features than in our human-human corpus. Our continuing work extends this methodology with the goal of enhancing ITSPOKE to predict and adapt to student emotions. We continue to manually annotate ITSPOKE data, and are exploring partial automation via semi-supervised machine learning (Maeireizo-Tokeshi et al., 2004). Further manual annotation might also improve reliability, as understanding systematic disagreements can lead to coding manual revisions. We are also expanding our feature set to include features suggested in prior dialogue research, tutoring-dependent features (e.g., pedagogical goal), and other features available in our logs (e.g., semantic analysis). Finally, we will explore how the recognized emotions can be used to improve system performance. First, we will label human tutor adaptations to emotional student turns in our human tutoring corpus; this labeling will be used to formulate adapt</context>
</contexts>
<marker>Maeireizo-Tokeshi, Litman, Hwa, 2004</marker>
<rawString>B. Maeireizo-Tokeshi, D. Litman, and R. Hwa. 2004. Co-training for predicting emotions with spoken dialogue data. In Companion Proc. Assoc. for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
</authors>
<title>Towards modeling user behavior in human-machine interaction: Effect of errors and emotions.</title>
<date>2002</date>
<booktitle>In Proc. ISLE Workshop on Dialogue Tagging</booktitle>
<institution>for Multi-modal Human Computer Interaction.</institution>
<contexts>
<context position="11592" citStr="Narayanan, 2002" startWordPosition="1784" endWordPosition="1785">lysis, the two annotators agreed on the annotations of 259/333 turns achieving 77.8% agreement, with Kappa = 0.5. In the EnE analysis, the two annotators agreed on the annotations of 220/333 turns achieving 66.1% agreement, with Kappa = 0.3. In the NPN analysis, the two annotators agreed on the annotations of 202/333 turns achieving 60.7% agreement, with Kappa = 0.4. This inter-annotator agreement is on par with that of prior studies of emotion annotation in naturally occurring computer-human dialogues (e.g., agreement of 71% and Kappa of 0.47 in (Ang et al., 2002), Kappa of 0.45 and 0.48 in (Narayanan, 2002), and Kappa ranging between 0.32 and 0.42 in (Shafran et al., 2003)). A number of researchers have accommodated for this low agreement by exploring ways of achieving consensus between disagreed annotations, to yield 100% agreement (e.g (Ang et al., 2002; Devillers et al., 2003)). As in (Ang et al., 2002), we will experiment below with predicting emotions using both our agreed data and consensuslabeled data. negative non-negative negative 89 36 non-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2: EnE Analysis Confusion </context>
</contexts>
<marker>Narayanan, 2002</marker>
<rawString>S. Narayanan. 2002. Towards modeling user behavior in human-machine interaction: Effect of errors and emotions. In Proc. ISLE Workshop on Dialogue Tagging for Multi-modal Human Computer Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-Y Oudeyer</author>
</authors>
<title>The production and recognition of emotions in speech: Features and Algorithms.</title>
<date>2002</date>
<journal>International Journal of Human Computer Studies,</journal>
<pages>59--1</pages>
<contexts>
<context position="14702" citStr="Oudeyer, 2002" startWordPosition="2285" endWordPosition="2286">2; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2. Prior studies (Oudeyer, 2002; Lee et al., 2002) have shown that “subject” and “gender” can play an important role in emotion recognition. “Subject” and “problem” are particularly important in our tutoring domain because students will use our system repeatedly, and problems are repeated across students. 5 Predicting Student Emotions 5.1 Feature Sets and Method We next created the 10 feature sets in Figure 3, to study the effects that various feature combinations had on predicting emotion. We compare an acoustic-prosodic feature set (“sp”), a humantranscribed lexical items feature set (“lex”) and an ITSPOKE-recognized lexi</context>
</contexts>
<marker>Oudeyer, 2002</marker>
<rawString>P-Y. Oudeyer. 2002. The production and recognition of emotions in speech: Features and Algorithms. International Journal of Human Computer Studies, 59(1-2):157–183.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Shafran</author>
<author>M Riley</author>
<author>M Mohri</author>
</authors>
<title>Voice signatures.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition</booktitle>
<contexts>
<context position="2641" citStr="Shafran et al., 2003" startWordPosition="382" endWordPosition="385"> our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions, and to investigate whether this improves learning and other measures of performance. Previous spoken dialogue research has shown that predictive models of emotion distinctions (e.g., emotional vs. non-emotional, negative vs. nonnegative) can be developed using features typically available to a spoken dialogue system in real-time (e.g, acoustic-prosodic, lexical, dialogue, and/or contextual) (Batliner et al., 2000; Lee et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Shafran et al., 2003). In prior work we built on and generalized such research, by defining a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). Like the non-tutoring studies, our results showed that combining feature types yielded the highest predictive accuracy. In this paper we investigate the application of our approach to a comparable corpus of computerhuman tutoring dialogues, which displays many different charact</context>
<context position="11659" citStr="Shafran et al., 2003" startWordPosition="1794" endWordPosition="1797"> turns achieving 77.8% agreement, with Kappa = 0.5. In the EnE analysis, the two annotators agreed on the annotations of 220/333 turns achieving 66.1% agreement, with Kappa = 0.3. In the NPN analysis, the two annotators agreed on the annotations of 202/333 turns achieving 60.7% agreement, with Kappa = 0.4. This inter-annotator agreement is on par with that of prior studies of emotion annotation in naturally occurring computer-human dialogues (e.g., agreement of 71% and Kappa of 0.47 in (Ang et al., 2002), Kappa of 0.45 and 0.48 in (Narayanan, 2002), and Kappa ranging between 0.32 and 0.42 in (Shafran et al., 2003)). A number of researchers have accommodated for this low agreement by exploring ways of achieving consensus between disagreed annotations, to yield 100% agreement (e.g (Ang et al., 2002; Devillers et al., 2003)). As in (Ang et al., 2002), we will experiment below with predicting emotions using both our agreed data and consensuslabeled data. negative non-negative negative 89 36 non-negative 38 170 Table 1: NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2: EnE Analysis Confusion Matrix negative neutral positive negative 89 30 6 neutral 32 94 38 </context>
<context position="14160" citStr="Shafran et al., 2003" startWordPosition="2191" endWordPosition="2194">atures human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features: subject, gender, problem Figure 2: Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs. Speaking Rate is automatically calculated as #syllables per second in the turn. While acoustic-prosodic features address how something is said, lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues (Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). This feature represents the “ideal” performance of ITSPOKE with respect to speech recognition. The second set represents ITSPOKE’s actual best speech recognition hypothesis of what is said in each student turn, again as a word occurrence vector. Finally, we recorded for each turn the 3 “identifier” features shown last in Figure 2. Prior studies (Oudeyer, 2002; Lee et al., 2002) have shown that “subject” and “gender”</context>
<context position="16225" citStr="Shafran et al., 2003" startWordPosition="2521" endWordPosition="2524">-prosodic features lex: human-transcribed lexical items asr: ITSPOKE recognized lexical items sp+lex: combined sp and lex features sp+asr: combined sp and asr features +id: each above set + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software (Witten and Frank, 1999) to automatically learn our emotion prediction models. In our humanhuman dialogue studies (Litman and Forbes, 2003), the use of boosted decision trees yielded the most robust performance across feature sets so we will continue their use here. 5.2 Predicting Agreed Turns As in (Shafran et al., 2003; Lee et al., 2001), our first study looks at the clearer cases of emotional turns, i.e. only those student turns where the two annotators agreed on an emotion label. Tables 4-6 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets (Figure 3), computed across 10 runs of 10-fold cross-validation.&apos; For comparison, the accuracy of a standard baseline algorithm (MAJ), which always predicts the majority class, is shown in each caption. For example, Table 4’s caption shows that for NnN, always predicting the majority class of non-negative</context>
<context position="19964" citStr="Shafran et al., 2003" startWordPosition="3125" endWordPosition="3128"> the mean +/- 2*SE = the 95% confidence interval. If the confidence intervals for two feature sets are non-overlapping, then their mean accuracies are significantly different with 95% confidence. With respect to the relative utility of lexical versus acoustic-prosodic features, without identifier features, using only lexical features (“lex” or “asr”) almost always produces statistically better performance than using only speech features (“sp”); the only exception is NPN “lex”, which performs statistically the same as NPN “sp”. This is consistent with others’ findings, e.g., (Lee et al., 2002; Shafran et al., 2003). When identifier features are added to both, the lexical sets don’t always significantly outperform the speech set; only in NPN and EnE “lex+id” is this the case. For NnN, just as using “sp+id” rather than “sp-id” improved performance when compared to the majority baseline, the addition of the identifier features also improves the utility of the speech features when compared to the lexical features. Interestingly, although we hypothesized that the “lex” feature sets would present an upper bound on the performance of the “asr” sets, because the human transcription is more accurate than the spe</context>
</contexts>
<marker>Shafran, Riley, Mohri, 2003</marker>
<rawString>I. Shafran, M. Riley, and M. Mohri. 2003. Voice signatures. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop. K. VanLehn, P. W. Jordan, C. P. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java implementations.</booktitle>
<contexts>
<context position="15927" citStr="Witten and Frank, 1999" startWordPosition="2473" endWordPosition="2476">l items feature set (“asr”). We further compare feature sets combining acoustic-prosodic and either transcribed or recognized lexical items (“sp+lex”, “sp+asr”). Finally, we compare each of these 5 feature sets with an identical set supplemented with our 3 identifier features (“+id”). sp: 12 acoustic-prosodic features lex: human-transcribed lexical items asr: ITSPOKE recognized lexical items sp+lex: combined sp and lex features sp+asr: combined sp and asr features +id: each above set + 3 identifier features Figure 3: Feature Sets for Machine Learning We use the Weka machine learning software (Witten and Frank, 1999) to automatically learn our emotion prediction models. In our humanhuman dialogue studies (Litman and Forbes, 2003), the use of boosted decision trees yielded the most robust performance across feature sets so we will continue their use here. 5.2 Predicting Agreed Turns As in (Shafran et al., 2003; Lee et al., 2001), our first study looks at the clearer cases of emotional turns, i.e. only those student turns where the two annotators agreed on an emotion label. Tables 4-6 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets (Figure </context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>I. H. Witten and E. Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java implementations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>