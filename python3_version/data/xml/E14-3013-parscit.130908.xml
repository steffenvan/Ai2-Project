<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994115">
Generating artificial errors for grammatical error correction
</title>
<author confidence="0.984043">
Mariano Felice
</author>
<affiliation confidence="0.9865895">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.624191">
United Kingdom
</address>
<email confidence="0.979925">
mf501@cam.ac.uk
</email>
<author confidence="0.994734">
Zheng Yuan
</author>
<affiliation confidence="0.9908695">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.628234">
United Kingdom
</address>
<email confidence="0.987618">
zy249@cam.ac.uk
</email>
<sectionHeader confidence="0.993632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992925">
This paper explores the generation of ar-
tificial errors for correcting grammatical
mistakes made by learners of English as
a second language. Artificial errors are in-
jected into a set of error-free sentences in a
probabilistic manner using statistics from
a corpus. Unlike previous approaches, we
use linguistic information to derive error
generation probabilities and build corpora
to correct several error types, including
open-class errors. In addition, we also
analyse the variables involved in the selec-
tion of candidate sentences. Experiments
using the NUCLE corpus from the CoNLL
2013 shared task reveal that: 1) training
on artificially created errors improves pre-
cision at the expense of recall and 2) dif-
ferent types of linguistic information are
better suited for correcting different error
types.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798595238095">
Building error correction systems using machine
learning techniques can require a considerable
amount of annotated data which is difficult to ob-
tain. Available error-annotated corpora are often
focused on particular groups of people (e.g. non-
native students), error types (e.g. spelling, syn-
tax), genres (e.g. university essays, letters) or top-
ics so it is not clear how representative they are
or how well systems based on them will gener-
alise. On the other hand, building new corpora is
not always a viable solution since error annotation
is expensive. As a result, researchers have tried
to overcome these limitations either by compiling
corpora automatically from the web (Mizumoto et
al., 2011; Tajiri et al., 2012; Cahill et al., 2013) or
using artificial corpora which are cheaper to pro-
duce and can be tailored to their needs.
Artificial error generation allows researchers to
create very large error-annotated corpora with lit-
tle effort and control variables such as topic and
error types. Errors can be injected into candidate
texts using a deterministic approach (e.g. fixed
rules) or probabilities derived from manually an-
notated samples in order to mimic real data.
Although artificial errors have been used in pre-
vious work, we present a new approach based on
linguistic information and evaluate it using the test
data provided for the CoNLL 2013 shared task on
grammatical error correction (Ng et al., 2013).
Our work makes the following contributions.
First, we are the first to use linguistic informa-
tion (such as part-of-speech (PoS) information or
semantic classes) to characterise contexts of natu-
rally occurring errors and replicate them in error-
free text. Second, we apply our technique to a
larger number of error types than any other pre-
vious approach, including open-class errors. The
resulting datasets are used to train error correction
systems aimed at learners of English as a second
language (ESL). Finally, we provide a detailed de-
scription of the variables that affect artificial error
generation.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999508642857143">
The use of artificial data to train error correction
systems has been explored by other researchers us-
ing a variety of techniques.
Izumi et al. (2003), for example, use artificial
errors to target article mistakes made by Japanese
learners of English. A corpus is created by replac-
ing a, an, the or the zero article by a different ar-
ticle chosen at random in more than 7,500 correct
sentences and used to train a maximum entropy
model. Results show an improvement for omis-
sion errors but no change for replacement errors.
Brockett et al. (2006) describe the use of a sta-
tistical machine translation (SMT) system for cor-
recting a set of 14 countable/uncountable nouns
</bodyText>
<page confidence="0.984208">
116
</page>
<note confidence="0.9951775">
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–126,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99993425">
which are often confusing for ESL learners. Their
training corpus consists of a large number of sen-
tences extracted from news articles which were de-
liberately modified to include typical countability
errors based on evidence from a Chinese learner
corpus. Their approach to artificial error injec-
tion is deterministic, using hand-coded rules to
change quantifiers (much → many), generate plu-
rals (advice → advices) or insert unnecessary de-
terminers. Experiments show their system was
generally able to beat the standard Microsoft Word
2003 grammar checker, although it produced a rel-
atively higher rate of erroneous corrections.
SMT systems are also used by Ehsan and Faili
(2013) to correct grammatical errors and context-
sensitive spelling mistakes in English and Farsi.
Training corpora are obtained by injecting arti-
ficial errors into well-formed treebank sentences
using predefined error templates. Whenever an
original sentence from the corpus matches one of
these templates, a pair of correct and incorrect sen-
tences is generated. This process is repeated mul-
tiple times if a single sentence matches more than
one error template, thereby generating many pairs
for the same original sentence. A comparison be-
tween the proposed systems and rule-based gram-
mar checkers show they are complementary, with
a hybrid system achieving the best performance.
</bodyText>
<subsectionHeader confidence="0.976721">
2.1 Probabilistic approaches
</subsectionHeader>
<bodyText confidence="0.999798388888889">
A few researchers have explored probabilistic
methods in an attempt to mimic real data more ac-
curately. Foster and Andersen (2009), for exam-
ple, describe a tool for generating artificial errors
based on statistics from other corpora, such as the
Cambridge Learner Corpus (CLC).1 Their experi-
ments show a drop in accuracy when artificial sen-
tences are used as a replacement for real incorrect
sentences, suggesting that they may not be as use-
ful as genuine text. Their report also includes an
extensive summary of previous work in the area.
Rozovskaya and Roth propose more sophis-
ticated probabilistic methods to generate artifi-
cial errors for articles (2010a) and prepositions
(2010b; 2011), also based on statistics from an
ESL corpus. In particular, they compile a set of
sentences from the English Wikipedia and apply
the following generation methods:
</bodyText>
<footnote confidence="0.8585355">
1http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item3646603/
Cambridge-International-Corpus-
Cambridge-Learner-Corpus/
</footnote>
<subsectionHeader confidence="0.368168">
General
</subsectionHeader>
<bodyText confidence="0.963334833333333">
Target words (e.g. articles) are replaced with oth-
ers of the same class with probability x (varying
from 0.05 to 0.18). Each new word is chosen uni-
formly at random.
Distribution before correction (in ESL data)
Target words in the error-free text are changed
to match the distribution observed in ESL error-
annotated data before any correction is made.
Distribution after correction (in ESL data)
Target words in the error-free text are changed
to match the distribution observed in ESL error-
annotated data after corrections are made.
</bodyText>
<subsectionHeader confidence="0.556263">
Native language-specific distributions
</subsectionHeader>
<bodyText confidence="0.999986764705882">
It has been observed that second language produc-
tion is affected by a learner’s native language (L1)
(Lee and Seneff, 2008; Leacock et al., 2010). A
common example is the difficulty in using English
articles appropriately by learners whose L1 has
no article system, such as Russian or Japanese.
Because word choice errors follow systematic pat-
terns (i.e. they do not occur randomly), this infor-
mation is extremely valuable for generating errors
more accurately.
L1-specific errors can be imitated by computing
word confusions in an error-annotated ESL cor-
pora and using these distributions to change tar-
get words accordingly in error-free text. More
specifically, if we estimate P(source|target) in an
error-tagged corpus (i.e. the probability of an
incorrect source word being used when the cor-
rect target is expected), we can generate more ac-
curate confusion sets where each candidate has
an associated probability depending on the ob-
served word. For example, supposing that a
group of learners use the preposition to in 10%
of cases where the preposition for should be used
(that is, P(source=to|target=for)=0.10), we can
replicate this error pattern by replacing the oc-
currences of the preposition for with to with a
probability of 0.10 in a corpus of error-free sen-
tences. When the source and target words are the
same, P(source=x|target=x) expresses the proba-
bility that a learner produces the correct/expected
word.
Because errors are generally sparse (and there-
fore error rates are low), replicating mistakes
based on observed probabilities can easily lead to
</bodyText>
<page confidence="0.996554">
117
</page>
<bodyText confidence="0.99981868115942">
low recall. In order to address this issue during ar-
tificial error generation, Rozovskaya et al. (2012)
propose an inflation method that boosts confusion
probabilities in order to generate a larger propor-
tion of artificial instances. This reformulation is
shown to improve F-scores when correcting deter-
miners and prepositions.
Experiments reveal that these approaches yield
better results than assuming uniform probabilis-
tic distributions where all errors and correc-
tions are equally likely. In particular, classifiers
trained on artificially generated data outperformed
those trained on native error-free text (Rozovskaya
and Roth, 2010a; Rozovskaya and Roth, 2011).
However, it has also been shown that using arti-
ficially generated data as a replacement for non-
native error-corrected data can lead to poorer per-
formance (Sj¨obergh and Knutsson, 2005; Foster
and Andersen, 2009). This would suggest that ar-
tificial errors are more useful than native data but
less useful than corrected non-native data.
Rozovskaya and Roth also control other vari-
ables in their experiments. On the one hand, they
only evaluate their systems on sentences that have
no spelling mistakes so as to avoid degrading per-
formance. This is particularly important when
training classifiers on features extracted with lin-
guistic tools (such as parsers or taggers) as they
could provide inaccurate results for malformed in-
put. On the other hand, the authors work on a lim-
ited set of error types (mainly articles and preposi-
tions) which are closed word classes and therefore
have reduced confusion sets. Thus, it becomes in-
teresting to investigate how their ideas extrapolate
to open-class error types, like verb form or content
word errors.
Their probabilistic error generation approach
has also been used by other researchers. Imamura
et al. (2012), for example, applied this method to
generate artificial incorrect sentences for Japanese
particle correction with an inflation factor ranging
from 0.0 (no errors) to 2.0 (double error rates).
Their results show that the performance of artifi-
cial corpora depends largely on the inflation rate
but can achieve good results when domain adapta-
tion is applied.
In a more exhaustive study, Cahill et al.
(2013) investigate the usefulness of automatically-
compiled sentences from Wikipedia revisions
for correcting preposition errors. A number
of classifiers are trained using error-free text,
automatically-compiled annotated corpora and ar-
tificial sentences generated using error probabili-
ties derived from Wikipedia revisions and Lang-
8.2 Their results reveal a number of interesting
points, namely that artificial errors provide com-
petitive results and perform robustly across differ-
ent test sets. A learning curve analysis also shows
system performance increases as more training
data is used, both real and artificial.
More recently, some teams have also reported
improvements by using artificial data in their
submissions to the CoNLL 2013 shared task.
Rozovskaya et al. (2013) apply their inflation
method to train a classifier for determiner errors
that achieves state-of-the-art performance while
Yuan and Felice (2013) use naively-generated arti-
ficial errors within an SMT framework that places
them third in terms of precision.
</bodyText>
<sectionHeader confidence="0.9846015" genericHeader="method">
3 Advanced generation of artificial
errors
</sectionHeader>
<bodyText confidence="0.9999736">
Our work is based on the hypothesis that using
carefully generated artificial errors improves the
performance of error correction systems. This im-
plies generating errors in a way that resembles
available error-annotated data, using similar texts
and accurate injection methods. Like other proba-
bilistic approaches, our method assumes we have
access to an error-corrected reference corpus from
which we can compute error generation probabili-
ties.
</bodyText>
<subsectionHeader confidence="0.999966">
3.1 Base text selection
</subsectionHeader>
<bodyText confidence="0.999699666666667">
We analyse a set of variables that we consider im-
portant for collecting suitable texts for error injec-
tion, namely:
</bodyText>
<subsectionHeader confidence="0.837596">
Topic
</subsectionHeader>
<bodyText confidence="0.999935875">
Replicating errors on texts about the same topic
as the training/test data is more likely to produce
better results than out-of-domain data, as vocab-
ulary and word senses are more likely to be sim-
ilar. In addition, similar texts are more likely to
exhibit suitable contexts for error injection and
consequently help the system focus on particularly
useful information.
</bodyText>
<subsectionHeader confidence="0.769455">
Genre
</subsectionHeader>
<bodyText confidence="0.9985515">
In cases where no a priori information about topic
is available (for example, because the test set is
</bodyText>
<footnote confidence="0.967875">
2http://lang-8.com/
</footnote>
<page confidence="0.9931">
118
</page>
<bodyText confidence="0.971912214285714">
unknown or the system will be used in different
scenarios), knowing the genre or type of text the
system will process can also be useful. Example
genres include expository (descriptions, essays,
reports, etc.), narrative (stories), persuasive (re-
views, advertisements, etc.), procedural (instruc-
tions, recipes, experiments, etc.) and transactional
texts (letters, interviews, etc.).
Style/register
As with the previous aspects, style (colloquial,
academic, etc.) and register (from formal writ-
ten to informal spoken) also affect production and
should therefore be modelled accurately in the
training data.
</bodyText>
<subsectionHeader confidence="0.843016">
Text complexity/language proficiency
</subsectionHeader>
<bodyText confidence="0.990636862068965">
Candidate texts should exhibit the same reading
complexity as training/test texts and be written by
or targeted at learners with similar English profi-
ciency. Otherwise, the overlap in vocabulary and
grammatical structures is more likely to be small
and thus hinder error injection.
Native language
Because second language production is known to
be affected by a learner’s L1, using candidate texts
produced by groups of the same L1 as the train-
ing/test data should provide more suitable contexts
for error injection. When such texts are not avail-
able, using data by speakers of other L1s that ex-
hibit similar phenomena (e.g. no article system,
agglutinative languages, etc.) might also be use-
ful. However, finding error-free texts written in
English by a specific population can be difficult,
which is why most approaches resort to native
English text.
In our experiments, the aforementioned vari-
ables are manually controlled although we believe
many of them could be assessed automatically.
For example, topics could be estimated using text
similarity measures, genres could be predicted us-
ing structural information and L1s could be in-
ferred using a native language identifier.3
For an analysis of other variables such as do-
main and error distributions, the reader should re-
fer to Cahill et al. (2013).
</bodyText>
<footnote confidence="0.950661333333333">
3See the First Edition of the Shared Task on Native
Language Identification (Tetreault et al., 2013) at https://
sites.google.com/site/nlisharedtask2013/
</footnote>
<subsectionHeader confidence="0.985394">
3.2 Error replication
</subsectionHeader>
<bodyText confidence="0.982309694444444">
Our approach to artificial error generation is sim-
ilar to the one proposed by Rozovskaya and Roth
(2010a) in that we also estimate probabilities in
a corpus of ESL learners which are then used to
distort error-free text. However, unlike them, we
refine our probabilities by imposing restrictions
on the linguistic functions of the words and the
contexts where they occur. Because we extend
generation to open-class error types (such as verb
form errors), this refinement becomes necessary to
overcome disambiguation issues and lead to more
accurate replication.
Our work is the first to exploit linguistic infor-
mation for error generation, as described below.
Error type distributions
We compute the probability of each error type p(t)
occurring over the total number of relevant in-
stances (e.g. noun phrases are relevant instances
for article errors). During generation, p(t) is uni-
formly distributed over all the possible choices for
the error type (e.g. for articles, choices are a, an,
the or the zero article). Relevant instances are de-
tected in the base text and changed for an alter-
native at random using the estimated probabilities.
The probability of leaving relevant instances un-
changed is 1 − p(t).
Morphology
We believe morphological information such as
person or number is particularly useful for identi-
fying and correcting specific error types, such as
articles, noun number or subject-verb agreement.
Thus, we compute the conditional probability of
words in specific classes for different morpholog-
ical contexts (such as noun number or PoS). The
following example shows confusion probabilities
for singular head nouns requiring an:
</bodyText>
<equation confidence="0.9998776">
P(source-det=an|target-det=anhead-noun=NN) = 0.942
P(source-det=the|target-det=anhead-noun=NN) = 0.034
P(source-det=a|target-det=anhead-noun=NN) = 0.015
P(source-det=other|target-det=anhead-noun=NN) = 0.005
P(source-det=∅|target-det=anhead-noun=NN) = 0.004
</equation>
<subsectionHeader confidence="0.437966">
PoS disambiguation
</subsectionHeader>
<bodyText confidence="0.989807">
Most approaches to artificial error generation are
aimed at correcting closed-class words such as
articles or prepositions, which rarely occur with
</bodyText>
<page confidence="0.995057">
119
</page>
<bodyText confidence="0.999570636363636">
a different part of speech in the text. However,
when we consider open-class error types, we
should perform PoS disambiguation since the
same surface form could play different roles in
a sentence. For example, consider generating
artificial verb form errors for the verb to play after
observing its distribution in an error-annotated
corpus. By using PoS tags, we can easily deter-
mine if an occurrence of the wordplay is a verb or
a noun and thus compute or apply the appropriate
probabilities:
</bodyText>
<equation confidence="0.999953">
P(source=play|target=playV) = 0.98
P(source=plays|target=playV) = 0.02
P(source=play|target=playN) = 0.84
P(source=plays|target=playN) = 0.16
</equation>
<subsectionHeader confidence="0.45016">
Semantic classes
</subsectionHeader>
<bodyText confidence="0.999672285714286">
We hypothesise that semantic information about
concepts in the sentences can shed light on
specific usage patterns that may otherwise be
hidden. For example, we could refine confusion
sets for prepositions according to the type of
object they are applied to (a location, a recipient,
an instrument, etc.):
</bodyText>
<equation confidence="0.993027333333333">
P(prep=in|noun class=location) = 0.39
P(prep=to|noun class=location) = 0.31
P(prep=at|noun class=location) = 0.16
P(prep=from|noun class=location) = 0.07
P(prep=∅|noun class=location) = 0.05
P(prep=other|noun class=location) = 0.03
</equation>
<bodyText confidence="0.999935">
By abstracting from surface forms, we can also
generate faithful errors for words that have not
been previously observed, e.g. we may have not
seen hospital but we may have seen school, my
sister’s house or church.
</bodyText>
<subsectionHeader confidence="0.812943">
Word senses
</subsectionHeader>
<bodyText confidence="0.999962181818182">
Polysemous words with the same PoS can exhibit
different patterns of usage for each of their mean-
ings (e.g. one meaning may co-occur with a spe-
cific preposition more often than the others). For
this reason, we introduce probabilities for each
word sense in an attempt to capture more accurate
usage. As an example, consider a hypothetical sit-
uation in which a group of learners confuse prepo-
sitions used with the word bank as a financial insti-
tution but they produce the right preposition when
it refers to a river bed:
</bodyText>
<equation confidence="0.99990775">
P(prep=in|noun=bank1) = 0.76
P(prep=at|noun=bank1) = 0.18
P(prep=on|noun=bank1) = 0.06
P(prep=on|noun=bank2) = 1.00
</equation>
<bodyText confidence="0.997485272727273">
Although it is rare that occurrences of the same
word will refer to different meanings within a
document (the so-called ‘one sense per discourse’
assumption (Gale et al., 1992)), this is not the
case when large corpora containing different doc-
uments are used for characterising and generating
errors. In such scenarios, word sense disambigua-
tion should produce more accurate results.
Table 1 lists the actual probabilities computed
from each type of information and the errors they
are able to generate.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="method">
4 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.969983">
4.1 Corpora and tools
</subsectionHeader>
<bodyText confidence="0.999981466666667">
We use the NUCLE v2.3 corpus (Dahlmeier et
al., 2013) released for the CoNLL 2013 shared
task on error correction, which comprises error-
annotated essays written in English by students
at the National University of Singapore. These
essays cover topics such as environmental pollu-
tion, health care, welfare, technology, etc. All the
sentences were manually annotated by human ex-
perts using a set of 27 error types, but we used the
filtered version containing only the five types se-
lected for the shared task: ArtOrDet (article or de-
terminer), Nn (noun number), Prep (preposition),
SVA (subject-verb agreements) and Vform (verb
form) errors. The training set of the NUCLE cor-
pus contains 57,151 sentences and 1,161,567 to-
kens while the test set comprises 1,381 sentences
and 29,207 tokens. The training portion of the cor-
pus was used to estimate the required conditional
probabilities and train a few variations of our sys-
tems while the test set was reserved to evaluate
performance.
Candidate native texts for error injection were
extracted from the English Wikipedia, controlling
the variables described Section 3.1 as follows:
Topic: We chose an initial set of 50 Wikipedia
articles based on keywords in the NUCLE
training data and proceeded to collect related
articles by following hyperlinks in their ‘See
also’ section. We retrieved a total of 494 arti-
cles which were later preprocessed to remove
</bodyText>
<page confidence="0.977853">
120
</page>
<table confidence="0.999941">
Information Probability Generated error types
Error type distribution P(error type) ArtOrDet, Nn, Prep, SVA, Vform
Morphology P(source=determiner|target=determiner, head noun tag) ArtOrDet, SVA
P(source=verb tag|target=verb tag, subj head noun tag)
PoS disambiguation P(source=word|target=word, PoS) Nn, Vform
Semantic classes P(source=determiner|target=determiner, head noun class) ArtOrDet, Prep
P(source=preposition|target=preposition, head noun class)
Word senses P(source=preposition|verb sense + obj head noun sense) ArtOrDet, Prep, SVA
P(source=preposition|target=preposition, head noun sense)
P(source=preposition|target=preposition, dep adj sense)
P(source=determiner|target=determiner, head noun sense)
P(source=verb tag|target=verb tag, subj head noun sense)
</table>
<tableCaption confidence="0.868011">
Table 1: Probabilities computed for each type of linguistic information. Error codes correspond to the
five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number),
Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).
</tableCaption>
<bodyText confidence="0.9776501">
wikicode tags, yielding 54,945 sentences and
approximately 1,123,739 tokens.
Genre: Both NUCLE and Wikipedia contain ex-
pository texts, although they are not necessar-
ily similar.
Style/register: Written, academic and formal.
Text complexity/language proficiency: Essays
in the NUCLE corpus are written by ad-
vanced university students and are therefore
comparable to standard English Wikipedia
articles. For less sophisticated language,
the Simple English Wikipedia could be an
alternative.
Native language: English Wikipedia articles are
mostly written by native speakers whereas
NUCLE essays are not. This is the only dis-
cordant variable.
PoS tagging was performed using RASP
(Briscoe et al., 2006). Word sense dis-
ambiguation was carried out using the
WordNet::SenseRelate:AllWords Perl module
(Pedersen and Kolhatkar, 2009) which assigns
a sense from WordNet (Miller, 1995) to each
content word in a text. As for semantic in-
formation, we use WordNet classes which are
readily available in NLTK (Bird et al., 2009).
WordNet classes respond to a classification in
lexicographers’ files4 and are defined for content
words as shown in Table 2, depending on their
location in the hierarchy.
</bodyText>
<footnote confidence="0.988528">
4http://wordnet.princeton.edu/man/
lexnames.5WN.html
</footnote>
<table confidence="0.998854214285714">
Part of speech WordNet classification
Adjective all, pertainyms, participial
Adverb all
Noun act, animal, artifact, attribute, body,
cognition, communication, event,
feeling, food, group, location, motive,
object, person, phenomenon, plant,
possession, process, quantity, relation,
shape, state, substance, time
Verb body, change, cognition,
communication, competition,
consumption, contact, creation,
emotion, motion, perception,
possession, social, stative, weather
</table>
<tableCaption confidence="0.896184">
Table 2: WordNet classes for content words.
</tableCaption>
<table confidence="0.874176333333333">
Name Composition
ED errors based on error type distributions
MORPH errors based on morphology
POS errors based on PoS disambiguation
SC errors based on semantic classes
WSD errors based on word senses
</table>
<tableCaption confidence="0.9973125">
Table 3: Generated artificial corpora based on dif-
ferent types of linguistic information.
</tableCaption>
<subsectionHeader confidence="0.813184">
4.2 Error generation
</subsectionHeader>
<bodyText confidence="0.999876666666667">
For each type of information in Table 1, we com-
pute the corresponding conditional probabilities
using the NUCLE training set. These probabili-
ties are then used to generate six different artificial
corpora using the inflation method (Rozovskaya et
al., 2012), as listed in Table 3.
</bodyText>
<subsectionHeader confidence="0.998937">
4.3 System training
</subsectionHeader>
<bodyText confidence="0.999588">
We approach the error correction task as a transla-
tion problem from incorrect into correct English.
Systems are built using an SMT framework and
different combinations of NUCLE and our artifi-
cial corpora, where the source side contains in-
</bodyText>
<page confidence="0.996674">
121
</page>
<table confidence="0.999694384615385">
Original Revised
C M U P R F1 C M U P R F1
NUCLE (baseline) 181 1462 513 0.2608 0.1102 0.1549 200 1483 495 0.2878 0.1188 0.1682
ED 53 1590 150 0.2611 0.0323 0.0574 62 1621 141 0.3054 0.0368 0.0657
MORPH 74 1569 333 0.1818 0.0450 0.0722 83 1600 324 0.2039 0.0493 0.0794
POS 42 1601 99 0.2979 0.0256 0.0471 42 1641 99 0.2979 0.0250 0.0461
SC 80 1563 543 0.1284 0.0487 0.0706 87 1596 536 0.1396 0.0517 0.0755
WSD 82 1561 305 0.2119 0.0499 0.0808 91 1592 296 0.2351 0.0541 0.0879
NUCLE+ED 173 1470 411 0.2962 0.1053 0.1554 194 1489 390 0.3322 0.1153 0.1712
NUCLE+MORPH 163 1480 427 0.2763 0.0992 0.1460 182 1501 408 0.3085 0.1081 0.1601
NUCLE+POS 164 1479 365 0.3100 0.0998 0.1510 182 1501 347 0.3440 0.1081 0.1646
NUCLE+SC 162 1481 488 0.2492 0.0986 0.1413 181 1502 469 0.2785 0.1075 0.1552
NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602
</table>
<tableCaption confidence="0.770862666666667">
Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested
by each system. Results in bold show improvements over the baseline.
</tableCaption>
<bodyText confidence="0.994638764705882">
correct sentences and the target side contains their
corrected versions. Our setup is similar to the one
described by Yuan and Felice (2013) in that we
train a PoS-factored phrase-based model (Koehn,
2010) using Moses (Koehn et al., 2007), Giza++
(Och and Ney, 2003) for word alignment and the
IRSTLM Toolkit (Federico et al., 2008) for lan-
guage modelling. However, unlike them, we do
not optimise decoding parameters but use default
values instead.
We build 11 different systems in total: a base-
line system using only the NUCLE training set,
one system per artificial corpus and other addi-
tional systems using combinations of the NUCLE
training data and our artificial corpora. Each of
these systems uses a single translation model that
tackles all error types at the same time.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999536549019608">
Each system was evaluated in terms of precision,
recall and F1 on the NUCLE test data using the
M2 Scorer (Dahlmeier and Ng, 2012), the official
evaluation script for the CoNLL 2013 shared task.
Table 4 shows results of evaluation on the original
test set (containing only one gold standard correc-
tion per error) and a revised version (which allows
for alternative corrections submitted by participat-
ing teams).
Results reveal our ED and POS corpora are able
to improve precision for both test sets. It is surpris-
ing, however, that the least informed dataset (ED)
is one of the best performers although this seems
reasonable if we consider it is the only dataset that
includes artificial instances for all error types (see
Table 1). Hybrid datasets containing the NUCLE
training set plus an artificial corpus also gener-
ally improve precision, except for NUCLE+SC. It
could be argued that the reason for this improve-
ment is corpus size, since our hybrid datasets are
double the size of each individual set, but the small
differences in precision between the ED and POS
datasets and their corresponding hybrid versions
seem to contradict that hypothesis. In fact, re-
sults would suggest artificial and naturally occur-
ring errors are not interchangeable but rather com-
plementary.
The observed improvement in precision, how-
ever, comes at the expense of recall, for which
none of the systems is able to beat the baseline.
This contradicts results by Rozovskaya and Roth
(2010a), who show their error inflation method in-
creases recall, although this could be due to differ-
ences in the training paradigm and data. Still, re-
sults are encouraging since precision is generally
preferred over recall in error correction scenarios
(Yuan and Felice, 2013).
We also evaluated performance by error type on
the original (Table 5) and revised (Table 6) test
data using an estimation approach similar to the
one in CoNLL 2013. Results show that the per-
formance of each dataset varies by error type, sug-
gesting that certain types of information are bet-
ter suited for specific error types. In particular,
we find that on the original test set, ED achieves
the highest precision for article and determiners,
WSD maximises precision for prepositions and
SC achieves the highest recall and F1. When us-
ing hybrid sets, results improve overall, with the
highest precision being as follows: NUCLE+POS
(ArtOrDet), NUCLE+ED (Nn), NUCLE+WSD
</bodyText>
<page confidence="0.995306">
122
</page>
<table confidence="0.999949615384615">
ArtOrDet Nn Prep SVA/Vform Other
P R F1 P R F1 P R F1 P R F1 C M U
NUCLE (b) 0.2716 0.1551 0.1974 0.4625 0.0934 0.1555 0.1333 0.0386 0.0599 0.2604 0.1016 0.1462 0 0 34
ED 0.2813 0.0391 0.0687 0.6579 0.0631 0.1152 0.0233 0.0032 0.0056 0.0000 0.0000 — 0 0 5
MORPH 0.1862 0.1058 0.1349 — 0.0000 — 0.0000 0.0000 — 0.1429 0.0041 0.0079 0 0 7
POS 0.0000 0.0000 — 0.4405 0.0934 0.1542 0.0000 0.0000 — 0.1515 0.0203 0.0358 0 0 10
SC 0.1683 0.0739 0.1027 — 0.0000 — 0.0986 0.0932 0.0959 0.0000 0.0000 — 0 0 21
WSD 0.2219 0.1029 0.1406 0.0000 0.0000 — 0.1905 0.0257 0.0453 0.1875 0.0122 0.0229 0 0 8
NUCLE+ED 0.3185 0.1348 0.1894 0.5465 0.1187 0.1950 0.1304 0.0386 0.0596 0.2658 0.0854 0.1292 0 0 35
NUCLE+MORPH 0.2857 0.1507 0.1973 0.4590 0.0707 0.1225 0.1719 0.0354 0.0587 0.2817 0.0813 0.1262 0 0 30
NUCLE+POS 0.3384 0.1290 0.1868 0.4659 0.1035 0.1694 0.1884 0.0418 0.0684 0.2625 0.0854 0.1288 0 0 29
NUCLE+SC 0.2890 0.1290 0.1784 0.4500 0.0682 0.1184 0.1492 0.0868 0.1098 0.2836 0.0772 0.1214 0 0 34
NUCLE+WSD 0.3003 0.1449 0.1955 0.4667 0.0707 0.1228 0.1948 0.0482 0.0773 0.2632 0.0813 0.1242 0 0 30
</table>
<tableCaption confidence="0.947596">
Table 5: Error type analysis of our correction systems over the original NUCLE test set using the M2
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside
the main categories suggested by each system. Results in bold show improvements over the baseline.
</tableCaption>
<table confidence="0.999948307692308">
ArtOrDet Nn Prep SVA/Vform Other
P R F1 P R F1 P R F1 P R F1 C M U
NUCLE (b) 0.3519 0.2026 0.2572 0.6163 0.1302 0.2150 0.2069 0.0682 0.1026 0.4105 0.1718 0.2422 0 0 34
ED 0.4063 0.0579 0.1014 0.7297 0.0684 0.1250 0.0465 0.0077 0.0132 0.1818 0.0183 0.0332 0 0 5
MORPH 0.2270 0.1311 0.1662 — 0.0000 — 0.0000 0.0000 — 0.2857 0.0092 0.0179 0 0 7
POS 0.0000 0.0000 — 0.5465 0.1169 0.1926 0.0000 0.0000 — 0.4242 0.0631 0.1098 0 0 10
SC 0.2112 0.0944 0.1305 — 0.0000 — 0.1088 0.1221 0.1151 0.0000 0.0000 — 0 0 21
WSD 0.2781 0.1313 0.1784 0.0000 0.0000 — 0.2143 0.0347 0.0598 0.2000 0.0138 0.0259 0 0 8
NUCLE+ED 0.4334 0.1849 0.2592 0.7000 0.1552 0.2540 0.1685 0.0575 0.0857 0.4744 0.1630 0.2426 0 0 35
NUCLE+MORPH 0.3791 0.2006 0.2624 0.6308 0.1017 0.1752 0.2295 0.0536 0.0870 0.4714 0.1454 0.2222 0 0 30
NUCLE+POS 0.4601 0.1761 0.2547 0.6087 0.1383 0.2254 0.2424 0.0613 0.0979 0.4430 0.1549 0.2295 0 0 29
NUCLE+SC 0.3961 0.1773 0.2450 0.6154 0.0993 0.1709 0.1844 0.1250 0.1490 0.4848 0.1410 0.2184 0 0 34
NUCLE+WSD 0.3994 0.1933 0.2605 0.6308 0.1017 0.1752 0.2432 0.0690 0.1075 0.4667 0.1535 0.2310 0 0 30
</table>
<tableCaption confidence="0.83937">
Table 6: Error type analysis of our correction systems over the revised NUCLE test set using the M2
</tableCaption>
<bodyText confidence="0.957058176470588">
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside
the main categories suggested by each system. Results in bold show improvements over the baseline.
(Prep) and NUCLE+SC (SVA/Vform). As ex-
pected, the use of alternative annotations in the re-
vised test set improves results but it does not reveal
any qualitative difference between datasets.
Finally, when compared to other systems in the
CoNLL 2013 shared task in terms of F1, our best
systems would rank 9th on both test sets. This
would suggest that using an off-the-shelf SMT
system trained on a combination of real and ar-
tificial data can yield better results than other ma-
chine learning techniques (Yi et al., 2013; van den
Bosch and Berck, 2013; Berend et al., 2013) or
rule-based approaches (Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Flickinger and Yu, 2013;
Sidorov et al., 2013).
</bodyText>
<sectionHeader confidence="0.997936" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999988347826087">
This paper presents early results on the genera-
tion and use of artificial errors for grammatical
error correction. Our approach uses conditional
probabilities derived from an ESL error-annotated
corpus to replicate errors in native error-free data.
Unlike previous work, we propose using linguistic
information such as PoS or sense disambiguation
to refine the contexts where errors occur and thus
replicate them more accurately. We use five differ-
ent types of information to generate our artificial
corpora, which are later evaluated in isolation as
well as coupled to the original ESL training data.
General results show error distributions and PoS
information produce the best results, although this
varies when we analyse each error type separately.
These results should allow us to generate errors
more efficiently in the future by using the best ap-
proach for each error type.
We have also observed that precision improves
at the expense of recall and this is more pro-
nounced when using purely artificial sets. Finally,
artificially generated errors seem to be a comple-
ment rather than an alternative to genuine data.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="discussions">
7 Future work
</sectionHeader>
<bodyText confidence="0.996393">
There are a number of issues we plan to address in
future research, as described below.
</bodyText>
<subsectionHeader confidence="0.997004">
Scaling up artificial data
</subsectionHeader>
<bodyText confidence="0.999841">
The experiments presented here use a small and
manually selected collection of Wikipedia articles.
</bodyText>
<page confidence="0.997594">
123
</page>
<bodyText confidence="0.999189111111111">
However, we plan to study the performance of our
systems as corpus size is increased. We are cur-
rently using a larger selection of Wikipedia ar-
ticles to produce new artificial datasets ranging
from 50K to 5M sentences. The resulting corpora
will be used to train new error correction systems
and study how precision and recall vary as more
data is added during the training process, similar
to Cahill et al. (2013).
</bodyText>
<subsectionHeader confidence="0.647436">
Reducing differences between datasets
</subsectionHeader>
<bodyText confidence="0.9992765">
As shown in Table 1, we are unable to produce
the same set of errors for each different type of in-
formation. This is a limitation of our conditional
probabilities which encode different information
in each case. In consequence, comparing overall
results between datasets seems unfair as they do
not target the same error types. In order to over-
come this problem, we will define new probabili-
ties so that we can generate the same types of error
in all cases.
</bodyText>
<subsectionHeader confidence="0.529338">
Exploring larger contexts
</subsectionHeader>
<bodyText confidence="0.9999805">
Our current probabilities model error contexts
in a limited way, mostly by considering rela-
tions between two or three words (e.g. arti-
cle+noun, verb+preposition+noun, etc.). In or-
der to improve error injection, we will define
new probabilities using larger contexts, such as
P(source=verbltarget=verb, subject class, auxil-
iary verbs, object class) for verb form errors.
Using more specific contexts can also be useful for
correcting complex error types, such as the use of
pronouns, which often requires analysing corefer-
ence chains.
</bodyText>
<subsectionHeader confidence="0.573138">
Using new linguistic information
</subsectionHeader>
<bodyText confidence="0.967359157894737">
In this work we have used five types of linguis-
tic information. However, we believe other types
of information and their associated probabilities
could also be useful, especially if we aim to cor-
rect more error types. Examples include spelling,
grammatical relations (dependencies) and word
order (syntax). Additionally, we believe the use
of semantic role labels can be explored as an al-
ternative to semantic classes, as they have proved
useful for error correction (Liu et al., 2010).
Mixed error generation
In our current experiments, each artificial corpus is
generated using only one type of information at a
time. However, having found that certain types of
information are more suitable than others for cor-
recting specific error types (see Tables 5 and 6), we
believe better artificial corpora could be created by
generating instances of each error type using only
the most appropriate linguistic information.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998748">
We would like to thank Prof Ted Briscoe for his
valuable comments and suggestions as well as
Cambridge English Language Assessment, a divi-
sion of Cambridge Assessment, for supporting this
research.
</bodyText>
<sectionHeader confidence="0.998612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994389076923077">
Gabor Berend, Veronika Vincze, Sina Zarrieß, and
Rich´ard Farkas. 2013. Lfg-based features
for noun number and article grammatical errors.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 62–67, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.
Ted Briscoe, John Carroll, and Rebecca Watson.
2006. The second release of the RASP sys-
tem. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL
’06, pages 77–80, Sydney, Australia. Association for
Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael
Gamon. 2006. Correcting ESL Errors Using
Phrasal SMT Techniques. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
249–256, Sydney, Australia, July. Association for
Computational Linguistics.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and
Diane Napolitano. 2013. Robust systems for
preposition error correction using wikipedia revi-
sions. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 507–517, Atlanta, Georgia,
June. Association for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012.
Better evaluation for grammatical error correc-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL 2012, pages 568 – 572,
Montreal, Canada.
</reference>
<page confidence="0.992734">
124
</page>
<reference confidence="0.998915578947369">
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu. 2013. Building a Large Annotated Corpus
of Learner English: The NUS Corpus of Learner
English. In Proceedings of the 8th Workshop on
Innovative Use of NLP for Building Educational
Applications, BEA 2013, pages 22–31, Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.
Nava Ehsan and Heshaam Faili. 2013. Grammatical
and context-sensitive error correction using a sta-
tistical machine translation framework. Software:
Practice and Experience, 43(2):187–206.
Marcello Federico, Nicola Bertoldi, and Mauro
Cettolo. 2008. IRSTLM: an open source toolkit
for handling large scale language models. In
Proceedings of the 9th Annual Conference of the
International Speech Communication Association,
INTERSPEECH 2008, pages 1618–1621, Brisbane,
Australia, September. ISCA.
Dan Flickinger and Jiye Yu. 2013. Toward
more precision in correction of grammatical er-
rors. In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 68–73, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Jennifer Foster and Øistein Andersen. 2009.
Generrate: Generating errors for use in grammati-
cal error detection. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 82–90, Boulder,
Colorado, June. Association for Computational
Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the workshop on Speech
and Natural Language, HLT ’91, pages 233–
237, Harriman, New York. Association for
Computational Linguistics.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 388–392, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga,
Thepchai Supnithi, and Hitoshi Isahara. 2003.
Automatic Error Detection in the Japanese Learners’
English Spoken Data. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - Volume 2, ACL ’03, pages 145–
148, Sapporo, Japan. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2013. Iitb system for conll 2013
shared task: A hybrid approach to grammatical er-
ror correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 82–87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan
and Claypool Publishers.
John Lee and Stephanie Seneff. 2008. An analysis of
grammatical errors in non-native speech in English.
In Amitava Das and Srinivas Bangalore, editors,
Proceedings of the 2008 IEEE Spoken Language
Technology Workshop, SLT 2008, pages 89–92,
Goa, India, December. IEEE.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based
verb selection for ESL. In Proceedings of
the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1068–
1076, Cambridge, MA, October. Association for
Computational Linguistics.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41, November.
Tomoya Mizumoto, Mamoru Komachi, Masaaki
Nagata, and Yuji Matsumoto. 2011. Mining
Revision Log of Language Learning SNS
for Automated Japanese Error Correction of
Second Language Learners. In Proceedings of
5th International Joint Conference on Natural
Language Processing, pages 147–155, Chiang Mai,
Thailand, November. Asian Federation of Natural
Language Processing.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The
CoNLL-2013 Shared Task on Grammatical Error
Correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 1–12, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
</reference>
<page confidence="0.982304">
125
</page>
<reference confidence="0.999162113402062">
Ted Pedersen and Varada Kolhatkar. 2009.
WordNet::SenseRelate::AllWords: a broad cover-
age word sense tagger that maximizes semantic
relatedness. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Companion Volume:
Demonstration Session, NAACL-Demonstrations
’9, pages 17–20, Boulder, Colorado. Association
for Computational Linguistics.
Desmond Darma Putra and Lili Szabo. 2013.
Uds at conll 2013 shared task. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
88–95, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2010a. Training
paradigms for correcting errors in grammar and us-
age. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics,
HLT ’10, pages 154–162, Los Angeles, California.
Association for Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2010b. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 961–970, Cambridge, Massachusetts.
Association for Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT
’11, pages 924–933, Portland, Oregon. Association
for Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI system in the HOO 2012 shared task
on error correction. In Proceedings of the Seventh
Workshop on Building Educational Applications
Using NLP, pages 272–280, Montreal, Canada.
Association for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of
Illinois System in the CoNLL-2013 Shared Task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13–19, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Dolors
Catala, Angels Catena, and Sandrine Fuentes. 2013.
Rule-based system for automatic grammar correc-
tion using syntactic n-grams for english language
learning (l2). In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 96–101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Jonas Sj¨obergh and Ola Knutsson. 2005. Faking
errors to avoid making errors: Very weakly su-
pervised learning for error detection in writing.
In Proceedings of RANLP 2005, pages 506–512,
Borovets, Bulgaria, September.
Toshikazu Tajiri, Mamoru Komachi, and Yuji
Matsumoto. 2012. Tense and aspect error cor-
rection for esl learners using global context. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 198–202, Jeju Island, Korea,
July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A Report on the First Native Language
Identification Shared Task. In Proceedings
of the Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
48–57, Atlanta, Georgia, June. Association for
Computational Linguistics.
Antal van den Bosch and Peter Berck. 2013. Memory-
based grammatical error correction. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
102–108, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction sys-
tem for conll-2013 shared task. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
123–127, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52–61, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
</reference>
<page confidence="0.998518">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577283">
<title confidence="0.999755">Generating artificial errors for grammatical error correction</title>
<author confidence="0.995604">Mariano</author>
<affiliation confidence="0.970396333333333">Computer University of United</affiliation>
<email confidence="0.921171">mf501@cam.ac.uk</email>
<author confidence="0.912688">Zheng</author>
<affiliation confidence="0.923410333333333">Computer University of United</affiliation>
<email confidence="0.850893">zy249@cam.ac.uk</email>
<abstract confidence="0.997553904761905">This paper explores the generation of artificial errors for correcting grammatical mistakes made by learners of English as a second language. Artificial errors are injected into a set of error-free sentences in a probabilistic manner using statistics from a corpus. Unlike previous approaches, we use linguistic information to derive error generation probabilities and build corpora to correct several error types, including open-class errors. In addition, we also analyse the variables involved in the selection of candidate sentences. Experiments using the NUCLE corpus from the CoNLL 2013 shared task reveal that: 1) training on artificially created errors improves precision at the expense of recall and 2) different types of linguistic information are better suited for correcting different error types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Gabor Berend</author>
</authors>
<title>Veronika Vincze,</title>
<location>Sina Zarrieß, and</location>
<marker>Berend, </marker>
<rawString>Gabor Berend, Veronika Vincze, Sina Zarrieß, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
</authors>
<title>Lfg-based features for noun number and article grammatical errors.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>62--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Farkas, 2013</marker>
<rawString>Rich´ard Farkas. 2013. Lfg-based features for noun number and article grammatical errors. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62–67, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="23205" citStr="Bird et al., 2009" startWordPosition="3488" endWordPosition="3491">cles. For less sophisticated language, the Simple English Wikipedia could be an alternative. Native language: English Wikipedia articles are mostly written by native speakers whereas NUCLE essays are not. This is the only discordant variable. PoS tagging was performed using RASP (Briscoe et al., 2006). Word sense disambiguation was carried out using the WordNet::SenseRelate:AllWords Perl module (Pedersen and Kolhatkar, 2009) which assigns a sense from WordNet (Miller, 1995) to each content word in a text. As for semantic information, we use WordNet classes which are readily available in NLTK (Bird et al., 2009). WordNet classes respond to a classification in lexicographers’ files4 and are defined for content words as shown in Table 2, depending on their location in the hierarchy. 4http://wordnet.princeton.edu/man/ lexnames.5WN.html Part of speech WordNet classification Adjective all, pertainyms, participial Adverb all Noun act, animal, artifact, attribute, body, cognition, communication, event, feeling, food, group, location, motive, object, person, phenomenon, plant, possession, process, quantity, relation, shape, state, substance, time Verb body, change, cognition, communication, competition, cons</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06,</booktitle>
<pages>77--80</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="22889" citStr="Briscoe et al., 2006" startWordPosition="3438" endWordPosition="3441">Genre: Both NUCLE and Wikipedia contain expository texts, although they are not necessarily similar. Style/register: Written, academic and formal. Text complexity/language proficiency: Essays in the NUCLE corpus are written by advanced university students and are therefore comparable to standard English Wikipedia articles. For less sophisticated language, the Simple English Wikipedia could be an alternative. Native language: English Wikipedia articles are mostly written by native speakers whereas NUCLE essays are not. This is the only discordant variable. PoS tagging was performed using RASP (Briscoe et al., 2006). Word sense disambiguation was carried out using the WordNet::SenseRelate:AllWords Perl module (Pedersen and Kolhatkar, 2009) which assigns a sense from WordNet (Miller, 1995) to each content word in a text. As for semantic information, we use WordNet classes which are readily available in NLTK (Bird et al., 2009). WordNet classes respond to a classification in lexicographers’ files4 and are defined for content words as shown in Table 2, depending on their location in the hierarchy. 4http://wordnet.princeton.edu/man/ lexnames.5WN.html Part of speech WordNet classification Adjective all, perta</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06, pages 77–80, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL Errors Using Phrasal SMT Techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3660" citStr="Brockett et al. (2006)" startWordPosition="570" endWordPosition="573">tion of the variables that affect artificial error generation. 2 Related work The use of artificial data to train error correction systems has been explored by other researchers using a variety of techniques. Izumi et al. (2003), for example, use artificial errors to target article mistakes made by Japanese learners of English. A corpus is created by replacing a, an, the or the zero article by a different article chosen at random in more than 7,500 correct sentences and used to train a maximum entropy model. Results show an improvement for omission errors but no change for replacement errors. Brockett et al. (2006) describe the use of a statistical machine translation (SMT) system for correcting a set of 14 countable/uncountable nouns 116 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–126, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics which are often confusing for ESL learners. Their training corpus consists of a large number of sentences extracted from news articles which were deliberately modified to include typical countability errors based on evidence from</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL Errors Using Phrasal SMT Techniques. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 249–256, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Diane Napolitano</author>
</authors>
<title>Robust systems for preposition error correction using wikipedia revisions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>507--517</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1812" citStr="Cahill et al., 2013" startWordPosition="267" endWordPosition="270">hich is difficult to obtain. Available error-annotated corpora are often focused on particular groups of people (e.g. nonnative students), error types (e.g. spelling, syntax), genres (e.g. university essays, letters) or topics so it is not clear how representative they are or how well systems based on them will generalise. On the other hand, building new corpora is not always a viable solution since error annotation is expensive. As a result, researchers have tried to overcome these limitations either by compiling corpora automatically from the web (Mizumoto et al., 2011; Tajiri et al., 2012; Cahill et al., 2013) or using artificial corpora which are cheaper to produce and can be tailored to their needs. Artificial error generation allows researchers to create very large error-annotated corpora with little effort and control variables such as topic and error types. Errors can be injected into candidate texts using a deterministic approach (e.g. fixed rules) or probabilities derived from manually annotated samples in order to mimic real data. Although artificial errors have been used in previous work, we present a new approach based on linguistic information and evaluate it using the test data provided</context>
<context position="10777" citStr="Cahill et al. (2013)" startWordPosition="1660" endWordPosition="1663">investigate how their ideas extrapolate to open-class error types, like verb form or content word errors. Their probabilistic error generation approach has also been used by other researchers. Imamura et al. (2012), for example, applied this method to generate artificial incorrect sentences for Japanese particle correction with an inflation factor ranging from 0.0 (no errors) to 2.0 (double error rates). Their results show that the performance of artificial corpora depends largely on the inflation rate but can achieve good results when domain adaptation is applied. In a more exhaustive study, Cahill et al. (2013) investigate the usefulness of automaticallycompiled sentences from Wikipedia revisions for correcting preposition errors. A number of classifiers are trained using error-free text, automatically-compiled annotated corpora and artificial sentences generated using error probabilities derived from Wikipedia revisions and Lang8.2 Their results reveal a number of interesting points, namely that artificial errors provide competitive results and perform robustly across different test sets. A learning curve analysis also shows system performance increases as more training data is used, both real and </context>
<context position="14886" citStr="Cahill et al. (2013)" startWordPosition="2278" endWordPosition="2281">so be useful. However, finding error-free texts written in English by a specific population can be difficult, which is why most approaches resort to native English text. In our experiments, the aforementioned variables are manually controlled although we believe many of them could be assessed automatically. For example, topics could be estimated using text similarity measures, genres could be predicted using structural information and L1s could be inferred using a native language identifier.3 For an analysis of other variables such as domain and error distributions, the reader should refer to Cahill et al. (2013). 3See the First Edition of the Shared Task on Native Language Identification (Tetreault et al., 2013) at https:// sites.google.com/site/nlisharedtask2013/ 3.2 Error replication Our approach to artificial error generation is similar to the one proposed by Rozovskaya and Roth (2010a) in that we also estimate probabilities in a corpus of ESL learners which are then used to distort error-free text. However, unlike them, we refine our probabilities by imposing restrictions on the linguistic functions of the words and the contexts where they occur. Because we extend generation to open-class error t</context>
<context position="34411" citStr="Cahill et al. (2013)" startWordPosition="5342" endWordPosition="5345">number of issues we plan to address in future research, as described below. Scaling up artificial data The experiments presented here use a small and manually selected collection of Wikipedia articles. 123 However, we plan to study the performance of our systems as corpus size is increased. We are currently using a larger selection of Wikipedia articles to produce new artificial datasets ranging from 50K to 5M sentences. The resulting corpora will be used to train new error correction systems and study how precision and recall vary as more data is added during the training process, similar to Cahill et al. (2013). Reducing differences between datasets As shown in Table 1, we are unable to produce the same set of errors for each different type of information. This is a limitation of our conditional probabilities which encode different information in each case. In consequence, comparing overall results between datasets seems unfair as they do not target the same error types. In order to overcome this problem, we will define new probabilities so that we can generate the same types of error in all cases. Exploring larger contexts Our current probabilities model error contexts in a limited way, mostly by c</context>
</contexts>
<marker>Cahill, Madnani, Tetreault, Napolitano, 2013</marker>
<rawString>Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane Napolitano. 2013. Robust systems for preposition error correction using wikipedia revisions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 507–517, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2012,</booktitle>
<pages>568--572</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="26857" citStr="Dahlmeier and Ng, 2012" startWordPosition="4071" endWordPosition="4074">TLM Toolkit (Federico et al., 2008) for language modelling. However, unlike them, we do not optimise decoding parameters but use default values instead. We build 11 different systems in total: a baseline system using only the NUCLE training set, one system per artificial corpus and other additional systems using combinations of the NUCLE training data and our artificial corpora. Each of these systems uses a single translation model that tackles all error types at the same time. 5 Results Each system was evaluated in terms of precision, recall and F1 on the NUCLE test data using the M2 Scorer (Dahlmeier and Ng, 2012), the official evaluation script for the CoNLL 2013 shared task. Table 4 shows results of evaluation on the original test set (containing only one gold standard correction per error) and a revised version (which allows for alternative corrections submitted by participating teams). Results reveal our ED and POS corpora are able to improve precision for both test sets. It is surprising, however, that the least informed dataset (ED) is one of the best performers although this seems reasonable if we consider it is the only dataset that includes artificial instances for all error types (see Table 1</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2012, pages 568 – 572, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>22--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>BEA</location>
<contexts>
<context position="19794" citStr="Dahlmeier et al., 2013" startWordPosition="3004" endWordPosition="3007">=on|noun=bank2) = 1.00 Although it is rare that occurrences of the same word will refer to different meanings within a document (the so-called ‘one sense per discourse’ assumption (Gale et al., 1992)), this is not the case when large corpora containing different documents are used for characterising and generating errors. In such scenarios, word sense disambiguation should produce more accurate results. Table 1 lists the actual probabilities computed from each type of information and the errors they are able to generate. 4 Experimental setup 4.1 Corpora and tools We use the NUCLE v2.3 corpus (Dahlmeier et al., 2013) released for the CoNLL 2013 shared task on error correction, which comprises errorannotated essays written in English by students at the National University of Singapore. These essays cover topics such as environmental pollution, health care, welfare, technology, etc. All the sentences were manually annotated by human experts using a set of 27 error types, but we used the filtered version containing only the five types selected for the shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (preposition), SVA (subject-verb agreements) and Vform (verb form) errors. The training s</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2013, pages 22–31, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nava Ehsan</author>
<author>Heshaam Faili</author>
</authors>
<title>Grammatical and context-sensitive error correction using a statistical machine translation framework.</title>
<date>2013</date>
<journal>Software: Practice and Experience,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="4711" citStr="Ehsan and Faili (2013)" startWordPosition="729" endWordPosition="732"> corpus consists of a large number of sentences extracted from news articles which were deliberately modified to include typical countability errors based on evidence from a Chinese learner corpus. Their approach to artificial error injection is deterministic, using hand-coded rules to change quantifiers (much → many), generate plurals (advice → advices) or insert unnecessary determiners. Experiments show their system was generally able to beat the standard Microsoft Word 2003 grammar checker, although it produced a relatively higher rate of erroneous corrections. SMT systems are also used by Ehsan and Faili (2013) to correct grammatical errors and contextsensitive spelling mistakes in English and Farsi. Training corpora are obtained by injecting artificial errors into well-formed treebank sentences using predefined error templates. Whenever an original sentence from the corpus matches one of these templates, a pair of correct and incorrect sentences is generated. This process is repeated multiple times if a single sentence matches more than one error template, thereby generating many pairs for the same original sentence. A comparison between the proposed systems and rule-based grammar checkers show the</context>
</contexts>
<marker>Ehsan, Faili, 2013</marker>
<rawString>Nava Ehsan and Heshaam Faili. 2013. Grammatical and context-sensitive error correction using a statistical machine translation framework. Software: Practice and Experience, 43(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Annual Conference of the International Speech Communication Association, INTERSPEECH</booktitle>
<pages>1618--1621</pages>
<publisher>ISCA.</publisher>
<location>Brisbane, Australia,</location>
<contexts>
<context position="26269" citStr="Federico et al., 2008" startWordPosition="3971" endWordPosition="3974">42 0.1075 0.1602 Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2 Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested by each system. Results in bold show improvements over the baseline. correct sentences and the target side contains their corrected versions. Our setup is similar to the one described by Yuan and Felice (2013) in that we train a PoS-factored phrase-based model (Koehn, 2010) using Moses (Koehn et al., 2007), Giza++ (Och and Ney, 2003) for word alignment and the IRSTLM Toolkit (Federico et al., 2008) for language modelling. However, unlike them, we do not optimise decoding parameters but use default values instead. We build 11 different systems in total: a baseline system using only the NUCLE training set, one system per artificial corpus and other additional systems using combinations of the NUCLE training data and our artificial corpora. Each of these systems uses a single translation model that tackles all error types at the same time. 5 Results Each system was evaluated in terms of precision, recall and F1 on the NUCLE test data using the M2 Scorer (Dahlmeier and Ng, 2012), the offici</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of the 9th Annual Conference of the International Speech Communication Association, INTERSPEECH 2008, pages 1618–1621, Brisbane, Australia, September. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Jiye Yu</author>
</authors>
<title>Toward more precision in correction of grammatical errors.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>68--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32603" citStr="Flickinger and Yu, 2013" startWordPosition="5049" endWordPosition="5052">e use of alternative annotations in the revised test set improves results but it does not reveal any qualitative difference between datasets. Finally, when compared to other systems in the CoNLL 2013 shared task in terms of F1, our best systems would rank 9th on both test sets. This would suggest that using an off-the-shelf SMT system trained on a combination of real and artificial data can yield better results than other machine learning techniques (Yi et al., 2013; van den Bosch and Berck, 2013; Berend et al., 2013) or rule-based approaches (Kunchukuttan et al., 2013; Putra and Szabo, 2013; Flickinger and Yu, 2013; Sidorov et al., 2013). 6 Conclusions This paper presents early results on the generation and use of artificial errors for grammatical error correction. Our approach uses conditional probabilities derived from an ESL error-annotated corpus to replicate errors in native error-free data. Unlike previous work, we propose using linguistic information such as PoS or sense disambiguation to refine the contexts where errors occur and thus replicate them more accurately. We use five different types of information to generate our artificial corpora, which are later evaluated in isolation as well as co</context>
</contexts>
<marker>Flickinger, Yu, 2013</marker>
<rawString>Dan Flickinger and Jiye Yu. 2013. Toward more precision in correction of grammatical errors. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 68–73, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Øistein Andersen</author>
</authors>
<title>Generrate: Generating errors for use in grammatical error detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5544" citStr="Foster and Andersen (2009)" startWordPosition="855" endWordPosition="858">rror templates. Whenever an original sentence from the corpus matches one of these templates, a pair of correct and incorrect sentences is generated. This process is repeated multiple times if a single sentence matches more than one error template, thereby generating many pairs for the same original sentence. A comparison between the proposed systems and rule-based grammar checkers show they are complementary, with a hybrid system achieving the best performance. 2.1 Probabilistic approaches A few researchers have explored probabilistic methods in an attempt to mimic real data more accurately. Foster and Andersen (2009), for example, describe a tool for generating artificial errors based on statistics from other corpora, such as the Cambridge Learner Corpus (CLC).1 Their experiments show a drop in accuracy when artificial sentences are used as a replacement for real incorrect sentences, suggesting that they may not be as useful as genuine text. Their report also includes an extensive summary of previous work in the area. Rozovskaya and Roth propose more sophisticated probabilistic methods to generate artificial errors for articles (2010a) and prepositions (2010b; 2011), also based on statistics from an ESL c</context>
<context position="9429" citStr="Foster and Andersen, 2009" startWordPosition="1444" endWordPosition="1447">s shown to improve F-scores when correcting determiners and prepositions. Experiments reveal that these approaches yield better results than assuming uniform probabilistic distributions where all errors and corrections are equally likely. In particular, classifiers trained on artificially generated data outperformed those trained on native error-free text (Rozovskaya and Roth, 2010a; Rozovskaya and Roth, 2011). However, it has also been shown that using artificially generated data as a replacement for nonnative error-corrected data can lead to poorer performance (Sj¨obergh and Knutsson, 2005; Foster and Andersen, 2009). This would suggest that artificial errors are more useful than native data but less useful than corrected non-native data. Rozovskaya and Roth also control other variables in their experiments. On the one hand, they only evaluate their systems on sentences that have no spelling mistakes so as to avoid degrading performance. This is particularly important when training classifiers on features extracted with linguistic tools (such as parsers or taggers) as they could provide inaccurate results for malformed input. On the other hand, the authors work on a limited set of error types (mainly arti</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Jennifer Foster and Øistein Andersen. 2009. Generrate: Generating errors for use in grammatical error detection. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>233--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Harriman, New York.</location>
<contexts>
<context position="19370" citStr="Gale et al., 1992" startWordPosition="2936" endWordPosition="2939">this reason, we introduce probabilities for each word sense in an attempt to capture more accurate usage. As an example, consider a hypothetical situation in which a group of learners confuse prepositions used with the word bank as a financial institution but they produce the right preposition when it refers to a river bed: P(prep=in|noun=bank1) = 0.76 P(prep=at|noun=bank1) = 0.18 P(prep=on|noun=bank1) = 0.06 P(prep=on|noun=bank2) = 1.00 Although it is rare that occurrences of the same word will refer to different meanings within a document (the so-called ‘one sense per discourse’ assumption (Gale et al., 1992)), this is not the case when large corpora containing different documents are used for characterising and generating errors. In such scenarios, word sense disambiguation should produce more accurate results. Table 1 lists the actual probabilities computed from each type of information and the errors they are able to generate. 4 Experimental setup 4.1 Corpora and tools We use the NUCLE v2.3 corpus (Dahlmeier et al., 2013) released for the CoNLL 2013 shared task on error correction, which comprises errorannotated essays written in English by students at the National University of Singapore. Thes</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 233– 237, Harriman, New York. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Kugatsu Sadamitsu</author>
<author>Hitoshi Nishikawa</author>
</authors>
<title>Grammar error correction using pseudo-error sentences and domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>388--392</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="10371" citStr="Imamura et al. (2012)" startWordPosition="1596" endWordPosition="1599">mance. This is particularly important when training classifiers on features extracted with linguistic tools (such as parsers or taggers) as they could provide inaccurate results for malformed input. On the other hand, the authors work on a limited set of error types (mainly articles and prepositions) which are closed word classes and therefore have reduced confusion sets. Thus, it becomes interesting to investigate how their ideas extrapolate to open-class error types, like verb form or content word errors. Their probabilistic error generation approach has also been used by other researchers. Imamura et al. (2012), for example, applied this method to generate artificial incorrect sentences for Japanese particle correction with an inflation factor ranging from 0.0 (no errors) to 2.0 (double error rates). Their results show that the performance of artificial corpora depends largely on the inflation rate but can achieve good results when domain adaptation is applied. In a more exhaustive study, Cahill et al. (2013) investigate the usefulness of automaticallycompiled sentences from Wikipedia revisions for correcting preposition errors. A number of classifiers are trained using error-free text, automaticall</context>
</contexts>
<marker>Imamura, Saito, Sadamitsu, Nishikawa, 2012</marker>
<rawString>Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa. 2012. Grammar error correction using pseudo-error sentences and domain adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 388–392, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Toyomi Saiga</author>
<author>Thepchai Supnithi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Automatic Error Detection in the Japanese Learners’ English Spoken Data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 2, ACL ’03,</booktitle>
<pages>145--148</pages>
<institution>Sapporo, Japan. Association for Computational Linguistics.</institution>
<contexts>
<context position="3266" citStr="Izumi et al. (2003)" startWordPosition="501" endWordPosition="504">classes) to characterise contexts of naturally occurring errors and replicate them in errorfree text. Second, we apply our technique to a larger number of error types than any other previous approach, including open-class errors. The resulting datasets are used to train error correction systems aimed at learners of English as a second language (ESL). Finally, we provide a detailed description of the variables that affect artificial error generation. 2 Related work The use of artificial data to train error correction systems has been explored by other researchers using a variety of techniques. Izumi et al. (2003), for example, use artificial errors to target article mistakes made by Japanese learners of English. A corpus is created by replacing a, an, the or the zero article by a different article chosen at random in more than 7,500 correct sentences and used to train a maximum entropy model. Results show an improvement for omission errors but no change for replacement errors. Brockett et al. (2006) describe the use of a statistical machine translation (SMT) system for correcting a set of 14 countable/uncountable nouns 116 Proceedings of the Student Research Workshop at the 14th Conference of the Euro</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic Error Detection in the Japanese Learners’ English Spoken Data. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 2, ACL ’03, pages 145– 148, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran,</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<marker>Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="26142" citStr="Koehn, 2010" startWordPosition="3951" endWordPosition="3952"> 0.2492 0.0986 0.1413 181 1502 469 0.2785 0.1075 0.1552 NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602 Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2 Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested by each system. Results in bold show improvements over the baseline. correct sentences and the target side contains their corrected versions. Our setup is similar to the one described by Yuan and Felice (2013) in that we train a PoS-factored phrase-based model (Koehn, 2010) using Moses (Koehn et al., 2007), Giza++ (Och and Ney, 2003) for word alignment and the IRSTLM Toolkit (Federico et al., 2008) for language modelling. However, unlike them, we do not optimise decoding parameters but use default values instead. We build 11 different systems in total: a baseline system using only the NUCLE training set, one system per artificial corpus and other additional systems using combinations of the NUCLE training data and our artificial corpora. Each of these systems uses a single translation model that tackles all error types at the same time. 5 Results Each system was</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Kunchukuttan</author>
<author>Ritesh Shah</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Iitb system for conll 2013 shared task: A hybrid approach to grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>82--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32555" citStr="Kunchukuttan et al., 2013" startWordPosition="5041" endWordPosition="5044">. (Prep) and NUCLE+SC (SVA/Vform). As expected, the use of alternative annotations in the revised test set improves results but it does not reveal any qualitative difference between datasets. Finally, when compared to other systems in the CoNLL 2013 shared task in terms of F1, our best systems would rank 9th on both test sets. This would suggest that using an off-the-shelf SMT system trained on a combination of real and artificial data can yield better results than other machine learning techniques (Yi et al., 2013; van den Bosch and Berck, 2013; Berend et al., 2013) or rule-based approaches (Kunchukuttan et al., 2013; Putra and Szabo, 2013; Flickinger and Yu, 2013; Sidorov et al., 2013). 6 Conclusions This paper presents early results on the generation and use of artificial errors for grammatical error correction. Our approach uses conditional probabilities derived from an ESL error-annotated corpus to replicate errors in native error-free data. Unlike previous work, we propose using linguistic information such as PoS or sense disambiguation to refine the contexts where errors occur and thus replicate them more accurately. We use five different types of information to generate our artificial corpora, whic</context>
</contexts>
<marker>Kunchukuttan, Shah, Bhattacharyya, 2013</marker>
<rawString>Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhattacharyya. 2013. Iitb system for conll 2013 shared task: A hybrid approach to grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 82–87, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="7122" citStr="Leacock et al., 2010" startWordPosition="1091" endWordPosition="1094">ing from 0.05 to 0.18). Each new word is chosen uniformly at random. Distribution before correction (in ESL data) Target words in the error-free text are changed to match the distribution observed in ESL errorannotated data before any correction is made. Distribution after correction (in ESL data) Target words in the error-free text are changed to match the distribution observed in ESL errorannotated data after corrections are made. Native language-specific distributions It has been observed that second language production is affected by a learner’s native language (L1) (Lee and Seneff, 2008; Leacock et al., 2010). A common example is the difficulty in using English articles appropriately by learners whose L1 has no article system, such as Russian or Japanese. Because word choice errors follow systematic patterns (i.e. they do not occur randomly), this information is extremely valuable for generating errors more accurately. L1-specific errors can be imitated by computing word confusions in an error-annotated ESL corpora and using these distributions to change target words accordingly in error-free text. More specifically, if we estimate P(source|target) in an error-tagged corpus (i.e. the probability o</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>An analysis of grammatical errors in non-native speech in English.</title>
<date>2008</date>
<booktitle>In Amitava Das and Srinivas Bangalore, editors, Proceedings of the 2008 IEEE Spoken Language Technology Workshop, SLT 2008,</booktitle>
<pages>89--92</pages>
<publisher>IEEE.</publisher>
<location>Goa, India,</location>
<contexts>
<context position="7099" citStr="Lee and Seneff, 2008" startWordPosition="1087" endWordPosition="1090">th probability x (varying from 0.05 to 0.18). Each new word is chosen uniformly at random. Distribution before correction (in ESL data) Target words in the error-free text are changed to match the distribution observed in ESL errorannotated data before any correction is made. Distribution after correction (in ESL data) Target words in the error-free text are changed to match the distribution observed in ESL errorannotated data after corrections are made. Native language-specific distributions It has been observed that second language production is affected by a learner’s native language (L1) (Lee and Seneff, 2008; Leacock et al., 2010). A common example is the difficulty in using English articles appropriately by learners whose L1 has no article system, such as Russian or Japanese. Because word choice errors follow systematic patterns (i.e. they do not occur randomly), this information is extremely valuable for generating errors more accurately. L1-specific errors can be imitated by computing word confusions in an error-annotated ESL corpora and using these distributions to change target words accordingly in error-free text. More specifically, if we estimate P(source|target) in an error-tagged corpus </context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008. An analysis of grammatical errors in non-native speech in English. In Amitava Das and Srinivas Bangalore, editors, Proceedings of the 2008 IEEE Spoken Language Technology Workshop, SLT 2008, pages 89–92, Goa, India, December. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Bo Han</author>
<author>Kuan Li</author>
<author>Stephan Hyeonjun Stiller</author>
<author>Ming Zhou</author>
</authors>
<title>SRL-based verb selection for ESL.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1068--1076</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="35987" citStr="Liu et al., 2010" startWordPosition="5592" endWordPosition="5595">mplex error types, such as the use of pronouns, which often requires analysing coreference chains. Using new linguistic information In this work we have used five types of linguistic information. However, we believe other types of information and their associated probabilities could also be useful, especially if we aim to correct more error types. Examples include spelling, grammatical relations (dependencies) and word order (syntax). Additionally, we believe the use of semantic role labels can be explored as an alternative to semantic classes, as they have proved useful for error correction (Liu et al., 2010). Mixed error generation In our current experiments, each artificial corpus is generated using only one type of information at a time. However, having found that certain types of information are more suitable than others for correcting specific error types (see Tables 5 and 6), we believe better artificial corpora could be created by generating instances of each error type using only the most appropriate linguistic information. Acknowledgments We would like to thank Prof Ted Briscoe for his valuable comments and suggestions as well as Cambridge English Language Assessment, a division of Cambri</context>
</contexts>
<marker>Liu, Han, Li, Stiller, Zhou, 2010</marker>
<rawString>Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun Stiller, and Ming Zhou. 2010. SRL-based verb selection for ESL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068– 1076, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="23065" citStr="Miller, 1995" startWordPosition="3464" endWordPosition="3465"> Essays in the NUCLE corpus are written by advanced university students and are therefore comparable to standard English Wikipedia articles. For less sophisticated language, the Simple English Wikipedia could be an alternative. Native language: English Wikipedia articles are mostly written by native speakers whereas NUCLE essays are not. This is the only discordant variable. PoS tagging was performed using RASP (Briscoe et al., 2006). Word sense disambiguation was carried out using the WordNet::SenseRelate:AllWords Perl module (Pedersen and Kolhatkar, 2009) which assigns a sense from WordNet (Miller, 1995) to each content word in a text. As for semantic information, we use WordNet classes which are readily available in NLTK (Bird et al., 2009). WordNet classes respond to a classification in lexicographers’ files4 and are defined for content words as shown in Table 2, depending on their location in the hierarchy. 4http://wordnet.princeton.edu/man/ lexnames.5WN.html Part of speech WordNet classification Adjective all, pertainyms, participial Adverb all Noun act, animal, artifact, attribute, body, cognition, communication, event, feeling, food, group, location, motive, object, person, phenomenon, </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Mamoru Komachi</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>147--155</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="1769" citStr="Mizumoto et al., 2011" startWordPosition="259" endWordPosition="262">re a considerable amount of annotated data which is difficult to obtain. Available error-annotated corpora are often focused on particular groups of people (e.g. nonnative students), error types (e.g. spelling, syntax), genres (e.g. university essays, letters) or topics so it is not clear how representative they are or how well systems based on them will generalise. On the other hand, building new corpora is not always a viable solution since error annotation is expensive. As a result, researchers have tried to overcome these limitations either by compiling corpora automatically from the web (Mizumoto et al., 2011; Tajiri et al., 2012; Cahill et al., 2013) or using artificial corpora which are cheaper to produce and can be tailored to their needs. Artificial error generation allows researchers to create very large error-annotated corpora with little effort and control variables such as topic and error types. Errors can be injected into candidate texts using a deterministic approach (e.g. fixed rules) or probabilities derived from manually annotated samples in order to mimic real data. Although artificial errors have been used in previous work, we present a new approach based on linguistic information a</context>
</contexts>
<marker>Mizumoto, Komachi, Nagata, Matsumoto, 2011</marker>
<rawString>Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 147–155, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The CoNLL-2013 Shared Task on Grammatical Error Correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2493" citStr="Ng et al., 2013" startWordPosition="377" endWordPosition="380"> be tailored to their needs. Artificial error generation allows researchers to create very large error-annotated corpora with little effort and control variables such as topic and error types. Errors can be injected into candidate texts using a deterministic approach (e.g. fixed rules) or probabilities derived from manually annotated samples in order to mimic real data. Although artificial errors have been used in previous work, we present a new approach based on linguistic information and evaluate it using the test data provided for the CoNLL 2013 shared task on grammatical error correction (Ng et al., 2013). Our work makes the following contributions. First, we are the first to use linguistic information (such as part-of-speech (PoS) information or semantic classes) to characterise contexts of naturally occurring errors and replicate them in errorfree text. Second, we apply our technique to a larger number of error types than any other previous approach, including open-class errors. The resulting datasets are used to train error correction systems aimed at learners of English as a second language (ESL). Finally, we provide a detailed description of the variables that affect artificial error gene</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26203" citStr="Och and Ney, 2003" startWordPosition="3960" endWordPosition="3963"> NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602 Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2 Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested by each system. Results in bold show improvements over the baseline. correct sentences and the target side contains their corrected versions. Our setup is similar to the one described by Yuan and Felice (2013) in that we train a PoS-factored phrase-based model (Koehn, 2010) using Moses (Koehn et al., 2007), Giza++ (Och and Ney, 2003) for word alignment and the IRSTLM Toolkit (Federico et al., 2008) for language modelling. However, unlike them, we do not optimise decoding parameters but use default values instead. We build 11 different systems in total: a baseline system using only the NUCLE training set, one system per artificial corpus and other additional systems using combinations of the NUCLE training data and our artificial corpora. Each of these systems uses a single translation model that tackles all error types at the same time. 5 Results Each system was evaluated in terms of precision, recall and F1 on the NUCLE </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ted Pedersen</author>
<author>Varada Kolhatkar</author>
</authors>
<title>WordNet::SenseRelate::AllWords: a broad coverage word sense tagger that maximizes semantic relatedness.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Demonstration Session, NAACL-Demonstrations ’9,</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="23015" citStr="Pedersen and Kolhatkar, 2009" startWordPosition="3454" endWordPosition="3457">ritten, academic and formal. Text complexity/language proficiency: Essays in the NUCLE corpus are written by advanced university students and are therefore comparable to standard English Wikipedia articles. For less sophisticated language, the Simple English Wikipedia could be an alternative. Native language: English Wikipedia articles are mostly written by native speakers whereas NUCLE essays are not. This is the only discordant variable. PoS tagging was performed using RASP (Briscoe et al., 2006). Word sense disambiguation was carried out using the WordNet::SenseRelate:AllWords Perl module (Pedersen and Kolhatkar, 2009) which assigns a sense from WordNet (Miller, 1995) to each content word in a text. As for semantic information, we use WordNet classes which are readily available in NLTK (Bird et al., 2009). WordNet classes respond to a classification in lexicographers’ files4 and are defined for content words as shown in Table 2, depending on their location in the hierarchy. 4http://wordnet.princeton.edu/man/ lexnames.5WN.html Part of speech WordNet classification Adjective all, pertainyms, participial Adverb all Noun act, animal, artifact, attribute, body, cognition, communication, event, feeling, food, gro</context>
</contexts>
<marker>Pedersen, Kolhatkar, 2009</marker>
<rawString>Ted Pedersen and Varada Kolhatkar. 2009. WordNet::SenseRelate::AllWords: a broad coverage word sense tagger that maximizes semantic relatedness. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Demonstration Session, NAACL-Demonstrations ’9, pages 17–20, Boulder, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Darma Putra</author>
<author>Lili Szabo</author>
</authors>
<title>Uds at conll 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32578" citStr="Putra and Szabo, 2013" startWordPosition="5045" endWordPosition="5048">Vform). As expected, the use of alternative annotations in the revised test set improves results but it does not reveal any qualitative difference between datasets. Finally, when compared to other systems in the CoNLL 2013 shared task in terms of F1, our best systems would rank 9th on both test sets. This would suggest that using an off-the-shelf SMT system trained on a combination of real and artificial data can yield better results than other machine learning techniques (Yi et al., 2013; van den Bosch and Berck, 2013; Berend et al., 2013) or rule-based approaches (Kunchukuttan et al., 2013; Putra and Szabo, 2013; Flickinger and Yu, 2013; Sidorov et al., 2013). 6 Conclusions This paper presents early results on the generation and use of artificial errors for grammatical error correction. Our approach uses conditional probabilities derived from an ESL error-annotated corpus to replicate errors in native error-free data. Unlike previous work, we propose using linguistic information such as PoS or sense disambiguation to refine the contexts where errors occur and thus replicate them more accurately. We use five different types of information to generate our artificial corpora, which are later evaluated i</context>
</contexts>
<marker>Putra, Szabo, 2013</marker>
<rawString>Desmond Darma Putra and Lili Szabo. 2013. Uds at conll 2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 88–95, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>154--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California.</location>
<contexts>
<context position="9187" citStr="Rozovskaya and Roth, 2010" startWordPosition="1406" endWordPosition="1409">n order to address this issue during artificial error generation, Rozovskaya et al. (2012) propose an inflation method that boosts confusion probabilities in order to generate a larger proportion of artificial instances. This reformulation is shown to improve F-scores when correcting determiners and prepositions. Experiments reveal that these approaches yield better results than assuming uniform probabilistic distributions where all errors and corrections are equally likely. In particular, classifiers trained on artificially generated data outperformed those trained on native error-free text (Rozovskaya and Roth, 2010a; Rozovskaya and Roth, 2011). However, it has also been shown that using artificially generated data as a replacement for nonnative error-corrected data can lead to poorer performance (Sj¨obergh and Knutsson, 2005; Foster and Andersen, 2009). This would suggest that artificial errors are more useful than native data but less useful than corrected non-native data. Rozovskaya and Roth also control other variables in their experiments. On the one hand, they only evaluate their systems on sentences that have no spelling mistakes so as to avoid degrading performance. This is particularly important</context>
<context position="15167" citStr="Rozovskaya and Roth (2010" startWordPosition="2318" endWordPosition="2321">could be assessed automatically. For example, topics could be estimated using text similarity measures, genres could be predicted using structural information and L1s could be inferred using a native language identifier.3 For an analysis of other variables such as domain and error distributions, the reader should refer to Cahill et al. (2013). 3See the First Edition of the Shared Task on Native Language Identification (Tetreault et al., 2013) at https:// sites.google.com/site/nlisharedtask2013/ 3.2 Error replication Our approach to artificial error generation is similar to the one proposed by Rozovskaya and Roth (2010a) in that we also estimate probabilities in a corpus of ESL learners which are then used to distort error-free text. However, unlike them, we refine our probabilities by imposing restrictions on the linguistic functions of the words and the contexts where they occur. Because we extend generation to open-class error types (such as verb form errors), this refinement becomes necessary to overcome disambiguation issues and lead to more accurate replication. Our work is the first to exploit linguistic information for error generation, as described below. Error type distributions We compute the pro</context>
<context position="28196" citStr="Rozovskaya and Roth (2010" startWordPosition="4290" endWordPosition="4293">xcept for NUCLE+SC. It could be argued that the reason for this improvement is corpus size, since our hybrid datasets are double the size of each individual set, but the small differences in precision between the ED and POS datasets and their corresponding hybrid versions seem to contradict that hypothesis. In fact, results would suggest artificial and naturally occurring errors are not interchangeable but rather complementary. The observed improvement in precision, however, comes at the expense of recall, for which none of the systems is able to beat the baseline. This contradicts results by Rozovskaya and Roth (2010a), who show their error inflation method increases recall, although this could be due to differences in the training paradigm and data. Still, results are encouraging since precision is generally preferred over recall in error correction scenarios (Yuan and Felice, 2013). We also evaluated performance by error type on the original (Table 5) and revised (Table 6) test data using an estimation approach similar to the one in CoNLL 2013. Results show that the performance of each dataset varies by error type, suggesting that certain types of information are better suited for specific error types. </context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010a. Training paradigms for correcting errors in grammar and usage. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 154–162, Los Angeles, California. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>961--970</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="9187" citStr="Rozovskaya and Roth, 2010" startWordPosition="1406" endWordPosition="1409">n order to address this issue during artificial error generation, Rozovskaya et al. (2012) propose an inflation method that boosts confusion probabilities in order to generate a larger proportion of artificial instances. This reformulation is shown to improve F-scores when correcting determiners and prepositions. Experiments reveal that these approaches yield better results than assuming uniform probabilistic distributions where all errors and corrections are equally likely. In particular, classifiers trained on artificially generated data outperformed those trained on native error-free text (Rozovskaya and Roth, 2010a; Rozovskaya and Roth, 2011). However, it has also been shown that using artificially generated data as a replacement for nonnative error-corrected data can lead to poorer performance (Sj¨obergh and Knutsson, 2005; Foster and Andersen, 2009). This would suggest that artificial errors are more useful than native data but less useful than corrected non-native data. Rozovskaya and Roth also control other variables in their experiments. On the one hand, they only evaluate their systems on sentences that have no spelling mistakes so as to avoid degrading performance. This is particularly important</context>
<context position="15167" citStr="Rozovskaya and Roth (2010" startWordPosition="2318" endWordPosition="2321">could be assessed automatically. For example, topics could be estimated using text similarity measures, genres could be predicted using structural information and L1s could be inferred using a native language identifier.3 For an analysis of other variables such as domain and error distributions, the reader should refer to Cahill et al. (2013). 3See the First Edition of the Shared Task on Native Language Identification (Tetreault et al., 2013) at https:// sites.google.com/site/nlisharedtask2013/ 3.2 Error replication Our approach to artificial error generation is similar to the one proposed by Rozovskaya and Roth (2010a) in that we also estimate probabilities in a corpus of ESL learners which are then used to distort error-free text. However, unlike them, we refine our probabilities by imposing restrictions on the linguistic functions of the words and the contexts where they occur. Because we extend generation to open-class error types (such as verb form errors), this refinement becomes necessary to overcome disambiguation issues and lead to more accurate replication. Our work is the first to exploit linguistic information for error generation, as described below. Error type distributions We compute the pro</context>
<context position="28196" citStr="Rozovskaya and Roth (2010" startWordPosition="4290" endWordPosition="4293">xcept for NUCLE+SC. It could be argued that the reason for this improvement is corpus size, since our hybrid datasets are double the size of each individual set, but the small differences in precision between the ED and POS datasets and their corresponding hybrid versions seem to contradict that hypothesis. In fact, results would suggest artificial and naturally occurring errors are not interchangeable but rather complementary. The observed improvement in precision, however, comes at the expense of recall, for which none of the systems is able to beat the baseline. This contradicts results by Rozovskaya and Roth (2010a), who show their error inflation method increases recall, although this could be due to differences in the training paradigm and data. Still, results are encouraging since precision is generally preferred over recall in error correction scenarios (Yuan and Felice, 2013). We also evaluated performance by error type on the original (Table 5) and revised (Table 6) test data using an estimation approach similar to the one in CoNLL 2013. Results show that the performance of each dataset varies by error type, suggesting that certain types of information are better suited for specific error types. </context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010b. Generating confusion sets for context-sensitive error correction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 961–970, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>924--933</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="9216" citStr="Rozovskaya and Roth, 2011" startWordPosition="1410" endWordPosition="1413">e during artificial error generation, Rozovskaya et al. (2012) propose an inflation method that boosts confusion probabilities in order to generate a larger proportion of artificial instances. This reformulation is shown to improve F-scores when correcting determiners and prepositions. Experiments reveal that these approaches yield better results than assuming uniform probabilistic distributions where all errors and corrections are equally likely. In particular, classifiers trained on artificially generated data outperformed those trained on native error-free text (Rozovskaya and Roth, 2010a; Rozovskaya and Roth, 2011). However, it has also been shown that using artificially generated data as a replacement for nonnative error-corrected data can lead to poorer performance (Sj¨obergh and Knutsson, 2005; Foster and Andersen, 2009). This would suggest that artificial errors are more useful than native data but less useful than corrected non-native data. Rozovskaya and Roth also control other variables in their experiments. On the one hand, they only evaluate their systems on sentences that have no spelling mistakes so as to avoid degrading performance. This is particularly important when training classifiers on</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 924–933, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>The UI system in the HOO 2012 shared task on error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>272--280</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="8652" citStr="Rozovskaya et al. (2012)" startWordPosition="1332" endWordPosition="1335">sition for should be used (that is, P(source=to|target=for)=0.10), we can replicate this error pattern by replacing the occurrences of the preposition for with to with a probability of 0.10 in a corpus of error-free sentences. When the source and target words are the same, P(source=x|target=x) expresses the probability that a learner produces the correct/expected word. Because errors are generally sparse (and therefore error rates are low), replicating mistakes based on observed probabilities can easily lead to 117 low recall. In order to address this issue during artificial error generation, Rozovskaya et al. (2012) propose an inflation method that boosts confusion probabilities in order to generate a larger proportion of artificial instances. This reformulation is shown to improve F-scores when correcting determiners and prepositions. Experiments reveal that these approaches yield better results than assuming uniform probabilistic distributions where all errors and corrections are equally likely. In particular, classifiers trained on artificially generated data outperformed those trained on native error-free text (Rozovskaya and Roth, 2010a; Rozovskaya and Roth, 2011). However, it has also been shown th</context>
<context position="24512" citStr="Rozovskaya et al., 2012" startWordPosition="3665" endWordPosition="3668">e, weather Table 2: WordNet classes for content words. Name Composition ED errors based on error type distributions MORPH errors based on morphology POS errors based on PoS disambiguation SC errors based on semantic classes WSD errors based on word senses Table 3: Generated artificial corpora based on different types of linguistic information. 4.2 Error generation For each type of information in Table 1, we compute the corresponding conditional probabilities using the NUCLE training set. These probabilities are then used to generate six different artificial corpora using the inflation method (Rozovskaya et al., 2012), as listed in Table 3. 4.3 System training We approach the error correction task as a translation problem from incorrect into correct English. Systems are built using an SMT framework and different combinations of NUCLE and our artificial corpora, where the source side contains in121 Original Revised C M U P R F1 C M U P R F1 NUCLE (baseline) 181 1462 513 0.2608 0.1102 0.1549 200 1483 495 0.2878 0.1188 0.1682 ED 53 1590 150 0.2611 0.0323 0.0574 62 1621 141 0.3054 0.0368 0.0657 MORPH 74 1569 333 0.1818 0.0450 0.0722 83 1600 324 0.2039 0.0493 0.0794 POS 42 1601 99 0.2979 0.0256 0.0471 42 1641 9</context>
</contexts>
<marker>Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. The UI system in the HOO 2012 shared task on error correction. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 272–280, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Kai-Wei Chang</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>The University of Illinois System in the CoNLL-2013 Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>13--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11548" citStr="Rozovskaya et al. (2013)" startWordPosition="1770" endWordPosition="1773">are trained using error-free text, automatically-compiled annotated corpora and artificial sentences generated using error probabilities derived from Wikipedia revisions and Lang8.2 Their results reveal a number of interesting points, namely that artificial errors provide competitive results and perform robustly across different test sets. A learning curve analysis also shows system performance increases as more training data is used, both real and artificial. More recently, some teams have also reported improvements by using artificial data in their submissions to the CoNLL 2013 shared task. Rozovskaya et al. (2013) apply their inflation method to train a classifier for determiner errors that achieves state-of-the-art performance while Yuan and Felice (2013) use naively-generated artificial errors within an SMT framework that places them third in terms of precision. 3 Advanced generation of artificial errors Our work is based on the hypothesis that using carefully generated artificial errors improves the performance of error correction systems. This implies generating errors in a way that resembles available error-annotated data, using similar texts and accurate injection methods. Like other probabilisti</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, and Dan Roth. 2013. The University of Illinois System in the CoNLL-2013 Shared Task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13–19, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Anubhav Gupta</author>
<author>Martin Tozer</author>
<author>Dolors Catala</author>
<author>Angels Catena</author>
<author>Sandrine Fuentes</author>
</authors>
<title>Rule-based system for automatic grammar correction using syntactic n-grams for english language learning (l2).</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>96--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32626" citStr="Sidorov et al., 2013" startWordPosition="5053" endWordPosition="5056">tations in the revised test set improves results but it does not reveal any qualitative difference between datasets. Finally, when compared to other systems in the CoNLL 2013 shared task in terms of F1, our best systems would rank 9th on both test sets. This would suggest that using an off-the-shelf SMT system trained on a combination of real and artificial data can yield better results than other machine learning techniques (Yi et al., 2013; van den Bosch and Berck, 2013; Berend et al., 2013) or rule-based approaches (Kunchukuttan et al., 2013; Putra and Szabo, 2013; Flickinger and Yu, 2013; Sidorov et al., 2013). 6 Conclusions This paper presents early results on the generation and use of artificial errors for grammatical error correction. Our approach uses conditional probabilities derived from an ESL error-annotated corpus to replicate errors in native error-free data. Unlike previous work, we propose using linguistic information such as PoS or sense disambiguation to refine the contexts where errors occur and thus replicate them more accurately. We use five different types of information to generate our artificial corpora, which are later evaluated in isolation as well as coupled to the original E</context>
</contexts>
<marker>Sidorov, Gupta, Tozer, Catala, Catena, Fuentes, 2013</marker>
<rawString>Grigori Sidorov, Anubhav Gupta, Martin Tozer, Dolors Catala, Angels Catena, and Sandrine Fuentes. 2013. Rule-based system for automatic grammar correction using syntactic n-grams for english language learning (l2). In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 96–101, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Sj¨obergh</author>
<author>Ola Knutsson</author>
</authors>
<title>Faking errors to avoid making errors: Very weakly supervised learning for error detection in writing.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<pages>506--512</pages>
<location>Borovets, Bulgaria,</location>
<marker>Sj¨obergh, Knutsson, 2005</marker>
<rawString>Jonas Sj¨obergh and Ola Knutsson. 2005. Faking errors to avoid making errors: Very weakly supervised learning for error detection in writing. In Proceedings of RANLP 2005, pages 506–512, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshikazu Tajiri</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Tense and aspect error correction for esl learners using global context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>198--202</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1790" citStr="Tajiri et al., 2012" startWordPosition="263" endWordPosition="266">t of annotated data which is difficult to obtain. Available error-annotated corpora are often focused on particular groups of people (e.g. nonnative students), error types (e.g. spelling, syntax), genres (e.g. university essays, letters) or topics so it is not clear how representative they are or how well systems based on them will generalise. On the other hand, building new corpora is not always a viable solution since error annotation is expensive. As a result, researchers have tried to overcome these limitations either by compiling corpora automatically from the web (Mizumoto et al., 2011; Tajiri et al., 2012; Cahill et al., 2013) or using artificial corpora which are cheaper to produce and can be tailored to their needs. Artificial error generation allows researchers to create very large error-annotated corpora with little effort and control variables such as topic and error types. Errors can be injected into candidate texts using a deterministic approach (e.g. fixed rules) or probabilities derived from manually annotated samples in order to mimic real data. Although artificial errors have been used in previous work, we present a new approach based on linguistic information and evaluate it using </context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>Toshikazu Tajiri, Mamoru Komachi, and Yuji Matsumoto. 2012. Tense and aspect error correction for esl learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 198–202, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A Report on the First Native Language Identification Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="14988" citStr="Tetreault et al., 2013" startWordPosition="2294" endWordPosition="2297">difficult, which is why most approaches resort to native English text. In our experiments, the aforementioned variables are manually controlled although we believe many of them could be assessed automatically. For example, topics could be estimated using text similarity measures, genres could be predicted using structural information and L1s could be inferred using a native language identifier.3 For an analysis of other variables such as domain and error distributions, the reader should refer to Cahill et al. (2013). 3See the First Edition of the Shared Task on Native Language Identification (Tetreault et al., 2013) at https:// sites.google.com/site/nlisharedtask2013/ 3.2 Error replication Our approach to artificial error generation is similar to the one proposed by Rozovskaya and Roth (2010a) in that we also estimate probabilities in a corpus of ESL learners which are then used to distort error-free text. However, unlike them, we refine our probabilities by imposing restrictions on the linguistic functions of the words and the contexts where they occur. Because we extend generation to open-class error types (such as verb form errors), this refinement becomes necessary to overcome disambiguation issues a</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A Report on the First Native Language Identification Shared Task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48–57, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Peter Berck</author>
</authors>
<title>Memorybased grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>102--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>van den Bosch, Berck, 2013</marker>
<rawString>Antal van den Bosch and Peter Berck. 2013. Memorybased grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 102–108, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bong-Jun Yi</author>
<author>Ho-Chang Lee</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Kunlp grammatical error correction system for conll-2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>123--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32450" citStr="Yi et al., 2013" startWordPosition="5024" endWordPosition="5027">e main categories suggested by each system. Results in bold show improvements over the baseline. (Prep) and NUCLE+SC (SVA/Vform). As expected, the use of alternative annotations in the revised test set improves results but it does not reveal any qualitative difference between datasets. Finally, when compared to other systems in the CoNLL 2013 shared task in terms of F1, our best systems would rank 9th on both test sets. This would suggest that using an off-the-shelf SMT system trained on a combination of real and artificial data can yield better results than other machine learning techniques (Yi et al., 2013; van den Bosch and Berck, 2013; Berend et al., 2013) or rule-based approaches (Kunchukuttan et al., 2013; Putra and Szabo, 2013; Flickinger and Yu, 2013; Sidorov et al., 2013). 6 Conclusions This paper presents early results on the generation and use of artificial errors for grammatical error correction. Our approach uses conditional probabilities derived from an ESL error-annotated corpus to replicate errors in native error-free data. Unlike previous work, we propose using linguistic information such as PoS or sense disambiguation to refine the contexts where errors occur and thus replicate </context>
</contexts>
<marker>Yi, Lee, Rim, 2013</marker>
<rawString>Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim. 2013. Kunlp grammatical error correction system for conll-2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 123–127, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Yuan</author>
<author>Mariano Felice</author>
</authors>
<title>Constrained grammatical error correction using statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>52--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11693" citStr="Yuan and Felice (2013)" startWordPosition="1790" endWordPosition="1793">om Wikipedia revisions and Lang8.2 Their results reveal a number of interesting points, namely that artificial errors provide competitive results and perform robustly across different test sets. A learning curve analysis also shows system performance increases as more training data is used, both real and artificial. More recently, some teams have also reported improvements by using artificial data in their submissions to the CoNLL 2013 shared task. Rozovskaya et al. (2013) apply their inflation method to train a classifier for determiner errors that achieves state-of-the-art performance while Yuan and Felice (2013) use naively-generated artificial errors within an SMT framework that places them third in terms of precision. 3 Advanced generation of artificial errors Our work is based on the hypothesis that using carefully generated artificial errors improves the performance of error correction systems. This implies generating errors in a way that resembles available error-annotated data, using similar texts and accurate injection methods. Like other probabilistic approaches, our method assumes we have access to an error-corrected reference corpus from which we can compute error generation probabilities. </context>
<context position="26077" citStr="Yuan and Felice (2013)" startWordPosition="3939" endWordPosition="3942">.3100 0.0998 0.1510 182 1501 347 0.3440 0.1081 0.1646 NUCLE+SC 162 1481 488 0.2492 0.0986 0.1413 181 1502 469 0.2785 0.1075 0.1552 NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602 Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2 Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested by each system. Results in bold show improvements over the baseline. correct sentences and the target side contains their corrected versions. Our setup is similar to the one described by Yuan and Felice (2013) in that we train a PoS-factored phrase-based model (Koehn, 2010) using Moses (Koehn et al., 2007), Giza++ (Och and Ney, 2003) for word alignment and the IRSTLM Toolkit (Federico et al., 2008) for language modelling. However, unlike them, we do not optimise decoding parameters but use default values instead. We build 11 different systems in total: a baseline system using only the NUCLE training set, one system per artificial corpus and other additional systems using combinations of the NUCLE training data and our artificial corpora. Each of these systems uses a single translation model that ta</context>
<context position="28468" citStr="Yuan and Felice, 2013" startWordPosition="4333" endWordPosition="4336">em to contradict that hypothesis. In fact, results would suggest artificial and naturally occurring errors are not interchangeable but rather complementary. The observed improvement in precision, however, comes at the expense of recall, for which none of the systems is able to beat the baseline. This contradicts results by Rozovskaya and Roth (2010a), who show their error inflation method increases recall, although this could be due to differences in the training paradigm and data. Still, results are encouraging since precision is generally preferred over recall in error correction scenarios (Yuan and Felice, 2013). We also evaluated performance by error type on the original (Table 5) and revised (Table 6) test data using an estimation approach similar to the one in CoNLL 2013. Results show that the performance of each dataset varies by error type, suggesting that certain types of information are better suited for specific error types. In particular, we find that on the original test set, ED achieves the highest precision for article and determiners, WSD maximises precision for prepositions and SC achieves the highest recall and F1. When using hybrid sets, results improve overall, with the highest preci</context>
</contexts>
<marker>Yuan, Felice, 2013</marker>
<rawString>Zheng Yuan and Mariano Felice. 2013. Constrained grammatical error correction using statistical machine translation. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52–61, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>