<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007883">
<title confidence="0.990553">
Speech-driven Access to the Deep Web on Mobile Devices
</title>
<author confidence="0.882335">
Taniya Mishra and Srinivas Bangalore
</author>
<affiliation confidence="0.754901">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.913474">
180 Park Avenue
Florham Park, NJ 07932 USA.
</address>
<email confidence="0.998384">
{taniya,srini}@research.att.com.
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753555555556">
The Deep Web is the collection of infor-
mation repositories that are not indexed
by search engines. These repositories are
typically accessible through web forms
and contain dynamically changing infor-
mation. In this paper, we present a sys-
tem that allows users to access such rich
repositories of information on mobile de-
vices using spoken language.
</bodyText>
<sectionHeader confidence="0.999377" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792870370371">
The World Wide Web (WWW) is the largest
repository of information known to mankind. It
is generally agreed that the WWW continues to
significantly enrich and transform our lives in un-
precedent ways. Be that as it may, the WWW that
we encounter is limited by the information that
is accessible through search engines. Search en-
gines, however, do not index a large portion of
WWW that is variously termed as the Deep Web,
Hidden Web, or Invisible Web.
Deep Web is the information that is in propri-
etory databases. Information in such databases is
usually more structured and changes at higher fre-
quency than textual web pages. It is conjectured
that the Deep Web is 500 times the size of the
surface web. Search engines are unable to index
this information and hence, unable to retrieve it
for the user who may be searching for such infor-
mation. So, the only way for users to access this
information is to find the appropriate web-form,
fill in the necessary search parameters, and use it
to query the database that contains the information
that is being searched for. Examples of such web
forms include, movie, train and bus times, and air-
line/hotel/restaurant reservations.
Contemporaneously, the devices to access infor-
mation have moved out of the office and home en-
vironment into the open world. The ubiquity of
mobile devices has made information access an
any time, any place activity. However, informa-
tion access using text input on mobile devices is te-
dious and unnatural because of the limited screen
space and the small (or soft) keyboards. In addi-
tion, by the mobile nature of these devices, users
often like to use them in hands-busy environments,
ruling out the possibility of typing text. Filling
web-forms using the small screens and tiny key-
boards of mobile devices is neither easy nor quick.
In this paper, we present a system, Qme!, de-
signed towards providing a spoken language inter-
face to the Deep Web. In its current form, Qme!
provides a unifed interface onn iPhone (shown in
Figure 1) that can be used by users to search for
static and dynamic questions. Static questions are
questions whose answers to these questions re-
main the same irrespective of when and where the
questions are asked. Examples of such questions
are What is the speed of light?, When is George
Washington’s birthday?. For static questions, the
system retrieves the answers from an archive of
human generated answers to questions. This en-
sures higher accuracy for the answers retrieved (if
found in the archive) and also allows us to retrieve
related questions on the user’s topic of interest.
</bodyText>
<figureCaption confidence="0.9617085">
Figure 1: Retrieval results for static and dynamic
questions using Qme!
</figureCaption>
<bodyText confidence="0.9924414">
Dynamic questions are questions whose an-
swers depend on when and where they are asked.
Examples of such questions are What is the stock
price of General Motors?, Who won the game last
night?, What is playing at the theaters near me?.
</bodyText>
<page confidence="0.982853">
60
</page>
<note confidence="0.6017015">
Proceedings of the ACL 2010 System Demonstrations, pages 60–65,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943857142857">
The answers to dynamic questions are often part of
the Deep Web. Our system retrieves the answers to
such dynamic questions by parsing the questions
to retrieve pertinent search keywords, which are in
turn used to query information databases accessi-
ble over the Internet using web forms. However,
the internal distinction between dynamic and static
questions, and the subsequent differential treat-
ment within the system is seamless to the user. The
user simply uses a single unified interface to ask a
question and receive a collection of answers that
potentially address her question directly.
The layout of the paper is as follows. In Sec-
tion 2, we present the system architecture. In
Section 3, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. In Section 4, we show how our system
retrieves answers to dynamic questions. In Sec-
tion 5, we show how our system retrieves answers
to static questions. We conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.886093" genericHeader="introduction">
2 Speech-driven Question Answer
System
</sectionHeader>
<bodyText confidence="0.986362115384615">
Speech-driven access to information has been a
popular application deployed by many compa-
nies on a variety of information resources (Mi-
crosoft, 2009; Google, 2009; YellowPages, 2009;
vlingo.com, 2009). In this prototype demonstra-
tion, we describe a speech-driven question-answer
application. The system architecture is shown in
Figure 2.
The user of this application provides a spoken
language query to a mobile device intending to
find an answer to the question. The speech recog-
nition module of the system recognizes the spo-
ken query. The result from the speech recognizer
can be either a single-best string or a weighted
word lattice.1 This textual output of recognition is
then used to classify the user query either as a dy-
namic query or a static query. If the user query is
static, the result of the speech recognizer is used to
search a large corpus of question-answer pairs to
retrieve the relevant answers. The retrieved results
are ranked using tf.idf based metric discussed in
Section 5. If the user query is dynamic, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com
for movie information). In Figure 1, we illustrate
the answers that Qme!returns for static and dy-
</bodyText>
<footnote confidence="0.8775218">
1For this paper, the ASR used to recognize these utter-
ances incorporates an acoustic model adapted to speech col-
lected from mobile devices and a four-gram language model
that is built from the corpus of questions.
namic questions.
</footnote>
<figureCaption confidence="0.944456">
Figure 2: The architecture of the speech-driven
question-answering system
</figureCaption>
<subsectionHeader confidence="0.987824">
2.1 Demonstration
</subsectionHeader>
<bodyText confidence="0.999907666666667">
In the demonstration, we plan to show the users
static and dynamic query handling on an iPhone
using spoken language queries. Users can use the
iphone and speak their queries using an interface
provided by Qme!. A Wi-Fi access spot will make
this demonstation more compelling.
</bodyText>
<sectionHeader confidence="0.992932" genericHeader="method">
3 Dynamic and Static Questions
</sectionHeader>
<bodyText confidence="0.999897483870968">
As mentioned in the introduction, dynamic ques-
tions require accessing the hidden web through a
web form with the appropriate parameters. An-
swers to dynamic questions cannot be preindexed
as can be done for static questions. They depend
on the time and geographical location of the ques-
tion. In dynamic questions, there may be no ex-
plicit reference to time, unlike the questions in the
TERQAS corpus (Radev and Sundheim., 2002)
which explicitly refer to the temporal properties
of the entities being questioned or the relative or-
dering of past and future events.
The time-dependency of a dynamic question
lies in the temporal nature of its answer. For exam-
ple, consider the question, What is the address of
the theater White Christmas is playing at in New
York?. White Christmas is a seasonal play that
plays in New York every year for a few weeks
in December and January, but not necessarily at
the same theater every year. So, depending when
this question is asked, the answer will be differ-
ent. If the question is asked in the summer, the
answer will be “This play is not currently playing
anywhere in NYC.” If the question is asked dur-
ing December, 2009, the answer might be different
than the answer given in December 2010, because
the theater at which White Christmas is playing
differs from 2009 to 2010.
There has been a growing interest in tempo-
ral analysis for question-answering since the late
1990’s. Early work on temporal expressions iden-
</bodyText>
<figure confidence="0.997363181818182">
Search
Rank
Classify
Match
Dynamic
Retrieve
from Web
Speech ASR 1−best
Lattice
Ranked Results
Q&amp;A corpus
</figure>
<page confidence="0.993536">
61
</page>
<bodyText confidence="0.999962466666667">
tification using a tagger culminated in the devel-
opment of TimeML (Pustejovsky et al., 2001),
a markup language for annotating temporal ex-
pressions and events in text. Other examples in-
clude, QA-by-Dossier with Constraints (Prager et
al., 2004), a method of improving QA accuracy by
asking auxiliary questions related to the original
question in order to temporally verify and restrict
the original answer. (Moldovan et al., 2005) detect
and represent temporally related events in natural
language using logical form representation. (Sa-
quete et al., 2009) use the temporal relations in a
question to decompose it into simpler questions,
the answers of which are recomposed to produce
the answers to the original question.
</bodyText>
<subsectionHeader confidence="0.999316">
3.1 Question Classification: Dynamic and
Static Questions
</subsectionHeader>
<bodyText confidence="0.999562141025641">
We automatically classify questions as dynamic
and static questions. The answers to static ques-
tions can be retrieved from the QA archive. To an-
swer dynamic questions, we query the database(s)
associated with the topic of the question through
web forms on the Internet. We first use a topic
classifier to detect the topic of a question followed
by a dynamic/static classifier trained on questions
related to a topic, as shown in Figure 3. For the
question what movies are playing around me?,
we detect it is a movie related dynamic ques-
tion and query a movie information web site (e.g.
www.fandango.com) to retrieve the results based
on the user’s GPS information.
Dynamic questions often contain temporal in-
dexicals, i.e., expressions of the form today, now,
this week, two summers ago, currently, recently,
etc. Our initial approach was to use such signal
words and phrases to automatically identify dy-
namic questions. The chosen signals were based
on annotations in TimeML. We also included spa-
tial indexicals, such as here and other clauses that
were observed to be contained in dynamic ques-
tions such as cost of, and how much is in the list of
signal phrases. These signals words and phrases
were encoded into a regular-expression-based rec-
ognizer.
This regular-expression based recognizer iden-
tified 3.5% of our dataset – which consisted of
several million questions – as dynamic. The type
of questions identified were What is playing in
the movie theaters tonight?, What is tomorrow’s
weather forecast for LA?, Where can I go to get
Thai food near here? However, random samplings
of the same dataset, annotated by four independent
human labelers, indicated that on average 13.5%
of the dataset is considered dynamic. This shows
that the temporal and spatial indexicals encoded as
a regular-expression based recognizer is unable to
identify a large percentage of the dynamic ques-
tions.
This approach leaves out dynamic questions
that do not contain temporal or spatial indexicals.
For example, What is playing at AMC Loew’s?, or
What is the score of the Chargers and Dolphines
game?. For such examples, considering the tense
of the verb in question may help. The last two ex-
amples are both in the present continuous tense.
But verb tense does not help for a question such
as Who got voted off Survivor?. This question is
certainly dynamic. The information that is most
likely being sought by this question is what is the
name of the person who got voted off the TV show
Survivor most recently, and not what is the name
of the person (or persons) who have gotten voted
off the Survivor at some point in the past.
Knowing the broad topic (such as movies, cur-
rent affairs, and music) of the question may be
very useful. It is likely that there may be many
dynamic questions about movies, sports, and fi-
nance, while history and geography may have few
or none. This idea is bolstered by the following
analysis. The questions in our dataset are anno-
tated with a broad topic tag. Binning the 3.5%
of our dataset identified as dynamic questions by
their broad topic produced a long-tailed distribu-
tion. Of the 104 broad topics, the top-5 topics con-
tained over 50% of the dynamic questions. These
top five topics were sports, TV and radio, events,
movies, and finance.
Considering the issues laid out in the previ-
ous section, our classification approach is to chain
two machine-learning-based classifiers: a topic
classifier chained to a dynamic/static classifier, as
shown in Figure 3. In this architecture, we build
one topic classifier, but several dynamic/static
classifiers, each trained on data pertaining to one
broad topic.
</bodyText>
<figureCaption confidence="0.994256">
Figure 3: Chaining two classifiers
</figureCaption>
<bodyText confidence="0.858669">
We used supervised learning to train the topic
</bodyText>
<page confidence="0.99653">
62
</page>
<bodyText confidence="0.999746941176471">
classifier, since our entire dataset is annotated by
human experts with topic labels. In contrast, to
train a dynamic/static classifier, we experimented
with the following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other
substrings such as cost of and how much is. A
question is considered static if it does not contain
any such words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001). The benefit of self-training is that we can
build a better classifier than that built from the
small seed corpus by simply adding in the large
unlabeled corpus without requiring hand-labeling.
Active-learning: This is another popular method
for training classifiers when not much annotated
data is available. The key idea in active learning
is to annotate only those instances of the dataset
that are most difficult for the classifier to learn to
classify. It is expected that training classifiers us-
ing this method shows better performance than if
samples were chosen randomly for the same hu-
man annotation effort.
We used the maximum entropy classifier in
LLAMA (Haffner, 2006) for all of the above clas-
sification tasks. We have chosen the active learn-
ing classifier due to its superior performance and
integrated it into the Qme! system. We pro-
vide further details about the learning methods in
(Mishra and Bangalore, 2010).
</bodyText>
<subsectionHeader confidence="0.981069">
3.2 Experiments and Results
3.2.1 Topic Classification
</subsectionHeader>
<bodyText confidence="0.9995665">
The topic classifier was trained using a training
set consisting of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the ques-
tions. The test set consisted of 15,000 randomly
selected questions. Word trigrams of the question
are used as features for a MaxEnt classifier which
outputs a score distribution on all of the 104 pos-
sible topic labels. The error rate results for models
selecting the top topic and the top two topics ac-
cording to the score distribution are shown in Ta-
ble 1. As can be seen these error rates are far lower
than the baseline model of selecting the most fre-
quent topic.
</bodyText>
<table confidence="0.99777275">
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
</table>
<tableCaption confidence="0.999879">
Table 1: Results of topic classification
</tableCaption>
<subsubsectionHeader confidence="0.530569">
3.2.2 Dynamic/static Classification
</subsubsectionHeader>
<bodyText confidence="0.999526375">
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluated
these methods on a 250 question test set drawn
from the broad topic of Movies. The error rates
are summarized in Table 2. We provide further de-
tails of this experiment in (Mishra and Bangalore,
2010).
</bodyText>
<table confidence="0.9991588">
Training approach Lowest Error rate
Baseline 27.70%
“Supervised” learning 22.09%
Self-training 8.84%
Active-learning 4.02%
</table>
<tableCaption confidence="0.8798045">
Table 2: Best Results of dynamic/static classifica-
tion
</tableCaption>
<sectionHeader confidence="0.990694" genericHeader="method">
4 Retrieving answers to dynamic
questions
</sectionHeader>
<bodyText confidence="0.999772703703704">
Following the classification step outlined in Sec-
tion 3.1, we know whether a user query is static or
dynamic, and the broad category of the question.
If the question is dynamic, then our system per-
forms a vertical search based on the broad topic
of the question. In our system, so far, we have in-
corporated vertical searches on three broad topics:
Movies, Mass Transit, and Yellow Pages.
For each broad topic, we have identified a few
trusted content aggregator websites. For example,
for dynamic questions related to Movies-related
dynamic user queries, www.fandango.com is
a trusted content aggregator website. Other such
trusted content aggregator websites have been
identified for Mass Transit related and for Yellow-
pages related dynamic user queries. We have also
identified the web-forms that can be used to search
these aggregator sites and the search parameters
that these web-forms need for searching. So, given
a user query, whose broad category has been deter-
mined and which has been classified as a dynamic
query by the system, the next step is to parse the
query to obtain pertinent search parameters.
The search parameters are dependent on the
broad category of the question, the trusted con-
tent aggregator website(s), the web-forms associ-
ated with this category, and of course, the content
</bodyText>
<page confidence="0.998762">
63
</page>
<bodyText confidence="0.999992869565218">
of the user query. From the search parameters, a
search query to the associated web-form is issued
to search the related aggregator site. For exam-
ple, for a movie-related query, What time is Twi-
light playing in Madison, New Jersey?, the per-
tinent search parameters that are parsed out are
movie-name: Twilight, city: Madison, and state:
New Jersey, which are used to build a search string
that Fandango’s web-form can use to search the
Fandango site. For a yellow-pages type of query,
Where is the Saigon Kitchen in Austin, Texas?, the
pertinent search parameters that are parsed out are
business-name: Saigon Kitchen, city: Austin, and
state: Texas, which are used to construct a search
string to search the Yellowpages website. These
are just two examples of the kinds of dynamic user
queries that we encounter. Within each broad cat-
egory, there is a wide variety of the sub-types of
user queries, and for each sub-type, we have to
parse out different search parameters and use dif-
ferent web-forms. Details of this extraction are
presented in (Feng and Bangalore, 2009).
It is quite likely that many of the dynamic
queries may not have all the pertinent search pa-
rameters explicitly outlined. For example, a mass
transit query may be When is the next train to
Princeton?. The bare minimum search parameters
needed to answer this query are a from-location,
and a to-location. However, the from-location is
not explicitly present in this query. In this case,
the from-location is inferred using the GPS sensor
present on the iPhone (on which our system is built
to run). Depending on the web-form that we are
querying, it is possible that we may be able to sim-
ply use the latitude-longitude obtained from the
GPS sensor as the value for the from-location pa-
rameter. At other times, we may have to perform
an intermediate latitude-longitude to city/state (or
zip-code) conversion in order to obtain the appro-
priate search parameter value.
Other examples of dynamic queries in which
search parameters are not explicit in the query, and
hence, have to be deduced by the system, include
queries such as Where is XMen playing? and How
long is Ace Hardware open?. In each of these
examples, the user has not specified a location.
Based on our understanding of natural language,
in such a scenario, our system is built to assume
that the user wants to find a movie theatre (or, is
referring to a hardware store) near where he is cur-
rently located. So, the system obtains the user’s
location from the GPS sensor and uses it to search
for a theatre (or locate the hardware store) within
a five-mile radius of her location.
In the last few paragraphs, we have discussed
how we search for answers to dynamic user
queries from the hidden web by using web-forms.
However, the search results returned by these web-
forms usually cannot be displayed as is in our
Qme! interface. The reason is that the results are
often HTML pages that are designed to be dis-
played on a desktop or a laptop screen, not a small
mobile phone screen. Displaying the results as
they are returned from search would make read-
ability difficult. So, we parse the HTML-encoded
result pages to get just the answers to the user
query and reformat it, to fit the Qme! interface,
which is designed to be easily readable on the
iPhone (as seen in Figure 1).2
</bodyText>
<sectionHeader confidence="0.984395" genericHeader="method">
5 Retrieving answers to static questions
</sectionHeader>
<bodyText confidence="0.993490166666667">
Answers to static user queries – questions whose
answers do not change over time – are retrieved
in a different way than answers to dynamic ques-
tions. A description of how our system retrieves
the answers to static questions is presented in this
section.
</bodyText>
<figureCaption confidence="0.9683295">
Figure 4: An example of an FST representing the
search index.
</figureCaption>
<subsectionHeader confidence="0.98208">
5.1 Representing Search Index as an FST
</subsectionHeader>
<bodyText confidence="0.999984666666667">
To obtain results for static user queries, we
have implemented our own search engine using
finite-state transducers (FST), in contrast to using
Lucene (Hatcher and Gospodnetic., 2004) as it is
a more efficient representation of the search index
that allows us to consider word lattices output by
ASR as input queries.
The FST search index is built as follows. We
index each question-answer (QA) pair from our
repository ((qi7 ai), qai for short) using the words
(wqi) in question qi. This index is represented as
a weighted finite-state transducer (SearchFST) as
shown in Figure 4. Here a word wqi (e.g old) is the
input symbol for a set of arcs whose output sym-
bol is the index of the QA pairs where old appears
</bodyText>
<footnote confidence="0.96095">
2We are aware that we could use SOAP (Simple Object
Access Protocol) encoding to do the search, however not all
aggregator sites use SOAP yet.
</footnote>
<figure confidence="0.99209475">
obama:qa450/c7
obama:qa25/c4
old:qa150/c5
how:qa12/c6
how:qa25/c1
old:qa25/c2
is:qa25/c3
0
</figure>
<page confidence="0.995304">
64
</page>
<bodyText confidence="0.999981428571429">
in the question. The weight of the arc c(wqi,qi) is
one of the similarity based weights discussed in
Section 4.1. As can be seen from Figure 4, the
words how, old, is and obama contribute a score to
the question-answer pair qa25; while other pairs,
qa150, qa12, qa450 are scored by only one of
these words.
</bodyText>
<subsectionHeader confidence="0.990188">
5.2 Search Process using FSTs
</subsectionHeader>
<bodyText confidence="0.999989333333333">
A user’s speech query, after speech recogni-
tion, is represented as a finite state automaton
(FSA, either 1-best or WCN), QueryFSA. The
QueryFSA is then transformed into another FSA
(NgramFSA) that represents the set of n-grams
of the QueryFSA. In contrast to most text search
engines, where stop words are removed from the
query, we weight the query terms with their idf val-
ues which results in a weighted NgramFSA. The
NgramFSA is composed with the SearchFST and
we obtain all the arcs (wq, qawq, c(wq,qawq )) where
wq is a query term, qawq is a QA index with the
query term and, c(wq,qawq) is the weight associ-
ated with that pair. Using this information, we
aggregate the weight for a QA pair (qaq) across
all query words and rank the retrieved QAs in the
descending order of this aggregated weight. We
select the top N QA pairs from this ranked list.
The query composition, QA weight aggregation
and selection of top N QA pairs are computed
with finite-state transducer operations as shown
in Equations 1 and 23. An evaluation of this
search methodology on word lattices is presented
in (Mishra and Bangalore, 2010).
</bodyText>
<equation confidence="0.937042">
D = 7r2(NgramFSA o SearchFST) (1)
TopN = fsmbestpath(fsmdeterminize(D), N)
(2)
</equation>
<sectionHeader confidence="0.9989" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999986857142857">
In this demonstration paper, we have presented
Qme!, a speech-driven question answering system
for use on mobile devices. The novelty of this sys-
tem is that it provides users with a single unified
interface for searching both the visible and the hid-
den web using the most natural input modality for
use on mobile phones – spoken language.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.864283">
We would like to thank Junlan Feng, Michael
Johnston and Mazin Gilbert for the help we re-
ceived in putting this system together. We would
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
also like to thank ChaCha for providing us the data
included in this system.
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999715069767442">
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of the 39th annual meeting of the as-
sociation for computational linguistics: ACL 2001,
pages 26–33.
J. Feng and S. Bangalore. 2009. Effects of word con-
fusion networks on voice search. In Proceedings of
EACL-2009, Athens, Greece.
Google, 2009. http://www.google.com/mobile.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239–261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Ac-
tion (In Action series). Manning Publications Co.,
Greenwich, CT, USA.
Microsoft, 2009. http://www.live.com.
T. Mishra and S. Bangalore. 2010. Qme!: A speech-
based question-answering system on mobile de-
vices. In Proceedings of NAACL-HLT.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, pages 1009–1104.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574–581.
J. Pustejovsky, R. Ingria, R. Sauri, J. Casta no,
J. Littman, and R. Gaizauskas., 2001. The language
of time: A reader, chapter The specification languae
– TimeML. Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in
question answering. Technical report, Brandies Uni-
versity.
E. Saquete, J. L. Vicedo, P. Martinez-Barco, R. Mu noz,
and H. Llorens. 2009. Enhancing qa systems with
complex temporal question processing capabilities.
Journal of Artificial Intelligence Research, 35:775–
811.
vlingo.com, 2009.
http://www.vlingomobile.com/downloads.html.
YellowPages, 2009. http://www.speak4it.com.
</reference>
<page confidence="0.999614">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966469">
<title confidence="0.999534">Speech-driven Access to the Deep Web on Mobile Devices</title>
<author confidence="0.998506">Mishra Bangalore</author>
<affiliation confidence="0.997484">AT&amp;T Labs - Research</affiliation>
<address confidence="0.9976145">180 Park Avenue Florham Park, NJ 07932 USA.</address>
<abstract confidence="0.9974813">The Deep Web is the collection of information repositories that are not indexed by search engines. These repositories are typically accessible through web forms and contain dynamically changing information. In this paper, we present a system that allows users to access such rich repositories of information on mobile devices using spoken language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th annual</booktitle>
<pages>26--33</pages>
<contexts>
<context position="13245" citStr="Banko and Brill, 2001" startWordPosition="2181" endWordPosition="2184">notated by human experts with topic labels. In contrast, to train a dynamic/static classifier, we experimented with the following three different techniques. Baseline: We treat questions as dynamic if they contain temporal indexicals, e.g. today, now, this week, two summers ago, currently, recently, which were based on the TimeML corpus. We also included spatial indexicals such as here, and other substrings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. Self-training with bagging: The general selftraining with bagging algorithm (Banko and Brill, 2001). The benefit of self-training is that we can build a better classifier than that built from the small seed corpus by simply adding in the large unlabeled corpus without requiring hand-labeling. Active-learning: This is another popular method for training classifiers when not much annotated data is available. The key idea in active learning is to annotate only those instances of the dataset that are most difficult for the classifier to learn to classify. It is expected that training classifiers using this method shows better performance than if samples were chosen randomly for the same human a</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th annual meeting of the association for computational linguistics: ACL 2001, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Feng</author>
<author>S Bangalore</author>
</authors>
<title>Effects of word confusion networks on voice search.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-2009,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="17976" citStr="Feng and Bangalore, 2009" startWordPosition="2951" endWordPosition="2954">ite. For a yellow-pages type of query, Where is the Saigon Kitchen in Austin, Texas?, the pertinent search parameters that are parsed out are business-name: Saigon Kitchen, city: Austin, and state: Texas, which are used to construct a search string to search the Yellowpages website. These are just two examples of the kinds of dynamic user queries that we encounter. Within each broad category, there is a wide variety of the sub-types of user queries, and for each sub-type, we have to parse out different search parameters and use different web-forms. Details of this extraction are presented in (Feng and Bangalore, 2009). It is quite likely that many of the dynamic queries may not have all the pertinent search parameters explicitly outlined. For example, a mass transit query may be When is the next train to Princeton?. The bare minimum search parameters needed to answer this query are a from-location, and a to-location. However, the from-location is not explicitly present in this query. In this case, the from-location is inferred using the GPS sensor present on the iPhone (on which our system is built to run). Depending on the web-form that we are querying, it is possible that we may be able to simply use the</context>
</contexts>
<marker>Feng, Bangalore, 2009</marker>
<rawString>J. Feng and S. Bangalore. 2009. Effects of word confusion networks on voice search. In Proceedings of EACL-2009, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<date>2009</date>
<note>http://www.google.com/mobile.</note>
<contexts>
<context position="4832" citStr="Google, 2009" startWordPosition="797" endWordPosition="798"> is as follows. In Section 2, we present the system architecture. In Section 3, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. In Section 4, we show how our system retrieves answers to dynamic questions. In Section 5, we show how our system retrieves answers to static questions. We conclude in Section 6. 2 Speech-driven Question Answer System Speech-driven access to information has been a popular application deployed by many companies on a variety of information resources (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009). In this prototype demonstration, we describe a speech-driven question-answer application. The system architecture is shown in Figure 2. The user of this application provides a spoken language query to a mobile device intending to find an answer to the question. The speech recognition module of the system recognizes the spoken query. The result from the speech recognizer can be either a single-best string or a weighted word lattice.1 This textual output of recognition is then used to classify the user query either as a dynamic query or a static query. If </context>
</contexts>
<marker>Google, 2009</marker>
<rawString>Google, 2009. http://www.google.com/mobile.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding. Speech Communication,</title>
<date>2006</date>
<contexts>
<context position="13926" citStr="Haffner, 2006" startWordPosition="2292" endWordPosition="2293">fier than that built from the small seed corpus by simply adding in the large unlabeled corpus without requiring hand-labeling. Active-learning: This is another popular method for training classifiers when not much annotated data is available. The key idea in active learning is to annotate only those instances of the dataset that are most difficult for the classifier to learn to classify. It is expected that training classifiers using this method shows better performance than if samples were chosen randomly for the same human annotation effort. We used the maximum entropy classifier in LLAMA (Haffner, 2006) for all of the above classification tasks. We have chosen the active learning classifier due to its superior performance and integrated it into the Qme! system. We provide further details about the learning methods in (Mishra and Bangalore, 2010). 3.2 Experiments and Results 3.2.1 Topic Classification The topic classifier was trained using a training set consisting of over one million questions downloaded from the web which were manually labeled by human experts as part of answering the questions. The test set consisted of 15,000 randomly selected questions. Word trigrams of the question are </context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(iv):239–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hatcher</author>
<author>O Gospodnetic</author>
</authors>
<title>Lucene in Action (In Action series).</title>
<date>2004</date>
<publisher>Manning Publications Co.,</publisher>
<location>Greenwich, CT, USA.</location>
<marker>Hatcher, Gospodnetic, 2004</marker>
<rawString>E. Hatcher and O. Gospodnetic. 2004. Lucene in Action (In Action series). Manning Publications Co., Greenwich, CT, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Microsoft</author>
</authors>
<date>2009</date>
<note>http://www.live.com.</note>
<contexts>
<context position="4818" citStr="Microsoft, 2009" startWordPosition="794" endWordPosition="796">yout of the paper is as follows. In Section 2, we present the system architecture. In Section 3, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. In Section 4, we show how our system retrieves answers to dynamic questions. In Section 5, we show how our system retrieves answers to static questions. We conclude in Section 6. 2 Speech-driven Question Answer System Speech-driven access to information has been a popular application deployed by many companies on a variety of information resources (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009). In this prototype demonstration, we describe a speech-driven question-answer application. The system architecture is shown in Figure 2. The user of this application provides a spoken language query to a mobile device intending to find an answer to the question. The speech recognition module of the system recognizes the spoken query. The result from the speech recognizer can be either a single-best string or a weighted word lattice.1 This textual output of recognition is then used to classify the user query either as a dynamic query or a sta</context>
</contexts>
<marker>Microsoft, 2009</marker>
<rawString>Microsoft, 2009. http://www.live.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mishra</author>
<author>S Bangalore</author>
</authors>
<title>Qme!: A speechbased question-answering system on mobile devices.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="14173" citStr="Mishra and Bangalore, 2010" startWordPosition="2332" endWordPosition="2335">vailable. The key idea in active learning is to annotate only those instances of the dataset that are most difficult for the classifier to learn to classify. It is expected that training classifiers using this method shows better performance than if samples were chosen randomly for the same human annotation effort. We used the maximum entropy classifier in LLAMA (Haffner, 2006) for all of the above classification tasks. We have chosen the active learning classifier due to its superior performance and integrated it into the Qme! system. We provide further details about the learning methods in (Mishra and Bangalore, 2010). 3.2 Experiments and Results 3.2.1 Topic Classification The topic classifier was trained using a training set consisting of over one million questions downloaded from the web which were manually labeled by human experts as part of answering the questions. The test set consisted of 15,000 randomly selected questions. Word trigrams of the question are used as features for a MaxEnt classifier which outputs a score distribution on all of the 104 possible topic labels. The error rate results for models selecting the top topic and the top two topics according to the score distribution are shown in </context>
<context position="23019" citStr="Mishra and Bangalore, 2010" startWordPosition="3826" endWordPosition="3829"> qawq, c(wq,qawq )) where wq is a query term, qawq is a QA index with the query term and, c(wq,qawq) is the weight associated with that pair. Using this information, we aggregate the weight for a QA pair (qaq) across all query words and rank the retrieved QAs in the descending order of this aggregated weight. We select the top N QA pairs from this ranked list. The query composition, QA weight aggregation and selection of top N QA pairs are computed with finite-state transducer operations as shown in Equations 1 and 23. An evaluation of this search methodology on word lattices is presented in (Mishra and Bangalore, 2010). D = 7r2(NgramFSA o SearchFST) (1) TopN = fsmbestpath(fsmdeterminize(D), N) (2) 6 Summary In this demonstration paper, we have presented Qme!, a speech-driven question answering system for use on mobile devices. The novelty of this system is that it provides users with a single unified interface for searching both the visible and the hidden web using the most natural input modality for use on mobile phones – spoken language. 7 Acknowledgments We would like to thank Junlan Feng, Michael Johnston and Mazin Gilbert for the help we received in putting this system together. We would 3We have dropp</context>
</contexts>
<marker>Mishra, Bangalore, 2010</marker>
<rawString>T. Mishra and S. Bangalore. 2010. Qme!: A speechbased question-answering system on mobile devices. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
</authors>
<title>Temporal context representation and reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1009--1104</pages>
<contexts>
<context position="8517" citStr="Moldovan et al., 2005" startWordPosition="1405" endWordPosition="1408">or question-answering since the late 1990’s. Early work on temporal expressions idenSearch Rank Classify Match Dynamic Retrieve from Web Speech ASR 1−best Lattice Ranked Results Q&amp;A corpus 61 tification using a tagger culminated in the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include, QA-by-Dossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 3.1 Question Classification: Dynamic and Static Questions We automatically classify questions as dynamic and static questions. The answers to static questions can be retrieved from the QA archive. To answer dynamic questions, we query the database(s) associated with the topic of the question through web for</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, 2005</marker>
<rawString>D. Moldovan, C. Clark, and S. Harabagiu. 2005. Temporal context representation and reasoning. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, pages 1009–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question answering using constraint satisfaction: Qa-bydossier-with-contraints.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual</booktitle>
<pages>574--581</pages>
<contexts>
<context position="8331" citStr="Prager et al., 2004" startWordPosition="1376" endWordPosition="1379">rent than the answer given in December 2010, because the theater at which White Christmas is playing differs from 2009 to 2010. There has been a growing interest in temporal analysis for question-answering since the late 1990’s. Early work on temporal expressions idenSearch Rank Classify Match Dynamic Retrieve from Web Speech ASR 1−best Lattice Ranked Results Q&amp;A corpus 61 tification using a tagger culminated in the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include, QA-by-Dossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 3.1 Question Classification: Dynamic and Static Questions We automatically classify questions as dynamic and static questi</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering using constraint satisfaction: Qa-bydossier-with-contraints. In Proceedings of the 42nd annual meeting of the association for computational linguistics: ACL 2004, pages 574–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>R Ingria</author>
<author>R Sauri</author>
<author>J Casta no</author>
<author>J Littman</author>
<author>R Gaizauskas</author>
</authors>
<title>The language of time: A reader, chapter The specification languae – TimeML.</title>
<date>2001</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="8179" citStr="Pustejovsky et al., 2001" startWordPosition="1353" endWordPosition="1356">he summer, the answer will be “This play is not currently playing anywhere in NYC.” If the question is asked during December, 2009, the answer might be different than the answer given in December 2010, because the theater at which White Christmas is playing differs from 2009 to 2010. There has been a growing interest in temporal analysis for question-answering since the late 1990’s. Early work on temporal expressions idenSearch Rank Classify Match Dynamic Retrieve from Web Speech ASR 1−best Lattice Ranked Results Q&amp;A corpus 61 tification using a tagger culminated in the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include, QA-by-Dossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answ</context>
</contexts>
<marker>Pustejovsky, Ingria, Sauri, no, Littman, Gaizauskas, 2001</marker>
<rawString>J. Pustejovsky, R. Ingria, R. Sauri, J. Casta no, J. Littman, and R. Gaizauskas., 2001. The language of time: A reader, chapter The specification languae – TimeML. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>B Sundheim</author>
</authors>
<title>Using timeml in question answering.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Brandies University.</institution>
<marker>Radev, Sundheim, 2002</marker>
<rawString>D. Radev and B. Sundheim. 2002. Using timeml in question answering. Technical report, Brandies University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Saquete</author>
<author>J L Vicedo</author>
<author>P Martinez-Barco</author>
<author>R Mu noz</author>
<author>H Llorens</author>
</authors>
<title>Enhancing qa systems with complex temporal question processing capabilities.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>35</volume>
<pages>811</pages>
<contexts>
<context position="8642" citStr="Saquete et al., 2009" startWordPosition="1422" endWordPosition="1426">e from Web Speech ASR 1−best Lattice Ranked Results Q&amp;A corpus 61 tification using a tagger culminated in the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include, QA-by-Dossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 3.1 Question Classification: Dynamic and Static Questions We automatically classify questions as dynamic and static questions. The answers to static questions can be retrieved from the QA archive. To answer dynamic questions, we query the database(s) associated with the topic of the question through web forms on the Internet. We first use a topic classifier to detect the topic of a question followed by a dynamic/static classifier</context>
</contexts>
<marker>Saquete, Vicedo, Martinez-Barco, noz, Llorens, 2009</marker>
<rawString>E. Saquete, J. L. Vicedo, P. Martinez-Barco, R. Mu noz, and H. Llorens. 2009. Enhancing qa systems with complex temporal question processing capabilities. Journal of Artificial Intelligence Research, 35:775– 811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>vlingo com</author>
</authors>
<date>2009</date>
<note>http://www.vlingomobile.com/downloads.html.</note>
<contexts>
<context position="4870" citStr="com, 2009" startWordPosition="801" endWordPosition="802"> the system architecture. In Section 3, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. In Section 4, we show how our system retrieves answers to dynamic questions. In Section 5, we show how our system retrieves answers to static questions. We conclude in Section 6. 2 Speech-driven Question Answer System Speech-driven access to information has been a popular application deployed by many companies on a variety of information resources (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009). In this prototype demonstration, we describe a speech-driven question-answer application. The system architecture is shown in Figure 2. The user of this application provides a spoken language query to a mobile device intending to find an answer to the question. The speech recognition module of the system recognizes the spoken query. The result from the speech recognizer can be either a single-best string or a weighted word lattice.1 This textual output of recognition is then used to classify the user query either as a dynamic query or a static query. If the user query is static, the result o</context>
</contexts>
<marker>com, 2009</marker>
<rawString>vlingo.com, 2009. http://www.vlingomobile.com/downloads.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>YellowPages</author>
</authors>
<date>2009</date>
<note>http://www.speak4it.com.</note>
<contexts>
<context position="4851" citStr="YellowPages, 2009" startWordPosition="799" endWordPosition="800">. In Section 2, we present the system architecture. In Section 3, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. In Section 4, we show how our system retrieves answers to dynamic questions. In Section 5, we show how our system retrieves answers to static questions. We conclude in Section 6. 2 Speech-driven Question Answer System Speech-driven access to information has been a popular application deployed by many companies on a variety of information resources (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009). In this prototype demonstration, we describe a speech-driven question-answer application. The system architecture is shown in Figure 2. The user of this application provides a spoken language query to a mobile device intending to find an answer to the question. The speech recognition module of the system recognizes the spoken query. The result from the speech recognizer can be either a single-best string or a weighted word lattice.1 This textual output of recognition is then used to classify the user query either as a dynamic query or a static query. If the user query is s</context>
</contexts>
<marker>YellowPages, 2009</marker>
<rawString>YellowPages, 2009. http://www.speak4it.com.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>