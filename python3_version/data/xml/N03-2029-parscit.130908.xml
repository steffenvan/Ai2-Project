<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004885">
<title confidence="0.998681">
Automatic Derivation of Surface Text Patterns for a Maximum Entropy
Based Question Answering System
</title>
<author confidence="0.945265">
Deepak Ravichandran
</author>
<affiliation confidence="0.915856">
USC Information Sciences Institute
</affiliation>
<address confidence="0.679309">
4676, Admiralty Way
Marina del Rey, CA, 90292
</address>
<email confidence="0.998962">
ravichan@isi.edu
</email>
<note confidence="0.653857">
Abraham Ittycheriah and Salim Roukos
IBM TJ Watson Research Center
</note>
<address confidence="0.578081">
Yorktown Heights, NY, 10598
</address>
<email confidence="0.994803">
abei,roukos @us.ibm.com
</email>
<sectionHeader confidence="0.993763" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999380777777778">
In this paper we investigate the use of surface
text patterns for a Maximum Entropy based
Question Answering (QA) system. These text
patterns are collected automatically in an un-
supervised fashion using a collection of trivia
question and answer pairs as seeds. These pat-
terns are used to generate features for a statis-
tical question answering system. We report our
results on the TREC-10 question set.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986905">
Several QA systems have investigated the use of text pat-
terns for QA (Soubbotin and Soubbotin, 2001), (Soub-
botin and Soubbotin, 2002), (Ravichandran and Hovy,
2002). For example, for questions like “When was
Gandhi born?”, typical answers are “Gandhi was born in
1869” and “Gandhi (1869-1948)”. These examples sug-
gest that the text patterns such as “ NAME was born in
BIRTHDATE ” and “ NAME ( BIRTHDATE -
DEATHYEAR )” when formulated as regular expres-
sions, can be used to select the answer phrase to ques-
tions. Another approach to a QA system is learning cor-
respondences between question and answer pairs. IBM’s
Statistical QA (Ittycheriah et al., 2001a) system uses a
probabilistic model trainable from Question-Answer sen-
tence pairs. The training is performed under a Maximum
Entropy model, using bag of words, syntactic and name
entity features. This QA system does not employ the use
of patterns. In this paper, we explore the inclusion of
surface text patterns into the framework of a statistical
question answering system.
</bodyText>
<sectionHeader confidence="0.991507" genericHeader="method">
2 KM Corpus
</sectionHeader>
<bodyText confidence="0.941059">
A corpus of question-answer pairs was obtained from
Knowledge Master (1999). We refer to this corpus as the
</bodyText>
<footnote confidence="0.987489">
1Work done while the author was an intern at IBM TJ Wat-
son Research Center during Summer 2002.
</footnote>
<bodyText confidence="0.996641666666667">
KM database. Each of the pairs in KM represents a trivia
question and its corresponding answer, such as the ones
used in the trivia card game. The question-answer pairs
in KM were filtered to retain only questions that look
similar to the ones presented in the TREC task2. Some
examples of QA pairs in KM:
</bodyText>
<listItem confidence="0.96779825">
1. Which country was invaded by the Libyan troops in
1983? - Chad
2. Who led the 1930 Salt March in India? - Mohandas
Gandhi
</listItem>
<sectionHeader confidence="0.942718" genericHeader="method">
3 Unsupervised Construction of Training
</sectionHeader>
<subsectionHeader confidence="0.81608">
Set for Pattern Extraction
</subsectionHeader>
<bodyText confidence="0.999930166666667">
We use an unsupervised technique that uses the QA in
KM as seeds to learn patterns. This method was first de-
scribed in Ravichandran and Hovy (2002). However, in
this work we have enriched the pattern format by induc-
ing specific semantic types of QTerms, and have learned
many more patterns using the KM.
</bodyText>
<subsectionHeader confidence="0.918595">
3.1 Algorithm for sentence construction
</subsectionHeader>
<listItem confidence="0.999808538461538">
1. For every question, we run a Named Entity Tagger
HMMNE 3 and identify chunks of words, that sig-
nify entities. Each such entity obtained from the
Question is defined as a Question term (QTerm).
The Answer Term (ATerm) is the Answer given by
the KM corpus.
2. Each of the question-answer pairs is submitted as
query to a popular Internet search engine4. We use
the top 50 relevant documents after stripping off the
HTML tags. The text is then tokenized to smoothen
white space variations and chopped to individual
sentences.
3. For every sentence obtained from Step (3) apply
</listItem>
<footnote confidence="0.998856666666667">
2This was done by retaining only those questions that had
10 words or less, and were not multiple choice.
3In these experiments we use HMMNE, a named entity tag-
ger similar to the BBN’s Identifinder HMM Tagger (Bikel et al.,
1999).
4Alta Vista http://www.altavista.com
</footnote>
<bodyText confidence="0.9573514">
HMMNE and retain only those sentences that con-
tains at least one of the QTerms plus the ATerm.
For example, we obtain the following sentences for the
QA pair “Which country was invaded by the Libyan
troops in 1983? - Chad”:
</bodyText>
<listItem confidence="0.919667666666667">
1. More than 7,000 Libyan troops entered Chad.
2. An OUA peacekeeping force of 3,500 troops replaced the
Libyan forces in the remainder of Chad.
3. In the summer of 1983, GUNT forces launched an offen-
sive against government positions in northern and eastern
Chad.
</listItem>
<bodyText confidence="0.9995715">
The underlined words indicate the QTerms and the
ATerms that helped to select the sentence as a potential
way of answering the Question. The algorithm described
above was applied to each of the 16,228 QA pairs in our
KM database. A total of more than 250K sentences was
obtained.
</bodyText>
<subsectionHeader confidence="0.999114">
3.2 Sentence Canonicalization
</subsectionHeader>
<bodyText confidence="0.996076666666667">
Every sentence obtained from the sentence construction
algorithm is canonicalized. Canonicalization of a sen-
tence is performed on the basis of the information pro-
vided by HMMNE, the QTerms and the ATerm. Canon-
icalization in this context may be defined as the general-
ization of a sentence based on the following process:
</bodyText>
<listItem confidence="0.994118888888889">
1. Apply HMMNE to each sentence obtained from the
sentence construction algorithm.
2. Identify the QTerms and ATerm in the answer sen-
tence.
3. Replace the ATerm by the tag “ ANSWER ”.
4. Replace each identified Named Entity by the class
of entity it represents.
5. If a given Named Entity is also a QTerm, indicate it
by the tag “QT”.
</listItem>
<bodyText confidence="0.9852532">
The following example illustrates canonicalization.
Consider the sentence:
More than 7,000 Libyan troops entered Chad.
The application of HMMNE results in:
The canonicalization step gives the sentence:
</bodyText>
<subsectionHeader confidence="0.991914">
3.3 Pattern Extraction
</subsectionHeader>
<bodyText confidence="0.883103">
Pattern extraction algorithm.
</bodyText>
<listItem confidence="0.997340777777778">
1. Every sentence obtained from sentence canon-
icalization algorithm is delimited by the tags
“ START ” and “ END ” and then passed
through a Suffix Tree. The Suffix Tree algorithm
obtains the counts of all sub-strings of the sentence.
2. From the Suffix Tree we obtain only those sub-
strings that are at least a trigram, contain both the
“ ANSWER ” and the “ QT ” tag and have at
least a count of 3 occurrences.
</listItem>
<table confidence="0.9518465">
Source Number of Questions
Trec8 200
Trec9 500
KM 4200
</table>
<tableCaption confidence="0.99994">
Table 1: Training source and sizes.
</tableCaption>
<bodyText confidence="0.995119909090909">
Some examples of patterns obtained from the Suffix Tree
algorithm are as follows:
A set of 22,353 such patterns were obtained by the ap-
plication of the pattern extraction algorithm from more
than 250,000 sentences. Some patterns are very general
and applicable to many questions, such as the ones in ex-
amples (7) and (8) while others are more specific to a
few questions, such as examples (9) and (10). Having
obtained these patterns we now can learn the appropriate
“weights” to use these patterns in a Question Answering
System.
</bodyText>
<sectionHeader confidence="0.994055" genericHeader="method">
4 Maximum Entropy Training
</sectionHeader>
<bodyText confidence="0.992629666666667">
For these experiments we use the Maximum Entropy for-
mulation (Della Pietra et al., 1995) and model the distri-
bution (Ittycheriah, 2001b),
</bodyText>
<equation confidence="0.952544">
(1)
</equation>
<bodyText confidence="0.970233866666667">
The patterns derived above are used as features to model
the distribution , which predicts the “correct-
ness” of the configuration of the question, , the predicted
answer tag, , and the answer candidate, . The training
data for the algorithm consists of TREC-8, TREC-9, and
a subset of the KM questions which have been judged to
have answers in the TREC corpus5. The total number of
questions available for training is shown in Table 1.
We perform 3 sets of experiment with different choice
of feature sets for training:
1. In the first experiment, the patterns obtained auto-
matically from the web are trained along with the
expected type of answer using the Maximum En-
tropy Framework. We refer to this system as the
Pat Only System. This feature collection consisted
</bodyText>
<footnote confidence="0.7084625">
5Tagging of answers was done in a semi automatic way by
human judges.
</footnote>
<figure confidence="0.9397505">
More than NUMEX TYPE=CARDINAL 7,000
/NUMEX HUMAN TYPE=PEOPLE Libyan
/HUMAN troops entered ENAMEX TYPE=
COUNTRY Chad /ENAMEX .
More than CARDINAL PEOPLE QT troops en-
tered ANSWER .
1. son of PERSON QT and ANSWER
2. of the ANSWER DISEASE QT
3. of ANSWER at LOCATION QT
4. ANSWER was the ORDINAL OCCUPATION
QT to
5. ANSWER was elected OCCUPATION QT of the
LOCATION QT
6. ANSWER was a prolific OCCUPATION QT
7. LOCATION QT , ANSWER
8. ANSWER , LOCATION QT
9. START ANSWER served as OCCUPATION
QT from DATE
10. START ANSWER is the PEOPLE QT
name for
</figure>
<table confidence="0.93570875">
Number of questions correct
Rank PAT ONLY IBM TREC11 ME PAT
1 117 157 167
2 24 21 32
3 16 21 14
4 16 22 11
5 8 8 10
MRR 0.29934 0.37573 0.39703
</table>
<tableCaption confidence="0.999466">
Table 2: Results on TREC-10.
</tableCaption>
<bodyText confidence="0.838596333333333">
of roughly 22,353 pattern features along with the
30 different expected answer types (the ones recog-
nized by HMMNE).
</bodyText>
<listItem confidence="0.998345166666667">
2. In the second experiment we use a Statistical QA
system that contains bag of words, syntactic and
named-entity features. We refer to this system as
the IBM TREC11 System. Details of this system
appear in (Ittycheriah and Roukos, 2002). This sys-
tem has approximately 8,000 features.
3. In the third experiment we add the patterns as ad-
ditional features to the base system IBM TREC11
and train the system. We refer to this system as the
ME PAT System. Hence, the total number of fea-
tures in this system is equal to the sum of the ones
in Pat Only and IBM TREC11 system.
</listItem>
<bodyText confidence="0.999865">
These systems were trained on TREC-9 and KM and for
picking the optimum model we used TREC-8 as held-out
test data.
</bodyText>
<sectionHeader confidence="0.997884" genericHeader="evaluation">
5 Results on TREC-10
</sectionHeader>
<bodyText confidence="0.999848166666667">
We then tested the model on TREC-10. We tabulate the
results in Table 2. The TREC-10 collection consisted of
500 questions. The Rank column indicates the number
of questions answered by the QA systems with that par-
ticular rank. Finally the Mean Rank Reciprocal (MRR)
scores are reported.
</bodyText>
<sectionHeader confidence="0.997009" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999956285714286">
Not surprisingly, the PAT ONLY system shows only av-
erage performance as compared to other TREC-10 sys-
tems. This is because the system has no information
about the question except about its expected answer-
type. Hence, the PAT ONLY system would answer all the
questions involving TIME such as: “When was A born?”,
“When did A die?”, “Which year did A start attending
college?”, “When did A author book B?”with the same
answer!
Nonetheless, the ME PAT results show that surface
text patterns are useful for a Question Answering System.
Although in these experiments a feature set of 22,353
patterns was trained on approximately 210,000 instances,
only 1500 patterns was actually found in the final train-
ing data which had a count of at least 8 instances. This
suggests that the approach used here to train weights suf-
fers from the problem of having very little training data
as compared to the number of features. A much better ap-
proach would be to train the weights of the patterns from
the unsupervised collection itself. However, the effect of
noise introduced due to such unsupervised training is un-
clear.
The above technique represents a very clean approach
to integrating the use of patterns into a QA system. Most
of the rule based systems take years to engineer and are
very difficult to duplicate. However, a good statistical
system can be duplicated to give good performance in a
relatively short amount of time.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99952559375">
D. Bikel, R. Schwartz and R. Weischedel. 1999. An Al-
gorithm that Learns What´s in a Name. Machine Learn-
ing Special Issue on NL Learning, 34, 1–3.
A. Ittycheriah, M. Franz, W. Zhu, A. Ratnaparki and R.
Mammone. 2001a. Question Answering Using Maxi-
mum Entropy Components Proceedings of the NAACL
Conference, Pittsburgh, PA, 33–39.
A. Ittycheriah. 2001b. Trainable Question Answering
System. PhD Thesis, Rutgers, The State University of
New Jersey, New Brunswick, NJ.
A. Ittycheriah and S. Roukos. 2002. IBM’s Statistical
Question Answering System for TREC-11. Proceed-
ings of the TREC-11 Conference, NIST, Gaithersburg,
MD, 394–401.
KnowledgeMaster. 1999. http://www.greatauk.com.
Academic Hallmarks.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. In-
ducing Features of Random Fields. Technical Report,
Department of Computer Science, Carnegie-Mellon
University, CMU-CS-95–144.
D. Ravichandran and E. Hovy. 2002. Surface Text Pat-
terns for a Question Answering system. Proceedings
of the ACL Conference. Philadelphia, PA, 425–432.
41–47.
M.M. Soubbotin and S.M. Soubbotin. 2001. Patterns
of Potential Answer Expressions as Clues to the Right
Answer. Proceedings of the TREC-10 Conference.
NIST, Gaithersburg, MD, 134–143.
M.M. Soubbotin and S.M. Soubbotin. 2002. Use of Pat-
terns for detection of likely Answer Strings: A System-
atic Approach Answer. Proceedings of the TREC-2002
Conference. NIST, Gaithersburg, MD, 175–182.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774517">
<title confidence="0.998672">Automatic Derivation of Surface Text Patterns for a Maximum Based Question Answering System</title>
<author confidence="0.897812">Deepak</author>
<affiliation confidence="0.991424">USC Information Sciences</affiliation>
<address confidence="0.995145">4676, Admiralty</address>
<author confidence="0.992957">Marina del Rey</author>
<author confidence="0.992957">CA</author>
<email confidence="0.999178">ravichan@isi.edu</email>
<author confidence="0.896545">Ittycheriah Roukos</author>
<affiliation confidence="0.999914">IBM TJ Watson Research Center</affiliation>
<address confidence="0.999383">Yorktown Heights, NY, 10598</address>
<email confidence="0.98229">abei,roukos@us.ibm.com</email>
<abstract confidence="0.9993035">In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An Algorithm that Learns What´s in a Name.</title>
<date>1999</date>
<journal>Machine Learning Special Issue on NL Learning,</journal>
<volume>34</volume>
<pages>1--3</pages>
<contexts>
<context position="3657" citStr="Bikel et al., 1999" startWordPosition="607" endWordPosition="610">(QTerm). The Answer Term (ATerm) is the Answer given by the KM corpus. 2. Each of the question-answer pairs is submitted as query to a popular Internet search engine4. We use the top 50 relevant documents after stripping off the HTML tags. The text is then tokenized to smoothen white space variations and chopped to individual sentences. 3. For every sentence obtained from Step (3) apply 2This was done by retaining only those questions that had 10 words or less, and were not multiple choice. 3In these experiments we use HMMNE, a named entity tagger similar to the BBN’s Identifinder HMM Tagger (Bikel et al., 1999). 4Alta Vista http://www.altavista.com HMMNE and retain only those sentences that contains at least one of the QTerms plus the ATerm. For example, we obtain the following sentences for the QA pair “Which country was invaded by the Libyan troops in 1983? - Chad”: 1. More than 7,000 Libyan troops entered Chad. 2. An OUA peacekeeping force of 3,500 troops replaced the Libyan forces in the remainder of Chad. 3. In the summer of 1983, GUNT forces launched an offensive against government positions in northern and eastern Chad. The underlined words indicate the QTerms and the ATerms that helped to se</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz and R. Weischedel. 1999. An Algorithm that Learns What´s in a Name. Machine Learning Special Issue on NL Learning, 34, 1–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W Zhu</author>
<author>A Ratnaparki</author>
<author>R Mammone</author>
</authors>
<title>Question Answering Using Maximum Entropy Components</title>
<date>2001</date>
<booktitle>Proceedings of the NAACL Conference,</booktitle>
<pages>33--39</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="1417" citStr="Ittycheriah et al., 2001" startWordPosition="220" endWordPosition="223">tigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). For example, for questions like “When was Gandhi born?”, typical answers are “Gandhi was born in 1869” and “Gandhi (1869-1948)”. These examples suggest that the text patterns such as “ NAME was born in BIRTHDATE ” and “ NAME ( BIRTHDATE - DEATHYEAR )” when formulated as regular expressions, can be used to select the answer phrase to questions. Another approach to a QA system is learning correspondences between question and answer pairs. IBM’s Statistical QA (Ittycheriah et al., 2001a) system uses a probabilistic model trainable from Question-Answer sentence pairs. The training is performed under a Maximum Entropy model, using bag of words, syntactic and name entity features. This QA system does not employ the use of patterns. In this paper, we explore the inclusion of surface text patterns into the framework of a statistical question answering system. 2 KM Corpus A corpus of question-answer pairs was obtained from Knowledge Master (1999). We refer to this corpus as the 1Work done while the author was an intern at IBM TJ Watson Research Center during Summer 2002. KM datab</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparki, Mammone, 2001</marker>
<rawString>A. Ittycheriah, M. Franz, W. Zhu, A. Ratnaparki and R. Mammone. 2001a. Question Answering Using Maximum Entropy Components Proceedings of the NAACL Conference, Pittsburgh, PA, 33–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
</authors>
<title>Trainable Question Answering System.</title>
<date>2001</date>
<tech>PhD Thesis,</tech>
<institution>Rutgers, The State University of New Jersey,</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="6598" citStr="Ittycheriah, 2001" startWordPosition="1108" endWordPosition="1109">llows: A set of 22,353 such patterns were obtained by the application of the pattern extraction algorithm from more than 250,000 sentences. Some patterns are very general and applicable to many questions, such as the ones in examples (7) and (8) while others are more specific to a few questions, such as examples (9) and (10). Having obtained these patterns we now can learn the appropriate “weights” to use these patterns in a Question Answering System. 4 Maximum Entropy Training For these experiments we use the Maximum Entropy formulation (Della Pietra et al., 1995) and model the distribution (Ittycheriah, 2001b), (1) The patterns derived above are used as features to model the distribution , which predicts the “correctness” of the configuration of the question, , the predicted answer tag, , and the answer candidate, . The training data for the algorithm consists of TREC-8, TREC-9, and a subset of the KM questions which have been judged to have answers in the TREC corpus5. The total number of questions available for training is shown in Table 1. We perform 3 sets of experiment with different choice of feature sets for training: 1. In the first experiment, the patterns obtained automatically from the</context>
</contexts>
<marker>Ittycheriah, 2001</marker>
<rawString>A. Ittycheriah. 2001b. Trainable Question Answering System. PhD Thesis, Rutgers, The State University of New Jersey, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>IBM’s Statistical Question Answering System for TREC-11.</title>
<date>2002</date>
<booktitle>Proceedings of the TREC-11 Conference,</booktitle>
<pages>394--401</pages>
<location>NIST, Gaithersburg, MD,</location>
<contexts>
<context position="8514" citStr="Ittycheriah and Roukos, 2002" startWordPosition="1451" endWordPosition="1454">ATION QT 9. START ANSWER served as OCCUPATION QT from DATE 10. START ANSWER is the PEOPLE QT name for Number of questions correct Rank PAT ONLY IBM TREC11 ME PAT 1 117 157 167 2 24 21 32 3 16 21 14 4 16 22 11 5 8 8 10 MRR 0.29934 0.37573 0.39703 Table 2: Results on TREC-10. of roughly 22,353 pattern features along with the 30 different expected answer types (the ones recognized by HMMNE). 2. In the second experiment we use a Statistical QA system that contains bag of words, syntactic and named-entity features. We refer to this system as the IBM TREC11 System. Details of this system appear in (Ittycheriah and Roukos, 2002). This system has approximately 8,000 features. 3. In the third experiment we add the patterns as additional features to the base system IBM TREC11 and train the system. We refer to this system as the ME PAT System. Hence, the total number of features in this system is equal to the sum of the ones in Pat Only and IBM TREC11 system. These systems were trained on TREC-9 and KM and for picking the optimum model we used TREC-8 as held-out test data. 5 Results on TREC-10 We then tested the model on TREC-10. We tabulate the results in Table 2. The TREC-10 collection consisted of 500 questions. The R</context>
</contexts>
<marker>Ittycheriah, Roukos, 2002</marker>
<rawString>A. Ittycheriah and S. Roukos. 2002. IBM’s Statistical Question Answering System for TREC-11. Proceedings of the TREC-11 Conference, NIST, Gaithersburg, MD, 394–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KnowledgeMaster</author>
</authors>
<date>1999</date>
<note>http://www.greatauk.com. Academic Hallmarks.</note>
<marker>KnowledgeMaster, 1999</marker>
<rawString>KnowledgeMaster. 1999. http://www.greatauk.com. Academic Hallmarks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features of Random Fields.</title>
<date>1995</date>
<tech>Technical Report,</tech>
<pages>95--144</pages>
<institution>Department of Computer Science, Carnegie-Mellon University,</institution>
<contexts>
<context position="6552" citStr="Pietra et al., 1995" startWordPosition="1099" endWordPosition="1102">obtained from the Suffix Tree algorithm are as follows: A set of 22,353 such patterns were obtained by the application of the pattern extraction algorithm from more than 250,000 sentences. Some patterns are very general and applicable to many questions, such as the ones in examples (7) and (8) while others are more specific to a few questions, such as examples (9) and (10). Having obtained these patterns we now can learn the appropriate “weights” to use these patterns in a Question Answering System. 4 Maximum Entropy Training For these experiments we use the Maximum Entropy formulation (Della Pietra et al., 1995) and model the distribution (Ittycheriah, 2001b), (1) The patterns derived above are used as features to model the distribution , which predicts the “correctness” of the configuration of the question, , the predicted answer tag, , and the answer candidate, . The training data for the algorithm consists of TREC-8, TREC-9, and a subset of the KM questions which have been judged to have answers in the TREC corpus5. The total number of questions available for training is shown in Table 1. We perform 3 sets of experiment with different choice of feature sets for training: 1. In the first experiment</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing Features of Random Fields. Technical Report, Department of Computer Science, Carnegie-Mellon University, CMU-CS-95–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Surface Text Patterns for a Question Answering system.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL Conference.</booktitle>
<pages>425--432</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="928" citStr="Ravichandran and Hovy, 2002" startWordPosition="136" endWordPosition="139">s, NY, 10598 abei,roukos @us.ibm.com Abstract In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set. 1 Introduction Several QA systems have investigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). For example, for questions like “When was Gandhi born?”, typical answers are “Gandhi was born in 1869” and “Gandhi (1869-1948)”. These examples suggest that the text patterns such as “ NAME was born in BIRTHDATE ” and “ NAME ( BIRTHDATE - DEATHYEAR )” when formulated as regular expressions, can be used to select the answer phrase to questions. Another approach to a QA system is learning correspondences between question and answer pairs. IBM’s Statistical QA (Ittycheriah et al., 2001a) system uses a probabilistic model trainable from Question-Answer sentence pairs. The training is performed u</context>
<context position="2654" citStr="Ravichandran and Hovy (2002)" startWordPosition="434" endWordPosition="437"> of the pairs in KM represents a trivia question and its corresponding answer, such as the ones used in the trivia card game. The question-answer pairs in KM were filtered to retain only questions that look similar to the ones presented in the TREC task2. Some examples of QA pairs in KM: 1. Which country was invaded by the Libyan troops in 1983? - Chad 2. Who led the 1930 Salt March in India? - Mohandas Gandhi 3 Unsupervised Construction of Training Set for Pattern Extraction We use an unsupervised technique that uses the QA in KM as seeds to learn patterns. This method was first described in Ravichandran and Hovy (2002). However, in this work we have enriched the pattern format by inducing specific semantic types of QTerms, and have learned many more patterns using the KM. 3.1 Algorithm for sentence construction 1. For every question, we run a Named Entity Tagger HMMNE 3 and identify chunks of words, that signify entities. Each such entity obtained from the Question is defined as a Question term (QTerm). The Answer Term (ATerm) is the Answer given by the KM corpus. 2. Each of the question-answer pairs is submitted as query to a popular Internet search engine4. We use the top 50 relevant documents after strip</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Surface Text Patterns for a Question Answering system. Proceedings of the ACL Conference. Philadelphia, PA, 425–432. 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Soubbotin</author>
<author>S M Soubbotin</author>
</authors>
<title>Patterns of Potential Answer Expressions as Clues to the Right Answer.</title>
<date>2001</date>
<booktitle>Proceedings of the TREC-10 Conference.</booktitle>
<pages>134--143</pages>
<location>NIST, Gaithersburg, MD,</location>
<contexts>
<context position="864" citStr="Soubbotin and Soubbotin, 2001" startWordPosition="127" endWordPosition="130">iah and Salim Roukos IBM TJ Watson Research Center Yorktown Heights, NY, 10598 abei,roukos @us.ibm.com Abstract In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set. 1 Introduction Several QA systems have investigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). For example, for questions like “When was Gandhi born?”, typical answers are “Gandhi was born in 1869” and “Gandhi (1869-1948)”. These examples suggest that the text patterns such as “ NAME was born in BIRTHDATE ” and “ NAME ( BIRTHDATE - DEATHYEAR )” when formulated as regular expressions, can be used to select the answer phrase to questions. Another approach to a QA system is learning correspondences between question and answer pairs. IBM’s Statistical QA (Ittycheriah et al., 2001a) system uses a probabilistic model trainable </context>
</contexts>
<marker>Soubbotin, Soubbotin, 2001</marker>
<rawString>M.M. Soubbotin and S.M. Soubbotin. 2001. Patterns of Potential Answer Expressions as Clues to the Right Answer. Proceedings of the TREC-10 Conference. NIST, Gaithersburg, MD, 134–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Soubbotin</author>
<author>S M Soubbotin</author>
</authors>
<title>Use of Patterns for detection of likely Answer Strings: A Systematic Approach Answer.</title>
<date>2002</date>
<booktitle>Proceedings of the TREC-2002 Conference.</booktitle>
<pages>175--182</pages>
<location>NIST, Gaithersburg, MD,</location>
<contexts>
<context position="897" citStr="Soubbotin and Soubbotin, 2002" startWordPosition="131" endWordPosition="135">n Research Center Yorktown Heights, NY, 10598 abei,roukos @us.ibm.com Abstract In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set. 1 Introduction Several QA systems have investigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). For example, for questions like “When was Gandhi born?”, typical answers are “Gandhi was born in 1869” and “Gandhi (1869-1948)”. These examples suggest that the text patterns such as “ NAME was born in BIRTHDATE ” and “ NAME ( BIRTHDATE - DEATHYEAR )” when formulated as regular expressions, can be used to select the answer phrase to questions. Another approach to a QA system is learning correspondences between question and answer pairs. IBM’s Statistical QA (Ittycheriah et al., 2001a) system uses a probabilistic model trainable from Question-Answer sentence pai</context>
</contexts>
<marker>Soubbotin, Soubbotin, 2002</marker>
<rawString>M.M. Soubbotin and S.M. Soubbotin. 2002. Use of Patterns for detection of likely Answer Strings: A Systematic Approach Answer. Proceedings of the TREC-2002 Conference. NIST, Gaithersburg, MD, 175–182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>