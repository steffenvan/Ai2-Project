<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996080333333333">
From English to Logic:
Context-Free Computation of
&apos;Conventional&apos; Logical Translation&apos;
</title>
<author confidence="0.985056">
Lenhart K. Schubert
</author>
<affiliation confidence="0.9733316">
Department of Computing Science
University of Alberta
Francis Jeffry Pelletier
Department of Philosophy
University of Alberta
</affiliation>
<bodyText confidence="0.967625266666667">
Edmonton, Canada T6G 2H1
We describe an approach to parsing and logical translation that was inspired by
Gazdar&apos;s work on context-free grammar for English. Each grammar rule consists of a
syntactic part that specifies an acceptable fragment of a parse tree, and a semantic part
that specifies how the logical formulas corresponding to the constituents of the fragment
are to be combined to yield the formula for the fragment. However, we have sought to
reformulate Gazdar&apos;s semantic rules so as to obtain more or less &apos;conventional&apos; logical
translations of English sentences, avoiding the interpretation of NPs as property sets and
the use of intensional functors other than certain propositional operators. The reformulat-
ed semantic rules often turn out to be slightly simpler than Gazdar&apos;s. Moreover, by using a
semantically ambiguous logical syntax for the preliminary translations, we can account for
quantifier and coordinator scope ambiguities in syntactically unambiguous sentences
without recourse to multiple semantic rules, and are able to separate the disambiguation
process from the operation of the parser-translator. We have implemented simple recur-
sive descent and left-corner parsers to demonstrate the practicality of our approach.
</bodyText>
<sectionHeader confidence="0.992135" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999972">
Our ultimate objective is the design of a natural
language understanding system whose syntactic, se-
mantic and pragmatic capabilities are encoded in an
easily comprehensible and extensible form. In addi-
tion, these encodings should be capable of supporting
efficient algorithms for parsing and comprehension.
In our view, the achievement of the former objec-
tive calls for a careful structural separation of the sub-
systems that specify possible constituent structure
(syntax), possible mappings from constituent structure
to underlying logical form (part of semantics), and
possible mappings from logical form to deeper, unam-
biguous representations as a function of discourse
context and world knowledge (part of pragmatics and
</bodyText>
<note confidence="0.603629">
1 Submitted August 1981; revised July 1982.
</note>
<bodyText confidence="0.9356008">
inference). This sort of view is now widely held, as
evidenced by a recent panel discussion on parsing
issues (Robinson 1981). In the words of one of the
panelists,
&amp;quot;I take it to be uncontroversial that, other
things being equal, a homogenized system is less
preferable on both practical and scientific
grounds than one that naturally decomposes.
Practically, such a system is easier to build and
maintain, since the parts can be designed, devel-
oped, and understood to a certain extent in
isolation... Scientifically, a decomposable sys-
tem is much more likely to provide insight into
the process of natural language comprehension,
whether by machines or people.&amp;quot; (Kaplan 1981)
The panelists also emphasized that structural decom-
position by no means precludes interleaving or paral-
Copyright 1982 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.415802">
0362-613X/82/010026-19$01.00
</page>
<note confidence="0.777145">
26 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.999578443478261">
lelism of the processes that draw on the various kinds
of linguistic and non-linguistic knowledge.
Note that we are making a distinction between the
logical form that corresponds directly to surface struc-
ture on the one hand, and an unambiguous deeper
representation on the other. Indeed, at the level of
logical form our theory of logical translation admits
ambiguities in all of the formal building blocks (terms,
functions, predicates, connectives, and quantifiers), as
well as in the scopes of quantifiers and coordinators.
For example, logical-form translations may contain
terms such as Mary2 and &lt;the 1 (little2 gir13)&gt;, ambi-
guous between various referents (e.g., MARYS and
MARY17), and quasi-predicates such as has3, good2,
cold5, and recovers 1, ambiguous between various
proper predicates (e.g., has3: OWNS1, AFFLICTED-
WITH, ...; good2: VIRTUOUS, GOOD-TASTING, ...;
cold5: COLD1, EMOTIONLESS, ...; and recovers 1 :
RE-COVERS1, REGAINS, ...). In other words, we
do not regard the logical form of a sentence as fully
determining its meaning â€” not even its &apos;literal&apos; mean-
ing; rather, its meaning is determined by its logical
form along with the context of its utterance. Thus
&amp;quot;She is becoming cold&amp;quot; might convey on one occasion
that Lady Godiva is beginning to feel cold, on another
that Queen Victoria is becoming emotionless, and on a
third that Mount St. Helens is cooling off; but the
logical form does no more than specify the feminine
gender of the referent and its property of &amp;quot;becoming
cold (in some sense) at the time of utterance&amp;quot;. Our
primary concern in this paper will be with the semantic
rules that define immediate logical form, although we
attempt to define this form in a way that minimizes
the remaining gap to the deeper representation.
All the experience gained within Al and linguistics
suggests that bridging this final gap will be very diffi-
cult. Some would take as their lesson that research
efforts should concentrate on the last, pragmatic phase
of comprehension, where &apos;the real problems&apos; lie. We
believe on the contrary that the only way to make the
pragmatic problems tractable is to have a precise con-
ception of the constituent structure and logical form of
the natural language input, in terms of which the prag-
matic operations can in turn be precisely formulated.
In Al research, the objectives of clarity and exten-
sibility have often been sacrificed to immediate per-
formance goals. One reason for this may have been
the need to establish the credibility of a relatively
young and controversial discipline. In any case, the
state of linguistic theory until fairly recently left no
real alternatives. The transformational grammars
whose study dominated theoretical linguistics seemed a
poor prospect even for the limited goal of describing
natural language syntax, because of the subtlety of
transformational rules and supplementary devices such
as co-indexing procedures, filters and constraints on
movement, and the complexity of their interactions.
Moreover, the prospects for writing efficient transfor-
mational parsers seemed poor, given that transforma-
tional grammars can in principle generate all recursive-
ly enumerable languages. But most importantly, gen-
erative grammarians developed syntactic theories more
or less independently of any semantic considerations,
offering no guidance to Al researchers whose primary
objective was to compute &apos;meaning representations&apos;
for natural language utterances. Katz and Fodor&apos;s
markerese (Katz &amp; Fodor 1963) was patently inade-
quate as a meaning representation language from an
AT point of view, and Generative Semantics (Lakoff
1971) never did develop into a formal theory of the
relation between surface form and meaning.
Theoretical linguistics took an important new turn
with the work of Montague on the logic of English and
later expansions and variants of his theory (e.g., see
Thomason 1974a, Partee 1976a, and Cresswell 1973).
According to Montague grammar the correspondence
between syntactic structure and logical form is much
simpler than had generally been supposed: to each
lexeme there corresponds a logical term or functor and
to each rule of syntactic composition there corre-
sponds a structurally analogous semantic rule of logical
composition; this is the so-called rule-to-rule hypothe-
sis [Bach 1976].2 Furthermore, the translations of all
consituents of a particular syntactic category are as-
signed formal meanings of the same set-theoretic type;
for example, all NPs, be they names or definite or
indefinite descriptions, are taken to denote property
sets. Crucially, the formal semantics of the logical
translations produced by the semantic rules of Mo-
ntague grammar accords by and large with intuitions
about entailment, synonymy, ambiguity and other se-
mantic phenomena.
2 Interestingly enough, this linguistic hypothesis was anticipat-
ed by Knuth&apos;s work on the semantics of attribute grammars (Knuth
1968). Schwind (1978) has applied Knuth&apos;s insights to the devel-
opment of a formal basis for question answering systems, anticipat-
ing some of the work by Gazdar and others on which our own
efforts are founded.
There is also some similarity between the rule-to-rule hypothe-
sis and the rule-based approach to the interpretation of syntactic
structures that emerged within Al during the 1960&apos;s and early 70&apos;s.
The idea of pairing semantic rules with phrase structure rules was at
the heart of DEACON (Craig et al. 1966), a system based on F.
B. Thompson&apos;s proposal to formalize English by limiting its subject
matter to well-defined computer memory structures (Thompson
1966). However, DEACON&apos;s semantic rules performed direct
semantic evaluation of sorts (via computations over a data base)
rather than constructing logical translations. The systems of Wino-
grad (1972) and Woods (1977) constructed input translations prior
to evaluation, using semantic rules associated with particular syn-
tactic structures. However, these rules neither corresponded one-
to-one to syntactic rules nor limited interpretive operations to
composition of logical expressions; for example, they incorporated
tests for selectional restrictions and other forms of inference, with
unrestricted use of the computational power of LISP.
</bodyText>
<note confidence="0.9278955">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 27
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.969237300884956">
The chief limitation of Montague&apos;s grammar was
that it treated only very small, syntactically (though
not semantically) simple fragments of English, and
efforts were soon under way to extend the fragments,
in some cases by addition of a transformational com-
ponent (Partee 1976b, Cooper &amp; Parsons 1976). At
the same time, however, linguists dissatisfied with
transformational theory were beginning to develop
non-transformational alternatives to traditional genera-
tive grammars (e.g., Peters &amp; Ritchie 1969, Bresnan
1978, Lapointe 1977, Brame 1978, Langendoen
1979). A particularly promising theory that emerged
from this development, and explicitly incorporates
Montague&apos;s approach to semantics, is the phrase struc-
ture theory advanced by Gazdar and others (Gazdar
1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp;
Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to
appear). The theory covers a wide range of the syn-
tactic phenomena that have exercised transformation-
alists from Chomsky onward, including subcategoriza-
tion, coordination, passivization, and unbounded de-
pendencies such as those occurring in topicalization,
relative clause constructions and comparatives. Yet
the grammar itself makes no use of transformations; it
consists entirely of phrase structure rules, with a node-
admissibility rather than generative interpretation. For
example, the rule [(S) (NP) (VP)] states that a frag-
ment with root S, left branch NP and right branch VP
is an admissible fragment of a syntactic tree.3 Such
phrase structure rules are easy to understand and per-
mit the use of efficient context-free parsing methods.
Moreover, the grammar realizes the rule-to-rule hy-
pothesis, pairing each syntactic rule with a Montague-
like semantic rule that supplies the intensional logic
translation of the constituent admitted by the syntactic
rule.
It has long been assumed by transformationalists
that linguistic generalizations cannot be adequately
captured in a grammar devoid of transformations.
Gazdar refutes the assumption by using metagrammat-
ical devices to achieve descriptive elegance. These
devices include rule-schemata (e.g., coordination sche-
mata that yield the rules of coordinate structure for all
coordinators and all syntactic categories), and
metarules (e.g., a passive metarule that takes any
transitive-VP rule as &apos;input&apos; and generates a corre-
sponding passive-VP rule as &apos;output&apos; by deleting the
3 We use traditional category symbols in our exposition, occa-
sionally followed by supplementary features, e.g., (V TRAN) for
transitive verb. Gazdar actually assumes a two-bar 5 system (e.g.,
see Bresnan 1976, Jackendoff 1977) that distinguishes between R,
and X categories (e.g., V, V, and V, equivalent to the traditional
S, VP and V respectively) and employs complex symbols whose
first component specifies the &apos;number of bars&apos; and whose second
component supplies a feature bundle encoding syntactic category,
subcategorization, and morphosyntactic and morphological informa-
tion.
object NP from the input rule and appending an op-
tional by-PP). Although metarules resemble trans-
formational rules, they map rules into rules rather than
trees into trees, leaving the grammar itself context-
free. Another key innovation is the use of categories
with &apos;gaps&apos;, such as NP/PP, denoting a NP from which
a PP has been deleted (not necessarily at the top lev-
el). A simple metarule and a few rule schemata are
used to introduce rules involving such derived categor-
ies, elegantly capturing unbounded dependencies.
The character of the syntactic theory will become
clearer in Section 4, where we supply a sampling of
grammatical rules (with our variants of the semantic
rules), along with the basic metarule for passives and
the coordination schemata. First, however, we would
like to motivate our attempt to reformulate Gazdar&apos;s
semantic rules so as to yield &apos;conventional&apos; logical
translations (Section 2), and to explain the syntactic
and semantic idiosyncrasies of our target logic
(Section 3).
By &apos;conventional&apos; logics we mean first-order (and
perhaps second-order) predicate logics, augmented
with a lambda operator, necessity operator, proposi-
tional attitude operators and perhaps other non-
extensional propositional operators, and with a Kripke-
style possible-worlds semantics (Hughes &amp; Cresswell
1968).4 The logic employed by Montague in his first
formal fragment of English comes rather close to what
we have in mind (Montague 1970a), while the inten-
sional logics of the later fragments introduce the un-
conventional features we hope to avoid (1970b,c). It
is the treatment in these later fragments that is usually
referred to by the term &amp;quot;Montague grammar&amp;quot;. (For a
detailed discussion of the distinction between conven-
tional logics in the above sense and intensional logics,
see Guenthner 1978).
We should stress that it is semantics, not syntax,
which is the crux of the distinction. We shall take
certain liberties with conventional logical syntax, align-
ing it more nearly with the surface structure; but this
will not lead to major departures from conventional
semantics. For example, our syntax of terms allows
syntactically unfamiliar formulas such as
[&lt; alll man2&gt; morta13].
4 We admit predicate modifiers and some second-order predi-
cate constants into our logical vocabulary, and may ultimately want
to employ a full-fledged second-order logic, in view of such sen-
tences as &amp;quot;Every good general has at least some of Napoleon&apos;s
qualities&amp;quot;. On the other hand, we may pare down rather than
expand the logical apparatus, opting for a logic that treats proper-
ties, propositions and other intensional entities as first-order indi-
viduals. This type of treatment, which avoids the unwanted identity
of logically equivalent propositions, appears to be gaining currency
(e.g., Fodor 1978, McCarthy 1979, Thomason 1980, Chierchia
1981). Some minor adjustments would be required in our rules of
logical translation.
</bodyText>
<page confidence="0.809304">
28 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.68961">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.978947">
But the formula derives its interpretation from its sti-
pulated logical equivalence to
</bodyText>
<equation confidence="0.804388">
(a111 x:[x man2])[x morta13],
which may in turn become
Vxax HUMAN] =&gt; [x MORTAL]],
after disambiguation.5
</equation>
<sectionHeader confidence="0.927126" genericHeader="categories and subject descriptors">
2. Intensional and &apos;Conventional&apos; Translations
</sectionHeader>
<bodyText confidence="0.982123028571429">
We should emphasize at the outset that our objec-
tive is not to impugn Montague grammar, but merely
to make the point that the choice between intensional
and conventional translations is as yet unclear. Given
that the conventional approach appears to have cer-
tain advantages, it is worth finding out where it leads;
but we are not irrevocably committed to this approach.
Fortunately, the translation component of a parser for
a Gazdar-style grammar is easily replaced.
Montague grammarians assume that natural lan-
guages closely resemble formal logical systems; more
specifically, they postulate a strict homomorphism
from the syntactic categories and rules of a natural
language to the semantic categories and rules required
for its formal interpretation. This postulate has led
them to an analysis of the logical content of natural
language sentences which differs in important respects
from the sorts of analyses traditionally employed by
philosophers of language (as well as linguists and Al
researchers, when they have explicitly concerned
themselves with logical content).
The most obvious difference is that intensional
logic translations of natural language sentences con-
form closely with the surface structure of those sen-
tences, except for some re-ordering of phrases, the
introduction of brackets, variables and certain logical
operators, and (perhaps) the reduction of idioms. For
example, since the constituent structure of &amp;quot;John loves
Mary&amp;quot; is
[John [loves Mary]],
the intensional logic translation likewise isolates a
component translating the VP &amp;quot;loves Mary&amp;quot;, compos-
ing this VP-translation with the translation of &amp;quot;John&amp;quot;
to give the sentence formula. By contrast, a conven-
tional translation will have the structure
</bodyText>
<sectionHeader confidence="0.31701" genericHeader="method">
[John loves Mary],
</sectionHeader>
<bodyText confidence="0.958046559322034">
in which &amp;quot;John&amp;quot; and &amp;quot;Mary&amp;quot; combine with the verb at
the same level of constituent structure.
In itself, this difference is not important. It only
becomes important when syntactic composition is as-
sumed to correspond to function application in the
semantic domain. This is done in Montague grammar
5 We consistently use infix form (with the predicate following
its first argument) and square brackets for complete sentential
formulas.
by resort to the Schoenfinkel-Church treatment of
many-place functions as one-place functions
(Schoenfinkel 1924, Church 1941). For example, the
predicate &amp;quot;loves&amp;quot; in the above sentence is interpreted
as a one-place function that yields a one-place function
when applied to its argument (in this instance, when
applied to the semantic value of &amp;quot;Mary&amp;quot;, it yields the
function that is the semantic value of &amp;quot;loves Mary&amp;quot;).
The resultant function in turn yields a sentence value
when applied to its argument (in this instance, when
applied to the semantic value of &amp;quot;John&amp;quot;, it yields the
proposition expressed by &amp;quot;John loves Mary&amp;quot;). Thus, a
dyadic predicator like &amp;quot;loves&amp;quot; is no longer interpreted
as a set of pairs of individuals (at each possible world
or index), but rather as a function into functions.
Similarly a triadic predicator like &amp;quot;gives&amp;quot; is interpreted
as a function into functions into functions.
Moreover, the arguments of these functions are not
individuals, because NPs in general and names in par-
ticular are assumed to denote property sets (or truth
functions over properties) rather than individuals. It is
easy to see how the postulate of syntactic-semantic
homomorphism leads to this further retreat from tradi-
tional semantics. Consider Gazdar&apos;s top-level rule of
declarative sentence structure and meaning:
&lt;10, [(S) (NP) (VP)], (VP&apos; NP&amp;quot;)&gt;.
The first element of this triple supplies the rule num-
ber (which we have set to 10 for consistency with the
sample grammar of Section 4), the second the syntac-
tic rule and the third the semantic rule. The semantic
rule states that the intensional logic translation of the
S-constituent is compounded of the VP-translation (as
functor) and the NP-translation (as operand), where
the latter is first to be prefixed with the intension op-
erator A. In general, a primed syntactic symbol de-
notes the logical translation of the corresponding con-
stituent, and a double-primed symbol the logical trans-
lation prefixed with the intension operator (thus, NP&amp;quot;
stands for ANP&apos;)
For example, if the NP dominates &amp;quot;John&amp;quot; and the
VP dominates &amp;quot;loves Mary&amp;quot;, then S&apos; (the translation
of S) is
((loves&apos; ^Mary&apos;) &amp;quot;John&apos;).
Similarly the translation of &amp;quot;Every boy loves Mary&amp;quot;
comes out as
((loves&apos; ^Mary&apos;) &amp;quot;(every&apos; boy&apos;)),
given suitable rules of NP and VP formation.6 Note
the uniform treatment of NPs in the logical formulas,
i.e., (every&apos; boy&apos;) is treated as being of the same se-
mantic category as John&apos;, namely the (unique) seman-
</bodyText>
<footnote confidence="0.996129">
6 The exact function of the intension operator need not con-
cern us here. Roughly speaking, it is used to bring meanings within
the domain of discourse; e.g., while an NP&apos; denotes a property set
at each index, the corresponding ANP denotes the entire NP
intension (mapping from indices to property sets) at each index.
</footnote>
<note confidence="0.723201">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 29
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.998884045045045">
tic category corresponding to the syntactic category
NP. What is the set-theoretic type of that category?
Since (every&apos; boy&apos;) cannot be interpreted as denoting
an individual (at least not without making the rules of
semantic valuation for formulas depend on the struc-
ture of the terms they contain), neither can John&apos;.
The solution is to regard NPs as denoting sets of prop-
erties, where a property determines a set of individuals
at each index, and VPs as sets of such property sets
(or in functional terms, as truth functions over truth
functions over properties). Thus John&apos; does not de-
note an individual, but rather a set of properties,
namely those which John has; (every&apos; boy&apos;) denotes
the set of properties shared by all boys, (a&apos; boy&apos;) the
set of all properties possessed by at least one boy, and
so on. It is not hard to see that the interpretation of
VPs as sets of property sets then leads to the appro-
priate truth conditions for sentences.7
With respect to our objective of building a compre-
hensible, expandable natural language understanding
system, the simplicity of Gazdar&apos;s semantic rules and
their one-to-one correspondence to phrase structure
rules is extremely attractive; however, the semantics of
the intensional logic translations, as sketched above,
seems to us quite unnatural.
Admittedly naturalness is partly a matter of famili-
arity, and we are not about to fault Montague gram-
mar for having novel features (as some writers do,
e.g., Harman 1975). But Montague&apos;s semantics is at
variance with pretheoretical intuitions as well as philo-
sophical tradition, as Montague himself acknowledged
(1970c:268). Intuitively, names denote individuals
(when they denote anything real), not sets of proper-
ties of individuals; extensional transitive verbs express
relations between pairs of individuals, not between
pairs of property sets, and so on; and intuitively,
quantified terms such as &amp;quot;everyone&amp;quot; and &amp;quot;no-one&amp;quot;
simply don&apos;t bear the same sort of relationship to ob-
jects in the world as names, even though the evidence
for placing them in the same syntactic category is
overwhelming. Such objections would carry no weight
if the sole purpose of formal semantics were to pro-
vide an explication of intuitions about truth and logical
consequence, for in that area intensional logic is re-
markably successful. But formal semantics should also
do justice to our intuitions about the relationship be-
tween word and object, where those intuitions are
clear â€” and intensional logic seems at odds with some
of the clearest of those intuitions.8
7 This was the approach in Montague (1970b) and is adopted
in Gazdar (1981a). In another, less commonly adopted approach
NPs are still interpreted as sets of properties but VPs are interpret-
ed simply as properties, the truth condition for a sentence being
that the property denoted by the VP be in the set of properties
denoted by the NP (Montague 1970c, Cresswell 1973),In other
words, the NP is thought of as predicating something about the VP,
rather than the other way around.
There is also a computational objection to inten-
sional logic translations. As indicated in our introduc-
tory remarks, a natural language understanding system
must be able to make inferences that relate the natural
language input to the system&apos;s stored knowledge and
discourse model. A great deal of work in AI has fo-
cused on inference during language understanding and
on the organization of the base of stored knowledge
on which the comprehension process draws. Almost
all of this work has employed more or less convention-
al logics for expressing the stored knowledge. (Even
such idiosyncratic formalisms as Schank&apos;s conceptual
dependency theory (Schank 1973) are much more akin
to, say, first order modal logic than to any form of
intensional logic â€” see Schubert 1976). How are in-
tensional logic formulas to be connected up with
stored knowledge of this conventional type?
One possible answer is that the stored knowledge
should not be of the conventional type at all, but
should itself be expressed in intensional logic. Howev-
er, the history of automatic deduction suggests that
higher-order logics are significantly harder to mecha-
nize than lower-order logics. Developing efficient
inference rules and strategies for intensional logics,
with their arbitrarily complex types and their inten-
sion, extension and lambda abstraction operators in
addition to the usual modal operators, promises to be
very difficult indeed.
Another possible answer is that the intensional
logic translations of input sentences should be post-
processed to yield translations expressed in the lower-
order, more conventional logic of the system&apos;s knowl-
edge base. A difficulty with this answer is that dis-
course inferences need to be computed &apos;on the fly&apos; to
guide syntactic choices. For example, in the sentences
&amp;quot;John saw the bird without binoculars&amp;quot; and &amp;quot;John saw
the bird without tail feathers&amp;quot; the syntactic roles of
the prepositional phrases (i.e., whether they modify
&amp;quot;saw&amp;quot; or &amp;quot;the bird&amp;quot;) can only be determined by infer-
ence. One could uncouple inference from parsing by
computing all possible parses and choosing among the
resultant translations, but this would be cumbersome
and psychologically implausible at best.
As a final remark on the disadvantages of inten-
sional translations, we note that Montague grammar
relies heavily on meaning postulates to deliver simple
consequences, such as
A boy smiles - There is a boy;
8 Thomason reminds us that &amp;quot;...we should not forget the
firmest and most irrefragable kind of data with which a semantic
theory must cope. The theory must harmonize with the actual
denotations taken by the expressions of natural languages,...&amp;quot;, but
confines his further remarks to sentence denotations, i.e., truth
values (Thomason, 1974b:54).
</bodyText>
<page confidence="0.517328">
30 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.524048">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.999746685714286">
(in this instance an extensionalizing postulate is re-
quired for &amp;quot;smiles&amp;quot; â€” see Montague 1970c:263). A
conventional approach dispensing with postulates of
this type would be preferable.
Having stated our misgivings about Montague
grammar, we need to confront the evidence in its fav-
our. Are there compelling reasons for regarding sen-
tential constituents as more or less directly and uni-
formly interpretable? In support of the affirmative,
one can point out the simplicity and elegance of this
strategy from a logical point of view. More tellingly,
one can cite its success record: it has made possible
for the first time the formal characterization of non-
trivial fragments of natural languages, with precisely
defined syntactic-semantic mappings; and as one
would hope, the formal semantics accounts for many
cases of entailment, ambiguity, contradictoriness, and
other semantic phenomena, including some of the sub-
tlest arising from intensional locutions.
Concerning the simplicity of the strategy, we note
that the connection between language and the world
could be just as simple as Montague grammar would
have it, without being quite so direct. Suppose, for a
moment, that people communicated in first-order logic.
Then, to express that Al, Bill and Clyde were born and
raised in New York, we would have to say, in effect,
&amp;quot;Al was born in New York. Al was raised in New
York. Bill was born in New York. ... Clyde was
raised in New York.&amp;quot; The pressure to condense such
redundant verbalizations would be great, and might
well lead to &apos;overlay&apos; verbalizations in which lists
enumerating the non-repeated constituents were fit-
ted into a common sentential matrix. In other words,
it might lead to something like constituent coordina-
tion. But unlike simple constituents, coordinated con-
stituents would not be meaningful in isolation; they
would realize their meaning only upon expansion of
the embedding overlay verbalization into a set of first-
order formulas. Yet the connection between language
and the world would remain simple, assuming that the
syntactic relation between overlay verbalizations and
their first-order translations were simple. It would be
quite pointless to reconstrue the semantics of the en-
hanced language so as to align the denotations of
names with the denotations of coordinated names, for
example, as is done in Montague grammar. While
formally simplifying the semantic mapping function,
such a move would lead to complex and counterintui-
tive semantic types.
The success of Montague grammar in characterizing
fragments of natural languages, with a proper account
of logical relations such as entailment, is indeed strong
evidence in its favour. The only way of challenging
this success is to offer an equally simple, equally via-
ble alternative. In part, this paper is intended as a
move in that direction. While we do not explicitly
discuss logical relations between the translations of
sentences, the kinds of translations produced by the
sample grammar in Section 4 should at least provide
some basis for discussion. To the extent that the
translations are of a conventional type (or easily con-
verted to conventional form), the entailment relations
should be more or less self-evident.
There is one linguistic phenomenon, however,
which deserves preliminary comment since it might be
thought to provide conclusive evidence in favour of
Montague grammar, or at least in favour of the inten-
sional treatment of NPs. This concerns intensional
verbs such as those in sentences (1) and (2), and per-
haps (3):
</bodyText>
<listItem confidence="0.999596333333333">
(1) John looks for a unicorn,
(2) John imagines a unicorn,
(3) John worships a unicorn.
</listItem>
<bodyText confidence="0.975136652173913">
These sentences admit non-referential readings with
respect to the NP &amp;quot;a unicorn&amp;quot;, i.e., readings that do
not entail the existence of a unicorn which is the refer-
ent of the NP. In intensional logic the nonreferential
reading of the first sentence would simply be
((looks-for&apos; A (a&apos; unicorn&apos;)) AJohn&apos;).
The formal semantic analysis of this formula turns out
just as required; that is, its value can be &amp;quot;true&amp;quot; or
&amp;quot;false&amp;quot; (in a given possible world) irrespective of
whether or not there are unicorns (in that world). The
referential reading is a little more complicated, but
presents no difficulties.
It is the non-referential reading which is trouble-
some for conventional logics. For the first sentence,
there seems to be only one conventional translation,
viz.,
3x[[John looks-for x] &amp; [x unicorn]],
and of course, this is the referential reading. There is
no direct way of representing the non-referential read-
ing, since the scope of a quantifier in conventional
logics is always a sentence, never a term.
The only possible escape from the difficulty lies in
translating intensional verbs as complex(non-atomic)
logical expressions involving opaque sentential
operators.9 The extant literature on this subject sup-
ports the view that a satisfactory decomposition can-
not be supplied in all cases (Montague 1970c, Bennett
1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp;
Peters 1981). A review of this literature would be out
of place here; but we would like to indicate that the
case against decomposition (and hence against conven-
tional translations) is not closed, by offering the fol-
9 With regard to our system-building objectives, such resort to
lexical decomposition is no liability: the need for some use of lexi-
cal decomposition to obtain &amp;quot;canonical&amp;quot; representations that facili-
tate inference is widely acknowledged by Al researchers, and car-
ried to extremes by some (e.g., Wilks 1974, Schank 1975).
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 31
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
lowing paraphrases of the three sample sentences.
(Paraphrase (1)&apos; is well-known, except perhaps for the
particular form of adverbial (Quine 1960, Bennett
1974, Partee 1974), while (2)&apos;-(3)&amp;quot; are original).
These could be formalized within a conventional logi-
cal framework allowing for non-truth-functional sen-
tential operators:
</bodyText>
<listItem confidence="0.939094">
(1)&apos; John tries to find a unicorn (by looking around),
(2)&apos; John forms a mental description which could
apply to a unicorn,
(3)&apos; John acts, thinks and feels as if he worshipped a
unicorn.
(3)&amp;quot; John worships an entity which he believes to be
a unicorn.
</listItem>
<bodyText confidence="0.990622288888889">
In each case the operator that is the key to the trans-
lation is italicized. Note that the original ambiguity of
(1) and (2) has been preserved, but can now be con-
strued as a quantifier scope ambiguity in the conven-
tional fashion. In (3)&apos; and (3)&amp;quot; the embedded
&amp;quot;worships&amp;quot; is to be taken in a veridical sense that
entails the existence of the worshippee. It is important
to understand that the translations corresponding to
(3)&apos; and (3)&amp;quot; would not be obtained directly by apply-
ing the rules of the grammar to the original sentence;
rather, they would be obtained by amending the direct
translation, which is patently false for a hearer who
interprets &amp;quot;worships&amp;quot; veridically and does not believe
in unicorns. Thus we are presupposing a mechanism
similar to that required to interpret metaphor on a
Gricean account (Grice 1975). The notion of &amp;quot;acting,
thinking and feeling as if...&amp;quot; may seem rather ad hoc,
but appears to be applicable in a wide variety of cases
where (arguably) non-intensional verbs of human ac-
tion and attitude are used non-referentially, as perhaps
in &amp;quot;John is communing with a spirit&amp;quot;, &amp;quot;John is afraid
of the boogie-man in the attic&amp;quot;, or &amp;quot;John is tracking
down a sasquatch&amp;quot;. Formulation (3)&amp;quot; represents a
more radical alternative, since it supplies an ac-
ceptable interpretation of (3) only if the entity actually
worshipped by John may be an &apos;imaginary unicorn&apos;.
But we may need to add imaginary entities to our
&apos;ontological stable&apos; in any event, since entities may be
explicitly described as imaginary (fictitious, hypotheti-
cal, supposed) and yet be freely referred to in ordinary
discourse. Also, sentences such as &amp;quot;John frequently
dreams about a certain unicorn&amp;quot; (based on an example
in Dowty, Wall and Peters 1981) seem to be untrans-
latable into any logic without recourse to imaginary
entities. Our paraphrases of (3) have the important
advantage of entailing that John has a specific unicorn
in mind, as intuitively required (in contrast with (1)
and (2)). This is not the case for the intensional logic
translation of (3) analogous to that of (1), a fact that
led Bennett to regard &amp;quot;worships&amp;quot; â€” correctly, we think
â€” as extensional (Bennett 1974).
In the light of these considerations, the convention-
al approach to logical translation seems well worth
pursuing. The simplicity of the semantic rules to
which we are led encourages us in this pursuit.
</bodyText>
<sectionHeader confidence="0.64396" genericHeader="method">
3. Syntactic and Semantic Preliminaries
</sectionHeader>
<bodyText confidence="0.9897785">
The logical-form syntax provides for the formation
of simple terms such as
Johnl, x,
quantified terms such as
&lt;some 1 man2&gt;, &lt;the 1 (little2 boy3)&gt;,
simple predicate formulas such as
man2, loves3, P4,
compound predicate formulas such as
</bodyText>
<equation confidence="0.9966365">
(loves2 Mary3), ((loves2 Mary3) Johnl),
[John1 loves2 Mary3],
</equation>
<bodyText confidence="0.866359809523809">
modified predicate formulas such as
(bright3 red4), (passionately2 (loves3 Mary4)),
and lambda abstracts such as
Xx[x shaves2 x], Xy[y expects2 [y wins4]].
Note the use of sharp angle brackets for quantified
terms, square brackets or blunt angle brackets for
compound predicate formulas, and round brackets for
modified predicate formulas. (We explain the use of
square brackets and blunt angle brackets below.) We
also permit sentences (i.e., compound predicate formu-
las with all arguments in place) as operands of senten-
tial operators, as in
[[John5 loves6 Mary7] possible3],
[Sue 1 believes2 [John5 loves5 Mary6]],
[[Johnl feverish3] because4
[John1 has5 malaria6]].
For coordination of expressions of all types
(quantifiers, terms, predicate formulas, modifiers, and
sentential operators) we use sharp angle brackets and
prefix form, as in
&lt;or2 many 1 few3&gt;, &lt;and2 Johnl Bill3&gt;,
</bodyText>
<equation confidence="0.565698">
&lt;and4 (hugs2 Mary3) (kisses5 Sue6)&gt;.
</equation>
<bodyText confidence="0.996171769230769">
The resemblance of coordinated expressions to quanti-
fied terms is intentional: in both cases the sharp angle
brackets signal the presence of an unscoped operator
(viz., the first element in brackets) to be scoped later
on.
Finally, we may want to admit second-order predi-
cates with first-order predicate arguments, as in
[Fido 1 little-for3 dog5], [blue 1 colour4],
[Xx[x kisses 1 Mary2] is-fun3],
though it remains to be seen whether such second-
order predications adequately capture the meaning of
English sentences involving implicit comparatives and
nominalization.
</bodyText>
<page confidence="0.586212">
32 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.86389">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.974205333333333">
Fuller explanations of several of the above features
follow. In outline, we first delve a little further into
the syntax and semantics of predicate formulas; then
we discuss the sources and significance of ambiguities
in the formulas.
Atomic sentences are of the form
[tn P t1 tn_i], (equivalently, (P t1 tn)),
where t1, tn are terms and P is a predicate con-
stant, and the square brackets and blunt angle brack-
ets distinguish infix and prefix syntax respectively.
We regard this sentential form as equivalent to
[tn (...((P t1) t2) tn_t )],
i.e., as obtained by applying an n-ary predicate
successively to n terms. For example,
[John loves Mary] = (loves Mary John)
.[John (loves Mary)] = ((loves Mary) John).10
As in Montague grammar, this predicate application
syntax helps to keep the rules of translation simple: in
most cases the translation of a phrase is just the com-
position of the translations of its top-level constitu-
ents. However, we saw earlier that a functional inter-
pretation of predicate application leads to the interpre-
tation of predicates as telescoped function-valued
functions, whereas we wish to interpret predicates as
n-ary relations (in each possible world) in the conven-
tional way.
We can satisfy this requirement by interpreting
predicate application not as function application, but
rather as leftmost section of the associated relation at
the value of the given argument. For example, let V
denote the semantic valuation function (with a particu-
lar interpretation and possible world understood) and
let
</bodyText>
<equation confidence="0.9980135">
V(P) = f&lt;a,b,c&gt;, &lt;a,b,d&gt;, &lt;e,f,g&gt;j,
V(x) = a, V(y) = b, and V(z) = d,
</equation>
<bodyText confidence="0.87852925">
where P is a triadic predicate symbol, x, y, and z are
individual constants or variables, and a, b, g are
elements of the individual domain D. Then
V((P x)) = f&lt;b,c&gt;, &lt;b,d&gt;j,
</bodyText>
<equation confidence="0.980788">
V((P x y)) = V(((P x) = &lt;c&gt;, &lt;d&gt;j, and
V([z P x y]) = V((((P x) y) z)) =
</equation>
<bodyText confidence="0.974100142857143">
We use the convention {&lt;&gt;} = true, { } = false.
Lambda abstraction can be defined compatibly by
Vi(Xxcb) = {{d} X Vi(x,d) (0) I d E
where I is an interpretation, I(x:d) is an interpretation
identical to I except that x denotes d, and X denotes
Cartesian product (and a particular possible world is
10 we provide the double syntax for purely cosmetic reasons.
In our use of the notation, expressions delimited by square brackets
will generally be complete open or closed sentences, while expres-
sions delimited by blunt angle brackets will be &apos;incomplete
sentences&apos;, i.e., predicates with one or more arguments missing (and
denoting a relation with adicity = number of missing arguments).
understood). It can be verified that the usual lambda-
conversion identities hold, i.e.,
</bodyText>
<equation confidence="0.9994775">
(Xx(P...x...) t) = (P...t...), and
P = Xx(P x) = XxXy(P x y) = ,
</equation>
<bodyText confidence="0.996116469387755">
where P is a predicate of any adicity (including null, if
we use {&lt;)} X A = A for any set A).
As far as modified predicate formulas such as
(bright3 red4) are concerned, we can interpret the
modifiers as functions from n-ary relations to n-ary
relations (perhaps with n restricted to 1).
We now turn to a consideration of the potential
sources of ambiguity in the formulas. One source of
ambiguity noted in the Introduction lies in the primi-
tive logical symbols themselves, which may correspond
ambiguously to various proper logical symbols. The
ambiguous symbols are obtained by the translator via
the first stage of a two-stage lexicon (and with the aid
of morphological analysis, not discussed here). This
first stage merely distinguishes the formal logical roles
of a lexeme, supplying a distinct (but in general still
ambiguous) symbol or compound expression for each
role, along with syntactic information. For example,
the entry for &amp;quot;recover&amp;quot; might distinguish (i) a predi-
cate role with preliminary translation &amp;quot;recovers-from&amp;quot;
and the syntactic information that this is a V admissi-
ble in the rule that expands a VP as a V optionally
followed by a (PP from); (this information is supplied
via the appropriate rule number); and (ii) a predicate
role with preliminary translation &amp;quot;recovers&amp;quot; and the
syntactic information that this is a V admissible in the
rule that expands a VP as a V followed by an NP.
Having obtained a preliminary translation of a lex-
eme in keeping with its apparent syntactic role, the
translator affixes an index to it which has not yet been
used in the current sentence (or if the translation is a
compound expression, it affixes the same index to all
of its primitive symbols). In this way indexed pre-
liminary translations such as Maryl, good2, and
recovers3 are obtained. For example, the verb trans-
lation selected for &amp;quot;recovers&amp;quot; in the sentence context
&amp;quot;John recovers the sofa&amp;quot; would be recovers2,
recovers-from2 being ruled out by the presence of the
NP complement. The second stage of the lexicon sup-
plies alternative final translations of the first-stage
symbols, which in the case of &amp;quot;recovers&amp;quot; might be
RE-COVERS, REGAINS, and so on. Naturally, the
processors that choose among these final symbols
would have to draw on knowledge stored in the propo-
sitional data base and in the representation of the
discourse context.
A second source of ambiguity lies in quantified
terms. The sentence
Someone loves every man
</bodyText>
<note confidence="0.612195">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 33
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.964209943925234">
illustrates a quantifier scope ambiguity arising from a
syntactically unambiguous construction. Its logical-
form translation is
[&lt;some 1 one2&gt; loves3 &lt;every4 man5&gt;],
wherein the relative scopes of the quantifiers somel
and every4 are ambiguous. Quantified terms are in-
tended to be &apos;extracted&apos; in the postprocessing phase to
positions left-adjacent to sentential formulas (which
may already be prefixed with other quantifiers). A
new variable is introduced into each extracted quanti-
fier expression, the angle brackets are changed to
round brackets, and the new variable is substituted for
all occurrences of the extracted term. (Thus the level
of extraction must be &apos;high&apos; enough to encompass all
of these occurrences.) In the above formula, quantifier
extraction reveals the implicit ambiguity, yielding ei-
ther
(some 1 x:[x one2])(every4 y:[y man5])[x loves3 y]
or
(every4 y:[y man5])(some 1 x:[x one2])[x loves3 y],
depending on the order of extraction.
Assuming that somel and every4 correspond to
the standard existential and universal quantifiers, these
translations could be further processed to yield
3x[[x one2] &amp; Vy[[y man5] =&gt; [x loves3 y]]] and
Vy[[y man5] =&gt; 3x[[x one2] &amp; [x loves3 y]]].
However, we may not implement this last conversion
step, since it cannot be carried out for all quantifiers.
For example, as Cresswell remarks, &amp;quot;most A&apos;s are B&apos;s&amp;quot;
cannot be rendered as &amp;quot;for most x, either x is not an
A or x is a B&amp;quot; (Cresswell 1973: 137). (Consider, for
instance, A = dog and B = beagle; then the last state-
ment is true merely because most things are not dogs â€”
irrespective of whether or not most dogs are in fact
beagles.) It appears from recent work by Goebel (to
appear) that standard mechanical inference methods
can readily be extended to deal with formulas with
restricted quantifiers.
A third source of ambiguity lies in coordinated
expressions. For example, the logical form of the
sentence &amp;quot;Every man loves Peggy or Sue&amp;quot; is
[&lt;every 1 man2&gt; loves3 &lt;or5 Peggy4 Sue6&gt;],
which is open to the readings
(every 1 x:[x man2])[ [x loves3 Peggy4] or5
[x loves3 Sue6]]
and
[(every 1 x:[x man2])[x loves3 Peggy4]
or5 (every 1 x:[x man2])[x loves3 Sue6]].
The postprocessing steps required to scope coordina-
tors are similar to those for quantifiers and are illus-
trated in Section 4.11
An important constraint on the disambiguation of
the basic symbols as well as quantified terms and coor-
dinated expressions is that identical expressions (i.e.,
expressions with identical constituent structure, includ-
ing indices) must be identically disambiguated. For
example, &amp;quot;John shaves himself&amp;quot; and &amp;quot;John shaves
John&amp;quot; translate respectively into
[Johnl Xx[x shaves2 x]] = [Johnl shaves2 Johnl],
and
[Johnl shaves2 John3].
The stated constraint ensures that both occurrences of
Johnl in the first formula will ultimately be replaced
by the same unambiguous constant. Similarly
&amp;quot;Someone shaves himself&amp;quot; and &amp;quot;Someone shaves
someone&amp;quot; translate initially into
[&lt;some 1 one2&gt; shaves3 &lt;some 1 one2&gt;] and
[&lt;some 1 one2&gt; shaves3 &lt;some4 one5&gt;]
respectively, and these translations become
(some 1 x:[x one2])[x shaves3 x] and
(some1 x:[x one2])(some4 y:[y one5])[x shaves3 y]
respectively after quantifier extraction. Note that the
two occurrences of &lt;some 1 one2&gt; in the first formula
are extracted in unison and replaced by a common
variable. Indexing will be seen to play a similar role in
the distribution of coordinators that coordinate non-
sentential constituents.
By allowing the above types of ambiguities in the
logical form translations, we are able to separate the
problem of disambiguation from the problems of pars-
ing and translation. This is an important advantage,
since disambiguation depends upon pragmatic factors.
For example, &amp;quot;John admires John&amp;quot; may refer to two
distinct individuals or just to one (perhaps whimsical-
ly), depending on such factors as whether more than
one individual named John has been mentioned in the
current context. Examples involving ambiguities in
nouns, verbs, determiners, etc., are easily supplied.
Similarly, the determination of relative quantifier
scopes involves pragmatic considerations in addition to
level of syntactic embedding and surface order. This
is true both for explicit quantifier scope ambiguities
such as in the sentence &amp;quot;Someone loves every man&amp;quot;,
and for scope ambiguities introduced by decomposi-
tion, such as the decomposition of &amp;quot;seeks&amp;quot; into
XyXx[x tries [x finds y]],
as a result of which a sentence like
John seeks a unicorn
admits the alternative translations
3x[[x unicorn] &amp; [John tries [John finds x]]], and
[John tries 3x[[x unicorn] &amp; [John finds x]]],
neglecting indices. It is simpler to produce a single
output which can then be subjected to pragmatic post-
11 If first-order predicates are to be allowed as arguments of
second-order predicates, then quantifier and coordinator scoping of
the following types must also be allowed: [P...&lt;() R&gt;...] Xx(Q
Rp[x P...y...1, &lt;C P R&gt; Xx[[x P] C [x R]].
</bodyText>
<page confidence="0.610449">
34 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.863757">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.999687">
processing to determine likely quantifier scopes, than
to generate all possible orderings and then to make a
pragmatic choice among them. Much the same can be
said about scoping of coordinators.
We also note that a grammar designed to generate
all possible unambiguous translations of English phras-
es and sentences would have to supply multiple seman-
tic rules for certain syntactic rules. For example, no
one semantic rule can translate a quantifier-noun com-
bination (rule 3 in Section 4) so as to deliver both
readings of &amp;quot;Someone loves every man&amp;quot; upon combi-
nation of the verb translation with the translations of
the NPs. Our use of an ambiguous logical form pre-
serves the rule-to-rule hypothesis.
</bodyText>
<listItem confidence="0.57915">
4. Sample Grammar
</listItem>
<bodyText confidence="0.995693235294118">
Our syntactic rules do not depart significantly from
Gazdar&apos;s. The semantic rules formally resemble
Gazdar&apos;s as well, but of course produce conventionally
interpretable translations of the type described in the
preceding section. As in Gazdar&apos;s semantic rules,
constituent translations are denoted by primed catego-
ry symbols such as NP&apos; and V&apos;. The semantic rules
show how to assemble such translations (along with
the occasional variable and lambda operator) to form
the translations of larger constituents. The transla-
tions of individual lexemes are obtained as described
above.
In operation, the translator generates the minimum
number of brackets consistent with the notational
equivalences stated earlier. For example, in assem-
bling [NP&apos; VP&apos;], with NP&apos; = Johnl and VP&apos; =
[loves2 Mary3], the result is
</bodyText>
<equation confidence="0.519099">
[Johnl loves2 Mary3],
rather than
[Johnl (loves2 Mary3)].
</equation>
<bodyText confidence="0.9996725">
Also, in binding a variable with lambda, the translator
replaces all occurrences of the variable with a previ-
ously unused variable, thus minimizing the need for
later renaming. Finally, it performs lambda conver-
sions on the fly. For example, the result of assembling
[NP&apos; VP&apos;] with NP&apos; = Johnl and
</bodyText>
<equation confidence="0.809122">
VP&apos; = Xx[x shaves2 x],
is
[Johnl shaves2 Johnl].
</equation>
<bodyText confidence="0.999239">
The rules that follow have been adapted from Gaz-
dar (1981a). Note that each rule that involves a lexi-
cal category such as PN, N or V is accompanied by a
specification of the subset of lexical items of that cate-
gory admissible in the rule. This feature is particularly
important for verb subcategorization. In addition,
each rule is followed by (a) a sample phrase accepted
by the rule, (b) an indication of how the logical trans-
lation of the phrase is obtained, and possibly (c) some
words of further explanation.
</bodyText>
<figure confidence="0.99266752631579">
&lt;1, [(NP) (PN)], PN&apos;&gt;, PN(1) = (John, Mary, New York, ...)
(a) Mary
(b) with PN&apos; = Mary6, NP&apos; becomes Mary6.
&lt;2, [(AN) (ADJP) (N)], (ADJP&apos; N&apos;)&gt;, N(2) = [boy, game, noise, ...)
(a) little boy
(b) with ADJP&apos; = little2, N&apos; = boy3,
AN&apos; becomes (little2 boy3);
(c) &amp;quot;little&amp;quot; is taken as a predicate modifier.12
&lt;3, [(NP) (Q) (AN)], &lt;Q&apos; AN&apos;&gt;&gt;, Q(3) = (a, the, all, many, ...}
(a) the little boy
(b) with Q&apos; = the1, AN&apos; = (little2 boy3),
NP&apos; -&gt; &lt;the1 (little2 boy3)&gt;.
&lt;4, [(PP to) (to) (NP)], NP&apos;&gt;
(a) to Mary
(b) with NP&apos; = Mary6, PP&apos; -&gt; Mary6;
(c) PP verb complements have the same meaning as their NP, as per Gazdar (1981a).
&lt;5, [(VP) (V)], V&apos;&gt;, V(5) = [run, smile, disappear, ...)
(a) smiles
(b) with V&apos; = smiles4, VP&apos; -&gt; smiles4.
</figure>
<footnote confidence="0.59209875">
12 Siegel (1979) argues rather persuasively that measure adjec-
tives, unlike genuine predicate modifiers such as &amp;quot;consummate&amp;quot;,
actually combine with terms. For such adjectives we might employ
the semantic rule Xx[[x ADJP] &amp; [x N&apos;]]; in the case of &amp;quot;little&amp;quot;,
we would use ADJP&apos; = (little-for P), where P is an indeterminate
predicate to be replaced pragmatically by a comparison-class predi-
cate. Thus the translation of &amp;quot;little boy&amp;quot; (neglecting indices)
would be Xxqx little-for P] &amp; [x boy]].
</footnote>
<note confidence="0.711539">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 35
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<figure confidence="0.966524">
&lt;6, [(VP) (V) (NP) (PP to)], OP PP&apos; NP&apos;)&gt;
V(6) = (give, hand, tell, ...}
(a) gives Fido to Mary
(b) with V&apos; = gives4, NP&apos; = Fido5, PP&apos; = Mary6, VP&apos; -&gt; (gives4 Mary6 Fido5).
&lt;7, [(VP INF) (to) (VP BASE)], VP&apos;&gt;
(a) to give Fido to Mary
(b) with VP&apos; = (gives4 Mary6 Fido5), the resultant infinitive has the
same meaning.
&lt;8, [(VP) (V) (VP INF)], Ax[x V&apos; [x VP&apos;]]&gt;,
V(8) = (want, expect, try, ...}
(a) wants to give Fido to Mary
(b) with V&apos; = wants2, VP&apos; = [gives4 Mary6 Fido5],
VP&apos; -&gt; Xx3[x3 wants2 [x3 gives4 Mary6 Fido5]];
(c) The formal lambda variable x given in the semantic rule has been replaced by
the new variable x3. Two pairs of square brackets have been deleted, in
accordance with the simplification rules stated earlier.
&lt;9, [(VP) (V) (NP) (VP INF)], (V&apos; [NP&apos; VP&apos;])&gt;,
V(9) = [want, expect, imagine, ...)
(a) wants Bill to give Fido to Mary
(b) with V&apos; = wants2, NP&apos; = 3i113, VP&apos; = (gives4 Mary6 Fido5),
VP&apos; -&gt; (wants2 [Bill3 gives4 Mary6 Fido5]).
&lt;10, [(S DECL) (NP) (VP)], [NP&apos; VP&apos;]&gt;
(a) the little boy smiles
(b) with NP&apos; = &lt;thel (little2 boy3)&gt; and VP&apos; = smiles4, the result is
S&apos; -&gt; [&lt;thel (little2 boy3)&gt; smiles4]. After pragmatic postprocessing
to extract quantifiers, the result might be
S&apos; = (the1 x5: [x5 (little2 boy3)])[x5 smiles4].
</figure>
<subsectionHeader confidence="0.232244">
Further postprocessing to determine referents and disambiguate operators
</subsectionHeader>
<bodyText confidence="0.876537333333333">
and predicates might then yield
S&apos; = [INDIV17 SMILES1],
where INDIV17 is a (possibly new) logical constant unambiguously denoting
the referent of (thel x5: [x5 (little2 boy3)]) and SMILES1 is an unambiguous
logical predicate. &apos;3 If constant INDIV17 is new, i.e., if the context provided
no referent for the definite description, a supplementary assertion like
[INDIV17 (LITTLE2 BOY1)]
would be added to the context representation.
(a)&apos; John wants to give Fido to Mary
(b)&apos; with NP&apos; = John1,
VP&apos; = Xx3[x3 wants2 [x3 gives4 Mary6 Fido5]],
S&apos; -&gt; [John1 wants2 [John1 gives4 Mary6 Fido5]];
(c)&apos; Note that John1 becomes the subject of both the main clause and the
embedded (subordinate) clause.
The reader will observe that we have more or less
fully traced the derivation and translation of the sen-
tences &amp;quot;The little boy smiles&amp;quot; and &amp;quot;John wants to give
Fido to Mary&amp;quot; in the course of the above examples.
The resultant phrase structure trees, with rule numbers
and translations indicated at each node, are shown in
Figs. 1 and 2.
</bodyText>
<footnote confidence="0.565913">
13 Definite singular terms often serve as descriptions to be used
for referent determination, and in such cases it is the name of the
referent, rather than the description itself, which is ultimately
wanted in the formula.
</footnote>
<page confidence="0.607119">
36 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<table confidence="0.6982647">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
rule 10: S&apos; = [NP&apos; VP&apos;)
= [John1 wants2 [John1 gives4 Mary6 Fido5]]
rule 1: NP&apos;
=PN&apos;=John1
rule 8: VP&apos; = Xxix V&apos; ix VP&apos;)]
= Xx31x3 wants2 1x3 gives4 Mary6 Fido5ll
rule 7: (VP INF)&apos; = (VP BASE)&apos;
= Kgives4 Mary6 Fido5)
(VP BASE)
rule 6: VP&apos; = (V&apos; PP&apos; NP&apos; )
= (gives4 Mary6 Fido5)
PN&apos;=John1 V&apos;=wants2
John wants
(PP to)
V&apos;=gives4 rule 1: NP&apos;=PN&apos; rule 4: PP&apos; =NP&apos;
= Fido5 = Mary6
give
PN&apos;=Fido5 rule 1: NP&apos;=PN&apos;
Fido = Mary6
</table>
<figureCaption confidence="0.9599855">
Figure 1. Phrase structure and translation of the sentence Mary
&amp;quot;John wants to give Fido to Mary&amp;quot;.
</figureCaption>
<note confidence="0.444407">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 37
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<equation confidence="0.839619533333333">
rule 10: S&apos; = [NP&apos; VP&apos;)
= [&lt;the1 (little2 boy3)&gt; smiles4]
rule 3: NP&apos; = &lt;Q&apos; AN&apos;&gt;
= &lt;the1 (little2 boy3)&gt;
rule 5: VP&apos;= V&apos;
= smiles4
smiles
Q&apos;=the1 rule 2: AN&apos; = (ADJP&apos; N&apos;)
= (little2 boy3)
the ////
(ADJO
rule n: ADJP&apos;= ADJ&apos;
= little2
ADJ&apos;= little2
little
</equation>
<figureCaption confidence="0.9928665">
Figure 2. Phrase structure and translation of the sentence
The little boy smiles.&amp;quot;
</figureCaption>
<figure confidence="0.922603">
N&apos;=boy3
1
boy
</figure>
<page confidence="0.903045">
38 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.879303">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.999067666666667">
All of the above rules, as well as our versions of
the remaining rules in Gazdar (1981a), are as simple
as the intensional logic versions or simpler. For exam-
ple, our semantic rule 8, i.e., Xx[x V&apos; [x VP&apos;]], may be
contrasted with the corresponding rule suggested by
Gazdar:
</bodyText>
<sectionHeader confidence="0.332988" genericHeader="method">
XP{P Xx[(V&apos; A (VP&apos; XP(P x))) XP(P x)]1.
</sectionHeader>
<bodyText confidence="0.99226505">
Here the lambda variable x, as in our formula, is used
to feed a common logical subject to V&apos; (the transla-
tion of the main verb) and to VP&apos; (the translation of
the embedded infinitive); the variables P and P, on the
other hand, serve to ensure that the arguments of the
V&apos; and VP&apos; functions will be of the correct type. Our
&apos;conventional&apos; rule is simpler because it makes no such
use of lambda abstraction for type-raising and dispens-
es with the intension operator.
Gazdar&apos;s approach to unbounded dependencies
carries over virtually unchanged and can be illustrated
with the sentence
To Mary John wants to give Fido.
Here the PP &amp;quot;to Mary&amp;quot; has been topicalized by ex-
traction from &amp;quot;John wants to give Fido to Mary&amp;quot;,
leaving a PP &apos;gap&apos; at the extraction site. This &apos;gap&apos; is
syntactically embedded within the infinitive VP &amp;quot;to
give Fido&amp;quot;, within the main VP &amp;quot;wants to give Fido&amp;quot;,
and at the highest level, within the sentence &amp;quot;John
wants to give Fido&amp;quot;. In general, the analysis of un-
</bodyText>
<figure confidence="0.9632756">
&lt;11, [(Pp to)/(PP to) t], h&gt;
(a) t
(b) PP&apos; -&gt; h
&lt;6, [(VP)/(PP to) (V) (NP) (PP to)/(PP to)],
(a) give Fido
</figure>
<bodyText confidence="0.9989442">
bounded dependencies requires derived rules for propa-
gating &apos;gaps&apos; from level to level and linking rules for
creating and filling them. The linking rules are ob-
tained from the correspondingly numbered basic rules
by means of the metarule
</bodyText>
<equation confidence="0.992812">
[A X C Y] ==&gt; [A/B X C/B Y],
</equation>
<bodyText confidence="0.9997816">
where A, B and C may be any basic (i.e., non-slash)
syntactic categories such that C can dominate B, and
X, Y may be any sequences (possibly empty) of bas-
ic categories. The linking rules for topicalization are
obtained from the rule schemata
</bodyText>
<listItem confidence="0.2229835">
&lt;11, [B/B t], h&gt;, and
&lt;12, [(S) B (S)/B], (AhS&apos; B&apos;)&gt;,
</listItem>
<bodyText confidence="0.999849230769231">
where B ranges over all basic phrasal categories, and t
is a dummy element (trace). The first of these sche-
mata introduces the free variable h as the translation
of the gap, while the second lambda-abstracts on h
and then supplies B&apos; as the value of the lambda varia-
ble, thus &apos;filling the gap&apos; at the sentence level. At
syntactic nodes intermediate between those admitted
by schemata 11 and 12, the B-gap is transmitted by
derived rules and h is still free.
Of the following rules, 6, 8, and 10 are the particu-
lar derived rules required to propagate the PP-gap in
our example and 11 and 12 the particular linking rules
that create and fill it:
</bodyText>
<figure confidence="0.913114470588235">
PP&apos; NP&apos;)&gt;
(b) with V&apos; = gives5, NP&apos; = Fido6, PP&apos; = h,
VP&apos; -&gt; (gives5 h Fido6)
(C) Note that the semantic rule is unchanged.
&lt;8, [(VP)/(PP to) (V) (VP INF)/(PP to)], Xx[x V&apos; [x VP&apos;]]&gt;
(a) wants to give Fido
(b) with V&apos; = wants3, VP&apos; = (gives5 h Fido6),
VP&apos; -&gt; Xx4[x4 wants3 [x4 gives5 h Fido6]]
&lt;10, [(S)/(PP to) (NP) (VP)/(PP to)], [NP&apos; VP&apos;]&gt;
(a) John wants to give Fido
(b) with NP&apos; = John2,
VP&apos; = Xx4[x4 wants3 [x4 gives5 h Fido6]],
S&apos; -&gt; [John2 wants3 [John2 gives5 h Fido6]]
&lt;12, [(S) (PP to) (S)/(PP to)], (NhS&apos; PP&apos;)&gt;
(a) To Mary John wants to give Fido
(b) With S&apos; as in 10 (b) above and PP&apos; = Mary1,
S&apos; -&gt; [John2 wants3 [John2 gives5 Mary1 Fido6]].
</figure>
<listItem confidence="0.905706666666667">
(c) This translation is logically indistinguishable from the
translation of the untopicalized sentence. However, the
fronting of &amp;quot;to Mary&amp;quot; has left a pragmatic trace: the
</listItem>
<note confidence="0.79756">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 39
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<tableCaption confidence="0.749355285714286">
corresponding argument Maryl has the lowest index, lower
than that of the subject translation John2 (assuming that
symbols are indexed in the order of occurrence of the
lexical items they translate). In subsequent pragmatic
processing, this feature could be used to detect the special
salience of Maryl, without re-examination of the superficial
sentence form.
</tableCaption>
<bodyText confidence="0.962921388888889">
Another example of a sentence that can be ana-
lyzed by such methods, using relative clause rules simi-
lar to those for topicalization, is
Every dog Mary wants to buy is small.
The rules analyze &amp;quot;Mary wants to buy&amp;quot; as an S/NP
with translation
[Mary wants [Mary buys h]],
neglecting indices. A further rule reduces the S/NP to
an R (relative clause), and its semantic part abstracts
on h to yield the predicate
R&apos; = Xh[Mary wants [Mary buys h]]
as the translation of the relative clause. The rules for
NPs can be formulated in such a way that &amp;quot;every dog&amp;quot;
will be translated as
&lt;every Xx[[x dog] &amp; [x R]]&gt;
where R is a free predicate variable that is replaced by
the translation of the relative clause when the NP-R
rule
&lt;13, [(NP) (NP) (R)], &lt;X RNP&apos; R&apos;)&gt;
is applied (cf., Gazdar 1981b; we have ignored multi-
ple relative clauses). The resulting NP translation is
&lt;every Xx[[x dog] &amp; [Mary wants
[Mary buys x]]]&gt;.
The translation of the complete sentence, after extrac-
tion of the quantifier and conversion of the constraint
on the universally quantified variable to an implicative
antecedent, would be
Vy[Ry dog] &amp; [Mary wants [Mary buys y]]]
=&gt; [y (small P)]],
where P is an undetermined predicate (= dog, in the
absence of contrary contextual information).
As a further illustration of Gazdar&apos;s approach and
how easily it is adapted to our purposes, we consider
his metarule for passives:
&lt;[(VP)(V TRAN) (NP) X], (g NP&amp;quot;)&gt; ==&gt;
&lt;[(VP PASS) (V) X {(PP by)}],
</bodyText>
<equation confidence="0.525043">
XP((g P) PP&amp;quot;)).;
</equation>
<bodyText confidence="0.98419556">
i.e., &amp;quot;for every active VP rule that expands VP as a
transitive verb followed by NP, there is to be a passive
VP rule that expands VP as V followed by what, if
anything, followed the NP in the active VP rule, fol-
lowed optionally by a by-PP&amp;quot; (Gazdar 1981a). In the
original and resultant semantic rules, (g ...) represents
the original rule matrix in which NP&amp;quot; is embedded;
thus (g P) is the result of substituting the lambda
variable P (which varies over NP intensions) for NP&amp;quot;
in the original rule. Intuitively, the lambda variable
&apos;reserves&apos; the NP&amp;quot; argument position for later binding
by the subject of the passive sentence. It can be seen
that the metarule will generate a passive VP rule cor-
responding to our rule 6 which will account for sen-
tences such as &amp;quot;Fido was given to Mary by John&amp;quot;.
Moreover, if we introduce a ditransitive rule
&lt;14, [ (VP) (V TRAN) (NP) (NP)],
(V&apos; NP NP &apos; )&gt;14
to allow for sentences such as &amp;quot;John gave Mary Fido&amp;quot;,
the metarule will generate a passive VP rule that ac-
counts for &amp;quot;Mary was given Fido by John&amp;quot;, in which
the indirect rather than direct object has been turned
into the sentence subject.
The only change needed for our purposes is the
replacement of the property variable P introduced by
the metarule by an individual variable x:
...(g NP&apos;)... ==&gt; ...Xx((g x) PP&apos;)...
Once the subject NP of the sentence is supplied via
rule 10, x is replaced by the translation of that NP
upon lambda conversion.
Finally in this section, we shall briefly consider
coordination. Gazdar has supplied general coordina-
tion rule schemata along with a cross-categorical se-
mantics that assigns appropriate formal meanings to
coordinate structures of any category (Gazdar 1980b).
Like Gazdar&apos;s rules, our rules generate logical-form
translations of coordinated constituents such as
&lt;and John Bill&gt;, &lt;or many few&gt;,
&lt;and (hugs Mary) (kisses Sue)&gt;,
echoing the surface forms. However, it should be
clear from our discussion in Section 2 that direct inter-
pretation of expressions translating, say, coordinated
NPs or VPs is not compatible with our conventional
conception of formal semantics. For example, no for-
mal semantic value is assigned directly to the coordi-
nated term in the formula
[&lt;and John Bill&gt; loves Mary].
Rather, interpretation is deferred until the pragmatic
processor has extracted the coordinator from the em-
bedding sentence (much as in the case of quantified
</bodyText>
<footnote confidence="0.92184925">
14 In the computational version of the semantic rules, primed
symbols are actually represented as numbers giving the positions of
the corresponding constituents, e.g., (1 2 3) in rule 14. Thus no
ambiguity can arise.
</footnote>
<page confidence="0.549103">
40 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<subsectionHeader confidence="0.449987">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</subsectionHeader>
<bodyText confidence="0.985467">
terms) and distributed the coordinated terms over
duplicate copies of that sentence, yielding
[[John loves Mary] and [Bill loves Mary]].
We adopt the following coordination schemata
without change. The superscript denotes sequences of
length 1 of the superscripted element. The schema-
ta are accompanied by examples of phrases they admit,
along with (unindexed) translations. The bracketing
in (a) and (a)&apos; indicates syntactic structure.
</bodyText>
<figure confidence="0.992304428571429">
&lt;15, [(A ST) (g) (A)], A&apos;&gt;,
where A is any syntactic category and ar 6 [and, or}
(a) and admires
(b) admires
(a)&apos; or Mary
(b)&apos; Mary
&lt;16, [(A) (A)+ (A ST)], &lt;ST&apos; A&apos;A&apos;...A1&gt;&gt;
(a) loves [and admires]
(b) &lt;and loves admires&gt;
(a)&apos; [Fido Kim] [or Mary]
(b)&apos; &lt;or Fido Kim Mary&gt;
&lt;17, [(A) (A) (A ST)41, &lt;arl A&apos;A&apos;...Al&gt;&gt;
(a) Fido [[or Kim] [or Mary]]
(b) &lt;or Fido Kim Mary&gt;
</figure>
<bodyText confidence="0.965464444444444">
The order in which coordinators are extracted and
distributed is a matter of pragmatic choice. However,
a crucial constraint is that multiple occurrences of a
particular coordinated expression (with particular ind-
ices) must be extracted and distributed in a single
operation, at the level of a sentential formula whose
scope encompasses all of those occurrences (much as
in the case of quantifier extraction). The following
examples illustrate this process.
</bodyText>
<figure confidence="0.6229225">
(a) John loves and admires Fido or Kim
(b) [Johnl &lt;and3 loves2 admires4&gt; &lt;or6 Fido5 Kim7&gt;] -&gt;
</figure>
<table confidence="0.904083571428571">
[[Johnl loves2 &lt;or6 Fido5 Kim7&gt;] and3
[Johnl admires4 &lt;or6 Fido5 Kim7&gt;]] -&gt;
[[[Johnl loves2 Fido5] and3
[Johnl admires4 Fido5]] or6
[[John1 loves2 Kim7] and3
[Johnl admires4 Kim7]]].
(c) Note that once the and3-conjunction has been chosen for initial
</table>
<bodyText confidence="0.948105375">
extraction and distribution, the simultaneous extraction and
distribution of both occurrences of the or6-disjunction at the
highest sentential level is compulsory. The resultant formula
expresses the sense of &amp;quot;John loves and admires Fido or loves
and admires Kim&amp;quot;. Initial extraction of the or6-disjunction
would have led to the (implausible) reading &amp;quot;John loves Fido or
Kim and admires Fido or Kim&amp;quot; (which is true even if John loves
only Fido and admires only Kim).
</bodyText>
<figure confidence="0.5727104">
(a)&apos; All men want to marry Peggy or Sue
(b)&apos; [&lt;alll man2&gt; wants3 [&lt;a111 man2&gt; marries4 &lt;or6 Peggy5 Sue7&gt;]]-&gt;
(a111 x: [x man2])[x wants3 [x marries4 &lt;or6 Peggy5 Sue7&gt;]] -&gt;
(a111 x:[x man2])[x wants3
[[x marries4 Peggy5] or6 [x marries4 Sue7]]].
</figure>
<footnote confidence="0.8853098">
(c)&apos; In the second step above, the coordinator or6 might instead
have been raised to the second highest sentential level, yielding
(a111 x: [x man2])[[x wants3 [x marries4 Peggy5]] or6
[x wants3 [x marries4 Sue7]]],
or to the highest sentential level, yielding
</footnote>
<note confidence="0.848481">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 41
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<equation confidence="0.5695515">
[(a111 x: [x man2])[x wants3 [x marries4 Peggy51] or6
(a111 x: [x man2])[x wants3 [x marries4 Sue7]]].
</equation>
<bodyText confidence="0.964761357142857">
The three readings are logically distinct and all are quite
plausible (in the absence of additional context). The reader
can verify that the first and second readings, but not the
third, could have been obtained by extracting the coordinator
first and the quantifier second.
Finally, we should remark that the distributive rules
are not appropriate for the group reading of coordi-
nate structures in sentences such as
John and Mary carried the sofa (together).
We envisage a mereological interpretation in which
John and Mary together comprise a two-component
entity. However, we refrain from introducing a logical
syntax for such entities here (but see the treatment of
plurals in Schubert, 1982).
</bodyText>
<sectionHeader confidence="0.975198" genericHeader="method">
5. Parsing
</sectionHeader>
<bodyText confidence="0.999990129032258">
Phrase structure grammars are relatively easy to
parse. The most advanced parser for Gazdar-style
grammars that we are aware of is Thompson&apos;s chart-
parser (Thompson 1981), which provides for slash
categories and coordination, but does not (as of this
writing) generate logical translations. We have imple-
mented two small parser-translators for preliminary
experimentation, one written in SNOBOL and the
other in MACLISP. The former uses a recursive de-
scent algorithm and generates intensional logic transla-
tions. The latter is a &apos;left corner&apos; parser that uses our
reformulated semantic rules to generate conventional
translations. It begins by finding a sequence of left-
most phrase-structure-rule branches that lead from the
first word upward to the sentence node. (e.g., Mary
PN NP S). The remaining branches of the
phrase structure rules thus selected form a &amp;quot;frontier&amp;quot;
of expectations. Next a similar initial-unit sequence is
found to connect the second word of the sentence to
the lowest-level (most immediate) expectation, and so
on. There is provision for the definition and use of
systems of features, although we find that the parser
needs to do very little feature checking to stay on the
right syntactic track. Neither parser at present han-
dles slash categories and coordination (although they
could be handled inefficiently by resort to closure of
the grammar under metarules and rule schemata). Ex-
traction of quantifiers from the logical-form transla-
tions is at present based on the level of syntactic em-
bedding and left-to-right order alone, and no other
form of postprocessing is attempted.15
</bodyText>
<footnote confidence="0.984475857142857">
15 Since submission of this paper for publication, we have
become aware of several additional papers on parser-translators
similar to ours. One is by Rosenschein &amp; Shieber (1982), another
by Gawron et al. (1982); in conception these are based quite
directly on the generalized phrase structure grammar of Gazdar and
his collaborators, and use recursive descent parsers. A related
Prolog-based approach is described by McCord (1981, 1982).
</footnote>
<bodyText confidence="0.9999515">
It has been gratifyingly easy to write these parser-
translators, confirming us in the conviction that
Gazdar-style grammars hold great promise for the
design of natural language understanding systems. It
is particularly noteworthy that we found the design of
the translator component an almost trivial task; no
modification of this component will be required even
when the parser is expanded to handle slash categories
and coordination directly. Encouraged by these re-
sults, we have begun to build a full-scale left-corner
parser. A morphological analyzer that can work with
arbitrary sets of formal affix rules is partially imple-
mented; this work, as well as some ideas on the con-
ventional translation of negative adjective prefixes,
plurals, and tense/aspect structure, is reported in
Schubert (1982).
</bodyText>
<sectionHeader confidence="0.799786" genericHeader="method">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999989233333333">
From the point of view of theoretical and com-
putational linguistics, Gazdar&apos;s approach to grammar
offers profound advantages over traditional ap-
proaches: it dispenses with transformations without
loss of insight, offers large linguistic coverage, and
couples simple, semantically well-motivated rules of
translation to the syntactic rules.
We have attempted to show that the advantages of
Gazdar&apos;s approach to grammar can be secured without
commitment to an intensional target logic for the
translations of natural language sentences. To moti-
vate this endeavour, we have argued that there are
philosophical and practical reasons for preferring a
conventional target logic, and that there are as yet no
compelling reasons for abandoning such logics in fav-
our of intensional ones. More concretely, we have
shown how to reformulate Gazdar&apos;s semantic rules
to yield conventional translations, and have briefly
described some extant PSG parsers, including one that
is capable of parsing and translating in accordance
with the reformulated Gazdar grammar (minus metal-
inguistic constructs).
We believe that a parser-interpreter of this type
will prove very useful as the first stage of a natural
language understanding system. Since the grammar
rules are expressed in a concise, individually compre-
hensible form, such a system will be easy to expand
indefinitely. The assignment of a well-defined logical
form to input sentences, compatible with favoured
knowledge representation formalisms, should help to
</bodyText>
<page confidence="0.693315">
42 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.673932">
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
</note>
<bodyText confidence="0.997015333333333">
bring a measure of precision and clarity to the rather
murky area of natural language interpretation by ma-
chine.
</bodyText>
<sectionHeader confidence="0.951186" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999395">
The authors are indebted to Ivan Sag for a series of
very stimulating seminars held by him at the Universi-
ty of Alberta on his linguistic research, and valuable
follow-up discussions. The helpful comments of the
referees and of Lotfi Zadeh are also appreciated. The
research was supported in part by NSERC Operating
Grants A8818 and A2252; preliminary work on the
left-corner parser was carried out by one of the au-
thors (LKS) under an Alexander von Humboldt fel-
lowship in 1978-79.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988820793814433">
Bach, E. 1976 An extension of classical transformational gram-
mar. Mimeo, Univ. of Massachusetts, Amherst, MA.
Bartsch, R. 1976 The Grammar of Adverbials. North-Holland,
Amsterdam.
Bennett, M. 1974 Some extensions of a Montague fragment of
English. Ph.D. dissertation, UCLA; available from the Indiana
University Linguistics Club.
Brame, M.K. 1978 Base Generated Syntax. Noit Amrofer, Seattle,
WA.
Bresnan, J.W. 1976 On the form and functioning of transforma-
tions. Linguistic Inquiry 7 3-40.
Bresnan, J.W. 1978 A realistic transformational grammar. In
Halle, M., Bresnan, J.W., and Miller, G.A., Ed., Linguistic
Theory and Psychological Reality MIT Press, Cambridge, MA.
Chierchia, G. 1981 Nominalization and Montague grammar. A
semantics without types for natural languages. MS, Dept. of
Linguistics, Univ. of Massachusetts, Amherst, MA.
Church, A. 1941 The Calculi of Lambda Conversion. Princeton
Univ. Press, Princeton, NJ.
Cooper, R., and Parsons, T. 1976 Montague grammar, generative
semantics and interpretive semantics. In Partee 1976a, 311-
362.
Craig, J.A., Berezner, S.C., Carney, H.C., and Longyear, C.R.
1966 DEACON: Direct English access and control. Fall Joint
Comp. Conf., Nov. 7-10, San Francisco, CA, AFIPS Conf.
Proc. vol. 29. Spartan Books, Washington, D.C.: 365-380.
Cresswell, M.J. 1973 Logics and Languages. Methuen, London.
Dowty, D.R. 1978 A guide to Montague&apos;s PTO. Indiana Univ.
Linguistics Club, Bloomington, IN.
Dowty, D.R. 1979 Word Meaning and Montague Grammar: The
Semantics of Verbs and Times in Generative Semantics and in
Montague&apos;s PTQ. D. Reidel, Dortrecht.
Dowty, DR., Wall, R., and Peters, S. 1981. An Introduction to
Montague Semantics. D. Reidel, Dortrecht.
Fodor, J.A. 1978 Propositional attitudes. The Monist 61, 501-
523.
Gawron, J.M., King, J.J., Lamping, J., Loebner, E.E., Paulson, E.
A., Pullum, G.K., Sag, I.A., and Wasow, T.A. 1982 Processing
English with a generalized phrase structure grammar. CLS-82-
5, Comp. Science Lab. Tech. Note Series, Hewlett Packard,
Palo Alto, CA. Presented at the 20th Ann. Meet, of the Assoc.
for Computational Linguistics, June 16-18, Univ. of Toronto,
Toronto, Ont.
Gazdar, G. 1980a A phrase structure syntax for comparative
clauses. Glot-Lexical Grammar 165-179.
Gazdar, G. 1980b A cross-categorical semantics for coordination
Linguistics and Philosophy 3, 407-409.
Gazdar, G. 1981a Phrase structure grammar. To appear in Jacob-
son, P. and Pullum, O.K., Ed., The Nature of Syntactic
Representation. D. Reidel, Dortrecht.
Gazdar, G. 1981b Unbounded dependencies and coordinate struc-
ture. Linguistic Inquiry 12.2.
Gazdar, G., Klein, E., Pullum, G. K., and Sag, I. to appear Eng-
lish Syntax.
Gazdar, G., Pullum, O.K., and Sag, I. 1980 A phrase structure
grammar of the English auxiliary system. Unpublished paper.
A slightly different version entitled &amp;quot;Auxiliaries and related
phenomena in a restricted theory of grammar&amp;quot; is available from
Indiana Univ. Linguistics Club; to appear as &amp;quot;Auxiliaries and
related phenomena&amp;quot; in Language.
Gazdar, G. and Sag, I. 1980 Passive and reflexives in phrase
structure grammar. In Groenendijk, J., Janssen, T., and Stok-
hof, M., Ed., Formal Methods in the Study of Language, Proc. 3rd
Amsterdam Coll., March 25-28, 1980. Mathematical Centre
Tracts, Amsterdam.
Goebel, R. 1982 Forthcoming Ph.D. thesis, Dept. of Computer
Science, Univ. of British Columbia, Vancouver, B.C.
Once, H.P. 1975 Logic and conversation. In Davidson, D. and
Harman, G., Ed., The Logic of Grammar. Dickenson, Encino,
CA: 64-75.
Guenthner, F. 1978 Systems of intensional logic and the semantics
of natural language. In Guenther, F. and Rohrer, C., Ed.,
Studies in Formal Semantics. North-Holland, Amsterdam: 41-74.
Harman, G. 1975 Logical form. In Davidson, D. and Harman, G.,
Ed., The Logic of Grammar. Dickenson, Encino, CA: 289-307.
Hughes, G.E. and Cresswell, M.J. 1968 An Introduction to Modal
Logic. Methuen, London.
Jackendoff, R. 1977 X Syntax: A Study of Phrase Structure. MIT
Press, Cambridge, MA.
Kaplan, R.M. (panelist) 1981 A view of parsing. Proc. 19th Ann.
Meet, of the Assoc. for Computational Linguistics, June 29 â€” July
1. Stanford Univ., Stanford, CA, 103-104.
Katz, J., and Fodor, J.A. 1963 The structure of a semantic theory.
Language 39, 170-210.
Knuth, D.E. 1968 Semantics of context-free languages. Mathemat-
ical Systems Theory 2, 127-145.
Lakoff, G. 1971 On generative semantics. In Steinberg, D.D. and
Jakobvitz, L.A., Ed., Semantics: An Interdisciplinary Reader in
Philosophy, Linguistics and Psychology. Cambridge Univ. Press,
New York: 232-296.
Langendoen, T. 1979 On the assignment of constituent structures
to the sentences generated by a transformational grammar. City
Univ. of New York Forum. New York, NY.
Lapointe, S. 1977 Recursiveness and deletion. Linguistic Analysis
3, 227-266.
McCarthy, J. 1979 First-order theories of individual concepts and
propositions. In Michie, D., Ed., Expert Systems in the Micro
Electronic Age. Edinburgh Univ. Press, Edinburgh: 271-287.
McCord, M.C. 1981 Focalizers, the scoping problem, and seman-
tic interpretation rules in logic grammars. Tech. Rep. No.
81-81. Univ. of Kentucky, Lexington, KY. To appear in Proc.
of the Int. Workshop on Logic Programming for Expert Systems,
Logicon, Woodland Hills, CA, Aug. 1981.
McCord, M.C. 1982 Using slots and modifiers in logic grammars
for natural language. Artificial Intelligence 18, 327-367.
Montague, R. 1970a English as a formal language. In Thomason
1974a, 188-221.
Montague, R. 1970b Universal grammar. In Thomason 1974a,
222-246.
Montague, R. 1970c The proper treatment of quantification in
ordinary English. In Thomason 1974a, 247-270.
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 43
Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic
Partee, B.H. 1974 Opacity and scope. In Munitz, M.K., and
Unger, P.K., Ed., Semantics and Philosophy. New York Univ.
Press, New York: 81-101.
Partee, B.H., Ed. 1976a Montague Grammar. Academic Press,
New York.
Partee, B.H. 1976b Some transformational extensions of Montague
grammar. In Partee 1976a, 51-76.
Peters, P.S., and Ritchie, R.W. 1969 Context-sensitive immediate
constituent analysis: context-free languages revisited. In First
Ann. Symp. on Theory of Computing, ACM, New York, 1-8.
Also in Math. Systems Theory 6 (1973) 324-333.
Quine, W.v.0. 1960 Word and Object. MIT Press, Cambridge,
MA.
Robinson, J.J. (chr.) 1981 Panel: perspectives on parsing issues.
Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics,
June 29-July 1, Stanford Univ., Stanford, CA, 95-106.
Rosenschein, S.J., and Shieber, S.M. 1982 Translating English into
logical form. Presented at the 20th Ann. Meet, of the Assoc.
for Computational Linguistics, June 16-18, Univ. of Toronto,
Toronto, Ont.
Sag, I. 1980 A semantic theory of NP-movement dependencies.
To appear in Jacobson, P. and Pullum, G.K., Ed., The Nature of
Syntactic Representation. D. Reidel, Dortrecht.
Schank, R.C. 1973 Identification of conceptualizations underlying
natural language. In Schank, R.C. and Colby, KM., Ed., Com-
puter Models of Thought and Language. W.H. Freeman, San
Francisco: 187-247.
Schank, R.C. 1975 The primitive ACTs of conceptual dependen-
cy. In Advance Papers of Theoretical Issues in Natural Language
Processing Workshop, June 10-13, MIT, Cambridge, MA, 34-37.
Schoenfinkel, M. 1924 Ueber die Bausteine der mathematischen
Logik. Math. Annalen 92, 305-316.
Schubert, L.K. 1976 Extending the expressive power of semantic
networks. Artificial Intelligence 7, 163-198.
Schubert, L.K. 1982 An approach to the syntax and semantics of
affixes in &apos;conventionalized&apos; phrase structure grammar. Proc. of
the 4th Biennial Conf. of the Can. Soc. for Computational Studies
of Intelligence (CSCSI/SCEIO), 17-19 May 1982, Univ. of
Saskatchewan, Saskatoon, Sask.: 189-195.
Schwind, C. 1978a A formalism for the description of question
answering systems. In Bolc, L., Ed., Natural Language Commu-
nication with Computers. Springer-Verlag, Berlin, Heidelberg &amp;
New York: 1-48.
Schwind, C. 1978b The translation of natural language texts into
state logic formulae. Tech. Rep. TUM-INFO-7806, Technische
Univ. Muenchen, available from Bibliothek des Fachbereichs
Mathematik, Technische Univ. Muenchen, D-8000 Muenchen
2, W. Germany.
Siegel, M.E.A. 1979 Measure adjectives in Montague grammar.
In Davis, S., and Mithun, M., Eds., Linguistics, Philosophy, and
Montague Grammar. Univ. of Texas Press, Austin, TX: 223-
262.
Thomason, R.H. ed. 1974a Formal Philosophy: Selected Papers of
Richard Montague. Yale Univ. Press, New Haven, CT.
Thomason, R.H. 1974b Introduction to Thomason 1974a, 1-69.
Thomason, R.H. 1980 A model theory for propositional attitudes.
Linguistics and Philosophy 4, 47-70.
Thompson, F.B. 1966 English for the computer. Fall Joint Comp.
Conf., Nov. 7-10, San Francisco, CA. AFIPS Conf. Proc. Vol.
29, Spartan Books, Washington, D.C., 349-356.
Thompson, H. 1981 Chart parsing and rule schemata in PSG.
Proc. 19th Ann. Meet, of the Assoc. for Computational
Linguistics, June 29-July 1, Stanford Univ., Stanford, CA: 167-
172.
Wilks, Y. 1974 An artificial intelligence approach to machine
translation. In Schank, R.C., and Colby, K.M. Ed., Computer
Models of Thought and Language. W.H. Freeman, San Francis-
co: 114-151.
Winograd, T. 1972 Understanding Natural Language. Academic
Press, New York.
Woods, W.A. 1977 Lunar rocks in natural English: Explorations
in natural language question answerIng. In Zampolli, A., Ed.,
Linguistic Structures Processing. North-Holland, Amsterdam:
521-569.
Lenhart K. Schubert is an associate professor of
computer science at the University of Alberta, Edmonton.
He received the Ph.D. degree in computer science from
the University of Toronto.
Francis Jeffry Pelletier is a professor of philosophy at
the University of Alberta. He received the Ph.D. degree
from the University of California at Los Angeles.
</reference>
<page confidence="0.943845">
44 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510802">
<title confidence="0.983819">From English to Logic: Context-Free Computation of &apos;Conventional&apos; Logical Translation&apos;</title>
<email confidence="0.627757">K.</email>
<affiliation confidence="0.9994215">Department of Computing University of Alberta</affiliation>
<author confidence="0.994463">Francis Jeffry Pelletier</author>
<affiliation confidence="0.9994065">Department of University of Alberta</affiliation>
<address confidence="0.909685">Edmonton, Canada T6G 2H1</address>
<abstract confidence="0.994880928571429">We describe an approach to parsing and logical translation that was inspired by Gazdar&apos;s work on context-free grammar for English. Each grammar rule consists of a syntactic part that specifies an acceptable fragment of a parse tree, and a semantic part that specifies how the logical formulas corresponding to the constituents of the fragment are to be combined to yield the formula for the fragment. However, we have sought to reformulate Gazdar&apos;s semantic rules so as to obtain more or less &apos;conventional&apos; logical translations of English sentences, avoiding the interpretation of NPs as property sets and the use of intensional functors other than certain propositional operators. The reformulated semantic rules often turn out to be slightly simpler than Gazdar&apos;s. Moreover, by using a semantically ambiguous logical syntax for the preliminary translations, we can account for quantifier and coordinator scope ambiguities in syntactically unambiguous sentences without recourse to multiple semantic rules, and are able to separate the disambiguation process from the operation of the parser-translator. We have implemented simple recursive descent and left-corner parsers to demonstrate the practicality of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bach</author>
</authors>
<title>An extension of classical transformational grammar.</title>
<date>1976</date>
<institution>Mimeo, Univ. of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="7904" citStr="Bach 1976" startWordPosition="1204" endWordPosition="1205">rm and meaning. Theoretical linguistics took an important new turn with the work of Montague on the logic of English and later expansions and variants of his theory (e.g., see Thomason 1974a, Partee 1976a, and Cresswell 1973). According to Montague grammar the correspondence between syntactic structure and logical form is much simpler than had generally been supposed: to each lexeme there corresponds a logical term or functor and to each rule of syntactic composition there corresponds a structurally analogous semantic rule of logical composition; this is the so-called rule-to-rule hypothesis [Bach 1976].2 Furthermore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descriptions, are taken to denote property sets. Crucially, the formal semantics of the logical translations produced by the semantic rules of Montague grammar accords by and large with intuitions about entailment, synonymy, ambiguity and other semantic phenomena. 2 Interestingly enough, this linguistic hypothesis was anticipated by Knuth&apos;s work on the semantics of attribute grammars (K</context>
</contexts>
<marker>Bach, 1976</marker>
<rawString>Bach, E. 1976 An extension of classical transformational grammar. Mimeo, Univ. of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bartsch</author>
</authors>
<title>The Grammar of Adverbials.</title>
<date>1976</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam.</location>
<marker>Bartsch, 1976</marker>
<rawString>Bartsch, R. 1976 The Grammar of Adverbials. North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bennett</author>
</authors>
<title>Some extensions of a Montague fragment of English.</title>
<date>1974</date>
<institution>Indiana University Linguistics Club.</institution>
<note>Ph.D. dissertation, UCLA; available from the</note>
<contexts>
<context position="32279" citStr="Bennett 1974" startWordPosition="4999" endWordPosition="5000">ms to be only one conventional translation, viz., 3x[[John looks-for x] &amp; [x unicorn]], and of course, this is the referential reading. There is no direct way of representing the non-referential reading, since the scope of a quantifier in conventional logics is always a sentence, never a term. The only possible escape from the difficulty lies in translating intensional verbs as complex(non-atomic) logical expressions involving opaque sentential operators.9 The extant literature on this subject supports the view that a satisfactory decomposition cannot be supplied in all cases (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal</context>
<context position="35774" citStr="Bennett 1974" startWordPosition="5572" endWordPosition="5573">and yet be freely referred to in ordinary discourse. Also, sentences such as &amp;quot;John frequently dreams about a certain unicorn&amp;quot; (based on an example in Dowty, Wall and Peters 1981) seem to be untranslatable into any logic without recourse to imaginary entities. Our paraphrases of (3) have the important advantage of entailing that John has a specific unicorn in mind, as intuitively required (in contrast with (1) and (2)). This is not the case for the intensional logic translation of (3) analogous to that of (1), a fact that led Bennett to regard &amp;quot;worships&amp;quot; â€” correctly, we think â€” as extensional (Bennett 1974). In the light of these considerations, the conventional approach to logical translation seems well worth pursuing. The simplicity of the semantic rules to which we are led encourages us in this pursuit. 3. Syntactic and Semantic Preliminaries The logical-form syntax provides for the formation of simple terms such as Johnl, x, quantified terms such as &lt;some 1 man2&gt;, &lt;the 1 (little2 boy3)&gt;, simple predicate formulas such as man2, loves3, P4, compound predicate formulas such as (loves2 Mary3), ((loves2 Mary3) Johnl), [John1 loves2 Mary3], modified predicate formulas such as (bright3 red4), (pass</context>
</contexts>
<marker>Bennett, 1974</marker>
<rawString>Bennett, M. 1974 Some extensions of a Montague fragment of English. Ph.D. dissertation, UCLA; available from the Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Brame</author>
</authors>
<title>Base Generated Syntax. Noit Amrofer,</title>
<date>1978</date>
<location>Seattle, WA.</location>
<contexts>
<context position="10520" citStr="Brame 1978" startWordPosition="1591" endWordPosition="1592"> K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause construc</context>
</contexts>
<marker>Brame, 1978</marker>
<rawString>Brame, M.K. 1978 Base Generated Syntax. Noit Amrofer, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Bresnan</author>
</authors>
<title>On the form and functioning of transformations.</title>
<date>1976</date>
<journal>Linguistic Inquiry</journal>
<volume>7</volume>
<pages>3--40</pages>
<contexts>
<context position="12579" citStr="Bresnan 1976" startWordPosition="1896" endWordPosition="1897">s the assumption by using metagrammatical devices to achieve descriptive elegance. These devices include rule-schemata (e.g., coordination schemata that yield the rules of coordinate structure for all coordinators and all syntactic categories), and metarules (e.g., a passive metarule that takes any transitive-VP rule as &apos;input&apos; and generates a corresponding passive-VP rule as &apos;output&apos; by deleting the 3 We use traditional category symbols in our exposition, occasionally followed by supplementary features, e.g., (V TRAN) for transitive verb. Gazdar actually assumes a two-bar 5 system (e.g., see Bresnan 1976, Jackendoff 1977) that distinguishes between R, and X categories (e.g., V, V, and V, equivalent to the traditional S, VP and V respectively) and employs complex symbols whose first component specifies the &apos;number of bars&apos; and whose second component supplies a feature bundle encoding syntactic category, subcategorization, and morphosyntactic and morphological information. object NP from the input rule and appending an optional by-PP). Although metarules resemble transformational rules, they map rules into rules rather than trees into trees, leaving the grammar itself contextfree. Another key i</context>
</contexts>
<marker>Bresnan, 1976</marker>
<rawString>Bresnan, J.W. 1976 On the form and functioning of transformations. Linguistic Inquiry 7 3-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Bresnan</author>
</authors>
<title>A realistic transformational grammar. In</title>
<date>1978</date>
<booktitle>Linguistic Theory and Psychological Reality</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10493" citStr="Bresnan 1978" startWordPosition="1587" endWordPosition="1588">January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalizatio</context>
</contexts>
<marker>Bresnan, 1978</marker>
<rawString>Bresnan, J.W. 1978 A realistic transformational grammar. In Halle, M., Bresnan, J.W., and Miller, G.A., Ed., Linguistic Theory and Psychological Reality MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chierchia</author>
</authors>
<title>Nominalization and Montague grammar. A semantics without types for natural languages.</title>
<date>1981</date>
<tech>MS,</tech>
<institution>Dept. of Linguistics, Univ. of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="15797" citStr="Chierchia 1981" startWordPosition="2392" endWordPosition="2393"> some second-order predicate constants into our logical vocabulary, and may ultimately want to employ a full-fledged second-order logic, in view of such sentences as &amp;quot;Every good general has at least some of Napoleon&apos;s qualities&amp;quot;. On the other hand, we may pare down rather than expand the logical apparatus, opting for a logic that treats properties, propositions and other intensional entities as first-order individuals. This type of treatment, which avoids the unwanted identity of logically equivalent propositions, appears to be gaining currency (e.g., Fodor 1978, McCarthy 1979, Thomason 1980, Chierchia 1981). Some minor adjustments would be required in our rules of logical translation. 28 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic But the formula derives its interpretation from its stipulated logical equivalence to (a111 x:[x man2])[x morta13], which may in turn become Vxax HUMAN] =&gt; [x MORTAL]], after disambiguation.5 2. Intensional and &apos;Conventional&apos; Translations We should emphasize at the outset that our objective is not to impugn Montague grammar, but merely to make the point that</context>
</contexts>
<marker>Chierchia, 1981</marker>
<rawString>Chierchia, G. 1981 Nominalization and Montague grammar. A semantics without types for natural languages. MS, Dept. of Linguistics, Univ. of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Church</author>
</authors>
<title>The Calculi of Lambda Conversion.</title>
<date>1941</date>
<publisher>Princeton Univ. Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="18584" citStr="Church 1941" startWordPosition="2808" endWordPosition="2809">nslation will have the structure [John loves Mary], in which &amp;quot;John&amp;quot; and &amp;quot;Mary&amp;quot; combine with the verb at the same level of constituent structure. In itself, this difference is not important. It only becomes important when syntactic composition is assumed to correspond to function application in the semantic domain. This is done in Montague grammar 5 We consistently use infix form (with the predicate following its first argument) and square brackets for complete sentential formulas. by resort to the Schoenfinkel-Church treatment of many-place functions as one-place functions (Schoenfinkel 1924, Church 1941). For example, the predicate &amp;quot;loves&amp;quot; in the above sentence is interpreted as a one-place function that yields a one-place function when applied to its argument (in this instance, when applied to the semantic value of &amp;quot;Mary&amp;quot;, it yields the function that is the semantic value of &amp;quot;loves Mary&amp;quot;). The resultant function in turn yields a sentence value when applied to its argument (in this instance, when applied to the semantic value of &amp;quot;John&amp;quot;, it yields the proposition expressed by &amp;quot;John loves Mary&amp;quot;). Thus, a dyadic predicator like &amp;quot;loves&amp;quot; is no longer interpreted as a set of pairs of individuals (a</context>
</contexts>
<marker>Church, 1941</marker>
<rawString>Church, A. 1941 The Calculi of Lambda Conversion. Princeton Univ. Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
<author>T Parsons</author>
</authors>
<title>Montague grammar, generative semantics and interpretive semantics.</title>
<date>1976</date>
<booktitle>In Partee 1976a,</booktitle>
<pages>311--362</pages>
<contexts>
<context position="10275" citStr="Cooper &amp; Parsons 1976" startWordPosition="1557" endWordPosition="1560">sions; for example, they incorporated tests for selectional restrictions and other forms of inference, with unrestricted use of the computational power of LISP. American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide ra</context>
</contexts>
<marker>Cooper, Parsons, 1976</marker>
<rawString>Cooper, R., and Parsons, T. 1976 Montague grammar, generative semantics and interpretive semantics. In Partee 1976a, 311-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Craig</author>
<author>S C Berezner</author>
<author>H C Carney</author>
<author>C R Longyear</author>
</authors>
<title>DEACON: Direct English access and control.</title>
<date>1966</date>
<booktitle>Fall Joint Comp. Conf.,</booktitle>
<volume>29</volume>
<pages>7--10</pages>
<publisher>Spartan Books,</publisher>
<location>San Francisco, CA,</location>
<contexts>
<context position="9024" citStr="Craig et al. 1966" startWordPosition="1378" endWordPosition="1381">is linguistic hypothesis was anticipated by Knuth&apos;s work on the semantics of attribute grammars (Knuth 1968). Schwind (1978) has applied Knuth&apos;s insights to the development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded. There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-defined computer memory structures (Thompson 1966). However, DEACON&apos;s semantic rules performed direct semantic evaluation of sorts (via computations over a data base) rather than constructing logical translations. The systems of Winograd (1972) and Woods (1977) constructed input translations prior to evaluation, using semantic rules associated with particular syntactic structures. However, these rules neither corresponded oneto-one to syntactic rules nor limited interpretive operations to </context>
</contexts>
<marker>Craig, Berezner, Carney, Longyear, 1966</marker>
<rawString>Craig, J.A., Berezner, S.C., Carney, H.C., and Longyear, C.R. 1966 DEACON: Direct English access and control. Fall Joint Comp. Conf., Nov. 7-10, San Francisco, CA, AFIPS Conf. Proc. vol. 29. Spartan Books, Washington, D.C.: 365-380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Cresswell</author>
</authors>
<title>Logics and Languages.</title>
<date>1973</date>
<location>Methuen, London.</location>
<contexts>
<context position="7520" citStr="Cresswell 1973" startWordPosition="1147" endWordPosition="1148">tions, offering no guidance to Al researchers whose primary objective was to compute &apos;meaning representations&apos; for natural language utterances. Katz and Fodor&apos;s markerese (Katz &amp; Fodor 1963) was patently inadequate as a meaning representation language from an AT point of view, and Generative Semantics (Lakoff 1971) never did develop into a formal theory of the relation between surface form and meaning. Theoretical linguistics took an important new turn with the work of Montague on the logic of English and later expansions and variants of his theory (e.g., see Thomason 1974a, Partee 1976a, and Cresswell 1973). According to Montague grammar the correspondence between syntactic structure and logical form is much simpler than had generally been supposed: to each lexeme there corresponds a logical term or functor and to each rule of syntactic composition there corresponds a structurally analogous semantic rule of logical composition; this is the so-called rule-to-rule hypothesis [Bach 1976].2 Furthermore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descr</context>
<context position="24319" citStr="Cresswell 1973" startWordPosition="3741" endWordPosition="3742">bly successful. But formal semantics should also do justice to our intuitions about the relationship between word and object, where those intuitions are clear â€” and intensional logic seems at odds with some of the clearest of those intuitions.8 7 This was the approach in Montague (1970b) and is adopted in Gazdar (1981a). In another, less commonly adopted approach NPs are still interpreted as sets of properties but VPs are interpreted simply as properties, the truth condition for a sentence being that the property denoted by the VP be in the set of properties denoted by the NP (Montague 1970c, Cresswell 1973),In other words, the NP is thought of as predicating something about the VP, rather than the other way around. There is also a computational objection to intensional logic translations. As indicated in our introductory remarks, a natural language understanding system must be able to make inferences that relate the natural language input to the system&apos;s stored knowledge and discourse model. A great deal of work in AI has focused on inference during language understanding and on the organization of the base of stored knowledge on which the comprehension process draws. Almost all of this work has</context>
<context position="44710" citStr="Cresswell 1973" startWordPosition="7019" endWordPosition="7020">ery4 y:[y man5])[x loves3 y] or (every4 y:[y man5])(some 1 x:[x one2])[x loves3 y], depending on the order of extraction. Assuming that somel and every4 correspond to the standard existential and universal quantifiers, these translations could be further processed to yield 3x[[x one2] &amp; Vy[[y man5] =&gt; [x loves3 y]]] and Vy[[y man5] =&gt; 3x[[x one2] &amp; [x loves3 y]]]. However, we may not implement this last conversion step, since it cannot be carried out for all quantifiers. For example, as Cresswell remarks, &amp;quot;most A&apos;s are B&apos;s&amp;quot; cannot be rendered as &amp;quot;for most x, either x is not an A or x is a B&amp;quot; (Cresswell 1973: 137). (Consider, for instance, A = dog and B = beagle; then the last statement is true merely because most things are not dogs â€” irrespective of whether or not most dogs are in fact beagles.) It appears from recent work by Goebel (to appear) that standard mechanical inference methods can readily be extended to deal with formulas with restricted quantifiers. A third source of ambiguity lies in coordinated expressions. For example, the logical form of the sentence &amp;quot;Every man loves Peggy or Sue&amp;quot; is [&lt;every 1 man2&gt; loves3 &lt;or5 Peggy4 Sue6&gt;], which is open to the readings (every 1 x:[x man2])[ [x</context>
</contexts>
<marker>Cresswell, 1973</marker>
<rawString>Cresswell, M.J. 1973 Logics and Languages. Methuen, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>A guide to Montague&apos;s PTO. Indiana Univ. Linguistics Club,</title>
<date>1978</date>
<location>Bloomington, IN.</location>
<contexts>
<context position="32304" citStr="Dowty 1978" startWordPosition="5003" endWordPosition="5004">nal translation, viz., 3x[[John looks-for x] &amp; [x unicorn]], and of course, this is the referential reading. There is no direct way of representing the non-referential reading, since the scope of a quantifier in conventional logics is always a sentence, never a term. The only possible escape from the difficulty lies in translating intensional verbs as complex(non-atomic) logical expressions involving opaque sentential operators.9 The extant literature on this subject supports the view that a satisfactory decomposition cannot be supplied in all cases (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal of Computational Linguis</context>
</contexts>
<marker>Dowty, 1978</marker>
<rawString>Dowty, D.R. 1978 A guide to Montague&apos;s PTO. Indiana Univ. Linguistics Club, Bloomington, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar: The Semantics of Verbs and Times in Generative Semantics and in</title>
<date>1979</date>
<marker>Dowty, 1979</marker>
<rawString>Dowty, D.R. 1979 Word Meaning and Montague Grammar: The Semantics of Verbs and Times in Generative Semantics and in Montague&apos;s PTQ. D. Reidel, Dortrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DR Dowty</author>
<author>R Wall</author>
<author>S Peters</author>
</authors>
<title>An Introduction to Montague Semantics.</title>
<date>1981</date>
<location>D. Reidel, Dortrecht.</location>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>Dowty, DR., Wall, R., and Peters, S. 1981. An Introduction to Montague Semantics. D. Reidel, Dortrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Fodor</author>
</authors>
<title>Propositional attitudes.</title>
<date>1978</date>
<journal>The Monist</journal>
<volume>61</volume>
<pages>501--523</pages>
<contexts>
<context position="15750" citStr="Fodor 1978" startWordPosition="2386" endWordPosition="2387">rta13]. 4 We admit predicate modifiers and some second-order predicate constants into our logical vocabulary, and may ultimately want to employ a full-fledged second-order logic, in view of such sentences as &amp;quot;Every good general has at least some of Napoleon&apos;s qualities&amp;quot;. On the other hand, we may pare down rather than expand the logical apparatus, opting for a logic that treats properties, propositions and other intensional entities as first-order individuals. This type of treatment, which avoids the unwanted identity of logically equivalent propositions, appears to be gaining currency (e.g., Fodor 1978, McCarthy 1979, Thomason 1980, Chierchia 1981). Some minor adjustments would be required in our rules of logical translation. 28 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic But the formula derives its interpretation from its stipulated logical equivalence to (a111 x:[x man2])[x morta13], which may in turn become Vxax HUMAN] =&gt; [x MORTAL]], after disambiguation.5 2. Intensional and &apos;Conventional&apos; Translations We should emphasize at the outset that our objective is not to impugn Mont</context>
</contexts>
<marker>Fodor, 1978</marker>
<rawString>Fodor, J.A. 1978 Propositional attitudes. The Monist 61, 501-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Gawron</author>
<author>J J King</author>
<author>J Lamping</author>
<author>E E Loebner</author>
<author>E A Paulson</author>
<author>G K Pullum</author>
<author>I A Sag</author>
<author>T A Wasow</author>
</authors>
<title>Processing English with a generalized phrase structure grammar. CLS-82-5,</title>
<date>1982</date>
<booktitle>Comp. Science Lab. Tech. Note Series,</booktitle>
<institution>Univ. of Toronto,</institution>
<location>Hewlett Packard, Palo Alto, CA.</location>
<contexts>
<context position="69696" citStr="Gawron et al. (1982)" startWordPosition="11189" endWordPosition="11192">right syntactic track. Neither parser at present handles slash categories and coordination (although they could be handled inefficiently by resort to closure of the grammar under metarules and rule schemata). Extraction of quantifiers from the logical-form translations is at present based on the level of syntactic embedding and left-to-right order alone, and no other form of postprocessing is attempted.15 15 Since submission of this paper for publication, we have become aware of several additional papers on parser-translators similar to ours. One is by Rosenschein &amp; Shieber (1982), another by Gawron et al. (1982); in conception these are based quite directly on the generalized phrase structure grammar of Gazdar and his collaborators, and use recursive descent parsers. A related Prolog-based approach is described by McCord (1981, 1982). It has been gratifyingly easy to write these parsertranslators, confirming us in the conviction that Gazdar-style grammars hold great promise for the design of natural language understanding systems. It is particularly noteworthy that we found the design of the translator component an almost trivial task; no modification of this component will be required even when the </context>
</contexts>
<marker>Gawron, King, Lamping, Loebner, Paulson, Pullum, Sag, Wasow, 1982</marker>
<rawString>Gawron, J.M., King, J.J., Lamping, J., Loebner, E.E., Paulson, E. A., Pullum, G.K., Sag, I.A., and Wasow, T.A. 1982 Processing English with a generalized phrase structure grammar. CLS-82-5, Comp. Science Lab. Tech. Note Series, Hewlett Packard, Palo Alto, CA. Presented at the 20th Ann. Meet, of the Assoc. for Computational Linguistics, June 16-18, Univ. of Toronto, Toronto, Ont.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Gazdar</author>
</authors>
<title>1980a A phrase structure syntax for comparative clauses.</title>
<journal>Glot-Lexical Grammar</journal>
<pages>165--179</pages>
<marker>Gazdar, </marker>
<rawString>Gazdar, G. 1980a A phrase structure syntax for comparative clauses. Glot-Lexical Grammar 165-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>A cross-categorical semantics for coordination</title>
<date>1980</date>
<journal>Linguistics and Philosophy</journal>
<volume>3</volume>
<pages>407--409</pages>
<contexts>
<context position="10743" citStr="Gazdar 1980" startWordPosition="1622" endWordPosition="1623">ts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause constructions and comparatives. Yet the grammar itself makes no use of transformations; it consists entirely of phrase structure rules, with a nodeadmissibility rather than generative interpretation. For example, the rule [(S) (NP)</context>
<context position="63253" citStr="Gazdar 1980" startWordPosition="10175" endWordPosition="10176">ect has been turned into the sentence subject. The only change needed for our purposes is the replacement of the property variable P introduced by the metarule by an individual variable x: ...(g NP&apos;)... ==&gt; ...Xx((g x) PP&apos;)... Once the subject NP of the sentence is supplied via rule 10, x is replaced by the translation of that NP upon lambda conversion. Finally in this section, we shall briefly consider coordination. Gazdar has supplied general coordination rule schemata along with a cross-categorical semantics that assigns appropriate formal meanings to coordinate structures of any category (Gazdar 1980b). Like Gazdar&apos;s rules, our rules generate logical-form translations of coordinated constituents such as &lt;and John Bill&gt;, &lt;or many few&gt;, &lt;and (hugs Mary) (kisses Sue)&gt;, echoing the surface forms. However, it should be clear from our discussion in Section 2 that direct interpretation of expressions translating, say, coordinated NPs or VPs is not compatible with our conventional conception of formal semantics. For example, no formal semantic value is assigned directly to the coordinated term in the formula [&lt;and John Bill&gt; loves Mary]. Rather, interpretation is deferred until the pragmatic proc</context>
</contexts>
<marker>Gazdar, 1980</marker>
<rawString>Gazdar, G. 1980b A cross-categorical semantics for coordination Linguistics and Philosophy 3, 407-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Phrase structure grammar.</title>
<date>1981</date>
<tech>D. Reidel, Dortrecht.</tech>
<note>To appear in</note>
<contexts>
<context position="24023" citStr="Gazdar (1981" startWordPosition="3691" endWordPosition="3692">en though the evidence for placing them in the same syntactic category is overwhelming. Such objections would carry no weight if the sole purpose of formal semantics were to provide an explication of intuitions about truth and logical consequence, for in that area intensional logic is remarkably successful. But formal semantics should also do justice to our intuitions about the relationship between word and object, where those intuitions are clear â€” and intensional logic seems at odds with some of the clearest of those intuitions.8 7 This was the approach in Montague (1970b) and is adopted in Gazdar (1981a). In another, less commonly adopted approach NPs are still interpreted as sets of properties but VPs are interpreted simply as properties, the truth condition for a sentence being that the property denoted by the VP be in the set of properties denoted by the NP (Montague 1970c, Cresswell 1973),In other words, the NP is thought of as predicating something about the VP, rather than the other way around. There is also a computational objection to intensional logic translations. As indicated in our introductory remarks, a natural language understanding system must be able to make inferences that</context>
<context position="50371" citStr="Gazdar (1981" startWordPosition="7912" endWordPosition="7914">stent with the notational equivalences stated earlier. For example, in assembling [NP&apos; VP&apos;], with NP&apos; = Johnl and VP&apos; = [loves2 Mary3], the result is [Johnl loves2 Mary3], rather than [Johnl (loves2 Mary3)]. Also, in binding a variable with lambda, the translator replaces all occurrences of the variable with a previously unused variable, thus minimizing the need for later renaming. Finally, it performs lambda conversions on the fly. For example, the result of assembling [NP&apos; VP&apos;] with NP&apos; = Johnl and VP&apos; = Xx[x shaves2 x], is [Johnl shaves2 Johnl]. The rules that follow have been adapted from Gazdar (1981a). Note that each rule that involves a lexical category such as PN, N or V is accompanied by a specification of the subset of lexical items of that category admissible in the rule. This feature is particularly important for verb subcategorization. In addition, each rule is followed by (a) a sample phrase accepted by the rule, (b) an indication of how the logical translation of the phrase is obtained, and possibly (c) some words of further explanation. &lt;1, [(NP) (PN)], PN&apos;&gt;, PN(1) = (John, Mary, New York, ...) (a) Mary (b) with PN&apos; = Mary6, NP&apos; becomes Mary6. &lt;2, [(AN) (ADJP) (N)], (ADJP&apos; N&apos;)&gt;</context>
<context position="56219" citStr="Gazdar (1981" startWordPosition="8932" endWordPosition="8933">glish to Logic rule 10: S&apos; = [NP&apos; VP&apos;) = [&lt;the1 (little2 boy3)&gt; smiles4] rule 3: NP&apos; = &lt;Q&apos; AN&apos;&gt; = &lt;the1 (little2 boy3)&gt; rule 5: VP&apos;= V&apos; = smiles4 smiles Q&apos;=the1 rule 2: AN&apos; = (ADJP&apos; N&apos;) = (little2 boy3) the //// (ADJO rule n: ADJP&apos;= ADJ&apos; = little2 ADJ&apos;= little2 little Figure 2. Phrase structure and translation of the sentence The little boy smiles.&amp;quot; N&apos;=boy3 1 boy 38 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic All of the above rules, as well as our versions of the remaining rules in Gazdar (1981a), are as simple as the intensional logic versions or simpler. For example, our semantic rule 8, i.e., Xx[x V&apos; [x VP&apos;]], may be contrasted with the corresponding rule suggested by Gazdar: XP{P Xx[(V&apos; A (VP&apos; XP(P x))) XP(P x)]1. Here the lambda variable x, as in our formula, is used to feed a common logical subject to V&apos; (the translation of the main verb) and to VP&apos; (the translation of the embedded infinitive); the variables P and P, on the other hand, serve to ensure that the arguments of the V&apos; and VP&apos; functions will be of the correct type. Our &apos;conventional&apos; rule is simpler because it makes</context>
<context position="60868" citStr="Gazdar 1981" startWordPosition="9764" endWordPosition="9765">mall. The rules analyze &amp;quot;Mary wants to buy&amp;quot; as an S/NP with translation [Mary wants [Mary buys h]], neglecting indices. A further rule reduces the S/NP to an R (relative clause), and its semantic part abstracts on h to yield the predicate R&apos; = Xh[Mary wants [Mary buys h]] as the translation of the relative clause. The rules for NPs can be formulated in such a way that &amp;quot;every dog&amp;quot; will be translated as &lt;every Xx[[x dog] &amp; [x R]]&gt; where R is a free predicate variable that is replaced by the translation of the relative clause when the NP-R rule &lt;13, [(NP) (NP) (R)], &lt;X RNP&apos; R&apos;)&gt; is applied (cf., Gazdar 1981b; we have ignored multiple relative clauses). The resulting NP translation is &lt;every Xx[[x dog] &amp; [Mary wants [Mary buys x]]]&gt;. The translation of the complete sentence, after extraction of the quantifier and conversion of the constraint on the universally quantified variable to an implicative antecedent, would be Vy[Ry dog] &amp; [Mary wants [Mary buys y]]] =&gt; [y (small P)]], where P is an undetermined predicate (= dog, in the absence of contrary contextual information). As a further illustration of Gazdar&apos;s approach and how easily it is adapted to our purposes, we consider his metarule for pass</context>
</contexts>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. 1981a Phrase structure grammar. To appear in Jacobson, P. and Pullum, O.K., Ed., The Nature of Syntactic Representation. D. Reidel, Dortrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Unbounded dependencies and coordinate structure. Linguistic Inquiry 12.2.</title>
<date>1981</date>
<contexts>
<context position="24023" citStr="Gazdar (1981" startWordPosition="3691" endWordPosition="3692">en though the evidence for placing them in the same syntactic category is overwhelming. Such objections would carry no weight if the sole purpose of formal semantics were to provide an explication of intuitions about truth and logical consequence, for in that area intensional logic is remarkably successful. But formal semantics should also do justice to our intuitions about the relationship between word and object, where those intuitions are clear â€” and intensional logic seems at odds with some of the clearest of those intuitions.8 7 This was the approach in Montague (1970b) and is adopted in Gazdar (1981a). In another, less commonly adopted approach NPs are still interpreted as sets of properties but VPs are interpreted simply as properties, the truth condition for a sentence being that the property denoted by the VP be in the set of properties denoted by the NP (Montague 1970c, Cresswell 1973),In other words, the NP is thought of as predicating something about the VP, rather than the other way around. There is also a computational objection to intensional logic translations. As indicated in our introductory remarks, a natural language understanding system must be able to make inferences that</context>
<context position="50371" citStr="Gazdar (1981" startWordPosition="7912" endWordPosition="7914">stent with the notational equivalences stated earlier. For example, in assembling [NP&apos; VP&apos;], with NP&apos; = Johnl and VP&apos; = [loves2 Mary3], the result is [Johnl loves2 Mary3], rather than [Johnl (loves2 Mary3)]. Also, in binding a variable with lambda, the translator replaces all occurrences of the variable with a previously unused variable, thus minimizing the need for later renaming. Finally, it performs lambda conversions on the fly. For example, the result of assembling [NP&apos; VP&apos;] with NP&apos; = Johnl and VP&apos; = Xx[x shaves2 x], is [Johnl shaves2 Johnl]. The rules that follow have been adapted from Gazdar (1981a). Note that each rule that involves a lexical category such as PN, N or V is accompanied by a specification of the subset of lexical items of that category admissible in the rule. This feature is particularly important for verb subcategorization. In addition, each rule is followed by (a) a sample phrase accepted by the rule, (b) an indication of how the logical translation of the phrase is obtained, and possibly (c) some words of further explanation. &lt;1, [(NP) (PN)], PN&apos;&gt;, PN(1) = (John, Mary, New York, ...) (a) Mary (b) with PN&apos; = Mary6, NP&apos; becomes Mary6. &lt;2, [(AN) (ADJP) (N)], (ADJP&apos; N&apos;)&gt;</context>
<context position="56219" citStr="Gazdar (1981" startWordPosition="8932" endWordPosition="8933">glish to Logic rule 10: S&apos; = [NP&apos; VP&apos;) = [&lt;the1 (little2 boy3)&gt; smiles4] rule 3: NP&apos; = &lt;Q&apos; AN&apos;&gt; = &lt;the1 (little2 boy3)&gt; rule 5: VP&apos;= V&apos; = smiles4 smiles Q&apos;=the1 rule 2: AN&apos; = (ADJP&apos; N&apos;) = (little2 boy3) the //// (ADJO rule n: ADJP&apos;= ADJ&apos; = little2 ADJ&apos;= little2 little Figure 2. Phrase structure and translation of the sentence The little boy smiles.&amp;quot; N&apos;=boy3 1 boy 38 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic All of the above rules, as well as our versions of the remaining rules in Gazdar (1981a), are as simple as the intensional logic versions or simpler. For example, our semantic rule 8, i.e., Xx[x V&apos; [x VP&apos;]], may be contrasted with the corresponding rule suggested by Gazdar: XP{P Xx[(V&apos; A (VP&apos; XP(P x))) XP(P x)]1. Here the lambda variable x, as in our formula, is used to feed a common logical subject to V&apos; (the translation of the main verb) and to VP&apos; (the translation of the embedded infinitive); the variables P and P, on the other hand, serve to ensure that the arguments of the V&apos; and VP&apos; functions will be of the correct type. Our &apos;conventional&apos; rule is simpler because it makes</context>
<context position="60868" citStr="Gazdar 1981" startWordPosition="9764" endWordPosition="9765">mall. The rules analyze &amp;quot;Mary wants to buy&amp;quot; as an S/NP with translation [Mary wants [Mary buys h]], neglecting indices. A further rule reduces the S/NP to an R (relative clause), and its semantic part abstracts on h to yield the predicate R&apos; = Xh[Mary wants [Mary buys h]] as the translation of the relative clause. The rules for NPs can be formulated in such a way that &amp;quot;every dog&amp;quot; will be translated as &lt;every Xx[[x dog] &amp; [x R]]&gt; where R is a free predicate variable that is replaced by the translation of the relative clause when the NP-R rule &lt;13, [(NP) (NP) (R)], &lt;X RNP&apos; R&apos;)&gt; is applied (cf., Gazdar 1981b; we have ignored multiple relative clauses). The resulting NP translation is &lt;every Xx[[x dog] &amp; [Mary wants [Mary buys x]]]&gt;. The translation of the complete sentence, after extraction of the quantifier and conversion of the constraint on the universally quantified variable to an implicative antecedent, would be Vy[Ry dog] &amp; [Mary wants [Mary buys y]]] =&gt; [y (small P)]], where P is an undetermined predicate (= dog, in the absence of contrary contextual information). As a further illustration of Gazdar&apos;s approach and how easily it is adapted to our purposes, we consider his metarule for pass</context>
</contexts>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. 1981b Unbounded dependencies and coordinate structure. Linguistic Inquiry 12.2.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G K Pullum</author>
<author>I Sag</author>
</authors>
<note>to appear English Syntax.</note>
<marker>Gazdar, Klein, Pullum, Sag, </marker>
<rawString>Gazdar, G., Klein, E., Pullum, G. K., and Sag, I. to appear English Syntax.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>O K Pullum</author>
<author>I Sag</author>
</authors>
<title>A phrase structure grammar of the English auxiliary system. Unpublished paper. A slightly different version entitled &amp;quot;Auxiliaries and related phenomena in a restricted theory of grammar&amp;quot; is available from Indiana Univ. Linguistics Club; to appear as &amp;quot;Auxiliaries and related phenomena&amp;quot; in Language.</title>
<date>1980</date>
<marker>Gazdar, Pullum, Sag, 1980</marker>
<rawString>Gazdar, G., Pullum, O.K., and Sag, I. 1980 A phrase structure grammar of the English auxiliary system. Unpublished paper. A slightly different version entitled &amp;quot;Auxiliaries and related phenomena in a restricted theory of grammar&amp;quot; is available from Indiana Univ. Linguistics Club; to appear as &amp;quot;Auxiliaries and related phenomena&amp;quot; in Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>I Sag</author>
</authors>
<title>Passive and reflexives in phrase structure grammar. In</title>
<date>1980</date>
<booktitle>Formal Methods in the Study of Language, Proc. 3rd</booktitle>
<location>Amsterdam Coll.,</location>
<contexts>
<context position="10795" citStr="Gazdar &amp; Sag 1980" startWordPosition="1630" endWordPosition="1633">, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause constructions and comparatives. Yet the grammar itself makes no use of transformations; it consists entirely of phrase structure rules, with a nodeadmissibility rather than generative interpretation. For example, the rule [(S) (NP) (VP)] states that a fragment with root S, left bran</context>
</contexts>
<marker>Gazdar, Sag, 1980</marker>
<rawString>Gazdar, G. and Sag, I. 1980 Passive and reflexives in phrase structure grammar. In Groenendijk, J., Janssen, T., and Stokhof, M., Ed., Formal Methods in the Study of Language, Proc. 3rd Amsterdam Coll., March 25-28, 1980. Mathematical Centre Tracts, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Goebel</author>
</authors>
<title>Forthcoming Ph.D.</title>
<date>1982</date>
<tech>thesis,</tech>
<institution>Dept. of Computer Science, Univ. of British Columbia,</institution>
<location>Vancouver, B.C.</location>
<marker>Goebel, 1982</marker>
<rawString>Goebel, R. 1982 Forthcoming Ph.D. thesis, Dept. of Computer Science, Univ. of British Columbia, Vancouver, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Once</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<pages>64--75</pages>
<location>Dickenson, Encino, CA:</location>
<marker>Once, 1975</marker>
<rawString>Once, H.P. 1975 Logic and conversation. In Davidson, D. and Harman, G., Ed., The Logic of Grammar. Dickenson, Encino, CA: 64-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Guenthner</author>
</authors>
<title>Systems of intensional logic and the semantics of natural language. In</title>
<date>1978</date>
<pages>41--74</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="14761" citStr="Guenthner 1978" startWordPosition="2231" endWordPosition="2232"> other nonextensional propositional operators, and with a Kripkestyle possible-worlds semantics (Hughes &amp; Cresswell 1968).4 The logic employed by Montague in his first formal fragment of English comes rather close to what we have in mind (Montague 1970a), while the intensional logics of the later fragments introduce the unconventional features we hope to avoid (1970b,c). It is the treatment in these later fragments that is usually referred to by the term &amp;quot;Montague grammar&amp;quot;. (For a detailed discussion of the distinction between conventional logics in the above sense and intensional logics, see Guenthner 1978). We should stress that it is semantics, not syntax, which is the crux of the distinction. We shall take certain liberties with conventional logical syntax, aligning it more nearly with the surface structure; but this will not lead to major departures from conventional semantics. For example, our syntax of terms allows syntactically unfamiliar formulas such as [&lt; alll man2&gt; morta13]. 4 We admit predicate modifiers and some second-order predicate constants into our logical vocabulary, and may ultimately want to employ a full-fledged second-order logic, in view of such sentences as &amp;quot;Every good g</context>
</contexts>
<marker>Guenthner, 1978</marker>
<rawString>Guenthner, F. 1978 Systems of intensional logic and the semantics of natural language. In Guenther, F. and Rohrer, C., Ed., Studies in Formal Semantics. North-Holland, Amsterdam: 41-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Harman</author>
</authors>
<title>Logical form. In</title>
<date>1975</date>
<pages>289--307</pages>
<location>Dickenson, Encino, CA:</location>
<contexts>
<context position="22871" citStr="Harman 1975" startWordPosition="3510" endWordPosition="3511"> VPs as sets of property sets then leads to the appropriate truth conditions for sentences.7 With respect to our objective of building a comprehensible, expandable natural language understanding system, the simplicity of Gazdar&apos;s semantic rules and their one-to-one correspondence to phrase structure rules is extremely attractive; however, the semantics of the intensional logic translations, as sketched above, seems to us quite unnatural. Admittedly naturalness is partly a matter of familiarity, and we are not about to fault Montague grammar for having novel features (as some writers do, e.g., Harman 1975). But Montague&apos;s semantics is at variance with pretheoretical intuitions as well as philosophical tradition, as Montague himself acknowledged (1970c:268). Intuitively, names denote individuals (when they denote anything real), not sets of properties of individuals; extensional transitive verbs express relations between pairs of individuals, not between pairs of property sets, and so on; and intuitively, quantified terms such as &amp;quot;everyone&amp;quot; and &amp;quot;no-one&amp;quot; simply don&apos;t bear the same sort of relationship to objects in the world as names, even though the evidence for placing them in the same syntacti</context>
</contexts>
<marker>Harman, 1975</marker>
<rawString>Harman, G. 1975 Logical form. In Davidson, D. and Harman, G., Ed., The Logic of Grammar. Dickenson, Encino, CA: 289-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hughes</author>
<author>M J Cresswell</author>
</authors>
<title>An Introduction to Modal Logic.</title>
<date>1968</date>
<location>Methuen, London.</location>
<contexts>
<context position="14267" citStr="Hughes &amp; Cresswell 1968" startWordPosition="2148" endWordPosition="2151">with the basic metarule for passives and the coordination schemata. First, however, we would like to motivate our attempt to reformulate Gazdar&apos;s semantic rules so as to yield &apos;conventional&apos; logical translations (Section 2), and to explain the syntactic and semantic idiosyncrasies of our target logic (Section 3). By &apos;conventional&apos; logics we mean first-order (and perhaps second-order) predicate logics, augmented with a lambda operator, necessity operator, propositional attitude operators and perhaps other nonextensional propositional operators, and with a Kripkestyle possible-worlds semantics (Hughes &amp; Cresswell 1968).4 The logic employed by Montague in his first formal fragment of English comes rather close to what we have in mind (Montague 1970a), while the intensional logics of the later fragments introduce the unconventional features we hope to avoid (1970b,c). It is the treatment in these later fragments that is usually referred to by the term &amp;quot;Montague grammar&amp;quot;. (For a detailed discussion of the distinction between conventional logics in the above sense and intensional logics, see Guenthner 1978). We should stress that it is semantics, not syntax, which is the crux of the distinction. We shall take c</context>
</contexts>
<marker>Hughes, Cresswell, 1968</marker>
<rawString>Hughes, G.E. and Cresswell, M.J. 1968 An Introduction to Modal Logic. Methuen, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>X Syntax: A Study of Phrase Structure.</title>
<date>1977</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12597" citStr="Jackendoff 1977" startWordPosition="1898" endWordPosition="1899">on by using metagrammatical devices to achieve descriptive elegance. These devices include rule-schemata (e.g., coordination schemata that yield the rules of coordinate structure for all coordinators and all syntactic categories), and metarules (e.g., a passive metarule that takes any transitive-VP rule as &apos;input&apos; and generates a corresponding passive-VP rule as &apos;output&apos; by deleting the 3 We use traditional category symbols in our exposition, occasionally followed by supplementary features, e.g., (V TRAN) for transitive verb. Gazdar actually assumes a two-bar 5 system (e.g., see Bresnan 1976, Jackendoff 1977) that distinguishes between R, and X categories (e.g., V, V, and V, equivalent to the traditional S, VP and V respectively) and employs complex symbols whose first component specifies the &apos;number of bars&apos; and whose second component supplies a feature bundle encoding syntactic category, subcategorization, and morphosyntactic and morphological information. object NP from the input rule and appending an optional by-PP). Although metarules resemble transformational rules, they map rules into rules rather than trees into trees, leaving the grammar itself contextfree. Another key innovation is the u</context>
</contexts>
<marker>Jackendoff, 1977</marker>
<rawString>Jackendoff, R. 1977 X Syntax: A Study of Phrase Structure. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>A view of parsing.</title>
<date>1981</date>
<booktitle>Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics,</booktitle>
<volume>29</volume>
<pages>103--104</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="2943" citStr="Kaplan 1981" startWordPosition="437" endWordPosition="438">y a recent panel discussion on parsing issues (Robinson 1981). In the words of one of the panelists, &amp;quot;I take it to be uncontroversial that, other things being equal, a homogenized system is less preferable on both practical and scientific grounds than one that naturally decomposes. Practically, such a system is easier to build and maintain, since the parts can be designed, developed, and understood to a certain extent in isolation... Scientifically, a decomposable system is much more likely to provide insight into the process of natural language comprehension, whether by machines or people.&amp;quot; (Kaplan 1981) The panelists also emphasized that structural decomposition by no means precludes interleaving or paralCopyright 1982 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/82/010026-19$01.00 26 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K</context>
</contexts>
<marker>Kaplan, 1981</marker>
<rawString>Kaplan, R.M. (panelist) 1981 A view of parsing. Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics, June 29 â€” July 1. Stanford Univ., Stanford, CA, 103-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Katz</author>
<author>J A Fodor</author>
</authors>
<title>The structure of a semantic theory.</title>
<date>1963</date>
<journal>Language</journal>
<volume>39</volume>
<pages>170--210</pages>
<contexts>
<context position="7095" citStr="Katz &amp; Fodor 1963" startWordPosition="1076" endWordPosition="1079">vices such as co-indexing procedures, filters and constraints on movement, and the complexity of their interactions. Moreover, the prospects for writing efficient transformational parsers seemed poor, given that transformational grammars can in principle generate all recursively enumerable languages. But most importantly, generative grammarians developed syntactic theories more or less independently of any semantic considerations, offering no guidance to Al researchers whose primary objective was to compute &apos;meaning representations&apos; for natural language utterances. Katz and Fodor&apos;s markerese (Katz &amp; Fodor 1963) was patently inadequate as a meaning representation language from an AT point of view, and Generative Semantics (Lakoff 1971) never did develop into a formal theory of the relation between surface form and meaning. Theoretical linguistics took an important new turn with the work of Montague on the logic of English and later expansions and variants of his theory (e.g., see Thomason 1974a, Partee 1976a, and Cresswell 1973). According to Montague grammar the correspondence between syntactic structure and logical form is much simpler than had generally been supposed: to each lexeme there correspo</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, J., and Fodor, J.A. 1963 The structure of a semantic theory. Language 39, 170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>Semantics of context-free languages.</title>
<date>1968</date>
<journal>Mathematical Systems Theory</journal>
<volume>2</volume>
<pages>127--145</pages>
<contexts>
<context position="8514" citStr="Knuth 1968" startWordPosition="1295" endWordPosition="1296">6].2 Furthermore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descriptions, are taken to denote property sets. Crucially, the formal semantics of the logical translations produced by the semantic rules of Montague grammar accords by and large with intuitions about entailment, synonymy, ambiguity and other semantic phenomena. 2 Interestingly enough, this linguistic hypothesis was anticipated by Knuth&apos;s work on the semantics of attribute grammars (Knuth 1968). Schwind (1978) has applied Knuth&apos;s insights to the development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded. There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject</context>
</contexts>
<marker>Knuth, 1968</marker>
<rawString>Knuth, D.E. 1968 Semantics of context-free languages. Mathematical Systems Theory 2, 127-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>On generative semantics. In</title>
<date>1971</date>
<pages>232--296</pages>
<publisher>Cambridge Univ. Press,</publisher>
<location>New York:</location>
<contexts>
<context position="7221" citStr="Lakoff 1971" startWordPosition="1098" endWordPosition="1099">rospects for writing efficient transformational parsers seemed poor, given that transformational grammars can in principle generate all recursively enumerable languages. But most importantly, generative grammarians developed syntactic theories more or less independently of any semantic considerations, offering no guidance to Al researchers whose primary objective was to compute &apos;meaning representations&apos; for natural language utterances. Katz and Fodor&apos;s markerese (Katz &amp; Fodor 1963) was patently inadequate as a meaning representation language from an AT point of view, and Generative Semantics (Lakoff 1971) never did develop into a formal theory of the relation between surface form and meaning. Theoretical linguistics took an important new turn with the work of Montague on the logic of English and later expansions and variants of his theory (e.g., see Thomason 1974a, Partee 1976a, and Cresswell 1973). According to Montague grammar the correspondence between syntactic structure and logical form is much simpler than had generally been supposed: to each lexeme there corresponds a logical term or functor and to each rule of syntactic composition there corresponds a structurally analogous semantic ru</context>
</contexts>
<marker>Lakoff, 1971</marker>
<rawString>Lakoff, G. 1971 On generative semantics. In Steinberg, D.D. and Jakobvitz, L.A., Ed., Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Psychology. Cambridge Univ. Press, New York: 232-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Langendoen</author>
</authors>
<title>On the assignment of constituent structures to the sentences generated by a transformational grammar. City Univ. of New York Forum.</title>
<date>1979</date>
<location>New York, NY.</location>
<contexts>
<context position="10538" citStr="Langendoen 1979" startWordPosition="1593" endWordPosition="1594"> and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause constructions and comparat</context>
</contexts>
<marker>Langendoen, 1979</marker>
<rawString>Langendoen, T. 1979 On the assignment of constituent structures to the sentences generated by a transformational grammar. City Univ. of New York Forum. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lapointe</author>
</authors>
<title>Recursiveness and deletion.</title>
<date>1977</date>
<journal>Linguistic Analysis</journal>
<volume>3</volume>
<pages>227--266</pages>
<contexts>
<context position="10508" citStr="Lapointe 1977" startWordPosition="1589" endWordPosition="1590">1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative cla</context>
</contexts>
<marker>Lapointe, 1977</marker>
<rawString>Lapointe, S. 1977 Recursiveness and deletion. Linguistic Analysis 3, 227-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
</authors>
<title>First-order theories of individual concepts and propositions.</title>
<date>1979</date>
<booktitle>In Michie, D., Ed., Expert Systems in the Micro Electronic Age. Edinburgh Univ.</booktitle>
<pages>271--287</pages>
<publisher>Press,</publisher>
<location>Edinburgh:</location>
<contexts>
<context position="15765" citStr="McCarthy 1979" startWordPosition="2388" endWordPosition="2389"> admit predicate modifiers and some second-order predicate constants into our logical vocabulary, and may ultimately want to employ a full-fledged second-order logic, in view of such sentences as &amp;quot;Every good general has at least some of Napoleon&apos;s qualities&amp;quot;. On the other hand, we may pare down rather than expand the logical apparatus, opting for a logic that treats properties, propositions and other intensional entities as first-order individuals. This type of treatment, which avoids the unwanted identity of logically equivalent propositions, appears to be gaining currency (e.g., Fodor 1978, McCarthy 1979, Thomason 1980, Chierchia 1981). Some minor adjustments would be required in our rules of logical translation. 28 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic But the formula derives its interpretation from its stipulated logical equivalence to (a111 x:[x man2])[x morta13], which may in turn become Vxax HUMAN] =&gt; [x MORTAL]], after disambiguation.5 2. Intensional and &apos;Conventional&apos; Translations We should emphasize at the outset that our objective is not to impugn Montague grammar, b</context>
</contexts>
<marker>McCarthy, 1979</marker>
<rawString>McCarthy, J. 1979 First-order theories of individual concepts and propositions. In Michie, D., Ed., Expert Systems in the Micro Electronic Age. Edinburgh Univ. Press, Edinburgh: 271-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Focalizers, the scoping problem, and semantic interpretation rules in logic grammars.</title>
<date>1981</date>
<booktitle>Proc. of the Int. Workshop on Logic Programming for Expert Systems,</booktitle>
<tech>Tech. Rep. No. 81-81.</tech>
<institution>Univ. of Kentucky,</institution>
<location>Lexington, KY.</location>
<note>To appear in</note>
<contexts>
<context position="69915" citStr="McCord (1981" startWordPosition="11223" endWordPosition="11224">tifiers from the logical-form translations is at present based on the level of syntactic embedding and left-to-right order alone, and no other form of postprocessing is attempted.15 15 Since submission of this paper for publication, we have become aware of several additional papers on parser-translators similar to ours. One is by Rosenschein &amp; Shieber (1982), another by Gawron et al. (1982); in conception these are based quite directly on the generalized phrase structure grammar of Gazdar and his collaborators, and use recursive descent parsers. A related Prolog-based approach is described by McCord (1981, 1982). It has been gratifyingly easy to write these parsertranslators, confirming us in the conviction that Gazdar-style grammars hold great promise for the design of natural language understanding systems. It is particularly noteworthy that we found the design of the translator component an almost trivial task; no modification of this component will be required even when the parser is expanded to handle slash categories and coordination directly. Encouraged by these results, we have begun to build a full-scale left-corner parser. A morphological analyzer that can work with arbitrary sets of</context>
</contexts>
<marker>McCord, 1981</marker>
<rawString>McCord, M.C. 1981 Focalizers, the scoping problem, and semantic interpretation rules in logic grammars. Tech. Rep. No. 81-81. Univ. of Kentucky, Lexington, KY. To appear in Proc. of the Int. Workshop on Logic Programming for Expert Systems, Logicon, Woodland Hills, CA, Aug. 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Using slots and modifiers in logic grammars for natural language.</title>
<date>1982</date>
<journal>Artificial Intelligence</journal>
<volume>18</volume>
<pages>327--367</pages>
<marker>McCord, 1982</marker>
<rawString>McCord, M.C. 1982 Using slots and modifiers in logic grammars for natural language. Artificial Intelligence 18, 327-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a formal language.</title>
<date>1970</date>
<booktitle>In Thomason 1974a,</booktitle>
<pages>188--221</pages>
<contexts>
<context position="14398" citStr="Montague 1970" startWordPosition="2173" endWordPosition="2174">r&apos;s semantic rules so as to yield &apos;conventional&apos; logical translations (Section 2), and to explain the syntactic and semantic idiosyncrasies of our target logic (Section 3). By &apos;conventional&apos; logics we mean first-order (and perhaps second-order) predicate logics, augmented with a lambda operator, necessity operator, propositional attitude operators and perhaps other nonextensional propositional operators, and with a Kripkestyle possible-worlds semantics (Hughes &amp; Cresswell 1968).4 The logic employed by Montague in his first formal fragment of English comes rather close to what we have in mind (Montague 1970a), while the intensional logics of the later fragments introduce the unconventional features we hope to avoid (1970b,c). It is the treatment in these later fragments that is usually referred to by the term &amp;quot;Montague grammar&amp;quot;. (For a detailed discussion of the distinction between conventional logics in the above sense and intensional logics, see Guenthner 1978). We should stress that it is semantics, not syntax, which is the crux of the distinction. We shall take certain liberties with conventional logical syntax, aligning it more nearly with the surface structure; but this will not lead to ma</context>
<context position="23990" citStr="Montague (1970" startWordPosition="3685" endWordPosition="3686">o objects in the world as names, even though the evidence for placing them in the same syntactic category is overwhelming. Such objections would carry no weight if the sole purpose of formal semantics were to provide an explication of intuitions about truth and logical consequence, for in that area intensional logic is remarkably successful. But formal semantics should also do justice to our intuitions about the relationship between word and object, where those intuitions are clear â€” and intensional logic seems at odds with some of the clearest of those intuitions.8 7 This was the approach in Montague (1970b) and is adopted in Gazdar (1981a). In another, less commonly adopted approach NPs are still interpreted as sets of properties but VPs are interpreted simply as properties, the truth condition for a sentence being that the property denoted by the VP be in the set of properties denoted by the NP (Montague 1970c, Cresswell 1973),In other words, the NP is thought of as predicating something about the VP, rather than the other way around. There is also a computational objection to intensional logic translations. As indicated in our introductory remarks, a natural language understanding system mus</context>
<context position="27432" citStr="Montague 1970" startWordPosition="4229" endWordPosition="4230"> There is a boy; 8 Thomason reminds us that &amp;quot;...we should not forget the firmest and most irrefragable kind of data with which a semantic theory must cope. The theory must harmonize with the actual denotations taken by the expressions of natural languages,...&amp;quot;, but confines his further remarks to sentence denotations, i.e., truth values (Thomason, 1974b:54). 30 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic (in this instance an extensionalizing postulate is required for &amp;quot;smiles&amp;quot; â€” see Montague 1970c:263). A conventional approach dispensing with postulates of this type would be preferable. Having stated our misgivings about Montague grammar, we need to confront the evidence in its favour. Are there compelling reasons for regarding sentential constituents as more or less directly and uniformly interpretable? In support of the affirmative, one can point out the simplicity and elegance of this strategy from a logical point of view. More tellingly, one can cite its success record: it has made possible for the first time the formal characterization of nontrivial fragments of natural languages</context>
<context position="32264" citStr="Montague 1970" startWordPosition="4997" endWordPosition="4998">tence, there seems to be only one conventional translation, viz., 3x[[John looks-for x] &amp; [x unicorn]], and of course, this is the referential reading. There is no direct way of representing the non-referential reading, since the scope of a quantifier in conventional logics is always a sentence, never a term. The only possible escape from the difficulty lies in translating intensional verbs as complex(non-atomic) logical expressions involving opaque sentential operators.9 The extant literature on this subject supports the view that a satisfactory decomposition cannot be supplied in all cases (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). A</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, R. 1970a English as a formal language. In Thomason 1974a, 188-221.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Montague</author>
</authors>
<title>1970b Universal grammar.</title>
<booktitle>In Thomason 1974a,</booktitle>
<pages>222--246</pages>
<marker>Montague, </marker>
<rawString>Montague, R. 1970b Universal grammar. In Thomason 1974a, 222-246.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Montague</author>
</authors>
<title>1970c The proper treatment of quantification in ordinary English.</title>
<booktitle>In Thomason 1974a,</booktitle>
<pages>247--270</pages>
<marker>Montague, </marker>
<rawString>Montague, R. 1970c The proper treatment of quantification in ordinary English. In Thomason 1974a, 247-270.</rawString>
</citation>
<citation valid="true">
<title>43 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<contexts>
<context position="69663" citStr="(1982)" startWordPosition="11186" endWordPosition="11186">ing to stay on the right syntactic track. Neither parser at present handles slash categories and coordination (although they could be handled inefficiently by resort to closure of the grammar under metarules and rule schemata). Extraction of quantifiers from the logical-form translations is at present based on the level of syntactic embedding and left-to-right order alone, and no other form of postprocessing is attempted.15 15 Since submission of this paper for publication, we have become aware of several additional papers on parser-translators similar to ours. One is by Rosenschein &amp; Shieber (1982), another by Gawron et al. (1982); in conception these are based quite directly on the generalized phrase structure grammar of Gazdar and his collaborators, and use recursive descent parsers. A related Prolog-based approach is described by McCord (1981, 1982). It has been gratifyingly easy to write these parsertranslators, confirming us in the conviction that Gazdar-style grammars hold great promise for the design of natural language understanding systems. It is particularly noteworthy that we found the design of the translator component an almost trivial task; no modification of this componen</context>
</contexts>
<marker>1982</marker>
<rawString>American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 43 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Partee</author>
</authors>
<title>Opacity and scope.</title>
<date>1974</date>
<pages>81--101</pages>
<publisher>Univ. Press,</publisher>
<location>New York</location>
<contexts>
<context position="32292" citStr="Partee 1974" startWordPosition="5001" endWordPosition="5002">one conventional translation, viz., 3x[[John looks-for x] &amp; [x unicorn]], and of course, this is the referential reading. There is no direct way of representing the non-referential reading, since the scope of a quantifier in conventional logics is always a sentence, never a term. The only possible escape from the difficulty lies in translating intensional verbs as complex(non-atomic) logical expressions involving opaque sentential operators.9 The extant literature on this subject supports the view that a satisfactory decomposition cannot be supplied in all cases (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal of Computati</context>
</contexts>
<marker>Partee, 1974</marker>
<rawString>Partee, B.H. 1974 Opacity and scope. In Munitz, M.K., and Unger, P.K., Ed., Semantics and Philosophy. New York Univ. Press, New York: 81-101.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B H Partee</author>
<author>Ed</author>
</authors>
<title>1976a Montague Grammar.</title>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Partee, Ed, </marker>
<rawString>Partee, B.H., Ed. 1976a Montague Grammar. Academic Press, New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B H Partee</author>
</authors>
<title>1976b Some transformational extensions of Montague grammar.</title>
<booktitle>In Partee 1976a,</booktitle>
<pages>51--76</pages>
<marker>Partee, </marker>
<rawString>Partee, B.H. 1976b Some transformational extensions of Montague grammar. In Partee 1976a, 51-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>Context-sensitive immediate constituent analysis: context-free languages revisited.</title>
<date>1969</date>
<booktitle>In First Ann. Symp. on Theory of Computing,</booktitle>
<volume>6</volume>
<pages>1--8</pages>
<publisher>ACM,</publisher>
<location>New York,</location>
<contexts>
<context position="10479" citStr="Peters &amp; Ritchie 1969" startWordPosition="1583" endWordPosition="1586">s, Volume 8, Number 1, January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague&apos;s grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in</context>
</contexts>
<marker>Peters, Ritchie, 1969</marker>
<rawString>Peters, P.S., and Ritchie, R.W. 1969 Context-sensitive immediate constituent analysis: context-free languages revisited. In First Ann. Symp. on Theory of Computing, ACM, New York, 1-8. Also in Math. Systems Theory 6 (1973) 324-333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W v 0 Quine</author>
</authors>
<title>Word and Object.</title>
<date>1960</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="33168" citStr="Quine 1960" startWordPosition="5138" endWordPosition="5139"> our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 31 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic lowing paraphrases of the three sample sentences. (Paraphrase (1)&apos; is well-known, except perhaps for the particular form of adverbial (Quine 1960, Bennett 1974, Partee 1974), while (2)&apos;-(3)&amp;quot; are original). These could be formalized within a conventional logical framework allowing for non-truth-functional sentential operators: (1)&apos; John tries to find a unicorn (by looking around), (2)&apos; John forms a mental description which could apply to a unicorn, (3)&apos; John acts, thinks and feels as if he worshipped a unicorn. (3)&amp;quot; John worships an entity which he believes to be a unicorn. In each case the operator that is the key to the translation is italicized. Note that the original ambiguity of (1) and (2) has been preserved, but can now be constr</context>
</contexts>
<marker>Quine, 1960</marker>
<rawString>Quine, W.v.0. 1960 Word and Object. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
</authors>
<title>Panel: perspectives on parsing issues.</title>
<date>1981</date>
<booktitle>Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics,</booktitle>
<pages>95--106</pages>
<location>Stanford Univ., Stanford, CA,</location>
<contexts>
<context position="2392" citStr="Robinson 1981" startWordPosition="349" endWordPosition="350">parsing and comprehension. In our view, the achievement of the former objective calls for a careful structural separation of the subsystems that specify possible constituent structure (syntax), possible mappings from constituent structure to underlying logical form (part of semantics), and possible mappings from logical form to deeper, unambiguous representations as a function of discourse context and world knowledge (part of pragmatics and 1 Submitted August 1981; revised July 1982. inference). This sort of view is now widely held, as evidenced by a recent panel discussion on parsing issues (Robinson 1981). In the words of one of the panelists, &amp;quot;I take it to be uncontroversial that, other things being equal, a homogenized system is less preferable on both practical and scientific grounds than one that naturally decomposes. Practically, such a system is easier to build and maintain, since the parts can be designed, developed, and understood to a certain extent in isolation... Scientifically, a decomposable system is much more likely to provide insight into the process of natural language comprehension, whether by machines or people.&amp;quot; (Kaplan 1981) The panelists also emphasized that structural de</context>
</contexts>
<marker>Robinson, 1981</marker>
<rawString>Robinson, J.J. (chr.) 1981 Panel: perspectives on parsing issues. Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics, June 29-July 1, Stanford Univ., Stanford, CA, 95-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Rosenschein</author>
<author>S M Shieber</author>
</authors>
<title>Translating English into logical form.</title>
<date>1982</date>
<booktitle>Presented at the 20th Ann. Meet, of the Assoc. for Computational Linguistics,</booktitle>
<institution>Univ. of Toronto,</institution>
<location>Toronto, Ont.</location>
<contexts>
<context position="69663" citStr="Rosenschein &amp; Shieber (1982)" startWordPosition="11183" endWordPosition="11186">y little feature checking to stay on the right syntactic track. Neither parser at present handles slash categories and coordination (although they could be handled inefficiently by resort to closure of the grammar under metarules and rule schemata). Extraction of quantifiers from the logical-form translations is at present based on the level of syntactic embedding and left-to-right order alone, and no other form of postprocessing is attempted.15 15 Since submission of this paper for publication, we have become aware of several additional papers on parser-translators similar to ours. One is by Rosenschein &amp; Shieber (1982), another by Gawron et al. (1982); in conception these are based quite directly on the generalized phrase structure grammar of Gazdar and his collaborators, and use recursive descent parsers. A related Prolog-based approach is described by McCord (1981, 1982). It has been gratifyingly easy to write these parsertranslators, confirming us in the conviction that Gazdar-style grammars hold great promise for the design of natural language understanding systems. It is particularly noteworthy that we found the design of the translator component an almost trivial task; no modification of this componen</context>
</contexts>
<marker>Rosenschein, Shieber, 1982</marker>
<rawString>Rosenschein, S.J., and Shieber, S.M. 1982 Translating English into logical form. Presented at the 20th Ann. Meet, of the Assoc. for Computational Linguistics, June 16-18, Univ. of Toronto, Toronto, Ont.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sag</author>
</authors>
<title>A semantic theory of NP-movement dependencies.</title>
<date>1980</date>
<tech>D. Reidel, Dortrecht.</tech>
<note>To appear in</note>
<contexts>
<context position="10776" citStr="Sag 1980" startWordPosition="1628" endWordPosition="1629"> fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague&apos;s approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause constructions and comparatives. Yet the grammar itself makes no use of transformations; it consists entirely of phrase structure rules, with a nodeadmissibility rather than generative interpretation. For example, the rule [(S) (NP) (VP)] states that a fragment wit</context>
</contexts>
<marker>Sag, 1980</marker>
<rawString>Sag, I. 1980 A semantic theory of NP-movement dependencies. To appear in Jacobson, P. and Pullum, G.K., Ed., The Nature of Syntactic Representation. D. Reidel, Dortrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Identification of conceptualizations underlying natural language. In</title>
<date>1973</date>
<journal>Computer Models of Thought</journal>
<pages>187--247</pages>
<location>San Francisco:</location>
<contexts>
<context position="25089" citStr="Schank 1973" startWordPosition="3863" endWordPosition="3864">nsional logic translations. As indicated in our introductory remarks, a natural language understanding system must be able to make inferences that relate the natural language input to the system&apos;s stored knowledge and discourse model. A great deal of work in AI has focused on inference during language understanding and on the organization of the base of stored knowledge on which the comprehension process draws. Almost all of this work has employed more or less conventional logics for expressing the stored knowledge. (Even such idiosyncratic formalisms as Schank&apos;s conceptual dependency theory (Schank 1973) are much more akin to, say, first order modal logic than to any form of intensional logic â€” see Schubert 1976). How are intensional logic formulas to be connected up with stored knowledge of this conventional type? One possible answer is that the stored knowledge should not be of the conventional type at all, but should itself be expressed in intensional logic. However, the history of automatic deduction suggests that higher-order logics are significantly harder to mechanize than lower-order logics. Developing efficient inference rules and strategies for intensional logics, with their arbitra</context>
</contexts>
<marker>Schank, 1973</marker>
<rawString>Schank, R.C. 1973 Identification of conceptualizations underlying natural language. In Schank, R.C. and Colby, KM., Ed., Computer Models of Thought and Language. W.H. Freeman, San Francisco: 187-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>The primitive ACTs of conceptual dependency.</title>
<date>1975</date>
<booktitle>In Advance Papers of Theoretical Issues in Natural Language Processing Workshop,</booktitle>
<pages>34--37</pages>
<location>MIT, Cambridge, MA,</location>
<contexts>
<context position="32861" citStr="Schank 1975" startWordPosition="5094" endWordPosition="5095"> (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 31 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic lowing paraphrases of the three sample sentences. (Paraphrase (1)&apos; is well-known, except perhaps for the particular form of adverbial (Quine 1960, Bennett 1974, Partee 1974), while (2)&apos;-(3)&amp;quot; are original). These could be formalized within a conventional logical framework allowing for non-truth-functional sentential operators: (1)&apos; John tries to find a unicorn (by looking around), (2)&apos; John forms a mental description which could apply </context>
</contexts>
<marker>Schank, 1975</marker>
<rawString>Schank, R.C. 1975 The primitive ACTs of conceptual dependency. In Advance Papers of Theoretical Issues in Natural Language Processing Workshop, June 10-13, MIT, Cambridge, MA, 34-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schoenfinkel</author>
</authors>
<title>Ueber die Bausteine der mathematischen Logik.</title>
<date>1924</date>
<journal>Math. Annalen</journal>
<volume>92</volume>
<pages>305--316</pages>
<contexts>
<context position="18570" citStr="Schoenfinkel 1924" startWordPosition="2806" endWordPosition="2807"> a conventional translation will have the structure [John loves Mary], in which &amp;quot;John&amp;quot; and &amp;quot;Mary&amp;quot; combine with the verb at the same level of constituent structure. In itself, this difference is not important. It only becomes important when syntactic composition is assumed to correspond to function application in the semantic domain. This is done in Montague grammar 5 We consistently use infix form (with the predicate following its first argument) and square brackets for complete sentential formulas. by resort to the Schoenfinkel-Church treatment of many-place functions as one-place functions (Schoenfinkel 1924, Church 1941). For example, the predicate &amp;quot;loves&amp;quot; in the above sentence is interpreted as a one-place function that yields a one-place function when applied to its argument (in this instance, when applied to the semantic value of &amp;quot;Mary&amp;quot;, it yields the function that is the semantic value of &amp;quot;loves Mary&amp;quot;). The resultant function in turn yields a sentence value when applied to its argument (in this instance, when applied to the semantic value of &amp;quot;John&amp;quot;, it yields the proposition expressed by &amp;quot;John loves Mary&amp;quot;). Thus, a dyadic predicator like &amp;quot;loves&amp;quot; is no longer interpreted as a set of pairs of </context>
</contexts>
<marker>Schoenfinkel, 1924</marker>
<rawString>Schoenfinkel, M. 1924 Ueber die Bausteine der mathematischen Logik. Math. Annalen 92, 305-316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
</authors>
<title>Extending the expressive power of semantic networks.</title>
<date>1976</date>
<journal>Artificial Intelligence</journal>
<volume>7</volume>
<pages>163--198</pages>
<contexts>
<context position="25200" citStr="Schubert 1976" startWordPosition="3884" endWordPosition="3885"> must be able to make inferences that relate the natural language input to the system&apos;s stored knowledge and discourse model. A great deal of work in AI has focused on inference during language understanding and on the organization of the base of stored knowledge on which the comprehension process draws. Almost all of this work has employed more or less conventional logics for expressing the stored knowledge. (Even such idiosyncratic formalisms as Schank&apos;s conceptual dependency theory (Schank 1973) are much more akin to, say, first order modal logic than to any form of intensional logic â€” see Schubert 1976). How are intensional logic formulas to be connected up with stored knowledge of this conventional type? One possible answer is that the stored knowledge should not be of the conventional type at all, but should itself be expressed in intensional logic. However, the history of automatic deduction suggests that higher-order logics are significantly harder to mechanize than lower-order logics. Developing efficient inference rules and strategies for intensional logics, with their arbitrarily complex types and their intension, extension and lambda abstraction operators in addition to the usual mod</context>
</contexts>
<marker>Schubert, 1976</marker>
<rawString>Schubert, L.K. 1976 Extending the expressive power of semantic networks. Artificial Intelligence 7, 163-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
</authors>
<title>An approach to the syntax and semantics of affixes in &apos;conventionalized&apos; phrase structure grammar.</title>
<date>1982</date>
<booktitle>Proc. of the 4th Biennial Conf. of the Can. Soc. for Computational Studies of Intelligence (CSCSI/SCEIO),</booktitle>
<pages>17--19</pages>
<institution>Univ. of Saskatchewan,</institution>
<location>Saskatoon, Sask.:</location>
<contexts>
<context position="67866" citStr="Schubert, 1982" startWordPosition="10904" endWordPosition="10905">ence of additional context). The reader can verify that the first and second readings, but not the third, could have been obtained by extracting the coordinator first and the quantifier second. Finally, we should remark that the distributive rules are not appropriate for the group reading of coordinate structures in sentences such as John and Mary carried the sofa (together). We envisage a mereological interpretation in which John and Mary together comprise a two-component entity. However, we refrain from introducing a logical syntax for such entities here (but see the treatment of plurals in Schubert, 1982). 5. Parsing Phrase structure grammars are relatively easy to parse. The most advanced parser for Gazdar-style grammars that we are aware of is Thompson&apos;s chartparser (Thompson 1981), which provides for slash categories and coordination, but does not (as of this writing) generate logical translations. We have implemented two small parser-translators for preliminary experimentation, one written in SNOBOL and the other in MACLISP. The former uses a recursive descent algorithm and generates intensional logic translations. The latter is a &apos;left corner&apos; parser that uses our reformulated semantic ru</context>
<context position="70725" citStr="Schubert (1982)" startWordPosition="11347" endWordPosition="11348">ding systems. It is particularly noteworthy that we found the design of the translator component an almost trivial task; no modification of this component will be required even when the parser is expanded to handle slash categories and coordination directly. Encouraged by these results, we have begun to build a full-scale left-corner parser. A morphological analyzer that can work with arbitrary sets of formal affix rules is partially implemented; this work, as well as some ideas on the conventional translation of negative adjective prefixes, plurals, and tense/aspect structure, is reported in Schubert (1982). 6. Concluding Remarks From the point of view of theoretical and computational linguistics, Gazdar&apos;s approach to grammar offers profound advantages over traditional approaches: it dispenses with transformations without loss of insight, offers large linguistic coverage, and couples simple, semantically well-motivated rules of translation to the syntactic rules. We have attempted to show that the advantages of Gazdar&apos;s approach to grammar can be secured without commitment to an intensional target logic for the translations of natural language sentences. To motivate this endeavour, we have argue</context>
</contexts>
<marker>Schubert, 1982</marker>
<rawString>Schubert, L.K. 1982 An approach to the syntax and semantics of affixes in &apos;conventionalized&apos; phrase structure grammar. Proc. of the 4th Biennial Conf. of the Can. Soc. for Computational Studies of Intelligence (CSCSI/SCEIO), 17-19 May 1982, Univ. of Saskatchewan, Saskatoon, Sask.: 189-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schwind</author>
</authors>
<title>A formalism for the description of question answering systems.</title>
<date>1978</date>
<journal>In Bolc, L., Ed., Natural Language Communication with Computers.</journal>
<pages>1--48</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg &amp; New York:</location>
<contexts>
<context position="8530" citStr="Schwind (1978)" startWordPosition="1297" endWordPosition="1298">ore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descriptions, are taken to denote property sets. Crucially, the formal semantics of the logical translations produced by the semantic rules of Montague grammar accords by and large with intuitions about entailment, synonymy, ambiguity and other semantic phenomena. 2 Interestingly enough, this linguistic hypothesis was anticipated by Knuth&apos;s work on the semantics of attribute grammars (Knuth 1968). Schwind (1978) has applied Knuth&apos;s insights to the development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded. There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-</context>
</contexts>
<marker>Schwind, 1978</marker>
<rawString>Schwind, C. 1978a A formalism for the description of question answering systems. In Bolc, L., Ed., Natural Language Communication with Computers. Springer-Verlag, Berlin, Heidelberg &amp; New York: 1-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schwind</author>
</authors>
<title>The translation of natural language texts into state logic formulae.</title>
<date>1978</date>
<booktitle>Univ. Muenchen, available from Bibliothek des Fachbereichs Mathematik, Technische Univ. Muenchen, D-8000 Muenchen 2, W.</booktitle>
<tech>Tech. Rep. TUM-INFO-7806, Technische</tech>
<contexts>
<context position="8530" citStr="Schwind (1978)" startWordPosition="1297" endWordPosition="1298">ore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descriptions, are taken to denote property sets. Crucially, the formal semantics of the logical translations produced by the semantic rules of Montague grammar accords by and large with intuitions about entailment, synonymy, ambiguity and other semantic phenomena. 2 Interestingly enough, this linguistic hypothesis was anticipated by Knuth&apos;s work on the semantics of attribute grammars (Knuth 1968). Schwind (1978) has applied Knuth&apos;s insights to the development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded. There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-</context>
</contexts>
<marker>Schwind, 1978</marker>
<rawString>Schwind, C. 1978b The translation of natural language texts into state logic formulae. Tech. Rep. TUM-INFO-7806, Technische Univ. Muenchen, available from Bibliothek des Fachbereichs Mathematik, Technische Univ. Muenchen, D-8000 Muenchen 2, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E A Siegel</author>
</authors>
<title>Measure adjectives in Montague grammar. In</title>
<date>1979</date>
<pages>223--262</pages>
<institution>Univ. of Texas Press,</institution>
<location>Austin, TX:</location>
<contexts>
<context position="51571" citStr="Siegel (1979)" startWordPosition="8135" endWordPosition="8136">, (ADJP&apos; N&apos;)&gt;, N(2) = [boy, game, noise, ...) (a) little boy (b) with ADJP&apos; = little2, N&apos; = boy3, AN&apos; becomes (little2 boy3); (c) &amp;quot;little&amp;quot; is taken as a predicate modifier.12 &lt;3, [(NP) (Q) (AN)], &lt;Q&apos; AN&apos;&gt;&gt;, Q(3) = (a, the, all, many, ...} (a) the little boy (b) with Q&apos; = the1, AN&apos; = (little2 boy3), NP&apos; -&gt; &lt;the1 (little2 boy3)&gt;. &lt;4, [(PP to) (to) (NP)], NP&apos;&gt; (a) to Mary (b) with NP&apos; = Mary6, PP&apos; -&gt; Mary6; (c) PP verb complements have the same meaning as their NP, as per Gazdar (1981a). &lt;5, [(VP) (V)], V&apos;&gt;, V(5) = [run, smile, disappear, ...) (a) smiles (b) with V&apos; = smiles4, VP&apos; -&gt; smiles4. 12 Siegel (1979) argues rather persuasively that measure adjectives, unlike genuine predicate modifiers such as &amp;quot;consummate&amp;quot;, actually combine with terms. For such adjectives we might employ the semantic rule Xx[[x ADJP] &amp; [x N&apos;]]; in the case of &amp;quot;little&amp;quot;, we would use ADJP&apos; = (little-for P), where P is an indeterminate predicate to be replaced pragmatically by a comparison-class predicate. Thus the translation of &amp;quot;little boy&amp;quot; (neglecting indices) would be Xxqx little-for P] &amp; [x boy]]. American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 35 Lenhart K. Schubert and Francis Jef</context>
</contexts>
<marker>Siegel, 1979</marker>
<rawString>Siegel, M.E.A. 1979 Measure adjectives in Montague grammar. In Davis, S., and Mithun, M., Eds., Linguistics, Philosophy, and Montague Grammar. Univ. of Texas Press, Austin, TX: 223-262.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R H ed Thomason</author>
</authors>
<title>1974a Formal Philosophy: Selected Papers of Richard Montague.</title>
<booktitle>R.H. 1974b Introduction to Thomason 1974a,</booktitle>
<pages>1--69</pages>
<editor>Haven, CT. Thomason,</editor>
<publisher>Yale Univ. Press,</publisher>
<location>New</location>
<marker>Thomason, </marker>
<rawString>Thomason, R.H. ed. 1974a Formal Philosophy: Selected Papers of Richard Montague. Yale Univ. Press, New Haven, CT. Thomason, R.H. 1974b Introduction to Thomason 1974a, 1-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Thomason</author>
</authors>
<title>A model theory for propositional attitudes.</title>
<date>1980</date>
<journal>Linguistics and Philosophy</journal>
<volume>4</volume>
<pages>47--70</pages>
<contexts>
<context position="15780" citStr="Thomason 1980" startWordPosition="2390" endWordPosition="2391">e modifiers and some second-order predicate constants into our logical vocabulary, and may ultimately want to employ a full-fledged second-order logic, in view of such sentences as &amp;quot;Every good general has at least some of Napoleon&apos;s qualities&amp;quot;. On the other hand, we may pare down rather than expand the logical apparatus, opting for a logic that treats properties, propositions and other intensional entities as first-order individuals. This type of treatment, which avoids the unwanted identity of logically equivalent propositions, appears to be gaining currency (e.g., Fodor 1978, McCarthy 1979, Thomason 1980, Chierchia 1981). Some minor adjustments would be required in our rules of logical translation. 28 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic But the formula derives its interpretation from its stipulated logical equivalence to (a111 x:[x man2])[x morta13], which may in turn become Vxax HUMAN] =&gt; [x MORTAL]], after disambiguation.5 2. Intensional and &apos;Conventional&apos; Translations We should emphasize at the outset that our objective is not to impugn Montague grammar, but merely to ma</context>
</contexts>
<marker>Thomason, 1980</marker>
<rawString>Thomason, R.H. 1980 A model theory for propositional attitudes. Linguistics and Philosophy 4, 47-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F B Thompson</author>
</authors>
<title>English for the computer.</title>
<date>1966</date>
<booktitle>Fall Joint Comp. Conf.,</booktitle>
<pages>7--10</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="9180" citStr="Thompson 1966" startWordPosition="1403" endWordPosition="1404"> development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded. There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-defined computer memory structures (Thompson 1966). However, DEACON&apos;s semantic rules performed direct semantic evaluation of sorts (via computations over a data base) rather than constructing logical translations. The systems of Winograd (1972) and Woods (1977) constructed input translations prior to evaluation, using semantic rules associated with particular syntactic structures. However, these rules neither corresponded oneto-one to syntactic rules nor limited interpretive operations to composition of logical expressions; for example, they incorporated tests for selectional restrictions and other forms of inference, with unrestricted use of</context>
</contexts>
<marker>Thompson, 1966</marker>
<rawString>Thompson, F.B. 1966 English for the computer. Fall Joint Comp. Conf., Nov. 7-10, San Francisco, CA. AFIPS Conf. Proc. Vol. 29, Spartan Books, Washington, D.C., 349-356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Thompson</author>
</authors>
<title>Chart parsing and rule schemata in PSG.</title>
<date>1981</date>
<booktitle>Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics,</booktitle>
<pages>167--172</pages>
<location>Stanford Univ., Stanford, CA:</location>
<contexts>
<context position="68048" citStr="Thompson 1981" startWordPosition="10932" endWordPosition="10933">er second. Finally, we should remark that the distributive rules are not appropriate for the group reading of coordinate structures in sentences such as John and Mary carried the sofa (together). We envisage a mereological interpretation in which John and Mary together comprise a two-component entity. However, we refrain from introducing a logical syntax for such entities here (but see the treatment of plurals in Schubert, 1982). 5. Parsing Phrase structure grammars are relatively easy to parse. The most advanced parser for Gazdar-style grammars that we are aware of is Thompson&apos;s chartparser (Thompson 1981), which provides for slash categories and coordination, but does not (as of this writing) generate logical translations. We have implemented two small parser-translators for preliminary experimentation, one written in SNOBOL and the other in MACLISP. The former uses a recursive descent algorithm and generates intensional logic translations. The latter is a &apos;left corner&apos; parser that uses our reformulated semantic rules to generate conventional translations. It begins by finding a sequence of leftmost phrase-structure-rule branches that lead from the first word upward to the sentence node. (e.g.</context>
</contexts>
<marker>Thompson, 1981</marker>
<rawString>Thompson, H. 1981 Chart parsing and rule schemata in PSG. Proc. 19th Ann. Meet, of the Assoc. for Computational Linguistics, June 29-July 1, Stanford Univ., Stanford, CA: 167-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>An artificial intelligence approach to machine translation. In</title>
<date>1974</date>
<journal>Computer Models of Thought</journal>
<pages>114--151</pages>
<location>San Francisco:</location>
<contexts>
<context position="32847" citStr="Wilks 1974" startWordPosition="5092" endWordPosition="5093">in all cases (Montague 1970c, Bennett 1974, Partee 1974, Dowty 1978, 1979, Dowty, Wall &amp; Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain &amp;quot;canonical&amp;quot; representations that facilitate inference is widely acknowledged by Al researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 31 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic lowing paraphrases of the three sample sentences. (Paraphrase (1)&apos; is well-known, except perhaps for the particular form of adverbial (Quine 1960, Bennett 1974, Partee 1974), while (2)&apos;-(3)&amp;quot; are original). These could be formalized within a conventional logical framework allowing for non-truth-functional sentential operators: (1)&apos; John tries to find a unicorn (by looking around), (2)&apos; John forms a mental description whic</context>
</contexts>
<marker>Wilks, 1974</marker>
<rawString>Wilks, Y. 1974 An artificial intelligence approach to machine translation. In Schank, R.C., and Colby, K.M. Ed., Computer Models of Thought and Language. W.H. Freeman, San Francisco: 114-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9374" citStr="Winograd (1972)" startWordPosition="1429" endWordPosition="1431"> rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-defined computer memory structures (Thompson 1966). However, DEACON&apos;s semantic rules performed direct semantic evaluation of sorts (via computations over a data base) rather than constructing logical translations. The systems of Winograd (1972) and Woods (1977) constructed input translations prior to evaluation, using semantic rules associated with particular syntactic structures. However, these rules neither corresponded oneto-one to syntactic rules nor limited interpretive operations to composition of logical expressions; for example, they incorporated tests for selectional restrictions and other forms of inference, with unrestricted use of the computational power of LISP. American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic </context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. 1972 Understanding Natural Language. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Lunar rocks in natural English: Explorations in natural language question answerIng.</title>
<date>1977</date>
<booktitle>In Zampolli, A., Ed., Linguistic Structures Processing.</booktitle>
<pages>521--569</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="9391" citStr="Woods (1977)" startWordPosition="1433" endWordPosition="1434">esis and the rule-based approach to the interpretation of syntactic structures that emerged within Al during the 1960&apos;s and early 70&apos;s. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson&apos;s proposal to formalize English by limiting its subject matter to well-defined computer memory structures (Thompson 1966). However, DEACON&apos;s semantic rules performed direct semantic evaluation of sorts (via computations over a data base) rather than constructing logical translations. The systems of Winograd (1972) and Woods (1977) constructed input translations prior to evaluation, using semantic rules associated with particular syntactic structures. However, these rules neither corresponded oneto-one to syntactic rules nor limited interpretive operations to composition of logical expressions; for example, they incorporated tests for selectional restrictions and other forms of inference, with unrestricted use of the computational power of LISP. American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitat</context>
</contexts>
<marker>Woods, 1977</marker>
<rawString>Woods, W.A. 1977 Lunar rocks in natural English: Explorations in natural language question answerIng. In Zampolli, A., Ed., Linguistic Structures Processing. North-Holland, Amsterdam: 521-569.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Lenhart</author>
</authors>
<title>Schubert is an associate professor of computer science at the</title>
<institution>University of Alberta, Edmonton.</institution>
<note>He received the Ph.D. degree in computer science from the</note>
<marker>Lenhart, </marker>
<rawString>Lenhart K. Schubert is an associate professor of computer science at the University of Alberta, Edmonton. He received the Ph.D. degree in computer science from the University of Toronto.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Francis Jeffry</author>
</authors>
<title>Pelletier is a professor of philosophy at the University of Alberta. He received the Ph.D. degree from the University of California at Los Angeles.</title>
<marker>Jeffry, </marker>
<rawString>Francis Jeffry Pelletier is a professor of philosophy at the University of Alberta. He received the Ph.D. degree from the University of California at Los Angeles.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>