<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.985115">
Coupling a Linguistic Formalism and a Script Language
</title>
<author confidence="0.949677">
Claude Roux
</author>
<affiliation confidence="0.893349">
Xerox Research Centre Europe/ 6,
</affiliation>
<address confidence="0.596247">
chemin de Maupertuis, 38240 Meylan,
France
</address>
<email confidence="0.9936">
Claude.roux@xrce.xerox.com
</email>
<sectionHeader confidence="0.993673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999603428571429">
This article presents a novel syntactic
parser architecture, in which a linguistic
formalism can be enriched with all sorts
of constraints, included extra-linguistic
ones, thanks to the seamless coupling of
the formalism with a programming lan-
guage.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995596875">
The utilization of constraints in natural language
parsers (see Blache and Balfourier,2001 or
Tapanainen and Järvinen , 1994) is central to
most systems today. However, these constraints
are often limited to purely linguistic features,
such as linearity or dependency relations be-
tween categories within a given syntactic tree.
Most linguistic formalisms have been created
with the sole purpose of extracting linguistic in-
formation from bits and pieces of text. They usu-
ally use a launch and forget strategy, where a text
is analyzed according to local constraints, dis-
played and then discarded to make room for the
next block of text. These parsers take each sen-
tence as an independent input, on which gram-
mar rules are applied together with constraints.
However, no sentence is independent of a text,
and no text is really independent of extra-
linguistic information. In order to assess cor-
rectly the phrase President Bush, we need to
know that Bush is a proper name, whose function
is “President”. Washington can be a town, a
state, the name of a famous president, but also
the name of an actor. Moreover, the analysis of a
sentence is never an independent process, if
President Bush is found in a text, the reference to
president, later in the document will be related to
this phrase.
These problems are certainly not new and a
dense literature has been written about how to
better deal with these issues. However, most
solutions rely on formalism enrichments with
solutions “engraved in stone”, that makes it diffi-
cult to adapt a grammar to new domains (see De-
clerck, 2002, or Roux, 2004), even though they
use XML representation or database to store
huge amounts of extra-linguistic information.
The interpretation of these data is intertwined
into the very fabric of the parser and requires
deep modifications to use new sources with a
complete different DTD.
Of course, there are many other ways to solve
these problems. For instance, in the case of lan-
guages such as Prolog or Lisp, the grammar for-
malism is often indistinguishable from the pro-
gramming language itself. For these parsers, the
querying of external information is easily solved
as grammar rules can be naturally augmented
with non-linguistic procedures that are written in
the same language. In other cases, when the
parser is independent from any specific pro-
gramming languages, the problem can prove dif-
ficult to solve. The formalism can of course be
augmented with new instructions to tackle the
querying of external information. However the
time required to enrich the parser language may
not be worth the effort, as the development of a
complete new instruction set is a heavy and
complex task that is loosely related to linguistic
parser programming.
We propose in this article a new way of building
natural language parsers, with the coupling of a
script language with an already rich linguistic
formalism.
</bodyText>
<sectionHeader confidence="0.991292" genericHeader="introduction">
2 Scripting
</sectionHeader>
<bodyText confidence="0.999972833333333">
The system we describe in this article mix a
natural language parser, namely Xerox Incre-
mental Parser (XIP hereafter, see Aït-Mohktar et
al., 2002, Roux, 1999) with a scripting language,
in our case Python. The interaction of a grammar
with scripting instructions is almost as old as
</bodyText>
<page confidence="0.989169">
33
</page>
<note confidence="0.5889205">
Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 33–40,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965742268041">
computational linguistics. Pereira and Shieber for
instance, in their book: Prolog and Natural Lan-
guage Processing (see Pereira and Shieber, 1987)
already suggested mixing grammar rules with
extra-linguistic information. This mélange was
made possible thanks to the homogeneity be-
tween grammar rules and the external code, writ-
ten in both cases in the same programming lan-
guage. However, these programming languages
are not exactly tuned to do linguistic analyses;
they are often slow and cumbersome. Moreover,
they blur the boundary between program and
grammar rules, as the programming language is
both the algorithm language and the rule lan-
guage. Allen (see Allen, 1994) proposes a differ-
ent approach in his TRAINS Parsing system. The
grammar formalism is independent to a certain
extent from the implementation language, which
is LISP in this case. However, since the grammar
is translated into a LISP program, it is easy for a
linguist to specialize the generated rules with
external LISP procedures. Nevertheless, the
grammar formalism remains very close to LISP
data description, which makes the grammar rules
somewhat difficult to read.
The other solution, which is usually favored by
computational linguists, is to store the external
information in databases, which are accessed
with some pre-defined instructions and translated
into linguistic features. For instance (see De-
clerk, 2002 or Roux, 2004), the external informa-
tion is presented as an XML document whose
DTD is defined once and for all. This DTD is
then enriched with extra-linguistic information
that a parser can exploit to guide rule application.
This method alleviates the necessity of a com-
plex interaction mechanism between the parser
and its external data sources. The XPath lan-
guage is used to query this document in order to
retrieve salient information at parsing time,
which is then translated into local linguistic fea-
tures. However, only static information can be
exploited, as these XML databases must be built
beforehand.
Similar mechanisms have also been proposed in
other architectures to help heterogeneous linguis-
tic modules to communicate through a common
XML interface (see Cunningham et al.,2002,
Blache and Guénot , 2003). These architectures
are very powerful as they connect together tools
that only need to comply with a common in-
put/output DTD. Specialized Java modules can
then be written which are applied to intermediate
representations to add their own touch of extra-
linguistic data. Since, the intermediate represen-
tation is an XML document, the number of pos-
sible enrichments is almost limitless, as each
module will only extract from this document the
XML markup tags that it is designed to handle.
However, since XML is by nature furiously ver-
bose, the overall system might be very slow as it
might spend a large amount of time translating
external XML representation into internal repre-
sentations.
Furthermore, applications that require natural
language processing also have different pur-
poses, different needs. They may require a shal-
low output, such as a simple tokenization with a
whiff of tagging, or a much deeper analysis. Syn-
tactic parsing is usually integrated as a black box
into these architectures, with little control left
over the grammar execution, control which nev-
ertheless might prove very important in many
cases. An XML document, for instance, often
contains some specific markup tags to identify a
title, a section or author name. If the parser is
given some indications about the input, it could
be guided through the grammar maze to favor
these rules that are better suited to analyze a title
for example.
Finally, syntactic parsing, when it is limited to
lexical information, often fails to assess correctly
some ambiguous relations. Thus, the only way to
deal with PP-attachment or anaphoric pronoun
antecedents is to use both previous analyses and
external information. However, most syntactic
parsers are often ill geared to link with external
modules. The formalism is engraved into a C
program as in Link Grammar (see Grinberg et
al.,1995) or as in Sylex (see Constant, 1995)
which offers little or no opening to the rest of the
world, as it is mainly designed to accomplish one
unique task. We will show how the seamless in-
tegration of a script language into the very fabric
of the formalism simplifies the task of keeping
track of previous analyses together with the use
of external sources of data.
</bodyText>
<sectionHeader confidence="0.986111" genericHeader="method">
3 Xerox Incremental Parser (XIP)
</sectionHeader>
<bodyText confidence="0.9999517">
The XIP engine has been developed by a re-
search team in computational linguistics at the
Xerox Research Centre Europe (see Aït-Mokhtar
et al., 2001). It has been designed from the be-
ginning to follow a strictly incremental strategy,
where rules apply one after the other. There is
only one analysis path that is followed for a
given linguistic unit (phrase, sentence or even
paragraph): the failure of a rule does not prevent
the whole analysis from continuing to comple-
</bodyText>
<page confidence="0.995027">
34
</page>
<bodyText confidence="0.90754675">
tion. Since the system never backtracks on any
rules, XIP cannot propel itself into a combinato-
rial explosion.
XIP can be divided into two main components:
</bodyText>
<listItem confidence="0.989751">
• A component that builds a chunk tree on
the basis of lexical nodes.
• A component that creates functions or
dependencies that connect together distant
nodes from the chunk tree.
</listItem>
<bodyText confidence="0.999965357142857">
The central goal of this parser is the extraction of
dependencies. A dependency is a function that
connects together distant nodes within a chunk
tree. The system constructs a dependency be-
tween two nodes, if these two nodes are in a spe-
cific configuration within the chunk tree or if a
specific set of dependencies has already been
extracted for some of these nodes (see Hagege
and Roux, 2002). The notion of constraint em-
bedded in XIP is both configurational and Boo-
lean. The configuration part is based on tree
regular rules which express constraints over node
configuration, while the Boolean constraints are
expressed over dependencies.
</bodyText>
<subsectionHeader confidence="0.998752">
3.1 Three Level of Analysis
</subsectionHeader>
<bodyText confidence="0.998847">
The parsing is done in three different stages:
</bodyText>
<listItem confidence="0.955351777777778">
• Part-of-speech disambiguation and
chunking.
• Dependency Extraction between words
on the basis of sub-tree patterns over the
chunk sequence.
• Combination of those dependencies with
Boolean operators to generate new de-
pendencies, or to modify or delete existing
dependencies.
</listItem>
<subsectionHeader confidence="0.999494">
3.2 The Different Steps of Analysis
</subsectionHeader>
<bodyText confidence="0.978994272727273">
Below is an example of how a sentence is parsed.
We present a little grammar, written in the XIP
formalism, together with the output yielded by
these rules.
Example
The chunking rules produce a chunk tree.
In a first stage, chunking rules are applied and
the following chunk tree is built for this sen-
tence.
Below is a small XIP grammar that can analyze
the above example:
</bodyText>
<listItem confidence="0.7938315">
1&gt; AP = Adj.
2&gt; NP @= Det,(AP),(Noun),Noun.
3&gt; FV= verb.
4&gt; SC= NP,FV.
</listItem>
<bodyText confidence="0.995649769230769">
Each rule is associated with a layer number,
which defines the order in which the rules must
be executed.
If this grammar is applied to the above sentence,
the result is the following:
TOP{SC{NP{The AP{chunking} rules}
FV{produce}}
NP{a chunk tree}
.}
TOP is a node that is automatically created, once
all chunking rules have applied, to transform this
sequence of chunks into a tree.
(The “@” denotes a longest match strategy. The
rule is then applied to the longest sequence of
categories found in the linguistic unit)
The next step consists of extracting some basic
dependencies from this tree. These dependencies
are obtained with some very basic rules that only
connect nodes that occur in a specific sub-tree
configuration.
SUBJ(produce,rules)
OBJ(produce,tree)
SUBJ is a subject relation, which has been ex-
tracted with the following rule:
 |NP{?*, noun#1}, FV{?*,verb#2}|
SUBJ(#2,#1).
This rule links together the noun and the verb
respectively the sub-nodes of a NP and a VP that
are next to each other. The “{...}” denotes a pat-
tern over sub-nodes.
Other rules may then be applied to this output,
to add or modify existing dependencies.
if (SUBJ(#1,#2) &amp; OBJ(#1,#3))
TRIPLET(#2,#1,#3).
For instance, the above rule will generate a three
slot dependency TRIPLET with the nodes ex-
tracted from the subject and object dependencies.
If we apply this rule to our previous example, we
will create: TRIPLET(rules,produce,tree).
</bodyText>
<page confidence="0.995303">
35
</page>
<subsectionHeader confidence="0.998985">
3.3 Script Language
</subsectionHeader>
<bodyText confidence="0.998885285714286">
The utilization of a script language deeply in-
grained into the parser fabric might sound like a
pure technical gadget with very little influence
on parsing theories. However, the development
of a parser often poses some very trivial prob-
lems, which we can sum up in the three questions
below:
</bodyText>
<listItem confidence="0.9906595">
• How can we use previous analyses?
• How do we access external information?
• How do we control the grammar from an
embedding application?
</listItem>
<bodyText confidence="0.999960473684211">
Usually, the answer for each of these questions
leads to three different implementations, as none
of these problems seem to have any connections
whatsoever. Their only common point seems to
be some extra-programming into the parser en-
gine. If a grammar and a parser are both written
in the same programming language, the problem
is relatively simple to solve. However, if the
grammar is written in a formalism specifically
designed for linguistic analysis interpreted with a
linguistic compiler (as it is the case for XIP),
then any new features that would implement
some of these instructions translate into a modi-
fication of the parsing engine itself. However,
one cannot expand the parser engine forever. The
solution that has been chosen in XIP is to de-
velop a script language, which linguists can use
to enrich the original grammatical formalism
with new instructions.
</bodyText>
<subsectionHeader confidence="0.98031">
3.4 First attempts
</subsectionHeader>
<bodyText confidence="0.99993255">
The first attempts to add scripting instructions to
XIP consisted in enriching the grammar with
numerical and string variables together with
some instructions to handle these values. For
instance, it is possible in XIP to declare a string
variable, to instantiate it with the lemma value of
a syntactic node and to apply some string modi-
fications upon it. However, the development of
such a script language, however useful it proved,
became closer and closer to a general-purpose
programming language, which XIP was not de-
signed to be. The task of developing a full-
fledged programming language with a large in-
struction set is a complex ongoing process,
which has little connection with parsing theories.
Nevertheless, there was a need for such an ad-
dendum, which led the development team to link
XIP with Python, whose own ongoing develop-
ment is backed up by thousands of dedicated
computer scientists.
</bodyText>
<subsectionHeader confidence="0.775001">
3.5 Python
</subsectionHeader>
<bodyText confidence="0.999969393939394">
Scripting languages have been around for a very
long time. Thus Perl and Awk have been part of
the Unix OS for at least twenty years. Python is
already an old language, in computational time
scale. It has been central to the Linux environ-
ment for more than ten years. Most of the basic
installation procedures are written in that lan-
guage. It has also been ported to a variety of plat-
forms such as Windows or Mac OS. The lan-
guage syntax is close to C, but lacks type verifi-
cation. However, the language is thoroughly
documented and a large quantity of specialized
libraries is available. Python has also been cho-
sen because of the simplicity of its API, which
allows programmers to link easily a Python en-
gine to their own application or to enlarge the
language with new libraries. The other reason of
this choice, over for instance a more conven-
tional language such as C or Java is the fact that
it is an interpreted language. A XIP grammar is a
set of text files, which are all compiled on the fly
in memory every time the parser is run. It stems
from this choice that any addenda to this gram-
mar should be written in a language that is also
compiled on the fly. In this way, the new instruc-
tions can be developed in parallel with the
grammar and immediately put in test. It also
simplifies the non-trivial task of debugging a
complete grammar as any modifications on any
parts of the grammar can be immediately ex-
perimented together with the python script.
We have produced two different versions of the
XIP-python parsing engine.
</bodyText>
<subsectionHeader confidence="0.991116">
3.6 Python Embedded within XIP
</subsectionHeader>
<bodyText confidence="0.999981222222222">
We have linked the python engine to XIP, which
allows us to call and execute python scripts from
within the parsing engine. In this case, a gram-
mar rule can call a python script to verify spe-
cific conditions. The python scripts are then ap-
pended to the grammar itself. These scripts have
full access to all linguistic objects constructed so
far. XIP is the master program with python
scripts being triggered by grammar rules.
</bodyText>
<subsectionHeader confidence="0.95254">
3.7 XIP as a Python Library
</subsectionHeader>
<bodyText confidence="0.9999714">
We have created a specific XIP library which can
be freely imported in python. In this case, the
XIP library exports a basic API, compliant with
the python programming interface, which allows
python developers to benefit from the XIP en-
</bodyText>
<page confidence="0.996081">
36
</page>
<bodyText confidence="0.999973">
gine. The XIP results are then returned as python
objects. Since the purpose in this article is to
show how a grammar formalism can be enriched
with new instructions, we will mainly concen-
trate on the first point.
</bodyText>
<subsectionHeader confidence="0.998383">
3.8 Interfacing Python and a XIP grammar
</subsectionHeader>
<bodyText confidence="0.947449">
A XIP grammar mainly handles syntactic nodes,
features, categories, and dependencies. In order
to be efficient, a Python script, called from a XIP
grammar, should have access to all this informa-
tion in a simple and natural way. The notion of
procedure has already been added to the XIP
formalism. They can be used in any sort of rule.
Example
if (subject(#1,#2) &amp; TestNode(#1))
ambiguous(#1).
The above rule tests the existence of a subject
dependency and will use the TestNode procedure
to check some properties of the #1 node. If all
these conditions are true, then a new depend-
ency: ambiguous is created with #1 as parameter.
</bodyText>
<subsectionHeader confidence="0.826877">
3.9 Interface
</subsectionHeader>
<bodyText confidence="0.995794935483871">
The TestNode procedure is declared in a XIP
grammar in the following way:
Example
Python: //XIP field name
TestNode(#1). //the XIP procedure name, with
XIP parameter style.
//All that follows is in Python
def TestNode(node):
The only constraint is that the XIP procedure
name (TestNode) should also have been imple-
mented as a Python procedure. If this Python
procedure is missing, then the grammar compila-
tion fails.
The system works as a very simple linker, where
the code integrity is verified to the presence of
common names in XIP and Python.
However, the next step, which consists in trans-
lating XIP data into Python data, is done at run-
time.
XIP recognizes many different sorts of data,
which can all be transmitted to a Python script,
such as syntactic nodes, dependencies, integer
variables, string variables, or even vector vari-
ables. Each of these data is then translated into
simple Python variables. However, the syntactic
nodes and the dependencies are not directly
transformed into Python objects; we simply
propagate them into the Python code as integers.
Each node and each dependency has a unique
index, which simplifies the task of sharing pa-
rameters between XIP and Python.
</bodyText>
<sectionHeader confidence="0.758364" genericHeader="method">
3.10 XIP API
</sectionHeader>
<bodyText confidence="0.986027707317073">
Python procedures have access to all internal
parsing data through a specific API. This API
consists of a dozen instructions, which can be
called anywhere in the Python code. For in-
stance, XIP provides Python instructions to re-
turn a node or a dependency object on the basis
of its index. We have implemented the Python
XipNode class, with the following fields:
class XipNode
index #the unique index of the node
POS #the part of speech
Lemma #a vector of possible lemmas
for the node
Surface #the surface form as it ap
pears in the sentence
features #a vector of attribute-value
features
leftoffset,rightoffset #the text offsets
next,previous,parent,child # indexes
A XipNode object is automatically created when
the object creator is called with the node index as
parameter. We can also travel through the syn-
tactic tree, thanks to the next, previous, parent,
child indexes that are provided by this class.
There is a big difference between using this API
and exploiting the regular output of a syntactic
parser. Since the Python procedures are called at
runtime from the grammar, they have full access
to the on-going linguistic data. Second, the selec-
tion of syntactic nodes on which to apply Python
procedures is done at the grammar level, which
means that the access of specific nodes is done
through the parsing engine itself, without any
need to duplicate any sorts of tree operators,
which would be mandatory in the case of a Java,
XML or C++ object. Finally, the memory foot-
print is only limited to the nodes that are re-
quested by the application, there is no need to
reduplicate the whole linguistic data structure.
The memory footprint reduction also has the ef-
fect of speeding up the execution.
</bodyText>
<page confidence="0.998802">
37
</page>
<subsectionHeader confidence="0.447001">
3.11 Other Basic Instructions
</subsectionHeader>
<bodyText confidence="0.966189">
XIP provides the following Python instructions:
</bodyText>
<listItem confidence="0.988126461538461">
• XipDependency(index) builds a Xip-
Dependency object.
• nodeset(POS) returns a vector of node
indices corresponding to a POS: node-
set(“noun&apos;)
• dependencyset(POS) returns a vector of
dependency indices corresponding to a
dependency name: dependen-
cyset(“SUBJECT&apos;)
• dependencyonfirstnode(n) returns a
vector of dependency indices, whose first
parameter is the node index n: depend-
encyonfirstnode(12)
</listItem>
<bodyText confidence="0.990403">
These basic instructions make it possible for a
Python script to access all internal XIP data at
any stages.
</bodyText>
<subsectionHeader confidence="0.981364">
3.12 An Example
</subsectionHeader>
<bodyText confidence="0.966792285714286">
Let us define the Python code of TestNbSenses,
which checks whether a verbal node is highly
ambiguous according to WordNet. As a demon-
stration, a verb will be said to be highly ambigu-
ous if the number of its senses is larger than 10.
def TestNbSenses(i):
n=XipNode(i)
senses=N[n.lemma].getSenses()
if len(senses)&gt;=10:
return 1
return 0
We can now use this procedure in a syntactic
rule to test the ambiguity of a verb in order to
guide the grammar:
if (subject(#1,#2) &amp; TestNbSenses(#1))
ambiguous(#1).
The dependency ambiguous will be created for a
verbal node, if this verb is highly ambiguous.
4 Back to the Initial Questions
The questions we wish to answer are the follow-
ing:
</bodyText>
<listItem confidence="0.9952175">
• How can we use previous analyses?
• How do we access external information?
• How do we control the grammar from an
embedding application?
</listItem>
<bodyText confidence="0.9998602">
We have shown in the previous section how new
instructions could be easily defined and thus be-
come part of the XIP formalism. These instruc-
tions are mapped to a Python program which
offers all we need to answer the above questions.
</bodyText>
<subsectionHeader confidence="0.999008">
4.1 How can we use previous analyses?
</subsectionHeader>
<bodyText confidence="0.983307952380952">
Since, we have a full access to the internal lin-
guistic representation of XIP, we can store what-
ever data we might find useful for a given task.
For instance, we could decide to count the num-
ber of time a word has been detected in the
course of parsing. This could be implemented
with a Python dictionary variable.
Python:
countword(#1).
getcount(#1).
...
The first procedure countword receives a node
index as input. It translates it into a XipNode,
and it uses the lemma as an entry for the Python
dictionary wordcounter. At the end of the proc-
ess, wordcounter contains a list of words with
their number of occurrences. The second proce-
dure implements a simple test which returns the
number of time a word has been found. It returns
0, if it is an unknown word.
The grammar rule below is used to count words:
</bodyText>
<equation confidence="0.647679">
|Noun#1 |{
countword(#1);
}
</equation>
<bodyText confidence="0.927789">
The instruction |noun#1 |automatically loops
between all noun nodes.
The rule below is used to test if a word has al-
ready been found:
if (subject(#1,#2) &amp; getcount(#2)) ...
</bodyText>
<subsectionHeader confidence="0.995523">
4.2 How do we access external information?
</subsectionHeader>
<bodyText confidence="0.998823888888889">
We have already given an example with Word-
Net. Thanks to the large number of libraries
available, a Python script can benefit from
WordNet information. It can also connect to a
variety of databases such as MySQL, which also
allows a grammar to query a database for spe-
cific data.
For instance, we could store in a database verb-
noun couples that have been extracted from a
</bodyText>
<page confidence="0.997972">
38
</page>
<bodyText confidence="0.966280125">
large corpus. Then, at runtime, a grammar could
check whether a certain verb and a certain noun
have already been found together in another
document.
Example
Python:
TestCouple(#1,#2).
def TestCouple(v,n):
noun=XipNode(n)
verb=XipNode(v)
cmd=”select * from couples where ”
cmd+=”verb=”+verb.lemma+&amp;quot;
cmd+=” and noun=”+noun.lemma+”;”
nb=mysql.execute(cmd)
return nb
In the XIP grammar:
|FV{verb#1},PP{prep,NP{noun#2}}|
if (TestCouple(#1,#2))
Complement(#1,#2).
If we have a verb followed by a PP, then if we
have already found in a previous analysis a link
between the verb and the noun embedded in the
PP, we create a dependency Complement over
the verb and the noun.
</bodyText>
<subsectionHeader confidence="0.552206">
4.3 How do we control the grammar from
an embedding application?
</subsectionHeader>
<bodyText confidence="0.9994929">
Since a Python script can exploit any sort of in-
put, from text files to databases; it becomes rela-
tively simple to implement a simple Python pro-
cedure that blocks the execution of certain
grammar rules. If we examine the above exam-
ple, we can see how the grammar execution can
be modified by an external calling program. For
instance, the selection of a different database will
have a strong influence on how dependencies are
constructed.
</bodyText>
<sectionHeader confidence="0.99629" genericHeader="method">
5 Expression Power
</sectionHeader>
<bodyText confidence="0.991850647058824">
The main goal of this article is to describe a way
to articulate no-linguistic constraints with a dedi-
cated linguistic formalism. The notion of con-
straint in this perspective does not only apply to
purely linguistic properties such as category or-
der or dependency building constraints; it is
enlarged to encompass properties that are rarely
taken into account in syntactic theories. It should
be noted, however, that if most theories are de-
signed to apply to a single sentence, nothing pre-
vents these formalisms to benefit from extra-
linguistic data through a complex feature system
that would encode the sentence context. How
these features are instantiated is nevertheless out
the realm of these theories. The originality of our
system lies in the fact that we intertwine from the
beginning these constraints into the fabric of the
formalism. Since any rules can be governed by a
Boolean expression, which in turn can accept any
Boolean python functions, it becomes feasible to
define a formalism in which a constraint is no
longer reduced to only linguistic data, but to any
properties that a full-fledged programming lan-
guage can allow. Thus, any rule can be con-
strained during its application with complex con-
straints which are implemented as a python script.
Example
pythontest is a generic Boolean python function,
which any XIP rules can embed within its own
set of constraints.
Below are some examples of XIP rules, which
are constrained with this generic python function.
A constraint in XIP is introduced with the key-
word “if”.
</bodyText>
<listItem confidence="0.983432666666667">
• A chunking rule:
PP = prep, NP#1, if (pythontest(#1)).
• A dependency rule:
</listItem>
<bodyText confidence="0.931727857142857">
if (subject(#1,#2) &amp; pythontest(#1)) ...
However, since any rule might be constrained
with an external process it should be noted that
this system can no longer be described as a pure
linguistic parser. Its expression power largely
exceeds what is usually expected from a syntac-
tic formalism.
</bodyText>
<sectionHeader confidence="0.994938" genericHeader="method">
6 Implementation Examples
</sectionHeader>
<bodyText confidence="0.999905384615384">
We have successfully used Python in our gram-
mars in two different applications so far. The
first implementation consists of a script that is
called at the end of any sentence analysis to store
the results in a MySQL database. Since the sav-
ing is done with a Python program, it is very
simple to modify this script to store only infor-
mation that is salient to a particular application.
In this respect, the maintenance of such a script
is much simpler and much flexible than its C++
or Java counterpart. The storage is also done at
runtime which limits the amount of data kept in
memory.
</bodyText>
<page confidence="0.997897">
39
</page>
<bodyText confidence="0.99995925">
The second example is the implementation of a
co-reference system (Salah Aït-Mohktar to ap-
pear), which uses Python as a backup language
to keep a specific representation of linguistic in-
formation that is used at the end of the analysis
to link together pronouns and their antecedents.
Once again, this program could have been cre-
ated in C++ or Java, using the C++ or the Java
XIP API, however, the development of such a
system in python benefits from the simplicity of
the language itself and its direct bridge to inter-
nal XIP representation.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986971428571">
The integration of a linguistic parser into an ap-
plication has always posed some tricky prob-
lems. First, the grammar, whether it has been
compiled into an external library or run through
an interpreter, often works as a black box, which
allows little or no possibility of interfering with
the internal execution. Second, the output is usu-
ally frozen into one single object which forces
the calling applications to perform format trans-
lation afterward. In many systems (Cunningham
et al.,2002, Grinberg et al., 1995), the output is
often a large, complex object, or a large XML
document. This has an impact on both memory
footprint (these objects might be very large) and
the analysis speed as the system must re-
implement some tree operators to traverse these
objects. Thereby, the automatic extraction of all
nodes that share a common property on the basis
of these objects requires some cumbersome pro-
gramming, when this could be more elegantly
handled through the linguistic formalism. Third,
the use of extra-linguistic information often im-
poses a modification of the parsing engine itself,
which prevents developers from switching
quickly between heterogeneous data sources. For
a long time, linguistic formalisms have been
conceived as specialized theoretical languages
with little if no algorithmic possibilities. How-
ever, today, the use of syntactic parsers in large
applications triggers the need for more than just
pure linguistic description. For all these reasons,
the integration of a script language as part of the
formalism seems a reasonable solution, as it will
transform dedicated linguistic formalisms to lin-
guistically driven programming languages.
</bodyText>
<sectionHeader confidence="0.994921" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.996760272727273">
Gazdar G., Klein E., Pullum G., Sag A. I., 1985. Gen-
eralized Phrase Structure Grammar, Blackwell,
Cambridge Mass., Harvard University Press.
Pereira F. and S. Shieber, 1987. Prolog and Natural
Language Analysis, CSLI, Chicago University
Press.
Allen J. F, 1994. TRAINS Parsing System, Natural
Language Understanding, Second Ed., chapters
3,4,5.
Tapanainen P., Järvinen T. 1994. Syntactic analysis
of natural language using linguistic rules and cor-
pus-based patterns, Proceedings of the 15th con-
ference on Computational linguistics, Kyoto, Japan,
pages: 629-634.
Constant P. 1995. L&apos;analyseur Linguistique SYLEX, 5
ème école d&apos;été du CNET.
Grinberg D., Lafferty John, Sleator D., 1995. A robust
parsing algorithm for link grammars, Carnegie
Mellon University Computer Science technical re-
port CMU-CS-95-125, also Proceedings of the
Fourth International Workshop on Parsing Tech-
nologies, Prague, September, 1995.
Fellbaum C., 1998. WordNet: An Electronic Lexical
Database, Rider University and Princeton Univer-
sity, Cambridge, MA: The MIT Press (Language,
speech, and communication series), 1998, xxii+423
pp; hardbound, ISBN 0-262-06197-X.
Roux C. 1999. Phrase-Driven Parser,Proceedings of
VEXTALL 99, Venezia, San Servolo, V.I.U. - 22-
24.
Blache P., Balfourier J.-M., 2001. Property Gram-
mars: a Flexible Constraint-Based Approach to
Parsing, in proceedings of IWPT-2001.
Aït-Mokhtar S., Chanod J-P., Roux C., 2002. Robust-
ness beyond shallowness incremental dependency
parsing, NLE Journal, 2002.
Hagège C., Roux C.,2002. A Robust And Flexible
Platform for Dependency Extraction, in proceed-
ings of LREC 2002.
Declerck T. 2002, A set of tools for integrating lin-
guistic and non-linguistic information, Proceedings
of SAAKM.
H. Cunningham, D. Maynard, K. Bontcheva, V. Tab-
lan.,2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools
and Applications, Proceedings of the 40th Anni-
versary Meeting of the Association for Computa-
tional Linguistics (ACL&apos;02), Philadelphia, July
2002.
Blache P., Guénot M-L. 2003. Flexible Corpus Anno-
tation with Property Grammars, BulTreeBank Pro-
ject
Roux C., 2004. Une Grammaire XML, TALN Confe-
rence, Fez, Morocco, April, 19-22, 2004.
[Python] http://www.python.org/
</reference>
<page confidence="0.998311">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348272">
<title confidence="0.998036">Coupling a Linguistic Formalism and a Script Language</title>
<author confidence="0.890897">Claude</author>
<affiliation confidence="0.930705">Xerox Research Centre Europe/</affiliation>
<address confidence="0.6960375">chemin de Maupertuis, 38240 France</address>
<email confidence="0.986612">Claude.roux@xrce.xerox.com</email>
<abstract confidence="0.975182">This article presents a novel syntactic parser architecture, in which a linguistic formalism can be enriched with all sorts of constraints, included extra-linguistic ones, thanks to the seamless coupling of the formalism with a programming language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>A I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar, Blackwell,</title>
<date>1985</date>
<publisher>University Press.</publisher>
<location>Cambridge Mass., Harvard</location>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar G., Klein E., Pullum G., Sag A. I., 1985. Generalized Phrase Structure Grammar, Blackwell, Cambridge Mass., Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>S Shieber</author>
</authors>
<title>Prolog and Natural Language Analysis, CSLI,</title>
<date>1987</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="3938" citStr="Pereira and Shieber, 1987" startWordPosition="628" endWordPosition="631">uistic formalism. 2 Scripting The system we describe in this article mix a natural language parser, namely Xerox Incremental Parser (XIP hereafter, see Aït-Mohktar et al., 2002, Roux, 1999) with a scripting language, in our case Python. The interaction of a grammar with scripting instructions is almost as old as 33 Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 33–40, Sydney, July 2006. c�2006 Association for Computational Linguistics computational linguistics. Pereira and Shieber for instance, in their book: Prolog and Natural Language Processing (see Pereira and Shieber, 1987) already suggested mixing grammar rules with extra-linguistic information. This mélange was made possible thanks to the homogeneity between grammar rules and the external code, written in both cases in the same programming language. However, these programming languages are not exactly tuned to do linguistic analyses; they are often slow and cumbersome. Moreover, they blur the boundary between program and grammar rules, as the programming language is both the algorithm language and the rule language. Allen (see Allen, 1994) proposes a different approach in his TRAINS Parsing system. The grammar</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Pereira F. and S. Shieber, 1987. Prolog and Natural Language Analysis, CSLI, Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>TRAINS Parsing System, Natural Language Understanding, Second Ed., chapters 3,4,5.</title>
<date>1994</date>
<contexts>
<context position="4466" citStr="Allen, 1994" startWordPosition="712" endWordPosition="713">n their book: Prolog and Natural Language Processing (see Pereira and Shieber, 1987) already suggested mixing grammar rules with extra-linguistic information. This mélange was made possible thanks to the homogeneity between grammar rules and the external code, written in both cases in the same programming language. However, these programming languages are not exactly tuned to do linguistic analyses; they are often slow and cumbersome. Moreover, they blur the boundary between program and grammar rules, as the programming language is both the algorithm language and the rule language. Allen (see Allen, 1994) proposes a different approach in his TRAINS Parsing system. The grammar formalism is independent to a certain extent from the implementation language, which is LISP in this case. However, since the grammar is translated into a LISP program, it is easy for a linguist to specialize the generated rules with external LISP procedures. Nevertheless, the grammar formalism remains very close to LISP data description, which makes the grammar rules somewhat difficult to read. The other solution, which is usually favored by computational linguists, is to store the external information in databases, whic</context>
</contexts>
<marker>Allen, 1994</marker>
<rawString>Allen J. F, 1994. TRAINS Parsing System, Natural Language Understanding, Second Ed., chapters 3,4,5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T Järvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns,</title>
<date>1994</date>
<booktitle>Proceedings of the 15th conference on Computational linguistics, Kyoto, Japan,</booktitle>
<pages>629--634</pages>
<marker>Tapanainen, Järvinen, 1994</marker>
<rawString>Tapanainen P., Järvinen T. 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns, Proceedings of the 15th conference on Computational linguistics, Kyoto, Japan, pages: 629-634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Constant</author>
</authors>
<title>L&apos;analyseur Linguistique SYLEX, 5 ème école d&apos;été du CNET.</title>
<date>1995</date>
<contexts>
<context position="7903" citStr="Constant, 1995" startWordPosition="1261" endWordPosition="1262">ns about the input, it could be guided through the grammar maze to favor these rules that are better suited to analyze a title for example. Finally, syntactic parsing, when it is limited to lexical information, often fails to assess correctly some ambiguous relations. Thus, the only way to deal with PP-attachment or anaphoric pronoun antecedents is to use both previous analyses and external information. However, most syntactic parsers are often ill geared to link with external modules. The formalism is engraved into a C program as in Link Grammar (see Grinberg et al.,1995) or as in Sylex (see Constant, 1995) which offers little or no opening to the rest of the world, as it is mainly designed to accomplish one unique task. We will show how the seamless integration of a script language into the very fabric of the formalism simplifies the task of keeping track of previous analyses together with the use of external sources of data. 3 Xerox Incremental Parser (XIP) The XIP engine has been developed by a research team in computational linguistics at the Xerox Research Centre Europe (see Aït-Mokhtar et al., 2001). It has been designed from the beginning to follow a strictly incremental strategy, where r</context>
</contexts>
<marker>Constant, 1995</marker>
<rawString>Constant P. 1995. L&apos;analyseur Linguistique SYLEX, 5 ème école d&apos;été du CNET.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Grinberg</author>
<author>Lafferty John</author>
<author>D Sleator</author>
</authors>
<title>A robust parsing algorithm for link grammars,</title>
<date>1995</date>
<booktitle>also Proceedings of the Fourth International Workshop on Parsing Technologies,</booktitle>
<tech>technical report CMU-CS-95-125,</tech>
<institution>Carnegie Mellon University Computer Science</institution>
<location>Prague,</location>
<contexts>
<context position="28218" citStr="Grinberg et al., 1995" startWordPosition="4679" endWordPosition="4682">m the simplicity of the language itself and its direct bridge to internal XIP representation. 7 Conclusion The integration of a linguistic parser into an application has always posed some tricky problems. First, the grammar, whether it has been compiled into an external library or run through an interpreter, often works as a black box, which allows little or no possibility of interfering with the internal execution. Second, the output is usually frozen into one single object which forces the calling applications to perform format translation afterward. In many systems (Cunningham et al.,2002, Grinberg et al., 1995), the output is often a large, complex object, or a large XML document. This has an impact on both memory footprint (these objects might be very large) and the analysis speed as the system must reimplement some tree operators to traverse these objects. Thereby, the automatic extraction of all nodes that share a common property on the basis of these objects requires some cumbersome programming, when this could be more elegantly handled through the linguistic formalism. Third, the use of extra-linguistic information often imposes a modification of the parsing engine itself, which prevents develo</context>
</contexts>
<marker>Grinberg, John, Sleator, 1995</marker>
<rawString>Grinberg D., Lafferty John, Sleator D., 1995. A robust parsing algorithm for link grammars, Carnegie Mellon University Computer Science technical report CMU-CS-95-125, also Proceedings of the Fourth International Workshop on Parsing Technologies, Prague, September, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<journal>ISBN</journal>
<pages>0--262</pages>
<publisher>The MIT Press</publisher>
<institution>Rider University and Princeton University,</institution>
<location>Cambridge, MA:</location>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum C., 1998. WordNet: An Electronic Lexical Database, Rider University and Princeton University, Cambridge, MA: The MIT Press (Language, speech, and communication series), 1998, xxii+423 pp; hardbound, ISBN 0-262-06197-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Roux</author>
</authors>
<date>1999</date>
<booktitle>Phrase-Driven Parser,Proceedings of VEXTALL 99,</booktitle>
<pages>22--24</pages>
<location>Venezia, San Servolo, V.I.U. -</location>
<contexts>
<context position="3501" citStr="Roux, 1999" startWordPosition="566" endWordPosition="567">structions to tackle the querying of external information. However the time required to enrich the parser language may not be worth the effort, as the development of a complete new instruction set is a heavy and complex task that is loosely related to linguistic parser programming. We propose in this article a new way of building natural language parsers, with the coupling of a script language with an already rich linguistic formalism. 2 Scripting The system we describe in this article mix a natural language parser, namely Xerox Incremental Parser (XIP hereafter, see Aït-Mohktar et al., 2002, Roux, 1999) with a scripting language, in our case Python. The interaction of a grammar with scripting instructions is almost as old as 33 Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 33–40, Sydney, July 2006. c�2006 Association for Computational Linguistics computational linguistics. Pereira and Shieber for instance, in their book: Prolog and Natural Language Processing (see Pereira and Shieber, 1987) already suggested mixing grammar rules with extra-linguistic information. This mélange was made possible thanks to the homogeneity between grammar rules and the e</context>
</contexts>
<marker>Roux, 1999</marker>
<rawString>Roux C. 1999. Phrase-Driven Parser,Proceedings of VEXTALL 99, Venezia, San Servolo, V.I.U. - 22-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blache</author>
<author>J-M Balfourier</author>
</authors>
<title>Property Grammars: a Flexible Constraint-Based Approach to Parsing, in proceedings of IWPT-2001.</title>
<date>2001</date>
<marker>Blache, Balfourier, 2001</marker>
<rawString>Blache P., Balfourier J.-M., 2001. Property Grammars: a Flexible Constraint-Based Approach to Parsing, in proceedings of IWPT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Aït-Mokhtar</author>
<author>J-P Chanod</author>
<author>C Roux</author>
</authors>
<title>Robustness beyond shallowness incremental dependency parsing,</title>
<date>2002</date>
<journal>NLE Journal,</journal>
<marker>Aït-Mokhtar, Chanod, Roux, 2002</marker>
<rawString>Aït-Mokhtar S., Chanod J-P., Roux C., 2002. Robustness beyond shallowness incremental dependency parsing, NLE Journal, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hagège</author>
</authors>
<title>Roux C.,2002. A Robust And Flexible Platform for Dependency Extraction,</title>
<date>2002</date>
<booktitle>in proceedings of LREC</booktitle>
<marker>Hagège, 2002</marker>
<rawString>Hagège C., Roux C.,2002. A Robust And Flexible Platform for Dependency Extraction, in proceedings of LREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Declerck</author>
</authors>
<title>A set of tools for integrating linguistic and non-linguistic information,</title>
<date>2002</date>
<booktitle>Proceedings of SAAKM.</booktitle>
<contexts>
<context position="2019" citStr="Declerck, 2002" startWordPosition="324" endWordPosition="326"> name, whose function is “President”. Washington can be a town, a state, the name of a famous president, but also the name of an actor. Moreover, the analysis of a sentence is never an independent process, if President Bush is found in a text, the reference to president, later in the document will be related to this phrase. These problems are certainly not new and a dense literature has been written about how to better deal with these issues. However, most solutions rely on formalism enrichments with solutions “engraved in stone”, that makes it difficult to adapt a grammar to new domains (see Declerck, 2002, or Roux, 2004), even though they use XML representation or database to store huge amounts of extra-linguistic information. The interpretation of these data is intertwined into the very fabric of the parser and requires deep modifications to use new sources with a complete different DTD. Of course, there are many other ways to solve these problems. For instance, in the case of languages such as Prolog or Lisp, the grammar formalism is often indistinguishable from the programming language itself. For these parsers, the querying of external information is easily solved as grammar rules can be n</context>
</contexts>
<marker>Declerck, 2002</marker>
<rawString>Declerck T. 2002, A set of tools for integrating linguistic and non-linguistic information, Proceedings of SAAKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02),</booktitle>
<location>Philadelphia,</location>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, V. Tablan.,2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications, Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02), Philadelphia, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blache</author>
<author>M-L Guénot</author>
</authors>
<title>Flexible Corpus Annotation with Property Grammars, BulTreeBank Project</title>
<date>2003</date>
<marker>Blache, Guénot, 2003</marker>
<rawString>Blache P., Guénot M-L. 2003. Flexible Corpus Annotation with Property Grammars, BulTreeBank Project</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Roux</author>
</authors>
<date>2004</date>
<booktitle>Une Grammaire XML, TALN Conference,</booktitle>
<location>Fez, Morocco,</location>
<contexts>
<context position="2035" citStr="Roux, 2004" startWordPosition="328" endWordPosition="329">on is “President”. Washington can be a town, a state, the name of a famous president, but also the name of an actor. Moreover, the analysis of a sentence is never an independent process, if President Bush is found in a text, the reference to president, later in the document will be related to this phrase. These problems are certainly not new and a dense literature has been written about how to better deal with these issues. However, most solutions rely on formalism enrichments with solutions “engraved in stone”, that makes it difficult to adapt a grammar to new domains (see Declerck, 2002, or Roux, 2004), even though they use XML representation or database to store huge amounts of extra-linguistic information. The interpretation of these data is intertwined into the very fabric of the parser and requires deep modifications to use new sources with a complete different DTD. Of course, there are many other ways to solve these problems. For instance, in the case of languages such as Prolog or Lisp, the grammar formalism is often indistinguishable from the programming language itself. For these parsers, the querying of external information is easily solved as grammar rules can be naturally augment</context>
<context position="5203" citStr="Roux, 2004" startWordPosition="825" endWordPosition="826"> implementation language, which is LISP in this case. However, since the grammar is translated into a LISP program, it is easy for a linguist to specialize the generated rules with external LISP procedures. Nevertheless, the grammar formalism remains very close to LISP data description, which makes the grammar rules somewhat difficult to read. The other solution, which is usually favored by computational linguists, is to store the external information in databases, which are accessed with some pre-defined instructions and translated into linguistic features. For instance (see Declerk, 2002 or Roux, 2004), the external information is presented as an XML document whose DTD is defined once and for all. This DTD is then enriched with extra-linguistic information that a parser can exploit to guide rule application. This method alleviates the necessity of a complex interaction mechanism between the parser and its external data sources. The XPath language is used to query this document in order to retrieve salient information at parsing time, which is then translated into local linguistic features. However, only static information can be exploited, as these XML databases must be built beforehand. Si</context>
</contexts>
<marker>Roux, 2004</marker>
<rawString>Roux C., 2004. Une Grammaire XML, TALN Conference, Fez, Morocco, April, 19-22, 2004.</rawString>
</citation>
<citation valid="false">
<note>[Python] http://www.python.org/</note>
<marker></marker>
<rawString>[Python] http://www.python.org/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>