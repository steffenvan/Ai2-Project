<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9832085">
Concise Integer Linear Programming Formulations
for Dependency Parsing
</title>
<author confidence="0.991578">
Andr´e F. T. Martins*† Noah A. Smith* Eric P. Xing*
</author>
<affiliation confidence="0.9348645">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.996049">
{afm,nasmith,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.984799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996447368421">
We formulate the problem of non-
projective dependency parsing as a
polynomial-sized integer linear pro-
gram. Our formulation is able to handle
non-local output features in an efficient
manner; not only is it compatible with
prior knowledge encoded as hard con-
straints, it can also learn soft constraints
from data. In particular, our model is able
to learn correlations among neighboring
arcs (siblings and grandparents), word
valency, and tendencies toward nearly-
projective parses. The model parameters
are learned in a max-margin framework
by employing a linear programming
relaxation. We evaluate the performance
of our parser on data in several natural
languages, achieving improvements over
existing state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.992499" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999438928571428">
Much attention has recently been devoted to in-
teger linear programming (ILP) formulations of
NLP problems, with interesting results in appli-
cations like semantic role labeling (Roth and Yih,
2005; Punyakanok et al., 2004), dependency pars-
ing (Riedel and Clarke, 2006), word alignment
for machine translation (Lacoste-Julien et al.,
2006), summarization (Clarke and Lapata, 2008),
and coreference resolution (Denis and Baldridge,
2007), among others. In general, the rationale for
the development of ILP formulations is to incorpo-
rate non-local features or global constraints, which
are often difficult to handle with traditional algo-
rithms. ILP formulations focus more on the mod-
eling of problems, rather than algorithm design.
While solving an ILP is NP-hard in general, fast
solvers are available today that make it a practical
solution for many NLP problems.
This paper presents new, concise ILP formu-
lations for projective and non-projective depen-
dency parsing. We believe that our formula-
tions can pave the way for efficient exploitation of
global features and constraints in parsing applica-
tions, leading to more powerful models. Riedel
and Clarke (2006) cast dependency parsing as
an ILP, but efficient formulations remain an open
problem. Our formulations offer the following
comparative advantages:
</bodyText>
<listItem confidence="0.744290375">
• The numbers of variables and constraints are
polynomial in the sentence length, as opposed to
requiring exponentially many constraints, elim-
inating the need for incremental procedures like
the cutting-plane algorithm;
• LP relaxations permit fast online discriminative
training of the constrained model;
• Soft constraints may be automatically learned
</listItem>
<bodyText confidence="0.953338090909091">
from data. In particular, our formulations han-
dle higher-order arc interactions (like siblings
and grandparents), model word valency, and can
learn to favor nearly-projective parses.
We evaluate the performance of the new parsers
on standard parsing tasks in seven languages. The
techniques that we present are also compatible
with scenarios where expert knowledge is avail-
able, for example in the form of hard or soft first-
order logic constraints (Richardson and Domin-
gos, 2006; Chang et al., 2008).
</bodyText>
<sectionHeader confidence="0.974142" genericHeader="method">
2 Dependency Parsing
</sectionHeader>
<subsectionHeader confidence="0.866268">
2.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.9985091">
A dependency tree is a lightweight syntactic repre-
sentation that attempts to capture functional rela-
tionships between words. Lately, this formalism
has been used as an alternative to phrase-based
parsing for a variety of tasks, ranging from ma-
chine translation (Ding and Palmer, 2005) to rela-
tion extraction (Culotta and Sorensen, 2004) and
question answering (Wang et al., 2007).
Let us first describe formally the set of legal de-
pendency parse trees. Consider a sentence x =
</bodyText>
<note confidence="0.912306666666667">
342
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.942613306122449">
hw0,... , wni, where wi denotes the word at the i-
th position, and w0 = $ is a wall symbol. We form
the (complete1) directed graph D = hV, Ai, with
vertices in V = {0, ... , n} (the i-th vertex corre-
sponding to the i-th word) and arcs in A = V 2.
Using terminology from graph theory, we say that
B ⊆ A is an r-arborescence2 of the directed
graph D if hV, Bi is a (directed) tree rooted at r.
We define the set of legal dependency parse trees
of x (denoted Y(x)) as the set of 0-arborescences
of D, i.e., we admit each arborescence as a poten-
tial dependency tree.
Let y ∈ Y(x) be a legal dependency tree for
x; if the arc a = hi, ji ∈ y, we refer to i as the
parent of j (denoted i = π(j)) and j as a child of
i. We also say that a is projective (in the sense of
Kahane et al., 1998) if any vertex k in the span of
a is reachable from i (in other words, if for any k
satisfying min(i, j) &lt; k &lt; max(i, j), there is a
directed path in y from i to k). A dependency tree
is called projective if it only contains projective
arcs. Fig. 1 illustrates this concept.3
The formulation to be introduced in §3 makes
use of the notion of the incidence vector associ-
ated with a dependency tree y ∈ Y(x). This is
the binary vector z °_ hzaia∈A with each compo-
nent defined as za = ff(a ∈ y) (here, ff(.) denotes
the indicator function). Considering simultane-
ously all incidence vectors of legal dependency
trees and taking the convex hull, we obtain a poly-
hedron that we call the arborescence polytope,
denoted by Z(x). Each vertex of Z(x) can be
identified with a dependency tree in Y(x). The
Minkowski-Weyl theorem (Rockafellar, 1970) en-
sures that Z(x) has a representation of the form
Z(x) = {z ∈ R|A  ||Az ≤ b}, for some p-by-|A|
matrix A and some vector b in Rp. However, it is
not easy to obtain a compact representation (where
p grows polynomially with the number of words
n). In §3, we will provide a compact represen-
tation of an outer polytope ¯Z(x) ⊇ Z(x) whose
integer vertices correspond to dependency trees.
Hence, the problem of finding the dependency tree
that maximizes some linear function of the inci-
1The general case where A C_ V 2 is also of interest; it
arises whenever a constraint or a lexicon forbids some arcs
from appearing in dependency tree. It may also arise as a
consequence of a first-stage pruning step where some candi-
date arcs are eliminated; this will be further discussed in §4.
</bodyText>
<footnote confidence="0.7283565">
2Or “directed spanning tree with designated root r.”
3In this paper, we consider unlabeled dependency parsing,
</footnote>
<bodyText confidence="0.755649">
where only the backbone structure (i.e., the arcs without the
labels depicted in Fig. 1) is to be predicted.
</bodyText>
<figureCaption confidence="0.934912333333333">
Figure 1: A projective dependency parse (top), and a non-
Figure 2: Nonprojective dependency graph.
projective dependency parse (bottom) for two English sen-
</figureCaption>
<bodyText confidence="0.943881428571429">
tences;examples from McDonald and Satta (2007).
those that assume each dependency decision
denceevectorsmcan befcastdas ansILP. A similar idea
was aplied to word alignment by Lacoste-Julien
that dependency graphs must be trees. Such mod
their parameters facor relative to individual edges
et al. (2006), where permutations (rather than ar-
of the graph (Paskin, 2001; McDonald et a.,
l are comny d o as gefacd
2005a). Edge-factored models have many computa-
h pm cto ativ ndidl dge
borescences) were the combinatorial structure be-
f th gah (Pki 2001 MDld t l
ing requiring representation.
</bodyText>
<equation confidence="0.962850333333333">
tional benefits, most notably
2005) Edgfatd dl
Letting X denote the set of possible sentences,
define Y°_ IJ,,,Ex y(x). Given a labeled dataset
my em in treatin ch dpedy a i
L hhx1, yi), ... , (xm, ym)) E2(X x y)m, we
</equation>
<bodyText confidence="0.9595974">
ai to earn a parse, i.e., a functo h : X → Y
mary problem in treating each dependency s in
Nonlocal information such as arity (o valy
that given x ∈ X ouputs a legal dependency parse
depedent is that it is not a realistic assumption.
and neighbouring dependencies can be crucial to
y ∈ Y(x). Te fct tht ter e xponentially
Nn-local informaton, such as arity (or valency)
obtaining high parsing accuracie (Kein and Man-
may candidates in Y(x) maks dependency pars-
and neighbouring dependencis, cn be crucial to
ning, 2002; McDonald and Pereira, 2006) How-
inga strucured clasification problem.
obaing high parsng accuracie (Klei
evr, in the data-driven parsing setting
</bodyText>
<subsectionHeader confidence="0.999882">
2.2 Arc Factorizaton and Lcality
</subsectionHeader>
<bodyText confidence="0.924373318181818">
er, in the data-driven parsing setting
rentations over the input (McDonald et
There has been much recent work on dependency
pay advd by
h go pog feu p
ur o rr
ndndi f h
so h ip (cald a, 00)
pial nre f n
parsing using graph-based, transition-based, and
pjeti parsig lgithm f bth lig ad
Th goal of hi wok i furthe r urrent
hybrid methods; see Nivre and McDonald (2008)
inference within the datadrven setting We sart by
dtdi of th pttil t f
for an overview. Typcal graph-bsed methods
invetigating and xtendng he edge-factored model
rojtie prsin lgoiths for bth leaig nd
consider liear classifiers of the fom
inference
of McDonald et al. (2005b) In partic
ithin the datadri en ettin
</bodyText>
<equation confidence="0.8415435">
gng gh, (x) = argmaxyEY wTf(x, y), (1)
tiular p
pealto the Matix Tree Theorem for multi-digraphs
t he
</equation>
<bodyText confidence="0.924970166666667">
where f(x, y) is a vector of features and w s the
tion over all possble depndency graphs for a givn
correspondingyweight vector. One wants hw. to
g bh pttion io a dge pect
haveasmallcexpected loss; the typictlnloss func-
tionnis thereHamming loss,cle(y&apos;; y)n°_  |{hi, jid∈ we sho
y0: hi, ji ∈/ y}|. Tractability s usually ensured
ing raiing gloally normalized log-linear mod-
ht they can be sed in many important earning
bystrong factorization assumptions, like the one
els, syntactic language modeling, and nsupervied
nd inference problem including minrisk decod
underlying the arc-factored mode (Esne, 1996;
ing training globally normalized log-linear mod-
McDonald et a., 2005), which forbids any feature
els syntactic language modeling and unsupervised
that depends on two or more arcs. This induces a
decomposition of the feature vector f(x, y) as:
</bodyText>
<equation confidence="0.780498">
f(x, y) = &amp;∈y fa(x). (2)
</equation>
<bodyText confidence="0.995762333333333">
Under this decomposition, each arc receives a
score; parsing amounts to choosing the configu-
ration that maximizes the overall score, which, as
</bodyText>
<page confidence="0.919574">
343
</page>
<bodyText confidence="0.999882333333333">
shown by McDonald et al. (2005), is an instance
of the maximal arborescence problem. Combi-
natorial algorithms (Chu and Liu, 1965; Edmonds,
1967) can solve this problem in cubic time.4 If
the dependency parse trees are restricted to be
projective, cubic-time algorithms are available via
dynamic programming (Eisner, 1996). While in
the projective case, the arc-factored assumption
can be weakened in certain ways while maintain-
ing polynomial parser runtime (Eisner and Satta,
1999), the same does not happen in the nonprojec-
tive case, where finding the highest-scoring tree
becomes NP-hard (McDonald and Satta, 2007).
Approximate algorithms have been employed to
handle models that are not arc-factored (although
features are still fairly local): McDonald and
Pereira (2006) adopted an approximation based
on O(n3) projective parsing followed by a hill-
climbing algorithm to rearrange arcs, and Smith
and Eisner (2008) proposed an algorithm based on
loopy belief propagation.
</bodyText>
<sectionHeader confidence="0.824071" genericHeader="method">
3 Dependency Parsing as an ILP
</sectionHeader>
<bodyText confidence="0.999970863636364">
Our approach will build a graph-based parser
without the drawback of a restriction to local fea-
tures. By formulating inference as an ILP, non-
local features can be easily accommodated in our
model; furthermore, by using a relaxation tech-
nique we can still make learning tractable. The im-
pact of LP-relaxed inference in the learning prob-
lem was studied elsewhere (Martins et al., 2009).
A linear program (LP) is an optimization prob-
lem of the form
If the problem is feasible, the optimum is attained
at a vertex of the polyhedron that defines the con-
straint space. If we add the constraint x E Zd, then
the above is called an integer linear program
(ILP). For some special parameter settings—e.g.,
when b is an integer vector and A is totally uni-
modular5—all vertices of the constraining polyhe-
dron are integer points; in these cases, the integer
constraint may be suppressed and (3) is guaran-
teed to have integer solutions (Schrijver, 2003).
Of course, this need not happen: solving a gen-
eral ILP is an NP-complete problem. Despite this
</bodyText>
<footnote confidence="0.752052">
4There is also a quadratic algorithm due to Tarjan (1977).
5A matrix is called totally unimodular if the determinants
of each square submatrix belong to {0, 1, −1}.
</footnote>
<bodyText confidence="0.999983952380953">
fact, fast solvers are available today that make this
a practical solution for many problems. Their per-
formance depends on the dimensions and degree
of sparsity of the constraint matrix A.
Riedel and Clarke (2006) proposed an ILP for-
mulation for dependency parsing which refines
the arc-factored model by imposing linguistically
motivated “hard” constraints that forbid some arc
configurations. Their formulation includes an ex-
ponential number of constraints—one for each
possible cycle. Since it is intractable to throw
in all constraints at once, they propose a cutting-
plane algorithm, where the cycle constraints are
only invoked when violated by the current solu-
tion. The resulting algorithm is still slow, and an
arc-factored model is used as a surrogate during
training (i.e., the hard constraints are only used at
test time), which implies a discrepancy between
the model that is optimized and the one that is ac-
tually going to be used.
Here, we propose ILP formulations that elim-
inate the need for cycle constraints; in fact, they
require only a polynomial number of constraints.
Not only does our model allow expert knowledge
to be injected in the form of constraints, it is also
capable of learning soft versions of those con-
straints from data; indeed, it can handle features
that are not arc-factored (correlating, for exam-
ple, siblings and grandparents, modeling valency,
or preferring nearly projective parses). While, as
pointed out by McDonald and Satta (2007), the
inclusion of these features makes inference NP-
hard, by relaxing the integer constraints we obtain
approximate algorithms that are very efficient and
competitive with state-of-the-art methods. In this
paper, we focus on unlabeled dependency parsing,
for clarity of exposition. If it is extended to labeled
parsing (a straightforward extension), our formu-
lation fully subsumes that of Riedel and Clarke
(2006), since it allows using the same hard con-
straints and features while keeping the ILP poly-
nomial in size.
</bodyText>
<subsectionHeader confidence="0.999289">
3.1 The Arborescence Polytope
</subsectionHeader>
<bodyText confidence="0.995469714285714">
We start by describing our constraint space. Our
formulations rely on a concise polyhedral repre-
sentation of the set of candidate dependency parse
trees, as sketched in §2.1. This will be accom-
plished by drawing an analogy with a network
flow problem.
Let D = (V, A) be the complete directed graph
</bodyText>
<figure confidence="0.942136142857143">
(3)
s.t. Ax � b.
min.ERd cTx
344
• Flow is zero on disabled arcs:
Oa ≤ nza, a ∈ A (8)
associated with a sentence x ∈ X, as stated in
</figure>
<listItem confidence="0.830010142857143">
§2. A subgraph y = hV, Bi is a legal dependency
tree (i.e., y ∈ Y(x)) if and only if the following
conditions are met:
1. Each vertex in V \ {0} must have exactly one
incoming arc in B,
2. 0 has no incoming arcs in B,
3. B does not contain cycles.
</listItem>
<equation confidence="0.8592315">
For each vertex v ∈ V , let S−(v) , {hi, ji ∈
A  |j = v} denote its set of incoming arcs, and
</equation>
<bodyText confidence="0.89884575">
S+(v) , {hi, ji ∈ A  |i = v} denote its set of
outgoing arcs. The two first conditions can be eas-
ily expressed by linear constraints on the incidence
vector z:
</bodyText>
<equation confidence="0.9998275">
EaEd−(j) za = 1, j ∈ V \ {0} (4)
EaEd−(0) za = 0 (5)
</equation>
<bodyText confidence="0.966891590909091">
Condition 3 is somewhat harder to express. Rather
than adding exponentially many constraints, one
for each potential cycle (like Riedel and Clarke,
2006), we equivalently replace condition 3 by
30. B is connected.
Note that conditions 1-2-3 are equivalent to 1-2-
30, in the sense that both define the same set Y(x).
However, as we will see, the latter set of condi-
tions is more convenient. Connectedness of graphs
can be imposed via flow constraints (by requir-
ing that, for any v ∈ V \ {0}, there is a directed
path in B connecting 0 to v). We adapt the single
commodity flow formulation for the (undirected)
minimum spanning tree problem, due to Magnanti
and Wolsey (1994), that requires O(n2) variables
and constraints. Under this model, the root node
must send one unit of flow to every other node.
By making use of extra variables, 0i , h0aiaEA,
to denote the flow of commodities through each
arc, we are led to the following constraints in ad-
dition to Eqs. 4–5 (we denote U , [0, 1], and
B , {0, 1} = U ∩ Z):
</bodyText>
<listItem confidence="0.811040333333333">
• Root sends flow n:
EaEd+(0) Oa = n (6)
• Each node consumes one unit of flow:
</listItem>
<equation confidence="0.97642275">
E �a − � Oa=1, j ∈ V \ {0} (7)
aEd−(j) aEd+(j)
• Each arc indicator lies in the unit interval:
za ∈ U, a ∈ A. (9)
</equation>
<bodyText confidence="0.581044">
These constraints project an outer bound of the ar-
borescence polytope, i.e.,
</bodyText>
<equation confidence="0.994472">
Z(x) , {z ∈ R|A  ||(z, 0) satisfy (4–9)}
⊇ Z(x). (10)
</equation>
<bodyText confidence="0.9591525">
Furthermore, the integer points of �Z(x) are pre-
cisely the incidence vectors of dependency trees
in Y(x); these are obtained by replacing Eq. 9 by
za ∈ B, a ∈ A. (11)
</bodyText>
<subsectionHeader confidence="0.920815">
3.2 Arc-Factored Model
</subsectionHeader>
<bodyText confidence="0.999948">
Given our polyhedral representation of (an outer
bound of) the arborescence polytope, we can
now formulate dependency parsing with an arc-
factored model as an ILP. By storing the arc-
local feature vectors into the columns of a matrix
F(x) , [fa(x)]aEA, and defining the score vec-
tor s , F(x)Tw (each entry is an arc score) the
inference problem can be written as
</bodyText>
<equation confidence="0.957516375">
wTf(x, y) = max
ZEZ(x)
= max
Z,O
s.t. A I J ≤ b
�
z ∈ B
(12)
</equation>
<bodyText confidence="0.999682636363636">
where A is a sparse constraint matrix (with O(|A|)
non-zero elements), and b is the constraint vec-
tor; A and b encode the constraints (4–9). This
is an ILP with O(|A|) variables and constraints
(hence, quadratic in n); if we drop the integer
constraint the problem becomes the LP relaxation.
As is, this formulation is no more attractive than
solving the problem with the existing combinato-
rial algorithms discussed in §2.2; however, we can
now start adding non-local features to build a more
powerful model.
</bodyText>
<subsectionHeader confidence="0.998226">
3.3 Sibling and Grandparent Features
</subsectionHeader>
<bodyText confidence="0.997476666666667">
To cope with higher-order features of the form
fa1,...,aK(x) (i.e., features whose values depend on
the simultaneous inclusion of arcs a1, ... , aK on
</bodyText>
<figure confidence="0.7671946">
max
yEY(x)
wTF(x)z
sTz
345
</figure>
<bodyText confidence="0.9149468">
a candidate dependency tree), we employ a lin-
earization trick (Boros and Hammer, 2002), defin-
ing extra variables zal...aK , zal ∧...∧zaK. This
logical relation can be expressed by the following
O(K) agreement constraints:6
</bodyText>
<equation confidence="0.998094666666667">
za1...aK ≤ zai, i = 1, ..., K
x
za1 ... aK ≥ �i=1 zai − K + 1. (13)
</equation>
<bodyText confidence="0.996682692307692">
As shown by McDonald and Pereira (2006) and
Carreras (2007), the inclusion of features that
correlate sibling and grandparent arcs may be
highly beneficial, even if doing so requires resort-
ing to approximate algorithms.7 Define Rsibl ,
{hi, j, ki  |hi, ji ∈ A, hi, ki ∈ A} and Rgrand ,
{hi, j, ki  |hi, ji
such features in our formulation, we need to add
extra variables zsibl , hzrir∈Rsibl and zgrand ,
hzrir∈Rgrand that indicate the presence of sibling
and grandparent arcs. Observe that these indica-
tor variables are conjunctions of arc indicator vari-
ables, i.e., zsibl
</bodyText>
<equation confidence="0.857865">
ijk = zij ∧ zik and zgrand
ijk = zij ∧ zjk.
</equation>
<bodyText confidence="0.969572666666667">
Hence, these features can be handled in our formu-
lation by adding the following O(|A |· |V |) vari-
ables and constraints:
</bodyText>
<equation confidence="0.874678333333333">
zsibl
ijk ≤ zij, zsibl
ijk ≤ zik, zsibl
ijk ≥ zij + zik −
1
for all triples hi, j, ki ∈ Rsibl, and
zijk ≤ zij, zgrand
grand ijk ≤ zjk, zgrand
ijk ≥ zij+zjk−1
</equation>
<bodyText confidence="0.94442092">
for all triples hi, j, ki ∈ Rgrand. Let R , A ∪
Rsibl ∪ Rgrand; by redefining z , hzrir∈R and
F(x) , [fr(x)]r∈R, we may express our inference
problem as in Eq. 12, with O(|A |· |V |) variables
and constraints.
Notice that the strategy just described to han-
dle sibling features is not fully compatible with
the features proposed by Eisner (1996) for pro-
jective parsing, as the latter correlate only con-
secutive siblings and are also able to place spe-
cial features on the first child of a given word.
The ability to handle such “ordered” features is
intimately associated with Eisner’s dynamic pro-
gramming parsing algorithm and with the Marko-
vian assumptions made explicitly by his genera-
tive model. We next show how similar features
6Actually, any logical condition can be encoded with lin-
ear constraints involving binary variables; see e.g. Clarke and
Lapata (2008) for an overview.
7By sibling features we mean features that depend on
pairs of sibling arcs (i.e., of the form (i, j) and (i, k)); by
grandparent features we mean features that depend on pairs
of grandparent arcs (of the form (i, j) and (j, k)).
can be incorporated in our model by adding “dy-
namic” constraints to our ILP. Define:
</bodyText>
<equation confidence="0.9905176">
I 1 if hi, ji and hi, ki are
consecutive siblings,
0 otherwise,
�
1 if j is the first child of i,
</equation>
<bodyText confidence="0.662215">
zfirst child ,
ij 0 otherwise.
</bodyText>
<listItem confidence="0.5506518">
Suppose (without loss of generality) that i &lt; j &lt;
k ≤ n. We could naively compose the constraints
(14) with additional linear constraints that encode
the logical relation
next sibl — sibl /�
</listItem>
<equation confidence="0.878474">
zijk — zijk ∧ /\jGlGk ¬zil,
</equation>
<bodyText confidence="0.994185666666667">
but this would yield a constraint matrix with
O(n4) non-zero elements. Instead, we define aux-
iliary variables βjk and γij:
</bodyText>
<equation confidence="0.9590308">
�
1, if ∃l s.t. π(l) = π(j) &lt; j &lt; l &lt; k
βjk = 0, otherwise,
�
1, if ∃k s.t. i &lt; k &lt; j and hi, ki ∈ y
γij �
=
an
znext sibl
ijk
zsibl
ijk∧(¬βjk)
d
zfirst child = zij ∧(¬γij), which can be encoded via
ij
next sibl sibl first child
zijk ≤ zijk zij ≤ zij
next sibl first child
zijk &lt; 1 — βjk zi . &lt; 1 — 1&apos;ij
next sibl sibl �rst child
zijk ≥ zijk − βjk zij ≥ zij − γij
βj(j+1) � 0 γi(i+1) � 0
βj(k+1)≥βjkγi(j+1) ≥ γij
zsibl γi(j+1) ≥ zij
ijk
</equation>
<bodyText confidence="0.7781104">
sibl
ijk
z
γi(j+1)≤ γij +zij
analogously for the case n
</bodyText>
<equation confidence="0.96024875">
i &gt; j &gt; k. This
results in a sparser constraint matrix, with only
≥
O(n3) non-zero elements.
</equation>
<bodyText confidence="0.69212075">
Then, we have that
The following
constraints encode the
logical relations for the auxiliary vari
</bodyText>
<equation confidence="0.9675036">
“dynamic”
ables (16):
�
βj(k+1) ≤βjk +
iGj
</equation>
<bodyText confidence="0.896259">
Auxiliary variables and constraints are defined
</bodyText>
<subsectionHeader confidence="0.990665">
3.4 Valency Features
</subsectionHeader>
<bodyText confidence="0.9789992">
A crucial fact about dependency grammars is that
words have preferences about the number and ar-
rangement of arguments an
d modifiers they ac-
cept. Therefore, it is desirable to include features
</bodyText>
<figure confidence="0.685804714285714">
∈ A, hj, ki ∈ A}. To include
znext sibl
ijk ,
0, otherwise.
�
βj(k+1) ≥
iGj
</figure>
<page confidence="0.58364">
346
</page>
<bodyText confidence="0.999455307692308">
that indicate, for a candidate arborescence, how
many outgoing arcs depart from each vertex; de-
note these quantities by vi , Pa∈δ+(i) za, for
each i ∈ V . We call vi the valency of the ith ver-
tex. We add valency indicators zval
ik , ff(vi = k)
for i ∈ V and k = 0,... , n − 1. This way, we are
able to penalize candidate dependency trees that
assign unusual valencies to some of their vertices,
by specifying a individual cost for each possible
value of valency. The following O(|V |2) con-
straints encode the agreement between valency in-
dicators and the other variables:
</bodyText>
<equation confidence="0.511266">
Pn−1 k=0 kzval = Pa∈δ+(i) za, i ∈ V (17)
ik = 1, i ∈ V , n − 1}
n−1 val ≥ 0, i ∈ V, k ∈ {0, ...
Pk=0 zik
zval
ik
</equation>
<subsectionHeader confidence="0.904878">
3.5 Projectivity Features
</subsectionHeader>
<bodyText confidence="0.999718666666667">
For most languages, dependency parse trees tend
to be nearly projective (cf. Buchholz and Marsi,
2006). We wish to make our model capable of
learning to prefer “nearly” projective parses when-
ever that behavior is observed in the data.
The multicommodity directed flow model of
Magnanti and Wolsey (1994) is a refinement of the
model described in §3.1 which offers a compact
and elegant way to indicate nonprojective arcs, re-
quiring O(n3) variables and constraints. In this
model, every node k =6 0 defines a commodity:
one unit of commodity k originates at the root
node and must be delivered to node k; the vari-
able φkij denotes the flow of commodity k in arc
hi, ji. We first replace (4–9) by (18–22):
</bodyText>
<listItem confidence="0.932837">
• The root sends one unit of commodity to each
node:
φka = −1, k ∈ V \ {0} (18)
• Any node consumes its own commodity and no
other:
</listItem>
<equation confidence="0.802348">
φka = δkj , j,k ∈ V \ {0} (19)
</equation>
<bodyText confidence="0.992505">
where δk j, ff(j = k) is the Kronecker delta.
</bodyText>
<listItem confidence="0.837991333333333">
• Disabled arcs do not carry any flow:
φka ≤ za, a ∈ A, k ∈ V (20)
• There are exactly n enabled arcs:
Pa∈A za = n (21)
• All variables lie in the unit interval:
za ∈ N, φka ∈ N, a ∈ A, k ∈ V (22)
</listItem>
<bodyText confidence="0.9987728">
We next define auxiliary variables ψjk that indi-
cate if there is a path from j to k. Since each ver-
tex except the root has only one incoming arc, the
following linear equalities are enough to describe
these new variables:
</bodyText>
<equation confidence="0.7844882">
ψjk = Pa∈δ−(j) φka, j, k ∈ V \ {0}
ψ0k = 1, k ∈ V \ {0}. (23)
Now, define indicators znp , hznp
a ia∈A, where
znp
</equation>
<bodyText confidence="0.999139">
a , ff(a ∈ y and a is nonprojective).
From the definition of projective arcs in §2.1, we
have that znp
</bodyText>
<equation confidence="0.8269652">
a = 1 if and only if the arc is active
(za = 1) and there is some vertex k in the span of
a = hi, ji
following O(|A |· |V |) constraints for hi, ji ∈ A:
zij ≤ zij
np
zij ≥ zij − ψik, min(i, j) ≤ k ≤ max(i,j)
np
/, +
z j ≤ − Pk =min(i,j)+1 4&apos;ik |j − i |− 1
</equation>
<bodyText confidence="0.999887625">
There are other ways to introduce nonprojectiv-
ity indicators and alternative definitions of “non-
projective arc.” For example, by using dynamic
constraints of the same kind as those in §3.3,
we can indicate arcs that “cross” other arcs with
O(n3) variables and constraints, and a cubic num-
ber of non-zero elements in the constraint matrix
(omitted for space).
</bodyText>
<subsectionHeader confidence="0.997011">
3.6 Projective Parsing
</subsectionHeader>
<bodyText confidence="0.99995625">
It would be straightforward to adapt the con-
straints in §3.5 to allow only projective parse trees:
simply force znp
a = 0 for any a ∈ A. But there are
more efficient ways of accomplish this. While it is
difficult to impose projectivity constraints or cycle
constraints individually, there is a simpler way of
imposing both. Consider 3 (or 30) from §3.1.
</bodyText>
<subsubsectionHeader confidence="0.502676">
Proposition 1 Replace condition 3 (or 30) with
</subsubsectionHeader>
<bodyText confidence="0.946440333333333">
300. If hi, ji ∈ B, then, for any k = 1, ... , n
such that k =6 j, the parent of k must satisfy
(defining i0 , min(i, j) and j0 , max(i, j)):
</bodyText>
<equation confidence="0.72035125">
i0 ≤ π(k) ≤ j0, if i0 &lt; k &lt; j0,
π(k) &lt; i0 ∨ π(k) &gt; j0, if k &lt; i0 or k &gt; j0
ork=i.
X X
</equation>
<figure confidence="0.938477090909091">
a∈δ−(0) φk −
a
a∈δ+(0)
X X
a∈δ−(j) φk −
a a∈δ+(j)
such that ψik = 0. We are led to the
⎧
⎨⎪
⎪⎩
347
</figure>
<bodyText confidence="0.98138625">
Then, Y(x) will be redefined as the set ofprojec-
tive dependency parse trees.
We omit the proof for space. Conditions 1, 2, and
3&amp;quot; can be encoded with O(n2) constraints.
</bodyText>
<sectionHeader confidence="0.997603" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999369230769231">
We report experiments on seven languages, six
(Danish, Dutch, Portuguese, Slovene, Swedish
and Turkish) from the CoNLL-X shared task
(Buchholz and Marsi, 2006), and one (English)
from the CoNLL-2008 shared task (Surdeanu et
al., 2008).8 All experiments are evaluated using
the unlabeled attachment score (UAS), using the
default settings.9 We used the same arc-factored
features as McDonald et al. (2005) (included in the
MSTParser toolkit10); for the higher-order models
described in §3.3–3.5, we employed simple higher
order features that look at the word, part-of-speech
tag, and (if available) morphological information
of the words being correlated through the indica-
tor variables. For scalability (and noting that some
of the models require O(|V  |� |A|) constraints and
variables, which, when A = V 2, grows cubically
with the number of words), we first prune the base
graph by running a simple algorithm that ranks the
k-best candidate parents for each word in the sen-
tence (we set k = 10); this reduces the number of
candidate arcs to |A |= kn.11 This strategy is sim-
ilar to the one employed by Carreras et al. (2008)
to prune the search space of the actual parser. The
ranker is a local model trained using a max-margin
criterion; it is arc-factored and not subject to any
structural constraints, so it is very fast.
The actual parser was trained via the online
structured passive-aggressive algorithm of Cram-
mer et al. (2006); it differs from the 1-best MIRA
algorithm of McDonald et al. (2005) by solv-
ing a sequence of loss-augmented inference prob-
lems.12 The number of iterations was set to 10.
The results are summarized in Table 1; for the
sake of comparison, we reproduced three strong
8We used the provided train/test splits except for English,
for which we tested on the development partition. For train-
ing, sentences longer than 80 words were discarded. For test-
ing, all sentences were kept (the longest one has length 118).
</bodyText>
<footnote confidence="0.9636125">
9http://nextens.uvt.nl/—conll/software.html
10http://sourceforge.net/projects/mstparser
</footnote>
<bodyText confidence="0.980391068965517">
11Note that, unlike reranking approaches, there are still ex-
ponentially many candidate parse trees after pruning. The
oracle constrained to pick parents from these lists achieves
&gt; 98% in every case.
12The loss-augmented inference problem can also be ex-
pressed as an LP for Hamming loss functions that factor over
arcs; we refer to Martins et al. (2009) for further details.
baselines, all of them state-of-the-art parsers based
on non-arc-factored models: the second order
model of McDonald and Pereira (2006), the hy-
brid model of Nivre and McDonald (2008), which
combines a (labeled) transition-based and a graph-
based parser, and a refinement of the latter, due
to Martins et al. (2008), which attempts to ap-
proximate non-local features.13 We did not repro-
duce the model of Riedel and Clarke (2006) since
the latter is tailored for labeled dependency pars-
ing; however, experiments reported in that paper
for Dutch (and extended to other languages in the
CoNLL-X task) suggest that their model performs
worse than our three baselines.
By looking at the middle four columns, we can
see that adding non-arc-factored features makes
the models more accurate, for all languages. With
the exception of Portuguese, the best results are
achieved with the full set of features. We can
also observe that, for some languages, the valency
features do not seem to help. Merely modeling
the number of dependents of a word may not be
as valuable as knowing what kinds of dependents
they are (for example, distinguishing among argu-
ments and adjuncts).
Comparing with the baselines, we observe that
our full model outperforms that of McDonald and
Pereira (2006), and is in line with the most ac-
curate dependency parsers (Nivre and McDonald,
2008; Martins et al., 2008), obtained by com-
bining transition-based and graph-based parsers.14
Notice that our model, compared with these hy-
brid parsers, has the advantage of not requiring an
ensemble configuration (eliminating, for example,
the need to tune two parsers). Unlike the ensem-
bles, it directly handles non-local output features
by optimizing a single global objective. Perhaps
more importantly, it makes it possible to exploit
expert knowledge through the form of hard global
constraints. Although not pursued here, the same
kind of constraints employed by Riedel and Clarke
(2006) can straightforwardly fit into our model,
after extending it to perform labeled dependency
parsing. We believe that a careful design of fea-
13Unlike our model, the hybrid models used here as base-
lines make use of the dependency labels at training time; in-
deed, the transition-based parser is trained to predict a la-
beled dependency parse tree, and the graph-based parser use
these predicted labels as input features. Our model ignores
this information at training time; therefore, this comparison
is slightly unfair to us.
</bodyText>
<table confidence="0.8429204">
14See also Zhang and Clark (2008) for a different approach
that combines transition-based and graph-based methods.
348
DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14)
DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16)
PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02)
SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20)
SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08)
TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02)
ENGLISH 90.85 – – 90.15 91.13 91.12 91.16 91.14 (-0.02)
</table>
<tableCaption confidence="0.996826">
Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order
</tableCaption>
<bodyText confidence="0.991373391304348">
model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The
four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features
(see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference
followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold
indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arc-
factored model and the second order model of McDonald and Pereira (2006) with statistical significance (p &lt; 0.01) according
to Dan Bikel’s randomized method (http://www.cis.upenn.edu/-dbikel/software.html).
tures and constraints can lead to further improve-
ments on accuracy.
We now turn to a different issue: scalability. In
previous work (Martins et al., 2009), we showed
that training the model via LP-relaxed inference
(as we do here) makes it learn to avoid frac-
tional solutions; as a consequence, ILP solvers
will converge faster to the optimum (on average).
Yet, it is known from worst case complexity the-
ory that solving a general ILP is NP-hard; hence,
these solvers may not scale well with the sentence
length. Merely considering the LP-relaxed version
of the problem at test time is unsatisfactory, as it
may lead to a fractional solution (i.e., a solution
whose components indexed by arcs, z = (z-)-EA,
are not all integer), which does not correspond to a
valid dependency tree. We propose the following
approximate algorithm to obtain an actual parse:
first, solve the LP relaxation (which can be done
in polynomial time with interior-point methods);
then, if the solution is fractional, project it onto the
feasible set Y(x). Fortunately, the Euclidean pro-
jection can be computed in a straightforward way
by finding a maximal arborescence in the directed
graph whose weights are defined by z (we omit
the proof for space); as we saw in §2.2, the Chu-
Liu-Edmonds algorithm can do this in polynomial
time. The overall parsing runtime becomes poly-
nomial with respect to the length of the sentence.
The last column of Table 1 compares the ac-
curacy of this approximate method with the ex-
act one. We observe that there is not a substantial
drop in accuracy; on the other hand, we observed
a considerable speed-up with respect to exact in-
ference, particularly for long sentences. The av-
erage runtime (across all languages) is 0.632 sec-
onds per sentence, which is in line with existing
higher-order parsers and is much faster than the
runtimes reported by Riedel and Clarke (2006).
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999871875">
We presented new dependency parsers based on
concise ILP formulations. We have shown how
non-local output features can be incorporated,
while keeping only a polynomial number of con-
straints. These features can act as soft constraints
whose penalty values are automatically learned
from data; in addition, our model is also compati-
ble with expert knowledge in the form of hard con-
straints. Learning through a max-margin frame-
work is made effective by the means of a LP-
relaxation. Experimental results on seven lan-
guages show that our rich-featured parsers outper-
form arc-factored and approximate higher-order
parsers, and are in line with stacked parsers, hav-
ing with respect to the latter the advantage of not
requiring an ensemble configuration.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.980012444444444">
The authors thank the reviewers for their com-
ments. Martins was supported by a grant from
FCT/ICTI through the CMU-Portugal Program,
and also by Priberam Inform´atica. Smith was
supported by NSF IIS-0836431 and an IBM Fac-
ulty Award. Xing was supported by NSF DBI-
0546594, DBI-0640543, IIS-0713379, and an Al-
fred Sloan Foundation Fellowship in Computer
Science.
</bodyText>
<page confidence="0.926322">
349
</page>
<sectionHeader confidence="0.992901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901734693878">
E. Boros and P.L. Hammer. 2002. Pseudo-Boolean op-
timization. Discrete Applied Mathematics, 123(1–
3):155–225.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for effi-
cient, feature-rich parsing. In Proc. of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints
as prior knowledge. In ICML Workshop on Prior
Knowledge for Text and Language Processing.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
J. Clarke and M. Lapata. 2008. Global inference
for sentence compression an integer linear program-
ming approach. JAIR, 31:399–429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551–585.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In Proc. of HLT-NAACL.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233–240.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING-ACL.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jor-
dan. 2006. Word alignment via quadratic assign-
ment. In Proc. of HLT-NAACL.
T. L. Magnanti and L. A. Wolsey. 1994. Optimal
Trees. Technical Report 290-94, Massachusetts In-
stitute of Technology, Operations Research Center.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL-HLT.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear program-
ming inference. In Proc. of COLING.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107–136.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In Proc. of EMNLP.
R. T. Rockafellar. 1970. Convex Analysis. Princeton
University Press.
D. Roth and W. T. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In
ICML.
A. Schrijver. 2003. Combinatorial Optimization:
Polyhedra and Efficiency, volume 24 of Algorithms
and Combinatorics. Springer.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez,
and J. Nivre. 2008. The conll-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of CoNLL.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25–36.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? A quasi-synchronous gram-
mar for QA. In Proceedings of EMNLP-CoNLL.
Y. Zhang and S. Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP.
</reference>
<page confidence="0.874809">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264679">
<title confidence="0.998314">Concise Integer Linear Programming Formulations for Dependency Parsing</title>
<author confidence="0.784619">F T A Eric P</author>
<note confidence="0.4801515">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA de Instituto Superior T´ecnico, Lisboa, Portugal</note>
<abstract confidence="0.999487">We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Boros</author>
<author>P L Hammer</author>
</authors>
<title>Pseudo-Boolean optimization.</title>
<date>2002</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>123</volume>
<issue>1</issue>
<pages>3--155</pages>
<contexts>
<context position="17951" citStr="Boros and Hammer, 2002" startWordPosition="3054" endWordPosition="3057">raints (hence, quadratic in n); if we drop the integer constraint the problem becomes the LP relaxation. As is, this formulation is no more attractive than solving the problem with the existing combinatorial algorithms discussed in §2.2; however, we can now start adding non-local features to build a more powerful model. 3.3 Sibling and Grandparent Features To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on max yEY(x) wTF(x)z sTz 345 a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK. This logical relation can be expressed by the following O(K) agreement constraints:6 za1...aK ≤ zai, i = 1, ..., K x za1 ... aK ≥ �i=1 zai − K + 1. (13) As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji such features in our formulation, we need to add extra variables zsibl , hzrir∈Rsibl and zg</context>
</contexts>
<marker>Boros, Hammer, 2002</marker>
<rawString>E. Boros and P.L. Hammer. 2002. Pseudo-Boolean optimization. Discrete Applied Mathematics, 123(1– 3):155–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="22598" citStr="Buchholz and Marsi, 2006" startWordPosition="3946" endWordPosition="3949">h vertex. We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1. This way, we are able to penalize candidate dependency trees that assign unusual valencies to some of their vertices, by specifying a individual cost for each possible value of valency. The following O(|V |2) constraints encode the agreement between valency indicators and the other variables: Pn−1 k=0 kzval = Pa∈δ+(i) za, i ∈ V (17) ik = 1, i ∈ V , n − 1} n−1 val ≥ 0, i ∈ V, k ∈ {0, ... Pk=0 zik zval ik 3.5 Projectivity Features For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints. In this model, every node k =6 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22)</context>
<context position="25753" citStr="Buchholz and Marsi, 2006" startWordPosition="4592" endWordPosition="4595"> , n such that k =6 j, the parent of k must satisfy (defining i0 , min(i, j) and j0 , max(i, j)): i0 ≤ π(k) ≤ j0, if i0 &lt; k &lt; j0, π(k) &lt; i0 ∨ π(k) &gt; j0, if k &lt; i0 or k &gt; j0 ork=i. X X a∈δ−(0) φk − a a∈δ+(0) X X a∈δ−(j) φk − a a∈δ+(j) such that ψik = 0. We are led to the ⎧ ⎨⎪ ⎪⎩ 347 Then, Y(x) will be redefined as the set ofprojective dependency parse trees. We omit the proof for space. Conditions 1, 2, and 3&amp;quot; can be encoded with O(n2) constraints. 4 Experiments We report experiments on seven languages, six (Danish, Dutch, Portuguese, Slovene, Swedish and Turkish) from the CoNLL-X shared task (Buchholz and Marsi, 2006), and one (English) from the CoNLL-2008 shared task (Surdeanu et al., 2008).8 All experiments are evaluated using the unlabeled attachment score (UAS), using the default settings.9 We used the same arc-factored features as McDonald et al. (2005) (included in the MSTParser toolkit10); for the higher-order models described in §3.3–3.5, we employed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V |� |A|)</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="26719" citStr="Carreras et al. (2008)" startWordPosition="4752" endWordPosition="4755">yed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V |� |A|) constraints and variables, which, when A = V 2, grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8We used the provided tr</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="18215" citStr="Carreras (2007)" startWordPosition="3106" endWordPosition="3107">on-local features to build a more powerful model. 3.3 Sibling and Grandparent Features To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on max yEY(x) wTF(x)z sTz 345 a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK. This logical relation can be expressed by the following O(K) agreement constraints:6 za1...aK ≤ zai, i = 1, ..., K x za1 ... aK ≥ �i=1 zai − K + 1. (13) As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji such features in our formulation, we need to add extra variables zsibl , hzrir∈Rsibl and zgrand , hzrir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator variables, i.e., zsibl ijk = zij ∧ zik and zgrand ijk = zij ∧ zjk. Hence, these features can be handled in our </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Constraints as prior knowledge.</title>
<date>2008</date>
<booktitle>In ICML Workshop on Prior Knowledge for Text and Language Processing.</booktitle>
<contexts>
<context position="3232" citStr="Chang et al., 2008" startWordPosition="468" endWordPosition="471">ns permit fast online discriminative training of the constrained model; • Soft constraints may be automatically learned from data. In particular, our formulations handle higher-order arc interactions (like siblings and grandparents), model word valency, and can learn to favor nearly-projective parses. We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). 2 Dependency Parsing 2.1 Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AF</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2008</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2008. Constraints as prior knowledge. In ICML Workshop on Prior Knowledge for Text and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="10104" citStr="Chu and Liu, 1965" startWordPosition="1680" endWordPosition="1683">blem including minrisk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f(x, y) as: f(x, y) = &amp;∈y fa(x). (2) Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as 343 shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are st</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y. J. Chu and T. H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>JAIR,</journal>
<pages>31--399</pages>
<contexts>
<context position="1449" citStr="Clarke and Lapata, 2008" startWordPosition="199" endWordPosition="202">ers are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective dependency parsing. We believe t</context>
<context position="19915" citStr="Clarke and Lapata (2008)" startWordPosition="3413" endWordPosition="3416">escribed to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features 6Actually, any logical condition can be encoded with linear constraints involving binary variables; see e.g. Clarke and Lapata (2008) for an overview. 7By sibling features we mean features that depend on pairs of sibling arcs (i.e., of the form (i, j) and (i, k)); by grandparent features we mean features that depend on pairs of grandparent arcs (of the form (i, j) and (j, k)). can be incorporated in our model by adding “dynamic” constraints to our ILP. Define: I 1 if hi, ji and hi, ki are consecutive siblings, 0 otherwise, � 1 if j is the first child of i, zfirst child , ij 0 otherwise. Suppose (without loss of generality) that i &lt; j &lt; k ≤ n. We could naively compose the constraints (14) with additional linear constraints t</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global inference for sentence compression an integer linear programming approach. JAIR, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<contexts>
<context position="27029" citStr="Crammer et al. (2006)" startWordPosition="4803" endWordPosition="4807">cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8We used the provided train/test splits except for English, for which we tested on the development partition. For training, sentences longer than 80 words were discarded. For testing, all sentences were kept (the longest one has length 118). 9http://nextens.uvt.nl/—conll/software.html 10http://sourceforge.net/projects/mstparser 11No</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3609" citStr="Culotta and Sorensen, 2004" startWordPosition="525" endWordPosition="528"> tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). 2 Dependency Parsing 2.1 Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1) directed graph D = hV, Ai, with vertices in V = {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Using terminology from graph theory, we say that B ⊆ A</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1505" citStr="Denis and Baldridge, 2007" startWordPosition="206" endWordPosition="209"> a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective dependency parsing. We believe that our formulations can pave the way for efficient expl</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammar.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3557" citStr="Ding and Palmer, 2005" startWordPosition="517" endWordPosition="520">formance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). 2 Dependency Parsing 2.1 Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1) directed graph D = hV, Ai, with vertices in V = {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Us</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammar. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<pages>71--233</pages>
<contexts>
<context position="10120" citStr="Edmonds, 1967" startWordPosition="1684" endWordPosition="1685">isk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f(x, y) as: f(x, y) = &amp;∈y fa(x). (2) Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as 343 shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10457" citStr="Eisner and Satta, 1999" startWordPosition="1732" endWordPosition="1735"> this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as 343 shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 Dependency Parsing as an ILP Our approach will build a graph-based parser without the drawback of a rest</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="10297" citStr="Eisner, 1996" startWordPosition="1710" endWordPosition="1711">eling and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f(x, y) as: f(x, y) = &amp;∈y fa(x). (2) Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as 343 shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) pro</context>
<context position="19394" citStr="Eisner (1996)" startWordPosition="3330" endWordPosition="3331">se features can be handled in our formulation by adding the following O(|A |· |V |) variables and constraints: zsibl ijk ≤ zij, zsibl ijk ≤ zik, zsibl ijk ≥ zij + zik − 1 for all triples hi, j, ki ∈ Rsibl, and zijk ≤ zij, zgrand grand ijk ≤ zjk, zgrand ijk ≥ zij+zjk−1 for all triples hi, j, ki ∈ Rgrand. Let R , A ∪ Rsibl ∪ Rgrand; by redefining z , hzrir∈R and F(x) , [fr(x)]r∈R, we may express our inference problem as in Eq. 12, with O(|A |· |V |) variables and constraints. Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features 6Actually, any logical condition can be encoded with linear constraints involving binary variables; see e.g. Clarke and Lapata (2008) for an overview. 7By sibling features we mean features that depend on pairs of</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudoprojectivity: a polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="4690" citStr="Kahane et al., 1998" startWordPosition="749" endWordPosition="752">= {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) &lt; k &lt; max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z °_ hzaia∈A with each component defined as za = ff(a ∈ y) (here, ff(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal depe</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudoprojectivity: a polynomially parsable non-projective dependency grammar. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>B Taskar</author>
<author>D Klein</author>
<author>M I Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1408" citStr="Lacoste-Julien et al., 2006" startWordPosition="194" endWordPosition="197">rd nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-pr</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jordan. 2006. Word alignment via quadratic assignment. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Magnanti</author>
<author>L A Wolsey</author>
</authors>
<title>Optimal Trees.</title>
<date>1994</date>
<tech>Technical Report 290-94,</tech>
<institution>Massachusetts Institute of Technology, Operations Research Center.</institution>
<contexts>
<context position="15864" citStr="Magnanti and Wolsey (1994)" startWordPosition="2663" endWordPosition="2666">r than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 30. B is connected. Note that conditions 1-2-3 are equivalent to 1-2- 30, in the sense that both define the same set Y(x). However, as we will see, the latter set of conditions is more convenient. Connectedness of graphs can be imposed via flow constraints (by requiring that, for any v ∈ V \ {0}, there is a directed path in B connecting 0 to v). We adapt the single commodity flow formulation for the (undirected) minimum spanning tree problem, due to Magnanti and Wolsey (1994), that requires O(n2) variables and constraints. Under this model, the root node must send one unit of flow to every other node. By making use of extra variables, 0i , h0aiaEA, to denote the flow of commodities through each arc, we are led to the following constraints in addition to Eqs. 4–5 (we denote U , [0, 1], and B , {0, 1} = U ∩ Z): • Root sends flow n: EaEd+(0) Oa = n (6) • Each node consumes one unit of flow: E �a − � Oa=1, j ∈ V \ {0} (7) aEd−(j) aEd+(j) • Each arc indicator lies in the unit interval: za ∈ U, a ∈ A. (9) These constraints project an outer bound of the arborescence poly</context>
<context position="22799" citStr="Magnanti and Wolsey (1994)" startWordPosition="3979" endWordPosition="3982">rtices, by specifying a individual cost for each possible value of valency. The following O(|V |2) constraints encode the agreement between valency indicators and the other variables: Pn−1 k=0 kzval = Pa∈δ+(i) za, i ∈ V (17) ik = 1, i ∈ V , n − 1} n−1 val ≥ 0, i ∈ V, k ∈ {0, ... Pk=0 zik zval ik 3.5 Projectivity Features For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints. In this model, every node k =6 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22): • The root sends one unit of commodity to each node: φka = −1, k ∈ V \ {0} (18) • Any node consumes its own commodity and no other: φka = δkj , j,k ∈ V \ {0} (19) where δk j, ff(j = k) is the Kroneck</context>
</contexts>
<marker>Magnanti, Wolsey, 1994</marker>
<rawString>T. L. Magnanti and L. A. Wolsey. 1994. Optimal Trees. Technical Report 290-94, Massachusetts Institute of Technology, Operations Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>D Das</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28313" citStr="Martins et al. (2008)" startWordPosition="5003" endWordPosition="5006">ally many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves &gt; 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observ</context>
<context position="31265" citStr="Martins et al. (2008)" startWordPosition="5472" endWordPosition="5475">.14) DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16) PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02) SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20) SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08) TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02) ENGLISH 90.85 – – 90.15 91.13 91.12 91.16 91.14 (-0.02) Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features (see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Polyhedral outer approximations with application to natural language parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="11365" citStr="Martins et al., 2009" startWordPosition="1875" endWordPosition="1878">(2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 Dependency Parsing as an ILP Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, by using a relaxation technique we can still make learning tractable. The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009). A linear program (LP) is an optimization problem of the form If the problem is feasible, the optimum is attained at a vertex of the polyhedron that defines the constraint space. If we add the constraint x E Zd, then the above is called an integer linear program (ILP). For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a</context>
<context position="27978" citStr="Martins et al. (2009)" startWordPosition="4950" endWordPosition="4953">r which we tested on the development partition. For training, sentences longer than 80 words were discarded. For testing, all sentences were kept (the longest one has length 118). 9http://nextens.uvt.nl/—conll/software.html 10http://sourceforge.net/projects/mstparser 11Note that, unlike reranking approaches, there are still exponentially many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves &gt; 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in t</context>
<context position="32137" citStr="Martins et al., 2009" startWordPosition="5606" endWordPosition="5609">ence followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p &lt; 0.01) according to Dan Bikel’s randomized method (http://www.cis.upenn.edu/-dbikel/software.html). tures and constraints can lead to further improvements on accuracy. We now turn to a different issue: scalability. In previous work (Martins et al., 2009), we showed that training the model via LP-relaxed inference (as we do here) makes it learn to avoid fractional solutions; as a consequence, ILP solvers will converge faster to the optimum (on average). Yet, it is known from worst case complexity theory that solving a general ILP is NP-hard; hence, these solvers may not scale well with the sentence length. Merely considering the LP-relaxed version of the problem at test time is unsatisfactory, as it may lead to a fractional solution (i.e., a solution whose components indexed by arcs, z = (z-)-EA, are not all integer), which does not correspond</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Polyhedral outer approximations with application to natural language parsing. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F C N Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="8026" citStr="McDonald and Pereira, 2006" startWordPosition="1333" endWordPosition="1336">dataset my em in treatin ch dpedy a i L hhx1, yi), ... , (xm, ym)) E2(X x y)m, we ai to earn a parse, i.e., a functo h : X → Y mary problem in treating each dependency s in Nonlocal information such as arity (o valy that given x ∈ X ouputs a legal dependency parse depedent is that it is not a realistic assumption. and neighbouring dependencies can be crucial to y ∈ Y(x). Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting 2.2 Arc Factorizaton and Lcality er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overv</context>
<context position="10750" citStr="McDonald and Pereira (2006)" startWordPosition="1775" endWordPosition="1778">n solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 Dependency Parsing as an ILP Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, by using a relaxation technique we can still make learning tractable. The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martin</context>
<context position="18195" citStr="McDonald and Pereira (2006)" startWordPosition="3101" endWordPosition="3104">wever, we can now start adding non-local features to build a more powerful model. 3.3 Sibling and Grandparent Features To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on max yEY(x) wTF(x)z sTz 345 a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK. This logical relation can be expressed by the following O(K) agreement constraints:6 za1...aK ≤ zai, i = 1, ..., K x za1 ... aK ≥ �i=1 zai − K + 1. (13) As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji such features in our formulation, we need to add extra variables zsibl , hzrir∈Rsibl and zgrand , hzrir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator variables, i.e., zsibl ijk = zij ∧ zik and zgrand ijk = zij ∧ zjk. Hence, these features ca</context>
<context position="28135" citStr="McDonald and Pereira (2006)" startWordPosition="4972" endWordPosition="4975">longest one has length 118). 9http://nextens.uvt.nl/—conll/software.html 10http://sourceforge.net/projects/mstparser 11Note that, unlike reranking approaches, there are still exponentially many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves &gt; 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-fact</context>
<context position="31188" citStr="McDonald and Pereira (2006)" startWordPosition="5458" endWordPosition="5461">graph-based methods. 348 DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14) DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16) PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02) SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20) SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08) TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02) ENGLISH 90.85 – – 90.15 91.13 91.12 91.16 91.14 (-0.02) Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features (see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored mod</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. T. McDonald and F. C. N. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="6725" citStr="McDonald and Satta (2007)" startWordPosition="1108" endWordPosition="1111">exicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. 2Or “directed spanning tree with designated root r.” 3In this paper, we consider unlabeled dependency parsing, where only the backbone structure (i.e., the arcs without the labels depicted in Fig. 1) is to be predicted. Figure 1: A projective dependency parse (top), and a nonFigure 2: Nonprojective dependency graph. projective dependency parse (bottom) for two English sentences;examples from McDonald and Satta (2007). those that assume each dependency decision denceevectorsmcan befcastdas ansILP. A similar idea was aplied to word alignment by Lacoste-Julien that dependency graphs must be trees. Such mod their parameters facor relative to individual edges et al. (2006), where permutations (rather than arof the graph (Paskin, 2001; McDonald et a., l are comny d o as gefacd 2005a). Edge-factored models have many computah pm cto ativ ndidl dge borescences) were the combinatorial structure bef th gah (Pki 2001 MDld t l ing requiring representation. tional benefits, most notably 2005) Edgfatd dl Letting X denot</context>
<context position="10592" citStr="McDonald and Satta, 2007" startWordPosition="1753" endWordPosition="1756">, as 343 shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 Dependency Parsing as an ILP Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, </context>
<context position="13658" citStr="McDonald and Satta (2007)" startWordPosition="2253" endWordPosition="2256">iscrepancy between the model that is optimized and the one that is actually going to be used. Here, we propose ILP formulations that eliminate the need for cycle constraints; in fact, they require only a polynomial number of constraints. Not only does our model allow expert knowledge to be injected in the form of constraints, it is also capable of learning soft versions of those constraints from data; indeed, it can handle features that are not arc-factored (correlating, for example, siblings and grandparents, modeling valency, or preferring nearly projective parses). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. In this paper, we focus on unlabeled dependency parsing, for clarity of exposition. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. 3.1 The Arborescence Polytope We start by describing our constraint space. Our for</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="8542" citStr="Nivre and McDonald (2008)" startWordPosition="1426" endWordPosition="1429">x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting 2.2 Arc Factorizaton and Lcality er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview. Typcal graph-bsed methods invetigating and xtendng he edge-factored model rojtie prsin lgoiths for bth leaig nd consider liear classifiers of the fom inference of McDonald et al. (2005b) In partic ithin the datadri en ettin gng gh, (x) = argmaxyEY wTf(x, y), (1) tiular p pealto the Matix Tree Theorem for multi-digraphs t he where f(x, y) is a vector of features and w s the tion over all possble depndency graphs for a givn correspondingyweight vector. One wants hw. to g bh pttion io a dge pect haveasmallcex</context>
<context position="28182" citStr="Nivre and McDonald (2008)" startWordPosition="4981" endWordPosition="4984">nl/—conll/software.html 10http://sourceforge.net/projects/mstparser 11Note that, unlike reranking approaches, there are still exponentially many candidate parse trees after pruning. The oracle constrained to pick parents from these lists achieves &gt; 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, f</context>
<context position="31239" citStr="Nivre and McDonald (2008)" startWordPosition="5467" endWordPosition="5470">80 91.06 90.98 91.18 91.04 (-0.14) DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16) PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02) SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20) SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08) TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02) ENGLISH 90.85 – – 90.15 91.13 91.12 91.16 91.14 (-0.02) Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008). The four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features (see §3.2–§3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Perei</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1292" citStr="Punyakanok et al., 2004" startWordPosition="178" endWordPosition="181">able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a p</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="3211" citStr="Richardson and Domingos, 2006" startWordPosition="463" endWordPosition="467">plane algorithm; • LP relaxations permit fast online discriminative training of the constrained model; • Soft constraints may be automatically learned from data. In particular, our formulations handle higher-order arc interactions (like siblings and grandparents), model word valency, and can learn to favor nearly-projective parses. We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). 2 Dependency Parsing 2.1 Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Machine Learning, 62(1):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1338" citStr="Riedel and Clarke, 2006" startWordPosition="185" endWordPosition="188">rcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This </context>
<context position="12396" citStr="Riedel and Clarke (2006)" startWordPosition="2052" endWordPosition="2055">hedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a general ILP is an NP-complete problem. Despite this 4There is also a quadratic algorithm due to Tarjan (1977). 5A matrix is called totally unimodular if the determinants of each square submatrix belong to {0, 1, −1}. fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints at once, they propose a cuttingplane algorithm, where the cycle constraints are only invoked when violated by the current solution. The resulting algorithm is still slow, and an arc-factored model is used as a surrogate during training (i.e., the hard constraints are only </context>
<context position="14071" citStr="Riedel and Clarke (2006)" startWordPosition="2314" endWordPosition="2317">t can handle features that are not arc-factored (correlating, for example, siblings and grandparents, modeling valency, or preferring nearly projective parses). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. In this paper, we focus on unlabeled dependency parsing, for clarity of exposition. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. 3.1 The Arborescence Polytope We start by describing our constraint space. Our formulations rely on a concise polyhedral representation of the set of candidate dependency parse trees, as sketched in §2.1. This will be accomplished by drawing an analogy with a network flow problem. Let D = (V, A) be the complete directed graph (3) s.t. Ax � b. min.ERd cTx 344 • Flow is zero on disabled arcs: Oa ≤ nza, a ∈ A (8) associated with a sentence x ∈ X, as stated in §2. A subgraph y = hV, Bi is a leg</context>
<context position="15343" citStr="Riedel and Clarke, 2006" startWordPosition="2569" endWordPosition="2572">if the following conditions are met: 1. Each vertex in V \ {0} must have exactly one incoming arc in B, 2. 0 has no incoming arcs in B, 3. B does not contain cycles. For each vertex v ∈ V , let S−(v) , {hi, ji ∈ A |j = v} denote its set of incoming arcs, and S+(v) , {hi, ji ∈ A |i = v} denote its set of outgoing arcs. The two first conditions can be easily expressed by linear constraints on the incidence vector z: EaEd−(j) za = 1, j ∈ V \ {0} (4) EaEd−(0) za = 0 (5) Condition 3 is somewhat harder to express. Rather than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 30. B is connected. Note that conditions 1-2-3 are equivalent to 1-2- 30, in the sense that both define the same set Y(x). However, as we will see, the latter set of conditions is more convenient. Connectedness of graphs can be imposed via flow constraints (by requiring that, for any v ∈ V \ {0}, there is a directed path in B connecting 0 to v). We adapt the single commodity flow formulation for the (undirected) minimum spanning tree problem, due to Magnanti and Wolsey (1994), that requires O(n2) variables and constraints. Under this model, the root nod</context>
<context position="28425" citStr="Riedel and Clarke (2006)" startWordPosition="5022" endWordPosition="5025">es &gt; 98% in every case. 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observe that, for some languages, the valency features do not seem to help. Merely modeling the number of dependents o</context>
<context position="29944" citStr="Riedel and Clarke (2006)" startWordPosition="5264" endWordPosition="5267"> parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers). Unlike the ensembles, it directly handles non-local output features by optimizing a single global objective. Perhaps more importantly, it makes it possible to exploit expert knowledge through the form of hard global constraints. Although not pursued here, the same kind of constraints employed by Riedel and Clarke (2006) can straightforwardly fit into our model, after extending it to perform labeled dependency parsing. We believe that a careful design of fea13Unlike our model, the hybrid models used here as baselines make use of the dependency labels at training time; indeed, the transition-based parser is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features. Our model ignores this information at training time; therefore, this comparison is slightly unfair to us. 14See also Zhang and Clark (2008) for a different approach that combines tran</context>
<context position="33860" citStr="Riedel and Clarke (2006)" startWordPosition="5898" endWordPosition="5901">huLiu-Edmonds algorithm can do this in polynomial time. The overall parsing runtime becomes polynomial with respect to the length of the sentence. The last column of Table 1 compares the accuracy of this approximate method with the exact one. We observe that there is not a substantial drop in accuracy; on the other hand, we observed a considerable speed-up with respect to exact inference, particularly for long sentences. The average runtime (across all languages) is 0.632 seconds per sentence, which is in line with existing higher-order parsers and is much faster than the runtimes reported by Riedel and Clarke (2006). 5 Conclusions We presented new dependency parsers based on concise ILP formulations. We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints. These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a max-margin framework is made effective by the means of a LPrelaxation. Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and a</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Rockafellar</author>
</authors>
<title>Convex Analysis.</title>
<date>1970</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="5527" citStr="Rockafellar, 1970" startWordPosition="904" endWordPosition="905">tains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z °_ hzaia∈A with each component defined as za = ff(a ∈ y) (here, ff(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x). Each vertex of Z(x) can be identified with a dependency tree in Y(x). The Minkowski-Weyl theorem (Rockafellar, 1970) ensures that Z(x) has a representation of the form Z(x) = {z ∈ R|A ||Az ≤ b}, for some p-by-|A| matrix A and some vector b in Rp. However, it is not easy to obtain a compact representation (where p grows polynomially with the number of words n). In §3, we will provide a compact representation of an outer polytope ¯Z(x) ⊇ Z(x) whose integer vertices correspond to dependency trees. Hence, the problem of finding the dependency tree that maximizes some linear function of the inci1The general case where A C_ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs fr</context>
</contexts>
<marker>Rockafellar, 1970</marker>
<rawString>R. T. Rockafellar. 1970. Convex Analysis. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W T Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1266" citStr="Roth and Yih, 2005" startWordPosition="174" endWordPosition="177">cular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. 1 Introduction Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are availa</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. T. Yih. 2005. Integer linear programming inference for conditional random fields. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schrijver</author>
</authors>
<title>Combinatorial Optimization: Polyhedra and Efficiency,</title>
<date>2003</date>
<volume>24</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="11921" citStr="Schrijver, 2003" startWordPosition="1974" endWordPosition="1975">earning problem was studied elsewhere (Martins et al., 2009). A linear program (LP) is an optimization problem of the form If the problem is feasible, the optimum is attained at a vertex of the polyhedron that defines the constraint space. If we add the constraint x E Zd, then the above is called an integer linear program (ILP). For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a general ILP is an NP-complete problem. Despite this 4There is also a quadratic algorithm due to Tarjan (1977). 5A matrix is called totally unimodular if the determinants of each square submatrix belong to {0, 1, −1}. fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated</context>
</contexts>
<marker>Schrijver, 2003</marker>
<rawString>A. Schrijver. 2003. Combinatorial Optimization: Polyhedra and Efficiency, volume 24 of Algorithms and Combinatorics. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10893" citStr="Smith and Eisner (2008)" startWordPosition="1797" endWordPosition="1800"> programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 Dependency Parsing as an ILP Our approach will build a graph-based parser without the drawback of a restriction to local features. By formulating inference as an ILP, nonlocal features can be easily accommodated in our model; furthermore, by using a relaxation technique we can still make learning tractable. The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009). A linear program (LP) is an optimization problem of the form If the problem is feasible, the optimum is attained at a vertex o</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The conll-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>Proc. of CoNLL.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="12075" citStr="Tarjan (1977)" startWordPosition="2001" endWordPosition="2002">timum is attained at a vertex of the polyhedron that defines the constraint space. If we add the constraint x E Zd, then the above is called an integer linear program (ILP). For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003). Of course, this need not happen: solving a general ILP is an NP-complete problem. Despite this 4There is also a quadratic algorithm due to Tarjan (1977). 5A matrix is called totally unimodular if the determinants of each square submatrix belong to {0, 1, −1}. fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Sinc</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R. E. Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="3652" citStr="Wang et al., 2007" startWordPosition="532" endWordPosition="535">esent are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). 2 Dependency Parsing 2.1 Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1) directed graph D = hV, Ai, with vertices in V = {0, ... , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2. Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed grap</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for QA. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="30500" citStr="Zhang and Clark (2008)" startWordPosition="5354" endWordPosition="5357"> the same kind of constraints employed by Riedel and Clarke (2006) can straightforwardly fit into our model, after extending it to perform labeled dependency parsing. We believe that a careful design of fea13Unlike our model, the hybrid models used here as baselines make use of the dependency labels at training time; indeed, the transition-based parser is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features. Our model ignores this information at training time; therefore, this comparison is slightly unfair to us. 14See also Zhang and Clark (2008) for a different approach that combines transition-based and graph-based methods. 348 DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14) DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16) PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02) SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20) SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08) TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02) ENGLISH 90.85 – – 90.15 91.13 91.12 91.16 91.14 (-0.02) Table 1: Results for nonprojective dependency parsing (unlabeled attachmen</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>