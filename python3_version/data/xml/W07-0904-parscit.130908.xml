<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.9967365">
Concept Disambiguation for Improved Subject Access
Using Multiple Knowledge Sources
</title>
<author confidence="0.998271">
Tandeep Sidhu, Judith Klavans, and Jimmy Lin
</author>
<affiliation confidence="0.997637">
College of Information Studies
University of Maryland
</affiliation>
<address confidence="0.753553">
College Park, M D 20742
</address>
<email confidence="0.991746">
tsidhu@umiacs.umd.edu, Oklavans, jimmylin}@umd.edu
</email>
<sectionHeader confidence="0.993764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920391304348">
We address the problem of mining text for
relevant image metadata. Our work is situ-
ated in the art and architecture domain,
where highly specialized technical vocabu-
lary presents challenges for NLP tech-
niques. To extract high quality metadata,
the problem of word sense disambiguation
must be addressed in order to avoid leading
the searcher to the wrong image as a result
of ambiguous and thus faulty meta-
data. In this paper, we present a disam-
biguation algorithm that attempts to select
the correct sense of nouns in textual de-
scriptions of art objects, with respect to a
rich domain-specific thesaurus, the Art and
Architecture Thesaurus (AAT). We per-
formed a series of intrinsic evaluations us-
ing a data set of 600 subject terms ex-
tracted from an online National Gallery of
Art (NGA) collection of images and text.
Our results showed that the use of external
knowledge sources shows an improvement
over a baseline.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9960485">
We describe an algorithm that takes noun phrases
and assigns a sense to the head noun or phrase,
given a large domain-specific thesaurus, the Art
and Architecture Thesaurus1 (published by the
Getty Research Institute). This research is part of
the Computational Linguistics for Metadata
</bodyText>
<footnote confidence="0.7630175">
1http://www.getty.edu/research/conducting_research/vocabul
aries/aat/
</footnote>
<bodyText confidence="0.998936162162162">
Building (CLiMB) project (Klavans 2006, Kla-
vans in preparation), which aims to improve im-
age access by automatically extracting metadata
from text associated with images. We present
here a component of an overall architecture that
automatically mines scholarly text for metadata
terms. In order to filter and associate a term with
a related concept, ambiguous terms must be clari-
fied. The disambiguation of terms is a basic chal-
lenge in computational linguistics (Ide and Vero-
nis 1990, Agirre and Edmonds 2006).
As more non-specialists in digital libraries
search for images, the need for subject term ac-
cess has increased. Subject terms enrich catalog
records with valuable broad-reaching metadata
and help improve image access (Layne 1994).
Image seekers will receive more relevant results
if image records contain terms that reflect con-
ceptual, semantic, and ontological relationships.
Furthermore, subject terms associated with hier-
archical and faceted thesaural senses promise to
further improve precision in image access. Such
terms map to standardized thesaurus records that
include the term&apos;s preferred, variant, and related
names, including both broader and specific con-
cepts, and other related concepts. This informa-
tion can then be filtered, linked, and subsequently
tested for usefulness in performing richer image
access. As with other research on disambigua-
tion, our hypothesis is that accurate assignment of
senses to metadata index terms will results in
higher precision for searchers. This hypothesis
will be fully tested as we incorporate the disam-
biguation module in our end-to-end CLiMB
Toolkit, and as we perform user studies.
Finding subject terms and mapping them to a
thesaurus is a time-intensive task for catalogers
</bodyText>
<page confidence="0.983453">
25
</page>
<note confidence="0.91535175">
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25–32,
Prague, 28 June 2007. c�2007 Association for Computational Linguistics
(Rasmussen 1997, Ferguson and Intner 1998).
Doing so typically involves reading image-related
</note>
<bodyText confidence="0.97992625">
text or other sources to find subject terms. Even
so, the lack of standard vocabulary in extensive
subject indexing means that the enriched number
of subject terms could be inadvertently offset by
the vocabulary naming problem (Baca 2002).
This paper reports on our results using the
subject terms in the AAT; the CLiMB project is
also using the Thesaurus of Geographic Names
(TGN) and the Union List of Artist Names
(ULAN). Since the focus of this paper is on dis-
ambiguation of common nouns rather than proper
nouns, the AAT is our primary resource.
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="introduction">
2. Resources
</sectionHeader>
<subsectionHeader confidence="0.999375">
2.1 Art and Architecture Thesaurus (AAT)
</subsectionHeader>
<bodyText confidence="0.999963411764706">
The AAT is a widely-used multi-faceted thesau-
rus of terms for the cataloging and indexing of
art, architecture, artifactual, and archival materi-
als. Since the AAT offers a controlled vocabulary
for recording and retrieval of data in object, bib-
liographic, and visual databases, it is of interest to
a wide community.
In the AAT, each concept is described
through a record which has a unique ID, preferred
name, record description, variant names, broader,
narrower, and related terms. In total, AAT has
31,000 such records. For the purpose of this arti-
cle, a record can be viewed as synonymous with
sense. Within the AAT, there are 1,400 homo-
nyms, i.e., records with same preferred name.
For example, the term wings has five senses in
the AAT (see Figure 1 below).
</bodyText>
<subsectionHeader confidence="0.608847">
Wings (5 senses):
</subsectionHeader>
<listItem confidence="0.997351636363636">
• Sense#1: Used for accessories that project outward
from the shoulder of a garment and are made of cloth
or metal.
• Sense#2: Lateral parts or appendages of a work of
art, such as those found on a triptych.
• Sense#3: The areas offstage and to the side of the
acting area.
• Sense#4: The two forward extensions to the sides of
the back on an easy chair.
• Sense#5: Subsidiary parts of buildings extending out
from the main portion.
</listItem>
<figureCaption confidence="0.999024">
Figure 1: Selection of AAT records for term &amp;quot;wings&amp;quot;
</figureCaption>
<bodyText confidence="0.956004666666667">
Table 1 shows the breakdown of the AAT vo-
cabulary by number of senses with a sample lexi-
cal item for each frequency.
</bodyText>
<figure confidence="0.866225133333333">
# of # of Example
Senses Homonyms
2 1097 bells
3 215 painting
4 50 alabaster
5 39 wings
6 9 boards
7 5 amber
8 2 emerald
9 1 plum
10 1 emerald green
11 1 magenta
12 1 ocher
13 1 carmine
14 2 slate
</figure>
<tableCaption confidence="0.996323">
Table 1: Scope of the disambiguation problem in AAT
</tableCaption>
<bodyText confidence="0.990577533333333">
Note that there are potentially three tasks that
could be addressed with our algorithm: (i) map-
ping a term to the correct sense in the AAT, (ii)
selecting amongst closely related terms in the
AAT, and (iii) mapping synonyms onto a single
AAT entry. In this paper, our primary focus is on
task (i); we handle task (ii) with a simple ranking
approach; we do not address task (iii).
Table 1 shows that multiple senses per term
makes mapping subject terms to AAT very chal-
lenging. Manual disambiguation would be slow,
tedious, and unrealistic. Thus we explore auto-
matic methods since, in order to identify the cor-
rect sense of a term in running text, each of these
senses needs to be viewed in context.
</bodyText>
<subsectionHeader confidence="0.998551">
2.2 The Test Collection
</subsectionHeader>
<bodyText confidence="0.99993">
The data set of terms that we use for evaluation
comes from the National Gallery of Art (NGA)
online archive2. This collection covers paintings,
sculpture, decorative arts, and works from the
Middle Ages to the present. We randomly se-
lected 20 images with corresponding text from
this collection and extracted noun phrases to form
the data set. The data set was divided into two
categories: the training set and the test set. The
training set consisted of 326 terms and was used
</bodyText>
<footnote confidence="0.889212">
2 http://www.nga.gov/home.htm
</footnote>
<page confidence="0.994555">
26
</page>
<bodyText confidence="0.9999061875">
With more than fifty individual scenes, the al-
tarpiece was about fourteen feet wide.
to develop the algorithm. The test set consisted
of 275 terms and was used to evaluate.
Following standard procedure in word sense
disambiguation tasks (Palmer et al. 2006),
groundtruth for the data set was created manually
by two labelers (referred to as Labeler 1 and La-
beler 2 in Section 4 below). These labelers were
part of the larger CLiMB project but they were
not involved in the development of the disam-
biguation algorithm. The process of creating the
groundtruth involved picking the correct AAT
record for each of the terms in the data set.
Terms not appearing in the AAT (as determined
by the labelers) were given an AAT record value
of zero. Each labeler worked independently on
this task and had access to the online version of
the AAT and the text where each term appeared.
Interannotator agreement for the task was encour-
agingly high, at 85% providing a notional upper
bound for automatic system performance (Gale et
al. 1992).
Not all terms in this dataset required disam-
biguation; 128 terms (out of 326) under the train-
ing set and 96 terms (out of 275) under the test
set required disambiguation, since they matched
more than one AAT record. The dataset we se-
lected was adequate to test our different ap-
proaches and to refine our techniques. We intend
to run over more data as we collect and annotate
more resources for evaluation.
</bodyText>
<subsectionHeader confidence="0.382835">
2.&apos; SenseRelate AllWords&apos; and WordNet&apos;
</subsectionHeader>
<bodyText confidence="0.995149833333333">
SenseRelate AllWords (Banerjee and Pederson
2003, Patwardhan et al. 2003) is a Perl program
that our algorithm employs to perform basic dis-
ambiguation of words. We have adapted Sen-
seRelate for the purpose of disambiguating AAT
senses.
Given a sentence, SenseRelate AllWords dis-
ambiguates all the words in that sentence. It uses
word sense definitions from WordNet (in this
case WordNet 2.1), a large lexical database of
English nouns, verbs, adjectives, and adverbs. As
an example, consider the text below:
</bodyText>
<footnote confidence="0.958437">
3 http://sourceforge.net/projects/senserelate
4 http://wordnet.princeton.edu/
</footnote>
<bodyText confidence="0.991102607142857">
The SenseRelate result is:
With more#a#2 than fifty#n#1 individual#n#1
scene#n#10 the altarpiece#n#1 be#v#1 about#r#1
fourteen#n#1 foot#n#2 wide#a#1
In the above example, more#a#2 means SenseRe-
late labeled more as an adjective and mapped it to
second meaning of more (found in WordNet).
fifty#n#1 means SenseRelate labeled fifty as a
noun and mapped it to first meaning of fifty
(found in WordNet). Note, that fifty#n#1 maps to
a sense in WordNet, whereas in our algorithm it
needs to map to an AAT sense. In Section 3, we
show how we translate a WordNet sense to an
AAT sense for use in our algorithm.
To perform disambiguation, SenseRelate re-
quires that certain parameters be set: (1) the
number of words around the target word (also
known as the context window), and (2) the simi-
larity measure. We used a value of 20 for the
context window, which means that SenseRelate
will use 10 words to the left and 10 words to the
right of the target word to determine the correct
sense. We used lesk as the similarity measure in
our algorithm which is based on Lesk (1986).
This decision was based on several experiments
we did with various context window sizes and
various similarity measures on a data set of 60
terms.
</bodyText>
<page confidence="0.995923">
27
</page>
<sectionHeader confidence="0.994702" genericHeader="method">
3. Methodology
</sectionHeader>
<subsectionHeader confidence="0.993323">
3.1 Disambiguation Algorith
</subsectionHeader>
<figureCaption confidence="0.992777">
Figure 2: Disambiguation Algorithm
</figureCaption>
<bodyText confidence="0.988910833333333">
Figure 2 above shows that first we identify the
noun phrases from the input document. Then we
disambiguate each noun phrase independently by
first looking it up in the AAT. If a record is
found, we move on to the next step; otherwise we
look up the head noun (as the noun phrase) in the
AAT.
Second, we filter out any AAT records where
the noun phrase (or the head noun) is used as an
adjective (for a term like painting this would be
painting techniques, painting knives, painting
equipment, etc). Third, if zero records are found
in the AAT, we label the term as &amp;quot;not found in
AAT.&amp;quot; If only one matching record is found, we
label the term with the ID of this record. Fourth,
if more than one record is found, we use the dis-
ambiguation techniques outlined in the next sec-
tion to find the correct record.
</bodyText>
<subsectionHeader confidence="0.962147">
3.2 Techniques for Disambiguation
</subsectionHeader>
<bodyText confidence="0.999679342105263">
For each of the terms, the following techniques
were applied in the order they are given in this
section. If a technique failed to disambiguate a
term, we applied the next technique. If none of
these techniques was able to disambiguate, we
selected the first AAT record as the correct re-
cord. Findings for each technique are provided in
the Results section below.
First, we used all modifiers that are in the
noun phrase to find the correct AAT record. We
searched for the modifiers in the record descrip-
tion, variant names, and the parent hierarchy
names of all the matching AAT senses. If this
technique narrowed down the option set to one
record, then we found our correct record. For
example, consider the term ceiling coffers. For
this term we found two records: coffers (coffered
ceiling components) and coffers (chests). The
first record has the modifier ceiling in its record
description, so we were able to determine that
this was the correct record.
Second, we used SenseRelate AllWords and
WordNet. This gave us the WordNet sense of our
noun phrase (or its head noun). Using that sense
definition from WordNet, we next examined
which of the AAT senses best matches with the
WordNet sense definition. For this, we used the
word overlapping technique where we awarded a
score of N to an AAT record where N words
overlap with the sense that SenseRelate picked.
The AAT record with the highest score was se-
lected as the correct record. If none of the AAT
records received any positive score (above a cer-
tain threshold), then it was decided that this tech-
nique could not find the one correct match.
As an example, consider finding the correct
sense for the single word noun bells using Sen-
seRelate:
</bodyText>
<tableCaption confidence="0.941597333333333">
1. Given the input sentence:
&amp;quot;... city officials, and citizens were followed by
women and children ringing bells for joy.&amp;quot;
2. Search for AAT records. There are two records
for the bells in AAT:
a. bells: &amp;quot;Flared or bulbous terminals found on
many open-ended aerophone tubes&amp;quot;.
b. bells: &amp;quot;Percussion vessels consisting of a hollow
object, usually of metal but in some cultures of
hard clay, wood, or glass, which when struck emits
a sound by the vibration of most of its mass;...&amp;quot;
3. Submit the input sentence to SenseRelate, which
provides a best guess for the corresponding
WordNet senses for each word.
4. Get SenseRelate output, which indicates that the
WordNet definition for bells is WordNet-Sensel,
i.e., &amp;quot;a hollow device made of metal that makes a
ringing sound when struck&amp;quot;
</tableCaption>
<page confidence="0.992527">
28
</page>
<note confidence="0.83975975">
SenseRelate output.
city#n#1 official#n#1 and citizen#n#1 be#v#1
follow#v#20 by#r#1 woman#n#1 and child#n#1
ringing#a#1 bell#n#1 for joy#n#1
</note>
<tableCaption confidence="0.900102125">
5. Find the correct AAT match using word overlap of
the WordNet definition and the two AAT defini-
tions for bells:
WordNet: &amp;quot;a hollow device made of metal that
makes a ringing sound when struck&amp;quot;
compared with:
AAT: &amp;quot;Flared or bulbous terminals found on many
open-ended aerophonetubes&amp;quot;
and compared with:
AAT: &amp;quot;Percussion vessels consisting of a hollow
object, usually of metal but in some cultures of
hard clay, wood, or glass, which when struck
emits a sound by the vibration of most of its
mass;...&amp;quot;
6. The second AAT sense is the correct sense accord-
ing to the word overlap (see Table 2 below):
</tableCaption>
<table confidence="0.997986">
Comparison Score Word Overlap
AAT — Definition 1 and 0 None
WordNet Sense1
AAT — Definition 2 and 4 hollow, metal,
WordNet Sense1 sound, struck
</table>
<tableCaption confidence="0.999541">
Table 2: Word Overlap to Select AAT Definition
</tableCaption>
<bodyText confidence="0.999890652173913">
Notice that we only used the AAT record descrip-
tion for performing the word overlap. We ex-
perimented by including other information pre-
sent in the AAT record (like variant names, par-
ent AAT record names) also, but simply using the
record description yielded the best results.
Third, we used AAT record names (preferred
and variant) to find the one correct match. If one
of the record names matched better than the other
record names to the noun phrase name, that re-
cord was deemed to be the correct record. For
example, the term altar more appropriately
matches altars (religious building fixtures) than
altarpieces (religious visual works). Another
example is children, which better matches chil-
dren (youth) than offspring (people by family re-
lationship).
Fourth, if none of the above techniques
succeeded in selecting one record, we used the
most common sense definition for a term (taken
from WordNet) in conjunction with the AAT re-
sults and word overlapping mentioned above to
find the one correct record.
</bodyText>
<sectionHeader confidence="0.995383" genericHeader="method">
4. Results and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.946274">
4.1 Methodologies
</subsectionHeader>
<bodyText confidence="0.999995666666667">
We used three different evaluation methods to
assess the performance of our algorithm. The
first evaluation method computes whether our
algorithm picked the correct AAT record (i.e., the
AAT sense picked is in agreement with the
groundtruth). The second method computes
whether the correct record is among the top three
records picked by our algorithm. In Table 3 be-
low, this is referred to as Top 3. The third evalua-
tion method computes whether the correct record
is in top five records picked by our algorithm,
Tops. The last two evaluations helped us deter-
mine the usability of our algorithm in situations
where it does not pick the correct record but it
still narrows down to top three or top five results.
We ranked the AAT records according to
their preferred name for the baseline, given the
absence of any other disambiguation algorithm.
Thus, AAT records that exactly matched the term
in question appear on top, followed by records
that partially matched the term. For example, for
term feet, the top three records were feet (terminal
elements of objects), French feet (bracket feet),
and Spanish feet (furniture components). For the
noun wings, the top three records were wings
(shoulder accessories), wings (visual works com-
ponents), and wings (backstage spaces).
</bodyText>
<subsectionHeader confidence="0.99458">
4.2 Overall Results
</subsectionHeader>
<bodyText confidence="0.999786571428571">
In this section, we present evaluation results for
all the terms. In the next section, we present re-
sults for only those terms that required disam-
biguation.
Overall results for the training set (326 terms)
are shown in Table 3. This table shows that over-
all accuracy of our algorithm is 76% and 68% for
Labeler 1 and Labeler 2, respectively. The base-
line accuracy is 69% for Labeler 1 and 62% for
Labeler 2. The other two evaluations show much
better results. The Top 3 and Top5 evaluations
have accuracy of 84% and 88% for Labeler 1 and
accuracy of 78% and 79% for Labeler 2. This
argues for bringing in additional techniques to
</bodyText>
<page confidence="0.997925">
29
</page>
<table confidence="0.945464">
enhance the SenseRelate approach in order to
select from Top3 or Tops.
Evaluation Labeler 1 Labeler 2
Algorithm Accuracy 76% 68%
Baseline Accuracy 69% 62%
Top3 84% 78%
Top5 88% 79%
</table>
<tableCaption confidence="0.998829">
Table 3: Results for Training Set (n=326 terms)
</tableCaption>
<bodyText confidence="0.9996612">
In contrast to Table 3 for the training set, Table 4
shows results for the test set. Labeler 1 shows an
accuracy of 74% on the algorithm and 72% on the
baseline; Labeler 2 has an accuracy of 73% on
the algorithm and 69% on the baseline.
</bodyText>
<table confidence="0.9998116">
Evaluation Labeler 1 Labeler 2
Algorithm Accuracy 74% 73%
Baseline Accuracy 72% 69%
Top3 79% 79%
Top5 81% 80%
</table>
<tableCaption confidence="0.998036">
Table 4: Results for Test Set (n=275 terms)
</tableCaption>
<subsectionHeader confidence="0.673731">
4.3 Results for Ambiguous Terms
</subsectionHeader>
<bodyText confidence="0.995210636363636">
This section shows the results for the terms from
the training set and the test set that required dis-
ambiguation. Table 5 below shows that our algo-
rithm&apos;s accuracy for Labeler 1 is 55% compared
to the baseline accuracy of 35%. For Labeler 2,
the algorithm accuracy is 48% compared to base-
line accuracy of 32%. This is significantly less
than the overall accuracy of our algorithm. Top3
and Top5 evaluations have accuracy of 71% and
82% for Labeler 1 and 71% and 75% for Labeler
2.
</bodyText>
<table confidence="0.9998604">
Evaluation Labeler 1 Labeler 2
Algorithm Accuracy 55% 48%
Baseline Accuracy 35% 32%
Top3 71% 71%
Top5 82% 75%
</table>
<tableCaption confidence="0.998929">
Table 5: Ambiguous Terms for Training (n=128 terms)
</tableCaption>
<bodyText confidence="0.999053">
Similar results can be seen for the test set (96
terms) in Table 6 below. Labeler 1 shows an ac-
curacy of 50% on the algorithm and 42% on the
baseline; Labeler 2 has an accuracy of 53% on
the algorithm and 39% on the baseline.
</bodyText>
<table confidence="0.9998206">
Evaluation Labeler 1 Labeler 2
Algorithm Accuracy 50% 53%
Baseline Accuracy 42% 39%
Top3 63% 68%
Top5 68% 71%
</table>
<tableCaption confidence="0.9672005">
Table 6: Results for Ambiguous Terms
under the Test Set (n=96 terms)
</tableCaption>
<subsectionHeader confidence="0.995162">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.997420857142857">
Table 7 shows that SenseRelate is used for most
of the AAT mappings, and provides a breakdown
based upon the disambiguation technique used.
Row One in Table 7 shows how few terms were
disambiguated using the lookup modifier tech-
nique, just 1 in the training set and 3 in the test
set.
</bodyText>
<table confidence="0.999162555555556">
Row Technique Training Test Set
Set(n=128) (n=96)
One Lookup 1 3
Modifier
Two SenseRelate 108 63
Three Best Record 14 12
Match
Four Most Common 5 18
Sense
</table>
<tableCaption confidence="0.9266845">
Table 7: Breakdown of AAT mappings
by Disambiguation Technique
</tableCaption>
<bodyText confidence="0.869156714285714">
Rows Two and Three show that most of the terms
were disambiguated using the SenseRelate tech-
nique followed by the Best Record Match tech-
nique. The Most Common Sense technique (Row
Four) accounted for the rest of the labelings.
Table 8 gives insight into the errors of our algo-
rithm for the training set terms:
</bodyText>
<table confidence="0.998856428571429">
Technique Reason for Error Error
Count
SenseRelate SenseRelate picked wrong 16
WordNet sense
WordNet does not have the 8
sense
Definitions did not overlap 11
Otherreasons 10
Best Record 10
Match
Lookup 0
Modifier
Most Com- 3
mon Sense
</table>
<tableCaption confidence="0.922835">
Table 8: Breakdown of the errors in our algorith
under training set (58 total errors)
Table 8 shows the following:
</tableCaption>
<bodyText confidence="0.5857768">
(1) Out of the total of 58 errors, 16 errors were
caused because SenseRelate picked the wrong
WordNet sense.
(2) 8 errors were caused because WordNet did
not contain the sense of the word in which it was
</bodyText>
<page confidence="0.994573">
30
</page>
<bodyText confidence="0.992285277777778">
being used. For example, consider the term work-
shop. WordNet has two definitions of workshop:
i. &amp;quot;small workplace where handcrafts or manufac-
turing are done&amp;quot; and
ii. &amp;quot;a brief intensive course for a small group; em-
phasizes problem solving&amp;quot;
but AAT has an additional definition that was
referred by term workshop in the NGA text:
&amp;quot;In the context of visual and decorative arts, refers
to groups of artists or craftsmen collaborating to
produce works, usually under a master&apos;s name&amp;quot;
(3) 11 errors occurred because the AAT record
definition and the WordNet sense definition did
not overlap. Consider the term figures in the sen-
tence, &amp;quot;As with The Holy Family, the style of the
figures offers no clear distinguishing characteris-
tic.&amp;quot; Then examine the AAT and WordNet sense
definitions below for figures:
</bodyText>
<sectionHeader confidence="0.6299775" genericHeader="method">
AAT sense: &amp;quot;Representations of humans or ani-
mals&amp;quot;
</sectionHeader>
<bodyText confidence="0.934114625">
WordNet sense: &amp;quot;a model of a bodily form (espe-
cially of a person)&amp;quot;
These definitions do not have any words in com-
mon, but they discuss the same concept.
(4) 10 errors occurred in the Best Record Match
technique, 0 errors occurred under the Lookup
Modifier Technique, and 3 errors occurred under
the Most Common Sense technique.
</bodyText>
<sectionHeader confidence="0.99819" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.997932372881356">
We have shown that it is possible to create an
automated program to perform word sense dis-
ambiguation in a field with specialized vocabu-
lary. Such an application could have great poten-
tial in rapid development of metadata for digital
collections. Still, much work must be done in
order to integrate our disambiguation program
into the CLiMB Toolkit, including the following:
(1) Our algorithm&apos;s disambiguation accuracy is
between 48-55% (Table 5 and Table 6), and so
there is room for improvement in the algorithm.
Currently we depend on an external program
(SenseRelate) to perform much of the disam-
biguation (Table 7). Furthermore, SenseRelate
maps terms to WordNet and we then map the
WordNet sense to an AAT sense. This extra step
is overhead, and it causes errors in our algorithm.
We can either explore the option of re-
implementing concepts behind SenseRelate to
directly map terms to the AAT, or we may need
to find additional approaches to employ hybrid
techniques (including machine learning) for dis-
ambiguation. At the same time, we may benefit
from the fact that WordNet, as a general resource,
is domain independent and thus offers wider cov-
erage. We will need to explore the trade-off in
precision between different configurations using
these different resources.
(2) We need more and better groundtruth. Our
current data set of noun phrases includes term
like favor, kind, and certain aspects. These terms
are unlikely to be used as meaningful subject
terms by a cataloger and will never be mapped to
AAT. Thus, we need to develop reliable heuris-
tics to determine which noun phrases are poten-
tially high value subject index terms. A simple
frequency count does not achieve this purpose.
Currently we are evaluating based on ground-
truth that our project members created. Instead,
we would like to extend the study to a wider set
of image catalogers as labelers, since they will be
the primary users of the CLiMB tool. Image
catalogers have experience in finding subject
terms and mapping subject terms to the AAT.
They can also help determine which terms are
high quality subject terms.
In contrast to working with the highly experi-
enced image cataloger, we also want to extend the
study to include various groups with different
user needs. For example, journalists have ongo-
ing needs for images, and they tend to search by
subject. Using participants like these for markup
and evaluation promises to provide comparative
results, ones which will enable us to effectively
reach a broad audience.
We also would like to test our algorithm on
more collections. This will help us ascertain
what kind of improvements or additions would
make CLiMB a more general tool.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="acknowledgments">
6. Acknowledgements
</sectionHeader>
<bodyText confidence="0.993291">
We thank Rachel Wadsworth and Carolyn Shef-
field. We also acknowledge Philip Resnik for
valuable discussion.
</bodyText>
<page confidence="0.999834">
31
</page>
<sectionHeader confidence="0.926522" genericHeader="references">
7. References
</sectionHeader>
<reference confidence="0.9999154">
Baca, Murtha, ed. 2002. Introduction to art image
access: issues, tools, standards, strategies. Getty
Research Institute.
Baner ee, S., and T. Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relat-
edness. In Proceedings of the Eighteenth Inter-
national Joint Conference on ArtificialIntelli-
gence, 805-810.
Ferguson, Bobby and Sheila Intner. 1998. Sub ect
Analysis: Blitz Cataloging Workbook. West-
port, CT:Libraries Unlimited Inc.
Gale, W. A., K. W. Church, and D. Yarowsky.
1992. Using bilingual materials to develop
word sense disambiguation methods. In Pro-
ceedings of the Fourth International Confer-
ence on Theoretical and Methodological Issues
in Machine Translation, 101-112, Montreal,
Canada.
Ide, Nancy M. and Jean Veronis. 1990. Mapping
Dictionaries: A Spreading Activation Ap-
proach. In Proceedings of the 6th Annual Con-
ference of the UW Centre for the New .ED and
Text Research, 52-64 Waterloo, Ontario.
Lesk, Michael. 1986. Automatic Sense Disam-
biguation Using Machine Readable Dictionar-
ies: How to Tell a Pine Cone from an Ice
Cream Cone. In Proceedings ofACM SIGD.C
Conference, 24-26, Toronto, Canada.
Klavans, Judith L. 2006. Computational Linguis-
tics for Metadata Building (CLiMB). In Pro-
cedings of the .ntoImage Workshop, G. Gref-
fenstette, ed. Language Resources and Evalua-
tion Conference (LREC), Genova, Italy.
Klavans, Judith L. (in preparation). Using Com-
putational Linguistic Techniques and Thesauri
for Enhancing Metadata Records in Image
Search: The CLiMB Pro ect.
Layne, Sara Shatford. 1994. Some issues in the
indexing of images. Journal of the American
Society for Information Science, 583-588.
Palmer, Martha, Hwee Tou Ng, &amp; Hoa Trang
Dang. 2006. Evaluation of WSD Systems.
Word Sense Disambiguation: Algorithms and
Applications. Eneko Agirre and Philip Ed-
monds, ed. 75-106. Dordrecht, The Nether-
lands:Springer.
Patwardhan, S., S. Baner ee, S. and T. Pedersen.
2003. Using measures of semantic relatedness
for word sense disambiguation. Proceedings of
the Fourth International Conference on Intelli-
gent Text Processing and Computational Lin-
guistics, 241-257.
Rasmussen, Edie. M. 1997. Indexing images. An-
nual Review of Information Science and Tech-
nology (ARIST), 32, 169-196.
</reference>
<page confidence="0.999298">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968970">
<title confidence="0.9987735">Concept Disambiguation for Improved Subject Using Multiple Knowledge Sources</title>
<author confidence="0.994335">Tandeep Sidhu</author>
<author confidence="0.994335">Judith Klavans</author>
<author confidence="0.994335">Jimmy</author>
<affiliation confidence="0.9998865">College of Information University of</affiliation>
<address confidence="0.988928">College Park, M D</address>
<email confidence="0.998272">tsidhu@umiacs.umd.edu,Oklavans,jimmylin}@umd.edu</email>
<abstract confidence="0.999542541666667">We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result ambiguous and thus faulty data. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Murtha Baca</author>
<author>ed</author>
</authors>
<title>Introduction to art image access: issues, tools, standards, strategies.</title>
<date>2002</date>
<journal>Getty Research Institute.</journal>
<marker>Baca, ed, 2002</marker>
<rawString>Baca, Murtha, ed. 2002. Introduction to art image access: issues, tools, standards, strategies. Getty Research Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baner ee</author>
<author>S</author>
<author>T Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on ArtificialIntelligence,</booktitle>
<pages>805--810</pages>
<marker>ee, S, Pedersen, 2003</marker>
<rawString>Baner ee, S., and T. Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on ArtificialIntelligence, 805-810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bobby Ferguson</author>
<author>Sheila Intner</author>
</authors>
<title>Sub ect Analysis: Blitz Cataloging Workbook. Westport, CT:Libraries Unlimited Inc.</title>
<date>1998</date>
<contexts>
<context position="3523" citStr="Ferguson and Intner 1998" startWordPosition="527" endWordPosition="530">ith other research on disambiguation, our hypothesis is that accurate assignment of senses to metadata index terms will results in higher precision for searchers. This hypothesis will be fully tested as we incorporate the disambiguation module in our end-to-end CLiMB Toolkit, and as we perform user studies. Finding subject terms and mapping them to a thesaurus is a time-intensive task for catalogers 25 Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25–32, Prague, 28 June 2007. c�2007 Association for Computational Linguistics (Rasmussen 1997, Ferguson and Intner 1998). Doing so typically involves reading image-related text or other sources to find subject terms. Even so, the lack of standard vocabulary in extensive subject indexing means that the enriched number of subject terms could be inadvertently offset by the vocabulary naming problem (Baca 2002). This paper reports on our results using the subject terms in the AAT; the CLiMB project is also using the Thesaurus of Geographic Names (TGN) and the Union List of Artist Names (ULAN). Since the focus of this paper is on disambiguation of common nouns rather than proper nouns, the AAT is our primary resourc</context>
</contexts>
<marker>Ferguson, Intner, 1998</marker>
<rawString>Ferguson, Bobby and Sheila Intner. 1998. Sub ect Analysis: Blitz Cataloging Workbook. Westport, CT:Libraries Unlimited Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="8075" citStr="Gale et al. 1992" startWordPosition="1325" endWordPosition="1328"> of the larger CLiMB project but they were not involved in the development of the disambiguation algorithm. The process of creating the groundtruth involved picking the correct AAT record for each of the terms in the data set. Terms not appearing in the AAT (as determined by the labelers) were given an AAT record value of zero. Each labeler worked independently on this task and had access to the online version of the AAT and the text where each term appeared. Interannotator agreement for the task was encouragingly high, at 85% providing a notional upper bound for automatic system performance (Gale et al. 1992). Not all terms in this dataset required disambiguation; 128 terms (out of 326) under the training set and 96 terms (out of 275) under the test set required disambiguation, since they matched more than one AAT record. The dataset we selected was adequate to test our different approaches and to refine our techniques. We intend to run over more data as we collect and annotate more resources for evaluation. 2.&apos; SenseRelate AllWords&apos; and WordNet&apos; SenseRelate AllWords (Banerjee and Pederson 2003, Patwardhan et al. 2003) is a Perl program that our algorithm employs to perform basic disambiguation of</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W. A., K. W. Church, and D. Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, 101-112, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy M Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Mapping Dictionaries: A Spreading Activation Approach.</title>
<date>1990</date>
<booktitle>In Proceedings of the 6th Annual Conference of the UW Centre for the New .ED and Text Research,</booktitle>
<pages>52--64</pages>
<location>Waterloo, Ontario.</location>
<contexts>
<context position="2043" citStr="Ide and Veronis 1990" startWordPosition="308" endWordPosition="312">is research is part of the Computational Linguistics for Metadata 1http://www.getty.edu/research/conducting_research/vocabul aries/aat/ Building (CLiMB) project (Klavans 2006, Klavans in preparation), which aims to improve image access by automatically extracting metadata from text associated with images. We present here a component of an overall architecture that automatically mines scholarly text for metadata terms. In order to filter and associate a term with a related concept, ambiguous terms must be clarified. The disambiguation of terms is a basic challenge in computational linguistics (Ide and Veronis 1990, Agirre and Edmonds 2006). As more non-specialists in digital libraries search for images, the need for subject term access has increased. Subject terms enrich catalog records with valuable broad-reaching metadata and help improve image access (Layne 1994). Image seekers will receive more relevant results if image records contain terms that reflect conceptual, semantic, and ontological relationships. Furthermore, subject terms associated with hierarchical and faceted thesaural senses promise to further improve precision in image access. Such terms map to standardized thesaurus records that in</context>
</contexts>
<marker>Ide, Veronis, 1990</marker>
<rawString>Ide, Nancy M. and Jean Veronis. 1990. Mapping Dictionaries: A Spreading Activation Approach. In Proceedings of the 6th Annual Conference of the UW Centre for the New .ED and Text Research, 52-64 Waterloo, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In Proceedings ofACM SIGD.C Conference,</booktitle>
<pages>24--26</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="10172" citStr="Lesk (1986)" startWordPosition="1674" endWordPosition="1675">s in our algorithm it needs to map to an AAT sense. In Section 3, we show how we translate a WordNet sense to an AAT sense for use in our algorithm. To perform disambiguation, SenseRelate requires that certain parameters be set: (1) the number of words around the target word (also known as the context window), and (2) the similarity measure. We used a value of 20 for the context window, which means that SenseRelate will use 10 words to the left and 10 words to the right of the target word to determine the correct sense. We used lesk as the similarity measure in our algorithm which is based on Lesk (1986). This decision was based on several experiments we did with various context window sizes and various similarity measures on a data set of 60 terms. 27 3. Methodology 3.1 Disambiguation Algorith Figure 2: Disambiguation Algorithm Figure 2 above shows that first we identify the noun phrases from the input document. Then we disambiguate each noun phrase independently by first looking it up in the AAT. If a record is found, we move on to the next step; otherwise we look up the head noun (as the noun phrase) in the AAT. Second, we filter out any AAT records where the noun phrase (or the head noun)</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, Michael. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. In Proceedings ofACM SIGD.C Conference, 24-26, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
</authors>
<title>Computational Linguistics for Metadata Building (CLiMB).</title>
<date>2006</date>
<booktitle>In Procedings of the .ntoImage Workshop, G. Greffenstette, ed. Language Resources and Evaluation Conference (LREC),</booktitle>
<location>Genova, Italy.</location>
<contexts>
<context position="1597" citStr="Klavans 2006" startWordPosition="239" endWordPosition="240"> subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline. 1. Introduction We describe an algorithm that takes noun phrases and assigns a sense to the head noun or phrase, given a large domain-specific thesaurus, the Art and Architecture Thesaurus1 (published by the Getty Research Institute). This research is part of the Computational Linguistics for Metadata 1http://www.getty.edu/research/conducting_research/vocabul aries/aat/ Building (CLiMB) project (Klavans 2006, Klavans in preparation), which aims to improve image access by automatically extracting metadata from text associated with images. We present here a component of an overall architecture that automatically mines scholarly text for metadata terms. In order to filter and associate a term with a related concept, ambiguous terms must be clarified. The disambiguation of terms is a basic challenge in computational linguistics (Ide and Veronis 1990, Agirre and Edmonds 2006). As more non-specialists in digital libraries search for images, the need for subject term access has increased. Subject terms </context>
</contexts>
<marker>Klavans, 2006</marker>
<rawString>Klavans, Judith L. 2006. Computational Linguistics for Metadata Building (CLiMB). In Procedings of the .ntoImage Workshop, G. Greffenstette, ed. Language Resources and Evaluation Conference (LREC), Genova, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Judith L Klavans</author>
</authors>
<title>(in preparation). Using Computational Linguistic Techniques and Thesauri for Enhancing Metadata Records in Image Search: The CLiMB Pro ect.</title>
<marker>Klavans, </marker>
<rawString>Klavans, Judith L. (in preparation). Using Computational Linguistic Techniques and Thesauri for Enhancing Metadata Records in Image Search: The CLiMB Pro ect.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Shatford Layne</author>
</authors>
<title>Some issues in the indexing of images.</title>
<date>1994</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>583--588</pages>
<contexts>
<context position="2300" citStr="Layne 1994" startWordPosition="349" endWordPosition="350">tadata from text associated with images. We present here a component of an overall architecture that automatically mines scholarly text for metadata terms. In order to filter and associate a term with a related concept, ambiguous terms must be clarified. The disambiguation of terms is a basic challenge in computational linguistics (Ide and Veronis 1990, Agirre and Edmonds 2006). As more non-specialists in digital libraries search for images, the need for subject term access has increased. Subject terms enrich catalog records with valuable broad-reaching metadata and help improve image access (Layne 1994). Image seekers will receive more relevant results if image records contain terms that reflect conceptual, semantic, and ontological relationships. Furthermore, subject terms associated with hierarchical and faceted thesaural senses promise to further improve precision in image access. Such terms map to standardized thesaurus records that include the term&apos;s preferred, variant, and related names, including both broader and specific concepts, and other related concepts. This information can then be filtered, linked, and subsequently tested for usefulness in performing richer image access. As wit</context>
</contexts>
<marker>Layne, 1994</marker>
<rawString>Layne, Sara Shatford. 1994. Some issues in the indexing of images. Journal of the American Society for Information Science, 583-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
</authors>
<title>Hwee Tou Ng, &amp; Hoa Trang Dang.</title>
<date>2006</date>
<booktitle>Evaluation of WSD Systems. Word Sense Disambiguation: Algorithms and Applications. Eneko Agirre and</booktitle>
<pages>75--106</pages>
<editor>Philip Edmonds, ed.</editor>
<location>Dordrecht, The Netherlands:Springer.</location>
<marker>Palmer, 2006</marker>
<rawString>Palmer, Martha, Hwee Tou Ng, &amp; Hoa Trang Dang. 2006. Evaluation of WSD Systems. Word Sense Disambiguation: Algorithms and Applications. Eneko Agirre and Philip Edmonds, ed. 75-106. Dordrecht, The Netherlands:Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Baner ee</author>
<author>S</author>
<author>T Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<contexts>
<context position="8595" citStr="Patwardhan et al. 2003" startWordPosition="1413" endWordPosition="1416">ingly high, at 85% providing a notional upper bound for automatic system performance (Gale et al. 1992). Not all terms in this dataset required disambiguation; 128 terms (out of 326) under the training set and 96 terms (out of 275) under the test set required disambiguation, since they matched more than one AAT record. The dataset we selected was adequate to test our different approaches and to refine our techniques. We intend to run over more data as we collect and annotate more resources for evaluation. 2.&apos; SenseRelate AllWords&apos; and WordNet&apos; SenseRelate AllWords (Banerjee and Pederson 2003, Patwardhan et al. 2003) is a Perl program that our algorithm employs to perform basic disambiguation of words. We have adapted SenseRelate for the purpose of disambiguating AAT senses. Given a sentence, SenseRelate AllWords disambiguates all the words in that sentence. It uses word sense definitions from WordNet (in this case WordNet 2.1), a large lexical database of English nouns, verbs, adjectives, and adverbs. As an example, consider the text below: 3 http://sourceforge.net/projects/senserelate 4 http://wordnet.princeton.edu/ The SenseRelate result is: With more#a#2 than fifty#n#1 individual#n#1 scene#n#10 the al</context>
</contexts>
<marker>Patwardhan, ee, S, Pedersen, 2003</marker>
<rawString>Patwardhan, S., S. Baner ee, S. and T. Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, 241-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M</author>
</authors>
<title>Indexing images.</title>
<date>1997</date>
<journal>Annual Review of Information Science and Technology (ARIST),</journal>
<volume>32</volume>
<pages>169--196</pages>
<marker>M, 1997</marker>
<rawString>Rasmussen, Edie. M. 1997. Indexing images. Annual Review of Information Science and Technology (ARIST), 32, 169-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>