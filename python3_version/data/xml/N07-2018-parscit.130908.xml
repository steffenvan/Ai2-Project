<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056264">
<title confidence="0.992864">
Clustered Sub-matrix Singular Value Decomposition
</title>
<author confidence="0.999346">
Fang Huang
</author>
<affiliation confidence="0.987586">
School of Computing
Robert Gordon University
</affiliation>
<address confidence="0.988084">
Aberdeen, AB25 1HG, UK
</address>
<email confidence="0.998646">
f.huang@rgu.ac.uk
</email>
<author confidence="0.997189">
Yorick Wilks
</author>
<affiliation confidence="0.997615">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.989337">
Sheffield, S1 4DP, UK
</address>
<email confidence="0.998457">
y.wilks@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987375">
This paper presents an alternative algo-
rithm based on the singular value decom-
position (SVD) that creates vector rep-
resentation for linguistic units with re-
duced dimensionality. The work was mo-
tivated by an application aimed to repre-
sent text segments for further processing
in a multi-document summarization sys-
tem. The algorithm tries to compensate
for SVD’s bias towards dominant-topic
documents. Our experiments on measur-
ing document similarities have shown that
the algorithm achieves higher average pre-
cision with lower number of dimensions
than the baseline algorithms - the SVD
and the vector space model.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999584021276596">
We present, in this paper, an alternative algorithm
called Clustered Sub-matrix Singular Value Decom-
position(CSSVD) algorithm, which applied cluster-
ing techniques before basis vector calculation in
SVD (Golub and Loan, 1996). The work was
motivated by an application aimed to provide vec-
tor representation for terms and text segments in a
document collection. These vector representations
were then used for further preprocessing in a multi-
document summarization system.
The SVD is an orthogonal decomposition tech-
nique closely related to eigenvector decomposition
and factor analysis. It is commonly used in infor-
mation retrieval as well as language analysis appli-
cations. In SVD, a real m-by-n matrix A is decom-
posed into three matrices, A = U E VT. E is an
m-by-n matrix such that the singular value ui=Eii is
the square root of the ith largest eigenvalue of AAT ,
and Eij = 0 for i =� j. Columns of orthogonal ma-
trices U and V define the orthonormal eigenvectors
associated with eigenvalues of AAT and ATA, re-
spectively. Zeroing out all but the k, k &lt; rank(A),
largest singular values yields Ak = Ek i�1 UiuiVTi ,
which is the closest rank-k matrix to A. Let A be a
term-document matrix. Applications such as latent
semantic indexing (Deerwester et al., 1990) apply
the rank-k approximation Ak to the original matrix
A, which corresponds to projecting A onto the k-
dimension subspace spanned by u1, u2, ..., uk. Be-
cause k « m, in this k-dimension space, minor
terms are ignored, so that terms are not indepen-
dent as they are in the traditional vector space model.
This allows semantically related documents to be re-
lated to each other even though they may not share
terms.
However, SVD tends to wipe out outlier
(minority-class) documents as well as minor terms
(Ando, 2000). Consequently, topics underlying out-
lier documents tend to be lost. In applications such
as multi-document summarization, a set of related
documents are used as the information source. Typ-
ically, the documents describe one broad topic from
several different view points or sub-topics. It is im-
portant for each of the sub-topics underlying the
document collection to be represented well.
Based on the above consideration, we propose the
CSSVD algorithm with the intention of compensat-
</bodyText>
<page confidence="0.992141">
69
</page>
<note confidence="0.4648895">
Proceedings of NAACL HLT 2007, Companion Volume, pages 69–72,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999866777777778">
ing for SVD’s tendency to wipe out minor topics.
The basic idea is to group the documents into a set
of clusters using clustering algorithms. The SVD
is then applied on each of the document clusters.
The algorithm thus selects basis vectors by treat-
ing equally each of the topics. Our experiments
on measuring document similarities have shown that
the algorithm achieves higher average precision with
lower number of dimensions than the SVD.
</bodyText>
<sectionHeader confidence="0.951788" genericHeader="method">
2 the Algorithm
</sectionHeader>
<bodyText confidence="0.999677">
The input to the CSSVD algorithm is an mxn term-
document matrix A. Documents in matrix A are
grouped into a set of document clusters. Here,
we adopt single-link algorithm to develop the ini-
tial clusters, then use K-means method to refine the
clusters. After clustering, columns in matrix A are
partitioned and regrouped into a set of sub-matrices
A1,A2,...,AQ. Each of these matrices represents a
document cluster. Assume Ai, 1 &lt; i &lt; q, is an
m x ni matrix, these sub-matrices are ranked in de-
creasing order of their sizes, i.e., n1 &gt; n2 &gt; ... &gt;
nQ, then n1 + n2 + ... + nQ = n.
The algorithm computes basis vectors as follows:
the first basis vector u1 is computed from A1, i.e.,
the first left singular vector of A1 is selected. In or-
der to ensure that the basis vectors are orthogonal,
singular vectors are actually computed on residual
matrices. Rij, the residual matrix of Ai after the se-
lection of basis vectors u1, u2,..., uj, is defined as
</bodyText>
<equation confidence="0.9289325">
=JAi j = 0
Rij l Ai − proj(Aij) otherwise
</equation>
<bodyText confidence="0.802483333333333">
where, proj(Aij) is the orthogonal projection of the
document vectors in Ai onto the span of u1,u2,...,uj,
i.e.,
</bodyText>
<equation confidence="0.997035">
proj(Aij) = � j ukuk Ai
k=1
</equation>
<bodyText confidence="0.9999105">
the residual matrix of Ai describes how much the
document vectors in Ai are excluded from the pro-
posed basis vectors u1, u2,..., uj. For the first ba-
sis vector computation, residual matrices are initial-
ized as original sub-matrices. The computation of
the residual matrix makes the remaining vectors per-
pendicular to the previous basis vectors, thus ensures
that the basis vectors are orthogonal, as the eigen-
vector computed next is a linear combination of the
remaining vectors.
After calculating a basis vector, the algorithm
judges whether the sub-matrices have been well rep-
resented by the derived basis vectors. The residual
ratio was defined as a criterion for this judgement,
</bodyText>
<equation confidence="0.921266">
rrij = ni x (ki + 1)
</equation>
<bodyText confidence="0.999980722222222">
where Rij is the residual matrix of Ai after j basis
vectors have been selected1; ni is the number of
the documents in matrix Ai; ki is the number
of singular vectors that have been selected from
matrix Ai. Residual ratios of each sub-matrix are
calculated. The sub-matrix with the largest residual
ratio is assumed to be the one that contains the
most information that has not been represented by
the previous chosen basis vectors. The first left
singular vector of this sub-matrix is computed and
selected as the next basis vector. As described
above, the computation of a basis vector uses the
corresponding residual matrix. Once a basis vector
is selected, its influence from each sub-matrix is
subtracted. The procedure is repeated until an
expected number of basis vectors have been chosen.
The pseudo-code of the algorithm for semantic
space construction is shown as follows:
</bodyText>
<listItem confidence="0.99808025">
1. Partition A into matrices A1,...,AQ corresponding
to document clusters, where Ai , 1 &lt; i &lt; q, is an
m x ni (n1 &gt; n2 &gt; ... &gt; nQ) matrix.
2. For i=1,2,...,q {Ri= Ai; k[i]=0;}
3. j=1; r=1;
4. ur= the first unit eigenvector of RjR� ;
5. For i=1,2,...,q Ri= Ri - urur Ri;
6. k[r]=k[r]+1; r=r+1;
7. For i=1,2,...,q rri=    |F ;
(ni x (k[i]+1))
8. j=t if rrt &gt; rrp for p=1,2,...,q and p =� t;
9. If rrj &lt; threshold then stop else goto step 4.
</listItem>
<bodyText confidence="0.9977994">
For the single-link algorithm used in the CSSVD,
we use a threshold 0.2 and cosine measure to cal-
culate the similarity between two clusters in our ex-
periments. The performance of the CSSVD is also
relative to the number of dimensions of the created
</bodyText>
<equation confidence="0.969602">
&apos;J�1||A||F = i,j A�ij
||Rij||2
F
</equation>
<page confidence="0.959139">
70
</page>
<bodyText confidence="0.982940214285714">
subspace. As described above, the algorithm uses
the residual ratio as a stopping criterion for the basis
vector computation. In each iteration, after a basis
vector is created, the residual ratio is compared to a
threshold. Once the residual ratio of each sub-matrix
fell below a certain threshold, the process of basis-
vector selection is finished. In our experiments, the
threshold was trained on corpus.
After all the k basis vectors are chosen, a term-
document vector di can be converted to dki , a
vector in the k-dimensional space, by multiply-
ing the matrix of basis vectors following the stan-
dard method of orthogonal transformation,i.e., dki =
[u1, u2, ..., uk]T di.
</bodyText>
<sectionHeader confidence="0.999729" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998437">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999911108108108">
For the evaluation of the algorithm, 38 topics from
the Text REtrieval Conference (TREC) collections
were used in our experiments. These topics include
foreign minorities, behavioral genetics, steel pro-
duction, etc. We deleted documents relevant to more
than one topic so that each document is related only
to one topic. The total number of documents used
was 2962. These documents were split into two dis-
joint groups, called ’pool 1’ and ’pool 2’. The num-
ber of documents in ’pool 1’ and ’pool 2’ were 1453
and 1509, respectively. Each of the two groups used
19 topics.
We generated training and testing data by simu-
lating the result obtained by a query search. This
simulation is further simplified by selecting docu-
ments containing same keywords from each docu-
ment group. Thirty document sets were generated
from each of the two document groups, i.e. 60 doc-
ument sets in total. The number of documents for
each set ranges from 51 to 582 with an average of
128; the number of topics ranges from 5 to 19 with
an average of 12. Due to the limited number of the
document sets we created, these sets were used both
for training and evaluation. For the evaluation of the
documents sets from ’pool 1’, ’pool 2’ was used for
training, and vice versa.
To construct the original term-document matrix,
the following operations were performed on each of
the documents: 1) filtering out all non-text tags in
the documents; 2) converting all the characters into
lower case; 3) removing stop words - a stoplist con-
taining 319 words was used; and 4) term indexing
- the tf.idf scheme was used to calculate a term’s
weight in a document. Finally, a document set is
represented as a matrix A = [aij], where aij de-
notes the normalized weight assigned to term i in
document j.
</bodyText>
<subsectionHeader confidence="0.999521">
3.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.994706590909091">
Our algorithm was motivated by a multi-document
summarization application which is mainly based
on measuring the similarities and differences among
text segments. Therefore, the basic requisite is to ac-
curately measure similarities among texts. Based on
this consideration, we used the CSSVD algorithm to
create the document vectors in a reduced space for
each of the document sets; cosine similarities among
these document vectors were computed; and the re-
sults were then compared with the TREC relevance
judgments. As each of the TREC documents we
used has one specific topic. Assume that similarity
should be higher for any document pair relevant to
the same topic than for any pair relevant to different
topics. The algorithm’s accuracy for measuring the
similarities among documents was evaluated using
average precision taken at various recall levels (Har-
man, 1995). Let pi denote the document pair that
has the ith largest similarity value among all pairs of
documents in the document set. The precision for an
intra-topic pair pk is calculated by
number of pj where j G k
</bodyText>
<equation confidence="0.980598">
precision(pk) = k
</equation>
<bodyText confidence="0.999994">
where pj is an intra-topic pair. The average of the
precision values over all intra-topic pairs is com-
puted as the average precision.
</bodyText>
<subsectionHeader confidence="0.949844">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999926">
The algorithms are evaluated by the average preci-
sion over 60 document sets. In order to make a com-
parison, two baseline algorithms besides CSSVD are
evaluated. One is the vector space model (VSM)
without dimension reduction. The other is SVD tak-
ing the left singular vectors as the basis vectors.
To treat the selection of dimensions as a separate
issue, we first evaluate the algorithms in terms of
the best average precision. The ’best average preci-
sion’ means the best over all the possible numbers
</bodyText>
<page confidence="0.99687">
71
</page>
<bodyText confidence="0.9990022">
of dimensions. The second row of Table 1 shows the
best average precision of our algorithm, VSM, and
SVD. The best average precision on average over 60
document sets of CSSVD is 69.6%, which is 11.5%
higher than VSM and 6.1% higher than SVD.
</bodyText>
<table confidence="0.9987268">
measure VSM SVD CSSVD
best average 58.1 63.5 69.6
precision (%)
average DR (%) N/A 54.4 32.1
average precision (%) 58.1 59.5 66.8
</table>
<tableCaption confidence="0.999901">
Table 1: the algorithm performance
</tableCaption>
<bodyText confidence="0.999949333333333">
In the experiments, we observed that the CSSVD al-
gorithm obtained its best performance with the num-
ber of dimensions lower than that of SVD. The Di-
mensional Ratio (DR) is defined as the number of
dimensions of the derived sub-space compared with
the dimension number of the original space, i.e.,
</bodyText>
<construct confidence="0.4682925">
DR —_ # of dimensions in derived space
# of dimensions in original space
</construct>
<bodyText confidence="0.999983678571429">
The average dimensional ratio is calculated over all
the 60 document sets. As the algorithms’ computa-
tional efficiency is dependent on the number of di-
mensions computed, our interest is in getting good
performance with an average dimensional ratio as
low as possible. The third row of Table 1 shows the
average dimensional ratio that yielded the best av-
erage precision. The average dimensional ratio that
CSSVD yielded the best average precision is 32.1%,
which is 22.3% lower than that of SVD. Thus, our
algorithm has the advantage of being computation-
ally inexpensive, assuming that we can find the op-
timal number of dimensions.
The bottom row of Table 1 shows the average
precision of the algorithms. The threshold used in
CSSVD algorithm was trained on corpus. Let p be
the threshold on residual ratio that yielded the best
average precision on the training data. The value
of p is then used as the threshold on the evaluation
data. For the SVD algorithm, the average dimen-
sional ratio that yielded the best average precision
on training data was used as the dimensional ratio
to determine the subspace dimensionality in evalua-
tion. The performance shown here are the average
of average precision over 60 document sets. Again,
the CSSVD achieves the best performance, which is
7.3% higher than the performance of SVD and 8.7%
higher than VSM.
</bodyText>
<sectionHeader confidence="0.999078" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999989">
We have presented an alternative algorithm, the
CSSVD, that creates vector representation for lin-
guistic units with reduced dimensionality. The al-
gorithm aims to compensate for SVD’s bias towards
dominant-topic documents by grouping documents
into clusters and selecting basis vectors from each
of the clusters. It introduces a threshold on the resid-
ual ratio of clusters as a stopping criterion of basis
vector selection. It thus treats each topic underly-
ing the document collection equally while focuses
on the dominant documents in each topic. The pre-
liminary experiments on measuring document simi-
larities have shown that the CSSVD achieves higher
average precision with lower number of dimensions
than the baseline algorithms.
Motivated by a multi-document summarization
application, the CSSVD algorithm’s emphasis on
topics and dominant information within each topic
meets the general demand of summarization. We ex-
pect that the algorithm fits the task of summarization
better than SVD. Our future work will focus on more
thorough evaluation of the algorithm and integrating
it into a summarization system.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974666666667">
We would like to thank Mark Sanderson, Horacio
Saggion, and Robert Gaizauskas for helpful com-
ments at the beginning of this research.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434076923077">
Ando R.K. 2000 Latent Sementic Space: Iterative
Scaling Improves Precision of Inter-document Similar-
ity Measurement. Proceedings of ACM SIGIR 2000,
Athens, Greece.
Deerwester S., Dumais S., Furnas G., and Landauer T.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41:391-407.
Golub G. and Loan C.V. 1996. Matrix Computations.
Johns-Hopkins University Press, Maryland, US.
Harman D.K. 1983. Overview of the second Text Re-
trieval Conference (TREC-2). Information Processing
Management, 31(3):271-289.
</reference>
<page confidence="0.998723">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.209820">
<title confidence="0.837452">Clustered Sub-matrix Singular Value Decomposition Fang School of</title>
<author confidence="0.925032">Robert Gordon</author>
<address confidence="0.466743">Aberdeen, AB25 1HG,</address>
<email confidence="0.996758">f.huang@rgu.ac.uk</email>
<author confidence="0.794903">Yorick</author>
<affiliation confidence="0.9992025">Department of Computer University of</affiliation>
<address confidence="0.829359">Sheffield, S1 4DP,</address>
<email confidence="0.998621">y.wilks@dcs.shef.ac.uk</email>
<abstract confidence="0.999646882352941">This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality. The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system. The algorithm tries to compensate for SVD’s bias towards dominant-topic documents. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms the SVD and the vector space model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
</authors>
<title>Latent Sementic Space: Iterative Scaling Improves Precision of Inter-document Similarity Measurement.</title>
<date>2000</date>
<booktitle>Proceedings of ACM SIGIR</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2675" citStr="Ando, 2000" startWordPosition="430" endWordPosition="431">e a term-document matrix. Applications such as latent semantic indexing (Deerwester et al., 1990) apply the rank-k approximation Ak to the original matrix A, which corresponds to projecting A onto the kdimension subspace spanned by u1, u2, ..., uk. Because k « m, in this k-dimension space, minor terms are ignored, so that terms are not independent as they are in the traditional vector space model. This allows semantically related documents to be related to each other even though they may not share terms. However, SVD tends to wipe out outlier (minority-class) documents as well as minor terms (Ando, 2000). Consequently, topics underlying outlier documents tend to be lost. In applications such as multi-document summarization, a set of related documents are used as the information source. Typically, the documents describe one broad topic from several different view points or sub-topics. It is important for each of the sub-topics underlying the document collection to be represented well. Based on the above consideration, we propose the CSSVD algorithm with the intention of compensat69 Proceedings of NAACL HLT 2007, Companion Volume, pages 69–72, Rochester, NY, April 2007. c�2007 Association for C</context>
</contexts>
<marker>Ando, 2000</marker>
<rawString>Ando R.K. 2000 Latent Sementic Space: Iterative Scaling Improves Precision of Inter-document Similarity Measurement. Proceedings of ACM SIGIR 2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="2161" citStr="Deerwester et al., 1990" startWordPosition="338" endWordPosition="341">as language analysis applications. In SVD, a real m-by-n matrix A is decomposed into three matrices, A = U E VT. E is an m-by-n matrix such that the singular value ui=Eii is the square root of the ith largest eigenvalue of AAT , and Eij = 0 for i =� j. Columns of orthogonal matrices U and V define the orthonormal eigenvectors associated with eigenvalues of AAT and ATA, respectively. Zeroing out all but the k, k &lt; rank(A), largest singular values yields Ak = Ek i�1 UiuiVTi , which is the closest rank-k matrix to A. Let A be a term-document matrix. Applications such as latent semantic indexing (Deerwester et al., 1990) apply the rank-k approximation Ak to the original matrix A, which corresponds to projecting A onto the kdimension subspace spanned by u1, u2, ..., uk. Because k « m, in this k-dimension space, minor terms are ignored, so that terms are not independent as they are in the traditional vector space model. This allows semantically related documents to be related to each other even though they may not share terms. However, SVD tends to wipe out outlier (minority-class) documents as well as minor terms (Ando, 2000). Consequently, topics underlying outlier documents tend to be lost. In applications s</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, 1990</marker>
<rawString>Deerwester S., Dumais S., Furnas G., and Landauer T. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41:391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Golub</author>
<author>C V Loan</author>
</authors>
<title>Matrix Computations.</title>
<date>1996</date>
<publisher>Johns-Hopkins University Press,</publisher>
<location>Maryland, US.</location>
<contexts>
<context position="1122" citStr="Golub and Loan, 1996" startWordPosition="157" endWordPosition="160">esent text segments for further processing in a multi-document summarization system. The algorithm tries to compensate for SVD’s bias towards dominant-topic documents. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the SVD and the vector space model. 1 Introduction We present, in this paper, an alternative algorithm called Clustered Sub-matrix Singular Value Decomposition(CSSVD) algorithm, which applied clustering techniques before basis vector calculation in SVD (Golub and Loan, 1996). The work was motivated by an application aimed to provide vector representation for terms and text segments in a document collection. These vector representations were then used for further preprocessing in a multidocument summarization system. The SVD is an orthogonal decomposition technique closely related to eigenvector decomposition and factor analysis. It is commonly used in information retrieval as well as language analysis applications. In SVD, a real m-by-n matrix A is decomposed into three matrices, A = U E VT. E is an m-by-n matrix such that the singular value ui=Eii is the square </context>
</contexts>
<marker>Golub, Loan, 1996</marker>
<rawString>Golub G. and Loan C.V. 1996. Matrix Computations. Johns-Hopkins University Press, Maryland, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
</authors>
<date>1983</date>
<booktitle>Overview of the second Text Retrieval Conference (TREC-2). Information Processing Management,</booktitle>
<pages>31--3</pages>
<marker>Harman, 1983</marker>
<rawString>Harman D.K. 1983. Overview of the second Text Retrieval Conference (TREC-2). Information Processing Management, 31(3):271-289.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>