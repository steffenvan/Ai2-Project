<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000856">
<title confidence="0.9976035">
Domain-Specific Semantic Relatedness From Wikipedia:
Can A Course Be Transferred?
</title>
<author confidence="0.997593">
Beibei Yang
</author>
<affiliation confidence="0.998908">
University of Massachusetts Lowell
</affiliation>
<address confidence="0.870993">
Lowell, MA 01854
</address>
<email confidence="0.998075">
byang1@cs.uml.edu
</email>
<author confidence="0.997368">
Jesse M. Heines
</author>
<affiliation confidence="0.99883">
University of Massachusetts Lowell
</affiliation>
<address confidence="0.871478">
Lowell, MA 01854
</address>
<email confidence="0.999116">
heines@cs.uml.edu
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999111818181818">
Semantic relatedness, or its inverse, seman-
tic distance, measures the degree of close-
ness between two pieces of text determined by
their meaning. Related work typically mea-
sures semantics based on a sparse knowledge
base such as WordNetl or CYC that requires
intensive manual efforts to build and main-
tain. Other work is based on the Brown cor-
pus, or more recently, Wikipedia. Wikipedia-
based measures, however, typically do not
take into account the rapid growth of that re-
source, which exponentially increases the time
to prepare and query the knowledge base. Fur-
thermore, the generalized knowledge domain
may be difficult to adapt to a specific domain.
To address these problems, this paper pro-
poses a domain-specific semantic relatedness
measure based on part of Wikipedia that ana-
lyzes course descriptions to suggest whether a
course can be transferred from one institution
to another. We show that our results perform
well when compared to previous work.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996232714285714">
Many NLP techniques have been adapted to the ed-
ucation field for building systems such as automated
scoring, intelligent tutoring, and learner cognition.
Few, however, address the identification of transfer
course equivalencies. A recent study by the Na-
tional Association for College Admission Counsel-
ing2 reveals that 1/3 of US college students trans-
</bodyText>
<footnote confidence="0.9890645">
1http://wordnet.princeton.edu/
2Special Report on the Transfer Admission Process:
http://www.nacacnet.org/research/research-data/Documents/
TransferFactSheet.pdf
</footnote>
<page confidence="0.997875">
35
</page>
<bodyText confidence="0.99988390625">
fer to another institution. Correspondingly, Univer-
sity of Massachusetts Lowell (UML) accepts hun-
dreds of transfer students every year. Each trans-
fer course must be evaluated for credits by manually
comparing its course description to courses offered
at UML. This process is labor-intensive and highly
inefficient. There is a publicly available course
transfer dictionary which lists course numbers from
hundreds of institutions and their equivalent courses
at UML, but the data set is sparse, non-uniform,
and always out of date. External institutions cancel
courses, change course numbers, etc., and such in-
formation is virtually impossible to keep up to date
in the transfer dictionary. Furthermore, the transfer
dictionary does not list course descriptions. From
our experience, course descriptions change over the
years even when course numbers do not, and this of
course affect equivalencies.
This work proposes a domain-specific semantic
relatedness measure using Wikipedia that automat-
ically suggests whether two courses from different
institutions are equivalent by analyzing their course
descriptions. The goal is to assist transfer coordina-
tors by suggesting equivalent courses within a rea-
sonable amount of time on a standard laptop system.
Our model is a mapping function: f : (C1, C2) —*
n, n E [0, 1], where C1 is a Computer Science (CS)
course from an external institution, and C2 is a CS
course offered at UML. Output n is the semantic re-
latedness score, where a bigger value indicates C1
and C2 are more related. Each course description is
a short text passage:
</bodyText>
<listItem confidence="0.8397125">
• Cl: [Analysis of Algorithms] Discusses basic methods
for designing and analyzing efficient algorithms empha-
</listItem>
<note confidence="0.654941">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 35–40,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<tableCaption confidence="0.994772625">
sizing methods used in practice. Topics include sorting,
searching, dynamic programming, greedy algorithms, ad-
vanced data structures, graph algorithms (shortest path,
spanning trees, tree traversals), matrix operations, string
matching, NP completeness.
9 C2: [Computing III] Object-oriented programming.
Classes, methods, polymorphism, inheritance. Object-
oriented design. C++. UNIX. Ethical and social issues.
</tableCaption>
<subsectionHeader confidence="0.794288">
Fragments of WordNet and Wikipedia Taxonomies
</subsectionHeader>
<bodyText confidence="0.964292368421053">
Figure 1. Fragments of WordNet 3.0 (top) and
English Wikipedia of 2011/7 (bottom) taxonomies.
The root/centroid node is shown in red.
We choose Wikipedia as the knowledge base
due to its rich contents (Figure 1) and continu-
ously coalescent growth (Bounova, 2011). Although
Wikipedia was launched 10 years later, it grew much
faster than WordNet over the last decade (Figure 2).
The contributions of this paper are twofold. First,
we address the problem of domain-specific semantic
relatedness using Wikipedia. We propose a method
to suggest course equivalencies by computing se-
mantic relatedness among Computer Science course
descriptions. Our approach can be easily modified
for other majors and even other languages. Second,
we evaluate the correlation of our approach and a hu-
man judgment data set we built. Both accuracy and
correlation indicate that our approach outperforms
previous work.
</bodyText>
<sectionHeader confidence="0.997673" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999915212121212">
Semantic relatedness has been used in applications
such as word sense disambiguation, named entity
disambiguation, text summarization and annotation,
lexical selection, automatic spelling correction, and
text structure evaluation. WordNet is commonly
used as a lexicographic resource to calculate se-
mantic relatedness (Budanitsky and Hirst, 2006).
A WordNet-based method uses one or more edge-
counting techniques in the WordNet taxonomy (Lea-
cock and Chodorow, 1998; Hirst and St-Onge,
1998). The relatedness of two concept nodes is a
function of the minimum number of hops between
them.
Some related work calculates co-occurrence on
one or more large corpora to deduce semantic re-
latedness (Sahami and Heilman, 2006; Cilibrasi and
Vitanyi, 2007). Two words are likely to be related if
they co-occur within similar contexts (Lin, 1998).
Others combine lexicographic resources with cor-
pus statistics (Jiang and Conrath, 1997). It has been
shown that these composite methods generally out-
perform lexicographic resource- and corpus- based
methods (Budanitsky and Hirst, 2006; Curran, 2004;
Mohammad, 2008). Li et al. (2006) propose a hybrid
method based on WordNet and the Brown corpus to
incorporate semantic similarity between words, se-
mantic similarity between sentences, and word order
similarity to measure the overall sentence similarity.
Yang and Heines (2011) modify this work to suggest
transfer course equivalencies, but the experiment is
based on non-technical courses. Due to the WordNet
sparsity on technical terms, the experiment does not
perform well on Computer Science courses.
</bodyText>
<table confidence="0.624752666666667">
# nodes: 25
WordNet [Root: synset(“technology”), #depth: 2]
# nodes: 3583
Wikipedia [Centroid: “Category:Technology”, #steps: 2]
1992 1996 2000 2004 2008 2012
Year
</table>
<figureCaption confidence="0.997668">
Figure 2. Growth of Wikipedia and WordNet
</figureCaption>
<figure confidence="0.990093">
Growth of English Wikipedia and WordNet
4000000
3500000
3000000
2500000
2000000
1500000
1000000
500000
Articles in Wikipedia
Synsets in WordNet
</figure>
<page confidence="0.986839">
36
</page>
<bodyText confidence="0.998482729166667">
In recent years, there has been increasing interest
in applying Wikipedia and related resources to ques-
tion answering (Buscaldi and Rosso, 2006), word
sense disambiguation (WSD) (Mihalcea and Cso-
mai, 2007), name entity disambiguation (Ni et al.,
2010), ontology evaluation (Yu et al., 2007), seman-
tic web (Wu, 2010), and computing semantic relat-
edness (Ponzetto and Strube, 2007). Ponzetto and
Strube (2007) deduce semantic relatedness of words
by modeling relations on the Wikipedia category
graph. Gabrilovich and Markovitch (2009) intro-
duce the Explicit Semantic Analysis (ESA) model
which calculates TF-IDF (Manning et al., 2008) val-
ues for every word in Wikipedia and further uses lo-
cal linkage information to build a second-level se-
mantic interpreter.
Our approach is different from prior work on
Wikipedia. While Mihalcea and Csomai (2007)
use the annotation in the page title of a concept to
perform WSD, our approach uses a page’s parent
category as a cue to the correct sense. Ponzetto
and Strube (2007) limit their measurement to word
pairs, while our work focuses on text of any length.
Gabrilovich and Markovitch (2009) computes TF-
IDF statistics for every word and every document
of Wikipedia which is highly inefficient. They also
remove category pages and disambiguation pages.
In contrast, our model is mainly based on the cate-
gory taxonomy and the corpus statistics are limited
to metadata that are mostly available in Wikipedia.
Furthermore, we compute concept relatedness on
a domain-specific hierarchy that weighs both path
lengths and diversions from the topic. The domain-
specific hierarchy is much smaller than the entire
Wikipedia corpus. As a result, our algorithm is more
efficient3 than previous work.
3In our experiment, the average time needed to compare
one pair of course descriptions ranged from 0.16 second (with
partial caching) to 1 minute (without caching) on a 2.6Ghz
Quad-Core PC. The most time-consuming part before compar-
ing courses was to index all the Wikipedia tables in a MySQL
database, which took overnight (same for ESA). It only took
us 15 minutes to go through 19K pages to build a hierarchy
of D = 4. In contrast, ESA’s first level semantic interpreter
(which tokenizes every Wikipedia page to compute TF-IDF)
took 7 days to build over the same 19K pages. Both imple-
mentations were single-threaded, coded in Python, and tested
over the English Wikipedia of July 2011.
</bodyText>
<sectionHeader confidence="0.991264" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999980666666667">
Our method contains four modules. Section 3.1 ex-
plains how to construct a domain-specific hierarchy
from Wikipedia. Section 3.2 presents semantic relat-
edness between concepts. Section 3.3 describes the
steps to generate features from course descriptions.
And section 3.4 evaluates course relatedness.
</bodyText>
<subsectionHeader confidence="0.999319">
3.1 Extract a Lexicographical Hierarchy
</subsectionHeader>
<bodyText confidence="0.99944348">
When a domain is specified (e.g., CS courses), we
start from a generic Wikipedia category in this do-
main, choose its parent as the root, and use a depth-
limited search to recursively traverse each subcate-
gory (including subpages) to build a lexicographical
hierarchy with depth D. For example, to find CS
course equivalencies, we built a hierarchy using the
parent of “Category:Computer science,” i.e., “Cat-
egory:Applied sciences,” as the root. The generic
category’s parent is chosen as the root to make sure
the hierarchy not only covers the terms in this do-
main, but also those in neighbor domains. The hier-
archy of “Category:Applied sciences” not only cov-
ers Computer Science, but also related fields such as
Computational Linguistics and Mathematics.
Both the number of nodes and number of edges
in the hierarchy grow exponentially4 as the depth
increases. Therefore, D need not be a big number
to cover most terms in the domain. We have found
the hierarchy speeds up the semantic measurement
dramatically and covers almost all the words in the
specific domain. In our experiment on CS courses
(D=6), we eliminated over 71% of Wikipedia arti-
cles,5 yet the hierarchy covered over 90% of CS ter-
minologies mentioned in the course descriptions.
</bodyText>
<subsectionHeader confidence="0.999553">
3.2 Semantic Relatedness Between Concepts
</subsectionHeader>
<bodyText confidence="0.999706666666667">
Similar to the work of Li et al. (2006), the seman-
tic relatedness between two Wikipedia concepts,6 t1
and t2 in the hierarchy is defined as:
</bodyText>
<equation confidence="0.963858333333333">
epd − e−pd
f′(t1,t2) = e−«p (α,� E [0, 1]), (1)
epd + e−pd
</equation>
<bodyText confidence="0.9984655">
where p is the shortest path between t1 and t2, and
d is the depth of the lowest common hypernym of t1
</bodyText>
<footnote confidence="0.999666166666667">
4In the hierarchy we built with “Category:Applied sciences”
as the root, the number of edges grows from 177,955 at D=4 to
494,039 at D=5 and 1,848,052 at D=6.
5The hierarchy contains 1,534,267 unique articles, as op-
posed to 5,329,186 articles in Wikipedia.
6Each concept corresponds to a Wikipedia page.
</footnote>
<page confidence="0.999201">
37
</page>
<bodyText confidence="0.999894857142857">
and t2 in the hierarchy (Section 3.1). This is differ-
ent from related work on semantic relatedness from
Wikipedia (Ponzetto and Strube, 2007) in that we
not only consider the shortest path (p) between two
concepts but also their common distance (d) from
the topic, which in turn emphasizes domain aware-
ness.
</bodyText>
<subsectionHeader confidence="0.970458">
3.3 Generate Course Description Features
</subsectionHeader>
<bodyText confidence="0.9258492">
The built-in redirection in Wikipedia is useful for
spelling corrections because variations of a term
redirect to the same page. To generate features from
a course description C, we start by generating n-
grams (n E [1, 3]) from C. We then query the redi-
rection data to fetch all pages that match any of the
n-grams.
The identified pages are still sparse. We therefore
query the title data to fetch those that match any of
the n-grams. Page topics are not discriminated in
this step. For example, unigram “Java” returns both
“Java (software platform)” and “Java (dance).”
Wikipedia contains a collection of disambigua-
tion pages. Each disambiguation page includes a list
of alternative uses of a term. Note that there are two
different Wikipedia disambiguation pages: explicit
and implicit. A page is explicit when the page ti-
tle is annotated by Wikipedia as “disambiguation,”
such as “Oil (disambiguation).” A page is implicit
when it is not so annotated, but points to a category
such as “Category:Disambiguation pages,” or “Cat-
egory:All disambiguation pages.” We iterate over
the pages fetched from the last step, using disam-
biguation pages to enrich and refine the features of a
course description.
Unlike the work of Mihalcea and Csomai (2007)
which uses the annotation in the page title of a con-
cept to perform WSD, our approach uses a page’s
parent category as a cue to the correct sense. Typi-
cally, the sense of a concept depends on the senses of
other concepts in the context. For example, a para-
graph on programming languages and data types
ensures that “data” more likely corresponds to a
page under “Category:Computer data” than one un-
der “Category:Star Trek.”
Algorithm 1 explains the steps to generate fea-
tures for a course C.
Given the C1 and C2 in section 1, their generated
features F1 and F2 are:
F,: Shortest path problem, Tree traversal, Spanning tree, Tree,
Analysis, List of algorithms, Completeness, Algorithm, Sort-
ing, Data structure, Structure, Design, Data.
F2: Unix, Social, Ethics, Object-oriented design, Computer
programming, C++, Object-oriented programming, Design.
Algorithm 1 Feature Generation (F) for Course C
</bodyText>
<listItem confidence="0.965058038461539">
1. Tc +— 0 (clear terms), Ta +— 0 (ambiguous terms).
2. Generate all possible n-grams (n E [1, 3]) G from C.
3. Fetch the pages whose titles match any of g E G
from Wikipedia redirection data. For each page pid
of term t, Tc +— Tc U It : pid}.
4. Fetch the pages whose titles match any of g E G
from Wikipedia page title data. If a disambiguation
page, include all the terms this page refers to. If a
page pid corresponds to a term t that is not ambigu-
ous, Tc +— Tc U It : pid}, else Ta +— Ta U It : pid}.
5. For each term ta E Ta, find the disambiguation that
is on average most related (Equation 1) to the set of
clear terms. If a page pid of ta is on average the most
related to the terms in Tc, and the relatedness score is
above a threshold S (S E [0,1]), set Tc +— Tc U Ita :
pid}. If ta and a clear term are different senses of the
same term, keep the one that is more related to all the
other clear terms.
6. Return clear terms as features.
Algorithm 2 Semantic Vector SV1 for F1 and J
1. for all words ti E J do
2. if ti E F1, set SV1i = 1 where SV1i E SV1.
3. if ti E� F1, the semantic relatedness between ti and
each term t1j E F1 is calculated (Equation 1). Set
SV1i to the highest score if the score exceeds the
preset threshold S, otherwise SV1i = 0.
</listItem>
<sectionHeader confidence="0.692658" genericHeader="method">
4. end for
</sectionHeader>
<subsectionHeader confidence="0.934677">
3.4 Determine Course Relatedness
</subsectionHeader>
<bodyText confidence="0.998386">
Given two course descriptions C1 and C2, we use
Algorithm 1 to generate features F1 for C1, and F2
for C2. Next, the two feature lists are joined together
into a unique set of terms, namely J. Similar to pre-
vious work (Li et al., 2006), semantic vectors SV1
(Algorithm 2) and SV2 are computed for F1 and F2.
Each value of an entry of SV1 for features F1 is
reweighed as:
</bodyText>
<equation confidence="0.993036">
SV1i = SV1i · I(ti) · I(tj), (2)
</equation>
<bodyText confidence="0.9982978">
where SV1i is the semantic relatedness between ti E
F1 and tj E J. I(ti) is the information content of ti,
and I(tj) is the information content of tj. Similarly,
we reweigh each value for the semantic vector SV2
of F2.
</bodyText>
<page confidence="0.997968">
38
</page>
<bodyText confidence="0.999185">
The information content I(t) of a term t is a
weighed sum of the category information content
I,(t) and the linkage information content Il(t):
</bodyText>
<equation confidence="0.999143">
I(t) = ,Y · I-(t) + (1 − ,Y) · Il(t). (3)
</equation>
<bodyText confidence="0.998122333333333">
Inspired by related work (Seco et al., 2004), the
category information content of term t is redefined
as a function of its siblings:
</bodyText>
<equation confidence="0.9999025">
log(siblings(t) + 1)
I�(t) = 1
</equation>
<bodyText confidence="0.9991645">
where siblings(t) is the number of siblings for t on
average, and N is the total number of terms in the
hierarchy (Section 3.1).
The linkage information content is a function of
outlinks and inlinks of the page pid that t corre-
sponds to:
</bodyText>
<equation confidence="0.9996635">
inlinks(pid) outlinks(pid)
Il(t) = 1 − MAXIN ·MAXOUT , (5)
</equation>
<bodyText confidence="0.999931857142857">
where inlinks(pid) and outlinks(pid) are the
numbers of inlinks and outlinks of a page pid.
MAXIN and MAXOUT are the maximum num-
bers of inlinks and outlinks that a page has in
Wikipedia.7
Finally, the relatedness of two courses is a cosine
coefficient of the two semantic vectors:
</bodyText>
<equation confidence="0.998708">
SV1 · SV2
f(C1, C2) = (6)
||SV1 ||· ||SV2||.
</equation>
<sectionHeader confidence="0.995757" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.995286789473684">
Wikipedia offers its content as database backup
dumps (wikidumps) freely available to download.
Our application is based on the English wikidump8
of July 2011. We have extracted redirections, ti-
tles, categories, and links from the wikidump into
separate tables in MySQL. Using the steps outlined
in Section 3.1, we built a table for the hierarchy
with “Category:Applied sciences” as the root. The
attributes of each table were indexed to speed up
queries. Our experiment used α = 0.2, Q = 0.5,
6 = 0.2, and -y = 0.6. These values were found
7The computation of MAXIN and MAXOUT could
be time-consuming. They are therefore based on the entire
Wikipedia instead of the constructed hierarchy to avoid the re-
calculation when the domain changes. This also ensures the
maximum linkage information is unbiased for every domain.
For the July 2011 wikidump, page “Geographic coordinate sys-
tem” has the most in-links, a total of 575,277. Page “List of Ital-
ian communes (2009)” has the most out-links, a total of 8,103.
</bodyText>
<footnote confidence="0.675295">
8http://dumps.wikimedia.org/enwiki/20110722/
</footnote>
<bodyText confidence="0.999441208333333">
empirically to perform well over randomly selected
samples.
We randomly selected 25 CS courses from 19
universities that can be transferred to University
of Massachusetts Lowell (UML) according to the
transfer dictionary. Each transfer course was com-
pared to all 44 CS courses offered at UML, a to-
tal of 1,100 comparisons. The result was consid-
ered correct for each course if the real equivalent
course in UML appears among the top 3 in the list
of highest scores. We excluded all Wikipedia pages
whose titles contained specific dates or were anno-
tated as “magazine”, “journal”, or “album.” We re-
moved both general and domain stop words (such
as “course,” “book,” and “student”) from course de-
scriptions. If a course description contains the key-
words “not” or “no,” e.g., “This course requires no
computer programming skills,” the segment after
such keyword is ignored.
We tested our approach against the work by Li
et al. (2006) and TF-IDF on the same data set of
course descriptions. The accuracy of our proposed
approach is 72%, compared to 52% using Li et al.
(2006), and 32% using TF-IDF.
</bodyText>
<table confidence="0.99823025">
Algorithm Pearson’s correlation P-value
Our approach 0.85 6.6 · 10−10
Li et al. (2006) 0.57 0.0006
TF-IDF 0.73 2 · 10−6
</table>
<tableCaption confidence="0.980258">
Table 1. Pearson’s correlation of course relatedness
scores with human judgments.
</tableCaption>
<bodyText confidence="0.9995599375">
Since the transfer dictionary is always out of date,
we found a few equivalent course pairs that were un-
intuitive. To make a more meaningful evaluation,
we set up a human judgment data set. We gave
6 annotators (CS students and professors) a list of
32 pairs of courses, with only course titles and de-
scriptions. They independently evaluated whether
each pair is equivalent on a scale from 1 to 5. We
averaged their evaluations for each pair and con-
verted the scale from [1,5] to [0,1]. Next, we ran
our approach, the work by Li et al. (2006), and TF-
IDF on the same 32 course pairs. Table 1 reports
the Pearson’s correlation coefficient of course relat-
edness scores with human judgment, and statistical
significances. Our approach has a higher correlation
to the human judgment data set compared to previ-
</bodyText>
<equation confidence="0.978358">
log(N)
, (4)
</equation>
<page confidence="0.996027">
39
</page>
<bodyText confidence="0.999941">
ous work. Furthermore, a smaller p-value indicates
our approach is more likely to correlate with human
judgment.
During the experiment, we have found some mis-
classified categories in the wikidump.9 For example,
“Category:Software” has over 350 subcategories
with names similar to “Category:A-Class Britney
Spears articles,” or “Category:FA-Class Coca-Cola
articles.” None of these appears in the Wikipedia
website or the Wikipedia API10 as a subcategory
of “Category:Software.” More study is required on
how they are formed.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982875">
This paper presents a domain-specific algorithm to
suggest equivalent courses based on analyzing their
semantic relatedness using Wikipedia. Both accu-
racy and correlation suggest our approach outper-
forms previous work. Future work includes com-
paring our approach with ESA, experimenting on
more courses from more universities, and adapting
our work to courses in other languages.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999991">
The authors thank Dr. Karen M. Daniels for review-
ing drafts of this paper. We also appreciate the in-
sightful suggestions from Dr. Saif Mohammad at the
early stage of our work. Last, but not least, we thank
the reviewers for their comments that guided im-
provement of the contents of this paper.
</bodyText>
<sectionHeader confidence="0.998551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995366">
Gergana Bounova. 2011. Topological Evolution of Net-
works: Case Studies in the US Airlines and Language
Wikipedias. Ph.D. thesis, MIT.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating Wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13–47.
David Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from Wikipedia from the question answering
task. In Proc. 5th Int’l. Conf. on Language Resources
&amp; Evaluation, Genoa, Italy.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Trans. on Knowledge
&amp; Data Engineering, 19:370–383.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, Univ. of Edinburgh.
</reference>
<footnote confidence="0.799778333333333">
9We have analyzed wikidumps of July 2011 and Oct 2010
and the problem persists in both versions.
10https://www.mediawiki.org/wiki/API
</footnote>
<reference confidence="0.998786">
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for NLP. J.
AI Research, 34:443–498.
Graeme Hirst and David St-Onge, 1998. Lexical Chains
as Representation of Context for the Detection and
Correction Malapropisms. The MIT Press.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proc. Int’l. Conf. on Research in Computa-
tional Linguistics, pages 19–33.
Claudia Leacock and Martin Chodorow. 1998. Using
corpus statistics and Wordnet relations for sense iden-
tification. Computational Linguistics, 24:147–165.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Trans. on Knowledge and Data Engineering, 18.
Dekang Lin. 1998. Extracting collocations from text cor-
pora. In Workshop on Computational Terminology.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Proc.
16th ACM Conf. on Information &amp; Knowledge Man-
agement, pages 233–242.
Saif Mohammad. 2008. Measuring Semantic Distance
Using Distributional Profiles of Concepts. Ph.D. the-
sis, Univ. of Toronto, Toronto, Canada.
Yuan Ni, Lei Zhang, Zhaoming Qiu, and Wang Chen.
2010. Enhancing the open-domain classification of
named entity using linked open data. In Proc. 9th Int’l.
Conf. on the Semantic Web, pages 566–581.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing se-
mantic relatedness. J. AI Research, 30:181–212, Oc-
tober.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proc. 15th Int’l. Conf. on WWW.
Nuno Seco, Tony Veale, and Jer Hayes. 2004. An intrin-
sic information content metric for semantic similarity
in Wordnet. In Proc. 16th European Conf. on AI.
Fei Wu. 2010. Machine Reading: from Wikipedia to the
Web. Ph.D. thesis, Univ. of Washington.
Beibei Yang and Jesse M. Heines. 2011. Using seman-
tic distance to automatically suggest transfer course
equivalencies. In Proc. 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 142–151.
Jonathan Yu, James A. Thom, and Audrey Tam. 2007.
Ontology evaluation using Wikipedia categories for
browsing. In Proc. 16th ACM Conf. on Information
and Knowledge Management, pages 223–232.
</reference>
<page confidence="0.99857">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.105639">
<title confidence="0.8280245">Domain-Specific Semantic Relatedness From Can A Course Be Transferred?</title>
<author confidence="0.403023">Beibei</author>
<affiliation confidence="0.998239">University of Massachusetts</affiliation>
<author confidence="0.568787">MA Lowell</author>
<email confidence="0.98917">byang1@cs.uml.edu</email>
<author confidence="0.999283">M Jesse</author>
<affiliation confidence="0.999343">University of Massachusetts</affiliation>
<address confidence="0.532998">Lowell, MA</address>
<email confidence="0.999355">heines@cs.uml.edu</email>
<abstract confidence="0.995720826086956">Semantic relatedness, or its inverse, semantic distance, measures the degree of closeness between two pieces of text determined by their meaning. Related work typically measures semantics based on a sparse knowledge such as CYC that requires intensive manual efforts to build and maintain. Other work is based on the Brown corpus, or more recently, Wikipedia. Wikipediabased measures, however, typically do not take into account the rapid growth of that resource, which exponentially increases the time to prepare and query the knowledge base. Furthermore, the generalized knowledge domain may be difficult to adapt to a specific domain. To address these problems, this paper proposes a domain-specific semantic relatedness measure based on part of Wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another. We show that our results perform well when compared to previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gergana Bounova</author>
</authors>
<title>Topological Evolution of Networks:</title>
<date>2011</date>
<booktitle>Case Studies in the US Airlines and Language Wikipedias. Ph.D. thesis, MIT.</booktitle>
<contexts>
<context position="4316" citStr="Bounova, 2011" startWordPosition="634" endWordPosition="635">s, advanced data structures, graph algorithms (shortest path, spanning trees, tree traversals), matrix operations, string matching, NP completeness. 9 C2: [Computing III] Object-oriented programming. Classes, methods, polymorphism, inheritance. Objectoriented design. C++. UNIX. Ethical and social issues. Fragments of WordNet and Wikipedia Taxonomies Figure 1. Fragments of WordNet 3.0 (top) and English Wikipedia of 2011/7 (bottom) taxonomies. The root/centroid node is shown in red. We choose Wikipedia as the knowledge base due to its rich contents (Figure 1) and continuously coalescent growth (Bounova, 2011). Although Wikipedia was launched 10 years later, it grew much faster than WordNet over the last decade (Figure 2). The contributions of this paper are twofold. First, we address the problem of domain-specific semantic relatedness using Wikipedia. We propose a method to suggest course equivalencies by computing semantic relatedness among Computer Science course descriptions. Our approach can be easily modified for other majors and even other languages. Second, we evaluate the correlation of our approach and a human judgment data set we built. Both accuracy and correlation indicate that our app</context>
</contexts>
<marker>Bounova, 2011</marker>
<rawString>Gergana Bounova. 2011. Topological Evolution of Networks: Case Studies in the US Airlines and Language Wikipedias. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating Wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--13</pages>
<contexts>
<context position="5314" citStr="Budanitsky and Hirst, 2006" startWordPosition="776" endWordPosition="779">tions. Our approach can be easily modified for other majors and even other languages. Second, we evaluate the correlation of our approach and a human judgment data set we built. Both accuracy and correlation indicate that our approach outperforms previous work. 2 Related Research Semantic relatedness has been used in applications such as word sense disambiguation, named entity disambiguation, text summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating Wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32:13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>Mining knowledge from Wikipedia from the question answering task.</title>
<date>2006</date>
<booktitle>In Proc. 5th Int’l. Conf. on Language Resources &amp; Evaluation,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7055" citStr="Buscaldi and Rosso, 2006" startWordPosition="1039" endWordPosition="1042">ical courses. Due to the WordNet sparsity on technical terms, the experiment does not perform well on Computer Science courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic</context>
</contexts>
<marker>Buscaldi, Rosso, 2006</marker>
<rawString>David Buscaldi and Paolo Rosso. 2006. Mining knowledge from Wikipedia from the question answering task. In Proc. 5th Int’l. Conf. on Language Resources &amp; Evaluation, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi L Cilibrasi</author>
<author>Paul M B Vitanyi</author>
</authors>
<title>The google similarity distance.</title>
<date>2007</date>
<journal>IEEE Trans. on Knowledge &amp; Data Engineering,</journal>
<pages>19--370</pages>
<contexts>
<context position="5712" citStr="Cilibrasi and Vitanyi, 2007" startWordPosition="840" endWordPosition="843">ext summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The google similarity distance. IEEE Trans. on Knowledge &amp; Data Engineering, 19:370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Edinburgh.</institution>
<contexts>
<context position="6051" citStr="Curran, 2004" startWordPosition="892" endWordPosition="893">and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the experiment is based on non-technical courses. Due to the WordNet sparsity on technical terms, the experiment does not perform well on Computer Science courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, Univ. of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Wikipedia-based semantic interpretation for NLP.</title>
<date>2009</date>
<journal>J. AI Research,</journal>
<pages>34--443</pages>
<contexts>
<context position="7444" citStr="Gabrilovich and Markovitch (2009)" startWordPosition="1096" endWordPosition="1099"> 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wikipedia. While Mihalcea and Csomai (2007) use the annotation in the page title of a concept to perform WSD, our approach uses a page’s parent category as a cue to the correct sense. Ponzetto and Strube (2007) limit their measurement to word pairs, while our work focuses on text of any length. Gabrilovich and Markovitch (2009) </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2009</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based semantic interpretation for NLP. J. AI Research, 34:443–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical Chains as Representation of Context for the Detection and Correction Malapropisms.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5457" citStr="Hirst and St-Onge, 1998" startWordPosition="799" endWordPosition="802">man judgment data set we built. Both accuracy and correlation indicate that our approach outperforms previous work. 2 Related Research Semantic relatedness has been used in applications such as word sense disambiguation, named entity disambiguation, text summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Moha</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge, 1998. Lexical Chains as Representation of Context for the Detection and Correction Malapropisms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="5890" citStr="Jiang and Conrath, 1997" startWordPosition="867" endWordPosition="870">emantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the experiment is based on non-technical courses. Due to the WordNet sparsity on technical terms</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Using corpus statistics and Wordnet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--147</pages>
<contexts>
<context position="5431" citStr="Leacock and Chodorow, 1998" startWordPosition="794" endWordPosition="798">ion of our approach and a human judgment data set we built. Both accuracy and correlation indicate that our approach outperforms previous work. 2 Related Research Semantic relatedness has been used in applications such as word sense disambiguation, named entity disambiguation, text summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Using corpus statistics and Wordnet relations for sense identification. Computational Linguistics, 24:147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Trans. on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Trans. on Knowledge and Data Engineering, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Extracting collocations from text corpora.</title>
<date>1998</date>
<booktitle>In Workshop on Computational Terminology.</booktitle>
<contexts>
<context position="5801" citStr="Lin, 1998" startWordPosition="857" endWordPosition="858">uation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the exp</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Extracting collocations from text corpora. In Workshop on Computational Terminology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proc. 16th ACM Conf. on Information &amp; Knowledge Management,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="7116" citStr="Mihalcea and Csomai, 2007" startWordPosition="1047" endWordPosition="1051">, the experiment does not perform well on Computer Science courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wi</context>
<context position="13178" citStr="Mihalcea and Csomai (2007)" startWordPosition="2040" endWordPosition="2043"> pages. Each disambiguation page includes a list of alternative uses of a term. Note that there are two different Wikipedia disambiguation pages: explicit and implicit. A page is explicit when the page title is annotated by Wikipedia as “disambiguation,” such as “Oil (disambiguation).” A page is implicit when it is not so annotated, but points to a category such as “Category:Disambiguation pages,” or “Category:All disambiguation pages.” We iterate over the pages fetched from the last step, using disambiguation pages to enrich and refine the features of a course description. Unlike the work of Mihalcea and Csomai (2007) which uses the annotation in the page title of a concept to perform WSD, our approach uses a page’s parent category as a cue to the correct sense. Typically, the sense of a concept depends on the senses of other concepts in the context. For example, a paragraph on programming languages and data types ensures that “data” more likely corresponds to a page under “Category:Computer data” than one under “Category:Star Trek.” Algorithm 1 explains the steps to generate features for a course C. Given the C1 and C2 in section 1, their generated features F1 and F2 are: F,: Shortest path problem, Tree t</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proc. 16th ACM Conf. on Information &amp; Knowledge Management, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Measuring Semantic Distance Using Distributional Profiles of Concepts.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="6068" citStr="Mohammad, 2008" startWordPosition="894" endWordPosition="895">998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the experiment is based on non-technical courses. Due to the WordNet sparsity on technical terms, the experiment does not perform well on Computer Science courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Techn</context>
</contexts>
<marker>Mohammad, 2008</marker>
<rawString>Saif Mohammad. 2008. Measuring Semantic Distance Using Distributional Profiles of Concepts. Ph.D. thesis, Univ. of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ni</author>
<author>Lei Zhang</author>
<author>Zhaoming Qiu</author>
<author>Wang Chen</author>
</authors>
<title>Enhancing the open-domain classification of named entity using linked open data. In</title>
<date>2010</date>
<booktitle>Proc. 9th Int’l. Conf. on the Semantic Web,</booktitle>
<pages>566--581</pages>
<contexts>
<context position="7162" citStr="Ni et al., 2010" startWordPosition="1055" endWordPosition="1058">ce courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wikipedia. While Mihalcea and Csomai (2007) use </context>
</contexts>
<marker>Ni, Zhang, Qiu, Chen, 2010</marker>
<rawString>Yuan Ni, Lei Zhang, Zhaoming Qiu, and Wang Chen. 2010. Enhancing the open-domain classification of named entity using linked open data. In Proc. 9th Int’l. Conf. on the Semantic Web, pages 566–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>J. AI Research,</journal>
<pages>30--181</pages>
<contexts>
<context position="7290" citStr="Ponzetto and Strube, 2007" startWordPosition="1075" endWordPosition="1078">echnology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wikipedia. While Mihalcea and Csomai (2007) use the annotation in the page title of a concept to perform WSD, our approach uses a page’s parent category as a cue to the correct</context>
<context position="11725" citStr="Ponzetto and Strube, 2007" startWordPosition="1801" endWordPosition="1804"> is defined as: epd − e−pd f′(t1,t2) = e−«p (α,� E [0, 1]), (1) epd + e−pd where p is the shortest path between t1 and t2, and d is the depth of the lowest common hypernym of t1 4In the hierarchy we built with “Category:Applied sciences” as the root, the number of edges grows from 177,955 at D=4 to 494,039 at D=5 and 1,848,052 at D=6. 5The hierarchy contains 1,534,267 unique articles, as opposed to 5,329,186 articles in Wikipedia. 6Each concept corresponds to a Wikipedia page. 37 and t2 in the hierarchy (Section 3.1). This is different from related work on semantic relatedness from Wikipedia (Ponzetto and Strube, 2007) in that we not only consider the shortest path (p) between two concepts but also their common distance (d) from the topic, which in turn emphasizes domain awareness. 3.3 Generate Course Description Features The built-in redirection in Wikipedia is useful for spelling corrections because variations of a term redirect to the same page. To generate features from a course description C, we start by generating ngrams (n E [1, 3]) from C. We then query the redirection data to fetch all pages that match any of the n-grams. The identified pages are still sparse. We therefore query the title data to f</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Knowledge derived from Wikipedia for computing semantic relatedness. J. AI Research, 30:181–212, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proc. 15th Int’l. Conf. on WWW.</booktitle>
<contexts>
<context position="5682" citStr="Sahami and Heilman, 2006" startWordPosition="836" endWordPosition="839">d entity disambiguation, text summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation. WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006). A WordNet-based method uses one or more edgecounting techniques in the WordNet taxonomy (Leacock and Chodorow, 1998; Hirst and St-Onge, 1998). The relatedness of two concept nodes is a function of the minimum number of hops between them. Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the ove</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In Proc. 15th Int’l. Conf. on WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Seco</author>
<author>Tony Veale</author>
<author>Jer Hayes</author>
</authors>
<title>An intrinsic information content metric for semantic similarity in Wordnet.</title>
<date>2004</date>
<booktitle>In Proc. 16th European Conf. on AI.</booktitle>
<contexts>
<context position="16240" citStr="Seco et al., 2004" startWordPosition="2630" endWordPosition="2633">semantic vectors SV1 (Algorithm 2) and SV2 are computed for F1 and F2. Each value of an entry of SV1 for features F1 is reweighed as: SV1i = SV1i · I(ti) · I(tj), (2) where SV1i is the semantic relatedness between ti E F1 and tj E J. I(ti) is the information content of ti, and I(tj) is the information content of tj. Similarly, we reweigh each value for the semantic vector SV2 of F2. 38 The information content I(t) of a term t is a weighed sum of the category information content I,(t) and the linkage information content Il(t): I(t) = ,Y · I-(t) + (1 − ,Y) · Il(t). (3) Inspired by related work (Seco et al., 2004), the category information content of term t is redefined as a function of its siblings: log(siblings(t) + 1) I�(t) = 1 where siblings(t) is the number of siblings for t on average, and N is the total number of terms in the hierarchy (Section 3.1). The linkage information content is a function of outlinks and inlinks of the page pid that t corresponds to: inlinks(pid) outlinks(pid) Il(t) = 1 − MAXIN ·MAXOUT , (5) where inlinks(pid) and outlinks(pid) are the numbers of inlinks and outlinks of a page pid. MAXIN and MAXOUT are the maximum numbers of inlinks and outlinks that a page has in Wikiped</context>
</contexts>
<marker>Seco, Veale, Hayes, 2004</marker>
<rawString>Nuno Seco, Tony Veale, and Jer Hayes. 2004. An intrinsic information content metric for semantic similarity in Wordnet. In Proc. 16th European Conf. on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
</authors>
<title>Machine Reading: from Wikipedia to the Web.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Washington.</institution>
<contexts>
<context position="7226" citStr="Wu, 2010" startWordPosition="1068" endWordPosition="1069"> # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wikipedia. While Mihalcea and Csomai (2007) use the annotation in the page title of a concept to perform WSD, ou</context>
</contexts>
<marker>Wu, 2010</marker>
<rawString>Fei Wu. 2010. Machine Reading: from Wikipedia to the Web. Ph.D. thesis, Univ. of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beibei Yang</author>
<author>Jesse M Heines</author>
</authors>
<title>Using semantic distance to automatically suggest transfer course equivalencies.</title>
<date>2011</date>
<booktitle>In Proc. 6th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="6330" citStr="Yang and Heines (2011)" startWordPosition="932" endWordPosition="935">007). Two words are likely to be related if they co-occur within similar contexts (Lin, 1998). Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997). It has been shown that these composite methods generally outperform lexicographic resource- and corpus- based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008). Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity. Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the experiment is based on non-technical courses. Due to the WordNet sparsity on technical terms, the experiment does not perform well on Computer Science courses. # nodes: 25 WordNet [Root: synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, th</context>
</contexts>
<marker>Yang, Heines, 2011</marker>
<rawString>Beibei Yang and Jesse M. Heines. 2011. Using semantic distance to automatically suggest transfer course equivalencies. In Proc. 6th Workshop on Innovative Use of NLP for Building Educational Applications, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Yu</author>
<author>James A Thom</author>
<author>Audrey Tam</author>
</authors>
<title>Ontology evaluation using Wikipedia categories for browsing.</title>
<date>2007</date>
<booktitle>In Proc. 16th ACM Conf. on Information and Knowledge Management,</booktitle>
<pages>223--232</pages>
<contexts>
<context position="7201" citStr="Yu et al., 2007" startWordPosition="1061" endWordPosition="1064">synset(“technology”), #depth: 2] # nodes: 3583 Wikipedia [Centroid: “Category:Technology”, #steps: 2] 1992 1996 2000 2004 2008 2012 Year Figure 2. Growth of Wikipedia and WordNet Growth of English Wikipedia and WordNet 4000000 3500000 3000000 2500000 2000000 1500000 1000000 500000 Articles in Wikipedia Synsets in WordNet 36 In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Csomai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007). Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph. Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter. Our approach is different from prior work on Wikipedia. While Mihalcea and Csomai (2007) use the annotation in the page title of a c</context>
</contexts>
<marker>Yu, Thom, Tam, 2007</marker>
<rawString>Jonathan Yu, James A. Thom, and Audrey Tam. 2007. Ontology evaluation using Wikipedia categories for browsing. In Proc. 16th ACM Conf. on Information and Knowledge Management, pages 223–232.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>