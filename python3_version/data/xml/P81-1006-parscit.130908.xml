<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.358458">
<sectionHeader confidence="0.967452" genericHeader="abstract">
EVALUATION OF NATURAL LANGUAGE INTERFACES TO DATABASE SYSTEMS:
A PANEL DISCUSSION
</sectionHeader>
<bodyText confidence="0.97625218918919">
Norman K. Sondheimer, Chair
Sperry Univac
Blue Bell, PA
For a natural language access to database
system to be practical it must achieve a good
match between the capabilities of the user
and the requirements of the task. The user
brings his own natural language and his own
style of interaction to the system. The task
brings the questions that must be answered
and the database domain&apos;s semantics. All
natural language access systems achieve some
degree of success. But to make progress as a
field, we need to be able to evaluate the
degree of this success.
For too long, the best we have managed has
been to produce a list of typical questions
or linguistic phenomena that a system
correctly processed. Missing has been a
discussion of their importance and a similar
list of unhandled phenomena. Only
occasionally were even informal evaluations
of systems conducted.
Recently, this has begun to change. In the
last several years, many of the current
generation of natural language access to
database systems have been subject to
laboratory or field testing. These include
INTELLECT, LADDER, PLANES, REL and TOA. We
have begun to discover what a user will ask a
system, how he reacts to its limits, and
where we need further work.
This panel brings together a good sampling of
the people involved in these tests including
individuals intimately involved with the
above systems. The position papers that
follow present their unique viewpoints on the
important issues in the evaluation of natural
language access to database systems. These
include:
I. What has been learned about a) user
needs, b)system&apos;s capabilities and c) their
match with respect to tasks. Under this,
what are the most important linguistic
phenomena to allow for? What other kinds of
interactions, beside retrievals, do users
request? How good are systems at satisfying
users? How good are users at finding ways to
use systems? How satisfied are users with
systems performance? How does these results
vary with respect to tasks?
/I. What have we learned about running
evaluations? Under this, what methodologies
are capable of revealing what sorts of facts?
What are the limits of field studies versus
controlled experiments? How good are studies
with a simulated system, such as Malhotra&apos;s
with its human intermediary[1]? What are the
independent variables that must be allowed
for? What tools are available to determine
user bias and experience beforehand, and user
satisfaction afterward?
III. On the basis of these evaluations, what
should the future look like for natural
language access to database? Under this
point, what niches look most promising for
natural language interfaces? What standards
should be set for natural language systems
performance? What kinds of evaluations
should be run in the future? How should they
be designed and how should they be judged?
In addition to the position papers that
follow, I strongly urge you to consult the
panelist more extensive publications.
</bodyText>
<sectionHeader confidence="0.902376" genericHeader="introduction">
Bibliography
</sectionHeader>
<reference confidence="0.997981">
(I] Malhotra, A., &amp;quot;Design Criteria for a
Knowledge-Based English Language System for
Management: An Experimental Analysis&amp;quot;, Ph.D.
Thesis, MIT, MAC TR-146, 1975.
</reference>
<page confidence="0.999114">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.055629">
<title confidence="0.9993755">EVALUATION OF NATURAL LANGUAGE INTERFACES TO DATABASE SYSTEMS: A PANEL DISCUSSION</title>
<author confidence="0.996454">Norman K Sondheimer</author>
<author confidence="0.996454">Chair</author>
<affiliation confidence="0.522641">Sperry Univac</affiliation>
<address confidence="0.480229">Blue Bell, PA</address>
<abstract confidence="0.999159591549296">For a natural language access to database system to be practical it must achieve a good match between the capabilities of the user and the requirements of the task. The user brings his own natural language and his own style of interaction to the system. The task brings the questions that must be answered and the database domain&apos;s semantics. All natural language access systems achieve some degree of success. But to make progress as a we be able to evaluate the degree of this success. For too long, the best we have managed has been to produce a list of typical questions or linguistic phenomena that a system correctly processed. Missing has been a discussion of their importance and a similar list of unhandled phenomena. Only occasionally were even informal evaluations of systems conducted. Recently, this has begun to change. In the last several years, many of the current generation of natural language access to database systems have been subject to laboratory or field testing. These include INTELLECT, LADDER, PLANES, REL and TOA. We have begun to discover what a user will ask a system, how he reacts to its limits, and where we need further work. This panel brings together a good sampling of the people involved in these tests including individuals intimately involved with the above systems. The position papers that follow present their unique viewpoints on the important issues in the evaluation of natural language access to database systems. These include: I. What has been learned about a) user needs, b)system&apos;s capabilities and c) their match with respect to tasks. Under this, what are the most important linguistic phenomena to allow for? What other kinds of interactions, beside retrievals, do users request? How good are systems at satisfying users? How good are users at finding ways to use systems? How satisfied are users with systems performance? How does these results vary with respect to tasks? /I. What have we learned about running evaluations? Under this, what methodologies are capable of revealing what sorts of facts? What are the limits of field studies versus controlled experiments? How good are studies with a simulated system, such as Malhotra&apos;s with its human intermediary[1]? What are the independent variables that must be allowed for? What tools are available to determine user bias and experience beforehand, and user satisfaction afterward? III. On the basis of these evaluations, what should the future look like for natural language access to database? Under this point, what niches look most promising for natural language interfaces? What standards should be set for natural language systems performance? What kinds of evaluations should be run in the future? How should they be designed and how should they be judged? In addition to the position papers that follow, I strongly urge you to consult the panelist more extensive publications.</abstract>
<note confidence="0.526596333333333">Bibliography (I] Malhotra, A., &amp;quot;Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis&amp;quot;, Ph.D. Thesis, MIT, MAC TR-146, 1975. 29</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis&amp;quot;,</title>
<date>1975</date>
<booktitle>Ph.D. Thesis, MIT, MAC TR-146,</booktitle>
<marker>1975</marker>
<rawString>(I] Malhotra, A., &amp;quot;Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis&amp;quot;, Ph.D. Thesis, MIT, MAC TR-146, 1975.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>