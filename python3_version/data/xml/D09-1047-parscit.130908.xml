<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009854">
<title confidence="0.995949">
Joint Learning of Preposition Senses and
Semantic Roles of Prepositional Phrases
</title>
<author confidence="0.994491">
Daniel Dahlmeier&apos;, Hwee Tou Ng&apos;,2, Tanja Schultz3
</author>
<affiliation confidence="0.971407666666667">
&apos;NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
3Cognitive Systems Lab, University of Karlsruhe
</affiliation>
<email confidence="0.9868665">
{danielhe,nght}@comp.nus.edu.sg
tanja@ira.uka.de
</email>
<sectionHeader confidence="0.993607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998874">
The sense of a preposition is related to the
semantics of its dominating prepositional
phrase. Knowing the sense of a prepo-
sition could help to correctly classify the
semantic role of the dominating preposi-
tional phrase and vice versa. In this pa-
per, we propose a joint probabilistic model
for word sense disambiguation of preposi-
tions and semantic role labeling of prepo-
sitional phrases. Our experiments on the
PropBank corpus show that jointly learn-
ing the word sense and the semantic role
leads to an improvement over state-of-the-
art individual classifier models on the two
tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986675862069">
Word sense disambiguation (WSD) and seman-
tic role labeling (SRL) are two key components
in natural language processing to find a semantic
representation for a sentence. Semantic role la-
beling is the task of determining the constituents
of a sentence that represent semantic arguments
with respect to a predicate and labeling each with
a semantic role. Word sense disambiguation tries
to determine the correct meaning of a word in a
given context. Ambiguous words occur frequently
in normal English text.
One word class which is both frequent and
highly ambiguous is preposition. The different
senses of a preposition express different relations
between the preposition complement and the rest
of the sentence. Semantic roles and word senses
offer two different inventories of “meaning” for
prepositional phrases (PP): semantic roles distin-
guish between different verb complements while
word senses intend to fully capture the preposition
semantics at a more fine-grained level. In this pa-
per, we use the semantic roles from the PropBank
corpus and the preposition senses from the Prepo-
sition Project (TPP). Both corpora are explained
in more detail in the following section. The re-
lationship between the two inventories (PropBank
semantic roles and TPP preposition senses) is not
a simple one-to-one mapping, as we can see from
the following examples:
</bodyText>
<listItem confidence="0.8721015">
• She now lives with relatives [insense1
Alabama.]ARGM-LOC
• The envelope arrives [insense1 the mail.]ARG4
• [Insense5 separate statements]ARGM-LOC the two
sides said they want to have “further discus-
sions.”
</listItem>
<bodyText confidence="0.999933458333333">
In the first two examples, the sense of the preposi-
tion in is annotated as sense 1 (“surrounded by or
enclosed in”), following the definitions of the TPP,
but the semantic roles are different. In the first
example the semantic role is a locative adjunctive
argument (ARGM-LOC), while in the second ex-
ample it is ARG4 which denotes the “end point or
destination” of the arriving action1. In the first and
third example, the semantic roles are the same, but
the preposition senses are different, i.e., sense 1
and sense 5 (“inclusion or involvement”).
Preposition senses and semantic roles provide
two different views on the semantics of PPs.
Knowing the semantic role of the PP could be
helpful to successfully disambiguate the sense of
the preposition. Likewise, the preposition sense
could provide valuable information to classify the
semantic role of the PP. This is especially so for
the semantic roles ARGM-LOC and ARGM-TMP,
where we expect a strong correlation with spatial
and temporal preposition senses respectively.
In this paper, we propose a probabilistic model
for joint inference on preposition senses and se-
mantic roles. For each prepositional phrase that
</bodyText>
<footnote confidence="0.979575">
1http://verbs.colorado.edu/framesets/arrive-v.html
</footnote>
<page confidence="0.903762">
450
</page>
<note confidence="0.996622">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450–458,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999963875">
has been identified as an argument of the pred-
icate, we jointly infer its semantic role and the
sense of the preposition that is the lexical head of
the prepositional phrase. That is, our model maxi-
mizes the joint probability of the semantic role and
the preposition sense.
Previous research has shown the benefit of
jointly learning semantic roles of multiple con-
stituents (Toutanova et al., 2008; Koomen et al.,
2005). In contrast, our joint model makes pre-
dictions for a single constituent, but multiple tasks
(WSD and SRL) .
Our experiments show that adding the SRL
information leads to statistically significant im-
provements over an independent, state-of-the-art
WSD classifier. For the SRL task, we show statis-
tically significant improvements of our joint model
over an independent, state-of-the-art SRL clas-
sifier for locative and temporal adjunctive argu-
ments, even though the overall improvement over
all semantic roles is small. To the best of our
knowledge, no previous research has attempted to
perform preposition WSD and SRL of preposi-
tional phrases in a joint learning approach.
The remainder of this paper is structured as fol-
lows: First, we give an introduction to the WSD
and SRL task. Then, in Section 3, we describe the
individual and joint classifier models. The details
of the data set used in our experiments are given
in Section 4. In Section 5, we present experiments
and results. Section 6 summarizes related work,
before we conclude in the final section.
</bodyText>
<sectionHeader confidence="0.977998" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999517">
This section gives an introduction to preposition
sense disambiguation and semantic role labeling
of prepositional phrases.
</bodyText>
<subsectionHeader confidence="0.997871">
2.1 Preposition Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999954466666667">
The task of word sense disambiguation is to find
the correct meaning of a word, given its context.
Most prior research on word sense disambigua-
tion has focused on disambiguating the senses of
nouns, verbs, and adjectives, but not on preposi-
tions. Word sense disambiguation can be framed
as a classification task. For each preposition, a
classifier is trained on a corpus of training exam-
ples annotated with preposition senses, and tested
on a set of unseen test examples.
To perform WSD for prepositions, it is neces-
sary to first find a set of suitable sense classes.
We adopt the sense inventory from the Preposition
Project (TPP) (Litkowski and Hargraves, 2005)
that was also used in the SemEval 2007 preposi-
tion WSD task (Litkowski and Hargraves, 2007).
TPP is an attempt to create a comprehensive lex-
ical database of English prepositions that is suit-
able for use in computational linguistics research.
For each of the over 300 prepositions and phrasal
prepositions, the database contains a set of sense
definitions, which are based on the Oxford Dic-
tionary of English. Every preposition has a set
of fine-grained senses, which are grouped together
into a smaller number of coarse-grained senses. In
our experiments, we only focus on coarse-grained
senses since better inter-annotator agreement can
be achieved on coarse-grained senses, which also
results in higher accuracy of the trained WSD clas-
sifier.
</bodyText>
<subsectionHeader confidence="0.998928">
2.2 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999894419354839">
The task of semantic role labeling in the context
of PropBank (Palmer et al., 2005) is to label tree
nodes with semantic roles in a syntactic parse tree.
The PropBank corpus adds a semantic layer to
parse trees from the Wall Street Journal section of
the Penn Treebank II corpus (Marcus et al., 1993).
There are two classes of semantic roles: core argu-
ments and adjunctive arguments. Core arguments
are verb sense specific, i.e., their meaning is de-
fined relative to a specific verb sense. They are
labeled with consecutive numbers ARG0, ARG1,
etc. ARG0 usually denotes the AGENT and ARG1
the THEME of the event. Besides the core ar-
guments, a verb can have a number of adjunc-
tive arguments that express more general proper-
ties like time, location, or manner. They are la-
beled as ARGM plus a functional tag, e.g., LOC for
locative or TMP for temporal modifiers. Preposi-
tional phrases can appear as adjunctive arguments
or core arguments.
The standard approach to semantic role labeling
is to divide the task into two sequential sub-tasks:
identification and classification. During the identi-
fication phase, the system separates the nodes that
fill some semantic roles from the rest. During the
classification phase, the system assigns the exact
semantic roles for all nodes that are identified as
arguments. In this paper, we focus on the classi-
fication phase. That is, we assume that preposi-
tional phrases that are semantic arguments have
been identified correctly and concentrate on the
</bodyText>
<page confidence="0.998043">
451
</page>
<bodyText confidence="0.99699">
task of determining the semantic role of preposi-
tional phrases. The reason is that argument identi-
fication mostly relies on syntactic features, like the
path from the constituent to the predicate (Pradhan
et al., 2005). Consider, for example, the phrase in
the dark in the sentence: “We are in the dark”, he
said. The phrase is clearly not an argument to the
verb say. But if we alter the syntactic structure
of the sentence appropriately (while the sense of
the preposition in remains unchanged), the same
phrase suddenly becomes an adjunctive argument:
In the dark, he said “We are”. On the other hand,
we can easily find examples, where in has a differ-
ent sense, but the phrase always fills some seman-
tic role:
</bodyText>
<listItem confidence="0.999967666666667">
• In a separate manner, he said ...
• In 1998, he said ...
• In Washington, he said ...
</listItem>
<bodyText confidence="0.9995712">
This illustrates that the preposition sense is inde-
pendent of whether the PP is an argument or not.
Thus, a joint learning model for argument identifi-
cation and preposition sense is unlikely to perform
better than the independent models.
</bodyText>
<sectionHeader confidence="0.992848" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9999645">
This section describes the models for preposition
sense disambiguation and semantic role labeling.
We compare three different models for each
task: First, we implement an independent model
that only uses task specific features from the liter-
ature. This serves as the baseline model. Second,
we extend the baseline model by adding the most
likely prediction of the other task as an additional
feature. This is equivalent to a pipeline model of
classifiers that feeds the prediction of one classifi-
cation step into the next stage. Finally, we present
a joint model to determine the preposition sense
and semantic role that maximize the joint proba-
bility.
</bodyText>
<subsectionHeader confidence="0.981916">
3.1 WSD model
</subsectionHeader>
<bodyText confidence="0.999880333333333">
Our approach to building a preposition WSD clas-
sifier follows that of Lee and Ng (2002), who eval-
uated a set of different knowledge sources and
learning algorithms for WSD. However, in this pa-
per we use maximum entropy models2 (instead of
support vector machines (SVM) reported in (Lee
</bodyText>
<footnote confidence="0.993472">
2Zhang Le’s Maximum Entropy Modeling Toolkit,
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
</footnote>
<bodyText confidence="0.997528375">
and Ng, 2002)), because maximum entropy mod-
els output probability distributions, unlike SVM.
This property is useful in the joint model, as we
will see later. Maxent models have been success-
fully applied to various NLP tasks and achieve
state-of-the-art performance. There are two train-
ing parameters that have to be adjusted for maxent
models: the number of training iterations and the
Gaussian smoothing parameter. We find optimal
values for both parameters through 10-fold cross-
validation on the training set.
For every preposition, a baseline maxent model
is trained using a set of features reported in
the state-of-the-art WSD system of Lee and
Ng (2002). These features encode three knowl-
edge sources:
</bodyText>
<listItem confidence="0.999916666666667">
• Part-of-speech (POS) of surrounding words
• Single words in the surrounding context
• Local collocations
</listItem>
<bodyText confidence="0.999650516129032">
For part-of-speech features, we include the POS
tags of surrounding tokens from the same sentence
within a window of seven tokens around the target
prepositions. All tokens (i.e., all words and punc-
tuation symbols) are considered. We use the Penn
Treebank II POS tag set.
For the knowledge source single words in the
surrounding context, we consider all words from
the same sentence. The input sentence is tokenized
and all tokens that do not contain at least one al-
phabetical character (such as punctuation symbols
and numbers) and all words that appear on a stop-
word list are removed. The remaining words are
converted to lower case and replaced by their mor-
phological root form. Every unique morphologi-
cal root word contributes one binary feature, in-
dicating whether or not the word is present in the
context. The position of a word in the sentence is
ignored in this knowledge source.
The third knowledge source, local collocations,
encodes position-specific information of words
within a small window around the target prepo-
sition. For this knowledge source, we consider
unigrams, bigrams, and trigrams from a window
of seven tokens. The position of the target prepo-
sition inside the n-gram is marked with a special
character ‘ ’. Words are converted to lower case,
but no stemming or removal of stopwords is per-
formed. If a token falls outside the sentence, it is
replaced by the empty token symbol nil.
During testing, the maxent model computes the
</bodyText>
<page confidence="0.99346">
452
</page>
<bodyText confidence="0.99926075">
conditional probability of the sense, given the fea-
ture representation of the surrounding context c.
The classifier outputs the sense that receives the
highest probability:
</bodyText>
<equation confidence="0.9939795">
sˆ = argmax P(s|Ψ(c)) (1)
s
</equation>
<bodyText confidence="0.9999295">
where Ψ(·) is a feature map from the surrounding
context to the feature representation.
To ensure that our model is competitive, we
tested our system on the data set from the SemEval
2007 preposition WSD task (Litkowski and Har-
graves, 2007). Our baseline classifier achieved a
coarse-grained accuracy of 70.7% (micro-average)
on the official test set. This would have made our
system the second best system in the competition,
behind the MELB-YB system (Ye and Baldwin,
2007).
We also investigate the effect of the semantic
role label by adding it as a feature to the base-
line model. This pipeline model is inspired by the
work of Dang and Palmer (2005) who investigated
the role of SRL features in verb WSD. We add
the semantic role of the prepositional phrase dom-
inating the preposition as a feature to the WSD
model. During training, the PropBank gold SRL
label is used. During testing, we rely on the base-
line SRL model (to be introduced in the next sub-
section) to predict the semantic role of the prepo-
sitional phrase. This is equivalent to first per-
forming semantic role labeling and adding the out-
put as a feature to the WSD classifier. In ear-
lier experiments, we found that training on gold
SRL labels gave better results than training on
automatically predicted SRL labels (using cross-
validation). Note that our approach uses automati-
cally assigned SRL labels during testing, while the
system of Dang and Palmer (2005) only uses gold
SRL labels.
</bodyText>
<subsectionHeader confidence="0.987402">
3.2 SRL model
</subsectionHeader>
<bodyText confidence="0.999690636363636">
Our semantic role labeling classifier is also based
on maxent models. It has been shown that max-
imum entropy models achieve state-of-the-art re-
sults on SRL (Xue and Palmer, 2004; Toutanova
et al., 2008). Again, we find optimal values
for the training parameters through 10-fold cross-
validation on the training set.
By treating SRL as a classification problem, the
choice of appropriate features becomes a key is-
sue. Features are encoded as binary-valued func-
tions. During testing, the maxent model computes
</bodyText>
<table confidence="0.999816185185185">
Baseline Features (Gildea and Jurafsky, 2002)
pred predicate lemma
path path from constituent to predicate
ptype syntactic category (NP, PP, etc.)
pos relative position to the predicate
voice active or passive voice
hw syntactic head word of the phrase
sub-cat rule expanding the predicate’s parent
Advanced Features (Pradhan et al., 2005)
hw POS POS of the syntactic head word
PP hw/POS head word and POS of the rightmost
first/last word NP child if the phrase is a PP
parent ptype first/last word and POS in the con-
parent hw/POS stituent
sister ptype syntactic category of the parent node
sister hw/POS head word and POS of the parent
temporal phrase type of left and right sister
partPath head word and POS of left and right
proPath sister
temporal key words present
partial path predicate
projected path without directions
Feature Combinations (Xue and Palmer, 2004)
pred &amp; ptype predicate and phrase type
pred &amp; hw predicate and head word
pred &amp; path predicate and path
pred &amp; pos predicate and relative position
</table>
<tableCaption confidence="0.999761">
Table 1: SRL features for the baseline model
</tableCaption>
<bodyText confidence="0.99527725">
the conditional probability P(a|t, p, v) of the ar-
gument label a, given the parse tree t, predicate p,
and constituent node v. The classifier outputs the
semantic role with the highest probability:
</bodyText>
<equation confidence="0.97092675">
aˆ = argmax P(a|t, p, v) (2)
a
= argmax P(a|Φ(t,p, v)) (3)
a
</equation>
<bodyText confidence="0.99998335">
where Φ(·, ·, ·) is a feature map to an appropriate
feature representation.
For our baseline SRL model, we adopt the fea-
tures used in other state-of-the-art SRL systems,
which include the seven baseline features from the
original work of Gildea and Jurafsky (2002), addi-
tional features taken from Pradhan et al. (2005),
and feature combinations which are inspired by
the system in Xue and Palmer (2004). Table 1 lists
the features we use for easy reference.
In the pipeline model, we investigate the use-
fulness of the preposition sense as a feature for
SRL by adding the preposition lemma concate-
nated with the sense number (e.g., on 1) as a fea-
ture. During training, the gold annotated prepo-
sition sense is used. During testing, the sense is
automatically tagged by the baseline WSD model.
This is equivalent to first running the WSD clas-
sifier for all prepositions, and adding the output
preposition sense as a feature to our baseline SRL
</bodyText>
<page confidence="0.997043">
453
</page>
<bodyText confidence="0.807069">
system.
</bodyText>
<subsectionHeader confidence="0.985187">
3.3 Joint Inference Model
</subsectionHeader>
<bodyText confidence="0.998912333333333">
The two previous models seek to maximize the
probability of the semantic role and the preposi-
tion sense individually, thus ignoring possible de-
pendencies between the two. Instead of maximiz-
ing the individual probabilities, we would like to
maximize the joint probability of the semantic role
and the preposition sense, given the parse tree,
predicate, constituent node, and surrounding con-
text.
</bodyText>
<equation confidence="0.9881635">
(a, s) = argmax P(a, s|t, p, v, c) (4)
(a,$)
</equation>
<bodyText confidence="0.999982818181818">
We assume that the probability of the semantic
role is already determined by the syntactic parse
tree t, the predicate p, and the constituent node v,
and is conditionally independent of the remaining
surrounding context c given t, p, and v. Likewise,
we assume that the probability of the preposition
sense is conditionally independent of the parse tree
t, predicate p, and constituent v, given the sur-
rounding context c and the semantic role a. This
assumption allows us to factor the joint probability
into an SRL and a WSD component:
</bodyText>
<equation confidence="0.9954915">
(a, s) = argmax P(a|t, p, v)×P(s|c, a) (5)
(a,$)
= argmax P(a|Φ(t, p, v))×P(s|Ψ(c, a))(6)
(a,$)
</equation>
<bodyText confidence="0.999961">
We observe that the first component in our joint
model corresponds to the baseline SRL model
and the second component corresponds to the
WSD pipeline model. Because our maxent mod-
els output a complete probability distribution, we
can combine both components by multiplying the
probabilities. Theoretically, the joint probability
could be factored in the other way, by first com-
puting the probability of the preposition sense and
then conditioning the SRL model on the predicted
preposition sense. However, in our early exper-
iments, we found that this approach gave lower
classification accuracy.
During testing, the classifier seeks to find the
tuple of semantic role and preposition sense that
maximizes the joint probability. For every se-
mantic role, the classifier computes its probability
given the SRL features, and multiplies it by the
probability of the most likely preposition sense,
given the context and the semantic role. The tu-
ple that receives the highest joint probability is the
final output of the joint classifier.
</bodyText>
<table confidence="0.999795444444445">
Semantic Role Total Training Test
ARG0 28 15 13
ARG1 374 208 166
ARG2 649 352 297
ARG3 111 67 44
ARG4 177 91 86
ARGM-ADV 141 101 40
ARGM-CAU 31 23 8
ARGM-DIR 28 19 9
ARGM-DIS 29 9 20
ARGM-EXT 61 42 19
ARGM-LOC 954 668 286
ARGM-MNR 316 225 91
ARGM-PNC 115 78 37
ARGM-PRD 1 1 0
ARGM-REC 1 0 1
ARGM-TMP 838 563 275
Total 3854 2462 1392
</table>
<tableCaption confidence="0.9950155">
Table 2: Number of annotated prepositional
phrases for each semantic role
</tableCaption>
<sectionHeader confidence="0.980925" genericHeader="method">
4 Data Set
</sectionHeader>
<bodyText confidence="0.99998375">
The joint model uses the probability of a prepo-
sition sense, given the semantic role of the dom-
inating prepositional phrase. To estimate this
probability, we need a corpus which is annotated
with both preposition senses and semantic roles.
Unfortunately, PropBank is not annotated with
preposition senses. Instead, we manually anno-
tated the seven most frequent prepositions in four
sections of the PropBank corpus with their senses
from the TPP dictionary. According to Juraf-
sky and Martin (2008), the most frequent English
prepositions are: of, in, for, to, with, on and at (in
order of frequency). Our counts on Sections 2 to
21 of PropBank revealed that these top 7 prepo-
sitions account for about 65% of all prepositional
phrases that are labeled with semantic roles.
The annotation proceeds in the following way.
First, we automatically extract all sentences which
have one of the prepositions as the lexical head of
a prepositional phrase. The position of the prepo-
sition is marked in the sentence. By only consid-
ering prepositional phrases, we automatically ex-
clude occurrences of the word to before infinitives
and instances of particle usage of prepositions,
such as phrasal verbs. The extracted prepositions
are manually tagged with their senses from the
TPP dictionary. Idiomatic usage of prepositions
like for example or in fact, and complex preposi-
tion constructions that involve more than one word
(e.g., because of, instead of, etc.) are excluded by
the annotators and compiled into a stoplist.
We annotated 3854 instances of the top 7 prepo-
</bodyText>
<page confidence="0.998287">
454
</page>
<table confidence="0.999922444444444">
Preposition Total Training Test
at 404 260 144
for 478 307 171
in 1590 1083 507
of 97 51 46
on 408 246 162
to 532 304 228
with 345 211 134
Total 3854 2462 1392
</table>
<tableCaption confidence="0.938048">
Table 3: Number of annotated prepositional
phrases for each preposition
</tableCaption>
<bodyText confidence="0.985367756097561">
sitions in Sections 2 to 4 and 23 of the PropBank
corpus. The data shows a strong correlation be-
tween semantic roles and preposition senses that
express a spatial or temporal meaning. For the
preposition in, 90.8% of the instances that ap-
pear inside an ARGM-LOC are tagged with sense 1
(“surrounded by or enclosed in”) or sense 5 (“in-
clusion or involvement”). 94.6% of the instances
that appear inside an ARGM-TMP role are tagged
with sense 2 (“period of time”). Our counts fur-
thermore show that about one third of the anno-
tated prepositional phrases fill core roles and that
ARGM-LOC and ARGM-TMP are the most fre-
quent roles. The detailed breakdown of semantic
roles is shown in Table 2.
To see how consistent humans can perform the
annotation task, we computed the inter-annotator
agreement between two annotators on Section 4 of
the PropBank corpus. We found that the two anno-
tators assigned the same sense in 86% of the cases.
Although not directly comparable, it is interesting
to note that this figure is similar to inter-annotator
agreement for open-class words reported in previ-
ous work (Palmer et al., 2000). In our final data
set, all labels were tagged by the same annotator,
which we believe makes our annotation reason-
ably consistent across different instances. Because
we annotate running text, not all prepositions have
the same number of annotated instances. The
numbers for all seven prepositions are shown in
Table 3. In our experiments, we use Sections 2 to 4
to train the models, and Section 23 is kept for test-
ing. Although our experiments are limited to three
sections of training data, it still allows us to train
competitive SRL models. Pradhan et al. (2005)
have shown that the benefit of using more training
data diminishes after a few thousand training in-
stances. We found that the accuracy of our SRL
baseline model, which is trained on the 5275 sen-
tences of these three sections, is only an absolute
at for in of on to with total
</bodyText>
<figureCaption confidence="0.983157">
Figure 1: Classification accuracy of the WSD
</figureCaption>
<bodyText confidence="0.8747862">
models for the seven most frequent prepositions
in test section 23
3.89% lower than the accuracy of the same model
when it is trained on twenty sections (71.71% ac-
curacy compared to 75.60% accuracy).
</bodyText>
<sectionHeader confidence="0.994477" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999667741935484">
We evaluate the performance of the joint model on
the annotated prepositional phrases in test section
23 and compare the results with the performance
of the baseline models and the pipeline models.
Figure 1 shows the classification accuracy of the
WSD models for each of the seven prepositions in
the test section. The results show that the pipeline
model and the joint model perform almost equally,
with the joint model performing marginally better
in the overall score. The detailed scores are given
in Table 4. Both models outperform the baseline
classifier for three of the seven prepositions: at,
for, and to. For the prepositions in, of, and on, the
SRL feature did not affect the WSD classification
accuracy significantly. For the preposition with,
the classification accuracy even dropped by about
6%.
Performing the student’s t-test, we found that
the improvement for the prepositions at, for, and
to is statistical significant (p &lt; 0.05), as is the
overall improvement. This confirms our hypoth-
esis that the semantic role of the prepositional
phrase is a strong hint for the preposition sense.
However, our results also show that it is the
SRL feature that brings the improvement, not the
joint model, because the pipeline and joint model
achieve about the same performance.
For the SRL task, we report the classification
accuracy over all annotated prepositional phrases
in the test section and the F1 measure for the se-
mantic roles ARGM-LOC and ARGM-TMP. Fig-
</bodyText>
<figure confidence="0.997924090909091">
Accuracy
90%
80%
70%
60%
50%
40%
30%
Baseline
Pipeline
Joint
</figure>
<page confidence="0.996572">
455
</page>
<table confidence="0.999917777777778">
Preposition Baseline Pipeline Joint
at 70.83 78.47* 78.47*
for 41.52 49.12* 49.12*
in 62.33 61.74 61.93
of 43.48 43.48 43.48
on 51.85 51.85 52.47
to 58.77 67.11* 66.67*
with 44.78 38.06 38.06
Total 56.54 58.76* 58.84*
</table>
<tableCaption confidence="0.952157">
Table 4: Classification accuracy of the baseline,
pipeline, and joint model on the WSD task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
</tableCaption>
<figureCaption confidence="0.742252666666667">
Figure 2: F1 measure of the SRL models for
ARGM-LOC and ARGM-TMP, and overall accu-
racy on prepositional phrases in test section 23
</figureCaption>
<bodyText confidence="0.99938875">
ure 2 shows the results. The joint model shows
a small performance increase of 0.43% over the
baseline in the overall accuracy. Adding the
preposition sense as a feature, on the other hand,
significantly lowers the accuracy by over 2%. For
ARGM-LOC and ARGM-TMP, the joint model im-
proves the F1 measure by about 1.3% each. The
improvement of the joint model for these roles
is statistically significant (p ≤ 0.05, student’s t-
test). Simply adding the preposition sense in the
pipeline model again lowers the F1 measure. The
detailed results are listed in Table 5.
</bodyText>
<table confidence="0.98577025">
Semantic Role Baseline Pipeline Joint
ARGM-LOC(F1) 72.88 71.54 74.27*
ARGM-TMP(FJ) 81.87 79.43 83.24*
Overall(A) 71.71 69.47 72.14
</table>
<tableCaption confidence="0.63971375">
Table 5: F1 measure and accuracy of the baseline,
pipeline, and joint model on the SRL task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
</tableCaption>
<bodyText confidence="0.999977923076923">
Our SRL experiments show that a pipeline
model degrades the performance. The reason is
the relatively high degree of noise in the WSD
classification and that the pipeline model does not
discriminate whether the previous classifier pre-
dicts the extra feature with high or low confi-
dence. Instead, the model only passes on the 1-
best WSD prediction, which can cause the next
classifier to make a wrong classification based on
the erroneous prediction of the previous step. In
principle, this problem can be mitigated by train-
ing the pipeline model on automatically predicted
labels using cross-validation, but in our case we
found that automatically predicted WSD labels
decreased the performance of the pipeline model
even more. In contrast, the joint model computes
the full probability distribution over the semantic
roles and preposition senses. If the noise level in
the first classification step is low, the joint model
and the pipeline model perform almost identically,
as we have seen in the previous WSD experiments.
But if the noise level is high, the joint model can
still improve while the pipeline model drops in
performance. Our experiments show that the joint
model is more robust in the presence of noisy fea-
tures than the pipeline model.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999964269230769">
There is relatively less prior research on preposi-
tions and prepositional phrases in the NLP com-
munity. O’Hara and Wiebe (2003) proposed a
WSD system to disambiguate function tags of
prepositional phrases. An extended version of
their work was recently presented in (O’Hara and
Wiebe, 2009). Ye and Baldwin (2006) extended
their work to a semantic role tagger specifically
for prepositional phrases. Their system first classi-
fies the semantic roles of all prepositional phrases
and later merges the output with a general SRL
system. Ye and Baldwin (2007) used semantic
role tags from surrounding tokens as part of the
MELB-YB preposition WSD system. They found
that the SRL features did not significantly help
their classifier, which is different from our find-
ings. Dang and Palmer (2005) showed that se-
mantic role features are helpful to disambiguate
verb senses. Their approach is similar to our
pipeline WSD model, but they do not present re-
sults with automatically predicted semantic roles.
Toutanova et al. (2008) presented a re-ranking
model to jointly learn the semantic roles of mul-
tiple constituents in the SRL task. Their work
dealt with joint learning in SRL, but it is not di-
rectly comparable to ours. The difference is that
</bodyText>
<figure confidence="0.988551727272727">
Argm−LOC Argm−TMP Overall
f1−measure
90%
75%
70%
85%
80%
65%
Baseline
Pipeline
Joint
</figure>
<page confidence="0.998233">
456
</page>
<bodyText confidence="0.99993845">
Toutanova et al. attempt to jointly learn semantic
role assignment of different constituents for one
task (SRL), while we attempt to jointly learn two
tasks (WSD and SRL) for one constituent. Be-
cause we only look at one constituent at a time,
we do not have to restrict ourselves to a re-ranking
approach like Toutanova et al., but can calculate
the full joint probability distribution of both tasks.
Andrew et al. (2004) propose a method to learn a
joint generative inference model from partially la-
beled data and apply their method to the problems
of word sense disambiguation for verbs and deter-
mination of verb subcategorization frames. Their
motivation is similar to ours, but they focus on
learning from partially labeled data and they in-
vestigate different tasks.
None of these systems attempted to jointly learn
the semantics of the prepositional phrase and the
preposition in a single model, which is the main
contribution of our work reported in this paper.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999997875">
We propose a probabilistic model to jointly clas-
sify the semantic role of a prepositional phrase
and the sense of the associated preposition. We
show that learning both tasks together leads to an
improvement over competitive, individual models
for both subtasks. For the WSD task, we show
that the SRL information improves the classifi-
cation accuracy, although joint learning does not
significantly outperform a simpler pipeline model
here. For the SRL task, we show that the joint
model improves over both the baseline model and
the pipeline model, especially for temporal and lo-
cation arguments. As we only disambiguate the
seven most frequent prepositions, potentially more
improvement could be gained by including more
prepositions into our data set.
</bodyText>
<sectionHeader confidence="0.99677" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.903131666666667">
This research was supported by a research grant
R-252-000-225-112 from National University of
Singapore Academic Research Fund.
</bodyText>
<sectionHeader confidence="0.996411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897666666667">
Galen Andrew, Trond Grenager, and Christopher D.
Manning. 2004. Verb Sense and Subcategorization:
Using Joint Inference to Improve Performance on
Complementary Tasks. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 150–157.
Hoa Trang Dang and Martha Palmer. 2005. The
Role of Semantic Roles in Disambiguating Verb
Senses. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 42–49.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245–288.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. Prentice-Hall, Inc. Up-
per Saddle River, NJ, USA.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of the 9th Conference on Computational
Natural Language Learning (CoNLL 2005), pages
181–184.
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empir-
ical Evaluation of Knowledge Sources and Learn-
ing Algorithms for Word Sense Disambiguation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2002), pages 41–48.
Kenneth C. Litkowski and Orin Hargraves. 2005. The
Preposition Project. In Proceedings of the 2ndACL-
SIGSEM Workshop on The Linguistic Dimensions of
Prepositions and Their Use in Computational Lin-
guistic Formalisms and Applications, pages 171–
179.
Kenneth C. Litkowski and Orin Hargraves. 2007.
SemEval-2007 Task 06: Word-Sense Disambigua-
tion of Prepositions. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2007), pages 24–29.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Tom O’Hara and Janyce Wiebe. 2003. Preposi-
tion Semantic Classification via Penn Treebank and
FrameNet. In Proceedings of the 7th Conference on
Computational Natural Language Learning (CoNLL
2003), pages 79–86.
Tom O’Hara and Janyce Wiebe. 2009. Exploiting Se-
mantic Role Resources for Preposition Disambigua-
tion. Computational Linguistics, 35(2):151–184.
Martha Palmer, Hoa Trang Dang, and Joseph Rosen-
zweig. 2000. Sense Tagging the Penn Treebank. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC 2000).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–105.
</reference>
<page confidence="0.97999">
457
</page>
<reference confidence="0.999737913043478">
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic
Argument Classification. Machine Learning, 60(1–
3):11–39.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2):161–191.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of the 2004 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2004),
pages 88–94.
Patrick Ye and Timothy Baldwin. 2006. Seman-
tic Role Labeling of Prepositional Phrases. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 5(3):228–244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Se-
mantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval
2007), pages 241–244.
</reference>
<page confidence="0.997744">
458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327751">
<title confidence="0.990523">Joint Learning of Preposition Senses Semantic Roles of Prepositional Phrases</title>
<author confidence="0.609445">Hwee Tou Tanja</author>
<affiliation confidence="0.608469333333333">Graduate School for Integrative Sciences and of Computer Science, National University of Systems Lab, University of</affiliation>
<email confidence="0.871572">tanja@ira.uka.de</email>
<abstract confidence="0.9973035">The sense of a preposition is related to the semantics of its dominating prepositional phrase. Knowing the sense of a preposition could help to correctly classify the semantic role of the dominating prepositional phrase and vice versa. In this paper, we propose a joint probabilistic model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases. Our experiments on the PropBank corpus show that jointly learning the word sense and the semantic role leads to an improvement over state-of-theart individual classifier models on the two tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Verb Sense and Subcategorization: Using Joint Inference to Improve Performance on Complementary Tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>150--157</pages>
<contexts>
<context position="29873" citStr="Andrew et al. (2004)" startWordPosition="4912" endWordPosition="4915">heir work dealt with joint learning in SRL, but it is not directly comparable to ours. The difference is that Argm−LOC Argm−TMP Overall f1−measure 90% 75% 70% 85% 80% 65% Baseline Pipeline Joint 456 Toutanova et al. attempt to jointly learn semantic role assignment of different constituents for one task (SRL), while we attempt to jointly learn two tasks (WSD and SRL) for one constituent. Because we only look at one constituent at a time, we do not have to restrict ourselves to a re-ranking approach like Toutanova et al., but can calculate the full joint probability distribution of both tasks. Andrew et al. (2004) propose a method to learn a joint generative inference model from partially labeled data and apply their method to the problems of word sense disambiguation for verbs and determination of verb subcategorization frames. Their motivation is similar to ours, but they focus on learning from partially labeled data and they investigate different tasks. None of these systems attempted to jointly learn the semantics of the prepositional phrase and the preposition in a single model, which is the main contribution of our work reported in this paper. 7 Conclusion We propose a probabilistic model to join</context>
</contexts>
<marker>Andrew, Grenager, Manning, 2004</marker>
<rawString>Galen Andrew, Trond Grenager, and Christopher D. Manning. 2004. Verb Sense and Subcategorization: Using Joint Inference to Improve Performance on Complementary Tasks. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>The Role of Semantic Roles in Disambiguating Verb Senses.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>42--49</pages>
<contexts>
<context position="13727" citStr="Dang and Palmer (2005)" startWordPosition="2204" endWordPosition="2207">m the surrounding context to the feature representation. To ensure that our model is competitive, we tested our system on the data set from the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). Our baseline classifier achieved a coarse-grained accuracy of 70.7% (micro-average) on the official test set. This would have made our system the second best system in the competition, behind the MELB-YB system (Ye and Baldwin, 2007). We also investigate the effect of the semantic role label by adding it as a feature to the baseline model. This pipeline model is inspired by the work of Dang and Palmer (2005) who investigated the role of SRL features in verb WSD. We add the semantic role of the prepositional phrase dominating the preposition as a feature to the WSD model. During training, the PropBank gold SRL label is used. During testing, we rely on the baseline SRL model (to be introduced in the next subsection) to predict the semantic role of the prepositional phrase. This is equivalent to first performing semantic role labeling and adding the output as a feature to the WSD classifier. In earlier experiments, we found that training on gold SRL labels gave better results than training on automa</context>
<context position="28914" citStr="Dang and Palmer (2005)" startWordPosition="4750" endWordPosition="4753">function tags of prepositional phrases. An extended version of their work was recently presented in (O’Hara and Wiebe, 2009). Ye and Baldwin (2006) extended their work to a semantic role tagger specifically for prepositional phrases. Their system first classifies the semantic roles of all prepositional phrases and later merges the output with a general SRL system. Ye and Baldwin (2007) used semantic role tags from surrounding tokens as part of the MELB-YB preposition WSD system. They found that the SRL features did not significantly help their classifier, which is different from our findings. Dang and Palmer (2005) showed that semantic role features are helpful to disambiguate verb senses. Their approach is similar to our pipeline WSD model, but they do not present results with automatically predicted semantic roles. Toutanova et al. (2008) presented a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task. Their work dealt with joint learning in SRL, but it is not directly comparable to ours. The difference is that Argm−LOC Argm−TMP Overall f1−measure 90% 75% 70% 85% 80% 65% Baseline Pipeline Joint 456 Toutanova et al. attempt to jointly learn semantic role assign</context>
</contexts>
<marker>Dang, Palmer, 2005</marker>
<rawString>Hoa Trang Dang and Martha Palmer. 2005. The Role of Semantic Roles in Disambiguating Verb Senses. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="15095" citStr="Gildea and Jurafsky, 2002" startWordPosition="2433" endWordPosition="2436">system of Dang and Palmer (2005) only uses gold SRL labels. 3.2 SRL model Our semantic role labeling classifier is also based on maxent models. It has been shown that maximum entropy models achieve state-of-the-art results on SRL (Xue and Palmer, 2004; Toutanova et al., 2008). Again, we find optimal values for the training parameters through 10-fold crossvalidation on the training set. By treating SRL as a classification problem, the choice of appropriate features becomes a key issue. Features are encoded as binary-valued functions. During testing, the maxent model computes Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicate’s parent Advanced Features (Pradhan et al., 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost first/last word NP child if the phrase is a PP parent ptype first/last word and POS in the conparent hw/POS stituent sister ptype syntactic category of the parent node sister hw/POS head word and POS of the parent temporal</context>
<context position="16636" citStr="Gildea and Jurafsky (2002)" startWordPosition="2696" endWordPosition="2699">icate and path pred &amp; pos predicate and relative position Table 1: SRL features for the baseline model the conditional probability P(a|t, p, v) of the argument label a, given the parse tree t, predicate p, and constituent node v. The classifier outputs the semantic role with the highest probability: aˆ = argmax P(a|t, p, v) (2) a = argmax P(a|Φ(t,p, v)) (3) a where Φ(·, ·, ·) is a feature map to an appropriate feature representation. For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002), additional features taken from Pradhan et al. (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004). Table 1 lists the features we use for easy reference. In the pipeline model, we investigate the usefulness of the preposition sense as a feature for SRL by adding the preposition lemma concatenated with the sense number (e.g., on 1) as a feature. During training, the gold annotated preposition sense is used. During testing, the sense is automatically tagged by the baseline WSD model. This is equivalent to first running the WSD classifier for all prepositio</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing. Prentice-Hall, Inc. Upper Saddle River,</title>
<date>2008</date>
<location>NJ, USA.</location>
<contexts>
<context position="20370" citStr="Jurafsky and Martin (2008)" startWordPosition="3325" endWordPosition="3329"> 1 ARGM-TMP 838 563 275 Total 3854 2462 1392 Table 2: Number of annotated prepositional phrases for each semantic role 4 Data Set The joint model uses the probability of a preposition sense, given the semantic role of the dominating prepositional phrase. To estimate this probability, we need a corpus which is annotated with both preposition senses and semantic roles. Unfortunately, PropBank is not annotated with preposition senses. Instead, we manually annotated the seven most frequent prepositions in four sections of the PropBank corpus with their senses from the TPP dictionary. According to Jurafsky and Martin (2008), the most frequent English prepositions are: of, in, for, to, with, on and at (in order of frequency). Our counts on Sections 2 to 21 of PropBank revealed that these top 7 prepositions account for about 65% of all prepositional phrases that are labeled with semantic roles. The annotation proceeds in the following way. First, we automatically extract all sentences which have one of the prepositions as the lexical head of a prepositional phrase. The position of the preposition is marked in the sentence. By only considering prepositional phrases, we automatically exclude occurrences of the word </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing. Prentice-Hall, Inc. Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Koomen</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Generalized Inference with Multiple Semantic Role Labeling Systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>181--184</pages>
<contexts>
<context position="4320" citStr="Koomen et al., 2005" startWordPosition="662" endWordPosition="665">//verbs.colorado.edu/framesets/arrive-v.html 450 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450–458, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP has been identified as an argument of the predicate, we jointly infer its semantic role and the sense of the preposition that is the lexical head of the prepositional phrase. That is, our model maximizes the joint probability of the semantic role and the preposition sense. Previous research has shown the benefit of jointly learning semantic roles of multiple constituents (Toutanova et al., 2008; Koomen et al., 2005). In contrast, our joint model makes predictions for a single constituent, but multiple tasks (WSD and SRL) . Our experiments show that adding the SRL information leads to statistically significant improvements over an independent, state-of-the-art WSD classifier. For the SRL task, we show statistically significant improvements of our joint model over an independent, state-of-the-art SRL classifier for locative and temporal adjunctive arguments, even though the overall improvement over all semantic roles is small. To the best of our knowledge, no previous research has attempted to perform prep</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. Generalized Inference with Multiple Semantic Role Labeling Systems. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL 2005), pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>41--48</pages>
<contexts>
<context position="10294" citStr="Lee and Ng (2002)" startWordPosition="1648" endWordPosition="1651">each task: First, we implement an independent model that only uses task specific features from the literature. This serves as the baseline model. Second, we extend the baseline model by adding the most likely prediction of the other task as an additional feature. This is equivalent to a pipeline model of classifiers that feeds the prediction of one classification step into the next stage. Finally, we present a joint model to determine the preposition sense and semantic role that maximize the joint probability. 3.1 WSD model Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. However, in this paper we use maximum entropy models2 (instead of support vector machines (SVM) reported in (Lee 2Zhang Le’s Maximum Entropy Modeling Toolkit, http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html and Ng, 2002)), because maximum entropy models output probability distributions, unlike SVM. This property is useful in the joint model, as we will see later. Maxent models have been successfully applied to various NLP tasks and achieve state-of-the-art performance. There are two training parame</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>The Preposition Project.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2ndACLSIGSEM Workshop on The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications,</booktitle>
<pages>171--179</pages>
<contexts>
<context position="6226" citStr="Litkowski and Hargraves, 2005" startWordPosition="968" endWordPosition="971">tion is to find the correct meaning of a word, given its context. Most prior research on word sense disambiguation has focused on disambiguating the senses of nouns, verbs, and adjectives, but not on prepositions. Word sense disambiguation can be framed as a classification task. For each preposition, a classifier is trained on a corpus of training examples annotated with preposition senses, and tested on a set of unseen test examples. To perform WSD for prepositions, it is necessary to first find a set of suitable sense classes. We adopt the sense inventory from the Preposition Project (TPP) (Litkowski and Hargraves, 2005) that was also used in the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). TPP is an attempt to create a comprehensive lexical database of English prepositions that is suitable for use in computational linguistics research. For each of the over 300 prepositions and phrasal prepositions, the database contains a set of sense definitions, which are based on the Oxford Dictionary of English. Every preposition has a set of fine-grained senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses sinc</context>
</contexts>
<marker>Litkowski, Hargraves, 2005</marker>
<rawString>Kenneth C. Litkowski and Orin Hargraves. 2005. The Preposition Project. In Proceedings of the 2ndACLSIGSEM Workshop on The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications, pages 171– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval</booktitle>
<pages>24--29</pages>
<contexts>
<context position="6318" citStr="Litkowski and Hargraves, 2007" startWordPosition="984" endWordPosition="987">d sense disambiguation has focused on disambiguating the senses of nouns, verbs, and adjectives, but not on prepositions. Word sense disambiguation can be framed as a classification task. For each preposition, a classifier is trained on a corpus of training examples annotated with preposition senses, and tested on a set of unseen test examples. To perform WSD for prepositions, it is necessary to first find a set of suitable sense classes. We adopt the sense inventory from the Preposition Project (TPP) (Litkowski and Hargraves, 2005) that was also used in the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). TPP is an attempt to create a comprehensive lexical database of English prepositions that is suitable for use in computational linguistics research. For each of the over 300 prepositions and phrasal prepositions, the database contains a set of sense definitions, which are based on the Oxford Dictionary of English. Every preposition has a set of fine-grained senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses since better inter-annotator agreement can be achieved on coarse-grained senses, which also resu</context>
<context position="13314" citStr="Litkowski and Hargraves, 2007" startWordPosition="2133" endWordPosition="2137">but no stemming or removal of stopwords is performed. If a token falls outside the sentence, it is replaced by the empty token symbol nil. During testing, the maxent model computes the 452 conditional probability of the sense, given the feature representation of the surrounding context c. The classifier outputs the sense that receives the highest probability: sˆ = argmax P(s|Ψ(c)) (1) s where Ψ(·) is a feature map from the surrounding context to the feature representation. To ensure that our model is competitive, we tested our system on the data set from the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). Our baseline classifier achieved a coarse-grained accuracy of 70.7% (micro-average) on the official test set. This would have made our system the second best system in the competition, behind the MELB-YB system (Ye and Baldwin, 2007). We also investigate the effect of the semantic role label by adding it as a feature to the baseline model. This pipeline model is inspired by the work of Dang and Palmer (2005) who investigated the role of SRL features in verb WSD. We add the semantic role of the prepositional phrase dominating the preposition as a feature to the WSD model. During training, the</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Kenneth C. Litkowski and Orin Hargraves. 2007. SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pages 24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary A Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7299" citStr="Marcus et al., 1993" startWordPosition="1144" endWordPosition="1147"> senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses since better inter-annotator agreement can be achieved on coarse-grained senses, which also results in higher accuracy of the trained WSD classifier. 2.2 Semantic Role Labeling The task of semantic role labeling in the context of PropBank (Palmer et al., 2005) is to label tree nodes with semantic roles in a syntactic parse tree. The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al., 1993). There are two classes of semantic roles: core arguments and adjunctive arguments. Core arguments are verb sense specific, i.e., their meaning is defined relative to a specific verb sense. They are labeled with consecutive numbers ARG0, ARG1, etc. ARG0 usually denotes the AGENT and ARG1 the THEME of the event. Besides the core arguments, a verb can have a number of adjunctive arguments that express more general properties like time, location, or manner. They are labeled as ARGM plus a functional tag, e.g., LOC for locative or TMP for temporal modifiers. Prepositional phrases can appear as adj</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Preposition Semantic Classification via Penn Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>79--86</pages>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2003. Preposition Semantic Classification via Penn Treebank and FrameNet. In Proceedings of the 7th Conference on Computational Natural Language Learning (CoNLL 2003), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Exploiting Semantic Role Resources for Preposition Disambiguation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<marker>O’Hara, Wiebe, 2009</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2009. Exploiting Semantic Role Resources for Preposition Disambiguation. Computational Linguistics, 35(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Sense Tagging the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="22781" citStr="Palmer et al., 2000" startWordPosition="3737" endWordPosition="3740">about one third of the annotated prepositional phrases fill core roles and that ARGM-LOC and ARGM-TMP are the most frequent roles. The detailed breakdown of semantic roles is shown in Table 2. To see how consistent humans can perform the annotation task, we computed the inter-annotator agreement between two annotators on Section 4 of the PropBank corpus. We found that the two annotators assigned the same sense in 86% of the cases. Although not directly comparable, it is interesting to note that this figure is similar to inter-annotator agreement for open-class words reported in previous work (Palmer et al., 2000). In our final data set, all labels were tagged by the same annotator, which we believe makes our annotation reasonably consistent across different instances. Because we annotate running text, not all prepositions have the same number of annotated instances. The numbers for all seven prepositions are shown in Table 3. In our experiments, we use Sections 2 to 4 to train the models, and Section 23 is kept for testing. Although our experiments are limited to three sections of training data, it still allows us to train competitive SRL models. Pradhan et al. (2005) have shown that the benefit of us</context>
</contexts>
<marker>Palmer, Dang, Rosenzweig, 2000</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Joseph Rosenzweig. 2000. Sense Tagging the Penn Treebank. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7082" citStr="Palmer et al., 2005" startWordPosition="1105" endWordPosition="1108">earch. For each of the over 300 prepositions and phrasal prepositions, the database contains a set of sense definitions, which are based on the Oxford Dictionary of English. Every preposition has a set of fine-grained senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses since better inter-annotator agreement can be achieved on coarse-grained senses, which also results in higher accuracy of the trained WSD classifier. 2.2 Semantic Role Labeling The task of semantic role labeling in the context of PropBank (Palmer et al., 2005) is to label tree nodes with semantic roles in a syntactic parse tree. The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al., 1993). There are two classes of semantic roles: core arguments and adjunctive arguments. Core arguments are verb sense specific, i.e., their meaning is defined relative to a specific verb sense. They are labeled with consecutive numbers ARG0, ARG1, etc. ARG0 usually denotes the AGENT and ARG1 the THEME of the event. Besides the core arguments, a verb can have a number of adjunctive arg</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument Classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<pages>3--11</pages>
<contexts>
<context position="8709" citStr="Pradhan et al., 2005" startWordPosition="1374" endWordPosition="1377">ification phase, the system separates the nodes that fill some semantic roles from the rest. During the classification phase, the system assigns the exact semantic roles for all nodes that are identified as arguments. In this paper, we focus on the classification phase. That is, we assume that prepositional phrases that are semantic arguments have been identified correctly and concentrate on the 451 task of determining the semantic role of prepositional phrases. The reason is that argument identification mostly relies on syntactic features, like the path from the constituent to the predicate (Pradhan et al., 2005). Consider, for example, the phrase in the dark in the sentence: “We are in the dark”, he said. The phrase is clearly not an argument to the verb say. But if we alter the syntactic structure of the sentence appropriately (while the sense of the preposition in remains unchanged), the same phrase suddenly becomes an adjunctive argument: In the dark, he said “We are”. On the other hand, we can easily find examples, where in has a different sense, but the phrase always fills some semantic role: • In a separate manner, he said ... • In 1998, he said ... • In Washington, he said ... This illustrates</context>
<context position="15389" citStr="Pradhan et al., 2005" startWordPosition="2478" endWordPosition="2481">l values for the training parameters through 10-fold crossvalidation on the training set. By treating SRL as a classification problem, the choice of appropriate features becomes a key issue. Features are encoded as binary-valued functions. During testing, the maxent model computes Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicate’s parent Advanced Features (Pradhan et al., 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost first/last word NP child if the phrase is a PP parent ptype first/last word and POS in the conparent hw/POS stituent sister ptype syntactic category of the parent node sister hw/POS head word and POS of the parent temporal phrase type of left and right sister partPath head word and POS of left and right proPath sister temporal key words present partial path predicate projected path without directions Feature Combinations (Xue and Palmer, 2004) pred &amp; ptype predicate and phrase type pred &amp; hw predicate and head </context>
<context position="16690" citStr="Pradhan et al. (2005)" startWordPosition="2705" endWordPosition="2708">able 1: SRL features for the baseline model the conditional probability P(a|t, p, v) of the argument label a, given the parse tree t, predicate p, and constituent node v. The classifier outputs the semantic role with the highest probability: aˆ = argmax P(a|t, p, v) (2) a = argmax P(a|Φ(t,p, v)) (3) a where Φ(·, ·, ·) is a feature map to an appropriate feature representation. For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002), additional features taken from Pradhan et al. (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004). Table 1 lists the features we use for easy reference. In the pipeline model, we investigate the usefulness of the preposition sense as a feature for SRL by adding the preposition lemma concatenated with the sense number (e.g., on 1) as a feature. During training, the gold annotated preposition sense is used. During testing, the sense is automatically tagged by the baseline WSD model. This is equivalent to first running the WSD classifier for all prepositions, and adding the output preposition sense as a featu</context>
<context position="23347" citStr="Pradhan et al. (2005)" startWordPosition="3833" endWordPosition="3836">s words reported in previous work (Palmer et al., 2000). In our final data set, all labels were tagged by the same annotator, which we believe makes our annotation reasonably consistent across different instances. Because we annotate running text, not all prepositions have the same number of annotated instances. The numbers for all seven prepositions are shown in Table 3. In our experiments, we use Sections 2 to 4 to train the models, and Section 23 is kept for testing. Although our experiments are limited to three sections of training data, it still allows us to train competitive SRL models. Pradhan et al. (2005) have shown that the benefit of using more training data diminishes after a few thousand training instances. We found that the accuracy of our SRL baseline model, which is trained on the 5275 sentences of these three sections, is only an absolute at for in of on to with total Figure 1: Classification accuracy of the WSD models for the seven most frequent prepositions in test section 23 3.89% lower than the accuracy of the same model when it is trained on twenty sections (71.71% accuracy compared to 75.60% accuracy). 5 Experiments and Results We evaluate the performance of the joint model on th</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support Vector Learning for Semantic Argument Classification. Machine Learning, 60(1– 3):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A Global Joint Model for Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4298" citStr="Toutanova et al., 2008" startWordPosition="658" endWordPosition="661">ional phrase that 1http://verbs.colorado.edu/framesets/arrive-v.html 450 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450–458, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP has been identified as an argument of the predicate, we jointly infer its semantic role and the sense of the preposition that is the lexical head of the prepositional phrase. That is, our model maximizes the joint probability of the semantic role and the preposition sense. Previous research has shown the benefit of jointly learning semantic roles of multiple constituents (Toutanova et al., 2008; Koomen et al., 2005). In contrast, our joint model makes predictions for a single constituent, but multiple tasks (WSD and SRL) . Our experiments show that adding the SRL information leads to statistically significant improvements over an independent, state-of-the-art WSD classifier. For the SRL task, we show statistically significant improvements of our joint model over an independent, state-of-the-art SRL classifier for locative and temporal adjunctive arguments, even though the overall improvement over all semantic roles is small. To the best of our knowledge, no previous research has att</context>
<context position="14745" citStr="Toutanova et al., 2008" startWordPosition="2380" endWordPosition="2383">rst performing semantic role labeling and adding the output as a feature to the WSD classifier. In earlier experiments, we found that training on gold SRL labels gave better results than training on automatically predicted SRL labels (using crossvalidation). Note that our approach uses automatically assigned SRL labels during testing, while the system of Dang and Palmer (2005) only uses gold SRL labels. 3.2 SRL model Our semantic role labeling classifier is also based on maxent models. It has been shown that maximum entropy models achieve state-of-the-art results on SRL (Xue and Palmer, 2004; Toutanova et al., 2008). Again, we find optimal values for the training parameters through 10-fold crossvalidation on the training set. By treating SRL as a classification problem, the choice of appropriate features becomes a key issue. Features are encoded as binary-valued functions. During testing, the maxent model computes Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicate’s par</context>
<context position="29144" citStr="Toutanova et al. (2008)" startWordPosition="4787" endWordPosition="4790">ses. Their system first classifies the semantic roles of all prepositional phrases and later merges the output with a general SRL system. Ye and Baldwin (2007) used semantic role tags from surrounding tokens as part of the MELB-YB preposition WSD system. They found that the SRL features did not significantly help their classifier, which is different from our findings. Dang and Palmer (2005) showed that semantic role features are helpful to disambiguate verb senses. Their approach is similar to our pipeline WSD model, but they do not present results with automatically predicted semantic roles. Toutanova et al. (2008) presented a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task. Their work dealt with joint learning in SRL, but it is not directly comparable to ours. The difference is that Argm−LOC Argm−TMP Overall f1−measure 90% 75% 70% 85% 80% 65% Baseline Pipeline Joint 456 Toutanova et al. attempt to jointly learn semantic role assignment of different constituents for one task (SRL), while we attempt to jointly learn two tasks (WSD and SRL) for one constituent. Because we only look at one constituent at a time, we do not have to restrict ourselves to a re-rank</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A Global Joint Model for Semantic Role Labeling. Computational Linguistics, 34(2):161–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>88--94</pages>
<contexts>
<context position="14720" citStr="Xue and Palmer, 2004" startWordPosition="2376" endWordPosition="2379">is is equivalent to first performing semantic role labeling and adding the output as a feature to the WSD classifier. In earlier experiments, we found that training on gold SRL labels gave better results than training on automatically predicted SRL labels (using crossvalidation). Note that our approach uses automatically assigned SRL labels during testing, while the system of Dang and Palmer (2005) only uses gold SRL labels. 3.2 SRL model Our semantic role labeling classifier is also based on maxent models. It has been shown that maximum entropy models achieve state-of-the-art results on SRL (Xue and Palmer, 2004; Toutanova et al., 2008). Again, we find optimal values for the training parameters through 10-fold crossvalidation on the training set. By treating SRL as a classification problem, the choice of appropriate features becomes a key issue. Features are encoded as binary-valued functions. During testing, the maxent model computes Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expa</context>
<context position="16774" citStr="Xue and Palmer (2004)" startWordPosition="2719" endWordPosition="2722"> of the argument label a, given the parse tree t, predicate p, and constituent node v. The classifier outputs the semantic role with the highest probability: aˆ = argmax P(a|t, p, v) (2) a = argmax P(a|Φ(t,p, v)) (3) a where Φ(·, ·, ·) is a feature map to an appropriate feature representation. For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002), additional features taken from Pradhan et al. (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004). Table 1 lists the features we use for easy reference. In the pipeline model, we investigate the usefulness of the preposition sense as a feature for SRL by adding the preposition lemma concatenated with the sense number (e.g., on 1) as a feature. During training, the gold annotated preposition sense is used. During testing, the sense is automatically tagged by the baseline WSD model. This is equivalent to first running the WSD classifier for all prepositions, and adding the output preposition sense as a feature to our baseline SRL 453 system. 3.3 Joint Inference Model The two previous models</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating Features for Semantic Role Labeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semantic Role Labeling of Prepositional Phrases.</title>
<date>2006</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="28439" citStr="Ye and Baldwin (2006)" startWordPosition="4674" endWordPosition="4677">t identically, as we have seen in the previous WSD experiments. But if the noise level is high, the joint model can still improve while the pipeline model drops in performance. Our experiments show that the joint model is more robust in the presence of noisy features than the pipeline model. 6 Related Work There is relatively less prior research on prepositions and prepositional phrases in the NLP community. O’Hara and Wiebe (2003) proposed a WSD system to disambiguate function tags of prepositional phrases. An extended version of their work was recently presented in (O’Hara and Wiebe, 2009). Ye and Baldwin (2006) extended their work to a semantic role tagger specifically for prepositional phrases. Their system first classifies the semantic roles of all prepositional phrases and later merges the output with a general SRL system. Ye and Baldwin (2007) used semantic role tags from surrounding tokens as part of the MELB-YB preposition WSD system. They found that the SRL features did not significantly help their classifier, which is different from our findings. Dang and Palmer (2005) showed that semantic role features are helpful to disambiguate verb senses. Their approach is similar to our pipeline WSD mo</context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2006. Semantic Role Labeling of Prepositional Phrases. ACM Transactions on Asian Language Information Processing (TALIP), 5(3):228–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval</booktitle>
<pages>241--244</pages>
<contexts>
<context position="13549" citStr="Ye and Baldwin, 2007" startWordPosition="2170" endWordPosition="2173"> representation of the surrounding context c. The classifier outputs the sense that receives the highest probability: sˆ = argmax P(s|Ψ(c)) (1) s where Ψ(·) is a feature map from the surrounding context to the feature representation. To ensure that our model is competitive, we tested our system on the data set from the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). Our baseline classifier achieved a coarse-grained accuracy of 70.7% (micro-average) on the official test set. This would have made our system the second best system in the competition, behind the MELB-YB system (Ye and Baldwin, 2007). We also investigate the effect of the semantic role label by adding it as a feature to the baseline model. This pipeline model is inspired by the work of Dang and Palmer (2005) who investigated the role of SRL features in verb WSD. We add the semantic role of the prepositional phrase dominating the preposition as a feature to the WSD model. During training, the PropBank gold SRL label is used. During testing, we rely on the baseline SRL model (to be introduced in the next subsection) to predict the semantic role of the prepositional phrase. This is equivalent to first performing semantic rol</context>
<context position="28680" citStr="Ye and Baldwin (2007)" startWordPosition="4712" endWordPosition="4715">presence of noisy features than the pipeline model. 6 Related Work There is relatively less prior research on prepositions and prepositional phrases in the NLP community. O’Hara and Wiebe (2003) proposed a WSD system to disambiguate function tags of prepositional phrases. An extended version of their work was recently presented in (O’Hara and Wiebe, 2009). Ye and Baldwin (2006) extended their work to a semantic role tagger specifically for prepositional phrases. Their system first classifies the semantic roles of all prepositional phrases and later merges the output with a general SRL system. Ye and Baldwin (2007) used semantic role tags from surrounding tokens as part of the MELB-YB preposition WSD system. They found that the SRL features did not significantly help their classifier, which is different from our findings. Dang and Palmer (2005) showed that semantic role features are helpful to disambiguate verb senses. Their approach is similar to our pipeline WSD model, but they do not present results with automatically predicted semantic roles. Toutanova et al. (2008) presented a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task. Their work dealt with joint </context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2007. MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pages 241–244.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>