<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002245">
<title confidence="0.999581">
A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival
Subcategorization Frames from Corpora
</title>
<author confidence="0.99916">
Judita Preiss, Ted Briscoe, and Anna Korhonen
</author>
<affiliation confidence="0.991808">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.990942">
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
</address>
<email confidence="0.991711">
Judita.Preiss, Ted.Briscoe, Anna.Korhonen@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807545454545">
This paper describes the first system for
large-scale acquisition of subcategorization
frames (SCFs) from English corpus data
which can be used to acquire comprehen-
sive lexicons for verbs, nouns and adjectives.
The system incorporates an extensive rule-
based classifier which identifies 168 verbal,
37 adjectival and 31 nominal frames from
grammatical relations (GRs) output by a ro-
bust parser. The system achieves state-of-
the-art performance on all three sets.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99963644">
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al., 1998),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) ((Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from unannotated English corpora
emerged over a decade ago (Brent, 1991; Manning,
1993). Subsequent research has yielded systems for
English (Carroll and Rooth, 1998; Briscoe and Car-
roll, 1997; Korhonen, 2002) capable of detecting
comprehensive sets of SCFs with promising accu-
racy and demonstrated success in application tasks
(e.g. (Carroll et al., 1998; Korhonen et al., 2003)).
Recently, a large publicly available subcategoriza-
tion lexicon was produced using such technology
which contains frame and frequency information for
over 6,300 English verbs – the VALEX lexicon (Ko-
rhonen et al., 2006).
While there has been considerable work in the
area, most of it has focussed on verbs. Although
verbs are the richest words in terms of subcatego-
rization and although verb SCF distribution data is
likely to offer the greatest boost in parser perfor-
mance, accurate and comprehensive knowledge of
the many noun and adjective SCFs in English could
improve the accuracy of parsing at several levels
(from tagging to syntactic and semantic analysis).
Furthermore the selection of the correct analysis
from the set returned by a parser which does not ini-
tially utilize fine-grained lexico-syntactic informa-
tion can depend on the interaction of conditional
probabilities of lemmas of different classes occur-
</bodyText>
<page confidence="0.937805">
912
</page>
<note confidence="0.926044">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912–919,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.929422272727273">
ring with specific SCFs. For example, a) and b) be-
low indicate the most plausible analyses in which the
sentential complement attaches to the noun and verb
respectively
a) Kim (VP believes (NP the evidence (Scomp that
Sandy was present)))
b) Kim (VP persuaded (NP the judge) (Scomp that
Sandy was present))
However, both a) and b) consist of an identical
sequence of coarse-grained lexical syntactic cate-
gories, so correctly ranking them requires learn-
</bodyText>
<construct confidence="0.7414825">
ing that P(NP  |believe).P(Scomp  |evidence) &gt;
P(NP&amp;Scomp  |believe).P(None  |evidence)
and P(NP  |persuade).P(Scomp  |judge) &lt;
P(NP&amp;Scomp  |persuade).P(None  |judge). If
</construct>
<bodyText confidence="0.991117593220339">
we acquired frames and frame frequencies for all
open-class predicates taking SCFs using a single sys-
tem applied to similar data, we would have a better
chance of modeling such interactions accurately.
In this paper we present the first system for large-
scale acquisition of SCFs from English corpus data
which can be used to acquire comprehensive lexi-
cons for verbs, nouns and adjectives. The classifier
incorporates 168 verbal, 37 adjectival and 31 nomi-
nal SCF distinctions. An improved acquisition tech-
nique is used which expands on the ideas Yallop et
al. (2005) recently explored for a small experiment
on adjectival SCF acquisition. It involves identifying
SCFs on the basis of grammatical relations (GRs) in
the output of the RASP (Robust Accurate Statistical
Parsing) system (Briscoe et al., 2006).
As detailed later, the system performs better with
verbs than previous comparable state-of-art systems,
achieving 68.9 F-measure in detecting SCF types. It
achieves similarly good performance with nouns and
adjectives (62.2 and 71.9 F-measure, respectively).
Additionally, we have developed a tool for lin-
guistic annotation of SCFs in corpus data aimed at
alleviating the process of obtaining training and test
data for subcategorization acquisition. The tool in-
corporates an intuitive interface with the ability to
significantly reduce the number of frames presented
to the user for each sentence.
We introduce the new system for SCF acquisition
in section 2. Details of the experimental evaluation
are supplied in section 3. Section 4 provides discus-
sion of our results and future work, and section 5
concludes.
2 Description of the System
A common strategy in existing large-scale SCF ac-
quisition systems (e.g. (Briscoe and Carroll, 1997))
is to extract SCFs from parse trees, introducing an
unnecessary dependence on the details of a particu-
lar parser. In our approach SCFs are extracted from
GRs — representations of head-dependent relations
which are more parser/grammar independent but at
the appropriate level of abstraction for extraction of
SCFs.
A similar approach was recently motivated and
explored by Yallop et al. (2005). A decision-tree
classifier was developed for 30 adjectival SCF types
which tests for the presence of GRs in the GR out-
put of the RASP (Robust Accurate Statistical Pars-
ing) system (Briscoe and Carroll, 2002). The results
reported with 9 test adjectives were promising (68.9
F-measure in detecting SCF types).
Our acquisition process consists of four main
steps: 1) extracting GRs from corpus data, 2) feeding
the GR sets as input to a rule-based classifier which
incrementally matches them with the corresponding
SCFs, 3) building lexical entries from the classified
data, and 4) filtering those entries to obtain a more
accurate lexicon. The details of these steps are pro-
vided in the subsequent sections.
</bodyText>
<subsectionHeader confidence="0.997442">
2.1 Obtaining Grammatical Relations
</subsectionHeader>
<bodyText confidence="0.999908375">
We obtain the GRs using the recent, second release
of the RASP toolkit (Briscoe et al., 2006). RASP is a
modular statistical parsing system which includes a
tokenizer, tagger, lemmatizer, and a wide-coverage
unification-based tag-sequence parser. We use the
standard scripts supplied with RASP to output the set
of GRs for the most probable analysis returned by the
parser or, in the case of parse failures, the GRs for
the most likely sequence of subanalyses. The GRs
are organized as a subsumption hierarchy as shown
in Figure 1.
The dependency relationships which the GRs em-
body correspond closely to the head-complement
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of easy
</bodyText>
<page confidence="0.996677">
913
</page>
<figure confidence="0.997498">
ncmod xmod cmod pmod
ta arg mod det aux conj
arg
mod
ncsubj xsubj csubj
subj
dependent
dobj obj2 iobj xcomp ccomp
subj dobj
obj pcomp clausal
comp
</figure>
<figureCaption confidence="0.999792">
Figure 1: The GR hierarchy used by RASP
</figureCaption>
<figure confidence="0.922843153846154">
i
1
SUBJECT NP 1r , ll
IVAL
ADJ-COMPS CPP[NP 3l&apos;
MOOD to-infinitivel
SUBJECT 3 J
OMISSION 1 )i
xcomp ?Y : pos=vb,val=be ?X : pos=adj
xcomp ?S : val=to ?Y : pos=vb,val=be ?W : pos=VV0
ncsubj ?Y : pos=vb,val=be ?Z : pos=noun
ncmod ?T : val=for ?X : pos=adj ?Y: pos=pron
ncsubj ?W : pos=VV0 ?V : pos=pron
</figure>
<figureCaption confidence="0.979271">
Figure 2: Feature structure for SCF
</figureCaption>
<equation confidence="0.934432416666667">
adj-obj-for-to-inf
(|These:1_DD2 ||example+s:2_NN2 ||of:3_IO|
|animal:4_JJ ||senses:5_NN2 ||be+:6_VBR|
|relatively:7_RR ||easy:8_JJ ||for:9_IF|
|we+:10_PPIO2 ||to:11_TO ||comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xcomp(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
</equation>
<figureCaption confidence="0.995255">
Figure 3: GRs from RASP for adj-obj-for-to-inf
</figureCaption>
<bodyText confidence="0.995330058823529">
in the sentence: These examples of animal senses
are relatively easy for us to comprehend as they are
not too far removed from our own experience. Ac-
cording to the COMLEX classification, this is an ex-
ample of the frame adj-obj-for-to-inf, shown in
Figure 2, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence is shown in Figure 3.
Each instantiated GR in Figure 3 corresponds to
one or more parts of the feature structure in Fig-
ure 2. xcomp( be[6] easy[8]) establishes be[6]
as the head of the VP in which easy[8] occurs as
a complement. The first (PP)-complement is for us,
as indicated by ncmod(for[9] easy[8] we+[10]),
with for as PFORM and we+ (us) as NP. The sec-
ond complement is represented by xcomp(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
</bodyText>
<figureCaption confidence="0.990154">
Figure 4: Pattern for frame adj-obj-for-to-inf
</figureCaption>
<bodyText confidence="0.999663375">
NP headed by examples is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) correspondsto
the coindexation marked by 3 : the subject of the
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
</bodyText>
<subsectionHeader confidence="0.993682">
2.2 SCF Classifier
SCF Frames
</subsectionHeader>
<bodyText confidence="0.9999945625">
The SCFs recognized by the classifier were ob-
tained by manually merging the frames exempli-
fied in the COMLEX Syntax (Grishman et al., 1994),
ANLT (Boguraev et al., 1987) and/or NOMLEX
(Macleod et al., 1997) dictionaries and including
additional frames found by manual inspection of
unclassifiable examples during development of the
classifier. These consisted of e.g. some occurrences
of phrasal verbs with complex complementation and
with flexible ordering of the preposition/particle,
some non-passivizable words with a surface direct
object, and some rarer combinations of governed
preposition and complementizer combinations.
The frames were created so that they abstract
over specific lexically-governed particles and prepo-
sitions and specific predicate selectional preferences
</bodyText>
<page confidence="0.991543">
914
</page>
<bodyText confidence="0.990777018867925">
but include some derived semi-predictable bounded
dependency constructions.
Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against one
or more rules which express the possible mappings
from GRs to SCFs. The rules were manually devel-
oped by examining a set of development sentences
to determine which relations were actually emitted
by the parser for each SCF.
In our rule representation, a GR pattern is a set of
partially instantiated GRs with variables in place of
heads and dependents, augmented with constraints
that restrict the possible instantiations of the vari-
ables. A match is successful if the set of GRs for
a sentence can be unified with any rule. Unifica-
tion of sentence GRs and a rule GR pattern occurs
when there is a one-to-one correspondence between
sentence elements and rule elements that includes a
consistent mapping from variables to values.
A sample pattern for matching
adj-obj-for-to-inf can be seen in Fig-
ure 4. Each element matches either an empty GR
slot ( ), a variable with possible constraints on part
of speech (pos) and word value (val), or an already
instantiated variable. Unlike in Yallop’s work (Yal-
lop et al., 2005), our rules are declarative rather than
procedural and these rules, written independently
of the acquisition system, are expanded by the
system in a number of ways prior to execution. For
example, the verb rules which contain an ncsubj
relation will not contain one inside an embedded
clause. For verbs, the basic rule set contains 248
rules but automatic expansion gives rise to 1088
classifier rules for verbs.
Numerous approaches were investigated to allow
an efficient execution of the system: for example, for
each target word in a sentence, we initially find the
number of ARGument GRs (see Figure 1) containing
it in head position, as the word must appear in ex-
actly the same set in a matching rule. This allows
us to discard all patterns which specify a different
number of GRs: for example, for verbs each group
only contains an average of 109 patterns.
For a further increase in speed, both the sentence
GRs and the GRs within the patterns are ordered (ac-
cording to frequency) and matching is performed us-
ing a backing off strategy allowing us to exploit the
relatively low number of possible GRs (compared
to the number of possible rules). The system exe-
cutes on 3500 sentences in approx. 1.5 seconds of
real time on a machine with a 3.2 GHz Intel Xenon
processor and 4GB of RAM.
</bodyText>
<subsectionHeader confidence="0.631514">
Lexicon Creation and Filtering
</subsectionHeader>
<bodyText confidence="0.99992845">
Lexical entries are constructed for each word and
SCF combination found in the corpus data. Each lex-
ical entry includes the raw and relative frequency of
the SCF with the word in question, and includes var-
ious additional information e.g. about the syntax of
detected arguments and the argument heads in dif-
ferent argument positions1.
Finally the entries are filtered to obtain a more
accurate lexicon. A way to maximise the accu-
racy of the lexicon would be to smooth (correct) the
acquired SCF distributions with back-off estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002) (see section 4) before filtering them.
However, in this first experiment with the new sys-
tem we filtered the entries directly so that we could
evaluate the performance of the new classifier with-
out any additional modules. For the same reason, the
filtering was done by using a very simple method:
by setting empirically determined thresholds on the
relative frequencies of SCFs.
</bodyText>
<sectionHeader confidence="0.998324" genericHeader="introduction">
3 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.968024">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999867071428572">
In order to test the accuracy of our system, we se-
lected a set of 183 verbs, 30 nouns and 30 adjec-
tives for experimentation. The words were selected
at random, subject to the constraint that they exhib-
ited multiple complementation patterns and had a
sufficient number of corpus occurrences (&gt; 150) for
experimentation. We took the 100M-word British
National Corpus (BNC) (Burnard, 1995), and ex-
tracted all sentences containing an occurrence of one
of the test words. The sentences were processed us-
ing the SCF acquisition system described in the pre-
vious section. The citations from which entries were
derived totaled approximately 744K for verbs and
219K for nouns and adjectives, respectively.
</bodyText>
<footnote confidence="0.8943025">
1The lexical entries are similar to those in the VALEX lexi-
con. See (Korhonen et al., 2006) for a sample entry.
</footnote>
<page confidence="0.990986">
915
</page>
<subsectionHeader confidence="0.996269">
3.2 Gold Standard
</subsectionHeader>
<bodyText confidence="0.999941135135135">
Our gold standard was based on a manual analysis
of some of the test corpus data, supplemented with
additional frames from the ANLT, COMLEX, and/or
NOMLEX dictionaries. The gold standard for verbs
was available, but it was extended to include addi-
tional SCFs missing from the old system. For nouns
and adjectives the gold standard was created. For
each noun and adjective, 100-300 sentences from the
BNC (an average of 267 per word) were randomly
extracted. The resulting c. 16K sentences were then
manually associated with appropriate SCFs, and the
SCF frequency counts were recorded.
To alleviate the manual analysis we developed
a tool which first uses the RASP parser with some
heuristics to reduce the number of SCF presented,
and then allows an annotator to select the preferred
choice in a window. The heuristics reduced the av-
erage number of SCFs presented alongside each sen-
tence from 52 to 7. The annotator was also presented
with an example sentence of each SCF and an intu-
itive name for the frame, such as PRED (e.g. Kim
is silly). The program includes an option to record
that particular sentences could not (initially) be clas-
sified. A screenshot of the tool is shown in Figure 5.
The manual analysis was done by two linguists;
one who did the first annotation for the whole data,
and another who re-evaluated and corrected some of
the initial frame assignments, and classified most of
the data left unclassified by the first annotator2). A
total of 27 SCF types were found for the nouns and
30 for the adjectives in the annotated data. The av-
erage number of SCFs taken by nouns was 9 (with
the average of 2 added from dictionaries to supple-
ment the manual annotation) and by adjectives 11
(3 of which were from dictionaries). The latter are
rare and may not be exemplified in the data given the
extraction system.
</bodyText>
<subsectionHeader confidence="0.993218">
3.3 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999657333333333">
We used the standard evaluation metrics to evaluate
the accuracy of the SCF lexicons: type precision (the
percentage of SCF types that the system proposes
</bodyText>
<footnote confidence="0.691433">
2The process precluded measurements of inter-annotator
agreement, but this was judged less important than the enhanced
accuracy of the gold standard data.
</footnote>
<figureCaption confidence="0.99838">
Figure 5: Sample screen of the annotation tool
</figureCaption>
<bodyText confidence="0.999777611111111">
which are correct), type recall (the percentage of SCF
types in the gold standard that the system proposes)
and the F-measure which is the harmonic mean of
type precision and recall.
We also compared the similarity between the ac-
quired unfiltered3 SCF distributions and gold stan-
dard SCF distributions using various measures of
distributional similarity: the Spearman rank corre-
lation (RC), Kullback-Leibler distance (KL), Jensen-
Shannon divergence (JS), cross entropy (CE), skew
divergence (SD) and intersection (IS). The details of
these measures and their application to subcatego-
rization acquisition can be found in (Korhonen and
Krymolowski, 2002).
Finally, we recorded the total number of gold
standard SCFs unseen in the system output, i.e. the
type of false negatives which were never detected
by the classifier.
</bodyText>
<sectionHeader confidence="0.833869" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999846384615385">
Table 1 includes the average results for the 183
verbs. The first column shows the results for Briscoe
and Carroll’s (1997) (B&amp;C) system when this sys-
tem is run with the original classifier but a more
recent version of the parser (Briscoe and Carroll,
2002) and the same filtering technique as our new
system (thresholding based on the relative frequen-
cies of SCFs). The classifier of B&amp;C system is com-
parable to our classifier in the sense that it targets al-
most the same set of verbal SCFs (165 out of the 168;
the 3 additional ones are infrequent in language and
thus unlikely to affect the comparison). The second
column shows the results for our new system (New).
</bodyText>
<footnote confidence="0.986484">
3No threshold was applied to remove the noisy SCFs from
the distributions.
</footnote>
<page confidence="0.993099">
916
</page>
<table confidence="0.99987025">
Measures Verbs - Method
B&amp;C New
Precision (%) 47.3 81.8
Recall (%) 40.4 59.5
F-measure 43.6 68.9
KL 3.24 1.57
JS 0.20 0.11
CE 4.85 3.10
SD 1.39 0.74
RC 0.33 0.66
IS 0.49 0.76
Unseen SCFs 28 17
</table>
<tableCaption confidence="0.999882">
Table 1: Average results for verbs
</tableCaption>
<bodyText confidence="0.99994862962963">
The figures show that the new system clearly per-
forms better than the B&amp;C system. It yields 68.9 F-
measure which is a 25.3 absolute improvement over
the B&amp;C system. The better performance can be ob-
served on all measures, but particularly on SCF type
precision (81.8% with our system vs. 47.3% with the
B&amp;C system) and on measures of distributional sim-
ilarity. The clearly higher IS (0.76 vs. 0.49) and the
fewer gold standard SCFs unseen in the output of the
classifier (17 vs. 28) indicate that the new system is
capable of detecting a higher number of SCFs.
The main reason for better performance is the
ability of the new system to detect a number of chal-
lenging or complex SCFs which the B&amp;C system
could not detect4. The improvement is partly at-
tributable to more accurate parses produced by the
second release of RASP and partly to the improved
SCF classifier developed here. For example, the new
system is now able to distinguish predicative PP ar-
guments, such as I sent him as a messenger from the
wider class of referential PP arguments, supporting
discrimination of several syntactically similar SCFs
with distinct semantics.
Running our system on the adjective and noun test
data yielded the results summarized in Table 2. The
F-measure is lower for nouns (62.2) than for verbs
(68.9); for adjectives it is slightly better (71.9).5
</bodyText>
<footnote confidence="0.9596442">
4The results reported here for the B&amp;C system are lower
than those recently reported in (Korhonen et al., 2006) for the
same set of 183 test verbs. This is because we use an improved
gold standard. However, the results for the B&amp;C system re-
ported using the less ambitious gold standard are still less ac-
curate (58.6 F-measure) than the ones reported here for the new
system.
5The results for different word classes are not directly com-
parable because they are affected by the total number of SCFs
evaluated for each word class, which is higher for verbs and
</footnote>
<table confidence="0.999780818181818">
Measures Nouns Adjectives
Precision (%) 91.2 95.5
Recall (%) 47.2 57.6
F-measure 62.2 71.9
KL 0.91 0.69
JS 0.09 0.05
CE 2.03 2.01
SD 0.48 0.36
RC 0.70 0.77
IS 0.62 0.72
Unseen SCFs 15 7
</table>
<tableCaption confidence="0.998971">
Table 2: Average results for nouns and adjectives
</tableCaption>
<bodyText confidence="0.99991653125">
The noun and adjective classifiers yield very high
precision compared to recall. The lower recall fig-
ures are mostly due to the higher number of gold
standard SCFs unseen in the classifier output (rather
than, for example, the filtering step). This is par-
ticularly evident for nouns for which 15 of the 27
frames exemplified in the gold standard are missing
in the classifier output. For adjectives only 7 of the
30 gold standard SCFs are unseen, resulting in better
recall (57.6% vs. 47.2% for nouns).
For verbs, subcategorization acquisition perfor-
mance often correlates with the size of the input
data to acquisition (the more data, the better perfor-
mance). When considering the F-measure results for
the individual words shown in Table 3 there appears
to be little such correlation for nouns and adjectives.
For example, although there are individual high fre-
quency nouns with high performance (e.g. plan,
freq. 5046, F 90.9) and low frequency nouns with
low performance (e.g. characterisation, freq. 91, F
40.0), there are also many nouns which contradict
the trend (compare e.g. answer, freq. 2510, F 50.0
with fondness, freq. 71, F 85.7).6
Although the SCF distributions for nouns and ad-
jectives appear Zipfian (i.e. the most frequent frames
are highly probable, but most frames are infre-
quent), the total number of SCFs per word is typi-
cally smaller than for verbs, resulting in better resis-
tance to sparse data problems.
There is, however, a clear correlation between
the performance and the type of gold standard SCFs
taken by individual words. Many of the gold stan-
</bodyText>
<footnote confidence="0.67550875">
lower for nouns and adjectives. This particularly applies to the
sensitive measures of distributional similarity.
6The frequencies here refer to the number of citations suc-
cessfully processed by the parser and the classifier.
</footnote>
<page confidence="0.981781">
917
</page>
<table confidence="0.999965">
Noun F Adjective F
abundance 75.0 able 66.7
acknowledgement 47.1 angry 62.5
answer 50.0 anxious 82.4
anxiety 53.3 aware 87.5
apology 50.0 certain 73.7
appearance 46.2 clear 77.8
appointment 66.7 curious 57.1
belief 76.9 desperate 83.3
call 58.8 difficult 77.8
characterisation 40.0 doubtful 63.6
communication 40.0 eager 83.3
condition 66.7 easy 66.7
danger 76.9 generous 57.1
decision 70.6 imperative 81.8
definition 42.8 important 60.9
demand 66.7 impractical 71.4
desire 71.4 improbable 54.6
doubt 66.7 insistent 80.0
evidence 66.7 kind 66.7
examination 54.6 likely 66.7
experimentation 60.0 practical 88.9
fondness 85.7 probable 80.0
message 66.7 sure 84.2
obsession 54.6 unaware 85.7
plan 90.9 uncertain 60.0
provision 70.6 unclear 63.2
reminder 63.2 unimportant 61.5
rumour 61.5 unlikely 69.6
temptation 71.4 unspecified 50.0
use 60.0 unsure 90.0
</table>
<tableCaption confidence="0.8655435">
Table 3: System performance for each test noun and
adjective
</tableCaption>
<bodyText confidence="0.999846">
dard nominal and adjectival SCFs unseen by the
classifier involve complex complementation patterns
which are challenging to extract, e.g. those exem-
plified in The argument of Jo with Kim about Fido
surfaced, Jo’s preference that Kim be sacked sur-
faced, and that Sandy came is certain. In addition,
many of these SCFs unseen in the data are also very
low in frequency, and some may even be true nega-
tives (recall that the gold standard was supplemented
with additional SCFs from dictionaries, which may
not necessarily appear in the test data).
The main problem is that the RASP parser system-
atically fails to select the correct analysis for some
SCFs with nouns and adjectives regardless of their
context of occurrence. In future work, we hope to al-
leviate this problem by using the weighted GR output
from the top n-ranked parses returned by the parser
as input to the SCF classifier.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9999855">
The current system needs refinement to alleviate the
bias against some SCFs introduced by the parser’s
unlexicalized parse selection model. We plan to in-
vestigate using weighted GR output with the clas-
sifier rather than just the GR set from the highest
ranked parse. Some SCF classes also need to be fur-
ther resolved mainly to differentiate control options
with predicative complementation. This requires a
lexico-semantic classification of predicate classes.
Experiments with Briscoe and Carroll’s system
have shown that it is possible to incorporate some
semantic information in the acquisition process us-
ing a technique that smooths the acquired SCF dis-
tributions using back-off (i.e. probability) estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002). The estimates help to correct the ac-
quired SCF distributions and predict SCFs which are
rare or unseen e.g. due to sparse data. They could
also form the basis for predicting control of predica-
tive complements.
We plan to modify and extend this technique for
the new system and use it to improve the perfor-
mance further. The technique has so far been applied
to verbs only, but it can also be applied to nouns
and adjectives because they can also be classified on
lexical-semantic grounds. For example, the adjec-
tive simple belongs to the class of EASY adjectives,
and this knowledge can help to predict that it takes
similar SCFs to the other class members and that
control of ‘understood’ arguments will pattern with
easy (e.g. easy, difficult, convenient): The problem
will be simple for John to solve, For John to solve
the problem will be simple, The problem will be sim-
ple to solve, etc.
Further research is needed before highly accurate
lexicons encoding information also about semantic
aspects of subcategorization (e.g. different predicate
senses, the mapping from syntactic arguments to
semantic representation of argument structure, se-
lectional preferences on argument heads, diathesis
alternations, etc.) can be obtained automatically.
However, with the extensions suggested above, the
system presented here is sufficiently accurate for
building an extensive SCF lexicon capable of sup-
porting various NLP application tasks. Such a lex-
icon will be built and distributed for research pur-
</bodyText>
<page confidence="0.956124">
918
</page>
<bodyText confidence="0.924187">
poses along with the gold standard described here.
L. Burnard, 1995. The BNC Users Reference Guide. British
National Corpus Consortium, Oxford, May.
</bodyText>
<sectionHeader confidence="0.996936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999047368421">
We have described the first system for automatically
acquiring verbal, nominal and adjectival subcat-
egorization and associated frequency information
from English corpora, which can be used to build
large-scale lexicons for NLP purposes. We have
also described a new annotation tool for producing
training and test data for the task. The acquisition
system, which is capable of distinguishing 168
verbal, 37 adjectival and 31 nominal frames, clas-
sifies corpus occurrences to SCFs on the basis of
GRs produced by a robust statistical parser. The
information provided by GRs closely matches the
structure that subcategorization acquisition seeks
to recover. Our experiment shows that the system
achieves state-of-the-art performance with each
word class. The discussion suggests ways in which
we could improve the system further before using it
to build a large subcategorization lexicon capable of
supporting various NLP application tasks.
</bodyText>
<sectionHeader confidence="0.996056" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99453">
This work was supported by the Royal Society and
UK EPSRC project ‘Accurate and Comprehensive
Lexical Classification for Natural Language Pro-
cessing Applications’ (ACLEX). We would like to
thank Diane Nicholls for her help during this work.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736965517241">
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon
from the Longman Dictionary of Contemporary English. In
Proc. of the 25th Annual Meeting of ACL, pages 193–200,
Stanford, CA.
M. Brent. 1991. Automatic acquisition of subcategorization
frames from untagged text. In Proc. of the 29th Meeting of
ACL, pages 209–214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction of
Subcategorization from Corpora. In Proc. of the 5th ANLP,
Washington DC, USA.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd LREC, pages
1499–1504, Las Palmas, Canary Islands, May.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the rasp system. In Proc. of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney, Australia.
G. Carroll and M. Rooth. 1998. Valence induction with a head-
lexicalized pcfg. In Proc. of the 3rd Conference on EMNLP,
Granada, Spain.
J. Carroll, G. Minnen, and E. J. Briscoe. 1998. Can Subcat-
egorisation Probabilities Help a Statistical Parser? In Pro-
ceedings of the 6th ACL/SIGDAT Workshop on Very Large
Corpora, pages 118–126, Montreal, Canada.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX
Syntax: Building a Computational Lexicon. In COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness
of Entropy-Based Similarity Measures in Evaluation of Sub-
categorization Acquisition Systems. In Proc. of the Sixth
CoNLL, pages 91–97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering
Polysemic Subcategorization Frame Distributions Semanti-
cally. In Proc. of the 41st Annual Meeting of ACL, pages
64–71, Sapporo, Japan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A
large subcategorization lexicon for natural language process-
ing applications. In Proc. of the 5th LREC, Genova, Italy.
A. Korhonen. 2002. Subcategorization acquisition. Ph.D. the-
sis, University of Cambridge Computer Laboratory.
C. Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves.
1997. Designing a dictionary of derived nominals. In Proc.
ofRANLP, Tzigov Chark, Bulgaria.
C. Manning. 1993. Automatic Acquisition of a Large Subcat-
egorization Dictionary from Corpora. In Proc. of the 31st
Meeting ofACL, pages 235–242.
S. Schulte im Walde and C. Brew. 2002. Inducing german se-
mantic verb classes from purely syntactic subcategorisation
information. In Proc. of the 40th Annual Meeting of ACL,
Philadephia, USA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proc. of the 41st Annual Meeting ofACL, Sapporo.
J. Yallop, A. Korhonen, and E. J. Briscoe. 2005. Auto-
matic acquisition of adjectival subcategorization from cor-
pora. In Proc. of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 614–621, Ann Arbor,
Michigan.
</reference>
<page confidence="0.998767">
919
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805595">
<title confidence="0.9884895">A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival Subcategorization Frames from Corpora</title>
<author confidence="0.999693">Judita Preiss</author>
<author confidence="0.999693">Ted Briscoe</author>
<author confidence="0.999693">Anna Korhonen</author>
<affiliation confidence="0.999994">Computer Laboratory University of Cambridge</affiliation>
<address confidence="0.994546">15 JJ Thomson Avenue Cambridge CB3 0FD, UK</address>
<email confidence="0.845419">Judita.Preiss,Ted.Briscoe,Anna.Korhonen@cl.cam.ac.uk</email>
<abstract confidence="0.998777583333333">This paper describes the first system for large-scale acquisition of subcategorization from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The system incorporates an extensive rulebased classifier which identifies 168 verbal, 37 adjectival and 31 nominal frames from relations output by a robust parser. The system achieves state-ofthe-art performance on all three sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>J Carroll</author>
<author>E J Briscoe</author>
<author>D Carter</author>
<author>C Grover</author>
</authors>
<title>The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English.</title>
<date>1987</date>
<booktitle>In Proc. of the 25th Annual Meeting of ACL,</booktitle>
<pages>193--200</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="10042" citStr="Boguraev et al., 1987" startWordPosition="1548" endWordPosition="1551">frame adj-obj-for-to-inf NP headed by examples is marked as the subject of the frame by ncsubj(be[6] examples[2]), and ncsubj(comprehend[12] we+[10]) correspondsto the coindexation marked by 3 : the subject of the VP is the NP of the PP. The only part of the feature structure which is not represented by the GRs is coindexation between the omitted direct object 1 of the VP-complement and the subject of the whole clause. 2.2 SCF Classifier SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and specific predicate selectiona</context>
</contexts>
<marker>Boguraev, Carroll, Briscoe, Carter, Grover, 1987</marker>
<rawString>B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover. 1987. The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English. In Proc. of the 25th Annual Meeting of ACL, pages 193–200, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text.</title>
<date>1991</date>
<booktitle>In Proc. of the 29th Meeting of ACL,</booktitle>
<pages>209--214</pages>
<contexts>
<context position="1964" citStr="Brent, 1991" startWordPosition="280" endWordPosition="281">orization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has foc</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>M. Brent. 1991. Automatic acquisition of subcategorization frames from untagged text. In Proc. of the 29th Meeting of ACL, pages 209–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic Extraction of Subcategorization from Corpora.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th ANLP,</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="2085" citStr="Briscoe and Carroll, 1997" startWordPosition="295" endWordPosition="299">f successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution dat</context>
<context position="5746" citStr="Briscoe and Carroll, 1997" startWordPosition="858" endWordPosition="861">notation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to significantly reduce the number of frames presented to the user for each sentence. We introduce the new system for SCF acquisition in section 2. Details of the experimental evaluation are supplied in section 3. Section 4 provides discussion of our results and future work, and section 5 concludes. 2 Description of the System A common strategy in existing large-scale SCF acquisition systems (e.g. (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GRs — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCFs. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 200</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>E. J. Briscoe and J. Carroll. 1997. Automatic Extraction of Subcategorization from Corpora. In Proc. of the 5th ANLP, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd LREC,</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="6348" citStr="Briscoe and Carroll, 2002" startWordPosition="954" endWordPosition="957">iscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GRs — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCFs. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCFs, 3) building lexical entries from the classified data, and 4) filtering those entries to obtain a more accurate lexicon. The details of these steps are provided in the subsequent sections. 2.1 Obtaining Grammatical Relations We obtain the GRs using the recent, second release of the RASP t</context>
<context position="18327" citStr="Briscoe and Carroll, 2002" startWordPosition="2905" endWordPosition="2908">ropy (CE), skew divergence (SD) and intersection (IS). The details of these measures and their application to subcategorization acquisition can be found in (Korhonen and Krymolowski, 2002). Finally, we recorded the total number of gold standard SCFs unseen in the system output, i.e. the type of false negatives which were never detected by the classifier. 3.4 Results Table 1 includes the average results for the 183 verbs. The first column shows the results for Briscoe and Carroll’s (1997) (B&amp;C) system when this system is run with the original classifier but a more recent version of the parser (Briscoe and Carroll, 2002) and the same filtering technique as our new system (thresholding based on the relative frequencies of SCFs). The classifier of B&amp;C system is comparable to our classifier in the sense that it targets almost the same set of verbal SCFs (165 out of the 168; the 3 additional ones are infrequent in language and thus unlikely to affect the comparison). The second column shows the results for our new system (New). 3No threshold was applied to remove the noisy SCFs from the distributions. 916 Measures Verbs - Method B&amp;C New Precision (%) 47.3 81.8 Recall (%) 40.4 59.5 F-measure 43.6 68.9 KL 3.24 1.57</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proc. of the 3rd LREC, pages 1499–1504, Las Palmas, Canary Islands, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the rasp system.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4803" citStr="Briscoe et al., 2006" startWordPosition="714" endWordPosition="717">ctions accurately. In this paper we present the first system for largescale acquisition of SCFs from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The classifier incorporates 168 verbal, 37 adjectival and 31 nominal SCF distinctions. An improved acquisition technique is used which expands on the ideas Yallop et al. (2005) recently explored for a small experiment on adjectival SCF acquisition. It involves identifying SCFs on the basis of grammatical relations (GRs) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe et al., 2006). As detailed later, the system performs better with verbs than previous comparable state-of-art systems, achieving 68.9 F-measure in detecting SCF types. It achieves similarly good performance with nouns and adjectives (62.2 and 71.9 F-measure, respectively). Additionally, we have developed a tool for linguistic annotation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to significantly reduce the number of frames presented to the user for each sentence.</context>
<context position="6977" citStr="Briscoe et al., 2006" startWordPosition="1054" endWordPosition="1057">sults reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCFs, 3) building lexical entries from the classified data, and 4) filtering those entries to obtain a more accurate lexicon. The details of these steps are provided in the subsequent sections. 2.1 Obtaining Grammatical Relations We obtain the GRs using the recent, second release of the RASP toolkit (Briscoe et al., 2006). RASP is a modular statistical parsing system which includes a tokenizer, tagger, lemmatizer, and a wide-coverage unification-based tag-sequence parser. We use the standard scripts supplied with RASP to output the set of GRs for the most probable analysis returned by the parser or, in the case of parse failures, the GRs for the most likely sequence of subanalyses. The GRs are organized as a subsumption hierarchy as shown in Figure 1. The dependency relationships which the GRs embody correspond closely to the head-complement structure which subcategorization acquisition attempts to recover, wh</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second release of the rasp system. In Proc. of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Valence induction with a headlexicalized pcfg.</title>
<date>1998</date>
<booktitle>In Proc. of the 3rd Conference on EMNLP,</booktitle>
<location>Granada,</location>
<contexts>
<context position="2058" citStr="Carroll and Rooth, 1998" startWordPosition="291" endWordPosition="294">tal for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and althoug</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>G. Carroll and M. Rooth. 1998. Valence induction with a headlexicalized pcfg. In Proc. of the 3rd Conference on EMNLP, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>E J Briscoe</author>
</authors>
<title>Can Subcategorisation Probabilities Help a Statistical Parser?</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>118--126</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1520" citStr="Carroll et al., 1998" startWordPosition="214" endWordPosition="217">eb, corpora of published text, etc.) is starting to produce large scale lexical resources which include frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detect</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1998</marker>
<rawString>J. Carroll, G. Minnen, and E. J. Briscoe. 1998. Can Subcategorisation Probabilities Help a Statistical Parser? In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora, pages 118–126, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>COMLEX Syntax: Building a Computational Lexicon. In</title>
<date>1994</date>
<booktitle>COLING, Kyoto.</booktitle>
<contexts>
<context position="10012" citStr="Grishman et al., 1994" startWordPosition="1543" endWordPosition="1546">VP. The Figure 4: Pattern for frame adj-obj-for-to-inf NP headed by examples is marked as the subject of the frame by ncsubj(be[6] examples[2]), and ncsubj(comprehend[12] we+[10]) correspondsto the coindexation marked by 3 : the subject of the VP is the NP of the PP. The only part of the feature structure which is not represented by the GRs is coindexation between the omitted direct object 1 of the VP-complement and the subject of the whole clause. 2.2 SCF Classifier SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX Syntax: Building a Computational Lexicon. In COLING, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
</authors>
<title>On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems.</title>
<date>2002</date>
<booktitle>In Proc. of the Sixth CoNLL,</booktitle>
<pages>91--97</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="17889" citStr="Korhonen and Krymolowski, 2002" startWordPosition="2830" endWordPosition="2833">type recall (the percentage of SCF types in the gold standard that the system proposes) and the F-measure which is the harmonic mean of type precision and recall. We also compared the similarity between the acquired unfiltered3 SCF distributions and gold standard SCF distributions using various measures of distributional similarity: the Spearman rank correlation (RC), Kullback-Leibler distance (KL), JensenShannon divergence (JS), cross entropy (CE), skew divergence (SD) and intersection (IS). The details of these measures and their application to subcategorization acquisition can be found in (Korhonen and Krymolowski, 2002). Finally, we recorded the total number of gold standard SCFs unseen in the system output, i.e. the type of false negatives which were never detected by the classifier. 3.4 Results Table 1 includes the average results for the 183 verbs. The first column shows the results for Briscoe and Carroll’s (1997) (B&amp;C) system when this system is run with the original classifier but a more recent version of the parser (Briscoe and Carroll, 2002) and the same filtering technique as our new system (thresholding based on the relative frequencies of SCFs). The classifier of B&amp;C system is comparable to our cl</context>
</contexts>
<marker>Korhonen, Krymolowski, 2002</marker>
<rawString>A. Korhonen and Y. Krymolowski. 2002. On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems. In Proc. of the Sixth CoNLL, pages 91–97, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
<author>Z Marx</author>
</authors>
<title>Clustering Polysemic Subcategorization Frame Distributions Semantically.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of ACL,</booktitle>
<pages>64--71</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2272" citStr="Korhonen et al., 2003" startWordPosition="324" endWordPosition="327">on which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at</context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering Polysemic Subcategorization Frame Distributions Semantically. In Proc. of the 41st Annual Meeting of ACL, pages 64–71, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
<author>E J Briscoe</author>
</authors>
<title>A large subcategorization lexicon for natural language processing applications.</title>
<date>2006</date>
<booktitle>In Proc. of the 5th LREC,</booktitle>
<location>Genova, Italy.</location>
<contexts>
<context position="2492" citStr="Korhonen et al., 2006" startWordPosition="356" endWordPosition="360">rization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at several levels (from tagging to syntactic and semantic analysis). Furthermore the selection of the correct analysis from the set returned by a parser which does not initially utilize fine-grained lexico-syntactic inform</context>
<context position="14985" citStr="Korhonen et al., 2006" startWordPosition="2350" endWordPosition="2353">ct to the constraint that they exhibited multiple complementation patterns and had a sufficient number of corpus occurrences (&gt; 150) for experimentation. We took the 100M-word British National Corpus (BNC) (Burnard, 1995), and extracted all sentences containing an occurrence of one of the test words. The sentences were processed using the SCF acquisition system described in the previous section. The citations from which entries were derived totaled approximately 744K for verbs and 219K for nouns and adjectives, respectively. 1The lexical entries are similar to those in the VALEX lexicon. See (Korhonen et al., 2006) for a sample entry. 915 3.2 Gold Standard Our gold standard was based on a manual analysis of some of the test corpus data, supplemented with additional frames from the ANLT, COMLEX, and/or NOMLEX dictionaries. The gold standard for verbs was available, but it was extended to include additional SCFs missing from the old system. For nouns and adjectives the gold standard was created. For each noun and adjective, 100-300 sentences from the BNC (an average of 267 per word) were randomly extracted. The resulting c. 16K sentences were then manually associated with appropriate SCFs, and the SCF fre</context>
<context position="20499" citStr="Korhonen et al., 2006" startWordPosition="3287" endWordPosition="3290">ly to the improved SCF classifier developed here. For example, the new system is now able to distinguish predicative PP arguments, such as I sent him as a messenger from the wider class of referential PP arguments, supporting discrimination of several syntactically similar SCFs with distinct semantics. Running our system on the adjective and noun test data yielded the results summarized in Table 2. The F-measure is lower for nouns (62.2) than for verbs (68.9); for adjectives it is slightly better (71.9).5 4The results reported here for the B&amp;C system are lower than those recently reported in (Korhonen et al., 2006) for the same set of 183 test verbs. This is because we use an improved gold standard. However, the results for the B&amp;C system reported using the less ambitious gold standard are still less accurate (58.6 F-measure) than the ones reported here for the new system. 5The results for different word classes are not directly comparable because they are affected by the total number of SCFs evaluated for each word class, which is higher for verbs and Measures Nouns Adjectives Precision (%) 91.2 95.5 Recall (%) 47.2 57.6 F-measure 62.2 71.9 KL 0.91 0.69 JS 0.09 0.05 CE 2.03 2.01 SD 0.48 0.36 RC 0.70 0.</context>
</contexts>
<marker>Korhonen, Krymolowski, Briscoe, 2006</marker>
<rawString>A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A large subcategorization lexicon for natural language processing applications. In Proc. of the 5th LREC, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
</authors>
<title>Subcategorization acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge Computer Laboratory.</institution>
<contexts>
<context position="2102" citStr="Korhonen, 2002" startWordPosition="300" endWordPosition="301">logy (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to of</context>
<context position="13783" citStr="Korhonen, 2002" startWordPosition="2154" endWordPosition="2156"> Creation and Filtering Lexical entries are constructed for each word and SCF combination found in the corpus data. Each lexical entry includes the raw and relative frequency of the SCF with the word in question, and includes various additional information e.g. about the syntax of detected arguments and the argument heads in different argument positions1. Finally the entries are filtered to obtain a more accurate lexicon. A way to maximise the accuracy of the lexicon would be to smooth (correct) the acquired SCF distributions with back-off estimates based on lexical-semantic classes of verbs (Korhonen, 2002) (see section 4) before filtering them. However, in this first experiment with the new system we filtered the entries directly so that we could evaluate the performance of the new classifier without any additional modules. For the same reason, the filtering was done by using a very simple method: by setting empirically determined thresholds on the relative frequencies of SCFs. 3 Experimental Evaluation 3.1 Data In order to test the accuracy of our system, we selected a set of 183 verbs, 30 nouns and 30 adjectives for experimentation. The words were selected at random, subject to the constraint</context>
<context position="25571" citStr="Korhonen, 2002" startWordPosition="4110" endWordPosition="4112">stigate using weighted GR output with the classifier rather than just the GR set from the highest ranked parse. Some SCF classes also need to be further resolved mainly to differentiate control options with predicative complementation. This requires a lexico-semantic classification of predicate classes. Experiments with Briscoe and Carroll’s system have shown that it is possible to incorporate some semantic information in the acquisition process using a technique that smooths the acquired SCF distributions using back-off (i.e. probability) estimates based on lexical-semantic classes of verbs (Korhonen, 2002). The estimates help to correct the acquired SCF distributions and predict SCFs which are rare or unseen e.g. due to sparse data. They could also form the basis for predicting control of predicative complements. We plan to modify and extend this technique for the new system and use it to improve the performance further. The technique has so far been applied to verbs only, but it can also be applied to nouns and adjectives because they can also be classified on lexical-semantic grounds. For example, the adjective simple belongs to the class of EASY adjectives, and this knowledge can help to pre</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>A. Korhonen. 2002. Subcategorization acquisition. Ph.D. thesis, University of Cambridge Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>A Meyers</author>
<author>R Grishman</author>
<author>L Barrett</author>
<author>R Reeves</author>
</authors>
<title>Designing a dictionary of derived nominals.</title>
<date>1997</date>
<booktitle>In Proc. ofRANLP, Tzigov Chark,</booktitle>
<contexts>
<context position="10079" citStr="Macleod et al., 1997" startWordPosition="1554" endWordPosition="1557">examples is marked as the subject of the frame by ncsubj(be[6] examples[2]), and ncsubj(comprehend[12] we+[10]) correspondsto the coindexation marked by 3 : the subject of the VP is the NP of the PP. The only part of the feature structure which is not represented by the GRs is coindexation between the omitted direct object 1 of the VP-complement and the subject of the whole clause. 2.2 SCF Classifier SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences 914 but include some de</context>
</contexts>
<marker>Macleod, Meyers, Grishman, Barrett, Reeves, 1997</marker>
<rawString>C. Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves. 1997. Designing a dictionary of derived nominals. In Proc. ofRANLP, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic Acquisition of a Large Subcategorization Dictionary from Corpora.</title>
<date>1993</date>
<booktitle>In Proc. of the 31st Meeting ofACL,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1980" citStr="Manning, 1993" startWordPosition="282" endWordPosition="283">cess to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. </context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>C. Manning. 1993. Automatic Acquisition of a Large Subcategorization Dictionary from Corpora. In Proc. of the 31st Meeting ofACL, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
<author>C Brew</author>
</authors>
<title>Inducing german semantic verb classes from purely syntactic subcategorisation information.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of ACL,</booktitle>
<location>Philadephia, USA.</location>
<contexts>
<context position="1620" citStr="Walde and Brew, 2002" startWordPosition="229" endWordPosition="232">ude frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>S. Schulte im Walde and C. Brew. 2002. Inducing german semantic verb classes from purely syntactic subcategorisation information. In Proc. of the 40th Annual Meeting of ACL, Philadephia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting ofACL,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="1781" citStr="Surdeanu et al., 2003" startWordPosition="251" endWordPosition="254">performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which co</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using predicate-argument structures for information extraction. In Proc. of the 41st Annual Meeting ofACL, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yallop</author>
<author>A Korhonen</author>
<author>E J Briscoe</author>
</authors>
<title>Automatic acquisition of adjectival subcategorization from corpora.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>614--621</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="4564" citStr="Yallop et al. (2005)" startWordPosition="678" endWordPosition="681">omp |judge) &lt; P(NP&amp;Scomp |persuade).P(None |judge). If we acquired frames and frame frequencies for all open-class predicates taking SCFs using a single system applied to similar data, we would have a better chance of modeling such interactions accurately. In this paper we present the first system for largescale acquisition of SCFs from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The classifier incorporates 168 verbal, 37 adjectival and 31 nominal SCF distinctions. An improved acquisition technique is used which expands on the ideas Yallop et al. (2005) recently explored for a small experiment on adjectival SCF acquisition. It involves identifying SCFs on the basis of grammatical relations (GRs) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe et al., 2006). As detailed later, the system performs better with verbs than previous comparable state-of-art systems, achieving 68.9 F-measure in detecting SCF types. It achieves similarly good performance with nouns and adjectives (62.2 and 71.9 F-measure, respectively). Additionally, we have developed a tool for linguistic annotation of SCFs in corpus data aimed at all</context>
<context position="6140" citStr="Yallop et al. (2005)" startWordPosition="919" endWordPosition="922"> section 3. Section 4 provides discussion of our results and future work, and section 5 concludes. 2 Description of the System A common strategy in existing large-scale SCF acquisition systems (e.g. (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GRs — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCFs. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCFs, 3) building lexical entries from the classified data, and 4) filtering those ent</context>
<context position="11870" citStr="Yallop et al., 2005" startWordPosition="1827" endWordPosition="1831">possible instantiations of the variables. A match is successful if the set of GRs for a sentence can be unified with any rule. Unification of sentence GRs and a rule GR pattern occurs when there is a one-to-one correspondence between sentence elements and rule elements that includes a consistent mapping from variables to values. A sample pattern for matching adj-obj-for-to-inf can be seen in Figure 4. Each element matches either an empty GR slot ( ), a variable with possible constraints on part of speech (pos) and word value (val), or an already instantiated variable. Unlike in Yallop’s work (Yallop et al., 2005), our rules are declarative rather than procedural and these rules, written independently of the acquisition system, are expanded by the system in a number of ways prior to execution. For example, the verb rules which contain an ncsubj relation will not contain one inside an embedded clause. For verbs, the basic rule set contains 248 rules but automatic expansion gives rise to 1088 classifier rules for verbs. Numerous approaches were investigated to allow an efficient execution of the system: for example, for each target word in a sentence, we initially find the number of ARGument GRs (see Fig</context>
</contexts>
<marker>Yallop, Korhonen, Briscoe, 2005</marker>
<rawString>J. Yallop, A. Korhonen, and E. J. Briscoe. 2005. Automatic acquisition of adjectival subcategorization from corpora. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 614–621, Ann Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>