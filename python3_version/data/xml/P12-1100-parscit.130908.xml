<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.976492">
Hierarchical Chunk-to-String Translation∗
</title>
<author confidence="0.992615">
Yang Fengt Dongdong Zhangt Mu Lit Ming Zhout Qun Liu*
</author>
<affiliation confidence="0.693785666666667">
t Department of Computer Science t Microsoft Research Asia
University of Sheffield dozhang@microsoft.com
Sheffield, UK muli@microsoft.com
</affiliation>
<email confidence="0.941869">
y.feng@shef.ac.uk mingzhou@microsoft.com
</email>
<affiliation confidence="0.965425666666667">
*Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<email confidence="0.989357">
liuqun@ict.ac.cn
</email>
<sectionHeader confidence="0.99657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998103875">
We present a hierarchical chunk-to-string
translation model, which can be seen as a
compromise between the hierarchical phrase-
based model and the tree-to-string model,
to combine the merits of the two models.
With the help of shallow parsing, our model
learns rules consisting of words and chunks
and meanwhile introduce syntax cohesion.
Under the weighed synchronous context-free
grammar defined by these rules, our model
searches for the best translation derivation
and yields target translation simultaneously.
Our experiments show that our model signif-
icantly outperforms the hierarchical phrase-
based model and the tree-to-string model on
English-Chinese Translation tasks.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976012166666667">
The hierarchical phrase-based model (Chiang, 2007)
makes an advance of statistical machine translation
by employing hierarchical phrases, which not only
uses phrases to learn local translations but also uses
hierarchical phrases to capture reorderings of words
and subphrases which can cover a large scope. Be-
sides, this model is formal syntax-based and does
not need to specify the syntactic constituents of
subphrases, so it can directly learn synchronous
context-free grammars (SCFG) from a parallel text
without relying on any linguistic annotations or as-
sumptions, which makes it used conveniently and
widely.
∗This work was done when the first author visited Microsoft
Research Asia as an intern.
However, it is often desirable to consider syntac-
tic constituents of subphrases, e.g. the hierarchical
phrase
</bodyText>
<equation confidence="0.885066">
X → (X 1 for X 2 , X 2 de X 1 )
</equation>
<figureCaption confidence="0.8109385">
can be applied to both of the following strings in
Figure 1
</figureCaption>
<bodyText confidence="0.986396153846154">
“A request for a purchase of shares”
“filed for bankruptcy”,
and get the following translation, respectively
“goumai gufen de shenqing”
“pochan de shenqing”.
In the former, “A request” is a NP and this rule acts
correctly while in the latter “filed” is a VP and this
rule gives a wrong reordering. If we specify the first
X on the right-hand side to NP, this kind of errors
can be avoided.
The tree-to-string model (Liu et al., 2006; Huang
et al., 2006) introduces linguistic syntax via source
parse to direct word reordering, especially long-
distance reordering. Furthermore, this model is for-
malised as Tree Substitution Grammars, so it ob-
serves syntactic cohesion. Syntactic cohesion means
that the translation of a string covered by a subtree
in a source parse tends to be continuous. Fox (2002)
shows that translation between English and French
satisfies cohesion in the majority cases. Many pre-
vious works show promising results with an as-
sumption that syntactic cohesion explains almost
all translation movement for some language pairs
(Wu, 1997; Yamada and Knight, 2001; Eisner, 2003;
Graehl and Knight, 2004; Quirk et al., 2005; Cherry,
2008; Feng et al., 2010).
</bodyText>
<page confidence="0.956505">
950
</page>
<note confidence="0.985795">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999782428571429">
But unfortunately, the tree-to-string model re-
quires each node must be strictly matched during
rule matching, which makes it strongly dependent
on the relationship of tree nodes and their roles in
the whole sentence. This will lead to data sparse-
ness and being vulnerable to parse errors.
In this paper, we present a hierarchical chunk-to-
string translation model to combine the merits of the
two models. Instead of parse trees, our model intro-
duces linguistic information in the form of chunks,
so it does not need to care the internal structures and
the roles in the main sentence of chunks. Based on
shallow parsing results, it learns rules consisting of
either words (terminals) or chunks (nonterminals),
where adjacent chunks are packed into one nonter-
minal. It searches for the best derivation through the
SCFG-motivated space defined by these rules and
get target translation simultaneously. In some sense,
our model can be seen as a compromise between
the hierarchical phrase-based model and the tree-to-
string model, specifically
</bodyText>
<note confidence="0.639518">
NP IN NP IN NP VBD VP
NP VBZ VBN IN NP
</note>
<figureCaption confidence="0.9937195">
Figure 1: A running example of two sentences. For each
sentence, the first row gives the chunk sequence.
</figureCaption>
<figure confidence="0.941400466666667">
A request
for a purchase
of shares was made
goumai gufen de shenqing bei dijiao
购买 股份 的 申请 被 递交
The bank
has filed for bankruptcy
gai yinhang yijing shenqing pochan
该 银行 已经 申请 破产
NP VP
has
bank
The
DT
NN
VBZ
VP
PP
VBN
filed
for
IN
NN
NP
S
• Compared with the hierarchical phrase-based
model, it integrates linguistic syntax and sat-
isfies syntactic cohesion.
bankruptcy
(a) A parse tree
</figure>
<listItem confidence="0.752821083333333">
• Compared with the tree-to-string model, it only
needs to perform shallow parsing which intro-
duces less parsing errors. Besides, our model
allows a nonterminal in a rule to cover several
chunks, which can alleviate data sparseness and
the influence of parsing errors.
• we refine our hierarchical chunk-to-string
model into two models: a loose model (Section
2.1) which is more similar to the hierarchical
phrase-based model and a tight model (Section
2.2) which is more similar to the tree-to-string
model.
</listItem>
<bodyText confidence="0.999303111111111">
The experiments show that on the 2008 NIST
English-Chinese MT translation test set, both the
loose model and the tight model outperform the hi-
erarchical phrase-based model and the tree-to-string
model, where the loose model has a better perfor-
mance. While in terms of speed, the tight model
runs faster and its speed ranking is between the tree-
to-string model and the hierarchical phrase-based
model.
</bodyText>
<figure confidence="0.928140333333333">
B-NP I-NP B-VBZ B-VBN B-IN B-NP
The bank has filed for bankruptcy
(b) A chunk sequence got from the parse tree
</figure>
<figureCaption confidence="0.997593">
Figure 2: An example of shallow parsing.
</figureCaption>
<sectionHeader confidence="0.945908" genericHeader="introduction">
2 Modeling
</sectionHeader>
<bodyText confidence="0.999740916666667">
Shallow parsing (also chunking) is an analysis of
a sentence which identifies the constituents (noun
groups, verbs, verb groups, etc), but neither spec-
ifies their internal structures, nor their roles in the
main sentence. In Figure 1, we give the chunk se-
quence in the first row for each sentence. We treat
shallow parsing as a sequence label task, and a sen-
tence f can have many possible different chunk la-
bel sequences. Therefore, in theory, the conditional
probability of a target translation e conditioned on
the source sentence f is given by taking the chunk
label sequences as a latent variable c:
</bodyText>
<equation confidence="0.98243">
p(e|f) = � p(c|f)p(e|f,c) (1)
C
</equation>
<page confidence="0.924437">
951
</page>
<bodyText confidence="0.942187">
In practice, we only take the best chunk label se- So the best translation is given by
quence c� got by
</bodyText>
<equation confidence="0.97386">
c� = argmax p(c|f) (2) �e� = argmax AkHk(d, e, c, f) (6)
c e k
</equation>
<bodyText confidence="0.947472">
Then we can ignore the conditional probability
p(8|f) as it holds the same value for each transla-
tion, and get:
</bodyText>
<equation confidence="0.9975935">
p(e|f) = p(�c|f)p(e|f, 8)
= p(e|f, 8) (3)
</equation>
<bodyText confidence="0.99996925">
We formalize our model as a weighted SCFG.
In a SCFG, each rule (usually called production in
SCFGs) has an aligned pair of right-hand sides —
the source side and the target side, just as follows:
</bodyText>
<equation confidence="0.979235">
X —* (α, Q, —)
</equation>
<bodyText confidence="0.997204941176471">
where X is a nonterminal, α and Q are both strings of
terminals and nonterminals, and — denotes one-to-
one links between nonterminal occurrences in α and
nonterminal occurrences in Q. A SCFG produces a
derivation by starting with a pair of start symbols
and recursively rewrites every two coindexed non-
terminals with the corresponding components of a
matched rule. A derivation yields a pair of strings
on the right-hand side which are translation of each
other.
In a weighted SCFG, each rule has a weight and
the total weight of a derivation is the production
of the weights of the rules used by the derivation.
A translation may be produced by many different
derivations and we only use the best derivation to
evaluate its probability. With d denoting a deriva-
tion and r denoting a rule, we have
</bodyText>
<equation confidence="0.87748">
p(e|f) = max p(d, e|f, �c)
d
= max
d � p(r, e|f, 8) (4)
r∈d
</equation>
<bodyText confidence="0.987057">
Following Och and Ney (2002), we frame our model
as a log-linear model:
</bodyText>
<equation confidence="0.76514">
_ exp Ek AkHk (d, e, 6, f )
p(e|f) f) — exp Ed′,e′,k AkHk(d′, e′, �c, f) (5)
�where Hk(d, e, ��, f) = hk(f, c,r)
r
</equation>
<bodyText confidence="0.999949214285714">
We employ the same set of features for the log-
linear model as the hierarchical phrase-based model
does(Chiang, 2005).
We further refine our hierarchical chunk-to-string
model into two models: a loose model which is more
similar to the hierarchical phrase-based model and
a tight model which is more similar to the tree-to-
string model. The two models differ in the form of
rules and the way of estimating rule probabilities.
While for decoding, we employ the same decoding
algorithm for the two models: given a test sentence,
the decoders first perform shallow parsing to get the
best chunk sequence, then apply a CYK parsing al-
gorithm with beam search.
</bodyText>
<subsectionHeader confidence="0.963313">
2.1 A Loose Model
</subsectionHeader>
<bodyText confidence="0.9999728">
In our model, we employ rules containing non-
terminals to handle long-distance reordering where
boundary words play an important role. So for the
subphrases which cover more than one chunk, we
just maintain boundary chunks: we bundle adjacent
chunks into one nonterminal and denote it as the first
chunk tag immediately followed by “-” and next fol-
lowed by the last chunk tag. Then, for the string pair
&lt;filed for bankruptcy, shenqing pochan&gt;, we can
get the rule
</bodyText>
<equation confidence="0.977889">
r1 : X —* (VBN 1 for NP 2 , VBN 1 NP 2 )
</equation>
<bodyText confidence="0.997775">
while for the string pair &lt;A request for a purchase
of shares, goumai gufen de shenqing&gt;, we can get
</bodyText>
<equation confidence="0.689444">
r2 : X —* (NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ).
</equation>
<bodyText confidence="0.999039">
The rule matching “A request for a purchase of
shares was” will be
</bodyText>
<equation confidence="0.987377">
r3 : X —* (NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ).
</equation>
<bodyText confidence="0.99948825">
We can see that in contrast to the method of rep-
resenting each chunk separately, this representation
form can alleviate data sparseness and the influence
of parsing errors.
</bodyText>
<page confidence="0.992293">
952
</page>
<table confidence="0.988944625">
hS 1, S 1 i ⇒hS 2 X 3 , S 2 X 3 i
⇒ hX 4 X 3 , X 4 X 3 i
⇒ hNP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 i
⇒ hNP 7 for NP-NP 8 VBD 6 X 3 , NP-NP8 de NP 7 VBD 6 X 3 i
⇒ hA request for NP-NP8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 i
⇒ hA request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 i
⇒ hA request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 i
⇒ hA request for a purchase of shares was made, goumai gufen de shenqing bei dijiaoi
(a) The loose model
hNP-VP 1, NP-VP 1 i ⇒ hNP-VBD 2 VP 3 , NP-VBD 2 VP 3 i
⇒ hNP-NP 4 VBD 5 VP 3 , NP-NP 4 VBD 5 VP 3 i
⇒ hNP 6 for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de NP 6 VBD 5 VP 3 i
⇒ hA request for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de shenqing VBD 5 VP 3 i
⇒ hA request for a purchase of shares VBD 5 VP 3 , goumai gufen de shenqing VBD 5 VP 3 i
⇒ hA request for a purchase of shares was VP 3 , goumai gufen de shenqing bei VP 3 i
⇒ hA request for a purchase of shares was made, goumai gufen de shenqing bei dijiaoi
</table>
<figure confidence="0.943828">
(b) The tight model
</figure>
<figureCaption confidence="0.999849">
Figure 3: The derivations of the sentence in Figure 1(a).
</figureCaption>
<bodyText confidence="0.999665333333333">
In these rules, the left-hand nonterminal symbol X
can not match any nonterminal symbol on the right-
hand side. So we need a set of rules such as
</bodyText>
<equation confidence="0.9062275">
NP → hX 1, X 1 i
NP-NP → hX 1 , X 1 i
</equation>
<bodyText confidence="0.893923857142857">
and so on, and set the probabilities of these rules to
1. To simplify the derivation, we discard this kind of
rules and assume that X can match any nonterminal
on the right-hand side.
Only with r2 and r3, we cannot produce any
derivation of the whole sentence in Figure 1 (a). In
this case we need two special glue rules:
</bodyText>
<equation confidence="0.997432">
r4 : S → hS 1 X 2 , S 1 X 2 i
r5 : S → hX 1 , X 1 i
</equation>
<bodyText confidence="0.50131">
Together with the following four lexical rules,
</bodyText>
<equation confidence="0.9997835">
r6 : X → ha request, shenqingi
r7 : X → ha purchase of shares, goumai gufeni
r8 : X → hwas, beii
rg : X → hmade, dijiaoi
</equation>
<figureCaption confidence="0.9946245">
Figure 3(a) shows the derivation of the sentence in
Figure 1(a).
</figureCaption>
<subsectionHeader confidence="0.995789">
2.2 A Tight Model
</subsectionHeader>
<bodyText confidence="0.996182444444444">
In the tight model, the right-hand side of each rule
remains the same as the loose model, but the left-
hand side nonterminal is not X but the correspond-
ing chunk labels. If a rule covers more than one
chunk, we just use the first and the last chunk la-
bels to denote the left-hand side nonterminal. The
rule set used in the tight model for the example in
Figure 1(a) corresponding to that in the loose model
becomes:
</bodyText>
<equation confidence="0.999781">
r2 : NP-NP → hNP 1 for NP-NP 2 , NP-NP 2 de NP 1 i
r3 : NP-VBD → hNP-NP 1 VBD 2 , NP-NP 1 VBD 2 i-
r6 : NP → ha request, shenqingi
r7 : NP-NP → ha purchase of shares, goumai gufeni
r8 : VBD → hwas, beii
rg : VP → hmade, dijiaoi
</equation>
<bodyText confidence="0.999393">
During decoding, we first collect rules for each
span. For a span which does not have any matching
rule, if we do not construct default rules for it, there
will be no derivation for the whole sentence, then we
need to construct default rules for this kind of span
by enumerating all possible binary segmentation of
the chunks in this span. For the example in Figure
1(a), there is no rule matching the whole sentence,
</bodyText>
<page confidence="0.965465">
953
</page>
<bodyText confidence="0.8599115">
so we need to construct default rules for it, which
should be
</bodyText>
<equation confidence="0.8393155">
NP-VP —* (NP-VBD 1 VP 2 , NP-VBD 1 VP 2 ).
NP-VP —* (NP-NP 1 VBD-VP 2 , NP-NP 1 VBD-VP 2 ).
</equation>
<bodyText confidence="0.79164">
and so on.
Figure 3(b) shows the derivation of the sentence
in Figure 1(a).
</bodyText>
<sectionHeader confidence="0.974184" genericHeader="method">
3 Shallow Parsing
</sectionHeader>
<bodyText confidence="0.999699619047619">
In a parse tree, a chunk is defined by a leaf node or
an inner node whose children are all leaf nodes (See
Figure 2 (a)). In our model, we identify chunks by
traversing a parse tree in a breadth-first order. Once
a node is recognized as a chunk, we skip its children.
In this way, we can get a sole chunk sequence given
a parse tree. Then we label each word with a label
indicating whether the word starts a chunk (B-) or
continues a chunk (I-). Figure 2(a) gives an example.
In this method, we get the training data for shallow
parsing from Penn Tree Bank.
We take shallow Parsing (chunking) as a sequence
label task and employ Conditional Random Field
(CRF)1 to train a chunker. CRF is a good choice for
label tasks as it can avoid label bias and use more
statistical correlated features. We employ the fea-
tures described in Sha and Pereira (2003) for CRF.
We do not introduce CRF-based chunkier in this pa-
per and more details can be got from Hammersley
and Clifford (1971), Lafferty et al. (2001), Taskar et
al. (2002), Sha and Pereira (2003).
</bodyText>
<sectionHeader confidence="0.989075" genericHeader="method">
4 Rule Extraction
</sectionHeader>
<bodyText confidence="0.999840636363636">
In what follows, we introduce how to get the rule
set. We learn rules from a corpus that first is bi-
directionally word-aligned by the GIZA++ toolkit
(Och and Ney, 2000) and then is refined using a
“final-and” strategy. We generate the rule set in two
steps: first, we extract two sets of phrases, basic
phrases and chunk-based phrases. Basic phrases are
defined using the same heuristic as previous systems
(Koehn et al., 2003; Och and Ney, 2004; Chiang,
2005). A chunk-based phrase is such a basic phrase
that covers one or more chunks on the source side.
</bodyText>
<footnote confidence="0.991132">
1We use the open source toolkit CRF++ got in
http://code.google.com/p/crfpp/ .
</footnote>
<listItem confidence="0.826154">
We identity chunk-based phrases (cj2 fjl , ez2) as
j1, follows:
1. A chunk-based phrase is a basic phrase;
2. cj1 begins with “B-”;
3. fj2 is the end word on the source side or cj2+1
does not begins with “I-”.
</listItem>
<bodyText confidence="0.6992425">
Given a sentence pair (f, e, —), we extract rules for
the loose model as follows
</bodyText>
<figure confidence="0.42480275">
1. If (fj2
j1 , ei2
i1) is a basic phrase, then we can have
a rule
</figure>
<equation confidence="0.9774273">
X —* (fj2 ei2)
j1 &apos; i1
2. Assume X —* (α, β) is a rule with α =
α1fj2
j1 α2 and β = β1ei2 i1β2,and (fj2
j1 ,ei2
i1) is
a chunk-based phrase with a chunk sequence
Yu · · · Yv, then we have the following rule
X —* (α1Yu-Yv k α2, β1Yu-Yv k β2).
</equation>
<bodyText confidence="0.995405666666667">
We evaluate the distribution of these rules in the
same way as Chiang (2007).
We extract rules for the tight model as follows
</bodyText>
<listItem confidence="0.534738">
1. If (fj2
</listItem>
<equation confidence="0.92933775">
j1 , ei2) is a chunk-based phrase with a
i1
chunk sequence Ys · · · Yt, then we can have a
rule
Ys-Yt —* (fj2
j1 , ei2
i1)
2. Assume Ys-Yt (α, β) is a rule with α =
α1fj2
j1 α2 and β = β1ei2
i1β2, and (fj2
j1 , ei2
i1) is
a chunk-based phrase with a chunk sequence
Yu · · · Yv, then we have the following rule
Ys-Yt —* (α1Yu-Yv k α2, β1Yu-Yv k β2).
</equation>
<bodyText confidence="0.9968659">
We evaluate the distribution of rules in the same way
as Liu et al. (2006).
For the loose model, the nonterminals must be co-
hesive, while the whole rule can be noncohesive: if
both ends of a rule are nonterminals, the whole rule
is cohesive, otherwise, it may be noncohesive. In
contrast, for the tight model, both the whole rule and
the nonterminal are cohesive.
Even with the cohesion constraints, our model
still generates a large number of rules, but not all
</bodyText>
<page confidence="0.997485">
954
</page>
<bodyText confidence="0.99967375">
of the rules are useful for translation. So we follow
the method described in Chiang (2007) to filter the
rule set except that we allow two nonterminals to be
adjacent.
</bodyText>
<sectionHeader confidence="0.999367" genericHeader="method">
5 Related Works
</sectionHeader>
<bodyText confidence="0.999974025641026">
Watanabe et al. (2003) presented a chunk-to-string
translation model where the decoder generates a
translation by first translating the words in each
chunk, then reordering the translation of chunks.
Our model distinguishes from their model mainly
in reordering model. Their model reorders chunks
resorting to a distortion model while our model re-
orders chunks according to SCFG rules which retain
the relative positions of chunks.
Nguyen et al. (2008) presented a tree-to-string
phrase-based method which is based on SCFGs.
This method generates SCFGs through syntac-
tic transformation including a word-to-phrase tree
transformation model and a phrase reordering model
while our model learns SCFG-based rules from
word-aligned bilingual corpus directly
There are also some works aiming to introduce
linguistic knowledge into the hierarchical phrase-
based model. Marton and Resnik (2008) took the
source parse tree into account and added soft con-
straints to hierarchical phrase-based model. Cherry
(2008) used dependency tree to add syntactic cohe-
sion. These methods work with the original SCFG
defined by hierarchical phrase-based model and use
linguistic knowledge to assist translation. Instead,
our model works under the new defined SCFG with
chunks.
Besides, some other researchers make efforts on
the tree-to-string model by employing exponentially
alternative parses to alleviate the drawback of 1-best
parse. Mi et al. (2008) presented forest-based trans-
lation where the decoder translates a packed forest
of exponentially many parses instead of i-best parse.
Liu and Liu (2010) proposed to parse and to trans-
late jointly by taking tree-based translation as pars-
ing. Given a source sentence, this decoder produces
a parse tree on the source side and a translation on
the target side simultaneously. Both the models per-
form in the unit of tree nodes rather than chunks.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999251">
6.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999077588235294">
Data for shallow parsing We got training data and
test data for shallow parsing from the standard Penn
Tree Bank (PTB) English parsing task by splitting
the sections 02-21 on the Wall Street Journal Portion
(Marcus et al., 1993) into two sets: the last 1000
sentences as the test set and the rest as the training
set. We filtered the features whose frequency was
lower than 3 and substituted ‘‘ and ’’ with ˝ to
keep consistent with translation data. We used L2
algorithm to train CRF.
Data for Translation We used the NIST training
set for Chinese-English translation tasks excluding
the Hong Kong Law and Hong Kong Hansard2 as the
training data, which contains 470K sentence pairs.
For the training data set, we first performed word
alignment in both directions using GIZA++ toolkit
(Och and Ney, 2000) then refined the alignments
using “final-and”. We trained a 5-gram language
model with modified Kneser-Ney smoothing on the
Xinhua portion of LDC Chinese Gigaword corpus.
For the tree-to-string model, we parsed English sen-
tences using Stanford parser and extracted rules us-
ing the GHKM algorithm (Galley et al., 2004).
We used our in-house English-Chinese data set
as the development set and used the 2008 NIST
English-Chinese MT test set (1859 sentences) as the
test set. Our evaluation metric was BLEU-4 (Pap-
ineni et al., 2002) based on characters (as the tar-
get language is Chinese), which performed case-
insensitive matching of n-grams up to n = 4 and
used the shortest reference for the brevity penalty.
We used the standard minimum error-rate training
(Och, 2003) to tune the feature weights to maximize
the BLEU score on the development set.
</bodyText>
<subsectionHeader confidence="0.99986">
6.2 Shallow Parsing
</subsectionHeader>
<bodyText confidence="0.89501">
The standard evaluation metrics for shallow parsing
are precision P, recall R, and their harmonic mean
F1 score, given by:
number of exactly recognized chunks
number of output chunks
number of exactly recognized chunks
R=
number of reference chunks
</bodyText>
<footnote confidence="0.872452">
2The source side and target side are reversed.
</footnote>
<page confidence="0.618893">
P=
955
</page>
<table confidence="0.999604909090909">
Word number Chunk number Accuracy %
23861 12258 94.48
Chunk type P % R % Fl % Found
All 91.14 91.35 91.25 12286
One 90.32 90.99 90.65 5236
NP 93.97 94.47 94.22 5523
ADVP 82.53 84.30 83.40 475
VP 93.66 92.04 92.84 284
ADJP 65.68 69.20 67.39 236
WHNP 96.30 95.79 96.04 189
QP 83.06 80.00 81.50 183
</table>
<tableCaption confidence="0.906667">
Table 1: Shallow parsing result. The collum Found gives
the number of chunks recognized by CRF, the row All
represents all types of chunks, and the row One represents
the chunks that consist of one word.
</tableCaption>
<equation confidence="0.9993445">
2 · P · R
F1 = P + R
</equation>
<bodyText confidence="0.999771888888889">
Besides, we need another metric, accuracy A, to
evaluate the accurate rate of individual labeling de-
cisions of every word as
number of exactly labeled words
A=
number of words
For example, given a reference sequence
B-NP I-NP I-NP B-VP I-VP B-VP, CRF out-
puts a sequence O-NP I-NP I-NP B-VP I-VP I-NP,
then P = 33.33%, A = 66.67%.
Table 1 summaries the results of shallow parsing.
For ‘‘ and ’’ were substituted with ˝ , the perfor-
mance was slightly influenced.
The F1 score of all chunks is 91.25% and the F1
score of One and NP, which in number account for
about 90% of chunks, is 90.65% and 94.22% respec-
tively. F score of NP chunking approaches 94.38%
given in Sha and Pereira (2003).
</bodyText>
<subsectionHeader confidence="0.999135">
6.3 Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999893125">
We compared our loose decoder and tight decoder
with our in-house hierarchical phrase-based decoder
(Chiang, 2007) and the tree-to-string decoder (Liu et
al., 2006). We set the same configuration for all the
decoders as follows: stack size = 30, nbest size = 30.
For the hierarchical chunk-based and phrase-based
decoders, we set max rule length to 5. For the tree-
to-string decoder, we set the configuration of rule
</bodyText>
<table confidence="0.936483">
System Dev NIST08 Speed
phrase 0.2843 0.3921 1.163
tree 0.2786 0.3817 1.107
tight 0.2914 0.3987 1.208
loose 0.2936 0.4023 1.429
</table>
<tableCaption confidence="0.991275">
Table 2: Performance comparison. Phrase represents
</tableCaption>
<bodyText confidence="0.969438">
the hierarchical phrase-based decoder, tree represents the
tree-to-string decoder, tight represents our tight decoder
and loose represents our loose decoder. The speed is re-
ported by seconds per sentence. The speed for the tree-to-
string decoder includes the parsing time (0.23s) and the
speed for the tight and loose models includes the shallow
parsing time, too.
extraction as: the height up to 3 and the number of
leaf nodes up to 5.
We give the results in Table 2. From the results,
we can see that both the loose and tight decoders
outperform the baseline decoders and the improve-
ment is significant using the sign-test of Collins et
al. (2005) (p &lt; 0.01). Specifically, the loose model
has a better performance while the tight model has a
faster speed.
Compared with the hierarchical phrase-based
model, the loose model only imposes syntactic cohe-
sion cohesion to nonterminals while the tight model
imposes syntax cohesion to both rules and nonter-
minals which reduces search space, so it decoders
faster. We can conclude that linguistic syntax can
indeed improve the translation performance; syntac-
tic cohesion for nonterminals can explain linguis-
tic phenomena well; noncohesive rules are useful,
too. The extra time consumption against hierarchi-
cal phrase-based system comes from shallow pars-
ing.
By investigating the translation result, we find that
our decoder does well in rule selection. For exam-
ple, in the hierarchical phrase-based model, this kind
of rules, such as
</bodyText>
<equation confidence="0.49057">
X --+ (X of X, *), X --+ (X for X, *)
</equation>
<bodyText confidence="0.999747714285714">
and so on, where * stands for the target component,
are used with a loose restriction as long as the ter-
minals are matched, while our models employ more
stringent constraints on these rules by specifying the
syntactic constituent of “X”. With chunk labels, our
models can make different treatment for different
situations.
</bodyText>
<page confidence="0.993812">
956
</page>
<table confidence="0.987291">
System Dev NIST08 Speed
cohesive 0.2936 0.4023 1.429
noncohesive 0.2937 0.3964 1.734
</table>
<tableCaption confidence="0.995911">
Table 3: Influence of cohesion. The row cohesive rep-
</tableCaption>
<bodyText confidence="0.914912357142857">
resents the loose system where nonterminals satisfy co-
hesion, and the row noncohesive represents the modified
version of the loose system where nonterminals can be
noncohesive.
Compared with the tree-to-string model, the re-
sult indicates that the change of the source-side lin-
guistic syntax from parses to chunks can improve
translation performance. The reasons should be our
model can reduce parse errors and it is enough to use
chunks as the basic unit for machine translation. Al-
though our decoders and tree-to-string decoder all
run in linear-time with beam search, tree-to-string
model runs faster for it searches through a smaller
SCFG-motivated space.
</bodyText>
<subsectionHeader confidence="0.99217">
6.4 Influence of Cohesion
</subsectionHeader>
<bodyText confidence="0.974280375">
We verify the influence of syntax cohesion via the
loose model. The cohesive model imposes syntax
cohesion on nonterminals to ensure the chunk is re-
ordered as a whole. In this experiment, we introduce
a noncohesive model by allowing a nonterminal to
match part of a chunk. For example, in the nonco-
hesive model, it is legal for a rule with the source
side
“NP for NP-NP”
to match
“request for a purchase of shares”
in Figure 1 (a), where “request” is part of NP. As
well, the rule with the source side
“NP for a NP-NP”
can match
“request for a purchase of shares”.
In this way, we can ensure all the rules used in the
cohesive system can be used in the noncohesive sys-
tem. Besides cohesive rules, the noncohesive system
can use noncohesive rules, too.
We give the results in Table 3. From the results,
we can see that cohesion helps to reduce search
space, so the cohesive system decodes faster. The
noncohesive system decoder slower, as it employs
</bodyText>
<table confidence="0.9992616">
System Number Dev NIST08 Speed
loose two 0.2936 0.4023 1.429
loose three 0.2978 0.4037 2.056
tight two 0.2914 0.3987 1.208
tight three 0.2954 0.4026 1.780
</table>
<tableCaption confidence="0.980455">
Table 4: The influence of the number of nonterminals.
The column number lists the number of nonterminals
used at most in a rule.
</tableCaption>
<bodyText confidence="0.99990575">
more rules, but this does not bring any improvement
of translation performance. As other researches said
in their papers, syntax cohesion can explain linguis-
tic phenomena well.
</bodyText>
<subsectionHeader confidence="0.968838">
6.5 Influence of the number of nonterminals
</subsectionHeader>
<bodyText confidence="0.999980333333333">
We also tried to allow a rule to hold three nonter-
minals at most. We give the result in Table 4. The
result shows that using three nonterminals does not
bring a significant improvement of translation per-
formance but quite more time consumption. So we
only retain two nonterminals at most in a rule.
</bodyText>
<sectionHeader confidence="0.998206" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999954545454546">
In this paper, we present a hierarchical chunk-
to-string model for statistical machine translation
which can be seen as a compromise of the hierarchi-
cal phrase-based model and the tree-to-string model.
With the help of shallow parsing, our model learns
rules consisting of either words or chunks and com-
presses adjacent chunks in a rule to a nonterminal,
then it searches for the best derivation under the
SCFG defined by these rules. Our model can com-
bine the merits of both the models: employing lin-
guistic syntax to direct decoding, being syntax co-
hesive and robust to parsing errors. We refine the hi-
erarchical chunk-to-string model into two models: a
loose model (more similar to the hierarchical phrase-
based model) and a tight model (more similar to the
tree-to-string model).
Our experiments show that our decoder can im-
prove translation performance significantly over the
hierarchical phrase-based decoder and the tree-to-
string decoder. Besides, the loose model gives a bet-
ter performance while the tight model gives a faster
speed.
</bodyText>
<page confidence="0.995019">
957
</page>
<sectionHeader confidence="0.996647" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999905">
We would like to thank Trevor Cohn, Shujie Liu,
Nan Duan, Lei Cui and Mo Yu for their help,
and anonymous reviewers for their valuable com-
ments and suggestions. This work was supported
in part by EPSRC grant EP/I034750/1 and in part
by High Technology R&amp;D Program Project No.
2011AA01A207.
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964186813187">
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. ofACL, pages
72–80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ofACL, pages 531–540.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. ofACL, pages
205–208.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling:Posters,
pages 285–293.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304–
3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc of
NAACL, pages 273–280.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. ofHLT-NAACL, pages 105–112.
J Hammersley and P Clifford. 1971. Markov fields on
finite graphs and lattices. In Unpublished manuscript.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings ofAMTA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127–133.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML, pages 282–289.
Yang Liu and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of COLING, pages 707–715.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of COLING-ACL, pages 609–616.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19:313–330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ofACL, pages 1003–1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ofACL, pages 192–199.
Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho,
Minh Le Nguyen, and Vinh Van Nguyen. 2008. A
tree-to-string phrase-based model for statistical ma-
chine translation. In Proc. of CoNLL, pages 143–150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. ofACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ofACL, pages 295–
302.
Frans J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417–449.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings ofACL,
pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings ofACL, pages 271–279.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of HLT-
NAACL, pages 134–141.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Eighteenth Conference on Uncertainty in Artificial
Intelligence.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno.
2003. Chunk-based statistical translation. In Proc. of
ACL, pages 303–310.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523–530.
</reference>
<page confidence="0.997212">
958
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.205817">
<title confidence="0.997793">Chunk-to-String</title>
<author confidence="0.985805">Dongdong Mu Ming Qun</author>
<affiliation confidence="0.726015">tDepartment of Computer Science tMicrosoft Research Asia of Sheffield UK</affiliation>
<email confidence="0.880218">y.feng@shef.ac.ukmingzhou@microsoft.com</email>
<affiliation confidence="0.9930375">Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.737397">Chinese Academy of</address>
<email confidence="0.869241">liuqun@ict.ac.cn</email>
<abstract confidence="0.996483117647059">We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model rules consisting of words and and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="3164" citStr="Cherry, 2008" startWordPosition="480" endWordPosition="481"> longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the me</context>
<context position="17487" citStr="Cherry (2008)" startWordPosition="3208" endWordPosition="3209">h retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proc. ofACL, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="8357" citStr="Chiang, 2005" startWordPosition="1389" endWordPosition="1390">e production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f) = max p(d, e|f, �c) d = max d � p(r, e|f, 8) (4) r∈d Following Och and Ney (2002), we frame our model as a log-linear model: _ exp Ek AkHk (d, e, 6, f ) p(e|f) f) — exp Ed′,e′,k AkHk(d′, e′, �c, f) (5) �where Hk(d, e, ��, f) = hk(f, c,r) r We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decoding, we employ the same decoding algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. 2.1 A Loose Model In our model, we employ rules containing nont</context>
<context position="14571" citStr="Chiang, 2005" startWordPosition="2676" endWordPosition="2677">and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . We identity chunk-based phrases (cj2 fjl , ez2) as j1, follows: 1. A chunk-based phrase is a basic phrase; 2. cj1 begins with “B-”; 3. fj2 is the end word on the source side or cj2+1 does not begins with “I-”. Given a sentence pair (f, e, —), we extract rules for the loose model as follows 1. If (fj2 j1 , ei2 i1) is a basic phrase, then we can have a rule X —* (fj2 ei2) j1 &apos; i1 2. Assume X —* (α, β) is a rule with α = α1</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="1160" citStr="Chiang, 2007" startWordPosition="150" endWordPosition="151">d model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. 1 Introduction The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗This work was done when the first </context>
<context position="15423" citStr="Chiang (2007)" startWordPosition="2851" endWordPosition="2852"> 1. A chunk-based phrase is a basic phrase; 2. cj1 begins with “B-”; 3. fj2 is the end word on the source side or cj2+1 does not begins with “I-”. Given a sentence pair (f, e, —), we extract rules for the loose model as follows 1. If (fj2 j1 , ei2 i1) is a basic phrase, then we can have a rule X —* (fj2 ei2) j1 &apos; i1 2. Assume X —* (α, β) is a rule with α = α1fj2 j1 α2 and β = β1ei2 i1β2,and (fj2 j1 ,ei2 i1) is a chunk-based phrase with a chunk sequence Yu · · · Yv, then we have the following rule X —* (α1Yu-Yv k α2, β1Yu-Yv k β2). We evaluate the distribution of these rules in the same way as Chiang (2007). We extract rules for the tight model as follows 1. If (fj2 j1 , ei2) is a chunk-based phrase with a i1 chunk sequence Ys · · · Yt, then we can have a rule Ys-Yt —* (fj2 j1 , ei2 i1) 2. Assume Ys-Yt (α, β) is a rule with α = α1fj2 j1 α2 and β = β1ei2 i1β2, and (fj2 j1 , ei2 i1) is a chunk-based phrase with a chunk sequence Yu · · · Yv, then we have the following rule Ys-Yt —* (α1Yu-Yv k α2, β1Yu-Yv k β2). We evaluate the distribution of rules in the same way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a</context>
<context position="21721" citStr="Chiang, 2007" startWordPosition="3930" endWordPosition="3931">B-NP I-NP I-NP B-VP I-VP B-VP, CRF outputs a sequence O-NP I-NP I-NP B-VP I-VP I-NP, then P = 33.33%, A = 66.67%. Table 1 summaries the results of shallow parsing. For ‘‘ and ’’ were substituted with ˝ , the performance was slightly influenced. The F1 score of all chunks is 91.25% and the F1 score of One and NP, which in number account for about 90% of chunks, is 90.65% and 94.22% respectively. F score of NP chunking approaches 94.38% given in Sha and Pereira (2003). 6.3 Performance Comparison We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006). We set the same configuration for all the decoders as follows: stack size = 30, nbest size = 30. For the hierarchical chunk-based and phrase-based decoders, we set max rule length to 5. For the treeto-string decoder, we set the configuration of rule System Dev NIST08 Speed phrase 0.2843 0.3921 1.163 tree 0.2786 0.3817 1.107 tight 0.2914 0.3987 1.208 loose 0.2936 0.4023 1.429 Table 2: Performance comparison. Phrase represents the hierarchical phrase-based decoder, tree represents the tree-to-string decoder, tight represents our tight decoder a</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="22850" citStr="Collins et al. (2005)" startWordPosition="4116" endWordPosition="4119">ed decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed is reported by seconds per sentence. The speed for the tree-tostring decoder includes the parsing time (0.23s) and the speed for the tight and loose models includes the shallow parsing time, too. extraction as: the height up to 3 and the number of leaf nodes up to 5. We give the results in Table 2. From the results, we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al. (2005) (p &lt; 0.01). Specifically, the loose model has a better performance while the tight model has a faster speed. Compared with the hierarchical phrase-based model, the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space, so it decoders faster. We can conclude that linguistic syntax can indeed improve the translation performance; syntactic cohesion for nonterminals can explain linguistic phenomena well; noncohesive rules are useful, too. The extra time consumption against hierar</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. ofACL, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>205--208</pages>
<contexts>
<context position="3105" citStr="Eisner, 2003" startWordPosition="470" endWordPosition="471">ntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hier</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. ofACL, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Haitao Mi</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>An efficient shift-reduce decoding algorithm for phrasedbased machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of Coling:Posters,</booktitle>
<pages>285--293</pages>
<contexts>
<context position="3184" citStr="Feng et al., 2010" startWordPosition="482" endWordPosition="485">reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two mode</context>
</contexts>
<marker>Feng, Mi, Liu, Liu, 2010</marker>
<rawString>Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrasedbased machine translation. In Proc. of Coling:Posters, pages 285–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>304--3111</pages>
<contexts>
<context position="2813" citStr="Fox (2002)" startWordPosition="426" endWordPosition="427">a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-t</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of EMNLP, pages 304– 3111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc of NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="19517" citStr="Galley et al., 2004" startWordPosition="3536" endWordPosition="3539">tion We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsi</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc of NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT-NAACL,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="3130" citStr="Graehl and Knight, 2004" startWordPosition="472" endWordPosition="475">e parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring t</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. ofHLT-NAACL, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hammersley</author>
<author>P Clifford</author>
</authors>
<title>Markov fields on finite graphs and lattices.</title>
<date>1971</date>
<note>In Unpublished manuscript.</note>
<contexts>
<context position="14021" citStr="Hammersley and Clifford (1971)" startWordPosition="2579" endWordPosition="2582">el each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that</context>
</contexts>
<marker>Hammersley, Clifford, 1971</marker>
<rawString>J Hammersley and P Clifford. 1971. Markov fields on finite graphs and lattices. In Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="2468" citStr="Huang et al., 2006" startWordPosition="371" endWordPosition="374">nsider syntactic constituents of subphrases, e.g. the hierarchical phrase X → (X 1 for X 2 , X 2 de X 1 ) can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; </context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="14537" citStr="Koehn et al., 2003" startWordPosition="2668" endWordPosition="2671">oduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . We identity chunk-based phrases (cj2 fjl , ez2) as j1, follows: 1. A chunk-based phrase is a basic phrase; 2. cj1 begins with “B-”; 3. fj2 is the end word on the source side or cj2+1 does not begins with “I-”. Given a sentence pair (f, e, —), we extract rules for the loose model as follows 1. If (fj2 j1 , ei2 i1) is a basic phrase, then we can have a rule X —* (fj2 ei2) j1 &apos; i1 2. Assume</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLTNAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="14045" citStr="Lafferty et al. (2001)" startWordPosition="2583" endWordPosition="2586">ting whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chun</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Joint parsing and translation.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>707--715</pages>
<contexts>
<context position="18069" citStr="Liu and Liu (2010)" startWordPosition="3293" endWordPosition="3296">al phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the</context>
</contexts>
<marker>Liu, Liu, 2010</marker>
<rawString>Yang Liu and Qun Liu. 2010. Joint parsing and translation. In Proc. of COLING, pages 707–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="2447" citStr="Liu et al., 2006" startWordPosition="367" endWordPosition="370">en desirable to consider syntactic constituents of subphrases, e.g. the hierarchical phrase X → (X 1 for X 2 , X 2 de X 1 ) can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some langu</context>
<context position="15907" citStr="Liu et al. (2006)" startWordPosition="2958" endWordPosition="2961">ve the following rule X —* (α1Yu-Yv k α2, β1Yu-Yv k β2). We evaluate the distribution of these rules in the same way as Chiang (2007). We extract rules for the tight model as follows 1. If (fj2 j1 , ei2) is a chunk-based phrase with a i1 chunk sequence Ys · · · Yt, then we can have a rule Ys-Yt —* (fj2 j1 , ei2 i1) 2. Assume Ys-Yt (α, β) is a rule with α = α1fj2 j1 α2 and β = β1ei2 i1β2, and (fj2 j1 , ei2 i1) is a chunk-based phrase with a chunk sequence Yu · · · Yv, then we have the following rule Ys-Yt —* (α1Yu-Yv k α2, β1Yu-Yv k β2). We evaluate the distribution of rules in the same way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all 954 of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003)</context>
<context position="21771" citStr="Liu et al., 2006" startWordPosition="3936" endWordPosition="3939">equence O-NP I-NP I-NP B-VP I-VP I-NP, then P = 33.33%, A = 66.67%. Table 1 summaries the results of shallow parsing. For ‘‘ and ’’ were substituted with ˝ , the performance was slightly influenced. The F1 score of all chunks is 91.25% and the F1 score of One and NP, which in number account for about 90% of chunks, is 90.65% and 94.22% respectively. F score of NP chunking approaches 94.38% given in Sha and Pereira (2003). 6.3 Performance Comparison We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006). We set the same configuration for all the decoders as follows: stack size = 30, nbest size = 30. For the hierarchical chunk-based and phrase-based decoders, we set max rule length to 5. For the treeto-string decoder, we set the configuration of rule System Dev NIST08 Speed phrase 0.2843 0.3921 1.163 tree 0.2786 0.3817 1.107 tight 0.2914 0.3987 1.208 loose 0.2936 0.4023 1.429 Table 2: Performance comparison. Phrase represents the hierarchical phrase-based decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed i</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proc. of COLING-ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18623" citStr="Marcus et al., 1993" startWordPosition="3388" endWordPosition="3391">onentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignm</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="17370" citStr="Marton and Resnik (2008)" startWordPosition="3188" endWordPosition="3191"> model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proc. ofACL, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="17916" citStr="Mi et al. (2008)" startWordPosition="3270" endWordPosition="3273">edge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) En</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. ofACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thai Phuong Nguyen</author>
</authors>
<title>Akira Shimazu, Tu Bao Ho, Minh Le Nguyen, and Vinh Van Nguyen.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>143--150</pages>
<marker>Nguyen, 2008</marker>
<rawString>Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho, Minh Le Nguyen, and Vinh Van Nguyen. 2008. A tree-to-string phrase-based model for statistical machine translation. In Proc. of CoNLL, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="14279" citStr="Och and Ney, 2000" startWordPosition="2625" endWordPosition="2628">sk and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . We identity chunk-based phrases (cj2 fjl , ez2) as j1, follows: 1. A chunk-based phrase is a basic phrase; 2. cj1 begins with “B-”; </context>
<context position="19199" citStr="Och and Ney, 2000" startWordPosition="3487" endWordPosition="3490">reet Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which perfor</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="8082" citStr="Och and Ney (2002)" startWordPosition="1332" endWordPosition="1335">y rewrites every two coindexed nonterminals with the corresponding components of a matched rule. A derivation yields a pair of strings on the right-hand side which are translation of each other. In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f) = max p(d, e|f, �c) d = max d � p(r, e|f, 8) (4) r∈d Following Och and Ney (2002), we frame our model as a log-linear model: _ exp Ek AkHk (d, e, 6, f ) p(e|f) f) — exp Ed′,e′,k AkHk(d′, e′, �c, f) (5) �where Hk(d, e, ��, f) = hk(f, c,r) r We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decodin</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ofACL, pages 295– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="14556" citStr="Och and Ney, 2004" startWordPosition="2672" endWordPosition="2675">kier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . We identity chunk-based phrases (cj2 fjl , ez2) as j1, follows: 1. A chunk-based phrase is a basic phrase; 2. cj1 begins with “B-”; 3. fj2 is the end word on the source side or cj2+1 does not begins with “I-”. Given a sentence pair (f, e, —), we extract rules for the loose model as follows 1. If (fj2 j1 , ei2 i1) is a basic phrase, then we can have a rule X —* (fj2 ei2) j1 &apos; i1 2. Assume X —* (α, β) is a r</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Frans J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19968" citStr="Och, 2003" startWordPosition="3614" endWordPosition="3615">word corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks 2The source side and target side are reversed. P= 955 Word number Chunk number Accuracy % 23861 12258 94.48 Chunk type P % R % Fl % Found All 91.14 91.35 91.25 12286 One 90.32 90.99 90.65 5236 NP 93.97 94.47 94.22 5523 ADVP 82.53 84.30 83.40 475 VP 93</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Frans J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19729" citStr="Papineni et al., 2002" startWordPosition="3571" endWordPosition="3575">e first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks 2The source </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="3150" citStr="Quirk et al., 2005" startWordPosition="476" endWordPosition="479">ordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings ofACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="13895" citStr="Sha and Pereira (2003)" startWordPosition="2556" endWordPosition="2559">gnized as a chunk, we skip its children. In this way, we can get a sole chunk sequence given a parse tree. Then we label each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heurist</context>
<context position="21578" citStr="Sha and Pereira (2003)" startWordPosition="3909" endWordPosition="3912">urate rate of individual labeling decisions of every word as number of exactly labeled words A= number of words For example, given a reference sequence B-NP I-NP I-NP B-VP I-VP B-VP, CRF outputs a sequence O-NP I-NP I-NP B-VP I-VP I-NP, then P = 33.33%, A = 66.67%. Table 1 summaries the results of shallow parsing. For ‘‘ and ’’ were substituted with ˝ , the performance was slightly influenced. The F1 score of all chunks is 91.25% and the F1 score of One and NP, which in number account for about 90% of chunks, is 90.65% and 94.22% respectively. F score of NP chunking approaches 94.38% given in Sha and Pereira (2003). 6.3 Performance Comparison We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006). We set the same configuration for all the decoders as follows: stack size = 30, nbest size = 30. For the hierarchical chunk-based and phrase-based decoders, we set max rule length to 5. For the treeto-string decoder, we set the configuration of rule System Dev NIST08 Speed phrase 0.2843 0.3921 1.163 tree 0.2786 0.3817 1.107 tight 0.2914 0.3987 1.208 loose 0.2936 0.4023 1.429 Table 2: Performance compar</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLTNAACL, pages 134–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Pieter Abbeel</author>
<author>Daphne Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>In Eighteenth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="14067" citStr="Taskar et al. (2002)" startWordPosition="2587" endWordPosition="2590">arts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side.</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002. Discriminative probabilistic models for relational data. In Eighteenth Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Chunk-based statistical translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>303--310</pages>
<contexts>
<context position="16507" citStr="Watanabe et al. (2003)" startWordPosition="3063" endWordPosition="3066">y as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all 954 of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003) presented a chunk-to-string translation model where the decoder generates a translation by first translating the words in each chunk, then reordering the translation of chunks. Our model distinguishes from their model mainly in reordering model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree tran</context>
</contexts>
<marker>Watanabe, Sumita, Okuno, 2003</marker>
<rawString>Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno. 2003. Chunk-based statistical translation. In Proc. of ACL, pages 303–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="3066" citStr="Wu, 1997" startWordPosition="464" endWordPosition="465">al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse e</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3091" citStr="Yamada and Knight, 2001" startWordPosition="466" endWordPosition="469"> introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proc. of ACL, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>