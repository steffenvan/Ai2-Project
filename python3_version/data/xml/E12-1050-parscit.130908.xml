<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000859">
<title confidence="0.9984725">
Feature-Rich Part-of-speech Tagging
for Morphologically Complex Languages: Application to Bulgarian
</title>
<author confidence="0.722555">
Georgi Georgiev and Valentin Zhikov
</author>
<note confidence="0.4628068">
Ontotext AD
135 Tsarigradsko Sh., Sofia, Bulgaria
Petya Osenova and Kiril Simov
IICT, Bulgarian Academy of Sciences
25A Acad. G. Bonchev, Sofia, Bulgaria
</note>
<email confidence="0.967929">
{georgi.georgiev,valentin.zhikov}@ontotext.com 1petya,kivsl@bultreebank.org
</email>
<author confidence="0.97573">
Preslav Nakov
</author>
<affiliation confidence="0.960163">
Qatar Computing Research Institute, Qatar Foundation
</affiliation>
<address confidence="0.613867">
Tornado Tower, floor 10, P.O. Box 5825, Doha, Qatar
</address>
<email confidence="0.985507">
pnakov@qf.org.qa
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992325">
We present experiments with part-of-
speech tagging for Bulgarian, a Slavic lan-
guage with rich inflectional and deriva-
tional morphology. Unlike most previous
work, which has used a small number of
grammatical categories, we work with 680
morpho-syntactic tags. We combine a large
morphological lexicon with prior linguis-
tic knowledge and guided learning from a
POS-annotated corpus, achieving accuracy
of 97.98%, which is a significant improve-
ment over the state-of-the-art for Bulgarian.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975616666667">
Part-of-speech (POS) tagging is the task of as-
signing each of the words in a given piece of text a
contextually suitable grammatical category. This
is not trivial since words can play different syn-
tactic roles in different contexts, e.g., can is a
noun in “I opened a can of coke.” but a verb in
“I can write.” Traditionally, linguists have classi-
fied English words into the following eight basic
POS categories: noun, pronoun, adjective, verb,
adverb, preposition, conjunction, and interjection;
this list is often extended a bit, e.g., with deter-
miners, particles, participles, etc., but the number
of categories considered is rarely more than 15.
Computational linguistics works with a larger
inventory of POS tags, e.g., the Penn Treebank
(Marcus et al., 1993) uses 48 tags: 36 for part-
of-speech, and 12 for punctuation and currency
symbols. This increase in the number of tags
is partially due to finer granularity, e.g., there
are special tags for determiners, particles, modal
verbs, cardinal numbers, foreign words, existen-
tial there, etc., but also to the desire to encode
morphological information as part of the tags.
For example, there are six tags for verbs in the
Penn Treebank: VB (verb, base form; e.g., sing),
VBD (verb, past tense; e.g., sang), VBG (verb,
gerund or present participle; e.g., singing), VBN
(verb, past participle; e.g., sung) VBP (verb, non-
3rd person singular present; e.g., sing), and VBZ
(verb, 3rd person singular present; e.g., sings);
these tags are morpho-syntactic in nature. Other
corpora have used even larger tagsets, e.g., the
Brown corpus (Kuˇcera and Francis, 1967) and the
Lancaster-Oslo/Bergen (LOB) corpus (Johansson
et al., 1986) use 87 and 135 tags, respectively.
POS tagging poses major challenges for mor-
phologically complex languages, whose tagsets
encode a lot of additional morpho-syntactic fea-
tures (for most of the basic POS categories), e.g.,
gender, number, person, etc. For example, the
BulTreeBank (Simov et al., 2004) for Bulgarian
uses 680 tags, while the Prague Dependency Tree-
bank (Hajiˇc, 1998) for Czech has over 1,400 tags.
Below we present experiments with POS tag-
ging for Bulgarian, which is an inflectional lan-
guage with rich morphology. Unlike most previ-
ous work, which has used a reduced set of POS
tags, we use all 680 tags in the BulTreeBank. We
combine prior linguistic knowledge and statistical
learning, achieving accuracy comparable to that
reported for state-of-the-art systems for English.
The remainder of the paper is organized as fol-
lows: Section 2 provides an overview of related
work, Section 3 describes Bulgarian morphology,
Section 4 introduces our approach, Section 5 de-
scribes the datasets, Section 6 presents our exper-
iments in detail, Section 7 discusses the results,
Section 8 offers application-specific error analy-
sis, and Section 9 concludes and points to some
promising directions for future work.
</bodyText>
<page confidence="0.974628">
492
</page>
<note confidence="0.993697">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492–502,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997354" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999519432098766">
Most research on part-of-speech tagging has fo-
cused on English, and has relied on the Penn Tree-
bank (Marcus et al., 1993) and its tagset for train-
ing and evaluation. The task is typically addressed
as a sequential tagging problem; one notable ex-
ception is the work of Brill (1995), who proposed
non-sequential transformation-based learning.
A number of different sequential learning
frameworks have been tried, yielding 96-97%
accuracy: Lafferty et al. (2001) experimented
with conditional random fields (CRFs) (95.7%
accuracy), Ratnaparkhi (1996) used a maximum
entropy sequence classifier (96.6% accuracy),
Brants (2000) employed a hidden Markov model
(96.6% accuracy), Collins (2002) adopted an av-
eraged perception discriminative sequence model
(97.1% accuracy). All these models fix the order
of inference from left to right.
Toutanova et al. (2003) introduced a cyclic de-
pendency network (97.2% accuracy), where the
search is bi-directional. Shen et al. (2007) have
further shown that better results (97.3% accu-
racy) can be obtained using guided learning, a
framework for bidirectional sequence classifica-
tion, which integrates token classification and in-
ference order selection into a single learning task
and uses a perceptron-like (Collins and Roark,
2004) passive-aggressive classifier to make the
easiest decisions first. Recently, Tsuruoka et al.
(2011), proposed a simple perceptron-based clas-
sifier applied from left to right but augmented
with a lookahead mechanism that searches the
space of future actions, yielding 97.3% accuracy.
For morphologically complex languages, the
problem of POS tagging typically includes mor-
phological disambiguation, which yields a much
larger number of tags. For example, for Arabic,
Habash and Rambow (2005) used support vector
machines (SVM), achieving 97.6% accuracy with
139 tags from the Arabic Treebank (Maamouri et
al., 2003). For Czech, Hajiˇc et al. (2001) com-
bined a hidden Markov model (HMM) with lin-
guistic rules, which yielded 95.2% accuracy using
an inventory of over 1,400 tags from the Prague
Dependency Treebank (Hajiˇc, 1998). For Ice-
landic, Dredze and Wallenberg (2008) reported
92.1% accuracy with 639 tags developed for the
Icelandic frequency lexicon (Pind et al., 1991),
they used guided learning and tag decomposition:
First, a coarse POS class is assigned (e.g., noun,
verb, adjective), then, additional fine-grained
morphological features like case, number and
gender are added, and finally, the proposed tags
are further reconsidered using non-local features.
Similarly, Smith et al. (2005) decomposed the
complex tags into factors, where models for pre-
dicting part-of-speech, gender, number, case, and
lemma are estimated separately, and then com-
posed into a single CRF model; this yielded com-
petitive results for Arabic, Korean, and Czech.
Most previous work on Bulgarian POS tagging
has started with large tagsets, which were then
reduced. For example, Dojchinova and Mihov
(2004) mapped their initial tagset of 946 tags to
just 40, which allowed them to achieve 95.5%
accuracy using the transformation-based learning
of Brill (1995), and 98.4% accuracy using manu-
ally crafted linguistic rules. Similarly, Georgiev
et al. (2009), who used maximum entropy and
the BulTreeBank (Simov et al., 2004), grouped
its 680 fine-grained POS tags into 95 coarse-
grained ones, and thus improved their accuracy
from 90.34% to 94.4%. Simov and Osenova
(2001) used a recurrent neural network to predict
(a) 160 morpho-syntactic tags (92.9% accuracy)
and (b) 15 POS tags (95.2% accuracy).
Some researchers did not reduce the tagset:
Savkov et al. (2011) used 680 tags (94.7% ac-
curacy), and Tanev and Mitkov (2002) used 303
tags and the BULMORPH morphological ana-
lyzer (Krushkov, 1997), achieving P=R=95%.
</bodyText>
<sectionHeader confidence="0.983847" genericHeader="method">
3 Bulgarian Morphology
</sectionHeader>
<bodyText confidence="0.999835352941176">
Bulgarian is an Indo-European language from the
Slavic language group, written with the Cyrillic
alphabet and spoken by about 9-12 million peo-
ple. It is also a member of the Balkan Sprachbund
and thus differs from most other Slavic languages:
it has no case declensions, uses a suffixed definite
article (which has a short and a long form for sin-
gular masculine), and lacks verb infinitive forms.
It further uses special evidential verb forms to ex-
press unwitnessed, retold, and doubtful activities.
Bulgarian is an inflective language with very
rich morphology. For example, Bulgarian verbs
have 52 synthetic wordforms on average, while
pronouns have altogether more than ten grammat-
ical features (not necessarily shared by all pro-
nouns), including case, gender, person, number,
definiteness, etc.
</bodyText>
<page confidence="0.998892">
493
</page>
<bodyText confidence="0.999901666666667">
This rich morphology inevitably leads to ambi-
guity proliferation; our analysis of BulTreeBank
shows four major types of ambiguity:
</bodyText>
<listItem confidence="0.950505136363637">
1. Between the wordforms of the same lexeme,
i.e., in the paradigm. For example, AuBaHa,
an inflected form of AuBaH (‘sofa’, mascu-
line), can mean (a) ‘the sofa’ (definite, singu-
lar, short definite article) or (b) a count form,
e.g., as in ABa AuBaHa (‘two sofas’).
2. Between two or more lexemes, i.e., conver-
sion. For example, xaTo can be (a) a subor-
dinator meaning ‘as, when’, or (b) a preposi-
tion meaning ‘like, such as’.
3. Between a lexeme and an inflected wordform
of another lexeme, i.e., across-paradigms.
For example, uoluTuxa can mean (a) ‘the
politician’ (masculine, singular, definite,
short definite article) or (b) ‘politics’ (fem-
inine, singular, indefinite).
4. Between the wordforms of two or more
lexemes, i.e., across-paradigms and quasi-
conversion. For example, BT,pBu can mean
(a) ‘walks’ (verb, 2nd or 3rd person, present
tense) or (b) ‘strings, laces’ (feminine, plu-
ral, indefinite).
</listItem>
<bodyText confidence="0.998904">
Some morpho-syntactic ambiguities in Bulgar-
ian are occasional, but many are systematic, e.g.,
neuter singular adjectives have the same forms
as adverbs. Overall, most ambiguities are local,
and thus arguably resolvable using n-grams, e.g.,
compare xy6aBo AeTe (‘beautiful child’), where
xy6aBo is a neuter adjective, and “Ilesr xy6aBo.”
(‘I sing beautifully.’), where it is an adverb of
manner. Other ambiguities, however, are non-
local and may require discourse-level analysis,
e.g., “BuAsrx ro.” can mean ‘I saw him.’, where
ro is a masculine pronoun, or ’I saw it.’, where
it is a neuter pronoun. Finally, there are ambi-
guities that are very hard or even impossible) to
resolve, e.g., “,JeTeTo Bleae Becelo.” can mean
both ‘The child came in happy.’ (Becelo is an ad-
jective) and ‘The child came in happily.’ (it is an
adverb); however, the latter is much more likely.
&apos;The problem also exists for English, e.g., the annotators
of the Penn Treebank were allowed to use tag combinations
for inherently ambiguous cases: JJ|NN (adjective or noun as
prenominal modifier), JJIVBG (adjective or gerund/present
participle), JJIVBN (adjective or past participle), NNIVBG
(noun or gerund), and RB|RP (adverb or particle).
In many cases, strong domain preferences exist
about how various systematic ambiguities should
be resolved. We made a study for the newswire
domain, analyzing a corpus of 546,029 words,
and we found that ambiguity type 2 (lexeme-
lexeme) prevailed for functional parts-of-speech,
while the other types were more frequent for in-
flecting parts-of-speech. Below we show the most
frequent types of morpho-syntactic ambiguities
and their frequency in our corpus:
</bodyText>
<listItem confidence="0.994310333333333">
• Ha: preposition (‘of’) vs. emphatic particle,
with a ratio of 28,554 to 38;
• Aa: auxiliary particle (‘to’) vs. affirmative
particle, with a ratio of 12,035 to 543;
• e: 3rd person present auxiliary verb (‘to be’)
vs. particle (‘well’) vs. interjection (‘wow’),
with a ratio of 9,136 to 21 to 5;
• singular masculine noun with a short definite
article vs. count form of a masculine noun,
with a ratio of 6,437 to 1,592;
• adverb vs. neuter singular adjective, with a
ratio of 3,858 to 1,753.
</listItem>
<bodyText confidence="0.9993812">
Overall, the following factors should be taken
into account when modeling Bulgarian morpho-
syntax: (1) locality vs. non-locality of grammat-
ical features, (2) interdependence of grammatical
features, and (3) domain-specific preferences.
</bodyText>
<sectionHeader confidence="0.987378" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.999729625">
We used the guided learning framework described
in (Shen et al., 2007), which has yielded state-of-
the-art results for English and has been success-
fully applied to other morphologically complex
languages such as Icelandic (Dredze and Wallen-
berg, 2008); we found it quite suitable for Bul-
garian as well. We used the feature set defined in
(Shen et al., 2007), which includes the following:
</bodyText>
<listItem confidence="0.972724375">
1. The feature set of Ratnaparkhi (1996), in-
cluding prefix, suffix and lexical, as well as
some bigram and trigram context features;
2. Feature templates as in (Ratnaparkhi, 1996),
which have been shown helpful in bidirec-
tional search;
3. More bigram and trigram features and bi-
lexical features as in (Shen et al., 2007).
</listItem>
<bodyText confidence="0.999211">
Note that we allowed prefixes and suffixes of
length up to 9, as in (Toutanova et al., 2003) and
(Tsuruoka and Tsujii, 2005).
</bodyText>
<page confidence="0.995155">
494
</page>
<bodyText confidence="0.999971837837838">
We further extended the set of features with
the tags proposed for the current word token by a
morphological lexicon, which maps words to pos-
sible tags; it is exhaustive, i.e., the correct tag is
always among the suggested ones for each token.
We also used 70 linguistically-motivated, high-
precision rules in order to further reduce the num-
ber of possible tags suggested by the lexicon.
The rules are similar to those proposed by Hin-
richs and Trushkina (2004) for German; we im-
plemented them as constraints in the CLaRK sys-
tem (Simov et al., 2003).
Here is an example of a rule: If a wordform
is ambiguous between a masculine count noun
(Ncmt) and a singular short definite masculine
noun (Ncmsh), the Ncmt tag should be chosen if
the previous token is a numeral or a number.
The 70 rules were developed by linguists based
on observations over the training dataset only.
They target primarily the most frequent cases of
ambiguity, and to a lesser extent some infrequent
but very problematic cases. Some rules operate
over classes of words, while other refer to partic-
ular wordforms. The rules were designed to be
100% accurate on our training dataset; our exper-
iments show that they are also 100% accurate on
the test and on the development dataset.
Note that some of the rules are dependent on
others, and thus the order of their cascaded appli-
cation is important. For example, the wordform a
is ambiguous between an accusative feminine sin-
gular short form of a personal pronoun (‘her’) and
an interjection (‘wow’). To handle this properly,
the rule for interjection, which targets sentence
initial positions, followed by a comma, needs to
be executed first. The rule for personal pronouns
is only applied afterwards.
</bodyText>
<table confidence="0.9978093">
Word Tags
To$i Ppe-os3m
obaqe Cc; Dd
nama Afsi; Vnitf-o3s; Vnitf-r3s;
Vpitf-o2s; Vpitf-o3s; Vpitf-r3s
v•zmo)Knost Ncfsi
da Ta;Tx
sledi Ncfpi; Vpitf-o2s; Vpitf-o3s; Vpitf-r3s;
Vpitz–2s
� � � � � �
</table>
<tableCaption confidence="0.667812666666667">
Table 1: Sample fragment showing the possible tags
suggested by the lexicon. The tags that are further
filtered by the rules are in italic; the correct tag is bold.
</tableCaption>
<bodyText confidence="0.999829571428571">
The rules are quite efficient at reducing the POS
ambiguity. On the test dataset, before the rule ap-
plication, 34.2% of the tokens (excluding punctu-
ation) had more than one tag in our morphological
lexicon. This number is reduced to 18.5% after
the cascaded application of the 70 linguistic rules.
Table 1 illustrates the effect of the rules on a small
sentence fragment. In this example, the rules have
left only one tag (the correct one) for three of the
ambiguous words. Since the rules in essence de-
crease the average number of tags per token, we
calculated that the lexicon suggests 1.6 tags per
token on average, and after the application of the
rules this number decreases to 1.44 per token.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="method">
5 Datasets
</sectionHeader>
<subsectionHeader confidence="0.996044">
5.1 BulTreeBank
</subsectionHeader>
<bodyText confidence="0.999295764705882">
We used the latest version of the BulTree-
Bank (Simov and Osenova, 2004), which contains
20,556 sentences and 321,542 word tokens (four
times less than the English Penn Treebank), anno-
tated using a total of 680 unique morpho-syntactic
tags. See (Simov et al., 2004) for a detailed de-
scription of the BulTreeBank tagset.
We split the data into training/development/test
as shown in Table 2. Note that only 552 of all 680
tag types were used in the training dataset, and
the development and the test datasets combined
contain a total of 128 new tag types that were not
seen in the training dataset. Moreover, 32% of the
word types in the development dataset and 31%
of those in the testing dataset do not occur in the
training dataset. Thus, data sparseness is an issue
at two levels: word-level and tag-level.
</bodyText>
<table confidence="0.99690775">
Dataset Sentences Tokens Types Tags
Train 16,532 253,526 38,659 552
Dev 2,007 32,995 9,635 425
Test 2,017 35,021 9,627 435
</table>
<tableCaption confidence="0.99915">
Table 2: Statistics about our datasets.
</tableCaption>
<subsectionHeader confidence="0.998961">
5.2 Morphological Lexicon
</subsectionHeader>
<bodyText confidence="0.999913">
In order to alleviate the data sparseness issues,
we further used a large morphological lexicon for
Bulgarian, which is an extended version of the
dictionary described in (Popov et al., 1998) and
(Popov et al., 2003). It contains over 1.5M in-
flected wordforms (for 110K lemmata and 40K
proper names), each mapped to a set of possible
morpho-syntactic tags.
</bodyText>
<page confidence="0.998578">
495
</page>
<sectionHeader confidence="0.99468" genericHeader="method">
6 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999851425">
State-of-the-art POS taggers for English typically
build a lexicon containing all tags a word type has
taken in the training dataset; this lexicon is then
used to limit the set of possible tags that an input
token can be assigned, i.e., it imposes a hard con-
straint on the possibilities explored by the POS
tagger. For example, if can has only been tagged
as a verb and as a noun in the training dataset,
it will be only assigned those two tags at test
time; other tags such as adjective, adverb and pro-
noun will not be considered. Out-of-vocabulary
words, i.e., those that were not seen in the train-
ing dataset, are constrained as well, e.g., to a small
set of frequent open-class tags.
In our experiments, we used a morphological
lexicon that is much larger than what could be
built from the training corpus only: building a
lexicon from the training corpus only is of lim-
ited utility since one can hardly expect to see in
the training corpus all 52 synthetic forms a verb
can possibly have. Moreover, we did not use the
tags listed in the lexicon as hard constraints (ex-
cept in one of our baselines); instead, we experi-
mented with a different, non-restrictive approach:
we used the lexicon’s predictions as features or
soft constraints, i.e., as suggestions only, thus al-
lowing each token to take any possible tag. Note
that for both known and out-of-vocabulary words
we used all 680 tags rather than the 552 tags ob-
served in the training dataset; we could afford to
explore this huge search space thanks to the effi-
ciency of the guided learning framework. Allow-
ing all 680 tags on training helped the model by
exposing it to a larger set of negative examples.
We combined these lexicon features with stan-
dard features extracted from the training corpus.
We further experimented with the 70 contextual
linguistic rules, using them (a) as soft and (b) as
hard constraints. Finally, we set four baselines:
three that do not use the lexicon and one that does.
</bodyText>
<table confidence="0.997249">
Accuracy (%)
# Baselines (token-level)
1 MFT + unknowns are wrong 78.10
2 MFT + unknowns are Ncrosi 78.52
3 MFT + guesser for unknowns 79.49
4 MFT + lexicon tag-classes 94.40
</table>
<tableCaption confidence="0.999692">
Table 3: Most-frequent-tag (MFT) baselines.
</tableCaption>
<subsectionHeader confidence="0.995783">
6.1 Baselines
</subsectionHeader>
<bodyText confidence="0.956606040816327">
First, we experimented with the most-frequent-
tag baseline, which is standard for POS tagging.
This baseline ignores context altogether and as-
signs each word type the POS tag it was most
frequently seen with in the training dataset; ties
are broken randomly. We coped with word types
not seen in the training dataset using three sim-
ple strategies: (a) we considered them all wrong,
(b) we assigned them Ncmsi, which is the most
frequent open-class tag in the training dataset, or
(c) we used a very simple guesser, which assigned
Ncfsi, Ncnsi, Ncfsi, and Ncmsf, if the target word
ended by -a, -o, -u, and - bT, respectively, other-
wise, it assigned Ncmsi. The results are shown
in lines 1-3 of Table 3: we can see that the token-
level accuracy ranges in 78-80% for (a)-(c), which
is relatively high, given that we use a large inven-
tory of 680 morpho-syntactic tags.
We further tried a baseline that uses the above-
described morphological lexicon, in addition to
the training dataset. We first built two frequency
lists, containing respectively (1) the most frequent
tag in the training dataset for each word type, as
before, and (2) the most frequent tag in the train-
ing dataset for each class of tags that can be as-
signed to some word type, according to the lexi-
con. For example, the most frequent tag for polu-
Tuxa is Ncfsi, and the most frequent tag for the
tag-class {Ncmt;Ncmsi} is Ncmt.
Given a target word type, this new baseline first
tries to assign it the most frequent tag from the
first list. If this is not possible, which happens
(i) in case of ties or (ii) when the word type was
not seen on training, it extracts the tag-class from
the lexicon and consults the second list. If there
is a single most frequent tag in the corpus for this
tag-class, it is assigned; otherwise a random tag
from this tag-class is selected.
Line 4 of Table 3 shows that this latter baseline
achieves a very high accuracy of 94.40%. Note,
however, that this is over-optimistic: the lexicon
contains a tag-class for each word type in our test-
ing dataset, i.e., while there can be word types
not seen in the training dataset, there are no word
types that are not listed in the lexicon. Thus, this
high accuracy is probably due to a large extent
to the scale and quality of our morphological lexi-
con, and it might not be as strong with smaller lex-
icons; we plan to investigate this in future work.
</bodyText>
<page confidence="0.995573">
496
</page>
<subsectionHeader confidence="0.992952">
6.2 Lexicon Tags as Soft Constraints
</subsectionHeader>
<bodyText confidence="0.937157">
We experimented with three types of features:
</bodyText>
<listItem confidence="0.986467166666667">
1. Word-related features only;
2. Word-related features + the tags suggested
by the lexicon;
3. Word-related features + the tags suggested
by the lexicon but then further filtered using
the 70 contextual linguistic rules.
</listItem>
<bodyText confidence="0.999949487179487">
Table 4 shows the sentence-level and the token-
level accuracy on the test dataset for the three
kinds of features: shown on lines 1, 3 and 4, re-
spectively. We can see that using the tags pro-
posed by the lexicon as features (lines 3 and 4)
has a major positive impact, yielding up to 49%
error reduction at the token-level and up to 37%
at the sentence-level, as compared to using word-
related features alone (line 1).
Interestingly, filtering the tags proposed by the
lexicon using the 70 contextual linguistic rules
yields a minor decrease in accuracy both at the
word token-level and at the sentence-level (com-
pare line 4 to line 2). This is surprising since
the linguistic rules are extremely reliable: they
were designed to be 100% accurate on the train-
ing dataset, and we found them experimentally to
be 100% correct on the development and on the
testing dataset as well.
One possible explanation is that by limiting the
set of available tags for a given token at training
time, we prevent the model from observing some
potentially useful negative examples. We tested
this hypothesis by using the unfiltered lexicon
predictions at training time but then making use
of the filtered ones at testing time; the results are
shown on line 5. We can observe a small increase
in accuracy compared to line 4: from 97.80% to
97.84% at the token-level, and from 70.30% to
70.40% at the sentence-level. Although these dif-
ferences are tiny, they suggest that having more
negative examples at training is helpful.
We can conclude that using the lexicon as a
source of soft constraints has a major positive im-
pact, e.g., because it provides access to impor-
tant external knowledge that is complementary
to what can be learned from the training corpus
alone; the improvements when using linguistic
rules as soft constraints are more limited.
</bodyText>
<subsectionHeader confidence="0.99866">
6.3 Linguistic Rules as Hard Constraints
</subsectionHeader>
<bodyText confidence="0.999946">
Next, we experimented with using the suggestions
of the linguistic rules as hard constraints. Table 4
shows that this is a very good idea. Comparing
line 1 to line 2, which do not use the morpholog-
ical lexicon, we can see very significant improve-
ments: from 95.72% to 97.20% at the token-level
and from 52.95% to 64.50% at the sentence-level.
The improvements are smaller but still consistent
when the morphological lexicon is used: compar-
ing lines 3 and 4 to lines 6 and 7, respectively, we
see an improvement from 97.83% to 97.91% and
from 97.80% to 97.93% at the token-level, and
about 1% absolute at the sentence-level.
</bodyText>
<subsectionHeader confidence="0.996696">
6.4 Increasing the Beam Size
</subsectionHeader>
<bodyText confidence="0.9999796">
Finally, we increased the beam size of guided
learning from 1 to 3 as in (Shen et al., 2007).
Comparing line 7 to line 8 in Table 4, we can see
that this yields further token-level improvement:
from 97.93% to 97.98%.
</bodyText>
<sectionHeader confidence="0.998839" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999716">
Table 5 compares our results to previously re-
ported evaluation results for Bulgarian. The
first four lines show the token-level accuracy for
standard POS tagging tools trained and evalu-
ated on the BulTreeBank:2 TreeTagger (Schmid,
1994), which uses decision trees, TnT (Brants,
2000), which uses a hidden Markov model,
SVMtool (Gim´enez and M`arquez, 2004), which
is based on support vector machines, and
ACOPOST (Schr¨oder, 2002), implementing the
memory-based model of Daelemans et al. (1996).
The following lines report the token-level accu-
racy reported in previous work, as compared to
our own experiments using guided learning.
We can see that we outperform by a very large
margin (92.53% vs. 97.98%, which represents
73% error reduction) the systems from the first
four lines, which are directly comparable to our
experiments: they are trained and evaluated on the
BulTreeBank using the full inventory of 680 tags.
We further achieved statistically significant im-
provement (p &lt; 0.0001; Pearson’s chi-squared
test (Plackett, 1983)) over the best pervious result
on 680 tags: from 94.65% to 97.98%, which rep-
resents 62.24% error reduction at the token-level.
</bodyText>
<footnote confidence="0.996769666666667">
2We used the pre-trained TreeTagger; for the rest, we re-
port the accuracy given on the Webpage of the BulTreeBank:
www.bultreebank.org/taggers/taggers.html
</footnote>
<page confidence="0.993575">
497
</page>
<table confidence="0.8188408">
# Lexicon Linguistic Rules (applied to filter): Beam Accuracy (%)
(source of) (a) the lexicon features (b) the output tags size Sentence-level Token-level
1 – – – 1 52.95 95.72
2 – – yes 1 64.50 97.20
3 features – – 1 70.40 97.83
4 features yes – 1 70.30 97.80
5 features yes, for test only – 1 70.40 97.84
6 features – yes 1 71.34 97.91
7 features yes yes 1 71.69 97.93
8 features yes yes 3 71.94 97.98
</table>
<tableCaption confidence="0.878722888888889">
Table 4: Evaluation results on the test dataset. Line 1 shows the evaluation results when using features derived
from the text corpus only; these features are used by all systems in the table. Line 2 further uses the contextual
linguistic rules to limit the set of possible POS tags that can be predicted. Note that these rules (1) consult the
lexicon, and (2) always predict a single POS tag. Line 3 uses the POS tags listed in the lexicon as features, i.e.,
as soft suggestions only. Line 4 is like line 3, but the list of feature-tags proposed by the lexicon is filtered by
the contextual linguistic rules. Line 5 is like line 4, but the linguistic rules filtering is only applied at test time;
it is not done on training. Lines 6 and 7 are similar to lines 3 and 4, respectively, but here the linguistic rules
are further applied to limit the set of possible POS tags that can be predicted, i.e., the rules are used as hard
constraints. Finally, line 8 is like line 7, but here the beam size is increased to 3.
</tableCaption>
<bodyText confidence="0.999934537037037">
Overall, we improved over almost all previ-
ously published results. Our accuracy is sec-
ond only to the manual rules approach of Do-
jchinova and Mihov (2004). Note, however, that
they used 40 tags only, i.e., their inventory is 17
times smaller than ours. Moreover, they have op-
timized their tagset specifically to achieve very
high POS tagging accuracy by choosing not to at-
tempt to resolve some inherently hard systematic
ambiguities, e.g., they do not try to choose be-
tween second and third person past singular verbs,
whose inflected forms are identical in Bulgarian
and hard to distinguish when the subject is not
present (Bulgarian is a pro-drop language).
In order to compare our results more closely
to the smaller tagsets in Table 5, we evaluated
our best model with respect to (a) the first letter
of the tag only (which is part-of-speech only, no
morphological information; 13 tags), e.g., Ncmsf
becomes N, and (b) the first two letters of the
tag (POS + limited morphological information;
49 tags), e.g., Ncmsf becomes Nc. This yielded
99.30% accuracy for (a) and 98.85% for (b).
The latter improves over (Dojchinova and Mihov,
2004), while using a bit larger number of tags.
Our best token-level accuracy of 97.98% is
comparable and even slightly better than the state-
of-the-art results for English: 97.33% when using
Penn Treebank data only (Shen et al., 2007), and
97.50% for Penn Treebank plus some additional
unlabeled data (Søgaard, 2011). Of course, our
results are only indirectly comparable to English.
Still, our performance is impressive because
(1) our model is trained on 253,526 tokens only
while the standard training sections 0-18 of the
Penn Treebank contain a total of 912,344 tokens,
i.e., almost four times more, and (2) we predict
680 rather than just 48 tags as for the Penn Tree-
bank, which is 14 times more.
Note, however, that (1) we used a large exter-
nal morphological lexicon for Bulgarian, which
yielded about 50% error reduction (without it,
our accuracy was 95.72% only), and (2) our
train/dev/test sentences are generally shorter, and
thus arguably simpler for a POS tagger to analyze:
we have 17.4 words per test sentence in the Bul-
TreeBank vs. 23.7 in the Penn Treebank.
Our results also compare favorably to the state-
of-the-art results for other morphologically com-
plex languages that use large tagsets, e.g., 95.2%
for Czech with 1,400+ tags (Hajiˇc et al., 2001),
92.1% for Icelandic with 639 tags (Dredze and
Wallenberg, 2008), 97.6% for Arabic with 139
tags (Habash and Rambow, 2005).
</bodyText>
<sectionHeader confidence="0.998832" genericHeader="method">
8 Error Analysis
</sectionHeader>
<bodyText confidence="0.999979125">
In this section, we present error analysis with re-
spect to the impact of the POS tagger’s perfor-
mance on other processing steps in a natural lan-
guage processing pipeline, such as lemmatization
and syntactic dependency parsing.
First, we explore the most frequently confused
pairs of tags for our best-performing POS tagging
system; these are shown in Table 6.
</bodyText>
<page confidence="0.997314">
498
</page>
<table confidence="0.999840263157895">
Tool/Authors Method # Tags Accuracy
(token-level, %)
*TreeTagger Decision Trees 680 89.21
*ACOPOST Memory-based Learning 680 89.91
*SVMtool Support Vector Machines 680 92.22
*TnT Hidden Markov Model 680 92.53
(Georgiev et al., 2009) Maximum Entropy 680 90.34
(Simov and Osenova, 2001) Recurrent Neural Network 160 92.87
(Georgiev et al., 2009) Maximum Entropy 95 94.43
(Savkov et al., 2011) SVM + Lexicon + Rules 680 94.65
(Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R)
(Simov and Osenova, 2001) Recurrent Neural Network 15 95.17
(Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50
(Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40
Guided Learning 680 95.72
Guided Learning + Lexicon 680 97.83
This work Guided Learning + Lexicon + Rules 680 97.98
Guided Learning + Lexicon + Rules 49 98.85
Guided Learning + Lexicon + Rules 13 99.30
</table>
<tableCaption confidence="0.986904666666667">
Table 5: Comparison to previous work for Bulgarian. The first four lines report evaluation results for various
standard POS tagging tools, which were retrained and evaluated on the BulTreeBank. The following lines report
token-level accuracy for previously published work, as compared to our own experiments using guided learning.
</tableCaption>
<bodyText confidence="0.99976375">
We can see that most of the wrong tags share
the same part-of-speech (indicated by the initial
uppercase letter), such as V for verb, N for noun,
etc. This means that most errors refer to the mor-
phosyntactic features. For example, personal or
impersonal verb; definite or indefinite feminine
noun; singular or plural masculine adjective, etc.
At the same time, there are also cases, where the
error has to do with the part-of-speech label itself.
For example, between an adjective and an adverb,
or between a numeral and an indefinite pronoun.
We want to use the above tagger to develop
</bodyText>
<listItem confidence="0.85688975">
(1) a rule-based lemmatizer, using the morpholog-
ical lexicon, e.g., as in (Plisson et al., 2004), and
(2) a dependency parser like MaltParser (Nivre et
al., 2007), trained on the dependency part of the
</listItem>
<bodyText confidence="0.972542">
BulTreeBank. We thus study the potential impact
of wrong tags on the performance of these tools.
The lemmatizer relies on the lexicon and uses
string transformation functions defined via two
operations – remove and concatenate:
if tag= Tag then
{remove OldEnd; concatenate NewEnd}
where Tag is the tag of the wordform, OldEnd is
the string that has to be removed from the end of
the wordform, and NewEnd is the string that has
to be concatenated to the beginning of the word-
form in order to produce the lemma.
Here is an example of such a rule:
if tag = Vpitf-o1s then
{remove ox; concatenate a}
The application of the above rule to the past
simple verb form neTox (‘I read’) would remove
ox, and then concatenate a. The result would be
the correct lemma neTa (‘to read’).
Such rules are generated for each wordform in
the morphological lexicon; the above functional
representation allows for compact representation
in a finite state automaton. Similar rules are ap-
plied to the unknown words, where the lemma-
tizer tries to guess the correct lemma.
Obviously, the applicability of each rule cru-
cially depends on the output of the POS tagger.
If the tagger suggests the correct tag, then the
wordform would be lemmatized correctly. Note
that, in some cases of wrongly assigned POS tags
in a given context, we might still get the correct
lemma. This is possible in the majority of the
erroneous cases in which the part-of-speech has
been assigned correctly, but the wrong grammat-
ical alternative has been selected. In such cases,
the error does not influence lemmatization.
In order to calculate the proportion of such
cases, we divided each tag into two parts:
(a) grammatical features that are common for all
wordforms of a given lemma, and (b) features that
are specific to the wordform.
</bodyText>
<page confidence="0.99745">
499
</page>
<table confidence="0.9860035">
Freq. Gold Tag Proposed Tag
43 Ansi Dm
23 Vpitf-r3s Vnitf-r3s
16 Npmsh Npmsi
14 Vpiif-r3s Vniif-r3s
13 Npfsd Npfsi
12 Dm Ansi
12 Vpitcam-smi Vpitcao-smi
12 Vpptf-r3p Vpitf-r3p
11 Vpptf-r3s Vpptf-o3s
10 Mcmsi Pfe-os-mi
10 Ppetas3n Ppetas3m
10 Ppetds3f Psot–3–f
9 Npnsi Npnsd
9 Vpptf-o3s Vpptf-r3s
8 Dm A-pi
8 Ppxts Ppxtd
7 Mcfsi Pfe-os-fi
7 Npfsi Npfsd
7 Ppetas3m Ppetas3n
7 Vnitf-r3s Vpitf-r3s
7 Vpitcam-p-i Vpitcao-p-i
</table>
<tableCaption confidence="0.996387">
Table 6: Most frequently confused pairs of tags.
</tableCaption>
<bodyText confidence="0.999894692307692">
The part-of-speech features are always deter-
mined by the lemma. For example, Bulgarian
verbs have the lemma features aspect and tran-
sitivity. If they are correct, then the lemma is pre-
dicted also correctly, regardless of whether cor-
rect or wrong on the grammatical features. For
example, if the verb participle form (aorist or
imperfect) has its correct aspect and transitivity,
then it is lemmatized also correctly, regardless
of whether the imperfect or aorist features were
guessed correctly; similarly, for other error types.
We evaluated these cases for the 711 errors in our
experiment, and we found that 206 of them (about
29%) were non-problematic for lemmatization.
For the MaltParser, we encode most of the
grammatical features of the wordforms as spe-
cific features for the parser. Hence, it is much
harder to evaluate the problematic cases due to
the tagger. Still, we were able to make an es-
timation of some cases. Our strategy was to ig-
nore the grammatical features that do not always
contribute to the syntactic behavior of the word-
forms. Such grammatical features for the verbs
are aspect and tense. Thus, proposing perfective
instead of imperfective for a verb or present in-
stead of past tense would not cause problems for
the MaltParser. Among our 711 errors, 190 cases
(or about 27%) were not problematic for parsing.
Finally, we should note that there are two spe-
cial classes of tokens for which it is generally
hard to predict some of the grammatical features:
(1) abbreviations and (2) numerals written with
digits. In sentences, they participate in agreement
relations only if they are pronounced as whole
phrases; unfortunately, it is very hard for the tag-
ger to guess such relations since it does not have
at its disposal enough features, such as the inflec-
tion of the numeral form, that might help detect
and use the agreement pattern.
</bodyText>
<sectionHeader confidence="0.982669" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991185185185">
We have presented experiments with part-of-
speech tagging for Bulgarian, a Slavic language
with rich inflectional and derivational morphol-
ogy. Unlike most previous work for this language,
which has limited the number of possible tags, we
used a very rich tagset of 680 morpho-syntactic
tags as defined in the BulTreeBank. By com-
bining a large morphological lexicon with prior
linguistic knowledge and guided learning from a
POS-annotated corpus, we achieved accuracy of
97.98%, which is a significant improvement over
the state-of-the-art for Bulgarian. Our token-level
accuracy is also comparable to the best results re-
ported for English.
In future work, we want to experiment with a
richer set of features, e.g., derived from unlabeled
data (Søgaard, 2011) or from the Web (Umansky-
Pesin et al., 2010; Bansal and Klein, 2011). We
further plan to explore ways to decompose the
complex Bulgarian morpho-syntactic tags, e.g., as
proposed in (Simov and Osenova, 2001) and
(Smith et al., 2005). Modeling long-distance
syntactic dependencies (Dredze and Wallenberg,
2008) is another promising direction; we believe
this can be implemented efficiently using poste-
rior regularization (Graca et al., 2009) or expecta-
tion constraints (Bellare et al., 2009).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999796375">
We would like to thank the anonymous reviewers
for their useful comments, which have helped us
improve the paper.
The research presented above has been par-
tially supported by the EU FP7 project 231720
EuroMatrixPlus, and by the SmartBook project,
funded by the Bulgarian National Science Fund
under grant D002-111/15.12.2008.
</bodyText>
<page confidence="0.990014">
500
</page>
<sectionHeader confidence="0.990096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999378901785715">
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, ACL-HLT ’10, pages 693–702, Portland, Ore-
gon, USA.
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning
with expectation constraints. In Proceedings of the
25th Conference on Uncertainty in Artificial Intel-
ligence, UAI ’09, pages 43–50, Montreal, Quebec,
Canada.
Thorsten Brants. 2000. TnT – a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing, ANLP ’00, pages
224–231, Seattle, Washington, USA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Comput. Linguist.,
21:543–565.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics, Main Volume, ACL ’04,
pages 111–118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’02, pages 1–8,
Philadelphia, PA, USA.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: A memory-based part
of speech tagger generator. In Eva Ejerhed and
Ido Dagan, editors, Fourth Workshop on Very Large
Corpora, pages 14–27, Copenhagen, Denmark.
Veselka Dojchinova and Stoyan Mihov. 2004. High
performance part-of-speech tagging of Bulgarian.
In Christoph Bussler and Dieter Fensel, editors,
AIMSA, volume 3192 of Lecture Notes in Computer
Science, pages 246–255. Springer.
Mark Dredze and Joel Wallenberg. 2008. Icelandic
data driven part of speech tagging. In Proceedings
of the 44th Annual Meeting of the Association of
Computational Linguistics: Short Papers, ACL ’08,
pages 33–36, Columbus, Ohio, USA.
Georgi Georgiev, Preslav Nakov, Petya Osenova, and
Kiril Simov. 2009. Cross-lingual adaptation as
a baseline: adapting maximum entropy models to
Bulgarian. In Proceedings of the RANLP’09 Work-
shop on Adaptation of Language Resources and
Technology to New Domains, AdaptLRTtoND ’09,
pages 35–38, Borovets, Bulgaria.
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of the 4th Inter-
national Conference on Language Resources and
Evaluation, LREC ’04, Lisbon, Portugal.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fer-
nando Pereira. 2009. Posterior vs parameter spar-
sity in latent variable models. In Yoshua Bengio,
Dale Schuurmans, John D. Lafferty, Christopher
K. I. Williams, and Aron Culotta, editors, Advances
in Neural Information Processing Systems 22, NIPS
’09, pages 664–672. Curran Associates, Inc., Van-
couver, British Columbia, Canada.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’05, pages
573–580, Ann Arbor, Michigan.
Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva,
and Vladim´ır Petkeviˇc. 2001. Serial combination
of rules and statistics: A case study in Czech tag-
ging. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
ACL ’01, pages 268–275, Toulouse, France.
Jan Hajiˇc. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajiˇcov´a, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov´a, pages 12–
19. Prague Karolinum, Charles University Press.
Erhard W. Hinrichs and Julia S. Trushkina. 2004.
Forging agreement: Morphological disambiguation
of noun phrases. Research on Language &amp; Compu-
tation, 2:621–648.
Stig Johansson, Eric Atwell, Roger Garside, and Geof-
frey Leech, 1986. The Tagged LOB Corpus: Users’
manual. ICAME, The Norwegian Computing Cen-
tre for the Humanities, Bergen University, Norway.
Hristo Krushkov. 1997. Modelling and building ma-
chine dictionaries and morphological processors
(in Bulgarian). Ph.D. thesis, University of Plov-
div, Faculty of Mathematics and Informatics, Plov-
div, Bulgaria.
Henry Kuˇcera and Winthrop Nelson Francis. 1967.
Computational analysis of present-day American
English. Brown University Press, Providence, RI.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA.
Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim
Buckwalter. 2003. Arabic Treebank: Part 1 v 2.0.
LDC2003T06.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
put. Linguist., 19:313–330.
</reference>
<page confidence="0.970931">
501
</page>
<reference confidence="0.9999158">
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
J¨orgen Pind, Fridrik Magn´usson, and Stef´an Briem.
1991. The Icelandic frequency dictionary. Techni-
cal report, The Institute of Lexicography, University
of Iceland, Reykjavik, Iceland.
Robin L. Plackett. 1983. Karl Pearson and the Chi-
Squared Test. International Statistical Review /Re-
vue Internationale de Statistique, 51(1):59–72.
Jo¨el Plisson, Nada Lavraˇc, and Dunja Mladeni´c. 2004.
A rule based approach to word lemmatization. In
Proceedings of the 7th International Multiconfer-
ence: Information Society, IS ’2004, pages 83–86,
Ljubljana, Slovenia.
Dimitar Popov, Kiril Simov, and Svetlomira Vidinska.
1998. Dictionary of Writing, Pronunciation and
Punctuation of Bulgarian Language (in Bulgarian).
Atlantis KL, Sofia, Bulgaria.
Dimityr Popov, Kiril Simov, Svetlomira Vidinska, and
Petya Osenova. 2003. Spelling Dictionary of Bul-
garian. Nauka i izkustvo, Sofia, Bulgaria.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Eva Ejerhed
and Ido Dagan, editors, Fourth Workshop on Very
Large Corpora, pages 133–142, Copenhagen, Den-
mark.
Aleksandar Savkov, Laska Laskova, Petya Osenova,
Kiril Simov, and Stanislava Kancheva. 2011.
A web-based morphological tagger for Bulgarian.
In Daniela Majchr´akov´a and Radovan Garabik,
editors, Slovko 2011. Sixth International Confer-
ence. Natural Language Processing, Multilingual-
ity, pages 126–137, Modra/Bratislava, Slovakia.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44–49, Manchester, UK.
Ingo Schr¨oder. 2002. A case study in part-of-speech-
tagging using the ICOPOST toolkit. Technical Re-
port FBI-HH-M-314/02, Department of Computer
Science, University of Hamburg.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 760–767, Prague, Czech Republic.
Kiril Simov and Petya Osenova. 2001. A hybrid
system for morphosyntactic disambiguation in Bul-
garian. In Proceedings of the EuroConference on
Recent Advances in Natural Language Processing,
RANLP ’01, pages 5–7, Tzigov chark, Bulgaria.
Kiril Simov and Petya Osenova. 2004. BTB-TR04:
BulTreeBank morphosyntactic annotation of Bul-
garian texts. Technical Report BTB-TR04, Bulgar-
ian Academy of Sciences.
Kiril Ivanov Simov, Alexander Simov, Milen
Kouylekov, Krasimira Ivanova, Ilko Grigorov, and
Hristo Ganev. 2003. Development of corpora
within the CLaRK system: The BulTreeBank
project experience. In Proceedings of the 10th con-
ference of the European chapter of the Association
for Computational Linguistics, EACL ’03, pages
243–246, Budapest, Hungary.
Kiril Simov, Petya Osenova, and Milena Slavcheva.
2004. BTB-TR03: BulTreeBank morphosyntac-
tic tagset. Technical Report BTB-TR03, Bulgarian
Academy of Sciences.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambigua-
tion with random fields. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 475–482, Vancouver, British
Columbia, Canada.
Anders Søgaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL-HLT ’10,
pages 48–52, Portland, Oregon, USA.
Hristo Tanev and Ruslan Mitkov. 2002. Shallow
language processing architecture for Bulgarian. In
Proceedings of the 19th International Conference
on Computational Linguistics, COLING ’02, pages
1–7, Taipei, Taiwan.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ’03, pages
173–180, Edmonton, Canada.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT-EMNLP ’05, pages 467–474, Vancouver,
British Columbia, Canada.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, ACL-HLT ’10, pages
238–246, Portland, Oregon, USA.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algo-
rithm for POS tagging of unknown words. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ’10,
pages 1274–1282, Beijing, China.
</reference>
<page confidence="0.997624">
502
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.392374">
<title confidence="0.996247">Feature-Rich Part-of-speech for Morphologically Complex Languages: Application to Bulgarian</title>
<author confidence="0.999414">Georgi Georgiev</author>
<author confidence="0.999414">Valentin Zhikov</author>
<affiliation confidence="0.971231">Ontotext AD</affiliation>
<address confidence="0.994144">135 Tsarigradsko Sh., Sofia, Bulgaria</address>
<author confidence="0.737072">Petya Osenova</author>
<author confidence="0.737072">Kiril</author>
<affiliation confidence="0.871632">IICT, Bulgarian Academy of</affiliation>
<address confidence="0.944072">25A Acad. G. Bonchev, Sofia, Bulgaria</address>
<author confidence="0.830786">Preslav Nakov</author>
<affiliation confidence="0.915443">Qatar Computing Research Institute, Qatar</affiliation>
<address confidence="0.975473">Tornado Tower, floor 10, P.O. Box 5825, Doha, Qatar</address>
<email confidence="0.984269">pnakov@qf.org.qa</email>
<abstract confidence="0.978434461538462">We present experiments with part-ofspeech tagging for Bulgarian, a Slavic language with rich inflectional and derivational morphology. Unlike most previous work, which has used a small number of grammatical categories, we work with 680 morpho-syntactic tags. We combine a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, achieving accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL-HLT ’10,</booktitle>
<pages>693--702</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="37463" citStr="Bansal and Klein, 2011" startWordPosition="6161" endWordPosition="6164">ed the number of possible tags, we used a very rich tagset of 680 morpho-syntactic tags as defined in the BulTreeBank. By combining a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, we achieved accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian. Our token-level accuracy is also comparable to the best results reported for English. In future work, we want to experiment with a richer set of features, e.g., derived from unlabeled data (Søgaard, 2011) or from the Web (UmanskyPesin et al., 2010; Bansal and Klein, 2011). We further plan to explore ways to decompose the complex Bulgarian morpho-syntactic tags, e.g., as proposed in (Simov and Osenova, 2001) and (Smith et al., 2005). Modeling long-distance syntactic dependencies (Dredze and Wallenberg, 2008) is another promising direction; we believe this can be implemented efficiently using posterior regularization (Graca et al., 2009) or expectation constraints (Bellare et al., 2009). Acknowledgments We would like to thank the anonymous reviewers for their useful comments, which have helped us improve the paper. The research presented above has been partially</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL-HLT ’10, pages 693–702, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Alternating projections for learning with expectation constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, UAI ’09,</booktitle>
<pages>43--50</pages>
<location>Montreal, Quebec, Canada.</location>
<marker>Bellare, Druck, McCallum, 2009</marker>
<rawString>Kedar Bellare, Gregory Druck, and Andrew McCallum. 2009. Alternating projections for learning with expectation constraints. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 43–50, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing, ANLP ’00,</booktitle>
<pages>224--231</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="4762" citStr="Brants (2000)" startWordPosition="717" endWordPosition="718">arch on part-of-speech tagging has focused on English, and has relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a</context>
<context position="25128" citStr="Brants, 2000" startWordPosition="4103" endWordPosition="4104">7.93% at the token-level, and about 1% absolute at the sentence-level. 6.4 Increasing the Beam Size Finally, we increased the beam size of guided learning from 1 to 3 as in (Shen et al., 2007). Comparing line 7 to line 8 in Table 4, we can see that this yields further token-level improvement: from 97.93% to 97.98%. 7 Discussion Table 5 compares our results to previously reported evaluation results for Bulgarian. The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gim´enez and M`arquez, 2004), which is based on support vector machines, and ACOPOST (Schr¨oder, 2002), implementing the memory-based model of Daelemans et al. (1996). The following lines report the token-level accuracy reported in previous work, as compared to our own experiments using guided learning. We can see that we outperform by a very large margin (92.53% vs. 97.98%, which represents 73% error reduction) the systems from the first four lines, which are directly comparable to our experiments: they are trained and evaluated on the BulTreeBank </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-ofspeech tagger. In Proceedings of the Sixth Applied Natural Language Processing, ANLP ’00, pages 224–231, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Comput. Linguist.,</journal>
<pages>21--543</pages>
<contexts>
<context position="4420" citStr="Brill (1995)" startWordPosition="675" endWordPosition="676">cific error analysis, and Section 9 concludes and points to some promising directions for future work. 492 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492–502, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics 2 Related Work Most research on part-of-speech tagging has focused on English, and has relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic depend</context>
<context position="7250" citStr="Brill (1995)" startWordPosition="1091" endWordPosition="1092">onsidered using non-local features. Similarly, Smith et al. (2005) decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into a single CRF model; this yielded competitive results for Arabic, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morpho</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging. Comput. Linguist., 21:543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume, ACL ’04,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="5404" citStr="Collins and Roark, 2004" startWordPosition="811" endWordPosition="814"> Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that searches the space of future actions, yielding 97.3% accuracy. For morphologically complex languages, the problem of POS tagging typically includes morphological disambiguation, which yields a much larger number of tags. For example, for Arabic, Habash and Rambow (2005) used support vector machines (SVM), achieving 97.6% accuracy with 139 tags from the Arabic Treebank (Maamour</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume, ACL ’04, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="4826" citStr="Collins (2002)" startWordPosition="726" endWordPosition="727">relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive cl</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02, pages 1–8, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>In Eva Ejerhed and Ido Dagan, editors, Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="25339" citStr="Daelemans et al. (1996)" startWordPosition="4132" endWordPosition="4135">ng line 7 to line 8 in Table 4, we can see that this yields further token-level improvement: from 97.93% to 97.98%. 7 Discussion Table 5 compares our results to previously reported evaluation results for Bulgarian. The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gim´enez and M`arquez, 2004), which is based on support vector machines, and ACOPOST (Schr¨oder, 2002), implementing the memory-based model of Daelemans et al. (1996). The following lines report the token-level accuracy reported in previous work, as compared to our own experiments using guided learning. We can see that we outperform by a very large margin (92.53% vs. 97.98%, which represents 73% error reduction) the systems from the first four lines, which are directly comparable to our experiments: they are trained and evaluated on the BulTreeBank using the full inventory of 680 tags. We further achieved statistically significant improvement (p &lt; 0.0001; Pearson’s chi-squared test (Plackett, 1983)) over the best pervious result on 680 tags: from 94.65% to</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. MBT: A memory-based part of speech tagger generator. In Eva Ejerhed and Ido Dagan, editors, Fourth Workshop on Very Large Corpora, pages 14–27, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselka Dojchinova</author>
<author>Stoyan Mihov</author>
</authors>
<title>High performance part-of-speech tagging of Bulgarian.</title>
<date>2004</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3192</volume>
<pages>246--255</pages>
<editor>In Christoph Bussler and Dieter Fensel, editors, AIMSA,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="7097" citStr="Dojchinova and Mihov (2004)" startWordPosition="1065" endWordPosition="1068">.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features. Similarly, Smith et al. (2005) decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into a single CRF model; this yielded competitive results for Arabic, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some rese</context>
<context position="27741" citStr="Dojchinova and Mihov (2004)" startWordPosition="4554" endWordPosition="4558">osed by the lexicon is filtered by the contextual linguistic rules. Line 5 is like line 4, but the linguistic rules filtering is only applied at test time; it is not done on training. Lines 6 and 7 are similar to lines 3 and 4, respectively, but here the linguistic rules are further applied to limit the set of possible POS tags that can be predicted, i.e., the rules are used as hard constraints. Finally, line 8 is like line 7, but here the beam size is increased to 3. Overall, we improved over almost all previously published results. Our accuracy is second only to the manual rules approach of Dojchinova and Mihov (2004). Note, however, that they used 40 tags only, i.e., their inventory is 17 times smaller than ours. Moreover, they have optimized their tagset specifically to achieve very high POS tagging accuracy by choosing not to attempt to resolve some inherently hard systematic ambiguities, e.g., they do not try to choose between second and third person past singular verbs, whose inflected forms are identical in Bulgarian and hard to distinguish when the subject is not present (Bulgarian is a pro-drop language). In order to compare our results more closely to the smaller tagsets in Table 5, we evaluated o</context>
<context position="31064" citStr="Dojchinova and Mihov, 2004" startWordPosition="5099" endWordPosition="5102">erforming POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Machines 680 92.22 *TnT Hidden Markov Model 680 92.53 (Georgiev et al., 2009) Maximum Entropy 680 90.34 (Simov and Osenova, 2001) Recurrent Neural Network 160 92.87 (Georgiev et al., 2009) Maximum Entropy 95 94.43 (Savkov et al., 2011) SVM + Lexicon + Rules 680 94.65 (Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R) (Simov and Osenova, 2001) Recurrent Neural Network 15 95.17 (Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50 (Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40 Guided Learning 680 95.72 Guided Learning + Lexicon 680 97.83 This work Guided Learning + Lexicon + Rules 680 97.98 Guided Learning + Lexicon + Rules 49 98.85 Guided Learning + Lexicon + Rules 13 99.30 Table 5: Comparison to previous work for Bulgarian. The first four lines report evaluation results for various standard POS tagging tools, which were retrained and evaluated on the BulTreeBank. The following lines report token-level accuracy for previously published work, as compared to our own e</context>
</contexts>
<marker>Dojchinova, Mihov, 2004</marker>
<rawString>Veselka Dojchinova and Stoyan Mihov. 2004. High performance part-of-speech tagging of Bulgarian. In Christoph Bussler and Dieter Fensel, editors, AIMSA, volume 3192 of Lecture Notes in Computer Science, pages 246–255. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Joel Wallenberg</author>
</authors>
<title>Icelandic data driven part of speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics: Short Papers, ACL ’08,</booktitle>
<pages>33--36</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="6275" citStr="Dredze and Wallenberg (2008)" startWordPosition="942" endWordPosition="945">future actions, yielding 97.3% accuracy. For morphologically complex languages, the problem of POS tagging typically includes morphological disambiguation, which yields a much larger number of tags. For example, for Arabic, Habash and Rambow (2005) used support vector machines (SVM), achieving 97.6% accuracy with 139 tags from the Arabic Treebank (Maamouri et al., 2003). For Czech, Hajiˇc et al. (2001) combined a hidden Markov model (HMM) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the Prague Dependency Treebank (Hajiˇc, 1998). For Icelandic, Dredze and Wallenberg (2008) reported 92.1% accuracy with 639 tags developed for the Icelandic frequency lexicon (Pind et al., 1991), they used guided learning and tag decomposition: First, a coarse POS class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features. Similarly, Smith et al. (2005) decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into a s</context>
<context position="12424" citStr="Dredze and Wallenberg, 2008" startWordPosition="1906" endWordPosition="1910">unt form of a masculine noun, with a ratio of 6,437 to 1,592; • adverb vs. neuter singular adjective, with a ratio of 3,858 to 1,753. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further</context>
<context position="30058" citStr="Dredze and Wallenberg, 2008" startWordPosition="4940" endWordPosition="4943">e, however, that (1) we used a large external morphological lexicon for Bulgarian, which yielded about 50% error reduction (without it, our accuracy was 95.72% only), and (2) our train/dev/test sentences are generally shorter, and thus arguably simpler for a POS tagger to analyze: we have 17.4 words per test sentence in the BulTreeBank vs. 23.7 in the Penn Treebank. Our results also compare favorably to the stateof-the-art results for other morphologically complex languages that use large tagsets, e.g., 95.2% for Czech with 1,400+ tags (Hajiˇc et al., 2001), 92.1% for Icelandic with 639 tags (Dredze and Wallenberg, 2008), 97.6% for Arabic with 139 tags (Habash and Rambow, 2005). 8 Error Analysis In this section, we present error analysis with respect to the impact of the POS tagger’s performance on other processing steps in a natural language processing pipeline, such as lemmatization and syntactic dependency parsing. First, we explore the most frequently confused pairs of tags for our best-performing POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Mach</context>
</contexts>
<marker>Dredze, Wallenberg, 2008</marker>
<rawString>Mark Dredze and Joel Wallenberg. 2008. Icelandic data driven part of speech tagging. In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics: Short Papers, ACL ’08, pages 33–36, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgi Georgiev</author>
<author>Preslav Nakov</author>
<author>Petya Osenova</author>
<author>Kiril Simov</author>
</authors>
<title>Cross-lingual adaptation as a baseline: adapting maximum entropy models to Bulgarian.</title>
<date>2009</date>
<booktitle>In Proceedings of the RANLP’09 Workshop on Adaptation of Language Resources and Technology to New Domains, AdaptLRTtoND ’09,</booktitle>
<pages>35--38</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="7345" citStr="Georgiev et al. (2009)" startWordPosition="1103" endWordPosition="1106">plex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into a single CRF model; this yielded competitive results for Arabic, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morphological analyzer (Krushkov, 1997), achieving P=R=95%. 3 Bulgarian Morphology Bulgarian is an In</context>
<context position="30731" citStr="Georgiev et al., 2009" startWordPosition="5046" endWordPosition="5049">, 2005). 8 Error Analysis In this section, we present error analysis with respect to the impact of the POS tagger’s performance on other processing steps in a natural language processing pipeline, such as lemmatization and syntactic dependency parsing. First, we explore the most frequently confused pairs of tags for our best-performing POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Machines 680 92.22 *TnT Hidden Markov Model 680 92.53 (Georgiev et al., 2009) Maximum Entropy 680 90.34 (Simov and Osenova, 2001) Recurrent Neural Network 160 92.87 (Georgiev et al., 2009) Maximum Entropy 95 94.43 (Savkov et al., 2011) SVM + Lexicon + Rules 680 94.65 (Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R) (Simov and Osenova, 2001) Recurrent Neural Network 15 95.17 (Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50 (Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40 Guided Learning 680 95.72 Guided Learning + Lexicon 680 97.83 This work Guided Learning + Lexicon + Rules 680 97.98 Guided Learning + Lexicon + Rules 49 98.85 Guided </context>
</contexts>
<marker>Georgiev, Nakov, Osenova, Simov, 2009</marker>
<rawString>Georgi Georgiev, Preslav Nakov, Petya Osenova, and Kiril Simov. 2009. Cross-lingual adaptation as a baseline: adapting maximum entropy models to Bulgarian. In Proceedings of the RANLP’09 Workshop on Adaptation of Language Resources and Technology to New Domains, AdaptLRTtoND ’09, pages 35–38, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC ’04,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A general POS tagger generator based on support vector machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC ’04, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Graca</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
</authors>
<title>Posterior vs parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22, NIPS ’09,</booktitle>
<pages>664--672</pages>
<editor>In Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I. Williams, and Aron Culotta, editors,</editor>
<publisher>Curran Associates, Inc.,</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<marker>Graca, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando Pereira. 2009. Posterior vs parameter sparsity in latent variable models. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I. Williams, and Aron Culotta, editors, Advances in Neural Information Processing Systems 22, NIPS ’09, pages 664–672. Curran Associates, Inc., Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>573--580</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="5895" citStr="Habash and Rambow (2005)" startWordPosition="881" endWordPosition="884">s token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that searches the space of future actions, yielding 97.3% accuracy. For morphologically complex languages, the problem of POS tagging typically includes morphological disambiguation, which yields a much larger number of tags. For example, for Arabic, Habash and Rambow (2005) used support vector machines (SVM), achieving 97.6% accuracy with 139 tags from the Arabic Treebank (Maamouri et al., 2003). For Czech, Hajiˇc et al. (2001) combined a hidden Markov model (HMM) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the Prague Dependency Treebank (Hajiˇc, 1998). For Icelandic, Dredze and Wallenberg (2008) reported 92.1% accuracy with 639 tags developed for the Icelandic frequency lexicon (Pind et al., 1991), they used guided learning and tag decomposition: First, a coarse POS class is assigned (e.g., noun, verb, adjectiv</context>
<context position="30116" citStr="Habash and Rambow, 2005" startWordPosition="4950" endWordPosition="4953">xicon for Bulgarian, which yielded about 50% error reduction (without it, our accuracy was 95.72% only), and (2) our train/dev/test sentences are generally shorter, and thus arguably simpler for a POS tagger to analyze: we have 17.4 words per test sentence in the BulTreeBank vs. 23.7 in the Penn Treebank. Our results also compare favorably to the stateof-the-art results for other morphologically complex languages that use large tagsets, e.g., 95.2% for Czech with 1,400+ tags (Hajiˇc et al., 2001), 92.1% for Icelandic with 639 tags (Dredze and Wallenberg, 2008), 97.6% for Arabic with 139 tags (Habash and Rambow, 2005). 8 Error Analysis In this section, we present error analysis with respect to the impact of the POS tagger’s performance on other processing steps in a natural language processing pipeline, such as lemmatization and syntactic dependency parsing. First, we explore the most frequently confused pairs of tags for our best-performing POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Machines 680 92.22 *TnT Hidden Markov Model 680 92.53 (Georgie</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 573–580, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
<author>Karel Oliva</author>
<author>Vladim´ır Petkeviˇc</author>
</authors>
<title>Serial combination of rules and statistics: A case study in Czech tagging.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>268--275</pages>
<location>Toulouse, France.</location>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petkeviˇc, 2001</marker>
<rawString>Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva, and Vladim´ır Petkeviˇc. 2001. Serial combination of rules and statistics: A case study in Czech tagging. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, ACL ’01, pages 268–275, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a Syntactically Annotated Corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a,</booktitle>
<pages>pages</pages>
<editor>In Eva Hajiˇcov´a, editor,</editor>
<publisher>Charles University Press.</publisher>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In Eva Hajiˇcov´a, editor, Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a, pages 12– 19. Prague Karolinum, Charles University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
<author>Julia S Trushkina</author>
</authors>
<title>Forging agreement: Morphological disambiguation of noun phrases.</title>
<date>2004</date>
<journal>Research on Language &amp; Computation,</journal>
<pages>2--621</pages>
<contexts>
<context position="13473" citStr="Hinrichs and Trushkina (2004)" startWordPosition="2088" endWordPosition="2092">cal features as in (Shen et al., 2007). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an example of a rule: If a wordform is ambiguous between a masculine count noun (Ncmt) and a singular short definite masculine noun (Ncmsh), the Ncmt tag should be chosen if the previous token is a numeral or a number. The 70 rules were developed by linguists based on observations over the training dataset only. They target primarily the most frequent cases of ambiguity, and to a lesser extent some infrequent but very problematic cases. Some rules operate over classes of words, while other refer t</context>
</contexts>
<marker>Hinrichs, Trushkina, 2004</marker>
<rawString>Erhard W. Hinrichs and Julia S. Trushkina. 2004. Forging agreement: Morphological disambiguation of noun phrases. Research on Language &amp; Computation, 2:621–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
<author>Eric Atwell</author>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
</authors>
<title>The Tagged LOB Corpus: Users’ manual. ICAME, The Norwegian Computing Centre for the Humanities,</title>
<date>1986</date>
<location>Bergen University,</location>
<contexts>
<context position="2693" citStr="Johansson et al., 1986" startWordPosition="398" endWordPosition="401"> to the desire to encode morphological information as part of the tags. For example, there are six tags for verbs in the Penn Treebank: VB (verb, base form; e.g., sing), VBD (verb, past tense; e.g., sang), VBG (verb, gerund or present participle; e.g., singing), VBN (verb, past participle; e.g., sung) VBP (verb, non3rd person singular present; e.g., sing), and VBZ (verb, 3rd person singular present; e.g., sings); these tags are morpho-syntactic in nature. Other corpora have used even larger tagsets, e.g., the Brown corpus (Kuˇcera and Francis, 1967) and the Lancaster-Oslo/Bergen (LOB) corpus (Johansson et al., 1986) use 87 and 135 tags, respectively. POS tagging poses major challenges for morphologically complex languages, whose tagsets encode a lot of additional morpho-syntactic features (for most of the basic POS categories), e.g., gender, number, person, etc. For example, the BulTreeBank (Simov et al., 2004) for Bulgarian uses 680 tags, while the Prague Dependency Treebank (Hajiˇc, 1998) for Czech has over 1,400 tags. Below we present experiments with POS tagging for Bulgarian, which is an inflectional language with rich morphology. Unlike most previous work, which has used a reduced set of POS tags, </context>
</contexts>
<marker>Johansson, Atwell, Garside, Leech, 1986</marker>
<rawString>Stig Johansson, Eric Atwell, Roger Garside, and Geoffrey Leech, 1986. The Tagged LOB Corpus: Users’ manual. ICAME, The Norwegian Computing Centre for the Humanities, Bergen University, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Krushkov</author>
</authors>
<title>Modelling and building machine dictionaries and morphological processors (in Bulgarian).</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Plovdiv, Faculty of Mathematics and Informatics,</institution>
<location>Plovdiv, Bulgaria.</location>
<contexts>
<context position="7883" citStr="Krushkov, 1997" startWordPosition="1193" endWordPosition="1194">cy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morphological analyzer (Krushkov, 1997), achieving P=R=95%. 3 Bulgarian Morphology Bulgarian is an Indo-European language from the Slavic language group, written with the Cyrillic alphabet and spoken by about 9-12 million people. It is also a member of the Balkan Sprachbund and thus differs from most other Slavic languages: it has no case declensions, uses a suffixed definite article (which has a short and a long form for singular masculine), and lacks verb infinitive forms. It further uses special evidential verb forms to express unwitnessed, retold, and doubtful activities. Bulgarian is an inflective language with very rich morph</context>
</contexts>
<marker>Krushkov, 1997</marker>
<rawString>Hristo Krushkov. 1997. Modelling and building machine dictionaries and morphological processors (in Bulgarian). Ph.D. thesis, University of Plovdiv, Faculty of Mathematics and Informatics, Plovdiv, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kuˇcera</author>
<author>Winthrop Nelson Francis</author>
</authors>
<title>Computational analysis of present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<marker>Kuˇcera, Francis, 1967</marker>
<rawString>Henry Kuˇcera and Winthrop Nelson Francis. 1967. Computational analysis of present-day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4599" citStr="Lafferty et al. (2001)" startWordPosition="695" endWordPosition="698">the Association for Computational Linguistics, pages 492–502, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics 2 Related Work Most research on part-of-speech tagging has focused on English, and has relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Hubert Jin</author>
<author>Tim Buckwalter</author>
</authors>
<date>2003</date>
<journal>Arabic Treebank: Part</journal>
<volume>1</volume>
<pages>2003--06</pages>
<contexts>
<context position="6019" citStr="Maamouri et al., 2003" startWordPosition="900" endWordPosition="903">, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that searches the space of future actions, yielding 97.3% accuracy. For morphologically complex languages, the problem of POS tagging typically includes morphological disambiguation, which yields a much larger number of tags. For example, for Arabic, Habash and Rambow (2005) used support vector machines (SVM), achieving 97.6% accuracy with 139 tags from the Arabic Treebank (Maamouri et al., 2003). For Czech, Hajiˇc et al. (2001) combined a hidden Markov model (HMM) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the Prague Dependency Treebank (Hajiˇc, 1998). For Icelandic, Dredze and Wallenberg (2008) reported 92.1% accuracy with 639 tags developed for the Icelandic frequency lexicon (Pind et al., 1991), they used guided learning and tag decomposition: First, a coarse POS class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed t</context>
</contexts>
<marker>Maamouri, Bies, Jin, Buckwalter, 2003</marker>
<rawString>Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim Buckwalter. 2003. Arabic Treebank: Part 1 v 2.0. LDC2003T06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<pages>19--313</pages>
<contexts>
<context position="1777" citStr="Marcus et al., 1993" startWordPosition="253" endWordPosition="256">his is not trivial since words can play different syntactic roles in different contexts, e.g., can is a noun in “I opened a can of coke.” but a verb in “I can write.” Traditionally, linguists have classified English words into the following eight basic POS categories: noun, pronoun, adjective, verb, adverb, preposition, conjunction, and interjection; this list is often extended a bit, e.g., with determiners, particles, participles, etc., but the number of categories considered is rarely more than 15. Computational linguistics works with a larger inventory of POS tags, e.g., the Penn Treebank (Marcus et al., 1993) uses 48 tags: 36 for partof-speech, and 12 for punctuation and currency symbols. This increase in the number of tags is partially due to finer granularity, e.g., there are special tags for determiners, particles, modal verbs, cardinal numbers, foreign words, existential there, etc., but also to the desire to encode morphological information as part of the tags. For example, there are six tags for verbs in the Penn Treebank: VB (verb, base form; e.g., sing), VBD (verb, past tense; e.g., sang), VBG (verb, gerund or present participle; e.g., singing), VBN (verb, past participle; e.g., sung) VBP </context>
<context position="4261" citStr="Marcus et al., 1993" startWordPosition="645" endWordPosition="648">roduces our approach, Section 5 describes the datasets, Section 6 presents our experiments in detail, Section 7 discusses the results, Section 8 offers application-specific error analysis, and Section 9 concludes and points to some promising directions for future work. 492 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492–502, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics 2 Related Work Most research on part-of-speech tagging has focused on English, and has relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception dis</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Comput. Linguist., 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="32447" citStr="Nivre et al., 2007" startWordPosition="5324" endWordPosition="5327">rb, N for noun, etc. This means that most errors refer to the morphosyntactic features. For example, personal or impersonal verb; definite or indefinite feminine noun; singular or plural masculine adjective, etc. At the same time, there are also cases, where the error has to do with the part-of-speech label itself. For example, between an adjective and an adverb, or between a numeral and an indefinite pronoun. We want to use the above tagger to develop (1) a rule-based lemmatizer, using the morphological lexicon, e.g., as in (Plisson et al., 2004), and (2) a dependency parser like MaltParser (Nivre et al., 2007), trained on the dependency part of the BulTreeBank. We thus study the potential impact of wrong tags on the performance of these tools. The lemmatizer relies on the lexicon and uses string transformation functions defined via two operations – remove and concatenate: if tag= Tag then {remove OldEnd; concatenate NewEnd} where Tag is the tag of the wordform, OldEnd is the string that has to be removed from the end of the wordform, and NewEnd is the string that has to be concatenated to the beginning of the wordform in order to produce the lemma. Here is an example of such a rule: if tag = Vpitf-</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨orgen Pind</author>
<author>Fridrik Magn´usson</author>
<author>Stef´an Briem</author>
</authors>
<title>The Icelandic frequency dictionary.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>The Institute of Lexicography, University of Iceland, Reykjavik, Iceland.</institution>
<marker>Pind, Magn´usson, Briem, 1991</marker>
<rawString>J¨orgen Pind, Fridrik Magn´usson, and Stef´an Briem. 1991. The Icelandic frequency dictionary. Technical report, The Institute of Lexicography, University of Iceland, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin L Plackett</author>
</authors>
<title>Karl Pearson and the ChiSquared Test.</title>
<date>1983</date>
<journal>International Statistical Review /Revue Internationale de Statistique,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="25880" citStr="Plackett, 1983" startWordPosition="4218" endWordPosition="4219">der, 2002), implementing the memory-based model of Daelemans et al. (1996). The following lines report the token-level accuracy reported in previous work, as compared to our own experiments using guided learning. We can see that we outperform by a very large margin (92.53% vs. 97.98%, which represents 73% error reduction) the systems from the first four lines, which are directly comparable to our experiments: they are trained and evaluated on the BulTreeBank using the full inventory of 680 tags. We further achieved statistically significant improvement (p &lt; 0.0001; Pearson’s chi-squared test (Plackett, 1983)) over the best pervious result on 680 tags: from 94.65% to 97.98%, which represents 62.24% error reduction at the token-level. 2We used the pre-trained TreeTagger; for the rest, we report the accuracy given on the Webpage of the BulTreeBank: www.bultreebank.org/taggers/taggers.html 497 # Lexicon Linguistic Rules (applied to filter): Beam Accuracy (%) (source of) (a) the lexicon features (b) the output tags size Sentence-level Token-level 1 – – – 1 52.95 95.72 2 – – yes 1 64.50 97.20 3 features – – 1 70.40 97.83 4 features yes – 1 70.30 97.80 5 features yes, for test only – 1 70.40 97.84 6 fea</context>
</contexts>
<marker>Plackett, 1983</marker>
<rawString>Robin L. Plackett. 1983. Karl Pearson and the ChiSquared Test. International Statistical Review /Revue Internationale de Statistique, 51(1):59–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nada Lavraˇc Jo¨el Plisson</author>
<author>Dunja Mladeni´c</author>
</authors>
<title>A rule based approach to word lemmatization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th International Multiconference: Information Society, IS ’2004,</booktitle>
<pages>83--86</pages>
<location>Ljubljana, Slovenia.</location>
<marker>Jo¨el Plisson, Mladeni´c, 2004</marker>
<rawString>Jo¨el Plisson, Nada Lavraˇc, and Dunja Mladeni´c. 2004. A rule based approach to word lemmatization. In Proceedings of the 7th International Multiconference: Information Society, IS ’2004, pages 83–86, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Popov</author>
<author>Kiril Simov</author>
<author>Svetlomira Vidinska</author>
</authors>
<date>1998</date>
<booktitle>Dictionary of Writing, Pronunciation and Punctuation of Bulgarian Language (in Bulgarian).</booktitle>
<location>Atlantis KL, Sofia, Bulgaria.</location>
<contexts>
<context position="17008" citStr="Popov et al., 1998" startWordPosition="2688" endWordPosition="2691">n in the training dataset. Moreover, 32% of the word types in the development dataset and 31% of those in the testing dataset do not occur in the training dataset. Thus, data sparseness is an issue at two levels: word-level and tag-level. Dataset Sentences Tokens Types Tags Train 16,532 253,526 38,659 552 Dev 2,007 32,995 9,635 425 Test 2,017 35,021 9,627 435 Table 2: Statistics about our datasets. 5.2 Morphological Lexicon In order to alleviate the data sparseness issues, we further used a large morphological lexicon for Bulgarian, which is an extended version of the dictionary described in (Popov et al., 1998) and (Popov et al., 2003). It contains over 1.5M inflected wordforms (for 110K lemmata and 40K proper names), each mapped to a set of possible morpho-syntactic tags. 495 6 Experiments and Evaluation State-of-the-art POS taggers for English typically build a lexicon containing all tags a word type has taken in the training dataset; this lexicon is then used to limit the set of possible tags that an input token can be assigned, i.e., it imposes a hard constraint on the possibilities explored by the POS tagger. For example, if can has only been tagged as a verb and as a noun in the training datas</context>
</contexts>
<marker>Popov, Simov, Vidinska, 1998</marker>
<rawString>Dimitar Popov, Kiril Simov, and Svetlomira Vidinska. 1998. Dictionary of Writing, Pronunciation and Punctuation of Bulgarian Language (in Bulgarian). Atlantis KL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimityr Popov</author>
</authors>
<title>Kiril Simov, Svetlomira Vidinska, and Petya Osenova.</title>
<date>2003</date>
<location>Sofia, Bulgaria.</location>
<marker>Popov, 2003</marker>
<rawString>Dimityr Popov, Kiril Simov, Svetlomira Vidinska, and Petya Osenova. 2003. Spelling Dictionary of Bulgarian. Nauka i izkustvo, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Eva Ejerhed and Ido Dagan, editors, Fourth Workshop on Very Large Corpora,</booktitle>
<pages>133--142</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="4687" citStr="Ratnaparkhi (1996)" startWordPosition="707" endWordPosition="708"> 2012. c�2012 Association for Computational Linguistics 2 Related Work Most research on part-of-speech tagging has focused on English, and has relied on the Penn Treebank (Marcus et al., 1993) and its tagset for training and evaluation. The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classifi</context>
<context position="12602" citStr="Ratnaparkhi (1996)" startWordPosition="1941" endWordPosition="1942">count when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Eva Ejerhed and Ido Dagan, editors, Fourth Workshop on Very Large Corpora, pages 133–142, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleksandar Savkov</author>
</authors>
<title>Laska Laskova, Petya Osenova, Kiril Simov, and Stanislava Kancheva.</title>
<date>2011</date>
<booktitle>In Daniela Majchr´akov´a and Radovan Garabik, editors, Slovko 2011. Sixth International Conference. Natural Language Processing, Multilinguality,</booktitle>
<pages>126--137</pages>
<location>Modra/Bratislava, Slovakia.</location>
<marker>Savkov, 2011</marker>
<rawString>Aleksandar Savkov, Laska Laskova, Petya Osenova, Kiril Simov, and Stanislava Kancheva. 2011. A web-based morphological tagger for Bulgarian. In Daniela Majchr´akov´a and Radovan Garabik, editors, Slovko 2011. Sixth International Conference. Natural Language Processing, Multilinguality, pages 126–137, Modra/Bratislava, Slovakia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="25081" citStr="Schmid, 1994" startWordPosition="4096" endWordPosition="4097">ment from 97.83% to 97.91% and from 97.80% to 97.93% at the token-level, and about 1% absolute at the sentence-level. 6.4 Increasing the Beam Size Finally, we increased the beam size of guided learning from 1 to 3 as in (Shen et al., 2007). Comparing line 7 to line 8 in Table 4, we can see that this yields further token-level improvement: from 97.93% to 97.98%. 7 Discussion Table 5 compares our results to previously reported evaluation results for Bulgarian. The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gim´enez and M`arquez, 2004), which is based on support vector machines, and ACOPOST (Schr¨oder, 2002), implementing the memory-based model of Daelemans et al. (1996). The following lines report the token-level accuracy reported in previous work, as compared to our own experiments using guided learning. We can see that we outperform by a very large margin (92.53% vs. 97.98%, which represents 73% error reduction) the systems from the first four lines, which are directly comparable to our experiments: the</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Schr¨oder</author>
</authors>
<title>A case study in part-of-speechtagging using the ICOPOST toolkit.</title>
<date>2002</date>
<tech>Technical Report FBI-HH-M-314/02,</tech>
<institution>Department of Computer Science, University of Hamburg.</institution>
<marker>Schr¨oder, 2002</marker>
<rawString>Ingo Schr¨oder. 2002. A case study in part-of-speechtagging using the ICOPOST toolkit. Technical Report FBI-HH-M-314/02, Department of Computer Science, University of Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>760--767</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5105" citStr="Shen et al. (2007)" startWordPosition="767" endWordPosition="770">er of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that searches the space of future actions, yielding 97.3% accuracy. For morphological</context>
<context position="12244" citStr="Shen et al., 2007" startWordPosition="1880" endWordPosition="1883"> auxiliary verb (‘to be’) vs. particle (‘well’) vs. interjection (‘wow’), with a ratio of 9,136 to 21 to 5; • singular masculine noun with a short definite article vs. count form of a masculine noun, with a ratio of 6,437 to 1,592; • adverb vs. neuter singular adjective, with a ratio of 3,858 to 1,753. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexi</context>
<context position="24707" citStr="Shen et al., 2007" startWordPosition="4034" endWordPosition="4037"> good idea. Comparing line 1 to line 2, which do not use the morphological lexicon, we can see very significant improvements: from 95.72% to 97.20% at the token-level and from 52.95% to 64.50% at the sentence-level. The improvements are smaller but still consistent when the morphological lexicon is used: comparing lines 3 and 4 to lines 6 and 7, respectively, we see an improvement from 97.83% to 97.91% and from 97.80% to 97.93% at the token-level, and about 1% absolute at the sentence-level. 6.4 Increasing the Beam Size Finally, we increased the beam size of guided learning from 1 to 3 as in (Shen et al., 2007). Comparing line 7 to line 8 in Table 4, we can see that this yields further token-level improvement: from 97.93% to 97.98%. 7 Discussion Table 5 compares our results to previously reported evaluation results for Bulgarian. The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gim´enez and M`arquez, 2004), which is based on support vector machines, and ACOPOST (Schr¨oder, 2002), implementing the memory-based </context>
<context position="28958" citStr="Shen et al., 2007" startWordPosition="4758" endWordPosition="4761">our best model with respect to (a) the first letter of the tag only (which is part-of-speech only, no morphological information; 13 tags), e.g., Ncmsf becomes N, and (b) the first two letters of the tag (POS + limited morphological information; 49 tags), e.g., Ncmsf becomes Nc. This yielded 99.30% accuracy for (a) and 98.85% for (b). The latter improves over (Dojchinova and Mihov, 2004), while using a bit larger number of tags. Our best token-level accuracy of 97.98% is comparable and even slightly better than the stateof-the-art results for English: 97.33% when using Penn Treebank data only (Shen et al., 2007), and 97.50% for Penn Treebank plus some additional unlabeled data (Søgaard, 2011). Of course, our results are only indirectly comparable to English. Still, our performance is impressive because (1) our model is trained on 253,526 tokens only while the standard training sections 0-18 of the Penn Treebank contain a total of 912,344 tokens, i.e., almost four times more, and (2) we predict 680 rather than just 48 tags as for the Penn Treebank, which is 14 times more. Note, however, that (1) we used a large external morphological lexicon for Bulgarian, which yielded about 50% error reduction (with</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 760–767, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Simov</author>
<author>Petya Osenova</author>
</authors>
<title>A hybrid system for morphosyntactic disambiguation in Bulgarian.</title>
<date>2001</date>
<booktitle>In Proceedings of the EuroConference on Recent Advances in Natural Language Processing, RANLP ’01,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7559" citStr="Simov and Osenova (2001)" startWordPosition="1138" endWordPosition="1141">, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morphological analyzer (Krushkov, 1997), achieving P=R=95%. 3 Bulgarian Morphology Bulgarian is an Indo-European language from the Slavic language group, written with the Cyrillic alphabet and spoken by about 9-12 million people. It is also a member of the Balkan Sprachbund and thus differs from most other Slavic </context>
<context position="30783" citStr="Simov and Osenova, 2001" startWordPosition="5054" endWordPosition="5057">ent error analysis with respect to the impact of the POS tagger’s performance on other processing steps in a natural language processing pipeline, such as lemmatization and syntactic dependency parsing. First, we explore the most frequently confused pairs of tags for our best-performing POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Machines 680 92.22 *TnT Hidden Markov Model 680 92.53 (Georgiev et al., 2009) Maximum Entropy 680 90.34 (Simov and Osenova, 2001) Recurrent Neural Network 160 92.87 (Georgiev et al., 2009) Maximum Entropy 95 94.43 (Savkov et al., 2011) SVM + Lexicon + Rules 680 94.65 (Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R) (Simov and Osenova, 2001) Recurrent Neural Network 15 95.17 (Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50 (Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40 Guided Learning 680 95.72 Guided Learning + Lexicon 680 97.83 This work Guided Learning + Lexicon + Rules 680 97.98 Guided Learning + Lexicon + Rules 49 98.85 Guided Learning + Lexicon + Rules 13 99.30 Table 5: Compari</context>
<context position="37601" citStr="Simov and Osenova, 2001" startWordPosition="6182" endWordPosition="6185">ge morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, we achieved accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian. Our token-level accuracy is also comparable to the best results reported for English. In future work, we want to experiment with a richer set of features, e.g., derived from unlabeled data (Søgaard, 2011) or from the Web (UmanskyPesin et al., 2010; Bansal and Klein, 2011). We further plan to explore ways to decompose the complex Bulgarian morpho-syntactic tags, e.g., as proposed in (Simov and Osenova, 2001) and (Smith et al., 2005). Modeling long-distance syntactic dependencies (Dredze and Wallenberg, 2008) is another promising direction; we believe this can be implemented efficiently using posterior regularization (Graca et al., 2009) or expectation constraints (Bellare et al., 2009). Acknowledgments We would like to thank the anonymous reviewers for their useful comments, which have helped us improve the paper. The research presented above has been partially supported by the EU FP7 project 231720 EuroMatrixPlus, and by the SmartBook project, funded by the Bulgarian National Science Fund under </context>
</contexts>
<marker>Simov, Osenova, 2001</marker>
<rawString>Kiril Simov and Petya Osenova. 2001. A hybrid system for morphosyntactic disambiguation in Bulgarian. In Proceedings of the EuroConference on Recent Advances in Natural Language Processing, RANLP ’01, pages 5–7, Tzigov chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Simov</author>
<author>Petya Osenova</author>
</authors>
<title>BTB-TR04: BulTreeBank morphosyntactic annotation of Bulgarian texts.</title>
<date>2004</date>
<tech>Technical Report BTB-TR04,</tech>
<institution>Bulgarian Academy of Sciences.</institution>
<contexts>
<context position="15891" citStr="Simov and Osenova, 2004" startWordPosition="2501" endWordPosition="2504">tag in our morphological lexicon. This number is reduced to 18.5% after the cascaded application of the 70 linguistic rules. Table 1 illustrates the effect of the rules on a small sentence fragment. In this example, the rules have left only one tag (the correct one) for three of the ambiguous words. Since the rules in essence decrease the average number of tags per token, we calculated that the lexicon suggests 1.6 tags per token on average, and after the application of the rules this number decreases to 1.44 per token. 5 Datasets 5.1 BulTreeBank We used the latest version of the BulTreeBank (Simov and Osenova, 2004), which contains 20,556 sentences and 321,542 word tokens (four times less than the English Penn Treebank), annotated using a total of 680 unique morpho-syntactic tags. See (Simov et al., 2004) for a detailed description of the BulTreeBank tagset. We split the data into training/development/test as shown in Table 2. Note that only 552 of all 680 tag types were used in the training dataset, and the development and the test datasets combined contain a total of 128 new tag types that were not seen in the training dataset. Moreover, 32% of the word types in the development dataset and 31% of those</context>
</contexts>
<marker>Simov, Osenova, 2004</marker>
<rawString>Kiril Simov and Petya Osenova. 2004. BTB-TR04: BulTreeBank morphosyntactic annotation of Bulgarian texts. Technical Report BTB-TR04, Bulgarian Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Ivanov Simov</author>
<author>Alexander Simov</author>
<author>Milen Kouylekov</author>
<author>Krasimira Ivanova</author>
<author>Ilko Grigorov</author>
<author>Hristo Ganev</author>
</authors>
<title>Development of corpora within the CLaRK system: The BulTreeBank project experience.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics, EACL ’03,</booktitle>
<pages>243--246</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="13561" citStr="Simov et al., 2003" startWordPosition="2106" endWordPosition="2109"> 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an example of a rule: If a wordform is ambiguous between a masculine count noun (Ncmt) and a singular short definite masculine noun (Ncmsh), the Ncmt tag should be chosen if the previous token is a numeral or a number. The 70 rules were developed by linguists based on observations over the training dataset only. They target primarily the most frequent cases of ambiguity, and to a lesser extent some infrequent but very problematic cases. Some rules operate over classes of words, while other refer to particular wordforms. The rules were designed to be 100% accurate on our training data</context>
</contexts>
<marker>Simov, Simov, Kouylekov, Ivanova, Grigorov, Ganev, 2003</marker>
<rawString>Kiril Ivanov Simov, Alexander Simov, Milen Kouylekov, Krasimira Ivanova, Ilko Grigorov, and Hristo Ganev. 2003. Development of corpora within the CLaRK system: The BulTreeBank project experience. In Proceedings of the 10th conference of the European chapter of the Association for Computational Linguistics, EACL ’03, pages 243–246, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Simov</author>
<author>Petya Osenova</author>
<author>Milena Slavcheva</author>
</authors>
<title>BTB-TR03: BulTreeBank morphosyntactic tagset.</title>
<date>2004</date>
<tech>Technical Report BTB-TR03,</tech>
<institution>Bulgarian Academy of Sciences.</institution>
<contexts>
<context position="2994" citStr="Simov et al., 2004" startWordPosition="444" endWordPosition="447">P (verb, non3rd person singular present; e.g., sing), and VBZ (verb, 3rd person singular present; e.g., sings); these tags are morpho-syntactic in nature. Other corpora have used even larger tagsets, e.g., the Brown corpus (Kuˇcera and Francis, 1967) and the Lancaster-Oslo/Bergen (LOB) corpus (Johansson et al., 1986) use 87 and 135 tags, respectively. POS tagging poses major challenges for morphologically complex languages, whose tagsets encode a lot of additional morpho-syntactic features (for most of the basic POS categories), e.g., gender, number, person, etc. For example, the BulTreeBank (Simov et al., 2004) for Bulgarian uses 680 tags, while the Prague Dependency Treebank (Hajiˇc, 1998) for Czech has over 1,400 tags. Below we present experiments with POS tagging for Bulgarian, which is an inflectional language with rich morphology. Unlike most previous work, which has used a reduced set of POS tags, we use all 680 tags in the BulTreeBank. We combine prior linguistic knowledge and statistical learning, achieving accuracy comparable to that reported for state-of-the-art systems for English. The remainder of the paper is organized as follows: Section 2 provides an overview of related work, Section </context>
<context position="7412" citStr="Simov et al., 2004" startWordPosition="1114" endWordPosition="1117">nder, number, case, and lemma are estimated separately, and then composed into a single CRF model; this yielded competitive results for Arabic, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morphological analyzer (Krushkov, 1997), achieving P=R=95%. 3 Bulgarian Morphology Bulgarian is an Indo-European language from the Slavic language group, written with t</context>
<context position="16084" citStr="Simov et al., 2004" startWordPosition="2532" endWordPosition="2535"> In this example, the rules have left only one tag (the correct one) for three of the ambiguous words. Since the rules in essence decrease the average number of tags per token, we calculated that the lexicon suggests 1.6 tags per token on average, and after the application of the rules this number decreases to 1.44 per token. 5 Datasets 5.1 BulTreeBank We used the latest version of the BulTreeBank (Simov and Osenova, 2004), which contains 20,556 sentences and 321,542 word tokens (four times less than the English Penn Treebank), annotated using a total of 680 unique morpho-syntactic tags. See (Simov et al., 2004) for a detailed description of the BulTreeBank tagset. We split the data into training/development/test as shown in Table 2. Note that only 552 of all 680 tag types were used in the training dataset, and the development and the test datasets combined contain a total of 128 new tag types that were not seen in the training dataset. Moreover, 32% of the word types in the development dataset and 31% of those in the testing dataset do not occur in the training dataset. Thus, data sparseness is an issue at two levels: word-level and tag-level. Dataset Sentences Tokens Types Tags Train 16,532 253,526</context>
</contexts>
<marker>Simov, Osenova, Slavcheva, 2004</marker>
<rawString>Kiril Simov, Petya Osenova, and Milena Slavcheva. 2004. BTB-TR03: BulTreeBank morphosyntactic tagset. Technical Report BTB-TR03, Bulgarian Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--482</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="6704" citStr="Smith et al. (2005)" startWordPosition="1004" endWordPosition="1007">HMM) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the Prague Dependency Treebank (Hajiˇc, 1998). For Icelandic, Dredze and Wallenberg (2008) reported 92.1% accuracy with 639 tags developed for the Icelandic frequency lexicon (Pind et al., 1991), they used guided learning and tag decomposition: First, a coarse POS class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features. Similarly, Smith et al. (2005) decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into a single CRF model; this yielded competitive results for Arabic, Korean, and Czech. Most previous work on Bulgarian POS tagging has started with large tagsets, which were then reduced. For example, Dojchinova and Mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic</context>
<context position="37626" citStr="Smith et al., 2005" startWordPosition="6187" endWordPosition="6190">prior linguistic knowledge and guided learning from a POS-annotated corpus, we achieved accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian. Our token-level accuracy is also comparable to the best results reported for English. In future work, we want to experiment with a richer set of features, e.g., derived from unlabeled data (Søgaard, 2011) or from the Web (UmanskyPesin et al., 2010; Bansal and Klein, 2011). We further plan to explore ways to decompose the complex Bulgarian morpho-syntactic tags, e.g., as proposed in (Simov and Osenova, 2001) and (Smith et al., 2005). Modeling long-distance syntactic dependencies (Dredze and Wallenberg, 2008) is another promising direction; we believe this can be implemented efficiently using posterior regularization (Graca et al., 2009) or expectation constraints (Bellare et al., 2009). Acknowledgments We would like to thank the anonymous reviewers for their useful comments, which have helped us improve the paper. The research presented above has been partially supported by the EU FP7 project 231720 EuroMatrixPlus, and by the SmartBook project, funded by the Bulgarian National Science Fund under grant D002-111/15.12.2008</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 475–482, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semi-supervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, ACL-HLT ’10,</booktitle>
<pages>48--52</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="29040" citStr="Søgaard, 2011" startWordPosition="4772" endWordPosition="4773">speech only, no morphological information; 13 tags), e.g., Ncmsf becomes N, and (b) the first two letters of the tag (POS + limited morphological information; 49 tags), e.g., Ncmsf becomes Nc. This yielded 99.30% accuracy for (a) and 98.85% for (b). The latter improves over (Dojchinova and Mihov, 2004), while using a bit larger number of tags. Our best token-level accuracy of 97.98% is comparable and even slightly better than the stateof-the-art results for English: 97.33% when using Penn Treebank data only (Shen et al., 2007), and 97.50% for Penn Treebank plus some additional unlabeled data (Søgaard, 2011). Of course, our results are only indirectly comparable to English. Still, our performance is impressive because (1) our model is trained on 253,526 tokens only while the standard training sections 0-18 of the Penn Treebank contain a total of 912,344 tokens, i.e., almost four times more, and (2) we predict 680 rather than just 48 tags as for the Penn Treebank, which is 14 times more. Note, however, that (1) we used a large external morphological lexicon for Bulgarian, which yielded about 50% error reduction (without it, our accuracy was 95.72% only), and (2) our train/dev/test sentences are ge</context>
<context position="37395" citStr="Søgaard, 2011" startWordPosition="6150" endWordPosition="6151">nlike most previous work for this language, which has limited the number of possible tags, we used a very rich tagset of 680 morpho-syntactic tags as defined in the BulTreeBank. By combining a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, we achieved accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian. Our token-level accuracy is also comparable to the best results reported for English. In future work, we want to experiment with a richer set of features, e.g., derived from unlabeled data (Søgaard, 2011) or from the Web (UmanskyPesin et al., 2010; Bansal and Klein, 2011). We further plan to explore ways to decompose the complex Bulgarian morpho-syntactic tags, e.g., as proposed in (Simov and Osenova, 2001) and (Smith et al., 2005). Modeling long-distance syntactic dependencies (Dredze and Wallenberg, 2008) is another promising direction; we believe this can be implemented efficiently using posterior regularization (Graca et al., 2009) or expectation constraints (Bellare et al., 2009). Acknowledgments We would like to thank the anonymous reviewers for their useful comments, which have helped u</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semi-supervised condensed nearest neighbor for part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, ACL-HLT ’10, pages 48–52, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Shallow language processing architecture for Bulgarian.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics, COLING ’02,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7812" citStr="Tanev and Mitkov (2002)" startWordPosition="1180" endWordPosition="1183">uracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. Similarly, Georgiev et al. (2009), who used maximum entropy and the BulTreeBank (Simov et al., 2004), grouped its 680 fine-grained POS tags into 95 coarsegrained ones, and thus improved their accuracy from 90.34% to 94.4%. Simov and Osenova (2001) used a recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 POS tags (95.2% accuracy). Some researchers did not reduce the tagset: Savkov et al. (2011) used 680 tags (94.7% accuracy), and Tanev and Mitkov (2002) used 303 tags and the BULMORPH morphological analyzer (Krushkov, 1997), achieving P=R=95%. 3 Bulgarian Morphology Bulgarian is an Indo-European language from the Slavic language group, written with the Cyrillic alphabet and spoken by about 9-12 million people. It is also a member of the Balkan Sprachbund and thus differs from most other Slavic languages: it has no case declensions, uses a suffixed definite article (which has a short and a long form for singular masculine), and lacks verb infinitive forms. It further uses special evidential verb forms to express unwitnessed, retold, and doubtf</context>
<context position="30946" citStr="Tanev and Mitkov, 2002" startWordPosition="5082" endWordPosition="5085">tion and syntactic dependency parsing. First, we explore the most frequently confused pairs of tags for our best-performing POS tagging system; these are shown in Table 6. 498 Tool/Authors Method # Tags Accuracy (token-level, %) *TreeTagger Decision Trees 680 89.21 *ACOPOST Memory-based Learning 680 89.91 *SVMtool Support Vector Machines 680 92.22 *TnT Hidden Markov Model 680 92.53 (Georgiev et al., 2009) Maximum Entropy 680 90.34 (Simov and Osenova, 2001) Recurrent Neural Network 160 92.87 (Georgiev et al., 2009) Maximum Entropy 95 94.43 (Savkov et al., 2011) SVM + Lexicon + Rules 680 94.65 (Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R) (Simov and Osenova, 2001) Recurrent Neural Network 15 95.17 (Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50 (Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40 Guided Learning 680 95.72 Guided Learning + Lexicon 680 97.83 This work Guided Learning + Lexicon + Rules 680 97.98 Guided Learning + Lexicon + Rules 49 98.85 Guided Learning + Lexicon + Rules 13 99.30 Table 5: Comparison to previous work for Bulgarian. The first four lines report evaluation results for various standard POS tagging tools, which were retrained and evaluated on th</context>
</contexts>
<marker>Tanev, Mitkov, 2002</marker>
<rawString>Hristo Tanev and Ruslan Mitkov. 2002. Shallow language processing architecture for Bulgarian. In Proceedings of the 19th International Conference on Computational Linguistics, COLING ’02, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’03,</booktitle>
<pages>173--180</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4993" citStr="Toutanova et al. (2003)" startWordPosition="750" endWordPosition="753"> one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al. (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented </context>
<context position="12976" citStr="Toutanova et al., 2003" startWordPosition="2004" endWordPosition="2007">cally complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an ex</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’03, pages 173–180, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT-EMNLP ’05,</booktitle>
<pages>467--474</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="13008" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2009" endWordPosition="2012">as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an example of a rule: If a wordform i</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT-EMNLP ’05, pages 467–474, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Learning with lookahead: Can history-based models rival globally optimized models?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL-HLT ’10,</booktitle>
<pages>238--246</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="5504" citStr="Tsuruoka et al. (2011)" startWordPosition="824" endWordPosition="827">model (97.1% accuracy). All these models fix the order of inference from left to right. Toutanova et al. (2003) introduced a cyclic dependency network (97.2% accuracy), where the search is bi-directional. Shen et al. (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Recently, Tsuruoka et al. (2011), proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that searches the space of future actions, yielding 97.3% accuracy. For morphologically complex languages, the problem of POS tagging typically includes morphological disambiguation, which yields a much larger number of tags. For example, for Arabic, Habash and Rambow (2005) used support vector machines (SVM), achieving 97.6% accuracy with 139 tags from the Arabic Treebank (Maamouri et al., 2003). For Czech, Hajiˇc et al. (2001) combined a hidden Markov model (HMM) with linguisti</context>
</contexts>
<marker>Tsuruoka, Miyao, Kazama, 2011</marker>
<rawString>Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Kazama. 2011. Learning with lookahead: Can history-based models rival globally optimized models? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL-HLT ’10, pages 238–246, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shulamit Umansky-Pesin</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>A multi-domain web-based algorithm for POS tagging of unknown words.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1274--1282</pages>
<location>Beijing, China.</location>
<marker>Umansky-Pesin, Reichart, Rappoport, 2010</marker>
<rawString>Shulamit Umansky-Pesin, Roi Reichart, and Ari Rappoport. 2010. A multi-domain web-based algorithm for POS tagging of unknown words. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1274–1282, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>