<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000232">
<title confidence="0.9980505">
Instance-based Sentence Boundary Determination by Optimization for
Natural Language Generation
</title>
<note confidence="0.551274">
Shimei Pan and James C. Shaw
IBM T. J. Watson Research Center
</note>
<address confidence="0.909424">
19 Skyline Drive
Hawthorne, NY 10532, USA
</address>
<email confidence="0.998642">
{shimei,shawjc}@us.ibm.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903607142857">
This paper describes a novel instance-
based sentence boundary determination
method for natural language generation
that optimizes a set of criteria based on
examples in a corpus. Compared to exist-
ing sentence boundary determination ap-
proaches, our work offers three signifi-
cant contributions. First, our approach
provides a general domain independent
framework that effectively addresses sen-
tence boundary determination by balanc-
ing a comprehensive set of sentence com-
plexity and quality related constraints.
Second, our approach can simulate the
characteristics and the style of naturally
occurring sentences in an application do-
main since our solutions are optimized
based on their similarities to examples
in a corpus. Third, our approach can
adapt easily to suit a natural language gen-
eration system’s capability by balancing
the strengths and weaknesses of its sub-
components (e.g. its aggregation and re-
ferring expression generation capability).
Our final evaluation shows that the pro-
posed method results in significantly bet-
ter sentence generation outcomes than a
widely adopted approach.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999302894736842">
The problem of sentence boundary determination in
natural language generation exists when more than
one sentence is needed to convey multiple concepts
and propositions. In the classic natural language
generation (NLG) architecture (Reiter, 1994), sen-
tence boundary decisions are made during the sen-
tence planning stage in which the syntactic struc-
ture and wording of sentences are decided. Sentence
boundary determination is a complex process that
directly impacts a sentence’s readability (Gunning,
1952), its semantic cohesion, its syntactic and lex-
ical realizability, and its smoothness between sen-
tence transitions. Sentences that are too complex are
hard to understand, so are sentences lacking seman-
tic cohesion and cross-sentence coherence. Further
more, bad sentence boundary decisions may even
make sentences unrealizable.
To design a sentence boundary determination
method that addresses these issues, we employ an
instance-based approach (Varges and Mellish, 2001;
Pan and Shaw, 2004). Because we optimize our so-
lutions based on examples in a corpus, the output
sentences can demonstrate properties, such as simi-
lar sentence length distribution and semantic group-
ing similar to those in the corpus. Our approach
also avoids problematic sentence boundaries by op-
timizing the solutions using all the instances in the
corpus. By taking a sentence’s lexical and syntac-
tic realizability into consideration, it can also avoid
sentence realization failures caused by bad sentence
boundary decisions. Moreover, since our solution
can be adapted easily to suit the capability of a natu-
ral language generator, we can easily tune the algo-
rithm to maximize the generation quality. To the best
of our knowledge, there is no existing comprehen-
sive solution that is domain-independent and pos-
sesses all the above qualities. In summary, our work
offers three significant contributions:
</bodyText>
<listItem confidence="0.722085">
1. It provides a general and flexible sentence
</listItem>
<page confidence="0.968165">
565
</page>
<note confidence="0.9915885">
Proceedings of the 43rd Annual Meeting of the ACL, pages 565–572,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991115">
boundary determination framework which
takes a comprehensive set of sentence com-
plexity and quality related criteria into consid-
eration and ensures that the proposed algorithm
is sensitive to not only the complexity of the
generated sentences, but also their semantic co-
hesion, multi-sentence coherence and syntactic
and lexical realizability.
</bodyText>
<listItem confidence="0.784833714285714">
2. Since we employ an instance-based method,
the proposed solution is sensitive to the style
of the sentences in the application domain in
which the corpus is collected.
3. Our approach can be adjusted easily to suit
a sentence generation system’s capability and
avoid some of its known weaknesses.
</listItem>
<bodyText confidence="0.999948153846154">
Currently, our work is embodied in a multimodal
conversation application in the real-estate domain in
which potential home buyers interact with the sys-
tem using multiple modalities, such as speech and
gesture, to request residential real-estate informa-
tion (Zhou and Pan, 2001; Zhou and Chen, 2003;
Zhou and Aggarwal, 2004). After interpreting the
request, the system formulates a multimedia pre-
sentation, including automatically generated speech
and graphics, as the response (Zhou and Aggarwal,
2004). The proposed sentence boundary determi-
nation module takes a set of propositions selected
by a content planner and passes the sentence bound-
ary decisions to SEGUE (Pan and Shaw, 2004), an
instance-based sentence generator, to formulate the
final sentences. For example, our system is called
upon to generate responses to a user’s request: “Tell
me more about this house.” Even though not all of
the main attributes of a house (more than 20) will be
conveyed, it is clear that a good sentence boundary
determination module can greatly ease the genera-
tion process and improve the quality of the output.
In the rest of the paper, we start with a discussion
of related work, and then describe our instance-base
approach to sentence boundary determination. Fi-
nally, we present our evaluation results.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999910162790698">
Existing approaches to sentence boundary determi-
nation typically employ one of the following strate-
gies. The first strategy uses domain-specific heuris-
tics to decide which propositions can be combined.
For example, Proteus (Davey, 1979; Ritchie, 1984)
produces game descriptions by employing domain-
specific sentence scope heuristics. This approach
can work well for a particular application, however,
it is not readily reusable for new applications.
The second strategy is to employ syntactic, lex-
ical, and sentence complexity constraints to con-
trol the aggregation of multiple propositions (Robin,
1994; Shaw, 1998). These strategies can generate
fluent complex sentences, but they do not take other
criteria into consideration, such as semantic cohe-
sion. Further more, since these approaches do not
employ global optimization as we do, the content of
each sentence might not be distributed evenly. This
may cause dangling sentence problem (Wilkinson,
1995).
Another strategy described in Mann and
Moore(1981) guided the aggregation process by
using an evaluation score that is sensitive to the
structure and term usage of a sentence. Similar to
our approach, they rely on search to find an optimal
solution. The main difference between this approach
and ours is that their evaluation score is computed
based on preference heuristics. For example, all
the semantic groups existing in a domain have to
be coded specifically in order to handle semantic
grouping. In contrast, in our framework, the score is
computed based on a sentence’s similarity to corpus
instances, which takes advantage of the naturally
occurring semantic grouping in the corpus.
Recently, Walker (2002) and Stent (2004) used
statistical features derived from corpus to rank gen-
erated sentence plans. Because the plan ranker was
trained with existing examples, it can choose a plan
that is consistent with the examples. However, de-
pending on the features used and the size of the train-
ing examples, it is unclear how well it can capture
patterns like semantic grouping and avoid problems
likes dangling sentences.
</bodyText>
<sectionHeader confidence="0.997383" genericHeader="method">
3 Examples
</sectionHeader>
<bodyText confidence="0.999986583333333">
Before we describe our approach in detail, we start
with a few examples from the real-estate domain
to demonstrate the properties of the proposed ap-
proach.
First, sentence complexity impacts sentence
boundary determination. As shown in Table 1, af-
ter receiving a user’s request (U1) for the details of a
house, the content planner asked the sentence plan-
ner to describe the house with a set of attributes in-
cluding its asking price, style, number of bedrooms,
number of bathrooms, square footage, garage, lot
size, property tax, and its associated town and school
</bodyText>
<page confidence="0.98945">
566
</page>
<bodyText confidence="0.870239380952381">
Example Turn Sentence
E1 U1 Tell me more about this house
S1 This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial
S2 with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk
S3 and in the Byram Hills school district.
This is a 1 million dollar house. This is a 3 bedroom house. This is a 2 bathroom
house. This house has 2000 square feet. This house has 2 acres of land.
This house has 2 car garage. This is a colonial house. The annual taxes are 8000 dollars.
This house is in Armonk. This house is in the Byram Hills school district.
This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk
with 2 acres of land. The asking price is 1 million dollar and the annual taxes
are 8000 dollars. The house is located in the Byram Hills School District.
E2 S4 This is a 1 million dollar 3 bedroom house. This is a 2 bathroom house with
S5 annual taxes of 8000 dollars.
This is a 3 bedroom and 2 bathroom house. Its price is 1 million dollar and
its annual taxes are 8000 dollars.
E3 S6 The tax rate of the house is 3 percent.
S7 The house has an asphalt roof.
E4 S8 This is a 3 bedroom, 2 bathroom colonial with 2000 square feet and 2 acres of land.
S9 The house has 2 bedrooms and 3 bathrooms. This house is a colonial.
It has 2000 square feet. The house is on 2 acres of land.
</bodyText>
<tableCaption confidence="0.992311">
Table 1: Examples
</tableCaption>
<bodyText confidence="0.99995926923077">
district name. Without proper sentence boundary
determination, a sentence planner may formulate a
single sentence to convey all the information, as in
S1. Even though S1 is grammatically correct, it
is too complex and too exhausting to read. Simi-
larly, output like S2, despite its grammatical correct-
ness, is choppy and too tedious to read. In contrast,
our instance-based sentence boundary determination
module will use examples in a corpus to partition
those attributes into several sentences in a more bal-
anced manner (S3).
Semantic cohesion also influences the quality of
output sentences. For example, in the real-estate
domain, the number of bedrooms and number of
bathrooms are two closely related concepts. Based
on our corpus, when both concepts appear, they al-
most always conveyed together in the same sen-
tence. Given this, if the content planner wants to
convey a house with the following attributes: price,
number of bedrooms, number of bathrooms, and
property tax, S4 is a less desirable solution than S5
because it splits these concepts into two separate
sentences. Since we use instance-based sentence
boundary determination, our method generates S5 to
minimize the difference from the corpus instances.
Sentence boundary placement is also sensitive to
the syntactic and lexical realizability of grouped
items. For example, if the sentence planner asks the
surface realizer to convey two propositions S6 and
S7 together in a sentence, a realization failure will
be triggered because both S6 and S7 only exist in
the corpus as independent sentences. Since neither
of them can be transformed into a modifier based on
the corpus, S6 and S7 cannot be aggregated in our
system. Our method takes a sentence’s lexical and
syntactic realizability into consideration in order to
avoid making such aggregation request to the sur-
face realizer in the first place.
A generation system’s own capability may also
influence sentence boundary determination. Good
sentence boundary decisions will balance a system’s
strengths and weaknesses. In contrast, bad decisions
will expose a system’s venerability. For example, if
a sentence generator is good at performing aggre-
gations and weak on referring expressions, we may
avoid incoherence between sentences by preferring
aggregating more attributes in one sentence (like in
S8) rather than by splitting them into multiple sen-
tences (like in S9).
In the following, we will demonstrate how our ap-
proach can achieve all the above goals in a unified
instance-based framework.
</bodyText>
<sectionHeader confidence="0.990074" genericHeader="method">
4 Instance-based boundary determination
</sectionHeader>
<bodyText confidence="0.9995866">
Instance-based generation automatically creates
sentences that are similar to those generated by hu-
mans, including their way of grouping semantic con-
tent, their wording and their style. Previously, Pan
and Shaw (2004) have demonstrated that instance-
based learning can be applied successfully in gen-
erating new sentences by piecing together existing
words and segments in a corpus. Here, we want to
demonstrate that by applying the same principle, we
can make better sentence boundary decisions.
</bodyText>
<page confidence="0.981721">
567
</page>
<bodyText confidence="0.999939833333333">
The key idea behind the new approach is to find a
sentence boundary solution that minimizes the ex-
pected difference between the sentences resulting
from these boundary decisions and the examples in
the corpus. Here we measure the expected differ-
ence based a set of cost functions.
</bodyText>
<subsectionHeader confidence="0.997541">
4.1 Optimization Criteria
</subsectionHeader>
<bodyText confidence="0.992734826666666">
We use three sentence complexity and quality re-
lated cost functions as the optimization criteria: sen-
tence boundary cost, insertion cost and deletion cost.
Sentence boundary cost (SBC): Assuming P is
a set of propositions to be conveyed and S is a col-
lection of example sentences selected from the cor-
pus to convey P. Then we say P can be realized
by S with a sentence boundary cost that is equal to
(|S |− 1) * SBC in which |S |is the number of sen-
tences and SBC is the sentence boundary cost. To
use a specific example from the real-estate domain,
the input P has three propositions:
p1. House1 has-attr (style=colonial).
p2. House1 has-attr(bedroom=3).
p3. House1 has-attr(bathroom=2).
One solution, S, contains 2 sentences:
s1. This is a 3 bedroom, 2 bathroom house.
s2. This is a colonial house.
Since only one sentence boundary is involved, S is a
solution containing one boundary cost. In the above
example, even though both s1 and s2 are grammati-
cal sentences, the transition from s1 to s2 is not quite
smooth. They sound choppy and disjointed. To pe-
nalize this, whenever there is a sentence break, there
is a SBC. In general, the SBC is a parameter that is
sensitive to a generation system’s capability such as
its competence in reference expression generation.
If a generation system does not have a robust ap-
proach for tracking the focus across sentences, it is
likely to be weak in referring expression generation
and adding sentence boundaries are likely to cause
fluency problems. In contrast, if a generation sys-
tem is very capable in maintaining the coherence be-
tween sentences, the proper sentence boundary cost
would be lower.
Insertion cost: Assume P is the set of propo-
sitions to be conveyed, and CZ is an instance in
the corpus that can be used to realize P by insert-
ing a missing proposition pj to CZ, then we say P
can be realized using CZ with an insertion cost of
icost(CH, pj), in which CH is the host sentence in
the corpus containing proposition pj. Using an ex-
ample from our real-estate domain, assume the input
P=(p2, p3, p4), where
p4. House1 has-attr (square footage=2000).
Assume CZ is a sentence selected from the cor-
pus to realize P: “This is 3 bedroom 2 bathroom
house”. Since CZ does not contain p4, p4 needs to
be added. We say that P can be realized using CZ
by inserting a proposition p4 with an insertion cost
of icost(CH, p4), in which CH is a sentence in the
corpus such as “This is a house with 2000 square
feet.”
The insertion cost is influenced by two main fac-
tors: the syntactic and lexical insertability of the
proposition pj and a system’s capability in aggre-
gating propositions. For example, if in the corpus,
the proposition pj is always realized as an indepen-
dent sentence and never as a modifier, icost(*,pj)
should be extremely high, which effectively pro-
hibit pj from becoming a part of another sen-
tence. icost(*,pj) is defined as the minimum in-
sertion cost among all the icost(CH, pj). Currently
icost(CH, pj) is computed dynamically based on
properties of corpus instances. In addition, since
whether a proposition is insertable depends on how
capable an aggregation module can combine propo-
sitions correctly into a sentence, the insertion cost
should be assigned high or low accordingly.
Deletion cost: Assume P is a set of input proposi-
tions to be conveyed and CZ is an instance in the cor-
pus that can be used to convey P by deleting an un-
needed proposition pj in CZ. Then, we say P can be
realized using CZ with a deletion cost dcost(CZ, pj).
As a specific example, assuming the input is P=(p2,
</bodyText>
<listItem confidence="0.961902428571428">
p3, p4), CZ is an instance in the corpus “This is a
3 bedroom, 2 bathroom, 2000 square foot colonial
house.” In addition to the propositions p2, p3 and
p4, CZ also conveys a proposition p1. Since p1 is
not needed when conveying P, we say that P can be
realized using CZ by deleting proposition p1 with a
deletion cost of dcost(CZ, p1).
</listItem>
<bodyText confidence="0.9997948">
The deletion cost is affected by two main fac-
tors as well: first the syntactic relation between
pj and its host sentence. Given a new instance
CZ, “This 2000 square foot 3 bedroom, 2 bathroom
house is a colonial”, deleting p1, the main object
</bodyText>
<page confidence="0.989806">
568
</page>
<bodyText confidence="0.999764944444444">
of the verb, will make the rest of the sentence in-
complete. As a result, dcost(Ci, p1) is very expen-
sive. In contrast, dcost(Ci, p4) is low because the
resulting sentence is still grammatically sound. Cur-
rently dcost(Ci, pj) is computed dynamically based
on properties of corpus instances. Second, the ex-
pected performance of a generation system in dele-
tion also impacts the deletion cost. Depending on
the sophistication of the generator to handle various
deletion situations, the expected deletion cost can
be high if the method employed is naive and error
prone, or is low if the system can handle most cases
accurately.
Overall cost: Assume P is the set of propositions
to be conveyed and S is the set of instances in the
corpus that are chosen to realize P by applying a set
of insertion, deletion and sentence breaking opera-
tions, the overall cost of the solution
</bodyText>
<equation confidence="0.999876">
Cost(P) = � �(Wi * icost(CHj, pj)
Ci j
+Wd * � dcost(Ci, pk))
k
+(Nb − 1) * SBC
</equation>
<bodyText confidence="0.999762333333333">
in which Wi, Wd and SBC are the insertion weight,
deletion weight and sentence boundary cost; Nb is
the number of sentences in the solution, Ci is a cor-
pus instance been selected to construct the solution
and CHj is the host sentence that proposition pj be-
longs.
</bodyText>
<subsectionHeader confidence="0.6427615">
4.2 Algorithm: Optimization based on overall
cost
</subsectionHeader>
<bodyText confidence="0.977906705882353">
We model the sentence boundary determination pro-
cess as a branch and bound tree search problem. Be-
fore we explain the algorithm itself, first a few no-
tations. The input P is a set of input propositions
chosen by the content planner to be realized. E is
the set of all possible propositions in an application
domain. Each instance Ci in the corpus C is repre-
sented as a subset of E. Assume S is a solution to
P, then it can be represented as the overall cost plus
a list of pairs like (Cis, Ois), in which Cis is one
of the instances selected to be used in that solution,
Ois is a set of deletion, insertion operations that can
be applied to Cis to transform it to a subsolution Si.
To explain this representation further, we use a spe-
cific example in which P=(a, d, e, f), E=(a, b, c, d,
e, f g, h, i). One of the boundary solution S can be
represented as
</bodyText>
<equation confidence="0.9998586">
S = (Cost(S), (S1, S2))
S1 = (C1 = (a, b, d, i), delete(b, i)),
S2 = (C2 = (e), insert(f as in C3 = (f, g)))
Cost(S) = Wd * (dcost(C1, b) + dcost(C1, i)) +
Wi * icost(C3, f) + 1 * SBC
</equation>
<bodyText confidence="0.997232545454546">
in which C1 and C2 are two corpus instances se-
lected as the bases to formulate the solution and C3
is the host sentence containing proposition f.
The general idea behind the instance-based
branch and bound tree search algorithm is that given
an input, P, for each corpus instance Ci, we con-
struct a search branch, representing all possible
ways to realize the input using the instance plus
deletions, insertions and sentence breaks. Since
each sentence break triggers a recursive call to
our sentence boundary determination algorithm, the
complexity of the algorithm is NP-hard. To speed up
the process, for each iteration, we prune unproduc-
tive branches using an upper bound derived by sev-
eral greedy algorithms. The details of our sentence
boundary determination algorithm, sbd(P), are de-
scribed below. P is the set of input propositions.
1. Set the current upper bound, UB, to the mini-
mum cost of solutions derived by greedy algo-
rithms, which we will describe later. This value
is used to prune unneeded branches to make the
search more efficient.
</bodyText>
<listItem confidence="0.8987644">
2. For each instance Ci in corpus C in which (Cifl
P) =� 0, loop from step 3 to 9. The goal here
is to identify all the useful corpus instances for
realizing P.
3. Delete all the propositions pj E D in which
</listItem>
<equation confidence="0.81157975">
D = Ci − P (D contains propositions in Ci
but not exist in P) with cost Costd(P) = Wd *
E
R∈D dcost(Ci, pj). This step computes the
</equation>
<bodyText confidence="0.96435">
deletion operators and their associated costs.
4. Let I = P − Ci (I contains propositions in P
but not in Ci). For each subset Ej C I (Ej in-
cludes 0 and I itself), iterate through step 5 to
9. These steps figure out all the possible ways
to add the missing propositions, including in-
serting into the instance Ci and separating the
rest as independent sentence(s).
</bodyText>
<page confidence="0.993875">
569
</page>
<listItem confidence="0.9971298">
5. Generate a solution in which bpk E Ej, insert
pk to CZ. All the propositions in Q = I − Ej
will be realized in different sentences, thus in-
curring a SBC.
6. We update the cost Cost(P) to
</listItem>
<equation confidence="0.996269333333333">
Costd(P) + WZ * E icost(*, pk)+
pk∈Ej
SBC + Cost(Q)
</equation>
<bodyText confidence="0.999051333333333">
in which Cost(Q) is the cost of sbd(Q) which
recursively computes the best solution for input
Q and Q C P. To facilitate dynamic program-
ming, we remember the best solution for Q de-
rived by sbd(Q) in case Q is used to formulate
other solutions.
</bodyText>
<listItem confidence="0.942846">
7. If the lower bound for Cost(P) is greater than
the established upper bound UB, prune this
branch.
8. Using the notation described in the beginning
of Sec. 4.2, we update the current solution to
</listItem>
<equation confidence="0.999256333333333">
sbd(P) = (Cost(P),(CZ,delete∀pj∈D(pj),
�
insert∀pk∈Ej(pk))) sbd(Q)
</equation>
<bodyText confidence="0.984903">
in which ® is an operator that composes two
partial solutions.
</bodyText>
<listItem confidence="0.9982146">
9. If sbd(P) is a complete solution (either Q is
empty or have a known best solution) and
Cost(P) &lt; UB, update the upper bound
UB = Cost(P).
10. Output the solution with the lowest overall cost.
</listItem>
<bodyText confidence="0.994583896551724">
To establish the initial UB for pruning, we use the
minimum of the following three bounds. In general,
the tighter the UB is, the more effective the pruning
is.
Greedy set partition: we employ a greedy set
partition algorithm in which we first match the set
S C P with the largest |S|. Repeat the same process
for P~ where P� = P − S. The solution cost is
Cost(P) = (N − 1) * SBC, and N is the number
of sentences in the solution. The complexity of this
computation is O(|P |), where |P |is the number of
propositions in P.
Revised minimum set covering: we employ a
greedy minimum set covering algorithm in which
we first find the set S in the corpus that maximizes
the overlapping of propositions in the input P. The
unwanted propositions in S − P are deleted. As-
sume P� = P − S, repeat the same process to P~
until P~ is empty. The only difference between this
and the previous approach is that S here might not
be a subset of P. The complexity of this computa-
tion is O(|P|).
One maximum overlapping sentence: we first
identify the instance CZ in corpus that covers the
maximum number of propositions in P. To arrive
at a solution for P, the rest of the propositions not
covered by CZ are inserted into CZ and all the un-
wanted propositions in CZ are deleted. The cost of
this solution is
</bodyText>
<equation confidence="0.958167">
Wd * E dcost(CZ, pj) + WZ * E icost(*,pk)
pj∈D pk∈I
</equation>
<bodyText confidence="0.999938">
in which D includes proposition in CZ but not in P,
and I includes propositions in P but not in CZ.
Currently, we update UB only after a complete
solution is found. It is possible to derive better UB
by establishing the upper bound for each partial so-
lution, but the computational overhead might not
justify doing so.
</bodyText>
<subsectionHeader confidence="0.999702">
4.3 Approximation Algorithm
</subsectionHeader>
<bodyText confidence="0.999842083333333">
Even with pruning and dynamic programming, the
exact solution still is very expensive computation-
ally. Computing exact solution for an input size
of 12 propositions has over 1.6 millions states and
takes more than 30 minutes (see Figure 1). To make
the search more efficient for tasks with a large num-
ber of propositions in the input, we naturally seek
a greedy strategy in which at every iteration the al-
gorithm myopically chooses the next best step with-
out regard for its implications on future moves. One
greedy search policy we implemented explores the
branch that uses the instance with maximum over-
lapping propositions with the input and ignores all
branches exploring other corpus instances. The in-
tuition behind this policy is that the more overlap
an instance has with the input, the less insertions or
sentence breaks are needed.
Figure 1 and Figure 2 demonstrate the trade-
off between computation efficiency and accuracy.
In this graph, we use instances from the real-
estate corpus with size 250, we vary the input sen-
tence length from one to twenty and the results
shown in the graphs are average value over sev-
eral typical weight configurations ((Wd,WZ,SBC)=
</bodyText>
<page confidence="0.979073">
570
</page>
<figure confidence="0.7431165">
Execution Time (Seconds)
Cost
</figure>
<bodyText confidence="0.975063571428571">
(1,3,5),(1,3,7),(1,5,3),(1,7,3),(1,1,1)). Figure 2 com-
pares the quality of the solutions when using exact
solutions versus approximation. In our interactive
multimedia system, we currently use exact solution
for input size of 7 propositions or less and switch to
greedy for any larger input size to ensure sub-second
performance for the NLG component.
</bodyText>
<figure confidence="0.986695">
200
180
160
140
120
100
80
60
40
20
0
2 4 6 8 9 10 12 14 16 18 20
# of Propositions in Input
</figure>
<figureCaption confidence="0.9705525">
Figure 1: Speed difference between exact solutions
and approximations
</figureCaption>
<figure confidence="0.993717615384615">
20
18
16
14
12
10
8
6
4
2
0
2 4 6 8 9 10 12 14 16 18 20
# of Proposition in Input
</figure>
<figureCaption confidence="0.9835585">
Figure 2: Cost difference between exact solutions
and approximations
</figureCaption>
<table confidence="0.9998116">
Measures Ours B-3 B-6
Dangling sentence (7) 0 100% 100%
Split Semantic Group 1% 61% 21%
Realization Failure 0 56% 72%
Fluency 59% 4% 8%
</table>
<tableCaption confidence="0.998115">
Table 2: Comparisons
</tableCaption>
<sectionHeader confidence="0.99878" genericHeader="evaluation">
5 Evaluations
</sectionHeader>
<bodyText confidence="0.999819578947368">
To evaluate the quality of our sentence boundary de-
cisions, we implemented a baseline system in which
boundary determination of the aggregation module
is based on a threshold of the maximum number
of propositions allowed in a sentence (a simplified
version of the second strategy in Section 2. We
have tested two threshold values, the average (3) and
maximum (6) number of propositions among cor-
pus instances. Other sentence complexity measures,
such as the number of words and depth of embed-
ding are not easily applicable for our comparison
because they require the propositions to be realized
first before the boundary decisions can be made.
We tune the relative weight of our approach to
best fit our system’s capability. Currently, the
weights are empirically established to Wd = 1,
Wi = 3 and SBC = 3. Based on the output gen-
erated from both systems, we derive four evaluation
metrics:
</bodyText>
<listItem confidence="0.793132">
1. Dangling sentences: We define dangling sen-
</listItem>
<bodyText confidence="0.924607485714286">
tences as the short sentences with only one
proposition that follow long sentences. This
measure is used to verify our claim that because
we use global instead of local optimization,
we can avoid generating dangling sentences by
making more balanced sentence boundary de-
cisions. In contrast, the baseline approaches
have dangling sentence problem when the in-
put proposition is 1 over the multiple of the
threshold values. The first row of Table 2 shows
that when the input proposition length is set
to 7, a pathological case, among the 200 input
proposition sets randomly generated, the base-
line approach always produce dangling sen-
tences (100%). In contrast, our approach al-
ways generates more balanced sentences (0%).
2. Semantic group splitting. Since we use an
instance-based approach, we can maintain the
semantic cohesion better. To test this, we
randomly generated 200 inputs with up to 10
propositions containing semantic grouping of
both the number of bedrooms and number of
bathrooms. The second row, Split Semantic
Group, in Table 2 shows that our algorithm can
maintain semantic group much better than the
baseline approach. Only in 1% of the output
sentences, our algorithm generated number of
bedrooms and number of bathrooms in separate
sentences. In contrast, the baseline approaches
did much worse (61% and 21%).
3. Sentence realization failure. This measure is
used to verify that since we also take a sen-
tence’s lexical and syntactical realizability into
consideration, our sentence boundary decisions
will result in less sentence realization failures.
</bodyText>
<figure confidence="0.9941265">
Greedy
Exact
Greedy
Exact
</figure>
<page confidence="0.99141">
571
</page>
<bodyText confidence="0.999485625">
An realization failure occurs when the aggre-
gation module failed to realize one sentence
for all the propositions grouped by the sentence
boundary determination module. The third row
in Table 2, Realization Failure, indicates that
given 200 randomly generated input proposi-
tion sets with length from 1 to 10, how many re-
alization happened in the output. Our approach
did not have any realization failure while for the
baseline approaches, there are 56% and 72%
outputs have one or more realization failures.
4. Fluency. This measure is used to verify our
claim that since we also optimize our solutions
based on boundary cost, we can reduce incoher-
ence across multiple sentences. Given 200 ran-
domly generated input propositions with length
from 1 to 10, we did a blind test and presented
pairs of generated sentences to two human sub-
jects randomly and asked them to rate which
output is more coherent. The last row, Flu-
ency, in Table 2 shows how often the human
subjects believe that a particular algorithm gen-
erated better sentences. The output of our al-
gorithm is preferred for more than 59% of the
cases, while the baseline approaches are pre-
ferred 4% and 8%, respectively. The other per-
centages not accounted for are cases where the
human subject felt there is no significant differ-
ence in fluency between the two given choices.
The result from this evaluation clearly demon-
strates the superiority of our approach in gener-
ating coherent sentences.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999948214285714">
In the paper, we proposed a novel domain indepen-
dent instance-based sentence boundary determina-
tion algorithm that is capable of balancing a com-
prehensive set of generation capability, sentence
complexity, and quality related constraints. This
is the first domain-independent algorithm that pos-
sesses many desirable properties, including balanc-
ing a system’s generation capabilities, maintaining
semantic cohesion and cross sentence coherence,
and preventing severe syntactic and lexical realiza-
tion failures. Our evaluation results also demon-
strate the superiority of the approach over a rep-
resentative domain independent sentence boundary
solution.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925630434783">
Anthony C. Davey. 1979. Discourse Production. Edin-
burgh University Press, Edinburgh.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill.
William C. Mann and James A. Moore. 1981. Computer
generation of multiparagraph English text. American
Journal of Computational Linguistics, 7(1):17–29.
Shimei Pan and James Shaw. 2004. SEGUE: A hy-
brid case-based surface natural language generator. In
Proc. ofICNLG, Brockenhurst, U.K.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proc. ofINLG, Kennebunkport, Maine.
Graeme D. Ritchie. 1984. A rational reconstruction of
the Proteus sentence planner. In Proc. of the COLING
and the ACL, Stanford, CA.
Jacques Robin. 1994. Automatic generation and revi-
sion of natural language summaries providing histori-
cal background. In Proc. of the Brazilian Symposium
on Artificial Intelligence, Fortaleza, CE, Brazil.
James Shaw. 1998. Segregatory coordination and ellipsis
in text generation. In Proc. of the COLING and the
ACL., Montreal, Canada.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Proc. of the ACL, Barcelona, Spain.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In Proc. of the
NAACL, Pittsburgh, PA.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. Computer Speech and Language.
John Wilkinson. 1995. Aggregation in natural language
generation: Another look. Co-op work term report,
Dept. of Computer Science, University of Waterloo.
Michelle Zhou and Vikram Aggarwal. 2004. An
optimization-based approach to dynamic data content
selection in intelligent multimedia interfaces. In Proc.
of the UIST, Santa Fe, NM.
Michelle X. Zhou and Min Chen. 2003. Automated
generation of graphic sketches by example. In IJCAI,
Acapulco, Mexico.
Michelle X. Zhou and Shimei Pan. 2001. Automated
authoring of coherent multimedia discourse in conver-
sation systems. In ACMMultimedia, Ottawa, Canada.
</reference>
<page confidence="0.997411">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940141">
<title confidence="0.9988845">Instance-based Sentence Boundary Determination by Optimization for Natural Language Generation</title>
<author confidence="0.999949">Pan C Shaw</author>
<affiliation confidence="0.999858">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.9865645">19 Skyline Drive Hawthorne, NY 10532, USA</address>
<abstract confidence="0.998850137931034">This paper describes a novel instancebased sentence boundary determination method for natural language generation that optimizes a set of criteria based on examples in a corpus. Compared to existing sentence boundary determination approaches, our work offers three significant contributions. First, our approach provides a general domain independent framework that effectively addresses sentence boundary determination by balancing a comprehensive set of sentence complexity and quality related constraints. Second, our approach can simulate the characteristics and the style of naturally occurring sentences in an application domain since our solutions are optimized based on their similarities to examples in a corpus. Third, our approach can adapt easily to suit a natural language generation system’s capability by balancing the strengths and weaknesses of its subcomponents (e.g. its aggregation and referring expression generation capability). Our final evaluation shows that the proposed method results in significantly better sentence generation outcomes than a widely adopted approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony C Davey</author>
</authors>
<title>Discourse Production.</title>
<date>1979</date>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh.</location>
<contexts>
<context position="5606" citStr="Davey, 1979" startWordPosition="844" endWordPosition="845">n 20) will be conveyed, it is clear that a good sentence boundary determination module can greatly ease the generation process and improve the quality of the output. In the rest of the paper, we start with a discussion of related work, and then describe our instance-base approach to sentence boundary determination. Finally, we present our evaluation results. 2 Related Work Existing approaches to sentence boundary determination typically employ one of the following strategies. The first strategy uses domain-specific heuristics to decide which propositions can be combined. For example, Proteus (Davey, 1979; Ritchie, 1984) produces game descriptions by employing domainspecific sentence scope heuristics. This approach can work well for a particular application, however, it is not readily reusable for new applications. The second strategy is to employ syntactic, lexical, and sentence complexity constraints to control the aggregation of multiple propositions (Robin, 1994; Shaw, 1998). These strategies can generate fluent complex sentences, but they do not take other criteria into consideration, such as semantic cohesion. Further more, since these approaches do not employ global optimization as we d</context>
</contexts>
<marker>Davey, 1979</marker>
<rawString>Anthony C. Davey. 1979. Discourse Production. Edinburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gunning</author>
</authors>
<title>The Technique of Clear Writing.</title>
<date>1952</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1850" citStr="Gunning, 1952" startWordPosition="266" endWordPosition="267"> results in significantly better sentence generation outcomes than a widely adopted approach. 1 Introduction The problem of sentence boundary determination in natural language generation exists when more than one sentence is needed to convey multiple concepts and propositions. In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. Sentence boundary determination is a complex process that directly impacts a sentence’s readability (Gunning, 1952), its semantic cohesion, its syntactic and lexical realizability, and its smoothness between sentence transitions. Sentences that are too complex are hard to understand, so are sentences lacking semantic cohesion and cross-sentence coherence. Further more, bad sentence boundary decisions may even make sentences unrealizable. To design a sentence boundary determination method that addresses these issues, we employ an instance-based approach (Varges and Mellish, 2001; Pan and Shaw, 2004). Because we optimize our solutions based on examples in a corpus, the output sentences can demonstrate proper</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>Robert Gunning. 1952. The Technique of Clear Writing. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>James A Moore</author>
</authors>
<title>Computer generation of multiparagraph English text.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>7</volume>
<issue>1</issue>
<marker>Mann, Moore, 1981</marker>
<rawString>William C. Mann and James A. Moore. 1981. Computer generation of multiparagraph English text. American Journal of Computational Linguistics, 7(1):17–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimei Pan</author>
<author>James Shaw</author>
</authors>
<title>SEGUE: A hybrid case-based surface natural language generator.</title>
<date>2004</date>
<booktitle>In Proc. ofICNLG,</booktitle>
<location>Brockenhurst, U.K.</location>
<contexts>
<context position="2340" citStr="Pan and Shaw, 2004" startWordPosition="334" endWordPosition="337"> are decided. Sentence boundary determination is a complex process that directly impacts a sentence’s readability (Gunning, 1952), its semantic cohesion, its syntactic and lexical realizability, and its smoothness between sentence transitions. Sentences that are too complex are hard to understand, so are sentences lacking semantic cohesion and cross-sentence coherence. Further more, bad sentence boundary decisions may even make sentences unrealizable. To design a sentence boundary determination method that addresses these issues, we employ an instance-based approach (Varges and Mellish, 2001; Pan and Shaw, 2004). Because we optimize our solutions based on examples in a corpus, the output sentences can demonstrate properties, such as similar sentence length distribution and semantic grouping similar to those in the corpus. Our approach also avoids problematic sentence boundaries by optimizing the solutions using all the instances in the corpus. By taking a sentence’s lexical and syntactic realizability into consideration, it can also avoid sentence realization failures caused by bad sentence boundary decisions. Moreover, since our solution can be adapted easily to suit the capability of a natural lang</context>
<context position="4743" citStr="Pan and Shaw, 2004" startWordPosition="704" endWordPosition="707">ation in the real-estate domain in which potential home buyers interact with the system using multiple modalities, such as speech and gesture, to request residential real-estate information (Zhou and Pan, 2001; Zhou and Chen, 2003; Zhou and Aggarwal, 2004). After interpreting the request, the system formulates a multimedia presentation, including automatically generated speech and graphics, as the response (Zhou and Aggarwal, 2004). The proposed sentence boundary determination module takes a set of propositions selected by a content planner and passes the sentence boundary decisions to SEGUE (Pan and Shaw, 2004), an instance-based sentence generator, to formulate the final sentences. For example, our system is called upon to generate responses to a user’s request: “Tell me more about this house.” Even though not all of the main attributes of a house (more than 20) will be conveyed, it is clear that a good sentence boundary determination module can greatly ease the generation process and improve the quality of the output. In the rest of the paper, we start with a discussion of related work, and then describe our instance-base approach to sentence boundary determination. Finally, we present our evaluat</context>
<context position="12158" citStr="Pan and Shaw (2004)" startWordPosition="1925" endWordPosition="1928">rming aggregations and weak on referring expressions, we may avoid incoherence between sentences by preferring aggregating more attributes in one sentence (like in S8) rather than by splitting them into multiple sentences (like in S9). In the following, we will demonstrate how our approach can achieve all the above goals in a unified instance-based framework. 4 Instance-based boundary determination Instance-based generation automatically creates sentences that are similar to those generated by humans, including their way of grouping semantic content, their wording and their style. Previously, Pan and Shaw (2004) have demonstrated that instancebased learning can be applied successfully in generating new sentences by piecing together existing words and segments in a corpus. Here, we want to demonstrate that by applying the same principle, we can make better sentence boundary decisions. 567 The key idea behind the new approach is to find a sentence boundary solution that minimizes the expected difference between the sentences resulting from these boundary decisions and the examples in the corpus. Here we measure the expected difference based a set of cost functions. 4.1 Optimization Criteria We use thre</context>
</contexts>
<marker>Pan, Shaw, 2004</marker>
<rawString>Shimei Pan and James Shaw. 2004. SEGUE: A hybrid case-based surface natural language generator. In Proc. ofICNLG, Brockenhurst, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible?</title>
<date>1994</date>
<booktitle>In Proc. ofINLG, Kennebunkport,</booktitle>
<location>Maine.</location>
<contexts>
<context position="1590" citStr="Reiter, 1994" startWordPosition="227" endWordPosition="228">an adapt easily to suit a natural language generation system’s capability by balancing the strengths and weaknesses of its subcomponents (e.g. its aggregation and referring expression generation capability). Our final evaluation shows that the proposed method results in significantly better sentence generation outcomes than a widely adopted approach. 1 Introduction The problem of sentence boundary determination in natural language generation exists when more than one sentence is needed to convey multiple concepts and propositions. In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. Sentence boundary determination is a complex process that directly impacts a sentence’s readability (Gunning, 1952), its semantic cohesion, its syntactic and lexical realizability, and its smoothness between sentence transitions. Sentences that are too complex are hard to understand, so are sentences lacking semantic cohesion and cross-sentence coherence. Further more, bad sentence boundary decisions may even make sentences unrealizable. To design a s</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Ehud Reiter. 1994. Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible? In Proc. ofINLG, Kennebunkport, Maine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme D Ritchie</author>
</authors>
<title>A rational reconstruction of the Proteus sentence planner.</title>
<date>1984</date>
<booktitle>In Proc. of the COLING and the ACL,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="5622" citStr="Ritchie, 1984" startWordPosition="846" endWordPosition="847"> conveyed, it is clear that a good sentence boundary determination module can greatly ease the generation process and improve the quality of the output. In the rest of the paper, we start with a discussion of related work, and then describe our instance-base approach to sentence boundary determination. Finally, we present our evaluation results. 2 Related Work Existing approaches to sentence boundary determination typically employ one of the following strategies. The first strategy uses domain-specific heuristics to decide which propositions can be combined. For example, Proteus (Davey, 1979; Ritchie, 1984) produces game descriptions by employing domainspecific sentence scope heuristics. This approach can work well for a particular application, however, it is not readily reusable for new applications. The second strategy is to employ syntactic, lexical, and sentence complexity constraints to control the aggregation of multiple propositions (Robin, 1994; Shaw, 1998). These strategies can generate fluent complex sentences, but they do not take other criteria into consideration, such as semantic cohesion. Further more, since these approaches do not employ global optimization as we do, the content o</context>
</contexts>
<marker>Ritchie, 1984</marker>
<rawString>Graeme D. Ritchie. 1984. A rational reconstruction of the Proteus sentence planner. In Proc. of the COLING and the ACL, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Automatic generation and revision of natural language summaries providing historical background.</title>
<date>1994</date>
<booktitle>In Proc. of the Brazilian Symposium on Artificial Intelligence,</booktitle>
<location>Fortaleza, CE,</location>
<contexts>
<context position="5974" citStr="Robin, 1994" startWordPosition="897" endWordPosition="898">ed Work Existing approaches to sentence boundary determination typically employ one of the following strategies. The first strategy uses domain-specific heuristics to decide which propositions can be combined. For example, Proteus (Davey, 1979; Ritchie, 1984) produces game descriptions by employing domainspecific sentence scope heuristics. This approach can work well for a particular application, however, it is not readily reusable for new applications. The second strategy is to employ syntactic, lexical, and sentence complexity constraints to control the aggregation of multiple propositions (Robin, 1994; Shaw, 1998). These strategies can generate fluent complex sentences, but they do not take other criteria into consideration, such as semantic cohesion. Further more, since these approaches do not employ global optimization as we do, the content of each sentence might not be distributed evenly. This may cause dangling sentence problem (Wilkinson, 1995). Another strategy described in Mann and Moore(1981) guided the aggregation process by using an evaluation score that is sensitive to the structure and term usage of a sentence. Similar to our approach, they rely on search to find an optimal sol</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Jacques Robin. 1994. Automatic generation and revision of natural language summaries providing historical background. In Proc. of the Brazilian Symposium on Artificial Intelligence, Fortaleza, CE, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Segregatory coordination and ellipsis in text generation.</title>
<date>1998</date>
<booktitle>In Proc. of the COLING and the ACL.,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="5987" citStr="Shaw, 1998" startWordPosition="899" endWordPosition="900">ing approaches to sentence boundary determination typically employ one of the following strategies. The first strategy uses domain-specific heuristics to decide which propositions can be combined. For example, Proteus (Davey, 1979; Ritchie, 1984) produces game descriptions by employing domainspecific sentence scope heuristics. This approach can work well for a particular application, however, it is not readily reusable for new applications. The second strategy is to employ syntactic, lexical, and sentence complexity constraints to control the aggregation of multiple propositions (Robin, 1994; Shaw, 1998). These strategies can generate fluent complex sentences, but they do not take other criteria into consideration, such as semantic cohesion. Further more, since these approaches do not employ global optimization as we do, the content of each sentence might not be distributed evenly. This may cause dangling sentence problem (Wilkinson, 1995). Another strategy described in Mann and Moore(1981) guided the aggregation process by using an evaluation score that is sensitive to the structure and term usage of a sentence. Similar to our approach, they rely on search to find an optimal solution. The ma</context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>James Shaw. 1998. Segregatory coordination and ellipsis in text generation. In Proc. of the COLING and the ACL., Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentation in spoken dialog systems.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL,</booktitle>
<location>Barcelona,</location>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>Amanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In Proc. of the ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Chris Mellish</author>
</authors>
<title>Instancebased natural language generation.</title>
<date>2001</date>
<booktitle>In Proc. of the NAACL,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2319" citStr="Varges and Mellish, 2001" startWordPosition="330" endWordPosition="333">e and wording of sentences are decided. Sentence boundary determination is a complex process that directly impacts a sentence’s readability (Gunning, 1952), its semantic cohesion, its syntactic and lexical realizability, and its smoothness between sentence transitions. Sentences that are too complex are hard to understand, so are sentences lacking semantic cohesion and cross-sentence coherence. Further more, bad sentence boundary decisions may even make sentences unrealizable. To design a sentence boundary determination method that addresses these issues, we employ an instance-based approach (Varges and Mellish, 2001; Pan and Shaw, 2004). Because we optimize our solutions based on examples in a corpus, the output sentences can demonstrate properties, such as similar sentence length distribution and semantic grouping similar to those in the corpus. Our approach also avoids problematic sentence boundaries by optimizing the solutions using all the instances in the corpus. By taking a sentence’s lexical and syntactic realizability into consideration, it can also avoid sentence realization failures caused by bad sentence boundary decisions. Moreover, since our solution can be adapted easily to suit the capabil</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Sebastian Varges and Chris Mellish. 2001. Instancebased natural language generation. In Proc. of the NAACL, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language.</title>
<date>2002</date>
<marker>Walker, Rambow, Rogati, 2002</marker>
<rawString>Marilyn Walker, Owen Rambow, and Monica Rogati. 2002. Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wilkinson</author>
</authors>
<title>Aggregation in natural language generation: Another look. Co-op work term report,</title>
<date>1995</date>
<institution>Dept. of Computer Science, University of Waterloo.</institution>
<contexts>
<context position="6329" citStr="Wilkinson, 1995" startWordPosition="951" endWordPosition="952">roach can work well for a particular application, however, it is not readily reusable for new applications. The second strategy is to employ syntactic, lexical, and sentence complexity constraints to control the aggregation of multiple propositions (Robin, 1994; Shaw, 1998). These strategies can generate fluent complex sentences, but they do not take other criteria into consideration, such as semantic cohesion. Further more, since these approaches do not employ global optimization as we do, the content of each sentence might not be distributed evenly. This may cause dangling sentence problem (Wilkinson, 1995). Another strategy described in Mann and Moore(1981) guided the aggregation process by using an evaluation score that is sensitive to the structure and term usage of a sentence. Similar to our approach, they rely on search to find an optimal solution. The main difference between this approach and ours is that their evaluation score is computed based on preference heuristics. For example, all the semantic groups existing in a domain have to be coded specifically in order to handle semantic grouping. In contrast, in our framework, the score is computed based on a sentence’s similarity to corpus </context>
</contexts>
<marker>Wilkinson, 1995</marker>
<rawString>John Wilkinson. 1995. Aggregation in natural language generation: Another look. Co-op work term report, Dept. of Computer Science, University of Waterloo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Zhou</author>
<author>Vikram Aggarwal</author>
</authors>
<title>An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces.</title>
<date>2004</date>
<booktitle>In Proc. of the UIST,</booktitle>
<location>Santa Fe, NM.</location>
<contexts>
<context position="4380" citStr="Zhou and Aggarwal, 2004" startWordPosition="650" endWordPosition="653">. 2. Since we employ an instance-based method, the proposed solution is sensitive to the style of the sentences in the application domain in which the corpus is collected. 3. Our approach can be adjusted easily to suit a sentence generation system’s capability and avoid some of its known weaknesses. Currently, our work is embodied in a multimodal conversation application in the real-estate domain in which potential home buyers interact with the system using multiple modalities, such as speech and gesture, to request residential real-estate information (Zhou and Pan, 2001; Zhou and Chen, 2003; Zhou and Aggarwal, 2004). After interpreting the request, the system formulates a multimedia presentation, including automatically generated speech and graphics, as the response (Zhou and Aggarwal, 2004). The proposed sentence boundary determination module takes a set of propositions selected by a content planner and passes the sentence boundary decisions to SEGUE (Pan and Shaw, 2004), an instance-based sentence generator, to formulate the final sentences. For example, our system is called upon to generate responses to a user’s request: “Tell me more about this house.” Even though not all of the main attributes of a </context>
</contexts>
<marker>Zhou, Aggarwal, 2004</marker>
<rawString>Michelle Zhou and Vikram Aggarwal. 2004. An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces. In Proc. of the UIST, Santa Fe, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle X Zhou</author>
<author>Min Chen</author>
</authors>
<title>Automated generation of graphic sketches by example.</title>
<date>2003</date>
<booktitle>In IJCAI,</booktitle>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="4354" citStr="Zhou and Chen, 2003" startWordPosition="646" endWordPosition="649">lexical realizability. 2. Since we employ an instance-based method, the proposed solution is sensitive to the style of the sentences in the application domain in which the corpus is collected. 3. Our approach can be adjusted easily to suit a sentence generation system’s capability and avoid some of its known weaknesses. Currently, our work is embodied in a multimodal conversation application in the real-estate domain in which potential home buyers interact with the system using multiple modalities, such as speech and gesture, to request residential real-estate information (Zhou and Pan, 2001; Zhou and Chen, 2003; Zhou and Aggarwal, 2004). After interpreting the request, the system formulates a multimedia presentation, including automatically generated speech and graphics, as the response (Zhou and Aggarwal, 2004). The proposed sentence boundary determination module takes a set of propositions selected by a content planner and passes the sentence boundary decisions to SEGUE (Pan and Shaw, 2004), an instance-based sentence generator, to formulate the final sentences. For example, our system is called upon to generate responses to a user’s request: “Tell me more about this house.” Even though not all of</context>
</contexts>
<marker>Zhou, Chen, 2003</marker>
<rawString>Michelle X. Zhou and Min Chen. 2003. Automated generation of graphic sketches by example. In IJCAI, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle X Zhou</author>
<author>Shimei Pan</author>
</authors>
<title>Automated authoring of coherent multimedia discourse in conversation systems.</title>
<date>2001</date>
<booktitle>In ACMMultimedia,</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="4333" citStr="Zhou and Pan, 2001" startWordPosition="642" endWordPosition="645">e and syntactic and lexical realizability. 2. Since we employ an instance-based method, the proposed solution is sensitive to the style of the sentences in the application domain in which the corpus is collected. 3. Our approach can be adjusted easily to suit a sentence generation system’s capability and avoid some of its known weaknesses. Currently, our work is embodied in a multimodal conversation application in the real-estate domain in which potential home buyers interact with the system using multiple modalities, such as speech and gesture, to request residential real-estate information (Zhou and Pan, 2001; Zhou and Chen, 2003; Zhou and Aggarwal, 2004). After interpreting the request, the system formulates a multimedia presentation, including automatically generated speech and graphics, as the response (Zhou and Aggarwal, 2004). The proposed sentence boundary determination module takes a set of propositions selected by a content planner and passes the sentence boundary decisions to SEGUE (Pan and Shaw, 2004), an instance-based sentence generator, to formulate the final sentences. For example, our system is called upon to generate responses to a user’s request: “Tell me more about this house.” E</context>
</contexts>
<marker>Zhou, Pan, 2001</marker>
<rawString>Michelle X. Zhou and Shimei Pan. 2001. Automated authoring of coherent multimedia discourse in conversation systems. In ACMMultimedia, Ottawa, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>