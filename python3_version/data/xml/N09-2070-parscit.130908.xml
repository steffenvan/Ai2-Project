<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<title confidence="0.777322">
Fast decoding for open vocabulary spoken term detection
</title>
<note confidence="0.775979">
&apos;B. Ramabhadran,&apos;A. Sethy, 2J. Mamou∗&apos; B. Kingsbury,&apos; U. Chaudhari
&apos;IBM T. J. Watson Research Center
</note>
<author confidence="0.617904">
Yorktown Heights,NY
</author>
<affiliation confidence="0.494616">
2IBM Haifa Research Labs
</affiliation>
<address confidence="0.43278">
Mount Carmel,Haifa
</address>
<sectionHeader confidence="0.979001" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998021578947369">
Information retrieval and spoken-term detec-
tion from audio such as broadcast news, tele-
phone conversations, conference calls, and
meetings are of great interest to the academic,
government, and business communities. Mo-
tivated by the requirement for high-quality in-
dexes, this study explores the effect of using
both word and sub-word information to find
in-vocabulary and OOV query terms. It also
explores the trade-off between search accu-
racy and the speed of audio transcription. We
present a novel, vocabulary independent, hy-
brid LVCSR approach to audio indexing and
search and show that using phonetic confu-
sions derived from posterior probabilities es-
timated by a neural network in the retrieval
of OOV queries can help in reducing misses.
These methods are evaluated on data sets from
the 2006 NIST STD task.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998672476190476">
Indexing and retrieval of speech content in vari-
ous forms such as broadcast news, customer care
data and on-line media has gained a lot of interest
for a wide range of applications from market in-
telligence gathering, to customer analytics and on-
line media search. Spoken term detection (STD) is
a key information retrieval technology which aims
open vocabulary search over large collections of
spoken documents. An approach for solving the out-
of-vocabulary (OOV) issues (Saraclar and Sproat,
2004) consists of converting speech into phonetic,
∗TThe work done by J. Mamou was partially funded by the
EU projects SAPIR and HERMES
syllabic or word-fragment transcripts and represent-
ing the query as a sequence of phones, syllables or
word-fragments respectively. Popular approaches
include subword decoding (Clements et al., 2002;
Mamou et al., 2007; Seide et al., 2004; Siohan and
Bacchiani, 2005) and representations enhanced with
phone confusion probabilities and approximate sim-
ilarity measures (Chaudhari and Picheny, 2007).
</bodyText>
<sectionHeader confidence="0.964028" genericHeader="method">
2 Fast Decoding Architecture
</sectionHeader>
<bodyText confidence="0.999892">
The first step in converting speech to a searchable in-
dex involves the use of an ASR system that produces
word, word-fragment or phonetic transcripts. In
this paper, the LVCSR system is a discriminatively
trained speaker-independent recognizer using PLP-
derived features and a quinphone acoustic model
with approximately 1200 context dependent states
and 30000 Gaussians. The acoustic model is trained
on 430 hours of audio from the 1996 and 1997 En-
glish Broadcast News Speech corpus (LDC97S44,
LDC98S71) and the TDT4 Multilingual Broadcast
News Speech corpus (LDC2005S11).
The language model used for decoding is a tri-
gram model with 84087 words trained on a collec-
tion of 335M words from the following data sources:
Hub4 Language Model data, EARS BN03 closed
captions and GALE Broadcast news and conversa-
tions data. A word-fragment language model is built
on this same data after tokenizing the text to frag-
ments using a fragment inventory of size 21000. A
greedy search algorithm assigns the longest possi-
ble matching fragment first and iteratively uses the
next longest possible fragment until the entire pro-
nunciation of the OOV term has been represented
</bodyText>
<page confidence="0.961511">
277
</page>
<note confidence="0.43225">
Proceedings of NAACL HLT 2009: Short Papers, pages 277–280,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.824758">
90
</page>
<bodyText confidence="0.993755076923077">
by sub-word units.
The speed and accuracy of the decoding are con-
trolled using two forms of pruning. The first is the
standard likelihood-based beam pruning that is used
in many Viterbi decoders. The second is a form
of Gaussian shortlisting in which the Gaussians in
the acoustic model are clustered into 1024 clusters,
each of which is represented by a single Gaussian.
When the decoder gets a new observation vector, it
computes the likelihood of the observation under all
1024 cluster models and then ranks the clusters by
likelihood. Observation likelihoods are then com-
puted only for those mixture components belonging
to the top maxL1 clusters; for components outside
this set a default, low likelihood is used. To illus-
trate the trade-offs in speed vs. accuracy that can
be achieved by varying the two pruning parame-
ters, we sweep through different values for the pa-
rameters and measure decoding accuracy, reported
as word error rate (WER), and decoding speed, re-
ported as times faster than real time (xfRT). For ex-
ample, a system that operates at 20xfRT will require
one minute of time (measured as elapsed time) to
process 20 minutes of speech. Figure 1 illustrates
this effect on the NIST 2006 Spoken Term Detec-
tion Dev06 test set.
</bodyText>
<sectionHeader confidence="0.974079" genericHeader="method">
3 Lucene Based Indexing and Search
</sectionHeader>
<bodyText confidence="0.999992">
The main difficulty with retrieving information from
spoken data is the low accuracy of the transcription,
particularly on terms of interest such as named en-
tities and content words. Generally, the accuracy
of a transcript is measured by its word error rate
(WER), which is characterized by the number of
substitutions, deletions, and insertions with respect
to the correct audio transcript. Mamou (Mamou
et al., 2007) presented the enhancement in recall
and precision by searching on word confusion net-
works instead of considering only the 1-best path
word transcript. We used this model for searching
in-vocabulary queries.
To handle OOV queries, a combination of
word and phonetic search was presented by
Mamou (Mamou et al., 2007). In this paper, we ex-
plore fuzzy phonetic search extending Lucene1, an
Apache open source search library written in Java,
for indexing and search. When searching for these
OOVs in word-fragment indexes, they are repre-
sented phonetically (and subsequently using word-
fragments) using letter-to-phoneme (L2P) rules.
</bodyText>
<subsectionHeader confidence="0.989839">
3.1 Indexing
</subsectionHeader>
<bodyText confidence="0.9999755">
Each transcript is composed of basic units (e.g.,
word, word-fragment, phones) associated with a be-
gin time, duration and posterior probability. An
inverted index is used in a Lucene-based indexing
scheme. Each occurrence of a unit of indexing u in
a transcript D is indexed on its timestamp. If the
posterior probability is provided, we store the confi-
dence level of the occurrence of u at the time t that
is evaluated by its posterior probability Pr(u|t, D).
Otherwise, we consider its posterior probability to
be one. This representation allows the indexing of
different types of transcripts into a single index.
</bodyText>
<subsectionHeader confidence="0.981736">
3.2 Retrieval
</subsectionHeader>
<bodyText confidence="0.997797214285714">
Since the vocabulary of the ASR system used to gen-
erate the word transcripts is known, we can easily
identify IV and OOV parts of the query. We present
two different algorithms, namely, exact and fuzzy
search on word-fragment transcripts. For search
on word-fragment or phonetic transcripts, the query
terms are converted to their word-fragment or pho-
netic representation.
Candidate lists of each query unit are extracted
from the inverted index. For fuzzy search, we re-
trieve several fuzzy matches from the inverted in-
dex for each unit of the query using the edit distance
weighted by the substitution costs provided by the
confusion matrix. Only the matches whose weighted
</bodyText>
<footnote confidence="0.764299">
1http://lucene.apache.org/
</footnote>
<figure confidence="0.972659142857143">
80
70
60
50
40
300 5 10 15 20 25 30
Real Time Factor
</figure>
<figureCaption confidence="0.990626">
Figure 1: Speed vs WER
</figureCaption>
<figure confidence="0.474301">
WER
</figure>
<page confidence="0.968414">
278
</page>
<bodyText confidence="0.999979533333333">
edit distance is below a given threshold are returned.
We use a dynamic programming algorithm to incor-
porate the confusion costs specified in the matrix
in the distance computation. Our implementation is
fail-fast since the procedure is aborted if it is discov-
ered that the minimal cost between the sequences is
greater than a certain threshold.
The score of each occurrence aggregates the pos-
terior probability of each indexed unit. The occur-
rence of each unit is also weighted (user defined
weight) according to its type, for example, a higher
weight can be assigned to word matches instead of
word-fragment or phonetic matches. Given the na-
ture of the index, a match for any query term cannot
span across two consecutively indexed units.
</bodyText>
<subsectionHeader confidence="0.996142">
3.3 Hybrid WordFragment Indexing
</subsectionHeader>
<bodyText confidence="0.999940375">
For the hybrid system we limited the word portion
of the ASR system’s lexicon to the 21K most fre-
quent (frequency greater than 5) words in the acous-
tic training data. This resulted in roughly 11M
(3.1%) OOV tokens in the hybrid LM training set
and 1127(2.5%) OOV tokens in the evaluation set.
A relative entropy criterion described in (Siohan and
Bacchiani, 2005) based on a 5-gram phone language
model was used to identify fragments. We selected
21K fragments to complement the 21K words result-
ing in a composite 42K vocabulary. The language
model text (11M (3.1%) fragment tokens and 320M
word tokens) was tokenized to contain words and
word-fragments (for the OOVs) and the resulting hy-
brid LM was used in conjunction with the acoustic
models described in Section 2.
</bodyText>
<sectionHeader confidence="0.998169" genericHeader="method">
4 Neural Network Based Posteriors for
</sectionHeader>
<subsectionHeader confidence="0.527441">
Fuzzy Search
</subsectionHeader>
<bodyText confidence="0.999971818181818">
In assessing the match of decoded transcripts with
search queries, recognition errors must be accounted
for. One method relies on converting both the de-
coded transcripts and queries into phonetic represen-
tations and modeling the confusion between phones,
typically represented as a confusion matrix. In this
work, we derive this matrix from broadcast news de-
velopment data. In particular, two systems: HMM
based automatic speech recognition (ASR) (Chaud-
hari and Picheny, 2007) and a neural network based
acoustic model (Kingsbury, 2009), are used to ana-
lyze the data and the results are compared to produce
confusion estimates.
Let X = {xt} represent the input feature frames
and S the set of context dependent HMM states.
Associated with S is a many to one map M from
each member sj ∈ S to a phone in the phone set
pk ∈ P. This map collapses the beginning, mid-
dle, and end context dependent states to the central
phone identity. The ASR system is used to generate
a state based alignment of the development data to
the training transcripts. This results in a sequence
of state labels (classes) {st}, st ∈ S, one for each
frame of the input data. Note that the aligned states
are collapsed to the phone identity with M, so the
frame class labels are given by {ct}, ct ∈ P.
Corresponding to each frame, we also use the
state posteriors derived from the output of a Neu-
ral Network acoustic model and the prior probabil-
ities computed on the training set. Define Xt =
{... , xt,...} to be the sub-sequence of the input
speech frames centered around time index t. The
neural network takes Xt as input and produces
</bodyText>
<equation confidence="0.930628">
lt(sj) = y(sj|Xt) − l(sj), sj ∈ S
</equation>
<bodyText confidence="0.9995">
where y is the neural network output and l is the
prior probability, both in the log domain. Again, the
state labels are mapped using M, so the above pos-
terior is interpreted as that for the collapsed phone:
</bodyText>
<equation confidence="0.978913">
lt(sj) ≡ lt(M(sj)) = lt(pj),pj = M(sj).
</equation>
<bodyText confidence="0.987456">
The result of both analyses gives the following set of
associations:
</bodyText>
<equation confidence="0.997452857142857">
c0 ↔ l0(p0),l0(p1), l0(p2),...
c1 ↔ l1(p0),l1(p1),l1(p2),...
.
.
ct ↔ lt(p0), lt(p1), lt(p2),...
Each log posterior li(pj) is converted into a count
ni,j = ceil[N × eli(pj)],
</equation>
<bodyText confidence="0.99998">
where N is a large constant, i ranges over the
time index, and j ranges over the context dependent
states. From the counts, the confusion matrix entries
are computed. The total count for each state is
</bodyText>
<equation confidence="0.910613">
nj(k) = � ni,k,
i:ci=pj
</equation>
<page confidence="0.97028">
279
</page>
<sectionHeader confidence="0.344945" genericHeader="method">
ATWV
</sectionHeader>
<bodyText confidence="0.987179">
where k is an index over the states.
</bodyText>
<equation confidence="0.994152571428571">
⎡ ⎤
n1(1) n1(2) ...
⎢ ⎥
⎢n2(1) n2(2) ... ⎥
⎢ ⎥
⎣ . ⎦
.
</equation>
<bodyText confidence="0.9999784">
The rows of the above matrix correspond to the ref-
erence and the columns to the observations. By nor-
malizing the rows, the entries can be interpreted as
”probability” of an observed phone (indicated by the
column) given the true phone.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99995">
The performance of a spoken term detection system
is measured using DET curves that plot the trade-off
between false alarms (FAs) and misses. This NIST
STD 2006 evaluation metric used Actual/Maximum
Term Weighted Value (ATWV/MTWV) that allows
one to weight FAs and Misses per the needs of the
task at hand (NIST, 2006).
Figure 2 illustrates the effect of speed on ATWV
on the NIST STD 2006 Dev06 data set using 1107
query terms. As the speed of indexing is increased to
many times faster than real time, the WER increases,
which in turn decreases the ATWV measure. It can
be seen that the use of word-fragments improves
the performance on OOV queries thus making the
combined search better than simple word search.
The primary advantage of using a hybrid decoding
scheme over a separate word and fragment based
decoding scheme is the speed of transforming the
audio into indexable units. The blue line in the fig-
ure illustrates that when using a hybrid setup, the
same performance can be achieved at speeds twice
as fast. For example, with the combined search
on two different decodes, an ATWV of 0.1 can be
achieved when indexing at a speed 15 times faster
than real time, but with a hybrid system, the same
performance can be reached at an indexing speed 30
times faster than real time. The ATWV on the hybrid
system also degrades gracefully with faster speeds
when compared to separate word and word-fragment
systems. Preliminary results indicate that fuzzy
search on one best output gives the same ATWV
performance as exact search (Figure 2) on consen-
sus output. Also, a closer look at the retrieval results
of OOV terms revealed that many more OOVs are
retrieved with the fuzzy search.
</bodyText>
<subsectionHeader confidence="0.236069">
Real Time Factor
</subsectionHeader>
<figureCaption confidence="0.989079333333333">
Figure 2: Effect of WER on ATWV. Note that the cuves
for exactWord and exactWordAndFrag lie on top of each
other.
</figureCaption>
<sectionHeader confidence="0.999671" genericHeader="conclusions">
6 CONCLUSION
</sectionHeader>
<bodyText confidence="0.9999284">
In this paper, we have presented the effect of rapid
decoding on a spoken term detection task. We
have demonstrated that hybrid systems perform well
and fuzzy search with phone confusion probabilities
help in OOV retrieval.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99900804">
U. V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. In Proc. ofASRU.
M. Clements, S. Robertson, and M. S. Miller. 2002.
Phonetic searching applied to on-line distance learning
modules. In Proc. of IEEE Digital Signal Processing
Workshop.
B. Kingsbury. 2009. Lattice-based optimization
of sequence classification criteria for neural-network
acoustic modeling. In Proc. ofICASSP.
J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vo-
cabulary independent spoken term detection. In Proc.
ofACMSIGIR.
NIST. 2006. The spoken term de-
tection (STD) 2006 evaluation plan.
http://www.nist.gov/speech/tests/std/docs/std06-
evalplan-v10.pdf.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabulary-
independent search in spontaneous speech. In Proc. of
ICASSP.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary in-
dependent audio search using path based graph index-
ing. In Proc. ofInterspeech.
</reference>
<figure confidence="0.988660416666667">
−0.2
−0.4
0 5 10 15 20 25 30 35
0.8
0.6
0.4
0.2
0
1
exactWord
exactWordAndFrag
exactHybrid
</figure>
<page confidence="0.930865">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.345648">
<title confidence="0.8699805">Fast decoding for open vocabulary spoken term detection Sethy, Chaudhari</title>
<author confidence="0.930873">T J Watson Research</author>
<affiliation confidence="0.9299845">Yorktown Heights,NY Haifa Research</affiliation>
<author confidence="0.712109">Mount Carmel</author>
<author confidence="0.712109">Haifa</author>
<abstract confidence="0.9822744">Information retrieval and spoken-term detection from audio such as broadcast news, telephone conversations, conference calls, and meetings are of great interest to the academic, government, and business communities. Motivated by the requirement for high-quality indexes, this study explores the effect of using both word and sub-word information to find in-vocabulary and OOV query terms. It also explores the trade-off between search accuracy and the speed of audio transcription. We present a novel, vocabulary independent, hybrid LVCSR approach to audio indexing and search and show that using phonetic confusions derived from posterior probabilities estimated by a neural network in the retrieval of OOV queries can help in reducing misses. These methods are evaluated on data sets from the 2006 NIST STD task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>U V Chaudhari</author>
<author>M Picheny</author>
</authors>
<title>Improvements in phone based audio search via constrained match with high order confusion estimates.</title>
<date>2007</date>
<booktitle>In Proc. ofASRU.</booktitle>
<contexts>
<context position="2085" citStr="Chaudhari and Picheny, 2007" startWordPosition="315" endWordPosition="318">ken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLPderived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians. The acoustic model is trained on 430 hours of audio from the 1996 and 1997 English Broadcast News Speech corpus (LDC97S44, LDC98S71) and the TDT4 Multilingual Broadcast News Speech corpus (LDC2005S1</context>
<context position="9247" citStr="Chaudhari and Picheny, 2007" startWordPosition="1478" endWordPosition="1482">and the resulting hybrid LM was used in conjunction with the acoustic models described in Section 2. 4 Neural Network Based Posteriors for Fuzzy Search In assessing the match of decoded transcripts with search queries, recognition errors must be accounted for. One method relies on converting both the decoded transcripts and queries into phonetic representations and modeling the confusion between phones, typically represented as a confusion matrix. In this work, we derive this matrix from broadcast news development data. In particular, two systems: HMM based automatic speech recognition (ASR) (Chaudhari and Picheny, 2007) and a neural network based acoustic model (Kingsbury, 2009), are used to analyze the data and the results are compared to produce confusion estimates. Let X = {xt} represent the input feature frames and S the set of context dependent HMM states. Associated with S is a many to one map M from each member sj ∈ S to a phone in the phone set pk ∈ P. This map collapses the beginning, middle, and end context dependent states to the central phone identity. The ASR system is used to generate a state based alignment of the development data to the training transcripts. This results in a sequence of stat</context>
</contexts>
<marker>Chaudhari, Picheny, 2007</marker>
<rawString>U. V. Chaudhari and M. Picheny. 2007. Improvements in phone based audio search via constrained match with high order confusion estimates. In Proc. ofASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Clements</author>
<author>S Robertson</author>
<author>M S Miller</author>
</authors>
<title>Phonetic searching applied to on-line distance learning modules.</title>
<date>2002</date>
<booktitle>In Proc. of IEEE Digital Signal Processing Workshop.</booktitle>
<contexts>
<context position="1886" citStr="Clements et al., 2002" startWordPosition="287" endWordPosition="290"> gathering, to customer analytics and online media search. Spoken term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLPderived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians.</context>
</contexts>
<marker>Clements, Robertson, Miller, 2002</marker>
<rawString>M. Clements, S. Robertson, and M. S. Miller. 2002. Phonetic searching applied to on-line distance learning modules. In Proc. of IEEE Digital Signal Processing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kingsbury</author>
</authors>
<title>Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling.</title>
<date>2009</date>
<booktitle>In Proc. ofICASSP.</booktitle>
<contexts>
<context position="9307" citStr="Kingsbury, 2009" startWordPosition="1490" endWordPosition="1491">dels described in Section 2. 4 Neural Network Based Posteriors for Fuzzy Search In assessing the match of decoded transcripts with search queries, recognition errors must be accounted for. One method relies on converting both the decoded transcripts and queries into phonetic representations and modeling the confusion between phones, typically represented as a confusion matrix. In this work, we derive this matrix from broadcast news development data. In particular, two systems: HMM based automatic speech recognition (ASR) (Chaudhari and Picheny, 2007) and a neural network based acoustic model (Kingsbury, 2009), are used to analyze the data and the results are compared to produce confusion estimates. Let X = {xt} represent the input feature frames and S the set of context dependent HMM states. Associated with S is a many to one map M from each member sj ∈ S to a phone in the phone set pk ∈ P. This map collapses the beginning, middle, and end context dependent states to the central phone identity. The ASR system is used to generate a state based alignment of the development data to the training transcripts. This results in a sequence of state labels (classes) {st}, st ∈ S, one for each frame of the i</context>
</contexts>
<marker>Kingsbury, 2009</marker>
<rawString>B. Kingsbury. 2009. Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mamou</author>
<author>B Ramabhadran</author>
<author>O Siohan</author>
</authors>
<title>Vocabulary independent spoken term detection.</title>
<date>2007</date>
<booktitle>In Proc. ofACMSIGIR.</booktitle>
<contexts>
<context position="1906" citStr="Mamou et al., 2007" startWordPosition="291" endWordPosition="294"> analytics and online media search. Spoken term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLPderived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians. The acoustic model </context>
<context position="5117" citStr="Mamou et al., 2007" startWordPosition="807" endWordPosition="810">ne minute of time (measured as elapsed time) to process 20 minutes of speech. Figure 1 illustrates this effect on the NIST 2006 Spoken Term Detection Dev06 test set. 3 Lucene Based Indexing and Search The main difficulty with retrieving information from spoken data is the low accuracy of the transcription, particularly on terms of interest such as named entities and content words. Generally, the accuracy of a transcript is measured by its word error rate (WER), which is characterized by the number of substitutions, deletions, and insertions with respect to the correct audio transcript. Mamou (Mamou et al., 2007) presented the enhancement in recall and precision by searching on word confusion networks instead of considering only the 1-best path word transcript. We used this model for searching in-vocabulary queries. To handle OOV queries, a combination of word and phonetic search was presented by Mamou (Mamou et al., 2007). In this paper, we explore fuzzy phonetic search extending Lucene1, an Apache open source search library written in Java, for indexing and search. When searching for these OOVs in word-fragment indexes, they are represented phonetically (and subsequently using wordfragments) using l</context>
</contexts>
<marker>Mamou, Ramabhadran, Siohan, 2007</marker>
<rawString>J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vocabulary independent spoken term detection. In Proc. ofACMSIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The spoken term detection (STD)</title>
<date>2006</date>
<note>evaluation plan. http://www.nist.gov/speech/tests/std/docs/std06-evalplan-v10.pdf.</note>
<contexts>
<context position="4625" citStr="NIST 2006" startWordPosition="730" endWordPosition="731"> mixture components belonging to the top maxL1 clusters; for components outside this set a default, low likelihood is used. To illustrate the trade-offs in speed vs. accuracy that can be achieved by varying the two pruning parameters, we sweep through different values for the parameters and measure decoding accuracy, reported as word error rate (WER), and decoding speed, reported as times faster than real time (xfRT). For example, a system that operates at 20xfRT will require one minute of time (measured as elapsed time) to process 20 minutes of speech. Figure 1 illustrates this effect on the NIST 2006 Spoken Term Detection Dev06 test set. 3 Lucene Based Indexing and Search The main difficulty with retrieving information from spoken data is the low accuracy of the transcription, particularly on terms of interest such as named entities and content words. Generally, the accuracy of a transcript is measured by its word error rate (WER), which is characterized by the number of substitutions, deletions, and insertions with respect to the correct audio transcript. Mamou (Mamou et al., 2007) presented the enhancement in recall and precision by searching on word confusion networks instead of consid</context>
<context position="11812" citStr="NIST, 2006" startWordPosition="1951" endWordPosition="1952">⎢n2(1) n2(2) ... ⎥ ⎢ ⎥ ⎣ . ⎦ . The rows of the above matrix correspond to the reference and the columns to the observations. By normalizing the rows, the entries can be interpreted as ”probability” of an observed phone (indicated by the column) given the true phone. 5 Experiments and Results The performance of a spoken term detection system is measured using DET curves that plot the trade-off between false alarms (FAs) and misses. This NIST STD 2006 evaluation metric used Actual/Maximum Term Weighted Value (ATWV/MTWV) that allows one to weight FAs and Misses per the needs of the task at hand (NIST, 2006). Figure 2 illustrates the effect of speed on ATWV on the NIST STD 2006 Dev06 data set using 1107 query terms. As the speed of indexing is increased to many times faster than real time, the WER increases, which in turn decreases the ATWV measure. It can be seen that the use of word-fragments improves the performance on OOV queries thus making the combined search better than simple word search. The primary advantage of using a hybrid decoding scheme over a separate word and fragment based decoding scheme is the speed of transforming the audio into indexable units. The blue line in the figure il</context>
</contexts>
<marker>NIST, 2006</marker>
<rawString>NIST. 2006. The spoken term detection (STD) 2006 evaluation plan. http://www.nist.gov/speech/tests/std/docs/std06-evalplan-v10.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saraclar</author>
<author>R Sproat</author>
</authors>
<title>Lattice-based search for spoken utterance retrieval.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="1557" citStr="Saraclar and Sproat, 2004" startWordPosition="238" endWordPosition="241">of OOV queries can help in reducing misses. These methods are evaluated on data sets from the 2006 NIST STD task. 1 Introduction Indexing and retrieval of speech content in various forms such as broadcast news, customer care data and on-line media has gained a lot of interest for a wide range of applications from market intelligence gathering, to customer analytics and online media search. Spoken term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a </context>
</contexts>
<marker>Saraclar, Sproat, 2004</marker>
<rawString>M. Saraclar and R. Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Seide</author>
<author>P Yu</author>
<author>C Ma</author>
<author>E Chang</author>
</authors>
<title>Vocabularyindependent search in spontaneous speech.</title>
<date>2004</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="1926" citStr="Seide et al., 2004" startWordPosition="295" endWordPosition="298">e media search. Spoken term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLPderived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians. The acoustic model is trained on 430 ho</context>
</contexts>
<marker>Seide, Yu, Ma, Chang, 2004</marker>
<rawString>F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabularyindependent search in spontaneous speech. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Siohan</author>
<author>M Bacchiani</author>
</authors>
<title>Fast vocabulary independent audio search using path based graph indexing.</title>
<date>2005</date>
<booktitle>In Proc. ofInterspeech.</booktitle>
<contexts>
<context position="1955" citStr="Siohan and Bacchiani, 2005" startWordPosition="299" endWordPosition="302">en term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the outof-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, ∗TThe work done by J. Mamou was partially funded by the EU projects SAPIR and HERMES syllabic or word-fragment transcripts and representing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate similarity measures (Chaudhari and Picheny, 2007). 2 Fast Decoding Architecture The first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLPderived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians. The acoustic model is trained on 430 hours of audio from the 1996 an</context>
<context position="8313" citStr="Siohan and Bacchiani, 2005" startWordPosition="1331" endWordPosition="1334"> to its type, for example, a higher weight can be assigned to word matches instead of word-fragment or phonetic matches. Given the nature of the index, a match for any query term cannot span across two consecutively indexed units. 3.3 Hybrid WordFragment Indexing For the hybrid system we limited the word portion of the ASR system’s lexicon to the 21K most frequent (frequency greater than 5) words in the acoustic training data. This resulted in roughly 11M (3.1%) OOV tokens in the hybrid LM training set and 1127(2.5%) OOV tokens in the evaluation set. A relative entropy criterion described in (Siohan and Bacchiani, 2005) based on a 5-gram phone language model was used to identify fragments. We selected 21K fragments to complement the 21K words resulting in a composite 42K vocabulary. The language model text (11M (3.1%) fragment tokens and 320M word tokens) was tokenized to contain words and word-fragments (for the OOVs) and the resulting hybrid LM was used in conjunction with the acoustic models described in Section 2. 4 Neural Network Based Posteriors for Fuzzy Search In assessing the match of decoded transcripts with search queries, recognition errors must be accounted for. One method relies on converting b</context>
</contexts>
<marker>Siohan, Bacchiani, 2005</marker>
<rawString>O. Siohan and M. Bacchiani. 2005. Fast vocabulary independent audio search using path based graph indexing. In Proc. ofInterspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>