<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.989019">
Cache-based Document-level Statistical Machine Translation
</title>
<author confidence="0.999438">
Zhengxian Gong1 Min Zhang2 Guodong Zhou1*
</author>
<affiliation confidence="0.759961">
1 School of Computer Science and Technology
Soochow University, Suzhou, China 215006
2 Institute for Infocomm Research, Singapore 138632
</affiliation>
<email confidence="0.983735">
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.997291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99843264516129">
Statistical machine translation systems are
usually trained on a large amount of bilingual
sentence pairs and translate one sentence at a
time, ignoring document-level information. In
this paper, we propose a cache-based approach
to document-level translation. Since caches
mainly depend on relevant data to supervise
subsequent decisions, it is critical to fill the
caches with highly-relevant data of a reasonable
size. In this paper, we present three kinds of
caches to store relevant document-level infor-
mation: 1) a dynamic cache, which stores bilin-
gual phrase pairs from the best translation
hypotheses of previous sentences in the test
document; 2) a static cache, which stores rele-
vant bilingual phrase pairs extracted from simi-
lar bilingual document pairs (i.e. source
documents similar to the test document and
their corresponding target documents) in the
training parallel corpus; 3) a topic cache, which
stores the target-side topic words related with
the test document in the source-side. In particu-
lar, three new features are designed to explore
various kinds of document-level information in
above three kinds of caches. Evaluation shows
the effectiveness of our cache-based approach
to document-level translation with the perfor-
mance improvement of 0.81 in BLUE score
over Moses. Especially, detailed analysis and
discussion are presented to give new insights to
document-level translation.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998288">
During last decade, tremendous work has been
done to improve the quality of statistical machine
</bodyText>
<note confidence="0.554732">
* Corresponding author.
</note>
<bodyText confidence="0.999840684210526">
translation (SMT) systems. However, there is still
a huge performance gap between the state-of-the-
art SMT systems and human translators. Bond
(2002) suggested nine ways to improve machine
translation by imitating the best practices of human
translators (Vida, 1964), with parsing the entire
document before translation as the first priority.
However, most SMT systems still treat parallel
corpora as a list of independent sentence-pairs and
ignore document-level information.
Document-level information can and should be
used to help document-level machine translation.
At least, the topic of a document can help choose
specific translation candidates, since when taken
out of the context from their document, some
words, phrases and even sentences may be rather
ambiguous and thus difficult to understand. Anoth-
er advantage of document-level machine transla-
tion is its ability in keeping a consistent translation.
However, document-level translation has drawn
little attention from the SMT research community.
The reasons are manifold. First of all, most of pa-
rallel corpora lack the annotation of document
boundaries (Tam, 2007). Secondly, although it is
easy to incorporate a new feature into the classical
log-linear model (Och, 2003), it is difficult to cap-
ture document-level information and model it via
some simple features. Thirdly, reference transla-
tions of a test document written by human transla-
tors tend to have flexible expressions in order to
avoid producing monotonous texts. This makes the
evaluation of document-level SMT systems ex-
tremely difficult.
Tiedemann (2010) showed that the repetition
and consistency are very important when modeling
natural language and translation. He proposed to
employ cache-based language and translation
models in a phrase-based SMT system for domain
</bodyText>
<page confidence="0.973824">
909
</page>
<note confidence="0.9579475">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999633246376812">
adaptation. Especially, the cache in the translation
model dynamically grows up by adding bilingual
phrase pairs from the best translation hypotheses of
previous sentences. One problem with the dynamic
cache is that those initial sentences in a test docu-
ment may not benefit from the dynamic cache.
Another problem is that the dynamic cache may be
prone to noise and cause error propagation. This
explains why the dynamic cache fails to much im-
prove the performance.
This paper proposes a cache-based approach for
document-level SMT using a static cache and a
dynamic cache. While such a approach applies to
both phrase-based and syntax-based SMT, this pa-
per focuses on phrase-based SMT. In particular,
the static cache is employed to store relevant bilin-
gual phrase pairs extracted from similar bilingual
document pairs (i.e. source documents similar to
the test document and their target counterparts) in
the training parallel corpus while the dynamic
cache is employed to store bilingual phrase pairs
from the best translation hypotheses of previous
sentences in the test document. In this way, our
cache-based approach can provide useful data at
the beginning of the translation process via the
static cache. As the translation process continues,
the dynamic cache grows and contributes more and
more to the translation of subsequent sentences.
Our motivation to employ similar bilingual doc-
ument pairs in the training parallel corpus is simple:
a human translator often collects similar bilingual
document pairs to help translation. If there are
translation pairs of sentences/phrases/words in
similar bilingual document pairs, this makes the
translation much easier. Given a test document, our
approach imitates this procedure by first retrieving
similar bilingual document pairs from the training
parallel corpus, which has often been applied in
IR-based adaptation of SMT systems (Zhao et
al.2004; Hildebrand et al.2005; Lu et al.2007) and
then extracting bilingual phrase pairs from similar
bilingual document pairs to store them in a static
cache.
However, such a cache-based approach may in-
troduce many noisy/unnecessary bilingual phrase
pairs in both the static and dynamic caches. In or-
der to resolve this problem, this paper employs a
topic model to weaken those noisy/unnecessary
bilingual phrase pairs by recommending the de-
coder to choose most likely phrase pairs according
to the topic words extracted from the target-side
text of similar bilingual document pairs. Just like a
human translator, even with a big bilingual dictio-
nary, is often confused when he meets a source
phrase which corresponds to several possible trans-
lations. In this case, some topic words can help
reduce the perplexity. In this paper, the topic words
are stored in a topic cache. In some sense, it has
the similar effect of employing an adaptive lan-
guage model with the advantage of avoiding the
interpolation of a global language model with a
specific domain language model.
The rest of this paper is organized as follows.
Section 2 reviews the related work. Section 3
presents our cache-based approach to document-
level SMT. Section 4 presents the experimental
results. Session 5 gives new insights on cache-
based document-level translation. Finally, we
conclude this paper in Section 6.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.997591866666667">
There are only a few studies on document-level
SMT. Representative work includes Zhao et al.
(2006), Tam et al. (2007), Carpuat (2009).
Zhao et al. (2006) assumed that the parallel sen-
tence pairs within a document pair constitute a
mixture of hidden topics and each word pair fol-
lows a topic-specific bilingual translation model. It
shows that the performance of word alignment can
be improved with the help of document-level in-
formation, which indirectly improves the quality of
SMT.
Tam et al. (2007) proposed a bilingual-LSA
model on the basis of a parallel document corpus
and built a topic-based language model for each
language. By automatically building the corres-
pondence between the source and target language
models, this method can match the topic-based
language model and improve the performance of
SMT.
Carpuat (2009) revisited the “one sense per dis-
course” hypothesis of Gale et al. (1992) and gave a
detailed comparison and analysis of the “one trans-
lation per discourse” hypothesis. However, she
failed to propose an effective way to integrate doc-
ument-level information into a SMT system. For
example, she simply recommended some transla-
tion candidates to replace some target words in the
post-process stage.
In principle, the cache-based approach can be
well suited for document-level translation. Basical-
</bodyText>
<page confidence="0.9935">
910
</page>
<bodyText confidence="0.9791985">
ly, the cache is analogous to “cache memory” in
hardware terminology, which tracks short-term
fluctuation (Iyer et al., 1999). As the cache
changes with different documents, the document-
level information should be capable of influencing
SMT.
Previous cache-based approaches mainly point
to cache-based language modeling (Kuhn and Mori,
1990), which uses a large global language model to
mix with a small local model estimated from recent
history data. However, applying such a language
model in SMT is very difficult due to the risk of
introducing extra noise (Raab, 2007).
For cache-based translation modeling, Nepveu
et al. (2004) explored user-edited translations in
the context of interactive machine translation. Tie-
demann (2010) proposed to fill the cache with bi-
lingual phrase pairs from the best translation
hypotheses of previous sentences in the test docu-
ment. Both Nepveu et al. (2004) and Tiedemann
(2010) also explored traditional cache-based lan-
guage models and found that a cache-based lan-
guage model often contributes much more than a
cache-based translation model.
</bodyText>
<sectionHeader confidence="0.95755" genericHeader="method">
3 Cache-based document-level SMT
</sectionHeader>
<bodyText confidence="0.84957">
Given a test document, our system works as fol-
lows:
</bodyText>
<listItem confidence="0.973976333333333">
1) clears the static, topic and dynamic caches
when switching to a new test document dx;
2) retrieves a set of most similar bilingual docu-
ment pairs dds for dx from the training parallel
corpus using the cosine similarity with tf-idf
weighting;
3) fills the static cache with bilingual phrase pairs
extracted from dds;
4) fills the topic cache with topic words extracted
from the target-side documents of dds;
5) for each sentence in the test document, trans-
lates it using cache-based SMT and conti-
</listItem>
<bodyText confidence="0.945363545454545">
nuously expands the dynamic cache with
bilingual phrase pairs obtained from the best
translation hypothesis of the previous sen-
tences.
In this way, our cache-based approach can pro-
vide useful data at the beginning of the translation
process via the static cache. As the translation
process continues, the dynamic cache grows and
contributes more and more to the translation of
subsequent sentences. Besides, the possibility of
choosing noisy/unnecessary bilingual phrase pairs
in both the static and dynamic caches is wakened
with the help of the topic words in the topic cache.
In particular, only the most similar document pair
is used to construct the static cache and the topic
cache unless specified.
In this section, we first introduce the basic
phrase-based SMT system and then present our
cache-based approach to achieve document-level
SMT with focus on constructing the caches (static,
dynamic and topic) and designing their corres-
ponding features.
</bodyText>
<subsectionHeader confidence="0.999809">
3.1 Basic phrase-based SMT system
</subsectionHeader>
<bodyText confidence="0.99993625">
It is well known that the translation process of
SMT can be modeled as obtaining the best transla-
tion e of the source sentence f by maximizing fol-
lowing posterior probability (Brown et al., 1993):
</bodyText>
<equation confidence="0.9964985">
ebest = argmaxP(e  |f)= argmaxP(f  |e)Plm(e) (1)
e e
</equation>
<bodyText confidence="0.9999254375">
where P(e|f) is a translation model and Plm is a lan-
guage model.
Our system adopted Moses (a state-of-art
phrase-based SMT system) as a baseline, which
follows Koehn et al. (2003) and mainly adopts six
groups of popular features: 1) two phrase transla-
tion probabilities (two directions): Pphr(e|f) and
Pphr(f|e); 2) two word translation probabilities (two
directions) : Pw(e|f) and Pw(f|e); 3) one language
model (target language): LM(e); 4) one phrase pe-
nalty (target language): PP(f); 5) one word penalty
(target language):WP(e); 6) a lexicalized reorder-
ing model. Besides, the log-linear model as de-
scribed in (Och and Ney, 2003) is employed to
linearly interpolate these features for obtaining the
best translation according to the formula (2):
</bodyText>
<equation confidence="0.995261">
M
ebest = argmax E, , mhje, f) } (2)
m=1
</equation>
<bodyText confidence="0.960533727272727">
where hm(e , f) is a feature function, and λm is the
weight of hm(e , f) optimized by a discriminative
training method on a held-out development data.
In principle, a phrase-based SMT system can
provide the best phrase segmentation and align-
ment that cover a bilingual sentence pair. Here, a
segmentation of a sentence into K phrases is de-
fined as:
(f~e)≈ EK j(fi, ei, ~) (3)
where tuple (fi, ei) refers to a phrase pair, and ~
indicates corresponding alignment information.
</bodyText>
<page confidence="0.988472">
911
</page>
<subsectionHeader confidence="0.996538">
3.2 Dynamic Cache
</subsectionHeader>
<bodyText confidence="0.999988571428571">
Our dynamic cache is mostly inspired by Tiede-
mann (2010), which adopts a dynamic cache to
store relevant bilingual phrase pairs from the best
translation hypotheses of previous sentences in the
test document. In particular, a specific feature is
incorporated Scache to capture useful document-
level information in the dynamic cache:
</bodyText>
<equation confidence="0.95758575">
=
K ( = )
i=1 c i
I f f
</equation>
<bodyText confidence="0.998953931034483">
where i
e−∂ is a decay factor to avoid the depen-
dence of the feature’s contribution on the cache
size. Given &lt;ec, fc&gt; an existing phrase pair in the
dynamic cache and &lt;ei,fi&gt; a phrase pair in a new
hypothesis, if ( ei=ec ∧ fi=fc ) is true (i.e. full match-
ing), function I(.) returns 1 , otherwise 0.
One problem with the dynamic cache in Tiede-
mann (2010) is that it continuously updates the
weight of a phrase pair in the dynamic cache. This
may cause noticeable computational burden with
the increasing number of phrase pairs in the dy-
namic cache. In addition, as a source phrase (fc)
may occur many times in the dynamic cache, the
weights for related phrase pairs may degrade se-
verely and thus his decoder needs a decay factor,
which is difficult to optimize. Finally, Tiedemann
(2010) only allowed full matching. This largely
lowers down the probability of hitting the dynamic
cache and thus much affects its effectiveness.
To overcome above problems, we only employ
the bilingual phrase pairs in the dynamic cache to
inform the decoder whether one bilingual phrase
pair exists in the dynamic cache or not, which is
slightly similar to (Nepveu et al, 2004) ,thus avoid-
ing extra computational burden and the fine-tuning
of the decay factor. In particular, following new
feature is incorporated to better explore the dynam-
ic cache:
</bodyText>
<equation confidence="0.994986">
Fd = EK 1 dpairmatch(ei, fi) (5)
</equation>
<bodyText confidence="0.537095">
where dpairmatch( ei, f )
</bodyText>
<equation confidence="0.9184085">
�1 (ei = ec ∧ fi = fc)
��V (e i = ec ∧ f j = fc ∧II ec II&gt; 3)
�V (ei = e c ∧ fi = f c ∧II ei II&gt; 3)
�0 other
</equation>
<bodyText confidence="0.999455090909091">
Here, Fd is called the dynamic cache feature.
Assume (ec,fc ) is a phrase pair in the dynamic
cache and (ei,fi) is a phrase pair candidate for a
new hypothesis. Besides full matching, we intro-
duce a symbol of “^” for sub-phrase, such as e i for
a sub-phrase of ei and e c for a sub-phrase of ec , to
allow partial matching. Finally, Fd measures the
overall value of a target candidate fi by summing
over the scores of K phrase pairs.
Obviously, Fd rewards both full matching and
partial matching. In order to avoid too much noise,
we put some constraints on the number of words in
the target phrase of &lt;ec,fc&gt; or &lt;ei,fi&gt;, such as
II ei II&gt; 3, where &amp;quot; IIII &amp;quot; measures the number of
non-blank characters in a phrase. For example, if
phrase pair “, 减少  and reduced” occurs in the
cache, phrase pair “, and” is not rewarded because
such shorter phrase pairs occur frequently and may
largely degrade the effect of the cache. In accor-
dance, the dynamic cache only contains phrase
pairs whose target phrases contain 4 or more non-
blank characters.
</bodyText>
<subsectionHeader confidence="0.998387">
3.3 Static Cache
</subsectionHeader>
<bodyText confidence="0.99996375">
In Tiedemann (2010), initial sentences in a test
document fail to benefit from the dynamic cache
due to the lack of contents in the dynamic cache at
the beginning of the translation process. To over-
come this problem, a static cache is included to
store relevant bilingual phrase pairs extracted from
similar bilingual document pairs in the training
parallel corpus. In particular, a static cache feature
F S is designed to capture useful information in the
static cache in the same way as the dynamic cache
feature, shown in Formula (5).
For this purpose, all the document pairs in the
training parallel corpus are aligned at the phrase
level using 2-fold cross-validation. That is, we
adopt 50% of the training parallel corpus to train a
model using Moses and apply the model to enforce
phrase alignment of the remaining training data,
and vice versa. Here, the enforcement is done by
guaranteeing the occurrence of the target phrase
candidate of a source phrase in the sentence pair.
Besides, all the words pairs trained on the whole
training parallel corpus are included in both folds
to ensure at least one possible translation. Finally,
the phrase pairs in the best translation hypothesis
of a sentence pair is retrieved from the decoder. In
this way, we can extract a set of phrase pairs for
each bilingual document pairs.
Given a test document, we first find a set of sim-
ilar source documents by computing the Cosine
similarity using the TF-IDF weighting scheme and
their corresponding target documents, from the
training parallel corpus. Then, the phrase pairs ex-
</bodyText>
<figure confidence="0.984791142857143">
(ec
Scache
 |fc)
I(&lt; ec, fc &gt;=&lt; ei, f,. &gt;)× e
K
E
i
=
1
i
−∂
E
(4)
=
</figure>
<page confidence="0.986299">
912
</page>
<bodyText confidence="0.99650825">
tracted from these similar bilingual document pairs
are collected into the static cache. To avoid noise,
we filter out those phrase pairs which occur less
than two times in the training parallel corpus.
</bodyText>
<tableCaption confidence="0.994917">
Table 1: Phrase pairs extracted from a document pair
with an economic topic
</tableCaption>
<bodyText confidence="0.999944090909091">
Similar to the dynamic cache, we only consider
those phrase pairs whose target phrases contain 4
or more non-blank characters to avoid noise. We
do not deliberately remove long phrase pairs. It is
possible to use these long phrase pairs if our test
document is very similar to one training document
pair. Table 1 shows some bilingual phrase pairs
extracted from a document pair, which reports a
piece of news about “impact on slowdown in US
economic growth”. Obviously, these phrase pairs
are closely related to economics.
</bodyText>
<subsectionHeader confidence="0.934598">
3.4 Topic Cache
</subsectionHeader>
<bodyText confidence="0.9958163">
Both the dynamic and static caches may still intro-
duce noisy/unnecessary bilingual phrase pairs even
with constraints on the length of phrases and their
occurrence frequency in the training parallel cor-
pus. In order to resolve this problem, this paper
adopts a topic cache to store relevant topic words
and employs a topic cache feature to weaken those
noisy/unnecessary phrase pairs.
Given wt is a topic word in the topic cache, the
topic cache feature Ft is designed as follows:
</bodyText>
<equation confidence="0.999274666666667">
K
Ft = topicexist(ei, fi) (6)
i=1
</equation>
<bodyText confidence="0.999538948717949">
where topicexist(ei, fi)
= j 1 (wt E e ,)
l 0 other
Here, the target phrase which contains a topic word
wt will be rewarded. wt is derived by a topic mod-
el, LDA (Latent Dirichlet Allocation). This is dif-
ferent from the previous work (Tam, 2007), which
mainly interpolated a topic language model with a
general language model and added additional two
adaptive lexicon probabilities in his phrase table.
In principle, LDA is a probabilistic model of
text data, which provides a generative analog of
PLSA (Blei et al., 2003), and is primarily meant to
reveal hidden topics in text documents. Like most
of the text mining techniques, LDA assumes that
documents are made up of words and the ordering
of the words within a document is unimportant (i.e.
the “bag-of-words” assumption).
Figure 1 shows the principle of LDA, where α is
the parameter of the uniform Dirichlet prior on the
per-document topic distributions, β is the parame-
ter of the uniform Dirichlet prior on the per-topic
word distribution, θi is the topic distribution for
document i, zij is the topic for the jth word in doc-
ument i, and wij is the specific word. Among all
variables, wij is the only observable variable with
all the other variables latent. In particular, K de-
notes the number of topics considered in the model
and tp is a K*V (V is the dimension of the vocabu-
lary) Markov matrix each line of which denotes the
word distribution of a topic. The inner plate over z
and w illustrates the repeated sampling of topics
and words until N words have been generated for
document d. The plate surrounding θ illustrates the
sampling of a distribution over topics for each
document d for a total of M documents. The plate
surrounding T illustrates the repeated sampling of
word distributions for each topic z until K topics
have been generated.
</bodyText>
<figureCaption confidence="0.998351">
Figure 1: LDA
</figureCaption>
<bodyText confidence="0.999943">
We use a LDA tool1 to build a topic model using
the target-side documents in the training parallel
corpus. Using LDA, we can obtain the topic distri-
bution of each word w, namely p(z|w) for topic z
EK. Moreover, using the obtained word topic dis-
tributions, we can infer the topic distribution of a
new document, namely p(z|d) for each topic z EK.
Given a test document, we first find the most
similar source document from the training data in
</bodyText>
<footnote confidence="0.968337">
1 http://www.arbylon.net/projects/
</footnote>
<page confidence="0.998601">
913
</page>
<bodyText confidence="0.654701045454546">
the same way as done in the static cache. After that,
we retrieve its corresponding target document.
Then, the topic of the target document is deter-
mined by its major topic, with the maximum p(z|d).
Finally, we load some topic words corresponding
to this topic z into the topic cache. In particular,
our LDA model deploy the setting of K=15, α=0.5
and R=0.1. Besides, only top 1000 topic words are
reserved for each topic. Table 2 shows top 10 topic
words for five topics.
Topic 1 Topic 2 Topic 3 Topic4 Topic5
company army party bush election
corporation armed represents united olympic
limited military study adminis- games
manager officers theory tration votes
board forces leadership policy bid
branch units political president gore
companies troops cadres clinton presi-
ltd force speech office dential
business soldiers comrade secretary party
personnel police central powell won
relations speech
</bodyText>
<tableCaption confidence="0.728828">
Table 2: Topic words extracted from target-side doc-
uments
</tableCaption>
<sectionHeader confidence="0.998159" genericHeader="method">
4 Experimentation
</sectionHeader>
<bodyText confidence="0.999919333333333">
We have systematically evaluated our cache-based
approach to document-level SMT on the Chinese-
English translation task.
</bodyText>
<subsectionHeader confidence="0.98393">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999949055555555">
Here, we use SRI language modeling toolkit to
train a trigram general language model on English
newswire text, mostly from the Xinhua portion of
the Gigaword corpus (2007) and performed word
alignment on the training parallel corpus using
GIZA++(Och and Ney,2000) in two directions. For
evaluation, the NIST BLEU script (version 13)
with the default setting is used to calculate the
Bleu score (Papineni et al. 2002), which measures
case-insensitive matching of n-grams with n up to
4. To see whether an improvement is statistically
significant, we also conduct significance tests us-
ing the paired bootstrap approach (Koehn, 2004)2.
In this paper, ‘***’, ‘**’, and ‘*’ denote
p-values less than or equal to 0.01, in-between
(0.01, 0.05), and bigger than 0.05, which mean
significantly better, moderately better and slightly
better, respectively.
</bodyText>
<footnote confidence="0.635427">
2 http://www.ark.cs.cmu.edu/MT
</footnote>
<bodyText confidence="0.999531">
In this paper, we use FBIS as the training data,
the 2003 NIST MT evaluation test data as the de-
velopment data, and the 2005 NIST MT test data
as the test data. Table 3 shows the statistics of
these data sets (with document boundaries anno-
tated).
</bodyText>
<table confidence="0.9982278">
Corpus Sentences Documents
Role Name
Train FBIS 239413 10353
Dev NIST2003 919 100
Test NIST2005 1082 100
</table>
<tableCaption confidence="0.999865">
Table 3: Corpus statistics
</tableCaption>
<bodyText confidence="0.999916833333333">
In particular, the sizes of the static, topic and
dynamic caches are fine-tuned to 2000, 1000 and
5000 items, respectively. For the dynamic cache,
we only keep those most recently-visited items,
while for the static cache; we always keep the most
frequently-occurring items.
</bodyText>
<subsectionHeader confidence="0.961069">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999129">
Table 4 shows the contribution of various caches in
our cache-based document-level SMT system. The
column of “BLEU_W” means the BLEU score
computed over the whole test set and “BLEU_D”
corresponds to the average BLEU score over sepa-
rated documents.
</bodyText>
<table confidence="0.997964181818182">
System BLEU on BLEU on Test(%)
Dev(%)
BLEU_W NIST BLEU_D
Moses 29.87 25.76 7.784 25.08
Fd 29.90 26.03 (*) 7.852 25.39
Fd+Fs 30.29 26.30 (**) 7.884 25.86
Fd+Ft 30.11 26.24 (**) 7.871 25.74
Fd+Fs+Ft 30.50 26.42 (***) 7.896 26.11
Fd+Fs+Ft - 26.57 (***) 7.901 26.32
with merg-
ing
</table>
<tableCaption confidence="0.991897">
Table 4: Contribution of various caches in our cache-
based document-level SMT system. Note that signific-
ance tests are done against Moses.
</tableCaption>
<bodyText confidence="0.959407444444444">
Contribution of dynamical cache (Fd)
Table 4 shows that the dynamic cache slightly im-
proves the performance by 0.27 (*) in BLEU_W.
This is similar to Tiedemann (2010). However,
detailed analysis indicates that the dynamic cache
does have negative effect on about one third of
documents, largely due to the instability of the dy-
namic cache at the beginning of translating a doc-
ument. Figure 2 shows the distribution of the
</bodyText>
<page confidence="0.996557">
914
</page>
<bodyText confidence="0.991709">
effectiveness of combining the dynamic and topic
caches (sorted by BLEU_D).
BLEU_D difference of 100 test documents (sorted
by BLEU_D). It shows that about 55% of test
documents benefit from the dynamic cache.
</bodyText>
<figureCaption confidence="0.993449">
Figure 2: Contribution of employing the dynamic
cache on different test documents
</figureCaption>
<bodyText confidence="0.977035307692308">
Contribution of static cache (Fs)
Table 4 shows that the combination of the static
cache with the dynamic cache further improves the
performance by 0.27(*) in BLEU_W. This sug-
gests the effectiveness of the static cache in elimi-
nating the instability of the dynamic cache when
translating first few sentences of a test document.
Together, the dynamic and static caches much im-
prove the performance by 0.54 (**) in BLEU_W
over Moses. Figure 3 shows the distribution of the
BLEU_D difference of 100 test documents (sorted
by BLEU_D), with more positive effect on those
borderline documents, compared to Figure 2.
</bodyText>
<figureCaption confidence="0.997613">
Figure 3: Contribution of combining the dynamic and
static cache on different test documents
</figureCaption>
<bodyText confidence="0.8442392">
Contribution of topic cache (Ft)
Table 4 shows that the topic cache has comparable
effect on improving the performance as the static
cache when combined with the dynamic cache
(0.48 vs. 0.54 in BLEU_W). Figure 4 shows the
</bodyText>
<figureCaption confidence="0.940948">
Figure 4: Contribution of combining the dynamic
and topic caches
</figureCaption>
<bodyText confidence="0.926582083333333">
However, detailed analysis shows that the topic
cache and the static cache are quite complementary
by contributing on different test documents, largely
due to that while the static cache tends to keep
translation consistent, the topic cache plays like a
document-specific language model. This is justi-
fied by Table 4 that the combination of the dynam-
ic, static and topic caches significantly improve the
performance by 0.66 (***) in BLEU_W, and by
Figure 5 that about 75% of test documents benefit
from the combination of the three caches (sorted
by BLEU_D).
</bodyText>
<figureCaption confidence="0.999342">
Figure 5: Contribution of combining the three caches
</figureCaption>
<bodyText confidence="0.945596444444444">
Contribution of merging phrase pairs of similar
document pairs
Here, the number of similar documents we adopt is
different from previous experiments. In the pre-
vious experiments, we only cache bilingual phrase
pairs extracted from the most similar document.
Here, we merge phrase pairs for several most simi-
lar documents (5 at most) which have the same
topic.
</bodyText>
<figure confidence="0.999195916666667">
8.00%
6.00%
4.00%
2.00%
0.00%
-2.00%
-4.00%
-6.00%
-8.00%
fd-moses
1
8
1522 29 36 43 50 57 64 71 78 85 92 99
6.00%
4.00%
2.00%
0.00%
-2.00%
-4.00%
-6.00%
15
8
fd+fs-moses
1
22 29 36 43 50 57 64 71 78 85 92 99
10.00%
8.00%
6.00%
4.00%
2.00%
0.00%
-2.00%
-4.00%
-6.00%
fd+ft-moses
1
7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97
1
9 17 25 33 41 49 57 65 73 81 89 97
-2.00%
-4.00%
fd+fs+ft-moses
10.00%
8.00%
6.00%
4.00%
2.00%
0.00%
</figure>
<page confidence="0.996725">
915
</page>
<bodyText confidence="0.989841">
Table 4 shows that employing this trick can fur-
ther improve the performance by 0.15 in BLEU_W.
As a result, the cache-based approach significantly
improve the performance by 0.81 (***) in
BLEU_W over Moses.
</bodyText>
<sectionHeader confidence="0.998034" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9986105">
In this section, we explore in more depth why the
static cache can help the dynamic cache, some
constrained factors which impact the effectiveness
of our cache-based approach.
</bodyText>
<subsectionHeader confidence="0.947546">
Effectiveness of the static cache
</subsectionHeader>
<bodyText confidence="0.999754">
We investigate why the static cache affects the per-
formance. Basically, it is difficult for the dynamic
cache to capture such similar information in the
static cache.
In principle, the static cache can both influence
the initial and subsequent sentences; however sub-
sequent ones can be affected by multiple caches. In
order to give an insight of the static cache, we eva-
luate its effectiveness on the first sentence for each
test document. Figure 6 shows the contribution of
the static cache on translating these first sentences
(y-axis shows BLEU value of the first sentence for
each test document). It notes that the most BLEU
scores of them are zeros because of the length limi-
tation of first sentences.
</bodyText>
<figureCaption confidence="0.858938">
Figure 6: Contribution of the static cache on the first
sentence of each test document
(i.e. with empty dynamic cache)
</figureCaption>
<bodyText confidence="0.9999412">
Furthermore, we count the hit (matching) fre-
quency of the static cache for each test documents.
Since we use 1 or 0 for the static cache feature, it is
easy to retrieve its effect for each test document.
Our statistics shows that the hit frequency on static
cache fluctuates between 5 and 18 for each test
document. Without the static cache, the hit fre-
quency of the dynamic cache is 504 on whole test
sets, this figure increases to 685 with the static
cache. This means that the static cache significant-
ly enlarges the effectiveness of the dynamic cache
by including more relevant phrase pairs to the dy-
namic cache, largely due to the positive impact of
the static cache on the initial sentences of each test
document.
</bodyText>
<subsectionHeader confidence="0.849376">
Size of topic cache
</subsectionHeader>
<bodyText confidence="0.998265666666667">
Table 5 shows the impact of the topic cache when
the number of the retained topic words for each
topic increases from 500 to 2000. It shows that too
more topic words actually harm the performance,
due to the increase of noise. 1000 topic words
seem a lot largely due to that we didn’t do stem-
ming for our topic modeling since we hope to in-
troduce some tense information of them in the
future.
</bodyText>
<figure confidence="0.952922">
Number of topic words BLEU_W
500 26.27
700 26.31
1000 26.42
1500 26.23
2000 26.19
</figure>
<tableCaption confidence="0.711049">
Table 5: Impact of the topic cache size
Influenced translations
</tableCaption>
<bodyText confidence="0.9999865">
In order to explore how our cache-based system
impacts on translation results, we manually in-
spected 5 documents respectively which is im-
proved or degraded in translation quality compared
to the baseline Moses output. Those documents
have 107 sentences in sum.
The good effectiveness of each kind of cache
can be observed by the example 1 and 2 showed in
Table 6. Both the example 1 and 2 come from the
same document whose “BLEU_D” score exceeds
Moses with 8.4 point. The example 1 benefits from
the topic cache which contains the item of “action”.
The example 2 benefits from the static cache which
contains a phrase pair of “*i4&amp;quot;  promised to”
while Moses use “commitment” for “*i4&amp;quot;” , which
may be the reason for missing the part of “prime
minister” in Moses output. Furthermore, due to the
phrase pair of “f-_?A 11x  the ceasefire agree-
ment” existing in our static cache, our decoder
keeps using “ceasefire” to translate “f-_?A” in the
whole document while Moses randomly use “cea-
sefire” or “cease-fire” for this translation.
</bodyText>
<figure confidence="0.997729583333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40 60 80 100 120
Fd FdFs
</figure>
<page confidence="0.990602">
916
</page>
<table confidence="0.898368787878788">
1 官员 预测 “ 准备 工作 将 会 进行 到 七月 , 然后 再 展开 政治 动作 ”
Moses: official forecasts said that preparatory work will be carried out in july and then launched a political maneuver .
Ours: official forecasts said that preparatory work will be carried out in july , then began a political action .
Reference: officials expected that &amp;quot;preparations would take place until July, after which political action will begin&amp;quot;.
2 关于 这 一 点 , 中东新闻社 说 , 以色列 总理 夏隆 承诺 “ 只 要 E*JIWJfl AP 41 停火 协议 , N
M 0 Vfl 的 tk)7 , 以色列 将 会 停ir Xlf E*JIWJfl ), 的 99 行动 ” o
Moses: on this point , said that israeli commitment to the palestinian authorities to respect the cease-fire agreement ,
where they are well under control , israel will stop its military actions against palestinians .
Ours: on this point , said that israeli prime minister promised to respect the ceasefire agreement , the palestinian au-
thorities to properly control their areas and israel will stop its military actions against palestinians .
Reference:For this point , MENA said Israeli Prime Minister Sharon has promised to &amp;quot; stop Israeli military operations
against the Palestinians insofar as they continue to respect the ceasefire deal and control their territory . &amp;quot;
3 17 Q R% ,A 3000 多 名 市R --Iq-: 市中,b 的 AZ I_* 观W T Eh 市政Rf *)� 的 ���� 的 ft�
R%会 ,TL-Al色 的 1TA R* 4 -* AN 的 -&amp;�g 9A ,著名 ft* -E*家 fl * T 不n tkC
的 Ra ft o
Moses: on the evening , nearly 3,000 residents in the downtown square of the weapons held by the municipal govern-
ment , watched a song and dance soiree , having colorful lighting disguise of ancient buildings around the square , sing-
ing and dancing famous artists staged different regions of ethnic song and dance .
Ours: later on, nearly 3,000 residents in the downtown square to watch the government of having a song and dance
performances were held under the disguise of colorful lighting around the square , a famous ancient buildings and local
artists of different ethnic song and dance .
Reference: On the night of the 17th , nearly 3,000 residents watched a wonderful gala of songs and dances , organized
by the municipal government , at Plaza da Armas . Colorful lights lighted up ancient architecture around the plaza .
Famous artists including singers and dancers staged performances of national songs and dances of different regions .
4 fq--q 的 JA市 _MV, E A 9JA Z X13 的 2.14 T)7��_q R展 到 2600 多 T)7�_q , ffU ),口 -9 增加
到 800 T7 2�L-Ai , f-KJ r �国 总),口 的 31% o
Moses: at lima &apos;s urban area from the beginning of 2600 square to 2.14 million square kilometers , while the popula-
tion has increased to 8 percent of the country &apos;s total , about 31% .
Ours: lima , the urban area from the beginning of 2600 square kilometers to 2.14 million square kilometers , but also
increased to about 8 million population , the country &apos;s total population of about 31% .
Reference: The area of Lima city has expanded to more than 2,600 square kilometers from the original 2.14 square ki-
lometers when the city was founded , while the population has increased to around 8 million , roughly accounting for
31% of the nation&apos;s total .
</table>
<tableCaption confidence="0.952766">
Table 6: Positive and negative examples
</tableCaption>
<bodyText confidence="0.999980181818182">
The example 3 and 4 also come from the same
document however whose performance degrades
with 2.17 point. We don’t think the translation
quality for example 4 in our system is worse than
Moses. However, the translation quality for exam-
ple 3 in our system is very bad and especially
showed on “re-ordering”. We found this sentence
did not match any item in our static cache and top-
ic cache. Although this phenomenon also happens
in other documents, but this is the most typical
negative example among these documents.
</bodyText>
<subsectionHeader confidence="0.683242">
Document-specific characteristics
</subsectionHeader>
<bodyText confidence="0.999990173913043">
It seems that using the same weight for the whole
test sets (all documents) is not very reasonable.
Actually, if we can determine those negative doc-
uments which are not suitable for the cache-based
approach, our cache-based approach may gain
much improvement. Tiedemann (2010) explored
the correlation to document length, baseline per-
formance and source document repetition. Howev-
er, it seems that there are no obvious rules to filter
out those negative documents. Besides, there may
be two more document-specific factors: repetition
of the reference text and document style.
Tiedemann (2010) only considered the repetition
of the test text in the source side. Since BLEU
score is computed against the reference text, the
repetition in the reference text may greatly influ-
ence the performance of our cache-based approach
to document-level SMT. As for document style, it
is quite possible that a document may contain sev-
eral topics. Therefore, it may be useful to track
such change over topics and refresh various caches
when there is a topic change. We will leave the
above issues to the future work.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996575">
We have shown that our cache-based approach
significantly improves the performance with the
help of various caches, such as the dynamic, static
and topic caches, although the cache-based ap-
</bodyText>
<page confidence="0.989057">
917
</page>
<bodyText confidence="0.998041823529412">
proach may introduce some negative impact on
BLEU scores for certain documents.
In the future, we will further explore how to re-
flect document divergence during training and dy-
namically adjust cache weights according to
different documents.
There are many useful components in training
documents, such as named entity, event and co-
reference. In this experiment, we only adopt the
flat data in our cache. However, the structured data
may improve the correctness of matching and thus
effectively avoid noise. We will explore more ef-
fective ways to pick up various kinds of useful in-
formation from the training parallel corpus to
expand our cache-based approach. Besides, we will
resort to comparable corpora to enlarge our cache-
based approach to document-level SMT.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997885">
This research was supported by Projects 90920004,
60873150, and 61003155 under the National Natu-
ral Science Foundation of China, Project
20093201110006 under the Specialized Research
Fund for the Doctoral Program of Higher Educa-
tion of China.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934536231885">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of machine
Learning Research 3, pages 993–1022.
Francis Bond. 2002. Toward a Science of Machine
Translation. Asian Association of Machine Transla-
tion (AAMT) Journal, N0.22, Tokyo, Japan, pages
12-20.
PF Brown, SA Della Pietra, VJ Della Pietra, RL Merc-
er.1992.The Mathematics of Statistical Machine
Translation: Parameter Estimation. Computational
Linguistics. 19(2):263-309.
Marine Carpuat. 2009. One Translation per Discourse.
In Proc. of the NAACL HLT workshop on Semantic
Evaluation, pages 19-26.
John DeNero, Alexandre Buchard-C&amp;quot;ot´e, and Dan
Klein. 2008. Sampling Alignment Structure under a
Bayesian Translation Model. In Proc. of EMNLP
2008, pages 314–323, Honolulu, October.
William A. Gale, Kenneth W. Church, and David Ya-
rowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural
Language, Harriman, NY.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel,and Alex Waibel. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation
based on Information Retrieval. Proceedings of
EAMT 2005:133-142.
Rukmini M. Iyer and Mari Ostendorf. 1999. Modeling
Long Distance Dependence in Language:Topic
Mixtures Versus Dynamic Cache Models. IEEE
Transactions on speech and audio processing, 7(1).
Philipp Koehn, Franz Josef Och ,and DanielMarcu.2003.
Statistical Phrase-Based Translation. Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 48-54.
Philipp Koehn.2004.Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388–395.
Roland Kuhn and Renato De Mori. 1990. A Cache-
based Natural Language Model for Speech Recogni-
tion. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence,12(6):570-583.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proc. of
EMNLP 2007,pages 343–350, Prague, Czech Repub-
lic, June.
Daniel Marcu and William Wong. 2002. A phrase-based
Joint Probability Model for Statistical Machine
Translation. In Proc. of EMNLP 2002, July.
Laurent Nepveu, Guy Lapalme, Philippe Langlais and
George Foster.2004. Adaptive Language and Trans-
lation Models for Interactive Machine Translation. In
Proc. of EMNLP 2004, pages 190-197.
Eugene A. Nida. 1964. Toward a Science of Translating.
Leiden, Netherlands:E.J.Brill.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL,
pages 160–167.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of ACL, pages
440–447.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proc. of
ACL02, pages 311–318.
Martin Raab. 2007. Language Modeling for Machine
Translation. VDM Verlag, Saarbrucken, Germany.
</reference>
<page confidence="0.978948">
918
</page>
<reference confidence="0.999925809523809">
G. Salton and C. Buckley. 1988. Term-weighting Ap-
proaches in Automatic Text Retrieval. Information
Processing and management,24(5):513-523,1988.
Yik-Cheung Tam, Ian Lane and Tanja Schultz. 2007.
Bilingual ISA-based Adaptation for Statistical Ma-
chine Translation. Machine Translation, 28:187-207.
Jorg Tiedemann. 2010. Context Adaptation in Statistical
Machine Translation Using Models with Exponen-
tially Decaying Cache. In Proc. of the 2010 work-
shop on domain adaptation for Natural Language
Processing, ACL 2010, pages 8-15.
Joern Wuebker and Arne Mauser and Hermann Ney.
2010. Training Phrase Translation Models with Leav-
ing-One-Out. In Proc. of ACL, pages 475-484.
Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language Model Adaptation for Statistical Ma-
chine Translation with Structured Query Models. In
COLING 2004, Geneva, August.
Bing Zhao and Eric P. Xing .2006. BiTAM:Bilingual
Topic Ad-Mixture Models for Word Alignment. In
Proc. of ACL2006.
</reference>
<page confidence="0.998778">
919
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279579">
<title confidence="0.99962">Cache-based Document-level Statistical Machine Translation</title>
<author confidence="0.999402">Min Guodong</author>
<affiliation confidence="0.597222">1School of Computer Science and</affiliation>
<note confidence="0.754602666666667">Soochow University, Suzhou, China 215006 2Institute for Infocomm Research, Singapore 138632 mzhang@i2r.a-star.edu.sg</note>
<abstract confidence="0.99823121875">Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>expand our cache-based approach. Besides, we will resort to comparable corpora to enlarge our cachebased approach to document-level SMT.</title>
<marker></marker>
<rawString>expand our cache-based approach. Besides, we will resort to comparable corpora to enlarge our cachebased approach to document-level SMT.</rawString>
</citation>
<citation valid="true">
<title>Acknowledgments This research was supported by Projects 90920004, 60873150,</title>
<date></date>
<booktitle>and 61003155 under the National Natural Science Foundation of China, Project 20093201110006 under the Specialized Research Fund for the Doctoral Program of Higher Education of</booktitle>
<marker></marker>
<rawString>Acknowledgments This research was supported by Projects 90920004, 60873150, and 61003155 under the National Natural Science Foundation of China, Project 20093201110006 under the Specialized Research Fund for the Doctoral Program of Higher Education of China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of machine Learning Research</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="19072" citStr="Blei et al., 2003" startWordPosition="3097" endWordPosition="3100">opic cache, the topic cache feature Ft is designed as follows: K Ft = topicexist(ei, fi) (6) i=1 where topicexist(ei, fi) = j 1 (wt E e ,) l 0 other Here, the target phrase which contains a topic word wt will be rewarded. wt is derived by a topic model, LDA (Latent Dirichlet Allocation). This is different from the previous work (Tam, 2007), which mainly interpolated a topic language model with a general language model and added additional two adaptive lexicon probabilities in his phrase table. In principle, LDA is a probabilistic model of text data, which provides a generative analog of PLSA (Blei et al., 2003), and is primarily meant to reveal hidden topics in text documents. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (i.e. the “bag-of-words” assumption). Figure 1 shows the principle of LDA, where α is the parameter of the uniform Dirichlet prior on the per-document topic distributions, β is the parameter of the uniform Dirichlet prior on the per-topic word distribution, θi is the topic distribution for document i, zij is the topic for the jth word in document i, and wij is the specific word</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of machine Learning Research 3, pages 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
</authors>
<title>Toward a Science of Machine Translation.</title>
<date>2002</date>
<journal>Asian Association of Machine Translation (AAMT) Journal,</journal>
<pages>12--20</pages>
<location>N0.22, Tokyo, Japan,</location>
<contexts>
<context position="1992" citStr="Bond (2002)" startWordPosition="284" endWordPosition="285">cument-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1 Introduction During last decade, tremendous work has been done to improve the quality of statistical machine * Corresponding author. translation (SMT) systems. However, there is still a huge performance gap between the state-of-theart SMT systems and human translators. Bond (2002) suggested nine ways to improve machine translation by imitating the best practices of human translators (Vida, 1964), with parsing the entire document before translation as the first priority. However, most SMT systems still treat parallel corpora as a list of independent sentence-pairs and ignore document-level information. Document-level information can and should be used to help document-level machine translation. At least, the topic of a document can help choose specific translation candidates, since when taken out of the context from their document, some words, phrases and even sentences</context>
</contexts>
<marker>Bond, 2002</marker>
<rawString>Francis Bond. 2002. Toward a Science of Machine Translation. Asian Association of Machine Translation (AAMT) Journal, N0.22, Tokyo, Japan, pages 12-20.</rawString>
</citation>
<citation valid="false">
<authors>
<author>PF Brown</author>
<author>SA Della Pietra</author>
<author>VJ Della Pietra</author>
</authors>
<booktitle>RL Mercer.1992.The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics.</booktitle>
<pages>19--2</pages>
<marker>Brown, Pietra, Pietra, </marker>
<rawString>PF Brown, SA Della Pietra, VJ Della Pietra, RL Mercer.1992.The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics. 19(2):263-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>One Translation per Discourse.</title>
<date>2009</date>
<booktitle>In Proc. of the NAACL HLT workshop on Semantic Evaluation,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7277" citStr="Carpuat (2009)" startWordPosition="1096" endWordPosition="1097"> employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents our cache-based approach to documentlevel SMT. Section 4 presents the experimental results. Session 5 gives new insights on cachebased document-level translation. Finally, we conclude this paper in Section 6. 2 Related work There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009). Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this me</context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. One Translation per Discourse. In Proc. of the NAACL HLT workshop on Semantic Evaluation, pages 19-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Buchard-Cot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling Alignment Structure under a Bayesian Translation Model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>314--323</pages>
<location>Honolulu,</location>
<marker>DeNero, Buchard-Cot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Buchard-C&amp;quot;ot´e, and Dan Klein. 2008. Sampling Alignment Structure under a Bayesian Translation Model. In Proc. of EMNLP 2008, pages 314–323, Honolulu, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One Sense per Discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<location>Harriman, NY.</location>
<contexts>
<context position="8046" citStr="Gale et al. (1992)" startWordPosition="1218" endWordPosition="1221"> topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this method can match the topic-based language model and improve the performance of SMT. Carpuat (2009) revisited the “one sense per discourse” hypothesis of Gale et al. (1992) and gave a detailed comparison and analysis of the “one translation per discourse” hypothesis. However, she failed to propose an effective way to integrate document-level information into a SMT system. For example, she simply recommended some translation candidates to replace some target words in the post-process stage. In principle, the cache-based approach can be well suited for document-level translation. Basical910 ly, the cache is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, th</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One Sense per Discourse. In Proceedings of the workshop on Speech and Natural Language, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval.</title>
<date>2005</date>
<booktitle>Proceedings of EAMT</booktitle>
<pages>2005--133</pages>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel. 2005. Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval. Proceedings of EAMT 2005:133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini M Iyer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Modeling Long Distance Dependence in Language:Topic Mixtures Versus Dynamic Cache Models.</title>
<date>1999</date>
<booktitle>IEEE Transactions on speech and audio processing,</booktitle>
<pages>7--1</pages>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>Rukmini M. Iyer and Mari Ostendorf. 1999. Modeling Long Distance Dependence in Language:Topic Mixtures Versus Dynamic Cache Models. IEEE Transactions on speech and audio processing, 7(1).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Franz Josef Och ,and DanielMarcu.2003. Statistical Phrase-Based Translation.</title>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<marker>Koehn, </marker>
<rawString>Philipp Koehn, Franz Josef Och ,and DanielMarcu.2003. Statistical Phrase-Based Translation. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp</author>
</authors>
<title>Koehn.2004.Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<marker>Philipp, 2004</marker>
<rawString>Philipp Koehn.2004.Statistical Significance Tests for Machine Translation Evaluation. In Proc. of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato De Mori</author>
</authors>
<title>A Cachebased Natural Language Model for Speech Recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine</journal>
<pages>12--6</pages>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>Roland Kuhn and Renato De Mori. 1990. A Cachebased Natural Language Model for Speech Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,12(6):570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP 2007,pages 343–350,</booktitle>
<location>Prague, Czech Republic,</location>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proc. of EMNLP 2007,pages 343–350, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP</booktitle>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based Joint Probability Model for Statistical Machine Translation. In Proc. of EMNLP 2002, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>Guy Lapalme</author>
<author>Philippe Langlais</author>
<author>George Foster 2004</author>
</authors>
<title>Adaptive Language and Translation Models for Interactive Machine Translation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>190--197</pages>
<contexts>
<context position="9102" citStr="Nepveu et al. (2004)" startWordPosition="1379" endWordPosition="1382">e is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the documentlevel information should be capable of influencing SMT. Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data. However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007). For cache-based translation modeling, Nepveu et al. (2004) explored user-edited translations in the context of interactive machine translation. Tiedemann (2010) proposed to fill the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. Both Nepveu et al. (2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model. 3 Cache-based document-level SMT Given a test document, our system works as follows: 1) clears the static, topic and dynamic caches when switching to </context>
<context position="14177" citStr="Nepveu et al, 2004" startWordPosition="2225" endWordPosition="2228">c cache. In addition, as a source phrase (fc) may occur many times in the dynamic cache, the weights for related phrase pairs may degrade severely and thus his decoder needs a decay factor, which is difficult to optimize. Finally, Tiedemann (2010) only allowed full matching. This largely lowers down the probability of hitting the dynamic cache and thus much affects its effectiveness. To overcome above problems, we only employ the bilingual phrase pairs in the dynamic cache to inform the decoder whether one bilingual phrase pair exists in the dynamic cache or not, which is slightly similar to (Nepveu et al, 2004) ,thus avoiding extra computational burden and the fine-tuning of the decay factor. In particular, following new feature is incorporated to better explore the dynamic cache: Fd = EK 1 dpairmatch(ei, fi) (5) where dpairmatch( ei, f ) �1 (ei = ec ∧ fi = fc) ��V (e i = ec ∧ f j = fc ∧II ec II&gt; 3) �V (ei = e c ∧ fi = f c ∧II ei II&gt; 3) �0 other Here, Fd is called the dynamic cache feature. Assume (ec,fc ) is a phrase pair in the dynamic cache and (ei,fi) is a phrase pair candidate for a new hypothesis. Besides full matching, we introduce a symbol of “^” for sub-phrase, such as e i for a sub-phrase </context>
</contexts>
<marker>Nepveu, Lapalme, Langlais, 2004, 2004</marker>
<rawString>Laurent Nepveu, Guy Lapalme, Philippe Langlais and George Foster.2004. Adaptive Language and Translation Models for Interactive Machine Translation. In Proc. of EMNLP 2004, pages 190-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene A Nida</author>
</authors>
<title>Toward a Science of Translating.</title>
<date>1964</date>
<location>Leiden, Netherlands:E.J.Brill.</location>
<marker>Nida, 1964</marker>
<rawString>Eugene A. Nida. 1964. Toward a Science of Translating. Leiden, Netherlands:E.J.Brill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in</title>
<date>2003</date>
<pages>160--167</pages>
<contexts>
<context position="3082" citStr="Och, 2003" startWordPosition="445" endWordPosition="446"> translation candidates, since when taken out of the context from their document, some words, phrases and even sentences may be rather ambiguous and thus difficult to understand. Another advantage of document-level machine translation is its ability in keeping a consistent translation. However, document-level translation has drawn little attention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Confere</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>440--447</pages>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of ACL, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="22394" citStr="Papineni et al. 2002" startWordPosition="3650" endWordPosition="3653">ords extracted from target-side documents 4 Experimentation We have systematically evaluated our cache-based approach to document-level SMT on the ChineseEnglish translation task. 4.1 Experimental Setting Here, we use SRI language modeling toolkit to train a trigram general language model on English newswire text, mostly from the Xinhua portion of the Gigaword corpus (2007) and performed word alignment on the training parallel corpus using GIZA++(Och and Ney,2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the Bleu score (Papineni et al. 2002), which measures case-insensitive matching of n-grams with n up to 4. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004)2. In this paper, ‘***’, ‘**’, and ‘*’ denote p-values less than or equal to 0.01, in-between (0.01, 0.05), and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively. 2 http://www.ark.cs.cmu.edu/MT In this paper, we use FBIS as the training data, the 2003 NIST MT evaluation test data as the development data, and the 2005 NIST MT test d</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proc. of ACL02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Raab</author>
</authors>
<title>Language Modeling for Machine Translation.</title>
<date>2007</date>
<pages>918</pages>
<publisher>VDM Verlag, Saarbrucken,</publisher>
<contexts>
<context position="9042" citStr="Raab, 2007" startWordPosition="1373" endWordPosition="1374">document-level translation. Basical910 ly, the cache is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the documentlevel information should be capable of influencing SMT. Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data. However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007). For cache-based translation modeling, Nepveu et al. (2004) explored user-edited translations in the context of interactive machine translation. Tiedemann (2010) proposed to fill the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. Both Nepveu et al. (2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model. 3 Cache-based document-level SMT Given a test document, our system works as follows: 1) cl</context>
</contexts>
<marker>Raab, 2007</marker>
<rawString>Martin Raab. 2007. Language Modeling for Machine Translation. VDM Verlag, Saarbrucken, Germany. 918</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<date>1988</date>
<booktitle>Term-weighting Approaches in Automatic Text Retrieval. Information Processing and</booktitle>
<pages>24--5</pages>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting Approaches in Automatic Text Retrieval. Information Processing and management,24(5):513-523,1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<date>2007</date>
<booktitle>Bilingual ISA-based Adaptation for Statistical Machine Translation. Machine Translation,</booktitle>
<pages>28--187</pages>
<contexts>
<context position="7261" citStr="Tam et al. (2007)" startWordPosition="1092" endWordPosition="1095">e similar effect of employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents our cache-based approach to documentlevel SMT. Section 4 presents the experimental results. Session 5 gives new insights on cachebased document-level translation. Finally, we conclude this paper in Section 6. 2 Related work There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009). Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane and Tanja Schultz. 2007. Bilingual ISA-based Adaptation for Statistical Machine Translation. Machine Translation, 28:187-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 workshop on domain adaptation for Natural Language</booktitle>
<contexts>
<context position="3430" citStr="Tiedemann (2010)" startWordPosition="498" endWordPosition="499">tention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics adaptation. Especially, the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences. One problem</context>
<context position="9204" citStr="Tiedemann (2010)" startWordPosition="1393" endWordPosition="1395">, 1999). As the cache changes with different documents, the documentlevel information should be capable of influencing SMT. Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data. However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007). For cache-based translation modeling, Nepveu et al. (2004) explored user-edited translations in the context of interactive machine translation. Tiedemann (2010) proposed to fill the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. Both Nepveu et al. (2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model. 3 Cache-based document-level SMT Given a test document, our system works as follows: 1) clears the static, topic and dynamic caches when switching to a new test document dx; 2) retrieves a set of most similar bilingual document pairs dds for dx from th</context>
<context position="12718" citStr="Tiedemann (2010)" startWordPosition="1970" endWordPosition="1972">g to the formula (2): M ebest = argmax E, , mhje, f) } (2) m=1 where hm(e , f) is a feature function, and λm is the weight of hm(e , f) optimized by a discriminative training method on a held-out development data. In principle, a phrase-based SMT system can provide the best phrase segmentation and alignment that cover a bilingual sentence pair. Here, a segmentation of a sentence into K phrases is defined as: (f~e)≈ EK j(fi, ei, ~) (3) where tuple (fi, ei) refers to a phrase pair, and ~ indicates corresponding alignment information. 911 3.2 Dynamic Cache Our dynamic cache is mostly inspired by Tiedemann (2010), which adopts a dynamic cache to store relevant bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. In particular, a specific feature is incorporated Scache to capture useful documentlevel information in the dynamic cache: = K ( = ) i=1 c i I f f where i e−∂ is a decay factor to avoid the dependence of the feature’s contribution on the cache size. Given &lt;ec, fc&gt; an existing phrase pair in the dynamic cache and &lt;ei,fi&gt; a phrase pair in a new hypothesis, if ( ei=ec ∧ fi=fc ) is true (i.e. full matching), function I(.) returns 1 , otherwise 0. </context>
<context position="15594" citStr="Tiedemann (2010)" startWordPosition="2502" endWordPosition="2503">h full matching and partial matching. In order to avoid too much noise, we put some constraints on the number of words in the target phrase of &lt;ec,fc&gt; or &lt;ei,fi&gt;, such as II ei II&gt; 3, where &amp;quot; IIII &amp;quot; measures the number of non-blank characters in a phrase. For example, if phrase pair “, 减少 and reduced” occurs in the cache, phrase pair “, and” is not rewarded because such shorter phrase pairs occur frequently and may largely degrade the effect of the cache. In accordance, the dynamic cache only contains phrase pairs whose target phrases contain 4 or more nonblank characters. 3.3 Static Cache In Tiedemann (2010), initial sentences in a test document fail to benefit from the dynamic cache due to the lack of contents in the dynamic cache at the beginning of the translation process. To overcome this problem, a static cache is included to store relevant bilingual phrase pairs extracted from similar bilingual document pairs in the training parallel corpus. In particular, a static cache feature F S is designed to capture useful information in the static cache in the same way as the dynamic cache feature, shown in Formula (5). For this purpose, all the document pairs in the training parallel corpus are alig</context>
<context position="24362" citStr="Tiedemann (2010)" startWordPosition="3970" endWordPosition="3971">BLEU score over separated documents. System BLEU on BLEU on Test(%) Dev(%) BLEU_W NIST BLEU_D Moses 29.87 25.76 7.784 25.08 Fd 29.90 26.03 (*) 7.852 25.39 Fd+Fs 30.29 26.30 (**) 7.884 25.86 Fd+Ft 30.11 26.24 (**) 7.871 25.74 Fd+Fs+Ft 30.50 26.42 (***) 7.896 26.11 Fd+Fs+Ft - 26.57 (***) 7.901 26.32 with merging Table 4: Contribution of various caches in our cachebased document-level SMT system. Note that significance tests are done against Moses. Contribution of dynamical cache (Fd) Table 4 shows that the dynamic cache slightly improves the performance by 0.27 (*) in BLEU_W. This is similar to Tiedemann (2010). However, detailed analysis indicates that the dynamic cache does have negative effect on about one third of documents, largely due to the instability of the dynamic cache at the beginning of translating a document. Figure 2 shows the distribution of the 914 effectiveness of combining the dynamic and topic caches (sorted by BLEU_D). BLEU_D difference of 100 test documents (sorted by BLEU_D). It shows that about 55% of test documents benefit from the dynamic cache. Figure 2: Contribution of employing the dynamic cache on different test documents Contribution of static cache (Fs) Table 4 shows </context>
<context position="34886" citStr="Tiedemann (2010)" startWordPosition="5833" endWordPosition="5834">slation quality for example 3 in our system is very bad and especially showed on “re-ordering”. We found this sentence did not match any item in our static cache and topic cache. Although this phenomenon also happens in other documents, but this is the most typical negative example among these documents. Document-specific characteristics It seems that using the same weight for the whole test sets (all documents) is not very reasonable. Actually, if we can determine those negative documents which are not suitable for the cache-based approach, our cache-based approach may gain much improvement. Tiedemann (2010) explored the correlation to document length, baseline performance and source document repetition. However, it seems that there are no obvious rules to filter out those negative documents. Besides, there may be two more document-specific factors: repetition of the reference text and document style. Tiedemann (2010) only considered the repetition of the test text in the source side. Since BLEU score is computed against the reference text, the repetition in the reference text may greatly influence the performance of our cache-based approach to document-level SMT. As for document style, it is qui</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>Jorg Tiedemann. 2010. Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache. In Proc. of the 2010 workshop on domain adaptation for Natural Language</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>