<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002011">
<title confidence="0.9981915">
Coarse-to-Fine Syntactic Machine Translation
using Language Projections
</title>
<author confidence="0.994239">
Slav Petrov Aria Haghighi Dan Klein
</author>
<affiliation confidence="0.9981315">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<address confidence="0.796926">
Berkeley, CA 94720
</address>
<email confidence="0.99041">
{petrov, aria42, klein}@eecs.berkeley.edu
</email>
<sectionHeader confidence="0.998282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.971614058823529">
The intersection of tree transducer-based
translation models with n-gram language
models results in huge dynamic programs for
machine translation decoding. We propose a
multipass, coarse-to-fine approach in which
the language model complexity is incremen-
tally introduced. In contrast to previous order-
based bigram-to-trigram approaches, we fo-
cus on encoding-based methods, which use
a clustered encoding of the target language.
Across various encoding schemes, and for
multiple language pairs, we show speed-ups of
up to 50 times over single-pass decoding while
improving BLEU score. Moreover, our entire
decoding cascade for trigram language models
is faster than the corresponding bigram pass
alone of a bigram-to-trigram decoder.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998393877551">
In the absence of an n-gram language model, decod-
ing a synchronous CFG translation model is very
efficient, requiring only a variant of the CKY al-
gorithm. As in monolingual parsing, dynamic pro-
gramming items are simply indexed by a source lan-
guage span and a syntactic label. Complexity arises
when n-gram language model scoring is added, be-
cause items must now be distinguished by their ini-
tial and final few target language words for purposes
of later combination. This lexically exploded search
space is a root cause of inefficiency in decoding, and
several methods have been suggested to combat it.
The approach most relevant to the current work is
Zhang and Gildea (2008), which begins with an ini-
tial bigram pass and uses the resulting chart to guide
a final trigram pass. Substantial speed-ups are ob-
tained, but computation is still dominated by the ini-
tial bigram pass. The key challenge is that unigram
models are too poor to prune well, but bigram mod-
els are already huge. In short, the problem is that
there are too many words in the target language. In
this paper, we propose a new, coarse-to-fine, mul-
tipass approach which allows much greater speed-
ups by translating into abstracted languages. That
is, rather than beginning with a low-order model of
a still-large language, we exploit language projec-
tions, hierarchical clusterings of the target language,
to effectively reduce the size of the target language.
In this way, initial passes can be very quick, with
complexity phased in gradually.
Central to coarse-to-fine language projection is
the construction of sequences of word clusterings
(see Figure 1). The clusterings are deterministic
mappings from words to clusters, with the property
that each clustering refines the previous one. There
are many choice points in this process, including
how these clusterings are obtained and how much
refinement is optimal for each pass. We demon-
strate that likelihood-based hierarchical EM train-
ing (Petrov et al., 2006) and cluster-based language
modeling methods (Goodman, 2001) are superior
to both rank-based and random-projection methods.
In addition, we demonstrate that more than two
passes are beneficial and show that our computa-
tion is equally distributed over all passes. In our
experiments, passes with less than 16-cluster lan-
guage models are most advantageous, and even a
single pass with just two word clusters can reduce
decoding time greatly.
</bodyText>
<page confidence="0.981092">
108
</page>
<note confidence="0.9621735">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999694266666667">
To follow related work and to focus on the effects
of the language model, we present translation re-
sults under an inversion transduction grammar (ITG)
translation model (Wu, 1997) trained on the Eu-
roparl corpus (Koehn, 2005), described in detail in
Section 3, and using a trigram language model. We
show that, on a range of languages, our coarse-to-
fine decoding approach greatly outperforms base-
line beam pruning and bigram-to-trigram pruning on
time-to-BLEU plots, reducing decoding times by up
to a factor of 50 compared to single pass decoding.
In addition, coarse-to-fine decoding increases BLEU
scores by up to 0.4 points. This increase is a mixture
of improved search and subtly advantageous coarse-
to-fine effects which are further discussed below.
</bodyText>
<sectionHeader confidence="0.997696" genericHeader="introduction">
2 Coarse-to-Fine Decoding
</sectionHeader>
<bodyText confidence="0.999962">
In coarse-to-fine decoding, we create a series of ini-
tially simple but increasingly complex search prob-
lems. We then use the solutions of the simpler prob-
lems to prune the search spaces for more complex
models, reducing the total computational cost.
</bodyText>
<sectionHeader confidence="0.941236" genericHeader="related work">
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.983954043478261">
Taken broadly, the coarse-to-fine approach is not
new to machine translation (MT) or even syntactic
MT. Many common decoder precomputations can
be seen as coarse-to-fine methods, including the A*-
like forward estimates used in the Moses decoder
(Koehn et al., 2007). In an ITG framework like
ours, Zhang and Gildea (2008) consider an approach
in which the results of a bigram pass are used as
an A* heuristic to guide a trigram pass. In their
two-pass approach, the coarse bigram pass becomes
computationally dominant. Our work differs in two
ways. First, we use posterior pruning rather than
A* search. Unlike A* search, posterior pruning
allows multipass methods. Not only are posterior
pruning methods simpler (for example, there is no
need to have complex multipart bounds), but they
can be much more effective. For example, in mono-
lingual parsing, posterior pruning methods (Good-
man, 1997; Charniak et al., 2006; Petrov and Klein,
2007) have led to greater speedups than their more
cautious A* analogues (Klein and Manning, 2003;
Haghighi et al., 2007), though at the cost of guaran-
teed optimality.
</bodyText>
<figureCaption confidence="0.9051614">
Figure 2: Possible state projections 7r for the target noun
phrase “the report for these states” using the clusters
from Figure 1. The number of bits used to encode the tar-
get language vocabulary is varied along the x-axis. The
language model order is varied along the y-axis.
</figureCaption>
<bodyText confidence="0.9998378125">
Second, we focus on an orthogonal axis of ab-
straction: the size of the target language. The in-
troduction of abstract languages gives better control
over the granularity of the search space and provides
a richer set of intermediate problems, allowing us
to adapt the level of refinement of the intermediate,
coarse passes to minimize total computation.
Beyond coarse-to-fine approaches, other related
approaches have also been demonstrated for syntac-
tic MT. For example, Venugopal et al. (2007) con-
siders a greedy first pass with a full model followed
by a second pass which bounds search to a region
near the greedy results. Huang and Chiang (2007)
searches with the full model, but makes assumptions
about the the amount of reordering the language
model can trigger in order to limit exploration.
</bodyText>
<subsectionHeader confidence="0.991514">
2.2 Language Model Projections
</subsectionHeader>
<bodyText confidence="0.9999236">
When decoding in a syntactic translation model with
an n-gram language model, search states are spec-
ified by a grammar nonterminal X as well as the
the n-1 left-most target side words ln−1, ... , l1 and
right-most target side words r1, ... , rn−1 of the gen-
erated hypothesis. We denote the resulting lexical-
ized state as ln−1, ... , l1-X-r1, ... , rn−1. Assum-
ing a vocabulary V and grammar symbol set G, the
state space size is up to |V |2(n−1)|G|, which is im-
mense for a large vocabulary when n &gt; 1. We
consider two ways to reduce the size of this search
space. First, we can reduce the order of the lan-
guage model. Second, we can reduce the number
of words in the vocabulary. Both can be thought
of as projections of the search space to smaller ab-
</bodyText>
<figure confidence="0.943094055555556">
0-NP-1 01-NP-10 010-NP-100
the-NP-states
0,1-NP-0,1 01,10-NP-00,10 010,100-NP-000,100
thejeport-NP-these,states
...
�
3
2
LM Order
1 2 3 ... ∞
Bits in language model
109
0 1
00 01
000 001 010 011 100 101 110 111
10 11
these we the that states of also will
one they a for report to been must
</figure>
<figureCaption confidence="0.949951">
Figure 1: An example of hierarchical clustering of target language vocabulary (see Section 4). Even with a small
number of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes.
</figureCaption>
<bodyText confidence="0.999476974358974">
stracted spaces. Figure 2 illustrates those two or-
thogonal axes of abstraction.
Order-based projections are simple. As shown
in Figure 2, they simply strip off the appropriate
words from each state, collapsing dynamic program-
ming items which are identical from the standpoint
of their left-to-right combination in the lower or-
der language model. However, having only order-
based projections is very limiting. Zhang and Gildea
(2008) found that their computation was dominated
by their bigram pass. The only lower-order pass
possible uses a unigram model, which provides no
information about the interaction of the language
model and translation model reorderings. We there-
fore propose encoding-based projections. These
projections reduce the size of the target language vo-
cabulary by deterministically projecting each target
language word to a word cluster. This projection ex-
tends to the whole search state in the obvious way:
assuming a bigram language model, the state l-X-r
projects to c(l)-X-c(r), where c(·) is the determin-
istic word-to-cluster mapping.
In our multipass approach, we will want a se-
quence c1 ... cn of such projections. This requires a
hierarchical clustering of the target words, as shown
in Figure 1. Each word’s cluster membership can be
represented by an n-bit binary string. Each prefix of
length k declares that word’s cluster assignment at
the k-bit level. As we vary k, we obtain a sequence
of projections ck(·), each one mapping words to a
more refined clustering. When performing inference
in a k-bit projection, we replace the detailed original
language model over words with a coarse language
model LMk over the k-bit word clusters. In addition,
we replace the phrase table with a projected phrase
table, which further increases the speed of projected
passes. In Section 4, we describe the various clus-
tering schemes explored, as well as how the coarse
LMk are estimated.
</bodyText>
<subsectionHeader confidence="0.998567">
2.3 Multipass Decoding
</subsectionHeader>
<bodyText confidence="0.999889482758621">
Unlike previous work, where the state space exists
only at two levels of abstraction (i.e. bigram and tri-
gram), we have multiple levels to choose from (Fig-
ure 2). Because we use both encoding-based and
order-based projections, our options form a lattice
of coarser state spaces, varying from extremely sim-
ple (a bigram model with just two word clusters) to
nearly the full space (a trigram model with 10 bits or
1024 word clusters).
We use this lattice to perform a series of coarse
passes with increasing complexity. More formally,
we decode a source sentence multiple times, in a
sequence of state spaces S0, S1, ... , Sn=S, where
each Si is a refinement of Si_1 in either language
model order, language encoding size, or both. The
state spaces Si and Sj (i &lt; j) are related to each
other via a projection operator 7rj,i(·) which maps
refined states deterministically to coarser states.
We start by decoding an input x in the simplest
state space S0. In particular, we compute the chart
of the posterior distributions p0(s) = P(s|x) for all
states s E S0. These posteriors will be used to prune
the search space S1 of the following pass. States s
whose posterior falls below a threshold t trigger the
removal of all more refined states s&apos; in the subse-
quent pass (see Figure 3). This technique is poste-
rior pruning, and is different from A* methods in
two main ways. First, it can be iterated in a multi-
pass setting, and, second, it is generally more effi-
</bodyText>
<page confidence="0.974427">
110
</page>
<figure confidence="0.999606333333333">
1-Bit Pass 0-X-0
1-X-0
0-X-1
1-X-1
&lt; t ? &lt; t ? &lt; t ? &lt; t ?
00-X-01
01-X-01
11-X-01
00-X-10
01-X-10
00-X-11
01-X-11
10-X-10
2-Bit Pass
00-X-00 01-X-00
10-X-00 11-X-00 10-X-01
11-X-10 10-X-11 11-X-11
&lt; t ? &lt; t ? &lt; t ? &lt; t ? &lt; t ? &lt; t ? &lt; t ? &lt; t ?
</figure>
<figureCaption confidence="0.984066333333333">
Figure 3: Example of state pruning in coarse-to-fine decoding using the language encoding projection (see Section 2.2).
During the coarse one-bit word cluster pass, two of the four possible states are pruned. Every extension of the pruned
one-bit states (indicated by the grey shading) are not explored during the two-bit word cluster pass.
</figureCaption>
<bodyText confidence="0.9992481">
cient with a potential cost of increased search errors
(see Section 2.1 for more discussion).
Looking at Figure 2, multipass coarse-to-fine de-
coding can be visualized as a walk from a coarse
point somewhere in the lower left to the most re-
fined point in the upper right of the grid. Many
coarse-to-fine schedules are possible. In practice,
we might start decoding with a 1-bit word bigram
pass, followed by an 3-bit word bigram pass, fol-
lowed by a 5-bit word trigram pass and so on (see
Section 5.3 for an empirical investigation). In terms
if time, we show that coarse-to-fine gives substantial
speed-ups. There is of course an additional mem-
ory requirement, but it is negligible. As we will see
in our experiments (Section 5) the largest gains can
be obtained with extremely coarse language mod-
els. In particular, the largest coarse model we use in
our best multipass decoder uses a 4-bit encoding and
hence has only 16 distinct words (or at most 4096
trigrams).
</bodyText>
<sectionHeader confidence="0.997768" genericHeader="method">
3 Inversion Transduction Grammars
</sectionHeader>
<bodyText confidence="0.999165634615385">
While our approach applies in principle to a vari-
ety of machine translation systems (phrase-based or
syntactic), we will use the inversion transduction
grammar (ITG) approach of Wu (1997) to facili-
tate comparison with previous work (Zens and Ney,
2003; Zhang and Gildea, 2008) as well as to focus on
language model complexity. ITGs are a subclass of
synchronous context-free grammars (SCFGs) where
there are only three kinds of rules. Preterminal unary
productions produce terminal strings on both sides
(words or phrases): X —* e/f. Binary in-order pro-
ductions combine two phrases monotonically (X —*
[Y Z]). Finally, binary inverted productions invert
the order of their children (X —* (Y Z)). These pro-
ductions are associated with rewrite weights in the
standard way.
Without a language model, SCFG decoding is just
like (monolingual) CFG parsing. The dynamic pro-
gramming states are specified by ZXj, where (i, j) is
a source sentence span and X is a nonterminal. The
only difference is that whenever we apply a CFG
production on the source side, we need to remem-
ber the corresponding synchronous production on
the target side and store the best obtainable transla-
tion via a backpointer. See Wu (1996) or Melamed
(2004) for a detailed exposition.
Once we integrate an n-gram language model, the
state space becomes lexicalized and combining dy-
namic programming items becomes more difficult.
Each state is now parametrized by the initial and
final n−1 words in the target language hypothesis:
ln−1, ..., l1-ZXj-r1, ..., rn−1. Whenever we combine
two dynamic programming items, we need to score
the fluency of their concatentation by incorporat-
ing the score of any language model features which
cross the target side boundaries of the two concate-
nated items (Chiang, 2005). Decoding with an in-
tegrated language model is computationally expen-
sive for two reasons: (1) the need to keep track of
a large number of lexicalized hypotheses for each
source span, and (2) the need to frequently query the
large language model for each hypothesis combina-
tion.
Multipass coarse-to-fine decoding can alleviate
both computational issues. We start by decoding
in an extremely coarse bigram search space, where
there are very few possible translations. We com-
pute standard inside/outside probabilities (i5/o5),
as follows. Consider the application of non-inverted
binary rule: we combine two items lb-ZBk-rb and
lc-kCj-rc spanning (i, k) and (k, j) respectively to
form a larger item lb-ZAj-rc, spanning (i, j). The
</bodyText>
<page confidence="0.957766">
111
</page>
<figure confidence="0.66176">
iS(lb-iAi-r�) += p(X→[YZ]) ·iS(lb-iBk-Tb) ·LM(rb, l�) · iS(l,-kCj-r,)
</figure>
<figureCaption confidence="0.995524666666667">
Figure 4: Monotonic combination of two hypotheses dur-
ing the inside pass involves scoring the fluency of the con-
catenation with the language model.
</figureCaption>
<bodyText confidence="0.765885">
inside score of the new item is incremented by:
</bodyText>
<equation confidence="0.9994685">
iS(lb-iAj-rc) += p(X —* [Y ZD · iS(lb-iBk-rb) ·
iS(lc-kCj-rc) · LM(rb, lc)
</equation>
<bodyText confidence="0.999833125">
This process is also illustrated in Figure 4. Of
course, we also loop over the split point k and ap-
ply the other two rule types (inverted concatenation,
terminal generation). We omit those cases from this
exposition, as well as the update for the outside pass;
they are standard and similar. Once we have com-
puted the inside and outside scores, we compute pos-
terior probabilities for all items:
</bodyText>
<equation confidence="0.9948215">
iS(la-iAj-ra)oS(la-iAj-ra)
p(la-iAj-ra) = iS(root)
</equation>
<bodyText confidence="0.999892727272727">
where iS(root) is sum of all translations’ scores.
States with low posteriors are then pruned away.
We proceed to compute inside/outside score in the
next, more refined search space, using the projec-
tions 7ri,i−1 to map between states in Si and Si−1.
In each pass, we skip all items whose projection into
the previous stage had a probability below a stage-
specific threshold. This process is illustrated in Fig-
ure 3. When we reach the most refined search space
Ste, we do not prune, but rather extract the Viterbi
derivation instead.1
</bodyText>
<sectionHeader confidence="0.97181" genericHeader="method">
4 Learning Coarse Languages
</sectionHeader>
<bodyText confidence="0.999622833333333">
Central to our encoding-based projections (see Sec-
tion 2.2) are hierarchical clusterings of the tar-
get language vocabulary. In the present work,
these clusterings are each k-bit encodings and yield
sequences of coarse language models LMk and
phrasetables PTk.
</bodyText>
<footnote confidence="0.990498">
1Other final decoding strategies are possible, of course, in-
cluding variational methods and minimum-risk methods (Zhang
and Gildea, 2008).
</footnote>
<bodyText confidence="0.999971">
Given a hierarchical clustering, we estimate the
corresponding LMk from a corpus obtained by re-
placing each token in a target language corpus with
the appropriate word cluster. As with our original
refined language model, we estimate each coarse
language model using the SRILM toolkit (Stolcke,
2002). The phrasetables PTk are similarly estimated
by replacing the words on the target side of each
phrase pair with the corresponding cluster. This pro-
cedure can potentially map two distinct phrase pairs
to the same coarse translation. In such cases we keep
only one coarse phrase pair and sum the scores of the
colliding originals.
There are many possible schemes for creating hi-
erarchical clusterings. Here, we consider several di-
visive clustering methods, where coarse word clus-
ters are recursively split into smaller subclusters.
</bodyText>
<subsectionHeader confidence="0.986067">
4.1 Random projections
</subsectionHeader>
<bodyText confidence="0.99981">
The simplest approach to splitting a cluster is to ran-
domly assign each word type to one of two new sub-
clusters. Random projections have been shown to be
a good and computationally inexpensive dimension-
ality reduction technique, especially for high dimen-
sional data (Bingham and Mannila, 2001). Although
our best performance does not come from random
projections, we still obtain substantial speed-ups
over a single pass fine decoder when using random
projections in coarse passes.
</bodyText>
<subsectionHeader confidence="0.992575">
4.2 Frequency clustering
</subsectionHeader>
<bodyText confidence="0.999841888888889">
In frequency clustering, we allocate words to clus-
ters by frequency. At each level, the most frequent
words go into one cluster and the rarest words go
into another one. Concretely, we sort the words in
a given cluster by frequency and split the cluster so
that the two halves have equal token mass. This ap-
proach can be seen as a radically simplified version
of Brown et al. (1992). It can, and does, result in
highly imbalanced cluster hierarchies.
</bodyText>
<subsectionHeader confidence="0.997602">
4.3 HMM clustering
</subsectionHeader>
<bodyText confidence="0.9998045">
An approach found to be effective by Petrov and
Klein (2007) for coarse-to-fine parsing is to use
likelihood-based hierarchical EM training. We
adopt this approach here by identifying each clus-
ter with a latent state in an HMM and determiniz-
ing the emissions so that each word type is emitted
</bodyText>
<equation confidence="0.882415333333333">
lb-iAj-rc lb-iBk-rb lc-kCj-rc
lb r,
lb rb 1, r,
</equation>
<page confidence="0.96856">
112
</page>
<figure confidence="0.9849375">
0 1 2 3 4 5 6 7 8 9 10
Number of bits in coarse language model
</figure>
<figureCaption confidence="0.99844875">
Figure 5: Results of coarse language model perplexity
experiment (see Section 4.5). HMM and JClustering have
lower perplexity than frequency and random clustering
for all number of bits in the language encoding.
</figureCaption>
<bodyText confidence="0.999989928571429">
by only one state. When splitting a cluster s into
s1 and s2, we initially clone and mildly perturb its
corresponding state. We then use EM to learn pa-
rameters, which splits the state, and determinize the
result. Specifically, each word w is assigned to s1 if
P(wIs1) &gt; P(wIs2) and s2 otherwise. Because of
this determinization after each round of EM, a word
in one cluster will be allocated to exactly one of that
cluster’s children. This process not only guarantees
that the clusters are hierarchical, it also avoids the
state drift discussed by Petrov and Klein (2007). Be-
cause the emissions are sparse, learning is very effi-
cient. An example of some of the words associated
with early splits can be seen in Figure 1.
</bodyText>
<subsectionHeader confidence="0.980504">
4.4 JCluster
</subsectionHeader>
<bodyText confidence="0.999979444444444">
Goodman (2001) presents a clustering scheme
which aims to minimize the entropy of a word given
a cluster. This is accomplished by incrementally
swapping words between clusters to locally mini-
mize entropy.2 This clustering algorithm was devel-
oped with a slightly different application in mind,
but fits very well into our framework, because the
hierarchical clusters it produces are trained to maxi-
mize predictive likelihood.
</bodyText>
<subsectionHeader confidence="0.999124">
4.5 Clustering Results
</subsectionHeader>
<bodyText confidence="0.999896">
We applied the above clustering algorithms to our
monolingual language model data to obtain hierar-
</bodyText>
<footnote confidence="0.979491">
2The software for this clustering technique is available at
http://research.microsoft.com/˜joshuago/.
</footnote>
<figure confidence="0.527609">
100 1000 10000 100000
Total time in seconds
</figure>
<figureCaption confidence="0.988891333333333">
Figure 6: Coarse-to-fine decoding with HMM or JClus-
tering coarse language models reduce decoding times
while increasing accuracy.
</figureCaption>
<bodyText confidence="0.999925090909091">
chical clusters. We then trained coarse language
models of varying granularity and evaluated them on
a held-out set. To measure the quality of the coarse
language models we use perplexity (exponentiated
cross-entropy).3 Figure 5 shows that HMM clus-
tering and JClustering have lower perplexity than
frequency and random based clustering for all com-
plexities. In the next section we will present a set of
machine translation experiments using these coarse
language models; the clusterings with better per-
plexities generally produce better decoders.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999584">
We ran our experiments on the Europarl corpus
(Koehn, 2005) and show results on Spanish, French
and German to English translation. We used the
setup and preprocessing steps detailed in the 2008
Workshop on Statistical Machine Translation.4 Our
baseline decoder uses an ITG with an integrated tri-
gram language model. Phrase translation parame-
ters are learned from parallel corpora with approx-
imately 8.5 million words for each of the language
pairs. The English language model is trained on the
entire corpus of English parliamentary proceedings
provided with the Europarl distribution. We report
results on the 2000 development test set sentences
of length up to 126 words (average length was 30
words).
</bodyText>
<footnote confidence="0.999375666666667">
3We assumed that each cluster had a uniform distribution
over all the words in that cluster.
4See http://www.statmt.org/wmt08 for details.
</footnote>
<figure confidence="0.984700859649123">
BMM
JCluster
Frequency
Random
BMM
JCluster
Frequence
Random
Single pass (no clustering)
Perplexity 90000
80000
70000
60000
50000
40000
30000
20000
10000
0
BLEU 29.4
29.2
29
28.8
28.6
28.4
28.2
28
113
Total time in minutes
BLEU
29.6
29.4
29.2
28.8
28.6
28.4
28.2
29
28
300
250
200
150
100
50
0
fine
4 bits
3 bits
2 bits
1 bit
Encoding+Order
Order
Encoding
Single pass
f 4-f 3-f 2-f 1-f 2-3-f 1-3-f 1-2-3-f 100 1000 10000 100000
Language model bits for coarse passes
</figure>
<figureCaption confidence="0.9975025">
Figure 7: Many passes with extremely simple language
models produce the highest speed-ups.
</figureCaption>
<bodyText confidence="0.999973925925926">
Our ITG translation model is broadly competitive
with state-of-the-art phrase-based-models trained on
the same data. For example, on the Europarl devel-
opment test set, we fall short of Moses (Koehn et al.,
2007) by less than one BLEU point. On Spanish-
English we get 29.47 BLEU (compared to Moses’s
30.40), on French-English 29.34 (vs. 29.95), and
23.80 (vs. 24.64) on German-English. These differ-
ences can be attributed primarily to the substantially
richer distortion model used by Moses.
The multipass coarse-to-fine architecture that we
have introduced presents many choice points. In
the following, we investigate various axes individu-
ally. We present our findings as BLEU-to-time plots,
where the tradeoffs were generated by varying the
complexity and the number of coarse passes, as well
as the pruning thresholds and beam sizes. Unless
otherwise noted, the experiments are on Spanish-
English using trigram language models. When
different decoder settings are applied to the same
model, MERT weights (Och, 2003) from the unpro-
jected single pass setup are used and are kept con-
stant across runs. In particular, the same MERT
weights are used for all coarse passes; note that this
slightly disadvantages the multipass runs, which use
MERT weights optimized for the single pass de-
coder.
</bodyText>
<subsectionHeader confidence="0.998182">
5.1 Clustering
</subsectionHeader>
<bodyText confidence="0.99998575">
In section Section 4, HMM clustering and JCluster-
ing gave lower perplexities than frequency and ran-
dom clustering when using the same number of bits
for encoding the language model. To test how these
</bodyText>
<figure confidence="0.340842">
Total time in seconds
</figure>
<figureCaption confidence="0.9828845">
Figure 8: A combination of order-based and encoding-
based coarse-to-fine decoding yields the best results.
</figureCaption>
<bodyText confidence="0.999708066666667">
models perform at pruning, we ran our decoder sev-
eral times, varying only the clustering source. In
each case, we used a 2-bit trigram model as a sin-
gle coarse pass, followed by a fine output pass. Fig-
ure 6 shows that we can obtain significant improve-
ments over the single-pass baseline regardless of the
clustering. To no great surprise, HMM clustering
and JClustering yield better results, giving a 30-fold
speed-up at the same accuracy, or improvements of
about 0.3 BLEU when given the same time as the
single pass decoder. We discuss this increase in ac-
curacy over the baseline in Section 5.5. Since the
performance differences between those two cluster-
ing algorithms are negligible, we will use the sim-
pler HMM clustering in all subsequent experiments.
</bodyText>
<subsectionHeader confidence="0.999739">
5.2 Spacing
</subsectionHeader>
<bodyText confidence="0.9999986875">
Given a hierarchy of coarse language models, all
trigam for the moment, we need to decide on the
number of passes and the granularity of the coarse
language models used in each pass. Figure 7 shows
how decoding time varies for different multipass
schemes to achieve the same translation quality.
A single coarse pass with a 4-bit language model
cuts decoding time almost in half. However, one
can further cut decoding time by starting with even
coarser language models. In fact, the best results
are achieved by decoding in sequence with 1-, 2-
and 3-bit language models before running the final
fine trigram pass. Interestingly, in this setting, each
pass takes about the same amount of time. A simi-
lar observation was reported in the parsing literature,
where coarse-to-fine inference with multiple passes
</bodyText>
<page confidence="0.987583">
114
</page>
<figure confidence="0.996227975">
French
German
100 1000 10000
Total time in seconds
100 1000 10000
Total time in seconds
BLEU
29.4
29.2
28.8
28.6
28.4
28.2
29
28
Coarse-To-Fine
Fine Baseline
BLEU
23.5
22.5
24
23
22
Coarse-To-Fine
Fine Baseline
Spanish
100 1000 10000
Total time in seconds
BLEU
29.6
29.4
29.2
28.8
28.6
28.4
28.2
29
28
Coarse-To-Fine
Fine Baseline
</figure>
<figureCaption confidence="0.9975385">
Figure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to better
BLEU scores on all language pairs and for all parameter settings.
</figureCaption>
<bodyText confidence="0.984533">
of roughly equal complexity produces tremendous
speed-ups (Petrov and Klein, 2007).
</bodyText>
<subsectionHeader confidence="0.984246">
5.3 Encoding vs. Order
</subsectionHeader>
<bodyText confidence="0.9999775">
As described in Section 2, the language model com-
plexity can be reduced either by decreasing the vo-
cabulary size (encoding-based projection) or by low-
ering the language model order from trigram to bi-
gram (order-based projection). Figure 7 shows that
both approaches alone yield comparable improve-
ments over the single pass baseline. Fortunately,
the two approaches are complimentary, allowing us
to obtain further improvements by combining both.
We found it best to first do a series of coarse bigram
passes, followed by a fine bigram pass, followed by
a fine trigram pass.
</bodyText>
<subsectionHeader confidence="0.970113">
5.4 Final Results
</subsectionHeader>
<bodyText confidence="0.999787615384615">
Figure 9 compares our multipass coarse-to-fine de-
coder using language refinement to single pass de-
coding on three different languages. On each lan-
guage we get significant improvements in terms of
efficiency as well as accuracy. Overall, we can
achieve up to 50-fold speed-ups at the same accu-
racy, or alternatively, improvements of 0.4 BLEU
points over the best single pass run.
In absolute terms, our decoder translates on aver-
age about two Spanish sentences per second at the
highest accuracy setting.5 This compares favorably
to the Moses decoder (Koehn et al., 2007), which
takes almost three seconds per sentence.
</bodyText>
<footnote confidence="0.7575805">
5Of course, the time for an average sentence is much lower,
since long sentences dominate the overall translation time.
</footnote>
<subsectionHeader confidence="0.993544">
5.5 Search Error Analysis
</subsectionHeader>
<bodyText confidence="0.999987419354839">
In multipass coarse-to-fine decoding, we noticed
that in addition to computational savings, BLEU
scores tend to improve. A first hypothesis is
that coarse-to-fine decoding simply improves search
quality, where fewer good items fall off the beam
compared to a simple fine pass. However, this hy-
pothesis turns out to be incorrect. Table 1 shows
the percentage of test sentences for which the BLEU
score or log-likelihood changes when we switch
from single pass decoding to coarse-to-fine multi-
pass decoding. Only about 30% of the sentences
get translated in the same way (if much faster) with
coarse-to-fine decoding. For the rest, coarse-to-fine
decoding mostly finds translations with lower likeli-
hood, but higher BLEU score, than single pass de-
coding.6 An increase of the underlying objectives of
interest when pruning despite an increase in model-
score search errors has also been observed in mono-
lingual coarse-to-fine syntactic parsing (Charniak et
al., 1998; Petrov and Klein, 2007). This effect may
be because coarse-to-fine approximates certain min-
imum Bayes risk objective. It may also be an effect
of model intersection between the various passes’
models. In any case, both possibilities are often per-
fectly desirable. It is also worth noting that the num-
ber of search errors incurred in the coarse-to-fine
approach can be dramatically reduced (at the cost
of decoding time) by increasing the pruning thresh-
olds. However, the fortuitous nature of coarse-to-
fine search errors seems to be a substantial and de-
sirable effect.
</bodyText>
<footnote confidence="0.9386135">
6We compared the influence of multipass decoding on the
TM score and the LM score; both decrease.
</footnote>
<page confidence="0.995038">
115
</page>
<table confidence="0.9955762">
LL
&gt; = &lt;
BLEU &gt; 3.6% - 26.3%
= 1.5% 29.6 % 12.9 %
&lt; 2.2% - 24.1%
</table>
<tableCaption confidence="0.972393666666667">
Table 1: Percentage of sentences for which the BLEU
score/log-likelihood improves/drops during coarse-to-
fine decoding (compared to single pass decoding).
</tableCaption>
<sectionHeader confidence="0.999452" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999977214285715">
We have presented a coarse-to-fine syntactic de-
coder which utilizes a novel encoding-based lan-
guage projection in conjunction with order-based
projections to achieve substantial speed-ups. Un-
like A* methods, a posterior pruning approach al-
lows multiple passes, which we found to be very
beneficial for total decoding time. When aggres-
sively pruned, coarse-to-fine decoding can incur ad-
ditional search errors, but we found those errors to
be fortuitous more often than harmful. Our frame-
work applies equally well to other translation sys-
tems, though of course interesting new challenges
arise when, for example, the underlying SCFGs be-
come more complex.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999518240740741">
E. Bingham and H.i Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In KDD ’01.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 61h Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al. 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ’06.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ’05.
J. Goodman. 1997. Global thresholding and multiple-
pass parsing. In EMNLP ’97.
J. Goodman. 2001. A bit of progress in language model-
ing. Technical report, Microsoft Research.
A. Haghighi, J. DeNero, and D. Klein. 2007. A* search
via approximate factoring. In NAACL ’07.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL
’07.
D. Klein and C. Manning. 2003. A* parsing: fast exact
viterbi parse selection. In NAACL ’03.
P. Koehn, H. Hoang, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL ’07.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In ACL ’04.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ’03.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ’07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ’06.
A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In ICSLP ’02.
A. Venugopal, A. Zollmann, and S. Vogel. 2007. An ef-
ficient two-pass approach to synchronous-CFG driven
statistical MT. In HLT-NAACL ’07.
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In ACL ’96.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. In
Computational Linguistics.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL ’03.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
ACL ’08.
</reference>
<page confidence="0.999014">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664450">
<title confidence="0.9993585">Coarse-to-Fine Syntactic Machine using Language Projections</title>
<author confidence="0.999914">Slav Petrov Aria Haghighi Dan Klein</author>
<affiliation confidence="0.994613">Computer Science Division, EECS University of California at</affiliation>
<address confidence="0.922059">Berkeley, CA</address>
<email confidence="0.699732">aria42,</email>
<abstract confidence="0.999557111111111">The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incremenintroduced. In contrast to previous orderapproaches, we foon which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bingham</author>
<author>H i Mannila</author>
</authors>
<title>Random projection in dimensionality reduction: applications to image and text data.</title>
<date>2001</date>
<booktitle>In KDD ’01.</booktitle>
<contexts>
<context position="18502" citStr="Bingham and Mannila, 2001" startWordPosition="3016" endWordPosition="3019">ranslation. In such cases we keep only one coarse phrase pair and sum the scores of the colliding originals. There are many possible schemes for creating hierarchical clusterings. Here, we consider several divisive clustering methods, where coarse word clusters are recursively split into smaller subclusters. 4.1 Random projections The simplest approach to splitting a cluster is to randomly assign each word type to one of two new subclusters. Random projections have been shown to be a good and computationally inexpensive dimensionality reduction technique, especially for high dimensional data (Bingham and Mannila, 2001). Although our best performance does not come from random projections, we still obtain substantial speed-ups over a single pass fine decoder when using random projections in coarse passes. 4.2 Frequency clustering In frequency clustering, we allocate words to clusters by frequency. At each level, the most frequent words go into one cluster and the rarest words go into another one. Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass. This approach can be seen as a radically simplified version of Brown et al. (1992). It</context>
</contexts>
<marker>Bingham, Mannila, 2001</marker>
<rawString>E. Bingham and H.i Mannila. 2001. Random projection in dimensionality reduction: applications to image and text data. In KDD ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>P deSouza</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="19098" citStr="Brown et al. (1992)" startWordPosition="3117" endWordPosition="3120">gham and Mannila, 2001). Although our best performance does not come from random projections, we still obtain substantial speed-ups over a single pass fine decoder when using random projections in coarse passes. 4.2 Frequency clustering In frequency clustering, we allocate words to clusters by frequency. At each level, the most frequent words go into one cluster and the rarest words go into another one. Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass. This approach can be seen as a radically simplified version of Brown et al. (1992). It can, and does, result in highly imbalanced cluster hierarchies. 4.3 HMM clustering An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. We adopt this approach here by identifying each cluster with a latent state in an HMM and determinizing the emissions so that each word type is emitted lb-iAj-rc lb-iBk-rb lc-kCj-rc lb r, lb rb 1, r, 112 0 1 2 3 4 5 6 7 8 9 10 Number of bits in coarse language model Figure 5: Results of coarse language model perplexity experiment (see Section 4.5). HMM and JClustering </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Edgebased best-first chart parsing.</title>
<date>1998</date>
<booktitle>61h Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="29478" citStr="Charniak et al., 1998" startWordPosition="4796" endWordPosition="4799">ws the percentage of test sentences for which the BLEU score or log-likelihood changes when we switch from single pass decoding to coarse-to-fine multipass decoding. Only about 30% of the sentences get translated in the same way (if much faster) with coarse-to-fine decoding. For the rest, coarse-to-fine decoding mostly finds translations with lower likelihood, but higher BLEU score, than single pass decoding.6 An increase of the underlying objectives of interest when pruning despite an increase in modelscore search errors has also been observed in monolingual coarse-to-fine syntactic parsing (Charniak et al., 1998; Petrov and Klein, 2007). This effect may be because coarse-to-fine approximates certain minimum Bayes risk objective. It may also be an effect of model intersection between the various passes’ models. In any case, both possibilities are often perfectly desirable. It is also worth noting that the number of search errors incurred in the coarse-to-fine approach can be dramatically reduced (at the cost of decoding time) by increasing the pruning thresholds. However, the fortuitous nature of coarse-tofine search errors seems to be a substantial and desirable effect. 6We compared the influence of </context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>E. Charniak, S. Goldwater, and M. Johnson. 1998. Edgebased best-first chart parsing. 61h Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Multi-level coarse-to-fine PCFG Parsing.</title>
<date>2006</date>
<booktitle>In HLTNAACL ’06.</booktitle>
<contexts>
<context position="5570" citStr="Charniak et al., 2006" startWordPosition="860" endWordPosition="863">s, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. Figure 2: Possible state projections 7r for the target noun phrase “the report for these states” using the clusters from Figure 1. The number of bits used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives be</context>
</contexts>
<marker>Charniak, Johnson, McClosky, 2006</marker>
<rawString>E. Charniak, M. Johnson, D. McClosky, et al. 2006. Multi-level coarse-to-fine PCFG Parsing. In HLTNAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="14872" citStr="Chiang, 2005" startWordPosition="2442" endWordPosition="2443">translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1, ..., l1-ZXj-r1, ..., rn−1. Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language model is computationally expensive for two reasons: (1) the need to keep track of a large number of lexicalized hypotheses for each source span, and (2) the need to frequently query the large language model for each hypothesis combination. Multipass coarse-to-fine decoding can alleviate both computational issues. We start by decoding in an extremely coarse bigram search space, where there are very few possible translations. We compute standard inside/outside probabilities (i5/o5), as follows. Consider the application of non-inverted binary rule: we combin</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Global thresholding and multiplepass parsing.</title>
<date>1997</date>
<booktitle>In EMNLP ’97.</booktitle>
<contexts>
<context position="5547" citStr="Goodman, 1997" startWordPosition="857" endWordPosition="859">mework like ours, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. Figure 2: Possible state projections 7r for the target noun phrase “the report for these states” using the clusters from Figure 1. The number of bits used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abst</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>J. Goodman. 1997. Global thresholding and multiplepass parsing. In EMNLP ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="3046" citStr="Goodman, 2001" startWordPosition="465" endWordPosition="466">n this way, initial passes can be very quick, with complexity phased in gradually. Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. In addition, we demonstrate that more than two passes are beneficial and show that our computation is equally distributed over all passes. In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly. 108 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, October 2008.c�2008 Association for Computational Linguistics To follow related work and to focus on th</context>
<context position="20554" citStr="Goodman (2001)" startWordPosition="3373" endWordPosition="3374">se EM to learn parameters, which splits the state, and determinize the result. Specifically, each word w is assigned to s1 if P(wIs1) &gt; P(wIs2) and s2 otherwise. Because of this determinization after each round of EM, a word in one cluster will be allocated to exactly one of that cluster’s children. This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed by Petrov and Klein (2007). Because the emissions are sparse, learning is very efficient. An example of some of the words associated with early splits can be seen in Figure 1. 4.4 JCluster Goodman (2001) presents a clustering scheme which aims to minimize the entropy of a word given a cluster. This is accomplished by incrementally swapping words between clusters to locally minimize entropy.2 This clustering algorithm was developed with a slightly different application in mind, but fits very well into our framework, because the hierarchical clusters it produces are trained to maximize predictive likelihood. 4.5 Clustering Results We applied the above clustering algorithms to our monolingual language model data to obtain hierar2The software for this clustering technique is available at http://r</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. A bit of progress in language modeling. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>A* search via approximate factoring.</title>
<date>2007</date>
<booktitle>In NAACL ’07.</booktitle>
<contexts>
<context position="5711" citStr="Haghighi et al., 2007" startWordPosition="883" endWordPosition="886">n their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. Figure 2: Possible state projections 7r for the target noun phrase “the report for these states” using the clusters from Figure 1. The number of bits used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of r</context>
</contexts>
<marker>Haghighi, DeNero, Klein, 2007</marker>
<rawString>A. Haghighi, J. DeNero, and D. Klein. 2007. A* search via approximate factoring. In NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL ’07.</booktitle>
<contexts>
<context position="6683" citStr="Huang and Chiang (2007)" startWordPosition="1045" endWordPosition="1048">thogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1, ... , l1 and right-most target side words r1, ... , rn−1 of the generated hypothesis. We denote the resulting lexicalized state as ln−1, ... , l1-X-r1, ... , rn−1. Assuming a vocabulary V and grammar symbol set G, the state space</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>A* parsing: fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In NAACL ’03.</booktitle>
<contexts>
<context position="5687" citStr="Klein and Manning, 2003" startWordPosition="879" endWordPosition="882">o guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. Figure 2: Possible state projections 7r for the target noun phrase “the report for these states” using the clusters from Figure 1. The number of bits used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. A* parsing: fast exact viterbi parse selection. In NAACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL ’07.</booktitle>
<marker>Koehn, Hoang, 2007</marker>
<rawString>P. Koehn, H. Hoang, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL ’07. P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In ACL ’04.</booktitle>
<contexts>
<context position="14321" citStr="Melamed (2004)" startWordPosition="2355" endWordPosition="2356">. Finally, binary inverted productions invert the order of their children (X —* (Y Z)). These productions are associated with rewrite weights in the standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by ZXj, where (i, j) is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1, ..., l1-ZXj-r1, ..., rn−1. Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language model is c</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. D. Melamed. 2004. Statistical machine translation by parsing. In ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03.</booktitle>
<contexts>
<context position="24338" citStr="Och, 2003" startWordPosition="3960" endWordPosition="3961">be attributed primarily to the substantially richer distortion model used by Moses. The multipass coarse-to-fine architecture that we have introduced presents many choice points. In the following, we investigate various axes individually. We present our findings as BLEU-to-time plots, where the tradeoffs were generated by varying the complexity and the number of coarse passes, as well as the pruning thresholds and beam sizes. Unless otherwise noted, the experiments are on SpanishEnglish using trigram language models. When different decoder settings are applied to the same model, MERT weights (Och, 2003) from the unprojected single pass setup are used and are kept constant across runs. In particular, the same MERT weights are used for all coarse passes; note that this slightly disadvantages the multipass runs, which use MERT weights optimized for the single pass decoder. 5.1 Clustering In section Section 4, HMM clustering and JClustering gave lower perplexities than frequency and random clustering when using the same number of bits for encoding the language model. To test how these Total time in seconds Figure 8: A combination of order-based and encodingbased coarse-to-fine decoding yields th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL ’07.</booktitle>
<contexts>
<context position="5595" citStr="Petrov and Klein, 2007" startWordPosition="864" endWordPosition="867">08) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. Figure 2: Possible state projections 7r for the target noun phrase “the report for these states” using the clusters from Figure 1. The number of bits used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the gra</context>
<context position="19246" citStr="Petrov and Klein (2007)" startWordPosition="3141" endWordPosition="3144">e pass fine decoder when using random projections in coarse passes. 4.2 Frequency clustering In frequency clustering, we allocate words to clusters by frequency. At each level, the most frequent words go into one cluster and the rarest words go into another one. Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass. This approach can be seen as a radically simplified version of Brown et al. (1992). It can, and does, result in highly imbalanced cluster hierarchies. 4.3 HMM clustering An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. We adopt this approach here by identifying each cluster with a latent state in an HMM and determinizing the emissions so that each word type is emitted lb-iAj-rc lb-iBk-rb lc-kCj-rc lb r, lb rb 1, r, 112 0 1 2 3 4 5 6 7 8 9 10 Number of bits in coarse language model Figure 5: Results of coarse language model perplexity experiment (see Section 4.5). HMM and JClustering have lower perplexity than frequency and random clustering for all number of bits in the language encoding. by only one state. When splitting a clus</context>
<context position="27135" citStr="Petrov and Klein, 2007" startWordPosition="4423" endWordPosition="4426">ith multiple passes 114 French German 100 1000 10000 Total time in seconds 100 1000 10000 Total time in seconds BLEU 29.4 29.2 28.8 28.6 28.4 28.2 29 28 Coarse-To-Fine Fine Baseline BLEU 23.5 22.5 24 23 22 Coarse-To-Fine Fine Baseline Spanish 100 1000 10000 Total time in seconds BLEU 29.6 29.4 29.2 28.8 28.6 28.4 28.2 29 28 Coarse-To-Fine Fine Baseline Figure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to better BLEU scores on all language pairs and for all parameter settings. of roughly equal complexity produces tremendous speed-ups (Petrov and Klein, 2007). 5.3 Encoding vs. Order As described in Section 2, the language model complexity can be reduced either by decreasing the vocabulary size (encoding-based projection) or by lowering the language model order from trigram to bigram (order-based projection). Figure 7 shows that both approaches alone yield comparable improvements over the single pass baseline. Fortunately, the two approaches are complimentary, allowing us to obtain further improvements by combining both. We found it best to first do a series of coarse bigram passes, followed by a fine bigram pass, followed by a fine trigram pass. 5</context>
<context position="29503" citStr="Petrov and Klein, 2007" startWordPosition="4800" endWordPosition="4803">st sentences for which the BLEU score or log-likelihood changes when we switch from single pass decoding to coarse-to-fine multipass decoding. Only about 30% of the sentences get translated in the same way (if much faster) with coarse-to-fine decoding. For the rest, coarse-to-fine decoding mostly finds translations with lower likelihood, but higher BLEU score, than single pass decoding.6 An increase of the underlying objectives of interest when pruning despite an increase in modelscore search errors has also been observed in monolingual coarse-to-fine syntactic parsing (Charniak et al., 1998; Petrov and Klein, 2007). This effect may be because coarse-to-fine approximates certain minimum Bayes risk objective. It may also be an effect of model intersection between the various passes’ models. In any case, both possibilities are often perfectly desirable. It is also worth noting that the number of search errors incurred in the coarse-to-fine approach can be dramatically reduced (at the cost of decoding time) by increasing the pruning thresholds. However, the fortuitous nature of coarse-tofine search errors seems to be a substantial and desirable effect. 6We compared the influence of multipass decoding on the</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<contexts>
<context position="2986" citStr="Petrov et al., 2006" startWordPosition="456" endWordPosition="459">language, to effectively reduce the size of the target language. In this way, initial passes can be very quick, with complexity phased in gradually. Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. In addition, we demonstrate that more than two passes are beneficial and show that our computation is equally distributed over all passes. In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly. 108 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, October 2008.c�2008 Association for Computa</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP ’02.</booktitle>
<contexts>
<context position="17654" citStr="Stolcke, 2002" startWordPosition="2883" endWordPosition="2884">ngs of the target language vocabulary. In the present work, these clusterings are each k-bit encodings and yield sequences of coarse language models LMk and phrasetables PTk. 1Other final decoding strategies are possible, of course, including variational methods and minimum-risk methods (Zhang and Gildea, 2008). Given a hierarchical clustering, we estimate the corresponding LMk from a corpus obtained by replacing each token in a target language corpus with the appropriate word cluster. As with our original refined language model, we estimate each coarse language model using the SRILM toolkit (Stolcke, 2002). The phrasetables PTk are similarly estimated by replacing the words on the target side of each phrase pair with the corresponding cluster. This procedure can potentially map two distinct phrase pairs to the same coarse translation. In such cases we keep only one coarse phrase pair and sum the scores of the colliding originals. There are many possible schemes for creating hierarchical clusterings. Here, we consider several divisive clustering methods, where coarse word clusters are recursively split into smaller subclusters. 4.1 Random projections The simplest approach to splitting a cluster </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In ICSLP ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>S Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In HLT-NAACL ’07.</booktitle>
<contexts>
<context position="6528" citStr="Venugopal et al. (2007)" startWordPosition="1016" endWordPosition="1019">ts used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1, ... , l1 and right-most target side words r1, ... , rn−1 of the generated </context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>A. Venugopal, A. Zollmann, and S. Vogel. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In HLT-NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In ACL ’96.</booktitle>
<contexts>
<context position="14303" citStr="Wu (1996)" startWordPosition="2352" endWordPosition="2353"> (X —* [Y Z]). Finally, binary inverted productions invert the order of their children (X —* (Y Z)). These productions are associated with rewrite weights in the standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by ZXj, where (i, j) is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1, ..., l1-ZXj-r1, ..., rn−1. Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated l</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A polynomial-time algorithm for statistical machine translation. In ACL ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="3784" citStr="Wu, 1997" startWordPosition="576" endWordPosition="577">ial and show that our computation is equally distributed over all passes. In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly. 108 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, October 2008.c�2008 Association for Computational Linguistics To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding. In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points. This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below. 2 Coarse-to-Fine Decodin</context>
<context position="13286" citStr="Wu (1997)" startWordPosition="2183" endWordPosition="2184"> gives substantial speed-ups. There is of course an additional memory requirement, but it is negligible. As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language models. In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams). 3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach of Wu (1997) to facilitate comparison with previous work (Zens and Ney, 2003; Zhang and Gildea, 2008) as well as to focus on language model complexity. ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules. Preterminal unary productions produce terminal strings on both sides (words or phrases): X —* e/f. Binary in-order productions combine two phrases monotonically (X —* [Y Z]). Finally, binary inverted productions invert the order of their children (X —* (Y Z)). These productions are associated with rewrite weights in the standard way. Without a langua</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03.</booktitle>
<contexts>
<context position="13350" citStr="Zens and Ney, 2003" startWordPosition="2192" endWordPosition="2195">itional memory requirement, but it is negligible. As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language models. In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams). 3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach of Wu (1997) to facilitate comparison with previous work (Zens and Ney, 2003; Zhang and Gildea, 2008) as well as to focus on language model complexity. ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules. Preterminal unary productions produce terminal strings on both sides (words or phrases): X —* e/f. Binary in-order productions combine two phrases monotonically (X —* [Y Z]). Finally, binary inverted productions invert the order of their children (X —* (Y Z)). These productions are associated with rewrite weights in the standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>R. Zens and H. Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Efficient multi-pass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In ACL ’08.</booktitle>
<contexts>
<context position="1682" citStr="Zhang and Gildea (2008)" startWordPosition="246" endWordPosition="249">a synchronous CFG translation model is very efficient, requiring only a variant of the CKY algorithm. As in monolingual parsing, dynamic programming items are simply indexed by a source language span and a syntactic label. Complexity arises when n-gram language model scoring is added, because items must now be distinguished by their initial and final few target language words for purposes of later combination. This lexically exploded search space is a root cause of inefficiency in decoding, and several methods have been suggested to combat it. The approach most relevant to the current work is Zhang and Gildea (2008), which begins with an initial bigram pass and uses the resulting chart to guide a final trigram pass. Substantial speed-ups are obtained, but computation is still dominated by the initial bigram pass. The key challenge is that unigram models are too poor to prune well, but bigram models are already huge. In short, the problem is that there are too many words in the target language. In this paper, we propose a new, coarse-to-fine, multipass approach which allows much greater speedups by translating into abstracted languages. That is, rather than beginning with a low-order model of a still-larg</context>
<context position="4975" citStr="Zhang and Gildea (2008)" startWordPosition="763" endWordPosition="766"> below. 2 Coarse-to-Fine Decoding In coarse-to-fine decoding, we create a series of initially simple but increasingly complex search problems. We then use the solutions of the simpler problems to prune the search spaces for more complex models, reducing the total computational cost. 2.1 Related Work Taken broadly, the coarse-to-fine approach is not new to machine translation (MT) or even syntactic MT. Many common decoder precomputations can be seen as coarse-to-fine methods, including the A*- like forward estimates used in the Moses decoder (Koehn et al., 2007). In an ITG framework like ours, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Pet</context>
<context position="8556" citStr="Zhang and Gildea (2008)" startWordPosition="1368" endWordPosition="1371">rchical clustering of target language vocabulary (see Section 4). Even with a small number of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes. stracted spaces. Figure 2 illustrates those two orthogonal axes of abstraction. Order-based projections are simple. As shown in Figure 2, they simply strip off the appropriate words from each state, collapsing dynamic programming items which are identical from the standpoint of their left-to-right combination in the lower order language model. However, having only orderbased projections is very limiting. Zhang and Gildea (2008) found that their computation was dominated by their bigram pass. The only lower-order pass possible uses a unigram model, which provides no information about the interaction of the language model and translation model reorderings. We therefore propose encoding-based projections. These projections reduce the size of the target language vocabulary by deterministically projecting each target language word to a word cluster. This projection extends to the whole search state in the obvious way: assuming a bigram language model, the state l-X-r projects to c(l)-X-c(r), where c(·) is the determinist</context>
<context position="13375" citStr="Zhang and Gildea, 2008" startWordPosition="2196" endWordPosition="2199">rement, but it is negligible. As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language models. In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams). 3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach of Wu (1997) to facilitate comparison with previous work (Zens and Ney, 2003; Zhang and Gildea, 2008) as well as to focus on language model complexity. ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules. Preterminal unary productions produce terminal strings on both sides (words or phrases): X —* e/f. Binary in-order productions combine two phrases monotonically (X —* [Y Z]). Finally, binary inverted productions invert the order of their children (X —* (Y Z)). These productions are associated with rewrite weights in the standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming s</context>
<context position="17352" citStr="Zhang and Gildea, 2008" startWordPosition="2834" endWordPosition="2837">ility below a stagespecific threshold. This process is illustrated in Figure 3. When we reach the most refined search space Ste, we do not prune, but rather extract the Viterbi derivation instead.1 4 Learning Coarse Languages Central to our encoding-based projections (see Section 2.2) are hierarchical clusterings of the target language vocabulary. In the present work, these clusterings are each k-bit encodings and yield sequences of coarse language models LMk and phrasetables PTk. 1Other final decoding strategies are possible, of course, including variational methods and minimum-risk methods (Zhang and Gildea, 2008). Given a hierarchical clustering, we estimate the corresponding LMk from a corpus obtained by replacing each token in a target language corpus with the appropriate word cluster. As with our original refined language model, we estimate each coarse language model using the SRILM toolkit (Stolcke, 2002). The phrasetables PTk are similarly estimated by replacing the words on the target side of each phrase pair with the corresponding cluster. This procedure can potentially map two distinct phrase pairs to the same coarse translation. In such cases we keep only one coarse phrase pair and sum the sc</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>H. Zhang and D. Gildea. 2008. Efficient multi-pass decoding for synchronous context free grammars. In ACL ’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>