<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9933475">
MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for
Information Queries
</title>
<author confidence="0.956204">
Jennifer Chu-Carroll
</author>
<affiliation confidence="0.888359">
Lucent Technologies Bell Laboratories
</affiliation>
<address confidence="0.9266585">
600 Mountain Avenue
Murray Hill, NJ 07974, U.S.A.
</address>
<email confidence="0.998507">
jencc@research.bell-labs.com
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999193538461538">
This paper describes MIMIC, an adaptive mixed initia-
tive spoken dialogue system that provides movie show-
time information. MIMIC improves upon previous
dialogue systems in two respects. First, it employs
initiative-oriented strategy adaptation to automatically
adapt response generation strategies based on the cumu-
lative effect of information dynamically extracted from
user utterances during the dialogue. Second, MIMIC&apos;s
dialogue management architecture decouples its initia-
tive module from the goal and response strategy selec-
tion processes, providing a general framework for devel-
oping spoken dialogue systems with different adaptation
behavior.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99979545">
In recent years, speech and natural language technolo-
gies have matured enough to enable the development of
spoken dialogue systems in limited domains. Most ex-
isting systems employ dialogue strategies pre-specified
during the design phase of the dialogue manager with-
out taking into account characteristics of actual dialogue
interactions. More specifically, mixed initiative systems
typically employ rules that specify conditions (generally
based on local dialogue context) under which initiative
may shift from one agent to the other. Previous research,
on the other hand, has shown that changes in initiative
strategies in human-human dialogues can be dynamically
modeled in terms of characteristics of the user and of
the on-going dialogue (Chu-Carroll and Brown, 1998)
and that adaptability of initiative strategies in dialogue
systems leads to better system performance (Litman and
Pan, 1999). However, no previous dialogue system takes
into account these dialogue characteristics or allows for
initiative-oriented adaptation of dialogue strategies.
In this paper, we describe MIMIC, a voice-enabled
telephone-based dialogue system that provides movie
showtime information, emphasizing its dialogue man-
agement aspects. MIMIC improves upon previous sys-
tems along two dimensions. First, MIMIC automat-
ically adapts dialogue strategies based on participant
roles, characteristics of the current utterance, and dia-
logue history. This automatic adaptation allows appro-
priate dialogue strategies to be employed based on both
local dialogue context and dialogue history, and has been
shown to result in significantly better performance than
non-adaptive systems. Second, MIMIC employs an ini-
tiative module that is decoupled from the goal selection
process in the dialogue manager, while allowing the out-
come of both components to jointly determine the strate-
gies chosen for response generation. As a result, MIMIC
can exhibit drastically different dialogue behavior with
very minor adjustments to parameters in the initiative
module, allowing for rapid development and comparison
of experimental prototypes and resulting in general and
portable dialogue systems.
</bodyText>
<sectionHeader confidence="0.9975295" genericHeader="introduction">
2 Adaptive Mixed Initiative Dialogue
Management
</sectionHeader>
<subsectionHeader confidence="0.991045">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.99996775">
In naturally occurring human-human dialogues, speakers
often adopt different dialogue strategies based on hearer
characteristics, dialogue history, etc. For instance, the
speaker may provide more guidance if the hearer is hav-
ing difficulty making progress toward task completion,
while taking a more passive approach when the hearer
is an expert in the domain. Our main goal is to enable
a spoken dialogue system to simulate such human be-
havior by dynamically adapting dialogue strategies dur-
ing an interaction based on information that can be au-
tomatically detected from the dialogue. Figure 1 shows
an excerpt from a dialogue between MIMIC and an ac-
tual user where the user is attempting to find the times
at which the movie Analyze This playing at theaters in
Montclair. S and U indicate system and user utterances,
respectively, and the italicized utterances are the output
of our automatic speech recognizer. In addition, each
system turn is annotated with its task and dialogue ini-
tiative holders, where task initiative tracks the lead in the
process toward achieving the dialogue participants&apos; do-
main goal, while dialogue initiative models the lead in
determining the current discourse focus (Chu-Carroll and
Brown, 1998). In our information query application do-
main, the system has task (and thus dialogue) initiative if
its utterances provide helpful guidance toward achieving
the user&apos;s domain goal, as in utterances (6) and (7) where
MIMIC provided valid response choices to its query in-
tending to solicit a theater name, while the system has
</bodyText>
<page confidence="0.999214">
97
</page>
<bodyText confidence="0.999977638888889">
dialogue but not task initiative if its utterances only spec-
ify the current discourse goal, as in utterance (4).1
This dialogue illustrates several features of our adap-
tive mixed initiative dialogue manager. First, MIMIC au-
tomatically adapted the initiative distribution based on
information extracted from user utterances and dialogue
history. More specifically, MIMIC took over task initia-
tive in utterance (6), after failing to obtain a valid an-
swer to its query soliciting a missing theater name in (4).
It retained task initiative until utterance (12), after the
user implicitly indicated her intention to take over task
initiative by providing a fully-specified query (utterance
(11)) to a limited prompt (utterance (10)). Second, ini-
tiative distribution may affect the strategies MIMIC se-
lects to achieve its goals. For instance, in the context
of soliciting missing information, when MIMIC did not
have task initiative, a simple information-seeking query
was generated (utterance (4)). On the other hand, when
MIMIC had task initiative, additional guidance was pro-
vided (in the form of valid response choices in utterance
(6)), which helped the user successfully respond to the
system&apos;s query. In the context of prompting the user for
a new query, when MIMIC had task initiative, a lim-
ited prompt was selected to better constrain the user&apos;s
response (utterance (10)), while an open-ended prompt
was generated to allow the user to take control of the
problem-solving process otherwise (utterances (1) and
(13)).
In the next section, we briefly review a framework for
dynamic initiative modeling. In Section 3, we discuss
how this framework was incorporated into the dialogue
management component of a spoken dialogue system to
allow for automatic adaptation of dialogue strategies. Fi-
nally, we outline experiments evaluating the resulting
system and show that MIMIC&apos;s automatic adaptation ca-
pabilities resulted in better system performance.
</bodyText>
<subsectionHeader confidence="0.9904155">
2.2 An Evidential Framework for Modeling
Initiative
</subsectionHeader>
<bodyText confidence="0.999828285714286">
In previous work, we proposed a framework for mod-
eling initiative during dialogue interaction (Chu-Carroll
and Brown, 1998). This framework predicts task and dia-
logue initiative holders on a turn-by-turn basis in human-
human dialogues based on participant roles (such as each
dialogue agent&apos;s level of expertise and the role that she
plays in the application domain), cues observed in the
current dialogue turn, and dialogue history. More specif-
ically, we utilize the Dempster-Shafer theory (Shafer,
1976; Gordon and Shortliffe, 1984), and represent the
current initiative distribution as two basic probability as-
signments (bpas) which indicate the amount of support
for each dialogue participant having the task and dia-
logue initiatives. For instance, the bpa mt—cur ({S)) =
</bodyText>
<footnote confidence="0.72841675">
&apos;Although the dialogues we collected in our experiments (Sec-
tion 5) include cases in which MIMIC has neither initiative, such cases
are rare in this application domain, and will not be discussed further in
this paper.
</footnote>
<bodyText confidence="0.999288702702703">
0.3, Mt—cur({U}) = 0.7 indicates that, with all evidence
taken into account, there is more support (to the degree
0.7) for the user having task initiative in the current turn
than for the system. At the end of each turn, the bpas
are updated based on the effects that cues observed dur-
ing that turn have on changing them, and the new bpas
are used to predict the next task and dialogue initiative
holders.
In this framework, cues that affect initiative distribu-
tion include NoNewInfo, triggered when the speaker sim-
ply repeats or rephrases an earlier utterance, implicitly
suggesting that the speaker may want to give up initia-
tive, AmbiguousActions, triggered when the speaker pro-
poses an action that is ambiguous in the application do-
main, potentially prompting the hearer to take over ini-
tiative to resolve the detected ambiguity, etc. The ef-
fects that each cue has on changing the current bpas
are also represented as bpas, which were determined by
an iterative training procedure using a corpus of tran-
scribed dialogues where each turn was annotated with the
task/dialogue initiative holders and the observed cues.
The bpas for the next turn are computed by combining
the bpas representing the current initiative distribution
and the bpas representing the effects of cues observed
during the current turn, using Dempster&apos;s combination
rule (Gordon and Shortliffe, 1984). The task and dia-
logue initiative holders are then predicted based on the
new bpas. This framework was evaluated using anno-
tated dialogues from four task-oriented domains, achiev-
ing, on average, a correct prediction rate of 97% and 88%
for task and dialogue initiative holders, respectively. In
Section 3.2, we discuss how this predictive model is con-
verted into a generative model by enabling the system to
automatically detect cues that were previously labelled
manually. We further discuss how the model is used
by the dialogue manager for dynamic dialogue strategy
adaptation.
</bodyText>
<sectionHeader confidence="0.8463855" genericHeader="method">
3 MIMIC: Mixed Initiative Movie
Information Consultant
</sectionHeader>
<bodyText confidence="0.99966">
MIMIC is a telephone-based dialogue system that pro-
vides movie showtime information. It consists of the fol-
lowing main components, implemented on a distributed,
client-server architecture (Zhou et al., 1997):
</bodyText>
<listItem confidence="0.977714">
1. Telephony server: this component detects rings
and hang-ups, and enables streaming of audio data
on channels of a telephony board.
2. Speech recognizer: the recognizer receives audio
data from the telephony server and generates the
word string hypothesis that best matches the audio
input. We used the Lucent Automatic Speech Rec-
ognizer (Reichl and Chou, 1998; Ortmanns et al.,
1999), configured to use class-based probabilistic n-
gram language models to allow for rapid updates of
movie/theater/town names.
</listItem>
<page confidence="0.993833">
98
</page>
<table confidence="0.864427285714286">
Utterance Task Dialogue
Initiative Initiative
S: Hello, this is MIMIC, the movie information system. User System
How can I help you?
U: What time is Analyze This playing in Montclair?
[what time is analyze this playing in montclair]
S: What theater would you like? User System
U: Montclair.
[montclair]
S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System System
What theater would you like?
U: Clearviews Screening Zone.
[urn movie screening zone]
S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, System System
</table>
<figureCaption confidence="0.749870625">
and 6:30pm.
Please say the name of the movie or theater or town you would like information about.
U: What time is Analyze This playing at the Wellmont Theater?
[what time is analyze this playing at the wellmont theater]
S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, User System
and 10:10pm.
Can I help you with anything else?
Figure 1: Excerpt of an Adaptive Mixed Initiative Dialogue
</figureCaption>
<listItem confidence="0.821625">
3. NLP/Dialogue component: this main application-
</listItem>
<bodyText confidence="0.891637909090909">
dependent component receives a user utterance hy-
pothesis from the speech recognizer, and generates
system utterance(s) in response. Three major tasks
are carried out by this component: 1) semantic in-
terpretation, which constructs frame-based seman-
tic representations from user utterances, 2) dialogue
management, where response strategies are selected
based on the semantic representation of the user&apos;s
utterance, system&apos;s domain knowledge, and initia-
tive distribution, and 3) utterance generation, where
utterance templates are chosen and instantiated to
realize the selected response strategies. These three
tasks will be discussed in further detail in the rest of
this section.
4. Text-to-speech engine: the TTS system receives
the word string comprising the system&apos;s response
from the dialogue component and converts the text
into speech for output over the telephone. We used
the Bell Labs TTS system (Sproat, 1998), which
in addition to converting plain text into speech, ac-
cepts text strings annotated to override default pitch
height, accent placement, speaking rate, etc.2
</bodyText>
<subsectionHeader confidence="0.996828">
3.1 Semantic Interpretation
</subsectionHeader>
<bodyText confidence="0.987653222222222">
MIMIC utilizes a non-recursive frame-based semantic
representation commonly used in spoken dialogue sys-
tems (e.g. (Seneff et al., 1991; Lamel, 1998)), which
represents an utterance as a set of attribute-value pairs.
Figure 2(a) shows the frame-based semantic representa-
tion for the utterance &amp;quot;What time is Analyze This playing
2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC&apos;s dialogue-
level knowledge is used to override default prosodic assignments for
concept-to-speech generation.
</bodyText>
<figure confidence="0.9966654">
Question-Type: When
Movie: Analyze This
Theater: null
Town: Montclair
(a) Semantic Representation
Question-Type: When
Movie: mandatory
Theater: mandatory
Town: optional
(b) Task Specification
</figure>
<figureCaption confidence="0.9782">
Figure 2: Semantic Representation and Task Specifica-
tion
</figureCaption>
<bodyText confidence="0.981739285714286">
in Montclair?&amp;quot;
MIMIC&apos;s semantic representation is constructed by
first extracting, for each attribute, a set of keywords from
the user utterance. Using a vector-based topic identifi-
cation process (Salton, 1971; Chu-Carroll and Carpen-
ter, 1999), these keywords are used to determine a set
of likely values (including null) for that attribute. Next,
the utterance is interpreted with respect to the dialogue
history and the system&apos;s domain knowledge. This al-
lows MIMIC to handle elliptical sentences and anaphoric
references, as well as automatically infer missing values
and detect inconsistencies in the current representation.
This semantic representation allows for decoupling
of domain-dependent task specifications and domain-
</bodyText>
<page confidence="0.993916">
99
</page>
<bodyText confidence="0.999973">
independent dialogue management strategies. Each
query type is specified by a template indicating, for each
attribute, whether a value must, must not, or can option-
ally be provided in order for a query to be considered
well-formed. Figure 2(b) shows that to solicit movie
showtime information (question type when), a movie
name and a theater name must be provided, whereas a
town may optionally be provided. These specifications
are determined based on domain semantics, and must be
reconstructed when porting the system to a new domain.
</bodyText>
<subsectionHeader confidence="0.996202">
3.2 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.999931363636363">
Given a semantic representation, the dialogue history and
the system&apos;s domain knowledge, the dialogue manager
selects a set of strategies that guides MIMIC&apos;s response
generation process. This task is carried out by three
subprocesses: 1) initiative modeling, which determines
the initiative distribution for the system&apos;s dialogue turn,
2) goal selection, which identifies a goal that MIMIC&apos;s
response attempts to achieve, and 3) strategy selection,
which chooses, based on the initiative distribution, a set
of dialogue acts that MIMIC will adopt in its attempt to
realize the selected goal.
</bodyText>
<subsectionHeader confidence="0.393342">
3.2.1 Initiative Modeling
</subsectionHeader>
<bodyText confidence="0.999868523809524">
MIMIC&apos;s initiative module determines the task and di-
alogue initiative holders for each system turn in order
to enable dynamic strategy adaptation. It automatically
detects cues triggered during the current user turn, and
combines the effects of these cues with the current ini-
tiative distribution to determine the initiative holders for
the system&apos;s turn.
Cue Detection The cues and the bpas representing
their effects are largely based on a subset of those de-
scribed in (Chu-Carroll and Brown, 1998),3 as shown
in Figures 3(a) and 3(b). Figure 3(a) shows that obser-
vation of TakeOverTask supports a task initiative shift
to the speaker to the degree .35. The remaining sup-
port is assigned to 0, the set of all possible conclusions
(i.e., {speaker,hearer}), indicating that to the degree .65,
observation of this cue does not commit to identifying
which dialogue participant should have task initiative in
the next dialogue turn.
The cues used in MIMIC are classified into two cate-
gories, discourse cues and analytical cues, based on the
types of knowledge needed to detect them:
</bodyText>
<listItem confidence="0.92111775">
I. Discourse cues, which can be detected by consider-
ing the semantic representation of the current utter-
ance and dialogue history:
• TakeOverTask, an implicit indication that the
user wants to take control of the problem-
solving process, triggered when the user pro-
vides more information than the discourse ex-
pectation.
</listItem>
<tableCaption confidence="0.562034666666667">
3We selected only those cues that can be automatically detected in
a spoken dialogue system with speech recognition errors and limited
semantic interpretation capabilities.
</tableCaption>
<listItem confidence="0.988359454545455">
• NoNewInfo, an indication that the user is un-
able to make progress toward task completion,
triggered when the semantic representations of
two consecutive user turns are identical (a re-
sult of the user not knowing what to say or the
speech recognizer failing to recognize the user
utterances).
2. Analytical cues, which can only be detected by tak-
ing into account MIMIC&apos;s domain knowledge:
• InvalidAction, an indication that the user made
an invalid assumption about the domain, trig-
gered when the system database lookup based
on the user&apos;s query returns null.
• InvalidActionResolved, triggered when the
previous invalid assumption is corrected.
• AmbiguousAction, an indication that the user
query is not well-formed, triggered when a
mandatory attribute is unspecified or when
more than one value is specified for an at-
tribute.
• AmbiguousActionResolved, triggered when the
attribute in question is uniquely instantiated.
</listItem>
<bodyText confidence="0.9947426">
Computing Initiative Distribution To determine the
initiative distribution, the bpas representing the effects
of cues detected in the current user utterance are instan-
tiated (i.e., speaker/hearer in Figure 3 are instantiated as
system/user accordingly). These effects are then inter-
preted with respect to the current initiative distribution
by applying Dempster&apos;s combination rule (Gordon and
Shortliffe, 1984) to the bpas representing the current ini-
tiative distribution and the instantiated bpas. This results
in two new bpas representing the task and dialogue initia-
tive distributions for the system&apos;s turn. The dialogue par-
ticipant with the greater degree of support for having the
task/dialogue initiative in these bpas is the task/dialogue
initiative holder for the system&apos;s turn4 (see Section 4 for
an example).
</bodyText>
<subsectionHeader confidence="0.467801">
3.2.2 Goal Selection
</subsectionHeader>
<bodyText confidence="0.999926666666667">
The goal selection module selects a goal that MIMIC at-
tempts to achieve in its response by utilizing informa-
tion from analytical cue detection as shown in Figure 4.
MIMIC&apos;s goals focus on two aspects of cooperative di-
alogue interaction: 1) initiating subdialogues to resolve
anomalies that occur during the dialogue by attempting
to instantiate an unspecified attribute, constraining an at-
tribute for which multiple values have been specified, or
correcting an invalid assumption in the case of invalid
</bodyText>
<footnote confidence="0.99625">
4In practice, this is the preferred initiative holder since practical
reasons may prevent the dialogue participant from actually holding the
initiative. For instance, if having task initiative dictates inclusion of
additional helpful information, this can only be realized if MIMIC&apos;s
knowledge base provides such information.
</footnote>
<page confidence="0.703638">
100
</page>
<table confidence="0.979361133333333">
Cue Class Cue BPA
Discourse TakeOverTask mt—tot({speaker)) = 0.35; mt—t0t(0) = 0.65
Analytical NoNewInfo mt—nm({hearer }) = 0.35; Int-nnz(e) = 0.65
InvalidAction nit—sa({hearer}) = 0.35; mt_,.(0) = 0.65
InvalidActionResolved mt--zar ((hearer)) = 0.35; mt-2.,(0) = 0.65
AmbiguousAction mt_aa ({hearer}) = 0.35; int—..(e) --= 0.65
AmbiguousActionResolved Trtt—aar ({speaker}) = 0.35; int—.(0) = 0.65
(a) Task Initiative
Cue Class Cue BPA
Discourse TakeOverTask md—tot({speaker}) = 0.35; Md-t0(e) = 0.65
Analytical NoNewInfo Ind-nnt ghear = 0.35; md—nn, (0) = 0.65
InvalidAction ind_ta({hearer}) = 0.7; 7nd-in(e) = 0.3
InvalidActionResolved rnd,({hearer}) = 0.7; md—,a, (e) = 0.3
AmbiguousAction md—aa({hearer}) = 0.7; md—aa (C) = 0.3
AmbiguousActionResolved md—aar({speakerl) = 0.7; md—.(0) = 0.3
</table>
<figure confidence="0.868819">
(b) Dialogue Initiative
</figure>
<figureCaption confidence="0.997198">
Figure 3: Cues and BPAs for Modeling Initiative in MIMIC
</figureCaption>
<figure confidence="0.987845928571429">
Seleet-Goal(SemRep):
If AmbiguousAction detected
ambiguous-attr get-ambiguous(SemRep)
1* get name of ambiguous attribute *1
If (number-values(ambiguous-attr) == 0)
/* attribute unspecified */
Instantiate(ambiguous-attr)
Else /* more than one value specified
Constrain(ambiguous-attr)
Else if InvalidAction detected
ProvideNegativeAnswer(SemRep)
Else 1* well-formed query */
answer 4-- database-query(SemRep)
ProvideAnswer(answer)
</figure>
<figureCaption confidence="0.999913">
Figure 4: Goal Selection Algorithm
</figureCaption>
<bodyText confidence="0.997529666666667">
user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti
and Zukerman, 1993; Qu and Beale, 1999), and 2) pro-
viding answers to well-formed queries (steps 9-11).
</bodyText>
<subsectionHeader confidence="0.54351">
3.2.3 Strategy Selection
</subsectionHeader>
<bodyText confidence="0.9996525">
Previous work has argued that initiative affects the de-
gree of control an agent has in the dialogue interaction
(Whittaker and Stenton, 1988; Walker and Whittaker,
1990; Chu-Carroll and Brown, 1998). Thus, a cooper-
ative system may adopt different strategies to achieve the
same goal depending on the initiative distribution. Since
task initiative models contribution to domain/problem-
solving goals, while dialogue initiative affects the cur-
</bodyText>
<footnote confidence="0.622559666666667">
5An alternative strategy to step (4) is to perform a database lookup
based on the ambiguous query and summarize the results (Litman et
al., 1998), which we leave for future work.
</footnote>
<bodyText confidence="0.996227857142857">
rent discourse goal, we developed alternative strategies
for achieving the goals in Figure 4 based on initiative
distribution, as shown in Table 1.
The strategies employed when MIMIC has only dia-
logue initiative are similar to the mixed initiative dia-
logue strategies employed by many existing spoken di-
alogue systems (e.g., (Bennacef et at., 1996; Stent et
al., 1999)). To instantiate an attribute, MIMIC adopts
the InfoSeek dialogue act to solicit the missing informa-
tion. In contrast, when MIMIC has both initiatives, it
plays a more active role by presenting the user with addi-
tional information comprising valid instantiations of the
attribute (GiveOptions). Given an invalid query, MIMIC
notifies the user of the failed query and provides an open-
ended prompt when it only has dialogue initiative. When
MIMIC has both initiatives, however, in addition to No-
tifyFailure, it suggests an alternative close to the user&apos;s
original query and provides a limited prompt. Finally,
when MIMIC has neither initiative, it simply adopts No-
tifyFailure, allowing the user to determine the next dis-
course goal.
</bodyText>
<subsectionHeader confidence="0.999042">
3.3 Utterance Generation
</subsectionHeader>
<bodyText confidence="0.9544581">
MIMIC employs a simple template-driven utterance gen-
eration approach. Templates are associated with dialogue
acts as shown in Table 2.6 The generation component re-
ceives from the dialogue manager the selected dialogue
acts and the parameters needed to instantiate the tem-
plates. It then generates the system response, which is
sent to the TTS module for spoken output synthesis.
61n most cases, there is a one-to-one-mapping between dialogue
acts and templates. The exceptions are Answer, NotifyFailure, and
SuggestAltemative, whose templates vary based on the question type.
</bodyText>
<page confidence="0.994153">
101
</page>
<table confidence="0.999846111111111">
Task + Dialogue Dialogue None
Constrain Clarify Clarify Acknowledge
Instantiate GiveOptions InfoSeek Acknowledge
InfoSeek
ProvideAnswer Answer Answer Answer
LimitedPrompt OpenPrompt
ProvideNegativeAnswer NotifyFailure NotifyFailure NotifyFailure
SuggestAlternative OpenPrompt
LimitedPrompt
</table>
<tableCaption confidence="0.998778">
Table 1: Strategy Selection Based on Goal and Initiative Distribution
</tableCaption>
<figure confidence="0.81630945">
Dialogue Act
Clarify
Acknowledge
GiveOptions
InfoSeek
Answer
OpenPrompt
LimitedPrompt
NotifyFailure
SuggestAlternative
Template
&amp;quot;Did you say &lt; valuer &gt; . . ., or &lt; value?, &gt;T&amp;quot;
&amp;quot;Uh-huh.&amp;quot;
&amp;quot;Choices for &lt; attribute &gt; are &lt; valuei &gt; &lt; value. &gt;.&amp;quot;
&amp;quot;What &lt; attribute &gt; would you like?&amp;quot;
E.g., &amp;quot;&lt; movie &gt; is playing at &lt; theater &gt; at &lt; timei &gt; . . . &lt; timen &gt;.&amp;quot;
&amp;quot;Can I help you with anything else?&amp;quot;
&amp;quot;Please say the name of the movie or theater or town you would like information about.&amp;quot;
E.g., &amp;quot;&lt; movie &gt; is not playing at &lt; theater &gt; .
E.g., &amp;quot;&lt; movie &gt; is playing at &lt; alternativetheater &gt; at &lt; timer &gt; &lt; timen &gt;.&amp;quot;
</figure>
<tableCaption confidence="0.967343">
Table 2: Mappings Between Dialogue Acts and Utterance Templates
</tableCaption>
<sectionHeader confidence="0.988058" genericHeader="method">
4 Examples
</sectionHeader>
<bodyText confidence="0.999978875">
To illustrate MIMIC&apos;s adaptation capabilities, we return
to the dialogue in Figure 1, which is repeated in Figure 5
and annotated with the cues detected in each user turn
(in boldfaced italics) and the dialogue acts employed for
response generation in each system turn (in boldface).
The bpas representing the initiative distribution for ut-
terance (3) are the initial bpas, which, based on MIMIC&apos;s
role as an information provider, are
</bodyText>
<equation confidence="0.997541">
Int—(3) ({S}) = 0.3, mt—(3)({U}) = 0.7;
md—(3)({S}) = 0.6, md—(3) = 0.4.
</equation>
<bodyText confidence="0.998845">
The cue AmbiguousAction is detected in utterance (3)
because the mandatory attribute theater was not specified
and cannot be inferred (since the town of Montclair has
multiple theaters). The bpas representing its effect are
instantiated as follows (Figure 3):
</bodyText>
<equation confidence="0.988481">
mt—aa({S}) = 0.35, mt_aa(0) = 0.65;
md—aa({S}) = 0.7, md_aa(0) = 0.3.
</equation>
<bodyText confidence="0.987935">
Combining the current bpas with the effects of the ob-
served cue, we obtain the following new bpas:
</bodyText>
<equation confidence="0.894258">
mt—(4)({S}) = 0.4, mt—(4)(IUD = 0.6;
md—(4)({S}) = 0.83, md—(4)({U}) = 0.17.
</equation>
<bodyText confidence="0.981410058823529">
The updated bpas indicate that MIMIC should have dia-
logue but not task initiative when attempting to resolve
the detected ambiguity in utterance (4).
MIMIC selects Instantiate as its goal to be achieved
(Figure 4), which, based on the initiative distribution,
leads it to select the InfoSeek action (Table 1) and gener-
ate the query &amp;quot;What theater would you like?&amp;quot;
The user&apos;s response in (5) again triggers Ambiguous-
Action, as well as NoNewInfo since the semantic repre-
sentations of (3) and (5) are identical, given the dialogue
context. When the effects of these cues are taken into
account, we have the following initiative distribution for
utterance (6):
= 0.62, mg—(6)({U}) = 0.38;
md_(6)({S}) = 0.96, md—(6)({U}) = 0.04.
Although Instantiate is again selected as the goal,
MIMIC now has both task and dialogue initiatives; thus
it selects both GiveOptions and InfoSeek to achieve this
goal and generates utterances (6) and (7). The addi-
tional information, in the form of valid theater choices,
helps the user provide the missing value in (8), allow-
ing MIMIC to answer the query in (9) and prompt for
the next query. However, despite the limited prompt,
the user provides a well-formed query in (11), trigger-
ing TakeOverTask. Thus, MIMIC answers the query and
switches to an open-ended prompt in (13), relinquishing
task initiative to the user.
In addition to its automatic adaptation capabilities, an-
other advantage of MIMIC is the ease of modifying its
adaptation behavior, enabled by the decoupling of the
initiative module from the goal and strategy selection
processes. For instance, a system-initiative version of
MIMIC can be achieved by setting the initial bpas as fol-
lows:
</bodyText>
<equation confidence="0.999491">
Mt—initial ({S}) = 1;
Md—initial({S}) = 1.
</equation>
<page confidence="0.993192">
102
</page>
<table confidence="0.98579125">
Sr Hello, this is MIMIC, the movie information system.
U: How can I help you?
What time is Analyze This playing in Montclair?
[what time is analyze this playing in montclair] [AmbiguousAction]
Sr What theater would you like? [InfoSeek]
U: Montclair.
[montclair] [AmbiguousAction,NoNewInfo]
Sr Choices for theater are Wellmont theater and Clearviews Screening Zone [GiveOptions]
What theater would you like? [InfoSeek]
U: Clearviews Screening Zone.
[um movie screening zone] [AmbiguousActionResolved]
Sr Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, [Answer]
4:10pm, and 6:30pm.
Please say the name of the movie or theater or town you would like information [LimitedPrompt]
about.
U: What time is Analyze This playing at the Wellmont theater?
[what time is analyze this playing at the wellmont theater] [TakeOverTask]
Sr Analyze this is playing at Wellmont theater in Montclair at 3:15pm, 5:45pm, [Answer]
8:20pm, and 10:10pm.
Can I help you with anything else? [OpenPrompt]
</table>
<figureCaption confidence="0.998671">
Figure 5: Annotated Dialogue Shown in Figure 1
</figureCaption>
<bodyText confidence="0.99973">
This is because in the Dempster-Shafer theory, if the
initial bpas or the bpas for a cue provide definite evi-
dence for drawing a certain conclusion, then no subse-
quent cue has any effect on changing that conclusion.
Thus, MIMIC will retain both initiatives throughout the
dialogue. Alternatively, versions of MIMIC with differ-
ent adaptation behavior can be achieved by tailoring the
initial bpas and/or the bpas for each cue based on the ap-
plication. For instance, for an electronic sales agent, the
effect of InvalidAction can be increased so that when the
user orders an out-of-stock item, the system will always
take over task initiative and suggest an alternative item.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="method">
5 System Evaluation
</sectionHeader>
<bodyText confidence="0.999717692307692">
We conducted two experiments to evaluate MIMIC&apos;s au-
tomatic adaptation capabilities. We compared MIMIC
with two control systems: MIMIC-SI, a system-initiative
version of MIMIC in which the system retains both ini-
tiatives throughout the dialogue, and MIMIC-MI, a non-
adaptive mixed-initiative version of MIMIC that resem-
bles the behavior of many existing dialogue systems. In
this section we summarize these experiments and their
results. A companion paper describes the evaluation pro-
cess and results in further detail (Chu-Carroll and Nick-
erson, 2000).
Each experiment involved eight users interacting with
MIMIC and MIMIC-SI or MIMIC-MI to perform a set of
tasks, each requiring the user to obtain specific movie in-
formation. User satisfaction was assessed by asking the
subjects to fill out a questionnaire after interacting with
each version of the system. Furthermore, a number of
performance features, largely based on the PARADISE
dialogue evaluation scheme (Walker et al., 1997), were
automatically logged, derived, or manually annotated. In
addition, we logged the cues automatically detected in
each user utterance, as well as the initiative distribution
for each turn and the dialogue acts selected to generate
each system response.
The features gathered from the dialogue interactions
were analyzed along three dimensions: system perfor-
mance, discourse features (in terms of characteristics
of the resulting dialogues, such as the cues detected in
user utterances), and initiative distribution. Our results
show that MIMIC&apos;s adaptation capabilities 1) led to bet-
ter system performance in terms of user satisfaction, dia-
logue efficiency (shorter dialogues), and dialogue quality
(fewer ASR timeouts), and 2) better matched user expec-
tations (by giving up task initiative when the user intends
to have control of the dialogue interaction) and more effi-
ciently resolved dialogue anomalies (by taking over task
initiative to provide guidance when no progress is made
in the dialogue, or to constrain user utterances when ASR
performance is poor).
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999275933333333">
In this paper, we discussed MIMIC, an adaptive mixed-
initiative spoken dialogue system. MIMIC&apos;s automatic
adaptation capabilities allow it to employ appropriate
strategies based on the cumulative effect of information
dynamically extracted from user utterances during dia-
logue interactions, enabling MIMIC to provide more co-
operative and satisfactory responses than existing non-
adaptive systems. Furthermore, MIMIC was imple-
mented as a general framework for information query
systems by decoupling its initiative module from the
goal selection process, while allowing the outcome of
both processes to jointly determine the response strate-
gies employed. This feature enables easy modification to
MIMIC&apos;s adaptation behavior, thus allowing the frame-
work to be used for rapid development and comparisons
</bodyText>
<page confidence="0.998038">
103
</page>
<bodyText confidence="0.973386">
of experimental prototypes of spoken dialogue systems.
</bodyText>
<sectionHeader confidence="0.995493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999836857142857">
The author would like to thank Egbert Ammicht, An-
toine Saad, Qiru Zhou, Wolfgang Reich], and Stefan
Ortmanns for their help on system integration and on
ASR/telephony server development, Jill Nickerson for
conducting the evaluation experiments, and Bob Carpen-
ter, Diane Litman, Christine Nakatani, and Jill Nickerson
for their comments on an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999128802083333">
S. Bennacef, L. Devillers, S. Rosset, and L. Lamel.
1996. Dialog in the RAILTEL telephone-based sys-
tem. In Proceedings of the 4th International Confer-
ence on Spoken Language Processing.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8(3-4):215-253.
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361-388.
Jennifer Chu-Carroll and Jill S. Nickerson. 2000. Evalu-
ating automatic dialogue strategy adaptation for a spo-
ken dialogue system. In Proceedings of the 1st Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. To appear.
Jean Gordon and Edward H. Shortliffe. 1984. The
Dempster-Shafer theory of evidence. In Bruce
Buchanan and Edward Shortliffe, editors, Rule-Based
Expert Systems: The MYCIN Experiments of the
Stanford Heuristic Programming Project, chapter 13,
pages 272-292. Addison-Wesley.
Lori Lamel. 1998. Spoken language dialog system de-
velopment and evaluation at LIMSI. In Proceedings
of the International Symposium on Spoken Dialogue,
pages 9-17.
Diane J. Litman and Shimei Pan. 1999. Empirically
evaluating an adaptable spoken dialogue system. In
Proceedings of the 7th International Conference on
User Modeling, pages 55-64.
Diane J. Litman, Shimei Pan, and Marilyn A. Walker.
1998. Evaluating response strategies in a web-based
spoken dialogue agent. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 780-786.
Christine H. Nakatani and Jennifer Chu-Carroll. 2000.
Using dialogue representations for concept-to-speech
generation. In Proceedings of the ANLP-NAACL
Workshop on Conversational Systems.
Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999.
An efficient decoding method for real time speech
recognition. In Proceedings of the 5th European Con-
ference on Speech Communication and Technology.
Yan Qu and Steve Beale. 1999. A constraint-based
model for cooperative response generation in informa-
tion dialogues. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intelligence.
Bhavani Raskutti and Ingrid Zukerman. 1993. Elicit-
ing additional information during cooperative consul-
tations. In Proceedings of the 15th Annual Meeting of
the Cognitive Science Society.
Wolfgang Reichl and Wu. Chou. 1998. Decision tree
state tying based on segmental clustering for acoustic
modeling. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing.
Gerald Salton. 1971. The SMART Retrieval System.
Prentice Hall, Inc.
Stephanie Seneff, James Glass, David Goddeau, David
Goodine, Lynette Hirschman, Hong Leung, Michael
Phillips, Joseph Polifroni, and Victor Zue. 1991. De-
velopment and preliminary evaluation of the MIT
ATIS system. In Proceedings of the DARPA Speech
and Natural Language Workshop, pages 88-93.
Glenn Shafer. 1976. A Mathematical Theory of Evi-
dence. Princeton University Press.
Richard Sproat, editor. 1998. Multilingual Text-to-
Speech Synthesis: The Bell Labs Approach. Kluwer,
Boston, MA.
Amanda Stent, John Dowding, Jean Mark Gawron, Eliz-
abeth Owen Bratt, and Robert Moore. 1999. The
CommandTalk spoken dialogue system. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 183-190.
Peter van Beek, Robin Cohen, and Ken Schmidt. 1993.
From plan critiquing to clarification dialogue for co-
operative response generation. Computational Intelli-
gence, 9 (2):132-154 .
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse
segmentation. In Proceedings of the 28th Annual
Meeting of the Association for Computational Lin-
guistics, pages 70-78.
Marilyn A. Walker, Diane J. Litman, Candance A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 271-280.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogues. In Proceedings of the
26th Annual Meeting of the Association for Computa-
tional Linguistics, pages 123-130.
Qiru Zhou, Chin-Hui Lee, Wu Chou, and Andrew Pargel-
lis. 1997. Speech technology integration and research
platform: A system study. In Proceedings of the 5th
European Conference on Speech Communication and
Technology.
</reference>
<page confidence="0.998753">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879453">
<title confidence="0.999634">MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries</title>
<author confidence="0.999444">Jennifer Chu-Carroll</author>
<affiliation confidence="0.99843">Lucent Technologies Bell Laboratories</affiliation>
<address confidence="0.999807">600 Mountain Avenue Murray Hill, NJ 07974, U.S.A.</address>
<email confidence="0.999919">jencc@research.bell-labs.com</email>
<abstract confidence="0.991173">This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC&apos;s dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bennacef</author>
<author>L Devillers</author>
<author>S Rosset</author>
<author>L Lamel</author>
</authors>
<title>Dialog in the RAILTEL telephone-based system.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th International Conference on Spoken Language Processing.</booktitle>
<marker>Bennacef, Devillers, Rosset, Lamel, 1996</marker>
<rawString>S. Bennacef, L. Devillers, S. Rosset, and L. Lamel. 1996. Dialog in the RAILTEL telephone-based system. In Proceedings of the 4th International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Michael K Brown</author>
</authors>
<title>An evidential model for tracking initiative in collaborative dialogue interactions. User Modeling and UserAdapted Interaction,</title>
<date>1998</date>
<pages>8--3</pages>
<contexts>
<context position="1667" citStr="Chu-Carroll and Brown, 1998" startWordPosition="226" endWordPosition="229">mains. Most existing systems employ dialogue strategies pre-specified during the design phase of the dialogue manager without taking into account characteristics of actual dialogue interactions. More specifically, mixed initiative systems typically employ rules that specify conditions (generally based on local dialogue context) under which initiative may shift from one agent to the other. Previous research, on the other hand, has shown that changes in initiative strategies in human-human dialogues can be dynamically modeled in terms of characteristics of the user and of the on-going dialogue (Chu-Carroll and Brown, 1998) and that adaptability of initiative strategies in dialogue systems leads to better system performance (Litman and Pan, 1999). However, no previous dialogue system takes into account these dialogue characteristics or allows for initiative-oriented adaptation of dialogue strategies. In this paper, we describe MIMIC, a voice-enabled telephone-based dialogue system that provides movie showtime information, emphasizing its dialogue management aspects. MIMIC improves upon previous systems along two dimensions. First, MIMIC automatically adapts dialogue strategies based on participant roles, charact</context>
<context position="4346" citStr="Chu-Carroll and Brown, 1998" startWordPosition="623" endWordPosition="626"> an excerpt from a dialogue between MIMIC and an actual user where the user is attempting to find the times at which the movie Analyze This playing at theaters in Montclair. S and U indicate system and user utterances, respectively, and the italicized utterances are the output of our automatic speech recognizer. In addition, each system turn is annotated with its task and dialogue initiative holders, where task initiative tracks the lead in the process toward achieving the dialogue participants&apos; domain goal, while dialogue initiative models the lead in determining the current discourse focus (Chu-Carroll and Brown, 1998). In our information query application domain, the system has task (and thus dialogue) initiative if its utterances provide helpful guidance toward achieving the user&apos;s domain goal, as in utterances (6) and (7) where MIMIC provided valid response choices to its query intending to solicit a theater name, while the system has 97 dialogue but not task initiative if its utterances only specify the current discourse goal, as in utterance (4).1 This dialogue illustrates several features of our adaptive mixed initiative dialogue manager. First, MIMIC automatically adapted the initiative distribution </context>
<context position="6798" citStr="Chu-Carroll and Brown, 1998" startWordPosition="1001" endWordPosition="1004">terances (1) and (13)). In the next section, we briefly review a framework for dynamic initiative modeling. In Section 3, we discuss how this framework was incorporated into the dialogue management component of a spoken dialogue system to allow for automatic adaptation of dialogue strategies. Finally, we outline experiments evaluating the resulting system and show that MIMIC&apos;s automatic adaptation capabilities resulted in better system performance. 2.2 An Evidential Framework for Modeling Initiative In previous work, we proposed a framework for modeling initiative during dialogue interaction (Chu-Carroll and Brown, 1998). This framework predicts task and dialogue initiative holders on a turn-by-turn basis in humanhuman dialogues based on participant roles (such as each dialogue agent&apos;s level of expertise and the role that she plays in the application domain), cues observed in the current dialogue turn, and dialogue history. More specifically, we utilize the Dempster-Shafer theory (Shafer, 1976; Gordon and Shortliffe, 1984), and represent the current initiative distribution as two basic probability assignments (bpas) which indicate the amount of support for each dialogue participant having the task and dialogu</context>
<context position="15715" citStr="Chu-Carroll and Brown, 1998" startWordPosition="2366" endWordPosition="2369">stribution, a set of dialogue acts that MIMIC will adopt in its attempt to realize the selected goal. 3.2.1 Initiative Modeling MIMIC&apos;s initiative module determines the task and dialogue initiative holders for each system turn in order to enable dynamic strategy adaptation. It automatically detects cues triggered during the current user turn, and combines the effects of these cues with the current initiative distribution to determine the initiative holders for the system&apos;s turn. Cue Detection The cues and the bpas representing their effects are largely based on a subset of those described in (Chu-Carroll and Brown, 1998),3 as shown in Figures 3(a) and 3(b). Figure 3(a) shows that observation of TakeOverTask supports a task initiative shift to the speaker to the degree .35. The remaining support is assigned to 0, the set of all possible conclusions (i.e., {speaker,hearer}), indicating that to the degree .65, observation of this cue does not commit to identifying which dialogue participant should have task initiative in the next dialogue turn. The cues used in MIMIC are classified into two categories, discourse cues and analytical cues, based on the types of knowledge needed to detect them: I. Discourse cues, w</context>
<context position="21112" citStr="Chu-Carroll and Brown, 1998" startWordPosition="3159" endWordPosition="3162">se /* more than one value specified Constrain(ambiguous-attr) Else if InvalidAction detected ProvideNegativeAnswer(SemRep) Else 1* well-formed query */ answer 4-- database-query(SemRep) ProvideAnswer(answer) Figure 4: Goal Selection Algorithm user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti and Zukerman, 1993; Qu and Beale, 1999), and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. The strategies employed when MI</context>
</contexts>
<marker>Chu-Carroll, Brown, 1998</marker>
<rawString>Jennifer Chu-Carroll and Michael K. Brown. 1998. An evidential model for tracking initiative in collaborative dialogue interactions. User Modeling and UserAdapted Interaction, 8(3-4):215-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Bob Carpenter</author>
</authors>
<date>1999</date>
<booktitle>Vectorbased natural language call routing. Computational Linguistics,</booktitle>
<pages>25--3</pages>
<contexts>
<context position="13546" citStr="Chu-Carroll and Carpenter, 1999" startWordPosition="2036" endWordPosition="2040">u-Carroll, 2000) for how MIMIC&apos;s dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation. Question-Type: When Movie: Analyze This Theater: null Town: Montclair (a) Semantic Representation Question-Type: When Movie: mandatory Theater: mandatory Town: optional (b) Task Specification Figure 2: Semantic Representation and Task Specification in Montclair?&amp;quot; MIMIC&apos;s semantic representation is constructed by first extracting, for each attribute, a set of keywords from the user utterance. Using a vector-based topic identification process (Salton, 1971; Chu-Carroll and Carpenter, 1999), these keywords are used to determine a set of likely values (including null) for that attribute. Next, the utterance is interpreted with respect to the dialogue history and the system&apos;s domain knowledge. This allows MIMIC to handle elliptical sentences and anaphoric references, as well as automatically infer missing values and detect inconsistencies in the current representation. This semantic representation allows for decoupling of domain-dependent task specifications and domain99 independent dialogue management strategies. Each query type is specified by a template indicating, for each att</context>
</contexts>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Jennifer Chu-Carroll and Bob Carpenter. 1999. Vectorbased natural language call routing. Computational Linguistics, 25(3):361-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Jill S Nickerson</author>
</authors>
<title>Evaluating automatic dialogue strategy adaptation for a spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<note>To appear.</note>
<contexts>
<context position="29319" citStr="Chu-Carroll and Nickerson, 2000" startWordPosition="4453" endWordPosition="4457">ys take over task initiative and suggest an alternative item. 5 System Evaluation We conducted two experiments to evaluate MIMIC&apos;s automatic adaptation capabilities. We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems. In this section we summarize these experiments and their results. A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000). Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information. User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system. Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated. In addition, we logged the cues automatically detected in each user utterance, as well as the initiative </context>
</contexts>
<marker>Chu-Carroll, Nickerson, 2000</marker>
<rawString>Jennifer Chu-Carroll and Jill S. Nickerson. 2000. Evaluating automatic dialogue strategy adaptation for a spoken dialogue system. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Gordon</author>
<author>Edward H Shortliffe</author>
</authors>
<title>The Dempster-Shafer theory of evidence.</title>
<date>1984</date>
<booktitle>Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project, chapter 13,</booktitle>
<pages>272--292</pages>
<editor>In Bruce Buchanan and Edward Shortliffe, editors,</editor>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="7208" citStr="Gordon and Shortliffe, 1984" startWordPosition="1064" endWordPosition="1067">ies resulted in better system performance. 2.2 An Evidential Framework for Modeling Initiative In previous work, we proposed a framework for modeling initiative during dialogue interaction (Chu-Carroll and Brown, 1998). This framework predicts task and dialogue initiative holders on a turn-by-turn basis in humanhuman dialogues based on participant roles (such as each dialogue agent&apos;s level of expertise and the role that she plays in the application domain), cues observed in the current dialogue turn, and dialogue history. More specifically, we utilize the Dempster-Shafer theory (Shafer, 1976; Gordon and Shortliffe, 1984), and represent the current initiative distribution as two basic probability assignments (bpas) which indicate the amount of support for each dialogue participant having the task and dialogue initiatives. For instance, the bpa mt—cur ({S)) = &apos;Although the dialogues we collected in our experiments (Section 5) include cases in which MIMIC has neither initiative, such cases are rare in this application domain, and will not be discussed further in this paper. 0.3, Mt—cur({U}) = 0.7 indicates that, with all evidence taken into account, there is more support (to the degree 0.7) for the user having t</context>
<context position="9042" citStr="Gordon and Shortliffe, 1984" startWordPosition="1362" endWordPosition="1365">ntially prompting the hearer to take over initiative to resolve the detected ambiguity, etc. The effects that each cue has on changing the current bpas are also represented as bpas, which were determined by an iterative training procedure using a corpus of transcribed dialogues where each turn was annotated with the task/dialogue initiative holders and the observed cues. The bpas for the next turn are computed by combining the bpas representing the current initiative distribution and the bpas representing the effects of cues observed during the current turn, using Dempster&apos;s combination rule (Gordon and Shortliffe, 1984). The task and dialogue initiative holders are then predicted based on the new bpas. This framework was evaluated using annotated dialogues from four task-oriented domains, achieving, on average, a correct prediction rate of 97% and 88% for task and dialogue initiative holders, respectively. In Section 3.2, we discuss how this predictive model is converted into a generative model by enabling the system to automatically detect cues that were previously labelled manually. We further discuss how the model is used by the dialogue manager for dynamic dialogue strategy adaptation. 3 MIMIC: Mixed Ini</context>
<context position="18123" citStr="Gordon and Shortliffe, 1984" startWordPosition="2737" endWordPosition="2740">well-formed, triggered when a mandatory attribute is unspecified or when more than one value is specified for an attribute. • AmbiguousActionResolved, triggered when the attribute in question is uniquely instantiated. Computing Initiative Distribution To determine the initiative distribution, the bpas representing the effects of cues detected in the current user utterance are instantiated (i.e., speaker/hearer in Figure 3 are instantiated as system/user accordingly). These effects are then interpreted with respect to the current initiative distribution by applying Dempster&apos;s combination rule (Gordon and Shortliffe, 1984) to the bpas representing the current initiative distribution and the instantiated bpas. This results in two new bpas representing the task and dialogue initiative distributions for the system&apos;s turn. The dialogue participant with the greater degree of support for having the task/dialogue initiative in these bpas is the task/dialogue initiative holder for the system&apos;s turn4 (see Section 4 for an example). 3.2.2 Goal Selection The goal selection module selects a goal that MIMIC attempts to achieve in its response by utilizing information from analytical cue detection as shown in Figure 4. MIMIC</context>
</contexts>
<marker>Gordon, Shortliffe, 1984</marker>
<rawString>Jean Gordon and Edward H. Shortliffe. 1984. The Dempster-Shafer theory of evidence. In Bruce Buchanan and Edward Shortliffe, editors, Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project, chapter 13, pages 272-292. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Lamel</author>
</authors>
<title>Spoken language dialog system development and evaluation at LIMSI.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Symposium on Spoken Dialogue,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="12713" citStr="Lamel, 1998" startWordPosition="1924" endWordPosition="1925">etail in the rest of this section. 4. Text-to-speech engine: the TTS system receives the word string comprising the system&apos;s response from the dialogue component and converts the text into speech for output over the telephone. We used the Bell Labs TTS system (Sproat, 1998), which in addition to converting plain text into speech, accepts text strings annotated to override default pitch height, accent placement, speaking rate, etc.2 3.1 Semantic Interpretation MIMIC utilizes a non-recursive frame-based semantic representation commonly used in spoken dialogue systems (e.g. (Seneff et al., 1991; Lamel, 1998)), which represents an utterance as a set of attribute-value pairs. Figure 2(a) shows the frame-based semantic representation for the utterance &amp;quot;What time is Analyze This playing 2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC&apos;s dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation. Question-Type: When Movie: Analyze This Theater: null Town: Montclair (a) Semantic Representation Question-Type: When Movie: mandatory Theater: mandatory Town: optional (b) Task Specification Figure 2: Semantic Representation and Task Specification in Montcla</context>
</contexts>
<marker>Lamel, 1998</marker>
<rawString>Lori Lamel. 1998. Spoken language dialog system development and evaluation at LIMSI. In Proceedings of the International Symposium on Spoken Dialogue, pages 9-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
</authors>
<title>Empirically evaluating an adaptable spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th International Conference on User Modeling,</booktitle>
<pages>55--64</pages>
<contexts>
<context position="1792" citStr="Litman and Pan, 1999" startWordPosition="244" endWordPosition="247"> into account characteristics of actual dialogue interactions. More specifically, mixed initiative systems typically employ rules that specify conditions (generally based on local dialogue context) under which initiative may shift from one agent to the other. Previous research, on the other hand, has shown that changes in initiative strategies in human-human dialogues can be dynamically modeled in terms of characteristics of the user and of the on-going dialogue (Chu-Carroll and Brown, 1998) and that adaptability of initiative strategies in dialogue systems leads to better system performance (Litman and Pan, 1999). However, no previous dialogue system takes into account these dialogue characteristics or allows for initiative-oriented adaptation of dialogue strategies. In this paper, we describe MIMIC, a voice-enabled telephone-based dialogue system that provides movie showtime information, emphasizing its dialogue management aspects. MIMIC improves upon previous systems along two dimensions. First, MIMIC automatically adapts dialogue strategies based on participant roles, characteristics of the current utterance, and dialogue history. This automatic adaptation allows appropriate dialogue strategies to </context>
</contexts>
<marker>Litman, Pan, 1999</marker>
<rawString>Diane J. Litman and Shimei Pan. 1999. Empirically evaluating an adaptable spoken dialogue system. In Proceedings of the 7th International Conference on User Modeling, pages 55-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
<author>Marilyn A Walker</author>
</authors>
<title>Evaluating response strategies in a web-based spoken dialogue agent.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>780--786</pages>
<contexts>
<context position="21499" citStr="Litman et al., 1998" startWordPosition="3218" endWordPosition="3221">ps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems (e.g., (Bennacef et at., 1996; Stent et al., 1999)). To instantiate an attribute, MIMIC adopts the InfoSeek dialogue act to solicit the missing information. In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the</context>
</contexts>
<marker>Litman, Pan, Walker, 1998</marker>
<rawString>Diane J. Litman, Shimei Pan, and Marilyn A. Walker. 1998. Evaluating response strategies in a web-based spoken dialogue agent. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 780-786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine H Nakatani</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Using dialogue representations for concept-to-speech generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL Workshop on Conversational Systems.</booktitle>
<contexts>
<context position="12930" citStr="Nakatani and Chu-Carroll, 2000" startWordPosition="1955" endWordPosition="1958">r output over the telephone. We used the Bell Labs TTS system (Sproat, 1998), which in addition to converting plain text into speech, accepts text strings annotated to override default pitch height, accent placement, speaking rate, etc.2 3.1 Semantic Interpretation MIMIC utilizes a non-recursive frame-based semantic representation commonly used in spoken dialogue systems (e.g. (Seneff et al., 1991; Lamel, 1998)), which represents an utterance as a set of attribute-value pairs. Figure 2(a) shows the frame-based semantic representation for the utterance &amp;quot;What time is Analyze This playing 2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC&apos;s dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation. Question-Type: When Movie: Analyze This Theater: null Town: Montclair (a) Semantic Representation Question-Type: When Movie: mandatory Theater: mandatory Town: optional (b) Task Specification Figure 2: Semantic Representation and Task Specification in Montclair?&amp;quot; MIMIC&apos;s semantic representation is constructed by first extracting, for each attribute, a set of keywords from the user utterance. Using a vector-based topic identification process (Salton, 1971; Chu-Carroll and </context>
</contexts>
<marker>Nakatani, Chu-Carroll, 2000</marker>
<rawString>Christine H. Nakatani and Jennifer Chu-Carroll. 2000. Using dialogue representations for concept-to-speech generation. In Proceedings of the ANLP-NAACL Workshop on Conversational Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Ortmanns</author>
<author>Wolfgang Reichl</author>
<author>Wu Chou</author>
</authors>
<title>An efficient decoding method for real time speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="10274" citStr="Ortmanns et al., 1999" startWordPosition="1552" endWordPosition="1555">vie Information Consultant MIMIC is a telephone-based dialogue system that provides movie showtime information. It consists of the following main components, implemented on a distributed, client-server architecture (Zhou et al., 1997): 1. Telephony server: this component detects rings and hang-ups, and enables streaming of audio data on channels of a telephony board. 2. Speech recognizer: the recognizer receives audio data from the telephony server and generates the word string hypothesis that best matches the audio input. We used the Lucent Automatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns et al., 1999), configured to use class-based probabilistic ngram language models to allow for rapid updates of movie/theater/town names. 98 Utterance Task Dialogue Initiative Initiative S: Hello, this is MIMIC, the movie information system. User System How can I help you? U: What time is Analyze This playing in Montclair? [what time is analyze this playing in montclair] S: What theater would you like? User System U: Montclair. [montclair] S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System System What theater would you like? U: Clearviews Screening Zone. [urn movie screening z</context>
</contexts>
<marker>Ortmanns, Reichl, Chou, 1999</marker>
<rawString>Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999. An efficient decoding method for real time speech recognition. In Proceedings of the 5th European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>Steve Beale</author>
</authors>
<title>A constraint-based model for cooperative response generation in information dialogues.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="20825" citStr="Qu and Beale, 1999" startWordPosition="3115" endWordPosition="3118">igure 3: Cues and BPAs for Modeling Initiative in MIMIC Seleet-Goal(SemRep): If AmbiguousAction detected ambiguous-attr get-ambiguous(SemRep) 1* get name of ambiguous attribute *1 If (number-values(ambiguous-attr) == 0) /* attribute unspecified */ Instantiate(ambiguous-attr) Else /* more than one value specified Constrain(ambiguous-attr) Else if InvalidAction detected ProvideNegativeAnswer(SemRep) Else 1* well-formed query */ answer 4-- database-query(SemRep) ProvideAnswer(answer) Figure 4: Goal Selection Algorithm user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti and Zukerman, 1993; Qu and Beale, 1999), and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup ba</context>
</contexts>
<marker>Qu, Beale, 1999</marker>
<rawString>Yan Qu and Steve Beale. 1999. A constraint-based model for cooperative response generation in information dialogues. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhavani Raskutti</author>
<author>Ingrid Zukerman</author>
</authors>
<title>Eliciting additional information during cooperative consultations.</title>
<date>1993</date>
<booktitle>In Proceedings of the 15th Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="20804" citStr="Raskutti and Zukerman, 1993" startWordPosition="3111" endWordPosition="3114">0.3 (b) Dialogue Initiative Figure 3: Cues and BPAs for Modeling Initiative in MIMIC Seleet-Goal(SemRep): If AmbiguousAction detected ambiguous-attr get-ambiguous(SemRep) 1* get name of ambiguous attribute *1 If (number-values(ambiguous-attr) == 0) /* attribute unspecified */ Instantiate(ambiguous-attr) Else /* more than one value specified Constrain(ambiguous-attr) Else if InvalidAction detected ProvideNegativeAnswer(SemRep) Else 1* well-formed query */ answer 4-- database-query(SemRep) ProvideAnswer(answer) Figure 4: Goal Selection Algorithm user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti and Zukerman, 1993; Qu and Beale, 1999), and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform</context>
</contexts>
<marker>Raskutti, Zukerman, 1993</marker>
<rawString>Bhavani Raskutti and Ingrid Zukerman. 1993. Eliciting additional information during cooperative consultations. In Proceedings of the 15th Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chou</author>
</authors>
<title>Decision tree state tying based on segmental clustering for acoustic modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="10250" citStr="Chou, 1998" startWordPosition="1550" endWordPosition="1551">nitiative Movie Information Consultant MIMIC is a telephone-based dialogue system that provides movie showtime information. It consists of the following main components, implemented on a distributed, client-server architecture (Zhou et al., 1997): 1. Telephony server: this component detects rings and hang-ups, and enables streaming of audio data on channels of a telephony board. 2. Speech recognizer: the recognizer receives audio data from the telephony server and generates the word string hypothesis that best matches the audio input. We used the Lucent Automatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns et al., 1999), configured to use class-based probabilistic ngram language models to allow for rapid updates of movie/theater/town names. 98 Utterance Task Dialogue Initiative Initiative S: Hello, this is MIMIC, the movie information system. User System How can I help you? U: What time is Analyze This playing in Montclair? [what time is analyze this playing in montclair] S: What theater would you like? User System U: Montclair. [montclair] S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System System What theater would you like? U: Clearviews Screening Zone</context>
</contexts>
<marker>Chou, 1998</marker>
<rawString>Wolfgang Reichl and Wu. Chou. 1998. Decision tree state tying based on segmental clustering for acoustic modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Salton</author>
</authors>
<date>1971</date>
<booktitle>The SMART Retrieval System.</booktitle>
<publisher>Prentice Hall, Inc.</publisher>
<contexts>
<context position="13512" citStr="Salton, 1971" startWordPosition="2034" endWordPosition="2035">akatani and Chu-Carroll, 2000) for how MIMIC&apos;s dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation. Question-Type: When Movie: Analyze This Theater: null Town: Montclair (a) Semantic Representation Question-Type: When Movie: mandatory Theater: mandatory Town: optional (b) Task Specification Figure 2: Semantic Representation and Task Specification in Montclair?&amp;quot; MIMIC&apos;s semantic representation is constructed by first extracting, for each attribute, a set of keywords from the user utterance. Using a vector-based topic identification process (Salton, 1971; Chu-Carroll and Carpenter, 1999), these keywords are used to determine a set of likely values (including null) for that attribute. Next, the utterance is interpreted with respect to the dialogue history and the system&apos;s domain knowledge. This allows MIMIC to handle elliptical sentences and anaphoric references, as well as automatically infer missing values and detect inconsistencies in the current representation. This semantic representation allows for decoupling of domain-dependent task specifications and domain99 independent dialogue management strategies. Each query type is specified by a</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>Gerald Salton. 1971. The SMART Retrieval System. Prentice Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
<author>James Glass</author>
<author>David Goddeau</author>
<author>David Goodine</author>
<author>Lynette Hirschman</author>
<author>Hong Leung</author>
<author>Michael Phillips</author>
<author>Joseph Polifroni</author>
<author>Victor Zue</author>
</authors>
<title>Development and preliminary evaluation of the MIT ATIS system.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="12699" citStr="Seneff et al., 1991" startWordPosition="1920" endWordPosition="1923">iscussed in further detail in the rest of this section. 4. Text-to-speech engine: the TTS system receives the word string comprising the system&apos;s response from the dialogue component and converts the text into speech for output over the telephone. We used the Bell Labs TTS system (Sproat, 1998), which in addition to converting plain text into speech, accepts text strings annotated to override default pitch height, accent placement, speaking rate, etc.2 3.1 Semantic Interpretation MIMIC utilizes a non-recursive frame-based semantic representation commonly used in spoken dialogue systems (e.g. (Seneff et al., 1991; Lamel, 1998)), which represents an utterance as a set of attribute-value pairs. Figure 2(a) shows the frame-based semantic representation for the utterance &amp;quot;What time is Analyze This playing 2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC&apos;s dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation. Question-Type: When Movie: Analyze This Theater: null Town: Montclair (a) Semantic Representation Question-Type: When Movie: mandatory Theater: mandatory Town: optional (b) Task Specification Figure 2: Semantic Representation and Task Specificat</context>
</contexts>
<marker>Seneff, Glass, Goddeau, Goodine, Hirschman, Leung, Phillips, Polifroni, Zue, 1991</marker>
<rawString>Stephanie Seneff, James Glass, David Goddeau, David Goodine, Lynette Hirschman, Hong Leung, Michael Phillips, Joseph Polifroni, and Victor Zue. 1991. Development and preliminary evaluation of the MIT ATIS system. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 88-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Shafer</author>
</authors>
<title>A Mathematical Theory of Evidence.</title>
<date>1976</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="7178" citStr="Shafer, 1976" startWordPosition="1062" endWordPosition="1063">tion capabilities resulted in better system performance. 2.2 An Evidential Framework for Modeling Initiative In previous work, we proposed a framework for modeling initiative during dialogue interaction (Chu-Carroll and Brown, 1998). This framework predicts task and dialogue initiative holders on a turn-by-turn basis in humanhuman dialogues based on participant roles (such as each dialogue agent&apos;s level of expertise and the role that she plays in the application domain), cues observed in the current dialogue turn, and dialogue history. More specifically, we utilize the Dempster-Shafer theory (Shafer, 1976; Gordon and Shortliffe, 1984), and represent the current initiative distribution as two basic probability assignments (bpas) which indicate the amount of support for each dialogue participant having the task and dialogue initiatives. For instance, the bpa mt—cur ({S)) = &apos;Although the dialogues we collected in our experiments (Section 5) include cases in which MIMIC has neither initiative, such cases are rare in this application domain, and will not be discussed further in this paper. 0.3, Mt—cur({U}) = 0.7 indicates that, with all evidence taken into account, there is more support (to the deg</context>
</contexts>
<marker>Shafer, 1976</marker>
<rawString>Glenn Shafer. 1976. A Mathematical Theory of Evidence. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<title>Multilingual Text-toSpeech Synthesis: The Bell Labs Approach.</title>
<date>1998</date>
<editor>Richard Sproat, editor.</editor>
<publisher>Kluwer,</publisher>
<location>Boston, MA.</location>
<marker>1998</marker>
<rawString>Richard Sproat, editor. 1998. Multilingual Text-toSpeech Synthesis: The Bell Labs Approach. Kluwer, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>John Dowding</author>
<author>Jean Mark Gawron</author>
<author>Elizabeth Owen Bratt</author>
<author>Robert Moore</author>
</authors>
<title>The CommandTalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="21901" citStr="Stent et al., 1999" startWordPosition="3283" endWordPosition="3286">o domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems (e.g., (Bennacef et at., 1996; Stent et al., 1999)). To instantiate an attribute, MIMIC adopts the InfoSeek dialogue act to solicit the missing information. In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute (GiveOptions). Given an invalid query, MIMIC notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative. When MIMIC has both initiatives, however, in addition to NotifyFailure, it suggests an alternative close to the user&apos;s original query and provides a limited prompt. </context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>Amanda Stent, John Dowding, Jean Mark Gawron, Elizabeth Owen Bratt, and Robert Moore. 1999. The CommandTalk spoken dialogue system. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter van Beek</author>
<author>Robin Cohen</author>
<author>Ken Schmidt</author>
</authors>
<title>From plan critiquing to clarification dialogue for cooperative response generation.</title>
<date>1993</date>
<journal>Computational Intelligence,</journal>
<volume>9</volume>
<pages>2--132</pages>
<marker>van Beek, Cohen, Schmidt, 1993</marker>
<rawString>Peter van Beek, Robin Cohen, and Ken Schmidt. 1993. From plan critiquing to clarification dialogue for cooperative response generation. Computational Intelligence, 9 (2):132-154 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed initiative in dialogue: An investigation into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>70--78</pages>
<contexts>
<context position="21082" citStr="Walker and Whittaker, 1990" startWordPosition="3155" endWordPosition="3158">stantiate(ambiguous-attr) Else /* more than one value specified Constrain(ambiguous-attr) Else if InvalidAction detected ProvideNegativeAnswer(SemRep) Else 1* well-formed query */ answer 4-- database-query(SemRep) ProvideAnswer(answer) Figure 4: Goal Selection Algorithm user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti and Zukerman, 1993; Qu and Beale, 1999), and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. T</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Marilyn Walker and Steve Whittaker. 1990. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candance A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="29753" citStr="Walker et al., 1997" startWordPosition="4521" endWordPosition="4524">tems. In this section we summarize these experiments and their results. A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000). Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information. User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system. Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated. In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response. The features gathered from the dialogue interactions were analyzed along three dimensions: system performance, discourse features (in terms of characteristics of the resulting dialogues, such as the cues detected in user utterances), and initiative distribution. Our results show that MIMIC&apos;s adaptation capabilities 1) led to better system p</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candance A. Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 271-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Whittaker</author>
<author>Phil Stenton</author>
</authors>
<title>Cues and control in expert-client dialogues.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--130</pages>
<contexts>
<context position="21054" citStr="Whittaker and Stenton, 1988" startWordPosition="3151" endWordPosition="3154">* attribute unspecified */ Instantiate(ambiguous-attr) Else /* more than one value specified Constrain(ambiguous-attr) Else if InvalidAction detected ProvideNegativeAnswer(SemRep) Else 1* well-formed query */ answer 4-- database-query(SemRep) ProvideAnswer(answer) Figure 4: Goal Selection Algorithm user queries (steps 1-8)5 (van Beek et al., 1993; Raskutti and Zukerman, 1993; Qu and Beale, 1999), and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribu</context>
</contexts>
<marker>Whittaker, Stenton, 1988</marker>
<rawString>Steve Whittaker and Phil Stenton. 1988. Cues and control in expert-client dialogues. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 123-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiru Zhou</author>
<author>Chin-Hui Lee</author>
<author>Wu Chou</author>
<author>Andrew Pargellis</author>
</authors>
<title>Speech technology integration and research platform: A system study.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="9886" citStr="Zhou et al., 1997" startWordPosition="1491" endWordPosition="1494">7% and 88% for task and dialogue initiative holders, respectively. In Section 3.2, we discuss how this predictive model is converted into a generative model by enabling the system to automatically detect cues that were previously labelled manually. We further discuss how the model is used by the dialogue manager for dynamic dialogue strategy adaptation. 3 MIMIC: Mixed Initiative Movie Information Consultant MIMIC is a telephone-based dialogue system that provides movie showtime information. It consists of the following main components, implemented on a distributed, client-server architecture (Zhou et al., 1997): 1. Telephony server: this component detects rings and hang-ups, and enables streaming of audio data on channels of a telephony board. 2. Speech recognizer: the recognizer receives audio data from the telephony server and generates the word string hypothesis that best matches the audio input. We used the Lucent Automatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns et al., 1999), configured to use class-based probabilistic ngram language models to allow for rapid updates of movie/theater/town names. 98 Utterance Task Dialogue Initiative Initiative S: Hello, this is MIMIC, the movie info</context>
</contexts>
<marker>Zhou, Lee, Chou, Pargellis, 1997</marker>
<rawString>Qiru Zhou, Chin-Hui Lee, Wu Chou, and Andrew Pargellis. 1997. Speech technology integration and research platform: A system study. In Proceedings of the 5th European Conference on Speech Communication and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>