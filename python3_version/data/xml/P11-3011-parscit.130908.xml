<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002453">
<title confidence="0.9922405">
Optimistic Backtracking
A Backtracking Overlay for Deterministic Incremental Parsing
</title>
<author confidence="0.98554">
Gisle Ytrestøl
</author>
<affiliation confidence="0.9988725">
Department of Informatics
University of Oslo
</affiliation>
<email confidence="0.98874">
gisley@ifi.uio.no
</email>
<sectionHeader confidence="0.998552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999471583333333">
This paper describes a backtracking strategy
for an incremental deterministic transition-
based parser for HPSG. The method could
theoretically be implemented on any other
transition-based parser with some adjust-
ments. In this paper, the algorithm is evaluated
on CuteForce, an efficient deterministic shift-
reduce HPSG parser. The backtracking strat-
egy may serve to improve existing parsers, or
to assess if a deterministic parser would bene-
fit from backtracking as a strategy to improve
parsing.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982706515151515">
Incremental deterministic parsing has received in-
creased awareness over the last decade. Process-
ing linguistic data linearly is attractive both from
a computational and a cognitive standpoint. While
there is a rich research tradition in statistical parsing,
the predominant approach derives from chart pars-
ing and is inherently non-deterministic.
A deterministic algorithm will incrementally ex-
pand a syntactic/semantic derivation as it reads the
input sentence one word/token at the time. There are
a number of attractive features to this approach. The
time-complexity will be linear when the algorithm is
deterministic, i.e. it does not allow for later changes
to the partial derivation, only extensions to it. For a
number of applications, e.g. speech recognition, the
ability to process input on the fly per word, and not
per sentence, can also be vital. However, there are
inherent challenges to an incremental parsing algo-
rithm. Garden paths are the canonical example of
58
sentences that are typically misinterpret due to an
early incorrect grammatical assumption.
(1) The horse raced past the barn fell.
The ability to reevaluate an earlier grammatical as-
sumption is disallowed by a deterministic parser.
Optimistic Backtracking is an method designed to
locate the incorrect parser decision in an earlier
stage if the parser reaches an illegal state, i.e. a state
in which a valid parse derivation cannot be retrieved.
The Optimistic Backtracking method will try to lo-
cate the first incorrect parsing decision made by the
parser, and replace this decision with the correct
transition, and resume parsing from this state.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999936176470588">
Incremental deterministic classifier-based parsing
algorithms have been studied in dependency pars-
ing (Nivre and Scholz, 2004; Yamada and Mat-
sumoto, 2003) and CFG parsing (Sagae and Lavie,
2005). Johansson and Nugues (2006) describe
a non-deterministic implementation to the depen-
dency parser outlined by Nivre and Scholz (2004),
where they apply an n-best beam search strategy.
For a highly constrained unification-based for-
malism like HPSG, a deterministic parsing strategy
could frequently lead to parse failures. Ninomiya
et al. (2009) suggest an algorithm for determinis-
tic shift-reduce parsing in HPSG. They outline two
backtracking strategies for HPSG parsing. Their ap-
proach allows the parser to enter an old state if pars-
ing fails or ends with non-sentential success, based
on the minimal distance between the best candidate
</bodyText>
<subsectionHeader confidence="0.377213">
Proceedings of the ACL-HLT 2011 Student Session, pages 58–63,
</subsectionHeader>
<bodyText confidence="0.9768789">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
and the second best candidate in the sequence of
transitions leading up to the current stage. Further
constraints may be added, i.e. restricting the number
of states the parser may backtrack. This algorithm
is expanded by using a beam-thresholding best-first
search algorithm, where each state in the parse has a
state probability defined by the product of the prob-
abilities of the selecting actions that has been taken
to reach the state.
</bodyText>
<sectionHeader confidence="0.996244" genericHeader="method">
3 CuteForce
</sectionHeader>
<bodyText confidence="0.998502352941176">
Optimistic Backtracking is in this paper used to
evaluate CuteForce, an incremental deterministic
HPSG parser currently in development. Simi-
lar to MaltParser (Nivre et al., 2007), it employs
a classifier-based oracle to guide the shift-reduce
parser that incrementally builds a syntactic/semantic
HPSG derivation defined by LinGO English Re-
source Grammar (ERG) (Flickinger, 2000).
Parser Layout CuteForce has a more complex
transition system than MaltParser in order to facil-
itate HPSG parsing. The sentence input buffer Q is
a list of tuples with token, part-of-speech tags and
HPSG lexical types (i.e. supertags (Bangalore and
Joshi, 1999)).
Given a set of ERG rules R and a sentence buffer
Q, a parser configuration is a tuple c = (α, Q, t, 7r)
where:
</bodyText>
<listItem confidence="0.997792625">
• α is a stack of “active” edges1
• Q is a list of tuples of word forms W,
part of speech tags POS and lexical
types LT derived from a sentence x =
((W1, POS1, LT1), ...(W,, POS,,, LTA,,)).
• t is the current input position in Q
• 7r is a stack of passive edges instantiating a
ERG rule
</listItem>
<bodyText confidence="0.997153">
The stack of passive edges 7r makes up the full
HPSG representation of the input string if the string
is accepted.
</bodyText>
<footnote confidence="0.9467955">
1An “active” edges in our sense is a hypothesis of an ap-
plication of a binary rule where the left daughter is known (an
element of ir), and the specific binary ERG rule and the right
daughter is yet to be found.
</footnote>
<bodyText confidence="0.9960504">
Transition System The shift-reduce parser has
four different transitions, two of which are param-
eterized with a unary or binary ERG rule, which are
added to the passive edges, hence building the HPSG
structure. The four transitions are:
</bodyText>
<listItem confidence="0.9987682">
• ACTIVE – (adds an active edge to stack α, and
increments t)
• UNIT(R1) – (adds unary passive edge to 7r in-
stantiating unary ERG rule (R1))
• PASSIVE(R2) – (pops α and adds binary pas-
sive edge to 7r instantiating binary ERG rule
(R2))
• ACCEPT – (terminates the parse of the sen-
tence. 7r represents the HPSG derivation of the
sentence)
</listItem>
<bodyText confidence="0.998639">
Derivation Example Figure 1 is a derivation ex-
ample from Redwoods Treebank (Oepen et al.,
2002). We note that the tree derivation consists
of unary and binay productions, corresponding to
the UNIT(R1) and PASSIVE(R2) parser transitions.
Further, the pre-terminal lexical types have a le suf-
fix, and are provided together with the terminal word
form in the input buffer for the parser.
</bodyText>
<figure confidence="0.400176">
sb-hd mc c
</figure>
<figureCaption confidence="0.99916">
Figure 1: HPSG derivation from Redwoods Treebank.
</figureCaption>
<bodyText confidence="0.894181214285714">
Parsing Configuration Mode CuteForce can op-
erate in three different oracle configurations: HPSG
Unification mode, CFG approximation mode and
unrestricted mode.
In HPSG Unification mode, the parser validates
that no oracle decisions lead to an invalid HPSG
derivation. All UNIT and PASSIVE transitions are
d - prt-div le
“some”
“can”
hd-cmp u c
v j-nb-pas-tr dlr
v pas odlr
v np* le
“specialized”
v n3s-bse ilr
v np* le
“narrate”
hdn bnp-pn c
w hyphen plr
n - pl le
“RSS-”
w period plr
n pl olr
n - mc le
“feeds.”
n ms ilr
n - m le
</bodyText>
<figure confidence="0.845122714285714">
hdn bnp c
np-hdn cpd c
“software”
aj-hdn norm c
v vp mdl-p le
sp-hd n c
hd-cmp u c
</figure>
<page confidence="0.982868">
59
</page>
<bodyText confidence="0.993811469387756">
an implicit unification. For each parsing stage, the probability. Utilizing global information also seems
parsing oracle returns a ranked list of transitions. more sound from a human point of view. Consider
The highest-ranked transition not violating a unifi- sentence (1), it’s first when the second verb (fell) is
cation constraint will be executed. If no transition encountered that we would re-evaluate our original
yields a valid unification, parsing fails for the given assumption, namely that raced may not be the head
sentence. verb of the sentence. That fell indeed is a verb is
In CFG mode, a naive CFG approximation of the surely relevant information for reconsidering raced
ERG is employed to guide the oracle. The CFG ap- as the head of a relative clause.
proximation consists of CFG rules harvested from When the parser halts, the backtracker will rank
the treebanks used in training the parser – for this each transition produced up until the point of fail-
purpose we have used existing Redwoods treebanks ure according to which transition is most likely to be
used in training, and augmented with derivations the first incorrect transition. When the best scoring
from WikiWoods, in total 300,000 sentences. Each transition is located, the parser will backtrack to this
ERG rule instantiation, using the identifiers shown position, and replace this transition with the pars-
in Figure 1 as non-terminal symbols, will be treated ing oracle’s second-best scoring transition for this
as a CFG rule, and each parser action will be val- current parsing state. If the parser later comes to
idated against the set of CFG rules. If the parser another halt, only the transitions occurring after the
action yields a CFG projection not found among the first backtrack will be subject to change. Hence, the
valid CFG rules in the CFG approximation, the CFG backtracker will always assume that its last back-
filter will block this transition. If the parser arrives track was correct (thus being Optimistic). Having
at a state where the CFG filter blocks all further tran- allowed the parser to backtrack unrestrictedly, we
sitions, parsing fails. could theoretically have reached close to 100 %
In unrestricted mode, the oracle chooses the high- coverage, but the insights of parsing incrementally
est scoring transition without any further restrictions would have become less pronounced.
imposed. In this setting, the parser typically reaches The search space for the backtracker is n * m
close to 100 % coverage – the only sentences not where n is the number of candidate transitions, and
covered in this setting are instances where the parser m is the total number of parser transitions. In Op-
enters an infinite unit production loop. Hence, we timistic Backtracking we disregard the m dimension
will only evaluate the parser in CFG and Unification altogether by always choosing the second-best tran-
mode in this paper. sition candidate ranked by the parsing oracle, as-
4 Optimistic Backtracking suming that the second-ranked transition in the given
Optimistic Backtracking can be added as an overlay state actually was the correct transition. Hence we
to a transition-based parser in order to evaluate the reduce the search-space to the n-dimension. In this
parser in non-deterministic mode. The overlay has paper, using CuteForce as HPSG parser, this as-
a linear time-complexity. This backtracking method sumption holds in about 80-90 % of the backtracks
is, to the best of our knowledge, the only method that in CFG mode, in HPSG Unification mode this num-
applies ranking rather than some probability-based ber is somewhat lower.
algorithm for backtracking. This aspect is critical 4.1 Baseline
for classification-based parsing oracles that do not As a baseline for identifying the incorrect transition,
yield a probability score in the ranking of candidate we use a strategy inspired by Ninomiya et al. (2009),
transitions. namely to pick the candidate transition with the min-
Treating backtracking as a ranking problem has imal probability difference between the best and the
several attractive features. It may combine global second best transition candidate. However, since we
and local syntactic and semantic information related do not have true probability, a pseudo-probability
to each candidate transition, contrary to a probabilis- is computed by taking the dot product of the fea-
tic approach that only employs the local transition ture vector and weight-vector for each best-scoring
60 (P) and second-best scoring (P2) candidate transi-
tion, and use the proportion of the second-best score
over the joint probability of the best and second-best
</bodyText>
<equation confidence="0.7430775">
scoring transition: P2
P+P2
</equation>
<bodyText confidence="0.9999383">
In our development test set of 1794 sentences, we
ran the parser in CFG and HPSG unification mode
in deterministic and non-deterministic mode. The
baseline results are found in Table 1 (CFG-BL) and
Table 2 (UNI-BL). In CFG mode (Table 1), we ob-
tain a 51.2 % reduction in parsing failure. In unifica-
tion mode (Table 2) the parser is much more likely
to fail, as the parse derivations are guaranteed to
be a valid HPSG derivation. Baseline backtracking
yields a mere 10 % reduction in parsing failures.
</bodyText>
<subsectionHeader confidence="0.648878">
4.2 Feature Model
</subsectionHeader>
<bodyText confidence="0.99998975">
Each candidate transition is mapped to a feature
vector that provides information about the transi-
tion. The task for the ranker is to identify the first
incorrect transition in the sequence of transitions.
The feature model used by the ranker employs fea-
tures that can roughly be divided in three. First, the
transition-specific features provide information on
the nature of the candidate transition and surround-
ing transitions. Here we also have features related to
the pseudo-probability of the transition (provided by
the parsing oracle), and the oracle score distance be-
tween the best-scoring and second-best scoring tran-
sition for each given state. Secondly we have fea-
tures related to the last token that was processed by
the parser before it reached an invalid state, and the
information on the incomplete HPSG derivation that
was built at that state. These features are used in
combination with local transition-specific features.
Third, we have features concerning the preliminary
HPSG derivation in the actual state of the transition.
Feature Types The list of transitions T = t0, t1, ...
t,,, comprises the candidate transitions that are sub-
ject to backtracking upon parsing failure. The fea-
ture types used by the backtracker includes:
</bodyText>
<listItem confidence="0.9948304">
• the pseudo-probability of the best scoring (P)
and second best scoring (P2) transition
• the transition category of the current transition
• the probability proportion of the second best
scoring transition over the joint probability
</listItem>
<page confidence="0.6356635">
P2
P+P2
</page>
<listItem confidence="0.9965004">
• the transition number in the list of applicable
candidates, and the number of remaining tran-
sitions, relative to the list of candidates
• the last lexical tag and part-of-speech tag that
were processed before parsing failure
• the head category of the HPSG derivation and
the left daughter unification candidate for the
HPSG derivation in the current position
• the lexical tag relative to the current position in
the buffer
</listItem>
<bodyText confidence="0.99987325">
The backtracker is trained as a linear SVM us-
ing SVM,, ,k (Joachims, 2006). Totally, the feature
vector maps 24 features for each transition, includ-
ing several combinations of the feature types above.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999489416666667">
In this paper we trained CuteForce with data from
Redwoods Treebank, augmented with derivations
from WikiWoods (Flickinger et al., 2010). The test
set contains a random sample of 1794 sentences
from the Redwoods Treebank (which was excluded
from the training data), with an average length of 14
tokens. Training data for the backtracker is extracted
by parsing derivations from WikiWoods determin-
istically, and record transition candidates each time
parsing fails, labeling the correct backtracking can-
didate, backtrack to this point, and resume parsing
from this state.
</bodyText>
<subsectionHeader confidence="0.613173">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999848823529412">
The first column (CFG-NB and UNI-NB) in Table 1
and 2 indicates the scores when the parser is run in
deterministic mode, i.e. without backtracking. The
second and third column contain results for baseline
and Optimistic backtracking. Coverage refers to the
proportion of sentences that received a parse. Pre-
cision refers to the backtracker’s precision with re-
spect to identifying the incorrect transition among
the candidate transitions. — BT Cand is the aver-
age number of candidate transitions the backtracker
ranks when trying to predict the incorrect transition,
and — BT Cand,1st is the number of candidates at
the initial point-of-failure. Exact Matches is the to-
tal number of parse derivations which are identical
to the gold standard.
For Ms per Sent (milliseconds per sentence) it
should be said that the code is not optimized, es-
</bodyText>
<page confidence="0.998331">
61
</page>
<bodyText confidence="0.9994202">
pecially with respect to the HPSG unification algo-
rithm2. How the figures relate to one another should
however give a good indication on how the compu-
tational costs vary between the different configura-
tions.
</bodyText>
<table confidence="0.999360714285714">
CFG -NB CFG -BL CFG -Opt
Coverage 0.754 0.880 0.899
Precision N/A 0.175 0.235
—BT Cand N/A 26.1 30.6
—BT Cand,1st N/A 51.5 51.5
Exact Matches 727 746 742
Ms per Sent 10.7 45.0 72.5
</table>
<tableCaption confidence="0.997632">
Table 1: Results – CFG mode
</tableCaption>
<table confidence="0.999828428571429">
UNI -NB UNI -BL UNI -Opt
Coverage 0.574 0.598 0.589
Precision N/A 0.183 0.206
—BT Cand N/A 12.89 20.12
—BT Cand,1st N/A 51.6 51.6
Exact Matches 776 777 776
Ms per Sent 1801.4 5519.1 5345.2
</table>
<tableCaption confidence="0.99965">
Table 2: Results – HPSG unification mode
</tableCaption>
<subsectionHeader confidence="0.980616">
5.4 Conclusion
</subsectionHeader>
<bodyText confidence="0.999984888888889">
to the fact that the backtracker still has relatively low
precision, as only a perfect prediction would leave
the parser capable of deriving an exact match.
The success rate of about 0.23 in picking the in-
correct transition in a set of in average 30 candidates
indicates that treating the backtracking as a ranking
problem is promising. The precision rate in itself is
however relatively low, which serves as an indica-
tion of the difficulty of this task.
</bodyText>
<subsectionHeader confidence="0.949453">
5.3 HPSG Unification
</subsectionHeader>
<bodyText confidence="0.999991111111111">
In unification mode the we see no substantive dif-
ference between deterministic mode, and baseline
and Optimistic backtracking, and practically no im-
provement in the quality of the parses produced.
In Table 2 we see that the only striking difference
between the figures for the parser in backtracking
mode and deterministic mode is the efficiency – the
time consumption is increased by approximately a
factor of 3.
</bodyText>
<subsectionHeader confidence="0.990725">
5.2 CFG approximation
</subsectionHeader>
<bodyText confidence="0.999348644444445">
The number of failed sentences is greatly reduced
when backtracking is enabled. Using baseline back-
tracking, the reduction is 51.2 %, whereas Opti-
mistic backtracking has a 59.1 % reduction in parse
failures. Further, Optimistic Backtracker performs
substantially better than baseline in identifying in-
correct transitions.
The average number of candidate transitions
ranged from 26 to 30 for the baseline and Optimistic
backtracking strategy. It’s interesting to observe that
even with a success rate of about 1/5 in identifying
the incorrect transition, the coverage is still greatly
increased. That backtracking manages to recover
so many sentences that initially failed, even if it
does not manage to identify the incorrect transition,
would seem to indicate that even when mistaken, the
backtracker is producing a good prediction. On the
other hand, the exact match score does not improve
the same way as the coverage, this is directly related
2Specifically, the current unification back-end preforms
non-destructive unification, i.e. it does not take advantage of
the deterministic nature of CuteForce
The findings in this paper are specific to CuteForce.
It is however very likely that the results would be
similar for other deterministic HPSG parsers.
In CFG mode, the number of failed parses are
more than halved compared to deterministic mode.
It is likely that further increase could be obtained by
relaxing constraints in the Optimistic algorithm.
In Unification mode, we experienced only a slight
increase in coverage. By relaxing the Optimistic
constraints, the time-complexity would go up. Con-
sidering how little the parser benefited from back-
tracking in unification mode with Optimistic con-
straints, it seems implausible that the parser will
improve considerably without a heavy relaxation of
the constraints in the Optimistic algorithm. If do-
ing so, the attractive features of the parser’s inher-
ently deterministic nature will be overshadowed by
a very large number of backtracks at a heavy compu-
tational cost. Hence, it’s hard to see that such a semi-
deterministic approach could have any advantages
over other non-deterministic HPSG parsers neither
in computational cost, performance or on a cogni-
tive level.
</bodyText>
<page confidence="0.998919">
62
</page>
<sectionHeader confidence="0.998508" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996007">
The author would like to thank Stephan Oepen (Uni-
versity of Oslo) and Joakim Nivre (Uppsala Uni-
versity) for their valued input and inspiring feed-
back during the writing of this paper, and in the
PhD project. Experimentation and engineering was
made possible through access to the TITAN high-
performance computing facilities at the University
of Oslo (UiO), and we are grateful to the Scientific
Computation staff at UiO, as well as to the Norwe-
gian Metacenter for Computational Science.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999624115384615">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, pages 237–265.
Dan Flickinger, Stephan Oepen, and Gisle Ytrestøl.
2010. Wikiwoods: Syntacto-semantic annotation
for english wikipedia. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC’10). European Language Re-
sources Association (ELRA).
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1):15 – 28.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 217–226. ACM.
Richard Johansson and Pierre Nugues. 2006. Investi-
gating multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 206–210. Association
for Computational Linguistics.
Takashi Ninomiya, Nobuyuki Shimizu, Takuya Mat-
suzaki, and Hiroshi Nakagawa. 2009. Deterministic
shift-reduce parsing for unification-based grammars
by using default unification. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 603–611.
Association for Computational Linguistics.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of English text. In Proceed-
ings of the 20th international conference on Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95–135.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Chris
Manning, Dan Flickinger, and Thorsten Brants. 2002.
The LinGO Redwoods treebank. Motivation and pre-
liminary applications. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125–132. Association for Compu-
tational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceedings of the 8th International Workshop on
Parsing Technologies, pages 195–206.
</reference>
<page confidence="0.999463">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672578">
<title confidence="0.997996">Optimistic Backtracking A Backtracking Overlay for Deterministic Incremental Parsing</title>
<author confidence="0.809011">Gisle</author>
<affiliation confidence="0.997695">Department of University of</affiliation>
<email confidence="0.912715">gisley@ifi.uio.no</email>
<abstract confidence="0.993165538461539">This paper describes a backtracking strategy for an incremental deterministic transitionbased parser for HPSG. The method could theoretically be implemented on any other transition-based parser with some adjustments. In this paper, the algorithm is evaluated on CuteForce, an efficient deterministic shiftreduce HPSG parser. The backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would benefit from backtracking as a strategy to improve parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing. Computational Linguistics,</title>
<date>1999</date>
<pages>237--265</pages>
<contexts>
<context position="4399" citStr="Bangalore and Joshi, 1999" startWordPosition="661" endWordPosition="664">tracking is in this paper used to evaluate CuteForce, an incremental deterministic HPSG parser currently in development. Similar to MaltParser (Nivre et al., 2007), it employs a classifier-based oracle to guide the shift-reduce parser that incrementally builds a syntactic/semantic HPSG derivation defined by LinGO English Resource Grammar (ERG) (Flickinger, 2000). Parser Layout CuteForce has a more complex transition system than MaltParser in order to facilitate HPSG parsing. The sentence input buffer Q is a list of tuples with token, part-of-speech tags and HPSG lexical types (i.e. supertags (Bangalore and Joshi, 1999)). Given a set of ERG rules R and a sentence buffer Q, a parser configuration is a tuple c = (α, Q, t, 7r) where: • α is a stack of “active” edges1 • Q is a list of tuples of word forms W, part of speech tags POS and lexical types LT derived from a sentence x = ((W1, POS1, LT1), ...(W,, POS,,, LTA,,)). • t is the current input position in Q • 7r is a stack of passive edges instantiating a ERG rule The stack of passive edges 7r makes up the full HPSG representation of the input string if the string is accepted. 1An “active” edges in our sense is a hypothesis of an application of a binary rule w</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics, pages 237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Gisle Ytrestøl</author>
</authors>
<title>Wikiwoods: Syntacto-semantic annotation for english wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="14230" citStr="Flickinger et al., 2010" startWordPosition="2302" endWordPosition="2305">cal tag and part-of-speech tag that were processed before parsing failure • the head category of the HPSG derivation and the left daughter unification candidate for the HPSG derivation in the current position • the lexical tag relative to the current position in the buffer The backtracker is trained as a linear SVM using SVM,, ,k (Joachims, 2006). Totally, the feature vector maps 24 features for each transition, including several combinations of the feature types above. 5 Evaluation In this paper we trained CuteForce with data from Redwoods Treebank, augmented with derivations from WikiWoods (Flickinger et al., 2010). The test set contains a random sample of 1794 sentences from the Redwoods Treebank (which was excluded from the training data), with an average length of 14 tokens. Training data for the backtracker is extracted by parsing derivations from WikiWoods deterministically, and record transition candidates each time parsing fails, labeling the correct backtracking candidate, backtrack to this point, and resume parsing from this state. 5.1 Results The first column (CFG-NB and UNI-NB) in Table 1 and 2 indicates the scores when the parser is run in deterministic mode, i.e. without backtracking. The s</context>
</contexts>
<marker>Flickinger, Oepen, Ytrestøl, 2010</marker>
<rawString>Dan Flickinger, Stephan Oepen, and Gisle Ytrestøl. 2010. Wikiwoods: Syntacto-semantic annotation for english wikipedia. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<pages>1--15</pages>
<contexts>
<context position="4137" citStr="Flickinger, 2000" startWordPosition="621" endWordPosition="622">nded by using a beam-thresholding best-first search algorithm, where each state in the parse has a state probability defined by the product of the probabilities of the selecting actions that has been taken to reach the state. 3 CuteForce Optimistic Backtracking is in this paper used to evaluate CuteForce, an incremental deterministic HPSG parser currently in development. Similar to MaltParser (Nivre et al., 2007), it employs a classifier-based oracle to guide the shift-reduce parser that incrementally builds a syntactic/semantic HPSG derivation defined by LinGO English Resource Grammar (ERG) (Flickinger, 2000). Parser Layout CuteForce has a more complex transition system than MaltParser in order to facilitate HPSG parsing. The sentence input buffer Q is a list of tuples with token, part-of-speech tags and HPSG lexical types (i.e. supertags (Bangalore and Joshi, 1999)). Given a set of ERG rules R and a sentence buffer Q, a parser configuration is a tuple c = (α, Q, t, 7r) where: • α is a stack of “active” edges1 • Q is a list of tuples of word forms W, part of speech tags POS and lexical types LT derived from a sentence x = ((W1, POS1, LT1), ...(W,, POS,,, LTA,,)). • t is the current input position </context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1):15 – 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13954" citStr="Joachims, 2006" startWordPosition="2263" endWordPosition="2264"> transition • the probability proportion of the second best scoring transition over the joint probability P2 P+P2 • the transition number in the list of applicable candidates, and the number of remaining transitions, relative to the list of candidates • the last lexical tag and part-of-speech tag that were processed before parsing failure • the head category of the HPSG derivation and the left daughter unification candidate for the HPSG derivation in the current position • the lexical tag relative to the current position in the buffer The backtracker is trained as a linear SVM using SVM,, ,k (Joachims, 2006). Totally, the feature vector maps 24 features for each transition, including several combinations of the feature types above. 5 Evaluation In this paper we trained CuteForce with data from Redwoods Treebank, augmented with derivations from WikiWoods (Flickinger et al., 2010). The test set contains a random sample of 1794 sentences from the Redwoods Treebank (which was excluded from the training data), with an average length of 14 tokens. Training data for the backtracker is extracted by parsing derivations from WikiWoods deterministically, and record transition candidates each time parsing fa</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Investigating multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>206--210</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2549" citStr="Johansson and Nugues (2006)" startWordPosition="379" endWordPosition="382">an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved. The Optimistic Backtracking method will try to locate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state. 2 Related Work Incremental deterministic classifier-based parsing algorithms have been studied in dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for deterministic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their approach allows the parser to enter an old state if parsing fails or ends with non-sentential success, based on the minimal distance between the best</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>Richard Johansson and Pierre Nugues. 2006. Investigating multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 206–210. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Nobuyuki Shimizu</author>
<author>Takuya Matsuzaki</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Deterministic shift-reduce parsing for unification-based grammars by using default unification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>603--611</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2865" citStr="Ninomiya et al. (2009)" startWordPosition="425" endWordPosition="428">on with the correct transition, and resume parsing from this state. 2 Related Work Incremental deterministic classifier-based parsing algorithms have been studied in dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for deterministic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their approach allows the parser to enter an old state if parsing fails or ends with non-sentential success, based on the minimal distance between the best candidate Proceedings of the ACL-HLT 2011 Student Session, pages 58–63, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics and the second best candidate in the sequence of transitions leading up to the current stage. Further constraints may be added, i.e. restricting the number of </context>
<context position="10673" citStr="Ninomiya et al. (2009)" startWordPosition="1733" endWordPosition="1736">de. The overlay has paper, using CuteForce as HPSG parser, this asa linear time-complexity. This backtracking method sumption holds in about 80-90 % of the backtracks is, to the best of our knowledge, the only method that in CFG mode, in HPSG Unification mode this numapplies ranking rather than some probability-based ber is somewhat lower. algorithm for backtracking. This aspect is critical 4.1 Baseline for classification-based parsing oracles that do not As a baseline for identifying the incorrect transition, yield a probability score in the ranking of candidate we use a strategy inspired by Ninomiya et al. (2009), transitions. namely to pick the candidate transition with the minTreating backtracking as a ranking problem has imal probability difference between the best and the several attractive features. It may combine global second best transition candidate. However, since we and local syntactic and semantic information related do not have true probability, a pseudo-probability to each candidate transition, contrary to a probabilis- is computed by taking the dot product of the featic approach that only employs the local transition ture vector and weight-vector for each best-scoring 60 (P) and second-</context>
</contexts>
<marker>Ninomiya, Shimizu, Matsuzaki, Nakagawa, 2009</marker>
<rawString>Takashi Ninomiya, Nobuyuki Shimizu, Takuya Matsuzaki, and Hiroshi Nakagawa. 2009. Deterministic shift-reduce parsing for unification-based grammars by using default unification. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–611. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2451" citStr="Nivre and Scholz, 2004" startWordPosition="363" endWordPosition="366">r grammatical assumption is disallowed by a deterministic parser. Optimistic Backtracking is an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved. The Optimistic Backtracking method will try to locate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state. 2 Related Work Incremental deterministic classifier-based parsing algorithms have been studied in dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for deterministic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their approach allows the parser to enter an old state if</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="3936" citStr="Nivre et al., 2007" startWordPosition="592" endWordPosition="595">ond best candidate in the sequence of transitions leading up to the current stage. Further constraints may be added, i.e. restricting the number of states the parser may backtrack. This algorithm is expanded by using a beam-thresholding best-first search algorithm, where each state in the parse has a state probability defined by the product of the probabilities of the selecting actions that has been taken to reach the state. 3 CuteForce Optimistic Backtracking is in this paper used to evaluate CuteForce, an incremental deterministic HPSG parser currently in development. Similar to MaltParser (Nivre et al., 2007), it employs a classifier-based oracle to guide the shift-reduce parser that incrementally builds a syntactic/semantic HPSG derivation defined by LinGO English Resource Grammar (ERG) (Flickinger, 2000). Parser Layout CuteForce has a more complex transition system than MaltParser in order to facilitate HPSG parsing. The sentence input buffer Q is a list of tuples with token, part-of-speech tags and HPSG lexical types (i.e. supertags (Bangalore and Joshi, 1999)). Given a set of ERG rules R and a sentence buffer Q, a parser configuration is a tuple c = (α, Q, t, 7r) where: • α is a stack of “acti</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Chris Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods treebank. Motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="5796" citStr="Oepen et al., 2002" startWordPosition="928" endWordPosition="931"> different transitions, two of which are parameterized with a unary or binary ERG rule, which are added to the passive edges, hence building the HPSG structure. The four transitions are: • ACTIVE – (adds an active edge to stack α, and increments t) • UNIT(R1) – (adds unary passive edge to 7r instantiating unary ERG rule (R1)) • PASSIVE(R2) – (pops α and adds binary passive edge to 7r instantiating binary ERG rule (R2)) • ACCEPT – (terminates the parse of the sentence. 7r represents the HPSG derivation of the sentence) Derivation Example Figure 1 is a derivation example from Redwoods Treebank (Oepen et al., 2002). We note that the tree derivation consists of unary and binay productions, corresponding to the UNIT(R1) and PASSIVE(R2) parser transitions. Further, the pre-terminal lexical types have a le suffix, and are provided together with the terminal word form in the input buffer for the parser. sb-hd mc c Figure 1: HPSG derivation from Redwoods Treebank. Parsing Configuration Mode CuteForce can operate in three different oracle configurations: HPSG Unification mode, CFG approximation mode and unrestricted mode. In HPSG Unification mode, the parser validates that no oracle decisions lead to an invali</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Chris Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank. Motivation and preliminary applications. In Proceedings of the 19th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2520" citStr="Sagae and Lavie, 2005" startWordPosition="375" endWordPosition="378">imistic Backtracking is an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved. The Optimistic Backtracking method will try to locate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state. 2 Related Work Incremental deterministic classifier-based parsing algorithms have been studied in dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for deterministic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their approach allows the parser to enter an old state if parsing fails or ends with non-sentential success, based on the mini</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="2480" citStr="Yamada and Matsumoto, 2003" startWordPosition="367" endWordPosition="371"> is disallowed by a deterministic parser. Optimistic Backtracking is an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved. The Optimistic Backtracking method will try to locate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state. 2 Related Work Incremental deterministic classifier-based parsing algorithms have been studied in dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for deterministic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their approach allows the parser to enter an old state if parsing fails or ends with n</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies, pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>