<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.982749">
Approximate Scalable Bounded Space Sketch for Large Data NLP
</title>
<author confidence="0.992059">
Amit Goyal and Hal Daum´e III
</author>
<affiliation confidence="0.997844">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.972156">
College Park, MD 20742
</address>
<email confidence="0.999608">
{amit,hal}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.997399" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992775">
We exploit sketch techniques, especially the
Count-Min sketch, a memory, and time effi-
cient framework which approximates the fre-
quency of a word pair in the corpus without
explicitly storing the word pair itself. These
methods use hashing to deal with massive
amounts of streaming text. We apply Count-
Min sketch to approximate word pair counts
and exhibit their effectiveness on three im-
portant NLP tasks. Our experiments demon-
strate that on all of the three tasks, we get
performance comparable to Exact word pair
counts setting and state-of-the-art system. Our
method scales to 49 GB of unzipped web data
using bounded space of 2 billion counters (8
GB memory).
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983585365854">
There is more data available today on the web than
there has ever been and it keeps increasing. Use
of large data in the Natural Language Processing
(NLP) community is not new. Many NLP problems
(Brants et al., 2007; Turney, 2008; Ravichandran et
al., 2005) have benefited from having large amounts
of data. However, processing large amounts of data
is still challenging.
This has motivated NLP community to use com-
modity clusters. For example, Brants et al. (2007)
used 1500 machines for a day to compute the rela-
tive frequencies of n-grams from 1.8TB of web data.
In another work, a corpus of roughly 1.6 Terawords
was used by Agirre et al. (2009) to compute pair-
wise similarities of the words in the test sets using
the MapReduce infrastructure on 2, 000 cores. How-
ever, the inaccessibility of clusters to an average user
has attracted the NLP community to use streaming,
randomized, and approximate algorithms to handle
large amounts of data (Goyal et al., 2009; Levenberg
et al., 2010; Van Durme and Lall, 2010).
Streaming approaches (Muthukrishnan, 2005)
provide memory and time-efficient framework to
deal with terabytes of data. However, these ap-
proaches are proposed to solve a singe problem.
For example, our earlier work (Goyal et al., 2009)
and Levenberg and Osborne (2009) build approxi-
mate language models and show their effectiveness
in Statistical Machine Translation (SMT). Stream-
based translation models (Levenberg et al., 2010)
has been shown effective to handle large parallel
streaming data for SMT. In Van Durme and Lall
(2009b), a Talbot Osborne Morris Bloom (TOMB)
Counter (Van Durme and Lall, 2009a) was used to
find the top-K verbs “y” given verb “x” using the
highest approximate online Pointwise Mutual Infor-
mation (PMI) values.
In this paper, we explore sketch techniques,
especially the Count-Min sketch (Cormode and
Muthukrishnan, 2004) to build a single model to
show its effectiveness on three important NLP tasks:
</bodyText>
<listItem confidence="0.909317">
• Predicting the Semantic Orientation of words
(Turney and Littman, 2003)
• Distributional Approaches for word similarity
(Agirre et al., 2009)
• Unsupervised Dependency Parsing (Cohen and
Smith, 2010) with a little linguistics knowl-
edge.
</listItem>
<bodyText confidence="0.965661">
In all these tasks, we need to compute association
measures like Pointwise Mutual Information (PMI),
</bodyText>
<page confidence="0.949141">
250
</page>
<note confidence="0.9579265">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250–261,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999293282051282">
and Log Likelihood ratio (LLR) between words. To
compute association scores (AS), we need to count
the number of times pair of words appear together
within a certain window size. However, explicitly
storing the counts of all word pairs is both computa-
tionally expensive and memory intensive (Agirre et
al., 2009; Pantel et al., 2009). Moreover, the mem-
ory usage keeps increasing with increase in corpus
size.
We explore Count-Min (CM) sketch to address
the issue of efficient storage of such data. The
CM sketch stores counts of all word pairs within a
bounded space. Storage space saving is achieved
by approximating the frequency of word pairs in
the corpus without explicitly storing the word pairs
themselves. Both updating (adding a new word pair
or increasing the frequency of existing word pair)
and querying (finding the frequency of a given word
pair) are constant time operations making it efficient
online storage data structure for large data. Sketches
are scalable and can easily be implemented in dis-
tributed setting.
We use CM sketch to store counts of word pairs
(except word pairs involving stop words) within a
window of size1 7 over different size corpora. We
store exact counts of words (except stop words) in
hash table (since the number of unique words is
not large that is quadratically less than the num-
ber of unique word pairs). The approximate PMI
and LLR scores are computed using these approxi-
mate counts and are applied to solve our three NLP
tasks. Our experiments demonstrate that on all of
the three tasks, we get performance comparable to
Exact word pair counts setting and state-of-the-art
system. Our method scales to 49 GB of unzipped
web data using bounded space of 2 billion counters
(8 GB memory). This work expands upon our ear-
lier workshop papers (Goyal et al., 2010a; Goyal et
al., 2010b).
</bodyText>
<sectionHeader confidence="0.987409" genericHeader="method">
2 Sketch Techniques
</sectionHeader>
<bodyText confidence="0.99992">
A sketch is a compact summary data structure to
store the frequencies of all items in the input stream.
Sketching techniques use hashing to map items in
streaming data onto a small sketch vector that can
be updated and queried in constant time. These tech-
</bodyText>
<page confidence="0.773398">
17 is chosen from intuition and not tuned.
</page>
<bodyText confidence="0.9994998">
niques generally process the input stream in one di-
rection, say from left to right, without re-processing
previous input. The main advantage of using these
techniques is that they require a storage which is
sub-linear in size of the input stream. The following
surveys comprehensively review the streaming liter-
ature: (Rusu and Dobra, 2007; Cormode and Had-
jieleftheriou, 2008).
There exists an extensive literature on sketch tech-
niques (Charikar et al., 2004; Li et al., 2008; Cor-
mode and Muthukrishnan, 2004; Rusu and Dobra,
2007) in algorithms community for solving many
large scale problems. However, in practice, re-
searchers have preferred Count-Min (CM) sketch
over other sketch techniques in many application ar-
eas, such as Security (Schechter et al., 2010), Ma-
chine Learning (Shi et al., 2009; Aggarwal and Yu,
2010), and Privacy (Dwork et al., 2010). This moti-
vated us to explore CM sketch to solve three impor-
tant NLP problems.2
</bodyText>
<subsectionHeader confidence="0.993965">
2.1 Count-Min Sketch
</subsectionHeader>
<bodyText confidence="0.999810294117647">
The Count-Min sketch (Cormode and Muthukrish-
nan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
on the data stream such as point, range and inner
product queries to be approximately answered very
quickly. It can also be applied to solve the finding
frequent items problem (Manku and Motwani, 2002)
in a data stream. In this paper, we are only interested
in point queries. The aim of a point query is to es-
timate the count of an item in the input stream. For
other details, the reader is referred to (Cormode and
Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters S and c, the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
</bodyText>
<listItem confidence="0.99966675">
• All reported frequencies are within the true fre-
quencies by at most cN with a probability of at
least 1-S.
• The space used by the algorithm is O(E log 1δ ).
</listItem>
<footnote confidence="0.975516">
2In future, in another line of research, we will explore com-
paring different sketch techniques for NLP problems.
</footnote>
<page confidence="0.991834">
251
</page>
<listItem confidence="0.996564">
• Constant time of O(log(1δ )) per each update and
query operation.
</listItem>
<subsectionHeader confidence="0.812521">
2.1.1 CM Data Structure
</subsectionHeader>
<bodyText confidence="0.999705666666667">
A Count-Min sketch (CM) with parameters (c,S)
is represented by a two-dimensional array with
width w and depth d :
</bodyText>
<equation confidence="0.998838">
sketch[1,1] • • • sketch[1, w]
..
.. ..
sketch[d,1] ••• sketch[d, w]
</equation>
<bodyText confidence="0.998546761904762">
Among the user chosen parameters, c controls the
amount of tolerable error in the returned count and
S controls the probability with which the returned
count is not within the accepted error. These val-
ues of c and S determine the width and depth of the
two-dimensional array respectively. To achieve the
guarantees mentioned in the previous section, we
set w=2 � and d=log(1δ). The depth d denotes the
number of pairwise-independent hash functions em-
ployed by the algorithm and there exists a one-to-
one correspondence between the rows and the set
of hash functions. Each of these hash functions
hk:{x1 ... xN} -+ {1... w},1 &lt; k &lt; d, takes a
word pair from the input stream and maps it into a
counter indexed by the corresponding hash function.
For example, h2(x) = 10 indicates that the word
pair “x” is mapped to the 10th position in the second
row of the sketch array.
Initialize the entire sketch array with zeros.
Update Procedure: When a new word pair “x”
with count c arrives, one counter in each row (as de-
cided by its corresponding hash function) is updated
by c.
sketch[k, hk(x)] �-- sketch[k, hk(x)] + c, V1 &lt; k &lt; d
Query Procedure: Since multiple word pairs can
get hashed to the same position, the frequency stored
by each position is guaranteed to overestimate the
true count. Thus, to answer the point query for a
given word pair, we return minimum over all the po-
sitions indexed by the k hash functions. The answer
to Query(x): cˆ = mink sketch[k, hk(x)]
Both update and query procedures involve evalu-
ating d hash functions and reading of all the values
in those indices and hence both these procedures are
linear in the number of hash functions. Hence both
these steps require O(log(1δ)) time. In our experi-
ments (see Section 3.1), we found that a small num-
ber of hash functions are sufficient and we use d=5.
Hence, the update and query operations take only a
constant time. The space used by the algorithm is
the size of the array i.e. wd counters, where w is the
width of each row.
</bodyText>
<subsectionHeader confidence="0.56773">
2.1.2 Properties
</subsectionHeader>
<bodyText confidence="0.9989958">
Apart from the advantages of being space ef-
ficient, and having constant update and constant
querying time, the Count-Min sketch has also other
advantages that makes it an attractive choice for
NLP applications.
</bodyText>
<listItem confidence="0.820319916666667">
• Linearity: Given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches in O(1� log 1δ )
time which is independent of the stream size.
• The linearity is especially attractive because it
allows the individual sketches to be computed
independent of each other, which means that it
is easy to implement it in distributed setting,
where each machine computes the sketch over
a sub set of corpus.
</listItem>
<subsectionHeader confidence="0.999429">
2.2 Conservative Update
</subsectionHeader>
<bodyText confidence="0.999951375">
Estan and Varghese introduced the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of computer networking. This can easily be used
with CM sketch to further improve the estimate of a
point query. To update a word pair “x” with fre-
quency c, we first compute the frequency cˆ of this
word pair from the existing data structure and the
counts are updated according to:
</bodyText>
<equation confidence="0.51724">
cˆ = mink sketch[k, hk(x)], V1 &lt; k &lt; d
sketch[k, hk(x)] �-- max{sketch[k, hk(x)], cˆ + c}
</equation>
<bodyText confidence="0.998763375">
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. Though this is a heuristic, it avoids
the unnecessary updates of counter values and thus
reduces the error.
In our experiments, we found that employing the
conservative update reduces the Average Relative
</bodyText>
<figure confidence="0.9311152">
⎡
⎢ ⎣
...
⎤
⎦ ⎥
</figure>
<page confidence="0.995201">
252
</page>
<bodyText confidence="0.99797875">
Error (ARE) of these counts approximately by a fac-
tor of 1.5. (see Section 3.1). But unfortunately,
this update can only be maintained over individual
sketches in distributed setting.
</bodyText>
<sectionHeader confidence="0.998727" genericHeader="method">
3 Intrinsic Evaluations
</sectionHeader>
<bodyText confidence="0.999988166666667">
To show the effectiveness of the CM sketch and CM
sketch with conservative update (CU) in the context
of NLP, we perform intrinsic evaluations. First, the
intrinsic evaluations are designed to measure the er-
ror in the approximate counts returned by CM sketch
compared to their true counts. Second, we compare
the word pairs association rankings obtained using
PMI and LLR with sketch and exact counts.
It is memory and time intensive to perform many
intrinsic evaluations on large data (Ravichandran et
al., 2005; Brants et al., 2007; Goyal et al., 2009).
Hence, we use a subset of corpus of 2 million sen-
tences (Subset) from Gigaword (Graff, 2003) for it.
We generate words and word pairs over a window
of size 7. We store exact counts of words (except
stop words) in a hash table and store approximate
counts of word pairs (except word pairs involving
stop words) in the sketch.
</bodyText>
<subsectionHeader confidence="0.999857">
3.1 Evaluating approximate sketch counts
</subsectionHeader>
<bodyText confidence="0.9999932">
To evaluate the amount of over-estimation error (see
Section 2.1) in CM and CU counts compared to the
true counts, we first group all word pairs with the
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequency word pairs are more
prone to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Moreover, to focus on
errors on low frequency counts, we have only plot-
ted word pairs with count at most 100. Average Rel-
ative error (ARE) is defined as the average of abso-
lute difference between the predicted and the exact
value divided by the exact value over all the word
pairs in each bucket.
</bodyText>
<equation confidence="0.618512">
|Exacti − Predictedi|
Exacti
</equation>
<bodyText confidence="0.99989722">
Where Exact and Predicted denotes values of ex-
act and CM/CU counts respectively; N denotes the
number of word pairs with same counts in a bucket.
In Fig. 1(a), we fixed the number of counters to 20
million (20M) with four bytes of memory per each
counter (thus it only requires 80 MB of main mem-
ory). Keeping the total number of counters fixed,
we try different values of depth (2, 3, 5 and 7) of the
sketch array and in each case the width is set to 20Md.
The ARE curves in each case are shown in Fig. 1(a).
We can make three main observations from Figure
1(a): First it shows that most of the errors occur on
low frequency word pairs. For frequent word pairs,
in almost all the different runs the ARE is close to
zero. Secondly, it shows that ARE is significantly
lower (by a factor of 1.5) for the runs which use
conservative update (CUx run) compared to the runs
that use direct CM sketch (CMx run). The encourag-
ing observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 50 million. The curves in 1(b) follow a
pattern which is similar to the previous setting. Low
frequency word pairs are more prone to error com-
pared to the frequent ones and employing conserva-
tive update reduces the ARE by a factor of 1.5. In
this setting, depth 5 does slightly better than depth 3
and gets lowest ARE.
We use CU counts and depth of 5 for the rest of
the paper. As 3 and 5 have lowest ARE in different
settings and using 5 hash functions, we get S = 0.01
(d = log(1δ) refer Section 2.1) that is probability of
failure is 1 in 100, making the algorithm more robust
to false positives compared with 3 hash functions,
S = 0.1 with probability of failure 1 in 10.
Fig. 1(c) studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE with fixed depth 5. As ex-
pected, using more number of counters decreases the
ARE in the counts. This is intuitive because, as the
length of each row in the sketch increases, the prob-
ability of collision decreases and hence the array is
more likely to contain true counts. By using 100
million counters, which is comparable to the length
of the stream 88 million, we are able to achieve al-
most zero ARE over all the counts including the rare
</bodyText>
<equation confidence="0.880957666666667">
1 N
ARE = N
i=1
</equation>
<page confidence="0.93677">
253
</page>
<figure confidence="0.99374766">
2
5
10M
20M
50M
100M
1
0
100 101 102
2.5
0.5
1.5
3
2
0
100 101 102
1
CM−7
CM−5
CM−3
CM−2
CU−7
CU−5
CU−3
CU−2
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
100 101 102
CM−7
CM−5
CM−3
CM−2
CU−7
CU−5
CU−3
CU−2
Average Relative Error
Average Relative Error
Average Relative Error
4
3
True frequency counts of word pairs (log scale) True frequency counts of word pairs (log scale) True frequency counts of word pairs (log scale)
(a) 20M counters (b) 50M counters (c) Different size models with depth 5
</figure>
<figureCaption confidence="0.978204">
Figure 1: Compare 20 and 50 million counter models with different (width,depth) settings. The notation CMx represents the
Count Min sketch with a depth of ’x’ and CUx represents the CM sketch along with conservative update and depth ’x’.
</figureCaption>
<bodyText confidence="0.999823444444444">
ones3. Note that the space we save by not storing the
exact counts is almost four times the memory that
we use here because on an average each word pair
is twelve characters long and requires twelve bytes
(thrice the size of an integer) and 4 bytes for storing
the integer count. Note, we get even bigger space
savings if we work with longer phrases (phrase clus-
tering), phrase pairs (paraphrasing/translation), and
varying length n-grams (Information Extraction).
</bodyText>
<subsectionHeader confidence="0.998049">
3.2 Evaluating word pairs association ranking
</subsectionHeader>
<bodyText confidence="0.999986526315789">
In this experiment, we compare the word pairs asso-
ciation rankings obtained using PMI and LLR with
CU and exact word pair counts. We use two kinds of
measures, namely recall and Spearman’s correlation
to measure the overlap in the rankings obtained by
exact and CU counts. Intuitively, recall captures the
number of word pairs that are found in both the sets
and then Spearman’s correlation captures if the rela-
tive order of these common word pairs is preserved
in both the rankings. In our experimental setup, if
the rankings match exactly, then we get a recall (R)
of 100% and a correlation (p) of 1.
The results with respect to different sized counter
(20 million (20M), 50 million (50M)) models are
shown in Table 1. If we compare the second and
third column of the table using PMI and LLR for
20M counters, we get exact rankings for LLR com-
pared to PMI while comparing TopK word pairs.
The explanation for such a behavior is: since we are
</bodyText>
<footnote confidence="0.981126">
3Even with other datasets we found that using counters lin-
ear in the size of the stream leads to ARE close to zero ∀ counts.
</footnote>
<table confidence="0.999940777777778">
# Cs 20M 50M
AS PMI LLR PMI LLR
TopK R p R p R p R p
50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0
500 .80 .98 1.0 1.0 .98 1.0 1.0 1.0
1000 .56 .99 1.0 1.0 .96 .99 1.0 1.0
5000 .35 .90 1.0 1.0 .85 .99 1.0 1.0
10000 .38 .55 1.0 1.0 .81 .95 1.0 1.0
</table>
<tableCaption confidence="0.946083">
Table 1: Evaluating the PMI and LLR rankings obtained using
CM sketch with conservative update (CU) and Exact counts
</tableCaption>
<bodyText confidence="0.999882866666666">
not throwing away any infrequent word pairs, PMI
will rank pairs with low frequency counts higher
(Church and Hanks, 1989). Hence, we are evaluat-
ing the PMI values for rare word pairs and we need
counters linear in size of stream to get almost perfect
ranking. This is also evident from the fourth column
for 50M of the Table 1, where CU PMI ranking gets
close to the optimal as the number of counters ap-
proaches stream size.
However, in some NLP problems, we are not in-
terested in low-frequency items. In such cases, even
using space less than linear in number of counters
would suffice. In our extrinsic evaluations, we show
that using space less than the length of the stream
does not degrade the performance.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="method">
4 Extrinsic Evaluations
</sectionHeader>
<subsectionHeader confidence="0.889559">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9834545">
Gigaword corpus (Graff, 2003) and a 50% portion
of a copy of web crawled by (Ravichandran et al.,
</bodyText>
<page confidence="0.991157">
254
</page>
<bodyText confidence="0.99954">
2005) are used to compute counts of words and word
pairs. For both the corpora, we split the text into
sentences, tokenize and convert into lower-case. We
generate words and word pairs over a window of size
7. We use four different sized corpora: SubSet (used
for intrinsic evaluations in Section 3), Gigaword
(GW), GigaWord + 20% of web data (GWB20), and
GigaWord + 50% of web data (GWB50). Corpus
Statistics are shown below. We store exact counts of
words in a hash table and store approximate counts
of word pairs in the sketch. Hence, the stream size
in our case is the total number of word pairs in a
corpus.
</bodyText>
<table confidence="0.998740714285714">
Corpus Subset GW GWB20 GWB50
Unzipped .32 9.8 22.8 49
Size (GB)
# of sentences 2.00 56.78 191.28 462.60
(Million)
Stream Size .088 2.67 6.05 13.20
(Billion)
</table>
<subsectionHeader confidence="0.980922">
4.2 Semantic Orientation
</subsectionHeader>
<bodyText confidence="0.999928285714286">
Given a word, the task of finding the Semantic Ori-
entation (SO) (Turney and Littman, 2003) of the
word is to identify if the word is more likely to be
used in positive or negative sense. We use a similar
framework as used by the authors to infer the SO.
We take the seven positive words (good, nice, excel-
lent, positive, fortunate, correct, and superior) and
the seven negative words (bad, nasty, poor, negative,
unfortunate, wrong, and inferior) used in (Turney
and Littman, 2003) work. The SO of a given word
is calculated based on the strength of its association
with the seven positive words, and the strength of
its association with the seven negative words. We
compute the SO of a word ”w” as follows:
</bodyText>
<equation confidence="0.950175">
SO-AS(W) = � AS(p, w)− � AS(n, w)
p∈Pwords n∈Nwords
</equation>
<bodyText confidence="0.999833142857143">
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively. We
use PMI and LLR to compute association scores
(AS). If this score is positive, we predict the word
as positive. Otherwise, we predict it as negative.
We use the General Inquirer lexicon4 (Stone et
al., 1966) as a benchmark to evaluate the semantic
</bodyText>
<footnote confidence="0.997017">
4The General Inquirer lexicon is freely available at http:
//www.wjh.harvard.edu/˜inquirer/
</footnote>
<bodyText confidence="0.999596571428572">
orientation scores similar to (Turney and Littman,
2003) work. Words with multiple senses have multi-
ple entries in the lexicon, we merge these entries for
our experiment. Our test set consists of 1597 posi-
tive and 1980 negative words. Accuracy is used as
an evaluation metric and is defined as the percentage
of number of correctly identified SO words.
</bodyText>
<figureCaption confidence="0.984648666666667">
Figure 2: Evaluating Semantic Orientation using PMI and LLR
with different number of counters of CU sketch built using Gi-
gaword.
</figureCaption>
<subsectionHeader confidence="0.913642">
4.2.1 Varying sketch size
</subsectionHeader>
<bodyText confidence="0.971167086956522">
We evaluate SO of words using PMI and LLR
on Gigaword (9.8GB). We compare approximate
SO computed using varying sizes of CU sketches:
50 million (50M), 100M, 200M, 500M, 1 billion
(1B) and 2 billion (2B) counters with Exact SO. To
compute these scores, we count the number of indi-
vidual words w1 and w2 and the pair (w1,w2) within
a window of size 7. Note that computing the exact
counts of all word pairs on these corpora is com-
putationally expensive and memory intensive, so we
consider only those pairs in which one word appears
in the prototype list and the other word appears in
the test set.
First, if we look at the Exact SO using PMI and
LLR in Figure 2(a) and 2(b) respectively, it shows
that using PMI, we get about 6 points higher ac-
curacy than LLR on this task (The 95% statistical
significance boundary for accuracy is about ± 1.5.).
Second, for both PMI and LLR, having more num-
ber of counters improve performance.5 Using 2B
counters, we get the same accuracy as Exact.
5We use maximum of 2B counters (8GB main memory), as
most of the current desktop machines have at most 8GB RAM.
</bodyText>
<figure confidence="0.998303235294118">
Model Size Model Size
(a) SO PMI (b) SO LLR
Accuracy
75
70
65
600 500M 113 1.513 213
CU
Exact
Accuracy
55
0 500M 113 1.513 213
70
65
60
CU
Exact
</figure>
<page confidence="0.983337">
255
</page>
<subsectionHeader confidence="0.63314">
4.2.2 Effect of Increasing Corpus Size
</subsectionHeader>
<bodyText confidence="0.999799153846154">
We evaluate SO of words on three different sized
corpora (see Section 4.1): GW (9.8GB), GWB20
(22.8GB), and GWB50 (49GB). First, since for this
task using PMI performs better than LLR, so we will
use PMI for this experiment. Second, we will fix
number of counters to 2B (CU-2B) as it performs
the best in Section 4.2.1. Third, we will compare the
CU-2B counter model with the Exact over increas-
ing corpus size.
We can make several observations from the Fig-
ure 3: • It shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 5.5 points in
accuracy, when we add 20% Web data to GigaWord
(GW). Adding 30% more Web data (GWB50), gives
a small increase of 1.3 points in accuracy which is
not even statistically significant. • Second, CU-2B
performs as good as exact for all corpus sizes. •
Third, the number of 2B counters (bounded space)
is less than the length of stream for GWB20 (6.05B
), and GWB50 (13.2B). Hence, it shows that using
counters less than the stream length does not degrade
the performance. • These results are also compara-
ble to Turney’s (2003) state-of-the-art work where
they report an accuracy of 82.84%. Note, they use a
billion word corpus which is larger than GWB50.
</bodyText>
<figure confidence="0.425296">
Corpus Size
</figure>
<figureCaption confidence="0.9843555">
Figure 3: Evaluating Semantic Orientation of words with Ex-
act and CU counts with increase in corpus size
</figureCaption>
<subsectionHeader confidence="0.997931">
4.3 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999939416666667">
Distributional similarity is based on the distribu-
tional hypothesis that similar terms appear in simi-
lar contexts (Firth, 1968; Harris, 1954). The context
vector for each term is represented by the strength
of association between the term and each of the lex-
ical, semantic, syntactic, and/or dependency units
that co-occur with it6. We use PMI and LLR to com-
pute association score (AS) between the term and
each of the context to generate the context vector.
Once, we have context vectors for each of the terms,
cosine similarity measure returns distributional sim-
ilarity between terms.
</bodyText>
<subsectionHeader confidence="0.842067">
4.3.1 Efficient Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999946368421053">
We propose an efficient approach for computing
distributional similarity between word pairs using
CU sketch. In the first step, we traverse the corpus
and store counts of all words (except stop words) in
hash table and all word pairs (except word pairs in-
volving stop words) in sketch. In the second step,
for a target word “x”, we consider all words (except
infrequent contexts which appear less than or equal
to 10.) as plausible context (since it is faster than
traversing the whole corpus.), and query the sketch
for vocabulary number of word pairs, and compute
approximate AS between word-context pairs. We
maintain only top K AS scores7 contexts using pri-
ority queue for every target word “x” and save them
onto the disk. In the third step, we use cosine simi-
larity using these approximate top K context vectors
to compute efficient distributional similarity.
The efficient distributional similarity using
sketches has following advantages:
</bodyText>
<listItem confidence="0.997433454545455">
• It can return semantic similarity between any
word pairs that are stored in the sketch.
• It can return the similarity between word pairs
in time O(K).
• We do not store word pairs explicitly, and use
fixed number of counters, hence the overall
space required is bounded.
• The additive property of sketch (Sec. 2.1.2) en-
ables us to parallelize most of the steps in the
algorithm. Thus it can be easily extended to
very large amounts of text data.
</listItem>
<bodyText confidence="0.912629833333333">
We use two test sets which consist of word pairs,
and their corresponding human rankings. We gen-
erate the word pair rankings using efficient distri-
butional similarity. We report the spearman’s rank
6Here, the context for a target word “x” is defined as words
appear within a window of size 7.
</bodyText>
<footnote confidence="0.993336">
7For this work, we use K = 1000 which is not tuned.
</footnote>
<figure confidence="0.947220555555556">
82
80
74
72 0 10GB 20GB 30GB 40GB 50GB
Accuracy
78
76
CU−2B
Exact
</figure>
<page confidence="0.951295">
256
</page>
<bodyText confidence="0.588232333333333">
correlation8 coefficient (p) between the human and
distributional similarity rankings. The two test sets
are:
</bodyText>
<listItem confidence="0.9957195">
1. WS-353 (Finkelstein et al., 2002) is a set of 353
word pairs.
2. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
</listItem>
<figureCaption confidence="0.966772">
Figure 4: Evaluating Distributional Similarity between word
pairs on WS-353 test set using PMI and LLR with different
number of counters of CU sketch built using Gigaword data-set.
</figureCaption>
<subsectionHeader confidence="0.991474">
4.3.2 Varying sketch size
</subsectionHeader>
<bodyText confidence="0.99995655">
We evaluate efficient distributional similarity be-
tween between word pairs on WS-353 test set us-
ing PMI and LLR association scores on Giga-
word (9.8GB). We compare different sizes of CU
sketch (similar to SO evaluation): 50 million (50M),
100M, 200M, 500M, 1 billion (1B) and 2 bil-
lion (2B) counters with the Exact word pair counts.
Here again, computing the exact counts of all word-
context pairs on these corpora is time, and memory
intensive, we generate context vectors for only those
words which are present in the test set.
First, if we look at word pair ranking using exact
PMI and LLR across Figures 4(a) and 4(b) respec-
tively, it shows that using LLR, we get better p of
.55 compared to p of .25 using PMI on this task (The
95% statistical significance boundary on p for WS-
353 is about f .08). The explanation for such a be-
havior is: PMI rank context pairs with low frequency
counts higher (Church and Hanks, 1989) compared
to frequent ones which are favored by LLR. Second,
</bodyText>
<footnote confidence="0.992789666666667">
8To calculate the Spearman correlations values are trans-
formed into ranks (if tied ranks exist, average of ranks is taken),
and we calculate the Pearson correlation on them.
</footnote>
<table confidence="0.9999136">
Test Set WS-353 RG-65
Model GW GWB20 GWB50 GW GWB20 GWB50
Agirre .64 .75
Exact .55 .55 .62 .65 .72 .74
CU-2B .53 .58 .62 .66 .72 .74
</table>
<tableCaption confidence="0.990997">
Table 2: Evaluating word pairs ranking with Exact and
CU counts. Scores are evaluated using p metric.
</tableCaption>
<bodyText confidence="0.997434">
for PMI in Fig. 4(a), having more counters does not
improve p. Third, for LLR in Fig. 4(b), having more
number of counters improve performance and using
2B counters, we get p close to the Exact.
</bodyText>
<subsectionHeader confidence="0.975739">
4.3.3 Effect of Increasing Corpus Size
</subsectionHeader>
<bodyText confidence="0.99993868">
We evaluate efficient distributional similarity be-
tween word pairs using three different sized cor-
pora: GW (9.8GB), GWB20 (22.8GB), and GWB50
(49GB) on two test sets: WS-353, and RG-65. First,
since for this task using LLR performs better than
PMI, so we will use LLR for this experiment. Sec-
ond, we will fix number of counters to 2B (CU-
2B) as it performs the best in Section 4.2.1. Third,
we will compare the CU-2B counter model with the
Exact over increasing corpus size. We also com-
pare our results against the state-of-the-art results
(Agirre) for distributional similarity (Agirre et al.,
2009). We report their results of context window of
size 7.
We can make several observations from the Ta-
ble 2: • It shows that increasing the amount of
data is not substantially improving the accuracy of
word pair rankings over both the test sets. • Here
again, CU-2B performs as good as exact for all cor-
pus sizes. • CU-2B and Exact performs same as the
state-of-the-art system. • The number of 2B coun-
ters (bounded space) is less than the length of stream
for GWB20 (6.05B ), and GWB50 (13.2B). Hence,
here again it shows that using counters less than the
stream length does not degrade the performance.
</bodyText>
<sectionHeader confidence="0.990265" genericHeader="method">
5 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999788714285714">
Recently, maximum spanning tree (MST) algo-
rithms for dependency parsing (McDonald et al.,
2005) have shown great promise, primarily in su-
pervised settings. In the MST framework, words in
a sentence form nodes in a graph, and connections
between nodes indicate how “related” they are. A
maximum spanning tree algorithm constructs a de-
</bodyText>
<figure confidence="0.999113947368421">
Accuracy
0.25
0.15
0.2
0.1
0 500M 1B 1.5B 2B
CU
Exact
Model Size
(a) Word Similarity PMI
0.55
0.5
0.45
0.40 500M 1B 1.5B 2B
Model Size
(b) Word Similarity LLR
Accuracy
CU
Exact
</figure>
<page confidence="0.990652">
257
</page>
<bodyText confidence="0.999922277777778">
pendency parse by linking together “most similar”
words. Typically the weights on edges in the graph
are parameterized as a linear function of features,
with weight learned by some supervised learning al-
gorithm. In this section, we ask the question: can
word association scores be used to derive syntactic
structures in an unsupervised manner?
A first pass answer is: clearly not. Metrics like
PMI would assign high association scores to rare
word pairs (mostly content words) leading to incor-
rect parses. Metrics like LLR would assign high
association scores to frequent words, also leading
to incorrect parses. However, with a small amount
of linguistic side information (Druck et al., 2009;
Naseem et al., 2010), we see that these issues can
be overcome. In particular, we see that large data
+ a little linguistics &gt; fancy unsupervised learning
algorithms.
</bodyText>
<subsectionHeader confidence="0.987066">
5.1 Graph Definition
</subsectionHeader>
<bodyText confidence="0.998908576923077">
Our approach is conceptually simple. We construct
a graph over nodes in the sentence with a unique
“root” node. The graph is directed and fully con-
nected, and for any two words in positions i and j,
the weight from word i to word j is defined as:
wij = αascasc(wi, wj)−αdistdist(i−j)+αlingling(ti, tj)
Here, asc(wi,wj) is a association score such as
PMI or LLR computed using approximate counts
from the sketch. Similarly, dist(i − j) is a simple
parameterized model of distances that favors short
dependencies. We use a simple unnormalized (log)
Laplacian prior of the form dist(i−j) = −|i−j−1|,
centered around 1 (encouraging short links to the
right). It is negated because we need to convert dis-
tances to similarities.
The final term, ling(ti, tj) asks: according to
some simple linguistic knowledge, how likely is if
that the (gold standard) part of speech tag associated
with word i points at that associated with word j?
For this, we use the same linguistic information
used by (Naseem et al., 2010), which does not
encode direction information. These rules are:
root → { aux, verb }; verb → { noun,
pronoun, adverb, verb }; aux → {
verb }; noun → { adj, art, noun,
num }; prep → { noun }; adj → { adv
</bodyText>
<table confidence="0.998931166666667">
len &lt; 10 len &lt; 20 all
COHEN-DIRICHLET 45.9 39.4 34.9
COHEN-BEST 59.4 45.9 40.5
ORACLE 75.1 66.6 63.0
BASELINE+LING 42.4 33.8 29.7
BASELINE 33.5 30.4 28.9
CU-2B LLR OPTIMAL 62.4 ± 7.7 51.1 ± 3.2 41.1 ± 1.9
CU-2B PMI OPTIMAL 63.3 ± 7.8 52.0 ± 3.2 41.1 ± 2.0
CU-2B LLR BALANCED 49.1 ± 7.6 43.6 ± 3.3 37.2 ± 1.9
CU-2B PMI BALANCED 49.5 ± 8.0 45.0 ± 3.2 38.3 ± 2.0
CU-2B LLR SEMISUP 55.7 ± 0.0 44.1 ± 0.0 39.4 ± 0.0
CU-2B PMI SEMISUP 56.5 ± 0.0 45.8 ± 0.0 39.9 ± 0.0
</table>
<tableCaption confidence="0.9981965">
Table 3: Comparing CU-2B build on GWB50 + a little lin-
guistics v/s fancy unsupervised learning algorithms.
</tableCaption>
<bodyText confidence="0.99658">
}. We simply give an additional weight of 1 to any
edge that agrees with one of these linguistic rules.
</bodyText>
<subsectionHeader confidence="0.990625">
5.2 Parameter Setting
</subsectionHeader>
<bodyText confidence="0.999984263157895">
The remaining issue is setting the interpolation pa-
rameters α associated with each of these scores.
This is a difficult problem in purely unsupervised
learning. We report results on three settings. First,
the OPTIMAL setting is based on grid search for op-
timal parameters. This is an oracle result based on
grid search over two of the three parameters (hold-
ing the third fixed at 1). In our second approach,
BALANCED, we normalize the three components to
“compete” equally. In particular, we scale and trans-
late all three components to have zero mean and unit
variance, and set the αs to all be equal to one. Fi-
nally, our third approach, SEMISUP, is based on us-
ing a small amount of labeled data to set the param-
eters. In particular, we use 10 labeled sentences to
select parameters based on the same grid search as
the OPTIMAL setting. Since this relies heavily on
which 10 sentences are used, we repeat this experi-
ment 20 times and report averages.
</bodyText>
<subsectionHeader confidence="0.878975">
5.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999752333333333">
Our experiments are on a dependency-converted ver-
sion of section 23 of the Penn Treebank using mod-
ified Collins’ head finding rules. We measure accu-
racies as directed, unlabeled dependency accuracy.
We separately report results of sentences of length
at most 10, at most 20 and finally of all length. Note
that there is no training or cross-validation: we sim-
ply run our MST parser on test data directly.
The results of the parsing experiments are shown
</bodyText>
<page confidence="0.98957">
258
</page>
<bodyText confidence="0.999953880952381">
in Table 3. We compare against the following al-
ternative systems. The first, Cohen-Dirichlet and
Cohen-Best, are previously reported state-of-the-art
results for unsupervised Bayesian dependency pars-
ing (Cohen and Smith, 2010). The first is results
using a simple Dirichlet prior; the second is the best
reported results for any system from that paper.
Next, we compare against an “oracle” system that
uses LLR extracted from the training data for the
Penn Treebank, where the LLR is based on the prob-
ability of observing an edge given two words. This
is not a true oracle in the sense that we might be
able to do better, but it is unlikely. The next two
baseline system are simple right branching base-
line trees. The Baseline system is a purely right-
branching tree. The Baseline+Ling system is one
that is right branching except that it can only create
edges that are compatible with the linguistic rules,
provided a relevant rule exists. For short sentences,
this is competitive with the Dirichlet prior results.
Finally we report variants of our approach using
association scores computed on the GWB50 using
CU sketch with 2 billion counters. We experiment
with two association scores: LLR and PMI. For each
measure, we report results based on the three ap-
proaches described earlier for setting the α hyper-
parameters. Error bars for our approaches are 95%
confidence intervals based on bootstrap resampling.
The results show that, for this task, PMI seems
slightly better than LLR, across the board. The OP-
TIMAL performance (based on tuning two hyperpa-
rameters) is amazingly strong: clearly beating out
all the baselines, and only about 15 points behind
the ORACLE system. Using the BALANCED ap-
proach causes a degradation of only 3 points from
the OPTIMAL on sentences of all lengths. In general,
the balancing approach seems to be slightly worse
than the semi-supervised approach, except on very
short sentences: for those, it is substantially better.
Overall, though, the results for both Balanced and
Semisup are competitive with state-of-the-art unsu-
pervised learning algorithms.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="discussions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.9999529375">
The advantage of using sketch in addition to being
memory and time efficient is that it contains counts
for all word pairs and hence can be used to com-
pute association scores like PMI and LLR between
any word pairs. We show that using sketch counts in
our experiments, on the three tasks, we get perfor-
mance comparable to Exact word pair counts setting
and state-of-the-art system. Our method scales to 49
GB of unzipped web data using bounded space of 2
billion counters (8 GB memory). Moreover, the lin-
earity property of the sketch makes it scalable and
usable in distributed setting. Association scores and
counts from sketch can be used for more NLP tasks
like small-space randomized language models, word
sense disambiguation, spelling correction, relation
learning, paraphrasing, and machine translation.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999970666666667">
The authors gratefully acknowledge the support of
NSF grant IIS-0712764 and Google Research Grant
for Large-Data NLP. Thanks to Suresh Venkatasub-
ramanian and Jagadeesh Jagarlamudi for useful dis-
cussions and the anonymous reviewers for many
helpful comments.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997818666666667">
Charu C. Aggarwal and Philip S. Yu. 2010. On classi-
fication of high-cardinality data streams. In SDM’10,
pages 802–813.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ’09: Pro-
ceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3–15, January.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76–83, Vancouver,
Canada, June.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017–3051.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
</reference>
<page confidence="0.97698">
259
</page>
<reference confidence="0.999826785046729">
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ’09, pages 360–
368, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.
Rothblum, and Sergey Yekhanin. 2010. Pan-private
streaming algorithms. In In Proceedings of ICS.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In ACM
Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daum´e III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and
Suresh Venkatasubramanian. 2010a. Sketch tech-
niques for scaling distributional similarity to the web.
In GEMS workshop at ACL, Uppsala, Sweden.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and
Suresh Venkatasubramanian. 2010b. Sketching tech-
niques for Large Scale NLP. In 6th WAC Workshop at
NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Z. Harris. 1954. Distributional structure. Word 10 (23),
pages 146–162.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
EMNLP, August.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 394–402. As-
sociation for Computational Linguistics.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953–960.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In VLDB.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
’05, pages 523–530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 1234–1244.
Association for Computational Linguistics.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627–633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ’07. ACM.
Stuart Schechter, Cormac Herley, and Michael Mitzen-
macher. 2010. Popularity is everything: a new
approach to protecting passwords from statistical-
guessing attacks. In Proceedings of the 5th USENIX
conference on Hot topics in security, HotSec’10, pages
1–8, Berkeley, CA, USA. USENIX Association.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. J. Mach. Learn. Res.,
10:2615–2637, December.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315–
346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI’09: Proceedings of the 21st international jont
conference on Artifical intelligence.
</reference>
<page confidence="0.951582">
260
</page>
<reference confidence="0.995719714285714">
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231–235, July.
</reference>
<page confidence="0.997824">
261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.887856">
<title confidence="0.9983">Approximate Scalable Bounded Space Sketch for Large Data NLP</title>
<author confidence="0.996557">Amit Goyal</author>
<author confidence="0.996557">Hal Daum´e</author>
<affiliation confidence="0.9998825">Dept. of Computer University of</affiliation>
<address confidence="0.988799">College Park, MD</address>
<abstract confidence="0.993927588235294">We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply Count- Min sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our scales to of unzipped web data bounded space of counters GB memory).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu C Aggarwal</author>
<author>Philip S Yu</author>
</authors>
<title>On classification of high-cardinality data streams.</title>
<date>2010</date>
<booktitle>In SDM’10,</booktitle>
<pages>802--813</pages>
<contexts>
<context position="6337" citStr="Aggarwal and Yu, 2010" startWordPosition="1025" endWordPosition="1028">ch is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data stream. In this paper, we are only interested in point quer</context>
</contexts>
<marker>Aggarwal, Yu, 2010</marker>
<rawString>Charu C. Aggarwal and Philip S. Yu. 2010. On classification of high-cardinality data streams. In SDM’10, pages 802–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of HLT-NAACL.</booktitle>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL ’09: Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="1092" citStr="Brants et al., 2007" startWordPosition="175" endWordPosition="178">aming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an ave</context>
<context position="12232" citStr="Brants et al., 2007" startWordPosition="2063" endWordPosition="2066">e maintained over individual sketches in distributed setting. 3 Intrinsic Evaluations To show the effectiveness of the CM sketch and CM sketch with conservative update (CU) in the context of NLP, we perform intrinsic evaluations. First, the intrinsic evaluations are designed to measure the error in the approximate counts returned by CM sketch compared to their true counts. Second, we compare the word pairs association rankings obtained using PMI and LLR with sketch and exact counts. It is memory and time intensive to perform many intrinsic evaluations on large data (Ravichandran et al., 2005; Brants et al., 2007; Goyal et al., 2009). Hence, we use a subset of corpus of 2 million sentences (Subset) from Gigaword (Graff, 2003) for it. We generate words and word pairs over a window of size 7. We store exact counts of words (except stop words) in a hash table and store approximate counts of word pairs (except word pairs involving stop words) in the sketch. 3.1 Evaluating approximate sketch counts To evaluate the amount of over-estimation error (see Section 2.1) in CM and CU counts compared to the true counts, we first group all word pairs with the same true frequency into a single bucket. We then compute</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Kevin Chen</author>
<author>Martin Farach-Colton</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2004</date>
<journal>Theor. Comput. Sci.,</journal>
<pages>312--3</pages>
<contexts>
<context position="5972" citStr="Charikar et al., 2004" startWordPosition="966" endWordPosition="969">g to map items in streaming data onto a small sketch vector that can be updated and queried in constant time. These tech17 is chosen from intuition and not tuned. niques generally process the input stream in one direction, say from left to right, without re-processing previous input. The main advantage of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to </context>
</contexts>
<marker>Charikar, Chen, Farach-Colton, 2004</marker>
<rawString>Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2004. Finding frequent items in data streams. Theor. Comput. Sci., 312:3–15, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>76--83</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="18749" citStr="Church and Hanks, 1989" startWordPosition="3268" endWordPosition="3271">her datasets we found that using counters linear in the size of the stream leads to ARE close to zero ∀ counts. # Cs 20M 50M AS PMI LLR PMI LLR TopK R p R p R p R p 50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0 500 .80 .98 1.0 1.0 .98 1.0 1.0 1.0 1000 .56 .99 1.0 1.0 .96 .99 1.0 1.0 5000 .35 .90 1.0 1.0 .85 .99 1.0 1.0 10000 .38 .55 1.0 1.0 .81 .95 1.0 1.0 Table 1: Evaluating the PMI and LLR rankings obtained using CM sketch with conservative update (CU) and Exact counts not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989). Hence, we are evaluating the PMI values for rare word pairs and we need counters linear in size of stream to get almost perfect ranking. This is also evident from the fourth column for 50M of the Table 1, where CU PMI ranking gets close to the optimal as the number of counters approaches stream size. However, in some NLP problems, we are not interested in low-frequency items. In such cases, even using space less than linear in number of counters would suffice. In our extrinsic evaluations, we show that using space less than the length of the stream does not degrade the performance. 4 Extrins</context>
<context position="28479" citStr="Church and Hanks, 1989" startWordPosition="4969" endWordPosition="4972"> with the Exact word pair counts. Here again, computing the exact counts of all wordcontext pairs on these corpora is time, and memory intensive, we generate context vectors for only those words which are present in the test set. First, if we look at word pair ranking using exact PMI and LLR across Figures 4(a) and 4(b) respectively, it shows that using LLR, we get better p of .55 compared to p of .25 using PMI on this task (The 95% statistical significance boundary on p for WS353 is about f .08). The explanation for such a behavior is: PMI rank context pairs with low frequency counts higher (Church and Hanks, 1989) compared to frequent ones which are favored by LLR. Second, 8To calculate the Spearman correlations values are transformed into ranks (if tied ranks exist, average of ranks is taken), and we calculate the Pearson correlation on them. Test Set WS-353 RG-65 Model GW GWB20 GWB50 GW GWB20 GWB50 Agirre .64 .75 Exact .55 .55 .62 .65 .72 .74 CU-2B .53 .58 .62 .66 .72 .74 Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using p metric. for PMI in Fig. 4(a), having more counters does not improve p. Third, for LLR in Fig. 4(b), having more number of counters improve</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>K. Church and P. Hanks. 1989. Word Association Norms, Mutual Information and Lexicography. In Proceedings of ACL, pages 76–83, Vancouver, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Covariance in unsupervised learning of probabilistic grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3017</pages>
<contexts>
<context position="3026" citStr="Cohen and Smith, 2010" startWordPosition="482" endWordPosition="485"> Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” given verb “x” using the highest approximate online Pointwise Mutual Information (PMI) values. In this paper, we explore sketch techniques, especially the Count-Min sketch (Cormode and Muthukrishnan, 2004) to build a single model to show its effectiveness on three important NLP tasks: • Predicting the Semantic Orientation of words (Turney and Littman, 2003) • Distributional Approaches for word similarity (Agirre et al., 2009) • Unsupervised Dependency Parsing (Cohen and Smith, 2010) with a little linguistics knowledge. In all these tasks, we need to compute association measures like Pointwise Mutual Information (PMI), 250 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250–261, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics and Log Likelihood ratio (LLR) between words. To compute association scores (AS), we need to count the number of times pair of words appear together within a certain window size. However, explicitly storing the counts of all word pairs is both computationally ex</context>
<context position="35355" citStr="Cohen and Smith, 2010" startWordPosition="6170" endWordPosition="6173">on 23 of the Penn Treebank using modified Collins’ head finding rules. We measure accuracies as directed, unlabeled dependency accuracy. We separately report results of sentences of length at most 10, at most 20 and finally of all length. Note that there is no training or cross-validation: we simply run our MST parser on test data directly. The results of the parsing experiments are shown 258 in Table 3. We compare against the following alternative systems. The first, Cohen-Dirichlet and Cohen-Best, are previously reported state-of-the-art results for unsupervised Bayesian dependency parsing (Cohen and Smith, 2010). The first is results using a simple Dirichlet prior; the second is the best reported results for any system from that paper. Next, we compare against an “oracle” system that uses LLR extracted from the training data for the Penn Treebank, where the LLR is based on the probability of observing an edge given two words. This is not a true oracle in the sense that we might be able to do better, but it is unlikely. The next two baseline system are simple right branching baseline trees. The Baseline system is a purely rightbranching tree. The Baseline+Ling system is one that is right branching exc</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>S. B. Cohen and N. A. Smith. 2010. Covariance in unsupervised learning of probabilistic grammars. Journal of Machine Learning Research, 11:3017–3051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2008</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="5890" citStr="Cormode and Hadjieleftheriou, 2008" startWordPosition="952" endWordPosition="956">cture to store the frequencies of all items in the input stream. Sketching techniques use hashing to map items in streaming data onto a small sketch vector that can be updated and queried in constant time. These tech17 is chosen from intuition and not tuned. niques generally process the input stream in one direction, say from left to right, without re-processing previous input. The main advantage of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min ske</context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2008</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2008. Finding frequent items in data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>S Muthukrishnan</author>
</authors>
<title>An improved data stream summary: The count-min sketch and its applications.</title>
<date>2004</date>
<journal>J. Algorithms.</journal>
<contexts>
<context position="2744" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="439" endWordPosition="442">yal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” given verb “x” using the highest approximate online Pointwise Mutual Information (PMI) values. In this paper, we explore sketch techniques, especially the Count-Min sketch (Cormode and Muthukrishnan, 2004) to build a single model to show its effectiveness on three important NLP tasks: • Predicting the Semantic Orientation of words (Turney and Littman, 2003) • Distributional Approaches for word similarity (Agirre et al., 2009) • Unsupervised Dependency Parsing (Cohen and Smith, 2010) with a little linguistics knowledge. In all these tasks, we need to compute association measures like Pointwise Mutual Information (PMI), 250 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250–261, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Comput</context>
<context position="6022" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="974" endWordPosition="978">small sketch vector that can be updated and queried in constant time. These tech17 is chosen from intuition and not tuned. niques generally process the input stream in one direction, say from left to right, without re-processing previous input. The main advantage of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input st</context>
</contexts>
<marker>Cormode, Muthukrishnan, 2004</marker>
<rawString>Graham Cormode and S. Muthukrishnan. 2004. An improved data stream summary: The count-min sketch and its applications. J. Algorithms.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Semi-supervised learning of dependency parsers using generalized expectation criteria.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>360--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31610" citStr="Druck et al., 2009" startWordPosition="5506" endWordPosition="5509">ghts on edges in the graph are parameterized as a linear function of features, with weight learned by some supervised learning algorithm. In this section, we ask the question: can word association scores be used to derive syntactic structures in an unsupervised manner? A first pass answer is: clearly not. Metrics like PMI would assign high association scores to rare word pairs (mostly content words) leading to incorrect parses. Metrics like LLR would assign high association scores to frequent words, also leading to incorrect parses. However, with a small amount of linguistic side information (Druck et al., 2009; Naseem et al., 2010), we see that these issues can be overcome. In particular, we see that large data + a little linguistics &gt; fancy unsupervised learning algorithms. 5.1 Graph Definition Our approach is conceptually simple. We construct a graph over nodes in the sentence with a unique “root” node. The graph is directed and fully connected, and for any two words in positions i and j, the weight from word i to word j is defined as: wij = αascasc(wi, wj)−αdistdist(i−j)+αlingling(ti, tj) Here, asc(wi,wj) is a association score such as PMI or LLR computed using approximate counts from the sketch</context>
</contexts>
<marker>Druck, Mann, McCallum, 2009</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2009. Semi-supervised learning of dependency parsers using generalized expectation criteria. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 360– 368, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Dwork</author>
<author>Moni Naor</author>
<author>Toniann Pitassi</author>
<author>Guy N Rothblum</author>
<author>Sergey Yekhanin</author>
</authors>
<title>Pan-private streaming algorithms. In</title>
<date>2010</date>
<booktitle>In Proceedings of ICS.</booktitle>
<contexts>
<context position="6371" citStr="Dwork et al., 2010" startWordPosition="1031" endWordPosition="1034"> stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data stream. In this paper, we are only interested in point queries. The aim of a point query is t</context>
</contexts>
<marker>Dwork, Naor, Pitassi, Rothblum, Yekhanin, 2010</marker>
<rawString>Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N. Rothblum, and Sergey Yekhanin. 2010. Pan-private streaming algorithms. In In Proceedings of ICS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Estan</author>
<author>George Varghese</author>
</authors>
<title>New directions in traffic measurement and accounting.</title>
<date>2002</date>
<journal>SIGCOMM Comput. Commun. Rev.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="10706" citStr="Estan and Varghese, 2002" startWordPosition="1797" endWordPosition="1800">o sketches s1 and s2 computed (using the same parameters w and d) over different input streams, the sketch of the combined data stream can be easily obtained by adding the individual sketches in O(1� log 1δ ) time which is independent of the stream size. • The linearity is especially attractive because it allows the individual sketches to be computed independent of each other, which means that it is easy to implement it in distributed setting, where each machine computes the sketch over a sub set of corpus. 2.2 Conservative Update Estan and Varghese introduced the idea of conservative update (Estan and Varghese, 2002) in the context of computer networking. This can easily be used with CM sketch to further improve the estimate of a point query. To update a word pair “x” with frequency c, we first compute the frequency cˆ of this word pair from the existing data structure and the counts are updated according to: cˆ = mink sketch[k, hk(x)], V1 &lt; k &lt; d sketch[k, hk(x)] �-- max{sketch[k, hk(x)], cˆ + c} The intuition is that, since the point query returns the minimum of all the d values, we will update a counter only if it is necessary as indicated by the above equation. Though this is a heuristic, it avoids th</context>
</contexts>
<marker>Estan, Varghese, 2002</marker>
<rawString>Cristian Estan and George Varghese. 2002. New directions in traffic measurement and accounting. SIGCOMM Comput. Commun. Rev., 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited. In</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems.</journal>
<contexts>
<context position="27253" citStr="Finkelstein et al., 2002" startWordPosition="4751" endWordPosition="4754">s it can be easily extended to very large amounts of text data. We use two test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using efficient distributional similarity. We report the spearman’s rank 6Here, the context for a target word “x” is defined as words appear within a window of size 7. 7For this work, we use K = 1000 which is not tuned. 82 80 74 72 0 10GB 20GB 30GB 40GB 50GB Accuracy 78 76 CU−2B Exact 256 correlation8 coefficient (p) between the human and distributional similarity rankings. The two test sets are: 1. WS-353 (Finkelstein et al., 2002) is a set of 353 word pairs. 2. RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs. Figure 4: Evaluating Distributional Similarity between word pairs on WS-353 test set using PMI and LLR with different number of counters of CU sketch built using Gigaword data-set. 4.3.2 Varying sketch size We evaluate efficient distributional similarity between between word pairs on WS-353 test set using PMI and LLR association scores on Gigaword (9.8GB). We compare different sizes of CU sketch (similar to SO evaluation): 50 million (50M), 100M, 200M, 500M, 1 billion (1B) and 2 billion (2B) count</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. In ACM Transactions on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1968</date>
<editor>In F. Palmer, editor, Selected Papers of J. R. Firth.</editor>
<publisher>Longman.</publisher>
<contexts>
<context position="24792" citStr="Firth, 1968" startWordPosition="4337" endWordPosition="4338"> length of stream for GWB20 (6.05B ), and GWB50 (13.2B). Hence, it shows that using counters less than the stream length does not degrade the performance. • These results are also comparable to Turney’s (2003) state-of-the-art work where they report an accuracy of 82.84%. Note, they use a billion word corpus which is larger than GWB50. Corpus Size Figure 3: Evaluating Semantic Orientation of words with Exact and CU counts with increase in corpus size 4.3 Distributional Similarity Distributional similarity is based on the distributional hypothesis that similar terms appear in similar contexts (Firth, 1968; Harris, 1954). The context vector for each term is represented by the strength of association between the term and each of the lexical, semantic, syntactic, and/or dependency units that co-occur with it6. We use PMI and LLR to compute association score (AS) between the term and each of the context to generate the context vector. Once, we have context vectors for each of the terms, cosine similarity measure returns distributional similarity between terms. 4.3.1 Efficient Distributional Similarity We propose an efficient approach for computing distributional similarity between word pairs using</context>
</contexts>
<marker>Firth, 1968</marker>
<rawString>J. Firth. 1968. A synopsis of linguistic theory 1930-1955. In F. Palmer, editor, Selected Papers of J. R. Firth. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Sketch techniques for scaling distributional similarity to the web.</title>
<date>2010</date>
<booktitle>In GEMS workshop at ACL,</booktitle>
<location>Uppsala, Sweden.</location>
<marker>Goyal, Jagarlamudi, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and Suresh Venkatasubramanian. 2010a. Sketch techniques for scaling distributional similarity to the web. In GEMS workshop at ACL, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Sketching techniques for Large Scale NLP.</title>
<date>2010</date>
<booktitle>In 6th WAC Workshop at NAACL-HLT.</booktitle>
<marker>Goyal, Jagarlamudi, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and Suresh Venkatasubramanian. 2010b. Sketching techniques for Large Scale NLP. In 6th WAC Workshop at NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA,</location>
<contexts>
<context position="12347" citStr="Graff, 2003" startWordPosition="2086" endWordPosition="2087">sketch and CM sketch with conservative update (CU) in the context of NLP, we perform intrinsic evaluations. First, the intrinsic evaluations are designed to measure the error in the approximate counts returned by CM sketch compared to their true counts. Second, we compare the word pairs association rankings obtained using PMI and LLR with sketch and exact counts. It is memory and time intensive to perform many intrinsic evaluations on large data (Ravichandran et al., 2005; Brants et al., 2007; Goyal et al., 2009). Hence, we use a subset of corpus of 2 million sentences (Subset) from Gigaword (Graff, 2003) for it. We generate words and word pairs over a window of size 7. We store exact counts of words (except stop words) in a hash table and store approximate counts of word pairs (except word pairs involving stop words) in the sketch. 3.1 Evaluating approximate sketch counts To evaluate the amount of over-estimation error (see Section 2.1) in CM and CU counts compared to the true counts, we first group all word pairs with the same true frequency into a single bucket. We then compute the average relative error in each of these buckets. Since low-frequency word pairs are more prone to errors, maki</context>
<context position="19402" citStr="Graff, 2003" startWordPosition="3386" endWordPosition="3387"> for rare word pairs and we need counters linear in size of stream to get almost perfect ranking. This is also evident from the fourth column for 50M of the Table 1, where CU PMI ranking gets close to the optimal as the number of counters approaches stream size. However, in some NLP problems, we are not interested in low-frequency items. In such cases, even using space less than linear in number of counters would suffice. In our extrinsic evaluations, we show that using space less than the length of the stream does not degrade the performance. 4 Extrinsic Evaluations 4.1 Data Gigaword corpus (Graff, 2003) and a 50% portion of a copy of web crawled by (Ravichandran et al., 254 2005) are used to compute counts of words and word pairs. For both the corpora, we split the text into sentences, tokenize and convert into lower-case. We generate words and word pairs over a window of size 7. We use four different sized corpora: SubSet (used for intrinsic evaluations in Section 3), Gigaword (GW), GigaWord + 20% of web data (GWB20), and GigaWord + 50% of web data (GWB50). Corpus Statistics are shown below. We store exact counts of words in a hash table and store approximate counts of word pairs in the ske</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="24807" citStr="Harris, 1954" startWordPosition="4339" endWordPosition="4340">ream for GWB20 (6.05B ), and GWB50 (13.2B). Hence, it shows that using counters less than the stream length does not degrade the performance. • These results are also comparable to Turney’s (2003) state-of-the-art work where they report an accuracy of 82.84%. Note, they use a billion word corpus which is larger than GWB50. Corpus Size Figure 3: Evaluating Semantic Orientation of words with Exact and CU counts with increase in corpus size 4.3 Distributional Similarity Distributional similarity is based on the distributional hypothesis that similar terms appear in similar contexts (Firth, 1968; Harris, 1954). The context vector for each term is represented by the strength of association between the term and each of the lexical, semantic, syntactic, and/or dependency units that co-occur with it6. We use PMI and LLR to compute association score (AS) between the term and each of the context to generate the context vector. Once, we have context vectors for each of the terms, cosine similarity measure returns distributional similarity between terms. 4.3.1 Efficient Distributional Similarity We propose an efficient approach for computing distributional similarity between word pairs using CU sketch. In </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word 10 (23), pages 146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for SMT.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<contexts>
<context position="2162" citStr="Levenberg and Osborne (2009)" startWordPosition="350" endWordPosition="353">ute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (Goyal et al., 2009; Levenberg et al., 2010; Van Durme and Lall, 2010). Streaming approaches (Muthukrishnan, 2005) provide memory and time-efficient framework to deal with terabytes of data. However, these approaches are proposed to solve a singe problem. For example, our earlier work (Goyal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” given verb “x” using the highest approximate online Pointwise Mutual Information (PMI) values. In this paper, we explore sketch techniques, especially the Count-Min sketch (Cormode and Muthukrishnan, 2004) to build a single</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for SMT. In EMNLP, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1866" citStr="Levenberg et al., 2010" startWordPosition="305" endWordPosition="308">ing. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (Goyal et al., 2009; Levenberg et al., 2010; Van Durme and Lall, 2010). Streaming approaches (Muthukrishnan, 2005) provide memory and time-efficient framework to deal with terabytes of data. However, these approaches are proposed to solve a singe problem. For example, our earlier work (Goyal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) </context>
</contexts>
<marker>Levenberg, Callison-Burch, Osborne, 2010</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 394–402. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth Ward Church</author>
<author>Trevor Hastie</author>
</authors>
<title>One sketch for all: Theory and application of conditional random sampling.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>953--960</pages>
<contexts>
<context position="5989" citStr="Li et al., 2008" startWordPosition="970" endWordPosition="973">ming data onto a small sketch vector that can be updated and queried in constant time. These tech17 is chosen from intuition and not tuned. niques generally process the input stream in one direction, say from left to right, without re-processing previous input. The main advantage of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequen</context>
</contexts>
<marker>Li, Church, Hastie, 2008</marker>
<rawString>Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008. One sketch for all: Theory and application of conditional random sampling. In Neural Information Processing Systems, pages 953–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Manku</author>
<author>R Motwani</author>
</authors>
<title>Approximate frequency counts over data streams.</title>
<date>2002</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="6867" citStr="Manku and Motwani, 2002" startWordPosition="1115" endWordPosition="1118"> Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data stream. In this paper, we are only interested in point queries. The aim of a point query is to estimate the count of an item in the input stream. For other details, the reader is referred to (Cormode and Muthukrishnan, 2004). Given an input stream of word pairs of length N and user chosen parameters S and c, the algorithm stores the frequencies of all the word pairs with the following guarantees: • All reported frequencies are within the true frequencies by at most cN with a probability of at least 1-S. • The space used by the algorithm is O(E log 1δ ). 2In future, in another line o</context>
</contexts>
<marker>Manku, Motwani, 2002</marker>
<rawString>G. S. Manku and R. Motwani. 2002. Approximate frequency counts over data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 523–530, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthukrishnan</author>
</authors>
<title>Data streams: Algorithms and applications. Foundations and Trends in</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="1937" citStr="Muthukrishnan, 2005" startWordPosition="316" endWordPosition="317">ple, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (Goyal et al., 2009; Levenberg et al., 2010; Van Durme and Lall, 2010). Streaming approaches (Muthukrishnan, 2005) provide memory and time-efficient framework to deal with terabytes of data. However, these approaches are proposed to solve a singe problem. For example, our earlier work (Goyal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y</context>
</contexts>
<marker>Muthukrishnan, 2005</marker>
<rawString>S. Muthukrishnan. 2005. Data streams: Algorithms and applications. Foundations and Trends in Theoretical Computer Science, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1234--1244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31632" citStr="Naseem et al., 2010" startWordPosition="5510" endWordPosition="5513"> graph are parameterized as a linear function of features, with weight learned by some supervised learning algorithm. In this section, we ask the question: can word association scores be used to derive syntactic structures in an unsupervised manner? A first pass answer is: clearly not. Metrics like PMI would assign high association scores to rare word pairs (mostly content words) leading to incorrect parses. Metrics like LLR would assign high association scores to frequent words, also leading to incorrect parses. However, with a small amount of linguistic side information (Druck et al., 2009; Naseem et al., 2010), we see that these issues can be overcome. In particular, we see that large data + a little linguistics &gt; fancy unsupervised learning algorithms. 5.1 Graph Definition Our approach is conceptually simple. We construct a graph over nodes in the sentence with a unique “root” node. The graph is directed and fully connected, and for any two words in positions i and j, the weight from word i to word j is defined as: wij = αascasc(wi, wj)−αdistdist(i−j)+αlingling(ti, tj) Here, asc(wi,wj) is a association score such as PMI or LLR computed using approximate counts from the sketch. Similarly, dist(i − </context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1234–1244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1134" citStr="Ravichandran et al., 2005" startWordPosition="181" endWordPosition="184">h to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community </context>
<context position="12211" citStr="Ravichandran et al., 2005" startWordPosition="2059" endWordPosition="2062">ely, this update can only be maintained over individual sketches in distributed setting. 3 Intrinsic Evaluations To show the effectiveness of the CM sketch and CM sketch with conservative update (CU) in the context of NLP, we perform intrinsic evaluations. First, the intrinsic evaluations are designed to measure the error in the approximate counts returned by CM sketch compared to their true counts. Second, we compare the word pairs association rankings obtained using PMI and LLR with sketch and exact counts. It is memory and time intensive to perform many intrinsic evaluations on large data (Ravichandran et al., 2005; Brants et al., 2007; Goyal et al., 2009). Hence, we use a subset of corpus of 2 million sentences (Subset) from Gigaword (Graff, 2003) for it. We generate words and word pairs over a window of size 7. We store exact counts of words (except stop words) in a hash table and store approximate counts of word pairs (except word pairs involving stop words) in the sketch. 3.1 Evaluating approximate sketch counts To evaluate the amount of over-estimation error (see Section 2.1) in CM and CU counts compared to the true counts, we first group all word pairs with the same true frequency into a single bu</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Computational Linguistics,</journal>
<pages>8--627</pages>
<contexts>
<context position="27325" citStr="Rubenstein and Goodenough, 1965" startWordPosition="4764" endWordPosition="4767">e use two test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using efficient distributional similarity. We report the spearman’s rank 6Here, the context for a target word “x” is defined as words appear within a window of size 7. 7For this work, we use K = 1000 which is not tuned. 82 80 74 72 0 10GB 20GB 30GB 40GB 50GB Accuracy 78 76 CU−2B Exact 256 correlation8 coefficient (p) between the human and distributional similarity rankings. The two test sets are: 1. WS-353 (Finkelstein et al., 2002) is a set of 353 word pairs. 2. RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs. Figure 4: Evaluating Distributional Similarity between word pairs on WS-353 test set using PMI and LLR with different number of counters of CU sketch built using Gigaword data-set. 4.3.2 Varying sketch size We evaluate efficient distributional similarity between between word pairs on WS-353 test set using PMI and LLR association scores on Gigaword (9.8GB). We compare different sizes of CU sketch (similar to SO evaluation): 50 million (50M), 100M, 200M, 500M, 1 billion (1B) and 2 billion (2B) counters with the Exact word pair counts. Here again, computing the exact cou</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Computational Linguistics, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florin Rusu</author>
<author>Alin Dobra</author>
</authors>
<title>Statistical analysis of sketch estimators.</title>
<date>2007</date>
<booktitle>In SIGMOD ’07.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="5853" citStr="Rusu and Dobra, 2007" startWordPosition="948" endWordPosition="951">pact summary data structure to store the frequencies of all items in the input stream. Sketching techniques use hashing to map items in streaming data onto a small sketch vector that can be updated and queried in constant time. These tech17 is chosen from intuition and not tuned. niques generally process the input stream in one direction, say from left to right, without re-processing previous input. The main advantage of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2</context>
</contexts>
<marker>Rusu, Dobra, 2007</marker>
<rawString>Florin Rusu and Alin Dobra. 2007. Statistical analysis of sketch estimators. In SIGMOD ’07. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Schechter</author>
<author>Cormac Herley</author>
<author>Michael Mitzenmacher</author>
</authors>
<title>Popularity is everything: a new approach to protecting passwords from statisticalguessing attacks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th USENIX conference on Hot topics in security, HotSec’10,</booktitle>
<pages>1--8</pages>
<publisher>USENIX Association.</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="6277" citStr="Schechter et al., 2010" startWordPosition="1014" endWordPosition="1017"> of using these techniques is that they require a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data</context>
</contexts>
<marker>Schechter, Herley, Mitzenmacher, 2010</marker>
<rawString>Stuart Schechter, Cormac Herley, and Michael Mitzenmacher. 2010. Popularity is everything: a new approach to protecting passwords from statisticalguessing attacks. In Proceedings of the 5th USENIX conference on Hot topics in security, HotSec’10, pages 1–8, Berkeley, CA, USA. USENIX Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qinfeng Shi</author>
<author>James Petterson</author>
<author>Gideon Dror</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>S V N Vishwanathan</author>
</authors>
<title>Hash kernels for structured data.</title>
<date>2009</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>10--2615</pages>
<contexts>
<context position="6313" citStr="Shi et al., 2009" startWordPosition="1021" endWordPosition="1024">uire a storage which is sub-linear in size of the input stream. The following surveys comprehensively review the streaming literature: (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008). There exists an extensive literature on sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007) in algorithms community for solving many large scale problems. However, in practice, researchers have preferred Count-Min (CM) sketch over other sketch techniques in many application areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), and Privacy (Dwork et al., 2010). This motivated us to explore CM sketch to solve three important NLP problems.2 2.1 Count-Min Sketch The Count-Min sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data stream. In this paper, we are only </context>
</contexts>
<marker>Shi, Petterson, Dror, Langford, Smola, Vishwanathan, 2009</marker>
<rawString>Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. 2009. Hash kernels for structured data. J. Mach. Learn. Res., 10:2615–2637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21335" citStr="Stone et al., 1966" startWordPosition="3727" endWordPosition="3730">) used in (Turney and Littman, 2003) work. The SO of a given word is calculated based on the strength of its association with the seven positive words, and the strength of its association with the seven negative words. We compute the SO of a word ”w” as follows: SO-AS(W) = � AS(p, w)− � AS(n, w) p∈Pwords n∈Nwords Where, Pwords and Nwords denote the seven positive and negative prototype words respectively. We use PMI and LLR to compute association scores (AS). If this score is positive, we predict the word as positive. Otherwise, we predict it as negative. We use the General Inquirer lexicon4 (Stone et al., 1966) as a benchmark to evaluate the semantic 4The General Inquirer lexicon is freely available at http: //www.wjh.harvard.edu/˜inquirer/ orientation scores similar to (Turney and Littman, 2003) work. Words with multiple senses have multiple entries in the lexicon, we merge these entries for our experiment. Our test set consists of 1597 positive and 1980 negative words. Accuracy is used as an evaluation metric and is defined as the percentage of number of correctly identified SO words. Figure 2: Evaluating Semantic Orientation using PMI and LLR with different number of counters of CU sketch built u</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>21</volume>
<pages>346</pages>
<contexts>
<context position="2898" citStr="Turney and Littman, 2003" startWordPosition="464" endWordPosition="467">mbased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” given verb “x” using the highest approximate online Pointwise Mutual Information (PMI) values. In this paper, we explore sketch techniques, especially the Count-Min sketch (Cormode and Muthukrishnan, 2004) to build a single model to show its effectiveness on three important NLP tasks: • Predicting the Semantic Orientation of words (Turney and Littman, 2003) • Distributional Approaches for word similarity (Agirre et al., 2009) • Unsupervised Dependency Parsing (Cohen and Smith, 2010) with a little linguistics knowledge. In all these tasks, we need to compute association measures like Pointwise Mutual Information (PMI), 250 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250–261, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics and Log Likelihood ratio (LLR) between words. To compute association scores (AS), we need to count the number of times pair of words a</context>
<context position="20361" citStr="Turney and Littman, 2003" startWordPosition="3556" endWordPosition="3559">or intrinsic evaluations in Section 3), Gigaword (GW), GigaWord + 20% of web data (GWB20), and GigaWord + 50% of web data (GWB50). Corpus Statistics are shown below. We store exact counts of words in a hash table and store approximate counts of word pairs in the sketch. Hence, the stream size in our case is the total number of word pairs in a corpus. Corpus Subset GW GWB20 GWB50 Unzipped .32 9.8 22.8 49 Size (GB) # of sentences 2.00 56.78 191.28 462.60 (Million) Stream Size .088 2.67 6.05 13.20 (Billion) 4.2 Semantic Orientation Given a word, the task of finding the Semantic Orientation (SO) (Turney and Littman, 2003) of the word is to identify if the word is more likely to be used in positive or negative sense. We use a similar framework as used by the authors to infer the SO. We take the seven positive words (good, nice, excellent, positive, fortunate, correct, and superior) and the seven negative words (bad, nasty, poor, negative, unfortunate, wrong, and inferior) used in (Turney and Littman, 2003) work. The SO of a given word is calculated based on the strength of its association with the seven positive words, and the strength of its association with the seven negative words. We compute the SO of a wor</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21:315– 346, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1106" citStr="Turney, 2008" startWordPosition="179" endWordPosition="180">CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic counting with randomized storage.</title>
<date>2009</date>
<booktitle>In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic counting with randomized storage. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming pointwise mutual information.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming pointwise mutual information. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online generation of locality sensitive hash signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>231--235</pages>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online generation of locality sensitive hash signatures. In Proceedings of the ACL 2010 Conference Short Papers, pages 231–235, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>