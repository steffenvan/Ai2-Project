<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000113">
<title confidence="0.999079">
Different Structures for Evaluating Answers to Complex Questions:
Pyramids Won’t Topple, and Neither Will Human Assessors
</title>
<author confidence="0.930948">
Hoa Trang Dang
</author>
<affiliation confidence="0.9431605">
Information Access Division
National Institute of Standards and Technology
</affiliation>
<address confidence="0.714631">
Gaithersburg, MD 20899
</address>
<email confidence="0.996156">
hoa.dang@nist.gov
</email>
<author confidence="0.993273">
Jimmy Lin
</author>
<affiliation confidence="0.9975065">
College of Information Studies
University of Maryland
</affiliation>
<address confidence="0.972181">
College Park, MD 20742
</address>
<email confidence="0.999639">
jimmylin@umd.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874">
The idea of “nugget pyramids” has re-
cently been introduced as a refinement to the
nugget-based methodology used to evaluate
answers to complex questions in the TREC
QA tracks. This paper examines data from
the 2006 evaluation, the first large-scale de-
ployment of the nugget pyramids scheme.
We show that this method of combining
judgments of nugget importance from multi-
ple assessors increases the stability and dis-
criminative power of the evaluation while in-
troducing only a small additional burden in
terms of manual assessment. We also con-
sider an alternative method for combining
assessor opinions, which yields a distinction
similar to micro- and macro-averaging in the
context of classification tasks. While the
two approaches differ in terms of underly-
ing assumptions, their results are neverthe-
less highly correlated.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976361111111">
The emergence of question answering (QA) systems
for addressing complex information needs has ne-
cessitated the development and refinement of new
methodologies for evaluating and comparing sys-
tems. In the Text REtrieval Conference (TREC) QA
tracks organized by the U.S. National Institute of
Standards and Technology (NIST), improvements in
evaluation processes have kept pace with the evolu-
tion of QA tasks. For the past several years, NIST
has implemented an evaluation methodology based
on the notion of “information nuggets” to assess an-
swers to complex questions. As it has become the
de facto standard for evaluating such systems, the
research community stands to benefit from a better
understanding of the characteristics of this evalua-
tion methodology.
This paper explores recent refinements to the
nugget-based evaluation methodology developed by
NIST. In particular, we examine the recent so-called
“pyramid extension” that incorporates relevance
judgments from multiple assessors to improve eval-
uation stability (Lin and Demner-Fushman, 2006).
We organize our discussion as follows: The next
section begins by providing a brief overview of
nugget-based evaluations and the pyramid exten-
sion. Section 3 presents results from the first large-
scale implementation of nugget pyramids for QA
evaluation in TREC 2006. Analysis shows that this
extension improves both stability and discriminative
power. In Section 4, we discuss an alternative for
combining multiple judgments that parallels the dis-
tinction between micro- and macro-averaging often
seen in classification tasks. Experiments reveal that
the methods yield almost exactly the same results,
despite operating on different granularities (individ-
ual nuggets vs. individual users).
</bodyText>
<sectionHeader confidence="0.991731" genericHeader="method">
2 Evaluating Complex Questions
</sectionHeader>
<bodyText confidence="0.999059333333333">
Complex questions are distinguished from factoid
questions such as “Who shot Abraham Lincoln?” in
that they cannot be answered by named entities (e.g.,
persons, organizations, dates, etc.). Typically, these
information needs are embedded in the context of a
scenario (i.e., user task) and often require systems to
</bodyText>
<page confidence="0.950968">
768
</page>
<note confidence="0.9257795">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 768–775,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99992919047619">
synthesize information from multiple documents or
to generate answers that cannot be easily extracted
(e.g., by leveraging inference capabilities).
To date, NIST has already conducted several
large-scale evaluations of complex questions: def-
inition questions in TREC 2003, “Other” ques-
tions in TREC 2004–2006, “relationship” questions
in TREC 2005, and the complex, interactive QA
(ciQA) task in TREC 2006. Definition and Other
questions are similar in that they both request novel
facts about “targets”, which can be persons, orga-
nizations, things, and events. Relationship ques-
tions evolved into the ciQA task and focus on in-
formation needs such as “What financial relation-
ships exist between South American drug cartels and
banks in Liechtenstein?” Such complex questions
focus on ties (financial, military, familial, etc.) that
connect two or more entities. All of these evalua-
tions have employed the nugget-based methodology,
which demonstrates its versatility and applicability
to a wide range of information needs.
</bodyText>
<subsectionHeader confidence="0.997843">
2.1 Basic Setup
</subsectionHeader>
<bodyText confidence="0.99997896">
In the TREC QA evaluations, an answer to a
complex question consists of an unordered set of
[document-id, answer string] pairs, where the strings
are presumed to provide some relevant information
that addresses the question. Although no explicit
limit is placed on the length of the answer, the final
metric penalizes verbosity (see below).
Evaluation of system output proceeds in two
steps. First, answer strings from all submissions
are gathered together and presented to a single as-
sessor. The source of each answer string is blinded
so that the assessor can not obviously tell which
systems generated what output. Using these an-
swers and searches performed during question de-
velopment, the assessor creates a list of relevant
nuggets. A nugget is a piece of information (i.e.,
“fact”) that addresses one aspect of the user’s ques-
tion. Nuggets should be atomic, in the sense that
an assessor should be able to make a binary de-
cision as to whether the nugget appears in an an-
swer string. Although a nugget represents a con-
ceptual entity, the assessor provides a natural lan-
guage description—primarily as a memory aid for
the subsequent evaluation steps. These descriptions
range from sentence-length document extracts to
</bodyText>
<equation confidence="0.999117916666667">
r = # of vital nuggets returned
a = # of okay nuggets returned
R = # of vital nuggets in the answer key
l = # of non-whitespace characters in entire run
recall: R = r/R
allowance: α = 100 × (r + a)
�
1 if l &lt; α
precision: P = 1 − l��
l otherwise
F(Q) = (Q2 +2 1) × P × R
Q2 × P + R
</equation>
<figureCaption confidence="0.986828">
Figure 1: Official definition of F-score for nugget
evaluation in TREC.
</figureCaption>
<bodyText confidence="0.999621366666667">
key phrases to telegraphic short-hand notes—their
readability greatly varies from assessor to assessor.
The assessor also manually classifies each nugget
as either vital or okay (non-vital). Vital nuggets rep-
resent concepts that must be present in a “good” an-
swer. Okay nuggets may contain interesting infor-
mation, but are not essential.
In the second step, the same assessor who cre-
ated the nuggets reads each system’s output in turn
and marks the appearance of the nuggets. An an-
swer string contains a nugget if there is a conceptual
match; that is, the match is independent of the partic-
ular wording used in the system’s output. A nugget
match is marked at most once per run—i.e., a sys-
tem is not rewarded for retrieving a nugget multiple
times. If the system’s output contains more than one
match for a nugget, the best match is selected and
the rest are left unmarked. A single [document-id,
answer string] pair in a system response can match
0, 1, or multiple nuggets.
The final F-score for an answer is calculated in the
manner described in Figure 1, and the final score of
a run is the average across the F-scores of all ques-
tions. The metric is a weighted harmonic mean be-
tween nugget precision and nugget recall, where re-
call is heavily favored (controlled by the Q parame-
ter, usually set to three). Nugget recall is calculated
solely on vital nuggets, while nugget precision is ap-
proximated by a length allowance based on the num-
ber of both vital and okay nuggets returned. In an
</bodyText>
<page confidence="0.988836">
769
</page>
<bodyText confidence="0.999968333333333">
earlier pilot study, researchers discovered that it was
not possible for assessors to consistently enumer-
ate the total set of nuggets contained in an answer,
which corresponds to the denominator in a precision
calculation (Voorhees, 2003). Thus, a penalty for
verbosity serves as a surrogate for precision.
</bodyText>
<subsectionHeader confidence="0.999191">
2.2 The Pyramid Extension
</subsectionHeader>
<bodyText confidence="0.999988878787879">
The vital/okay distinction has been identified as
a weakness in the TREC nugget-based evalua-
tion methodology (Hildebrandt et al., 2004; Lin
and Demner-Fushman, 2005; Lin and Demner-
Fushman, 2006). There do not appear to be any re-
liable indicators for predicting nugget importance,
which makes it challenging to develop algorithms
sensitive to this consideration. Since only vital
nuggets affect nugget recall, it is difficult for sys-
tems to achieve non-zero scores on topics with few
vital nuggets in the answer key. Thus, scores are
easily affected by assessor errors and other random
variations in evaluation conditions.
One direct consequence is that in previous TREC
evaluations, the median score for many questions
turned out to be zero. A binary distinction on nugget
importance is insufficient to discriminate between
the quality of runs that return no vital nuggets but
different numbers of okay nuggets. Also, a score
distribution heavily skewed towards zero makes
meta-analyses of evaluation stability difficult to per-
form (Voorhees, 2005).
The pyramid extension (Lin and Demner-
Fushman, 2006) was proposed to address the issues
mentioned above. The idea was relatively simple: by
soliciting vital/okay judgments from multiple asses-
sors (after the list of nuggets has been produced by
a primary assessor), it is possible to define nugget
importance with greater granularity. Each nugget is
assigned a weight between zero and one that is pro-
portional to the number of assessors who judged it
to be vital. Nugget recall from Figure 1 can be rede-
fined to incorporate these weights:
</bodyText>
<equation confidence="0.392033">
EmEA wm
� EnEV wn
</equation>
<bodyText confidence="0.9901486">
Where A is the set of reference nuggets that are
matched in a system’s output and V is the set of all
reference nuggets; wm and wn are the weights of
nuggets m and n, respectively.1 The calculation of
nugget precision remains the same.
</bodyText>
<sectionHeader confidence="0.982918" genericHeader="method">
3 Nugget Pyramids in TREC 2006
</sectionHeader>
<bodyText confidence="0.999945461538462">
Lin and Demner-Fushman (2006) present exper-
imental evidence in support of nugget pyramids
by applying the proposal to results from previous
TREC QA evaluations. Their simulation studies ap-
pear to support the assertion that pyramids address
many of the issues raised in Section 2.2. Based on
the results, NIST proceeded with a trial deployment
of nugget pyramids in the TREC 2006 QA track. Al-
though scores based on the binary vital/okay distinc-
tion were retained as the “official” metric, pyramid
scores were simultaneously computed. This pro-
vided an opportunity to compare the two method-
ologies on a large scale.
</bodyText>
<subsectionHeader confidence="0.998831">
3.1 The Data
</subsectionHeader>
<bodyText confidence="0.999968384615385">
The basic unit of evaluation for the main QA task
at TREC 2006 was the “question series”. Each se-
ries focused on a “target”, which could be a person,
organization, thing, or event. Individual questions
in a series inquired about different facets of the tar-
get, and were explicitly classified as factoid, list, or
Other. One complete series is shown in Figure 2.
The Other questions can be best paraphrased as “Tell
me interesting things about X that I haven’t already
explicitly asked about.” It was the system’s task to
retrieve interesting nuggets about the target (in the
opinion of the assessor), but credit was not given
for retrieving facts already explicitly asked for in the
factoid and list questions. The Other questions were
evaluated using the nugget-based methodology, and
are the subject of this analysis.
The QA test set in TREC 2006 contained 75 se-
ries. Of the 75 targets, 19 were persons, 19 were
organizations, 19 were events, and 18 were things.
The series contained a total of 75 Other questions
(one per target). Each series contained 6–9 ques-
tions (counting the Other question), with most se-
ries containing 8 questions. The task employed the
AQUAINT collection of newswire text (LDC cat-
alog number LDC2002T31), consisting of English
data drawn from three sources: the New York Times,
</bodyText>
<footnote confidence="0.995003333333333">
1Note that this new scoring model captures the existing
binary vital/okay distinction in a straightforward way: vital
nuggets get a score of one, and okay nuggets zero.
</footnote>
<page confidence="0.94113">
770
</page>
<table confidence="0.997068444444444">
147 Britain’s Prince Edward marries
147.1 FACTOID When did Prince Edward engage to marry?
147.2 FACTOID Who did the Prince marry?
147.3 FACTOID Where did they honeymoon?
147.4 FACTOID Where was Edward in line for the throne at the time of the wedding?
147.5 FACTOID What was the Prince’s occupation?
147.6 FACTOID How many people viewed the wedding on television?
147.7 LIST What individuals were at the wedding?
147.8 OTHER
</table>
<figureCaption confidence="0.913167">
Figure 2: Sample question series from TREC 2006.
</figureCaption>
<bodyText confidence="0.925361857142857">
Nugget 0 1 2 3 4 5 6 7 8
The couple had a long courtship 1 0 0 0 0 0 1 1 0
Queen Elizabeth II was delighted with the match 0 1 0 1 0 0 0 0 1
Queen named couple Earl and Contessa of Wessex 0 1 0 0 1 1 1 0 0
All marriages of Edward’s siblings ended in divorce 0 0 0 0 0 1 0 0 1
Edward arranged for William to appear more cheerful in photo 0 0 0 0 0 0 0 0 0
they were married in St. Georges Chapel, Windsor 1 1 1 0 1 0 1 1 0
</bodyText>
<figureCaption confidence="0.9803605">
Figure 3: Multiple assessors’ judgments of nugget importance for Series 147 (vital=1, okay=0). Assessor 2
was the same as the primary assessor (assessor 0), but judgments were elicited at different times.
</figureCaption>
<bodyText confidence="0.9997386">
the Associated Press, and the Xinhua News Service.
There are approximately one million articles in the
collection, totaling roughly three gigabytes. In to-
tal, 59 runs from 27 participants were submitted to
NIST. For more details, see (Dang et al., 2006).
For the Other questions, nine sets of judgments
were elicited from eight judges (the primary assessor
who originally created the nuggets later annotated
the nuggets once again). Each assessor was asked to
assign the vital/okay label in a rapid fashion, without
giving each decision much thought. Figure 3 gives
an example of the multiple judgments for nuggets in
Series 147. There is variation in notions of impor-
tance not only between different assessors, but also
for a single assessor over time.
</bodyText>
<subsectionHeader confidence="0.85247">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.985502580645161">
After the human annotation process, nugget pyra-
mids were built in the manner described by Lin and
Demner-Fushman (2006). Two scores were com-
puted for each run submitted to the TREC 2006 main
QA task: one based on the vital/okay judgments of
the primary assessor (which we call the binary F-
score) and one based on the nugget pyramids (the
pyramid F-score). The characteristics of the pyra-
mid method can be inferred by comparing these two
sets of scores.
Figure 4 plots the average binary and average
pyramid F-scores for each run (which represents av-
erage performance across all series). Even though
the nugget pyramid does not represent any single
real user (a point we return to later), pyramid F-
scores do correlate highly with the binary F-scores.
The Pearson’s correlation is 0.987, with a 95% con-
fidence interval of [0.980, 1.00].
While the average F-score for a run is stable given
a sufficient number of questions, the F-score for
a single Other question exhibits greater variability
across assessors. This is shown in Figure 5, which
plots binary and pyramid F-scores for individual
questions from all runs. In this case, the Pearson
correlation is 0.870, with a 95% confidence interval
of [0.863, 1.00].
For 16.4% of all Other questions, the nugget pyra-
mid assigned a non-zero F-score where the origi-
nal binary F-score was zero. This can be seen in
the band of points on the left edge of the plot in
Figure 5. This highlights the strength of nugget
</bodyText>
<page confidence="0.948998">
771
</page>
<figure confidence="0.985029">
0.00 0.05 0.10 0.15
Average binary F−
</figure>
<figureCaption confidence="0.958053">
Figure 4: Scatter plot comparing the binary and
pyramid F-scores for each run.
</figureCaption>
<figure confidence="0.9871675">
0.0 0.2 0.4 0
Binary F−scor
</figure>
<figureCaption confidence="0.994741">
Figure 5: Scatter plot comparing thebinary and
</figureCaption>
<figure confidence="0.519517333333333">
pyramid F-scores for each Other question.
ag py
0
</figure>
<bodyText confidence="0.991420423076923">
pyramids—their ability to smooth out assessor dif-
ferences and more finely discriminate among sys-
tem outputs. This is a key capability that is useful
for system developers, particularly since algorithmic
improvements are often incremental and small.
Because it is more stable than the single-assessor
method of evaluation, the pyramid method also ap-
pears to have greater discriminative power. We fit
a two-way analysis of variance model with the se-
ries and run as factors, and the binary F-score as
the dependent variable. We found significant differ-
ences between series and between runs (p essentially
equal to 0 for both factors). To determine which runs
were significantly different from each other, we per-
formed a multiple comparison using Tukey’s hon-
estly significant difference criterion and controlling
for the experiment-wise Type I error so that the prob-
ability of declaring a difference between two runs to
be significant, when it is actually not, is at most 5%.
With 59 runs, there are C59
2 = 1711 different pairs
that can be compared. The single-assessor method
was able to declare one run to be significantly better
than the other in 557 of these pairs. Using the pyra-
mid F-scores, it was possible to find significant dif-
ferences in performance between runs in 617 pairs.
</bodyText>
<subsectionHeader confidence="0.966778">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.987204615384615">
Any evaluation represents a compromise between
effort (which correlates with cost) and insightful-
ness of results. The level of detail and meaning-
fulness of evaluations are constantly in tension with
the availability of resources. Modifications to exist-
ing processes usually come at a cost that needs to be
weighed against potential gains. Based on these con-
siderations, the balance sheet for nugget pyramids
shows a favorable orientation. In the TREC 2006
QA evaluation, soliciting vital/okay judgments from
multiple assessors was not very time-consuming (a
couple of hours per assessor). Analysis confirms
that pyramid scores confer many benefits at an ac-
ceptable cost, thus arguing for its adoption in future
evaluations.
Cost considerations precluded exploring other re-
finements to the nugget-based evaluation methodol-
ogy. One possible alternative would involve ask-
ing multiple assessors to create different sets of
nuggets from scratch. Not only would this be time-
consuming, one would then need to deal with the
additional complexities of aligning each assessor’s
nuggets list. This includes resolving issues such as
nugget granularity, overlap in information content,
implicature and other relations between nuggets, etc.
a
</bodyText>
<sectionHeader confidence="0.954525" genericHeader="method">
4 Exploration of Alternative Structures
</sectionHeader>
<bodyText confidence="0.999949">
Despite the demonstrated effectiveness of nugget
pyramids, there are a few potential drawbacks that
are worth discussing. One downside is that the
nugget pyramid does not represent a single assessor.
The nugget weights reflect the aggregation of opin-
ions across a sample population, but there is no guar-
</bodyText>
<equation confidence="0.435748">
0
</equation>
<page confidence="0.967391">
772
</page>
<bodyText confidence="0.957873648648649">
antee that the method for computing those weights then compute the mean across those scores. We de-
actually captures any aspect of real user behavior. fine the macro-averaged binary F-score over a set
It can be argued that the binary F-score is more re- A = jai,..., aN} of N assessors as:
alistic since it reflects the opinion of a real user (the
primary assessor), whereas the pyramid F-score tries
to model the opinion of a mythical average user.
Although this point may seem somewhat counter-
intuitive, it represents a well-established tradition
in the information retrieval literature (Voorhees,
2002). In document retrieval, for example, relevance
judgments are provided by a single assessor—even
though it is well known that there are large indi-
vidual differences in notions of relevance. IR re-
searchers believe that human idiosyncrasies are an
inescapable fact present in any system designed for
human users, and hence any attempt to remove those
elements in the evaluation setup is actually undesir-
able. It is the responsibility of researchers to develop
systems that are robust and flexible. This premise,
however, does not mean that IR evaluation results
are unstable or unreliable. Analyses have shown
that despite large variations in human opinions, sys-
tem rankings are remarkably stable (Voorhees, 2000;
Sormunen, 2002)—that is, one can usually be confi-
dent about system comparisons.
The philosophy in IR sharply contrasts with work
in NLP annotation tasks such as parsing, word sense
disambiguation, and semantic role labeling—where
researchers strive for high levels of interannota-
tor agreement, often through elaborate guidelines.
The difference in philosophies arises because unlike
these NLP annotation tasks, where the products are
used primarily by other NLP system components, IR
(and likewise QA) is an end-user task. These sys-
tems are intended for real world use. Since people
differ, systems must be able to accommodate these
differences. Hence, there is a strong preference in
</bodyText>
<table confidence="0.860931428571429">
QA for evaluations that maintain a model of the in-
dividual user.
4.1 Micro- vs. Macro-Averaging
The current nugget pyramid method leverages mul-
tiple judgments to define a weight for each individ-
ual nugget, and then incorporates this weight into
the F-score computation. As an alternative, we pro-
pose another method for combining the opinions of
multiple assessors: evaluate system responses indi-
vidually against N sets of binary judgments, and
773
� a�A Fa
� =
N
</table>
<bodyText confidence="0.990693538461538">
Where Fa is the binary F-score according to the
vital/okay judgments of assessor a. The differ-
ences between the pyramid F-score and the macro-
averaged binary F-score correspond to the distinc-
tion between micro- and macro-averaging discussed
in the context of text classification (Lewis, 1991).
In those applications, both measures are mean-
ingful depending on focus: individual instances or
entire classes. In tasks where it is important
to correctly classify individual instances, micro-
averaging is more appropriate. In tasks where it
is important to correctly identify a class, macro-
averaging better quantifies performance. In classi-
fication tasks, imbalance in the prevalence of each
class can lead to large differences in macro- and
micro-averaged scores. Analogizing to our work,
the original formulation of nugget pyramids corre-
sponds to micro-averaging (since we focus on indi-
vidual nuggets), while the alternative corresponds to
macro-averaging (since we focus on the assessor).
We additionally note that the two methods en-
code different assumptions. Macro-averaging as-
sumes that there is nothing intrinsically interesting
about a nugget—it is simply a matter of a particular
user with particular needs finding a particular nugget
to be of interest. Micro-averaging, on the other hand,
assumes that some nuggets are inherently interest-
ing, independent of the particular interests of users.2
Each approach has characteristics that make it
desirable. From the perspective of evaluators, the
macro-averaged binary F-score is preferable be-
cause it models real users; each set of binary judg-
ments represents the information need of a real user,
each binary F-score represents how well an answer
will satisfy a real user, and the macro-averaged bi-
nary F-score represents how well an answer will sat-
isfy, on average, a sample population of real users.
From the perspective of QA system developers, the
micro-averaged nugget pyramid F-score is prefer-
able because it allows finer discrimination in in-
2We are grateful to an anonymous reviewer for this insight.
dividual nugget performance, which enables better
techniques for system training and optimization.
The macro-averaged binary F-score has the same
desirable properties as the micro-averaged pyramid
F-score in that fewer responses will have zero F-
scores as compared to the single-assessor binary F-
score. We demonstrate this as follows. Let X be a
response that receives a non-zero pyramid F-score.
Let A = ja1, a2, a3,..., aN} be the set of N asses-
sors. Then it can be proven that X also receives a
non-zero macro-averaged binary F-score:
</bodyText>
<listItem confidence="0.955987166666667">
1. There exists some nugget v with weight greater
than 0, such that an answer string r in X
matches v. (def. ofpyramid recall)
2. There exists some assessor ap E A who marked
v as vital. (def. ofpyramid nugget weight)
3. To show that X will also receive a non-zero
macro-averaged binary score, it is sufficient to
show that there is some assessor am E A such
that X receives a non-zero F-score when evalu-
ated using just the vital/okay judgments of am.
(def. of macro-averaged binary F-score)
4. But, such an assessor does exist, namely asses-
</listItem>
<bodyText confidence="0.934392666666667">
sor ap: Consider the binary F-score assigned
to X according to just assessor ap. The re-
call of X is greater than zero, since X contains
the response r that matches the nugget v that
was marked as vital by ap (from (2), (1), and
the def. of recall). The precision must also be
greater than zero (def. ofprecision). Therefore,
the macro-averaged binary F-score of X is non-
zero. (def. of F-score)
</bodyText>
<subsectionHeader confidence="0.994616">
4.2 Analysis from TREC 2006
</subsectionHeader>
<bodyText confidence="0.999871833333333">
While the macro-averaged method is guaranteed to
produce no more zero-valued scores than the micro-
averaged pyramid method, it is not guaranteed that
the scores will be the same for any given response.
What are the empirical characteristics of each ap-
proach? To explore this question, we once again ex-
amined data from TREC 2006.
Figure 6 shows a scatter plot of the pyramid F-
score and macro-averaged binary F-score for every
Other questions in all runs from the TREC 2006
QA track main task. Despite focusing on differ-
ent aspects of the evaluation setup, these measures
</bodyText>
<figure confidence="0.9516185">
0.0 0.2 0.4 0.6
Pyramid F−score
</figure>
<figureCaption confidence="0.9087425">
Figure 6: Scatter plot comparing the pyramid and
macro-averaged binary F-scores for all questions.
</figureCaption>
<table confidence="0.99187">
binary micro macro
binary 1.000/1.000 0.870/0.987 0.861/0.988
micro - 1.000/1.000 0.985/0.996
macro - - 1.000/1.000
</table>
<tableCaption confidence="0.998366">
Table 1: Pearson’s correlation of F-scores, by ques-
</tableCaption>
<bodyText confidence="0.9372643125">
tion and by run.
are highly correlated, even at the level of individ-
ual questions. Table 1 provides a summary of the
correlations between the original binary F-score, the
(micro-averaged) pyramid F-score, and the macro-
averaged binary F-score. Pearson’sar is given for
F-scores at the individual question level (first num-
ber) and at the run level (second number). The cor-
relation between all three variants are about equal at
the level of system runs. At the level of individual
questions, the micro- and macro-averaged F-scores
(using multiple judgments) are still highly correlated
with each other, but each is less correlated with the
single-assessor binary F-score.
−
a
</bodyText>
<subsectionHeader confidence="0.938568">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.998808142857143">
The differences between macro- and micro-
averaging methods invoke a more general discus-
sion on notions of nugget importance. There are
actually two different issues we are attempting to
address with our different approaches: the first is
a more granular scale of nugget importance, the
second is variations across a population of users. In
</bodyText>
<figure confidence="0.948136">
b
</figure>
<page confidence="0.993429">
774
</page>
<bodyText confidence="0.999957166666667">
the micro-averaged pyramid F-scores, we achieve
the first by leveraging the second, i.e., binary
judgments from a large population are combined
to yield weights for individual nuggets. In the
macro-averaged binary F-score, we focus solely on
population effects without addressing granularity of
nugget importance.
Exploring this thread of argument, we can for-
mulate additional approaches for tackling these is-
sues. We could, for example, solicit more granular
individual judgments on each nugget from each as-
sessor, perhaps on a Likert scale or as a continuous
quantity ranging from zero to one. This would yield
two more methods for computing F-scores, both a
macro-averaged and a micro-averaged variant. The
macro-averaged variant would be especially attrac-
tive because it reflects real users and yet individual
F-scores remain discriminative. Despite its possi-
ble advantages, this extension is rejected based on
resource considerations; making snap binary judg-
ments on individual nuggets is much quicker than a
multi-scaled value assignment—at least at present,
the additional costs are not sufficient to offset the
potential gains.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999875">
The important role that large-scale evaluations play
in guiding research in human language technologies
means that the community must “get it right.” This
would ordinarily call for a more conservative ap-
proach to avoid changes that might have unintended
consequences. However, evaluation methodologies
must evolve to reflect the shifting interests of the re-
search community to remain relevant. Thus, orga-
nizers of evaluations must walk a fine line between
progress and chaos. Nevertheless, the introduction
of nugget pyramids in the TREC QA evaluation pro-
vides a case study showing how this fine balance can
indeed be achieved. The addition of multiple judg-
ments of nugget importance yields an evaluation that
is both more stable and more discriminative than the
original single-assessor evaluation, while requiring
only a small additional cost in terms of human labor.
We have explored two different methods for com-
bining judgments from multiple assessors to address
shortcomings in the original nugget-based evalua-
tion setup. Although they make different assump-
tions about the evaluation, results from both ap-
proaches are highly correlated. Thus, we can con-
tinue employing the pyramid-based method, which
is well-suited for developing systems, and still be as-
sured that the results remain consistent with an eval-
uation method that maintains a model of real indi-
vidual users.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996815">
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE). The second
author would like to thank Kiri and Esther for their
kind support.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832925925926">
H. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. In Proc. of
TREC 2006.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering
definition questions with multiple knowledge sources.
In Proc. HLT/NAACL 2004.
D. Lewis. 1991. Evaluating text categorization. In Proc.
of the Speech and Natural Language Workshop.
J. Lin and D. Demner-Fushman. 2005. Automatically
evaluating answers to definition questions. In Proc. of
HLT/EMNLP 2005.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? In Proc. of HLT/NAACL
2006.
E. Sormunen. 2002. Liberal relevance criteria of
TREC—counting on negligible documents? In Proc.
of SIGIR 2002.
E. Voorhees. 2000. Variations in relevance judgments
and the measurement of retrieval effectiveness. IP&amp;M,
36(5):697–716.
E. Voorhees. 2002. The philosophy of information re-
trieval evaluation. In Proc. of CLEF Workshop.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. In Proc. of TREC 2003.
E. Voorhees. 2005. Using question series to evaluate
question answering system effectiveness. In Proc. of
HLT/EMNLP 2005.
</reference>
<page confidence="0.998542">
775
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873510">
<title confidence="0.9969355">Different Structures for Evaluating Answers to Complex Questions: Pyramids Won’t Topple, and Neither Will Human Assessors</title>
<author confidence="0.945784">Hoa Trang Dang</author>
<affiliation confidence="0.9740985">Information Access Division National Institute of Standards and Technology</affiliation>
<address confidence="0.999843">Gaithersburg, MD 20899</address>
<email confidence="0.991692">hoa.dang@nist.gov</email>
<author confidence="0.999254">Jimmy Lin</author>
<affiliation confidence="0.9995755">College of Information Studies University of Maryland</affiliation>
<address confidence="0.999941">College Park, MD 20742</address>
<email confidence="0.999833">jimmylin@umd.edu</email>
<abstract confidence="0.997362523809524">The idea of “nugget pyramids” has recently been introduced as a refinement to the nugget-based methodology used to evaluate answers to complex questions in the TREC QA tracks. This paper examines data from the 2006 evaluation, the first large-scale deployment of the nugget pyramids scheme. We show that this method of combining judgments of nugget importance from multiple assessors increases the stability and discriminative power of the evaluation while introducing only a small additional burden in terms of manual assessment. We also consider an alternative method for combining assessor opinions, which yields a distinction similar to microand macro-averaging in the context of classification tasks. While the two approaches differ in terms of underlying assumptions, their results are nevertheless highly correlated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Dang</author>
<author>J Lin</author>
<author>D Kelly</author>
</authors>
<title>question answering track.</title>
<date>2006</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proc. of TREC</booktitle>
<contexts>
<context position="13261" citStr="Dang et al., 2006" startWordPosition="2170" endWordPosition="2173">0 0 0 1 0 0 1 Edward arranged for William to appear more cheerful in photo 0 0 0 0 0 0 0 0 0 they were married in St. Georges Chapel, Windsor 1 1 1 0 1 0 1 1 0 Figure 3: Multiple assessors’ judgments of nugget importance for Series 147 (vital=1, okay=0). Assessor 2 was the same as the primary assessor (assessor 0), but judgments were elicited at different times. the Associated Press, and the Xinhua News Service. There are approximately one million articles in the collection, totaling roughly three gigabytes. In total, 59 runs from 27 participants were submitted to NIST. For more details, see (Dang et al., 2006). For the Other questions, nine sets of judgments were elicited from eight judges (the primary assessor who originally created the nuggets later annotated the nuggets once again). Each assessor was asked to assign the vital/okay label in a rapid fashion, without giving each decision much thought. Figure 3 gives an example of the multiple judgments for nuggets in Series 147. There is variation in notions of importance not only between different assessors, but also for a single assessor over time. 3.2 Results After the human annotation process, nugget pyramids were built in the manner described </context>
</contexts>
<marker>Dang, Lin, Kelly, 2006</marker>
<rawString>H. Dang, J. Lin, and D. Kelly. 2006. Overview of the TREC 2006 question answering track. In Proc. of TREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hildebrandt</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Answering definition questions with multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In Proc. HLT/NAACL</booktitle>
<contexts>
<context position="8064" citStr="Hildebrandt et al., 2004" startWordPosition="1274" endWordPosition="1277">solely on vital nuggets, while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. In an 769 earlier pilot study, researchers discovered that it was not possible for assessors to consistently enumerate the total set of nuggets contained in an answer, which corresponds to the denominator in a precision calculation (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 2.2 The Pyramid Extension The vital/okay distinction has been identified as a weakness in the TREC nugget-based evaluation methodology (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005; Lin and DemnerFushman, 2006). There do not appear to be any reliable indicators for predicting nugget importance, which makes it challenging to develop algorithms sensitive to this consideration. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to b</context>
</contexts>
<marker>Hildebrandt, Katz, Lin, 2004</marker>
<rawString>W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering definition questions with multiple knowledge sources. In Proc. HLT/NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Evaluating text categorization.</title>
<date>1991</date>
<booktitle>In Proc. of the Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="21110" citStr="Lewis, 1991" startWordPosition="3434" endWordPosition="3435">ltiple judgments to define a weight for each individual nugget, and then incorporates this weight into the F-score computation. As an alternative, we propose another method for combining the opinions of multiple assessors: evaluate system responses individually against N sets of binary judgments, and 773 � a�A Fa � = N Where Fa is the binary F-score according to the vital/okay judgments of assessor a. The differences between the pyramid F-score and the macroaveraged binary F-score correspond to the distinction between micro- and macro-averaging discussed in the context of text classification (Lewis, 1991). In those applications, both measures are meaningful depending on focus: individual instances or entire classes. In tasks where it is important to correctly classify individual instances, microaveraging is more appropriate. In tasks where it is important to correctly identify a class, macroaveraging better quantifies performance. In classification tasks, imbalance in the prevalence of each class can lead to large differences in macro- and micro-averaged scores. Analogizing to our work, the original formulation of nugget pyramids corresponds to micro-averaging (since we focus on individual nug</context>
</contexts>
<marker>Lewis, 1991</marker>
<rawString>D. Lewis. 1991. Evaluating text categorization. In Proc. of the Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Automatically evaluating answers to definition questions.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<contexts>
<context position="8094" citStr="Lin and Demner-Fushman, 2005" startWordPosition="1278" endWordPosition="1281">hile nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. In an 769 earlier pilot study, researchers discovered that it was not possible for assessors to consistently enumerate the total set of nuggets contained in an answer, which corresponds to the denominator in a precision calculation (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 2.2 The Pyramid Extension The vital/okay distinction has been identified as a weakness in the TREC nugget-based evaluation methodology (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005; Lin and DemnerFushman, 2006). There do not appear to be any reliable indicators for predicting nugget importance, which makes it challenging to develop algorithms sensitive to this consideration. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to be zero. A binary distinction o</context>
</contexts>
<marker>Lin, Demner-Fushman, 2005</marker>
<rawString>J. Lin and D. Demner-Fushman. 2005. Automatically evaluating answers to definition questions. In Proc. of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Will pyramids built of nuggets topple over?</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL</booktitle>
<contexts>
<context position="2259" citStr="Lin and Demner-Fushman, 2006" startWordPosition="328" endWordPosition="331">veral years, NIST has implemented an evaluation methodology based on the notion of “information nuggets” to assess answers to complex questions. As it has become the de facto standard for evaluating such systems, the research community stands to benefit from a better understanding of the characteristics of this evaluation methodology. This paper explores recent refinements to the nugget-based evaluation methodology developed by NIST. In particular, we examine the recent so-called “pyramid extension” that incorporates relevance judgments from multiple assessors to improve evaluation stability (Lin and Demner-Fushman, 2006). We organize our discussion as follows: The next section begins by providing a brief overview of nugget-based evaluations and the pyramid extension. Section 3 presents results from the first largescale implementation of nugget pyramids for QA evaluation in TREC 2006. Analysis shows that this extension improves both stability and discriminative power. In Section 4, we discuss an alternative for combining multiple judgments that parallels the distinction between micro- and macro-averaging often seen in classification tasks. Experiments reveal that the methods yield almost exactly the same resul</context>
<context position="9831" citStr="Lin and Demner-Fushman (2006)" startWordPosition="1563" endWordPosition="1566">has been produced by a primary assessor), it is possible to define nugget importance with greater granularity. Each nugget is assigned a weight between zero and one that is proportional to the number of assessors who judged it to be vital. Nugget recall from Figure 1 can be redefined to incorporate these weights: EmEA wm � EnEV wn Where A is the set of reference nuggets that are matched in a system’s output and V is the set of all reference nuggets; wm and wn are the weights of nuggets m and n, respectively.1 The calculation of nugget precision remains the same. 3 Nugget Pyramids in TREC 2006 Lin and Demner-Fushman (2006) present experimental evidence in support of nugget pyramids by applying the proposal to results from previous TREC QA evaluations. Their simulation studies appear to support the assertion that pyramids address many of the issues raised in Section 2.2. Based on the results, NIST proceeded with a trial deployment of nugget pyramids in the TREC 2006 QA track. Although scores based on the binary vital/okay distinction were retained as the “official” metric, pyramid scores were simultaneously computed. This provided an opportunity to compare the two methodologies on a large scale. 3.1 The Data The</context>
<context position="13893" citStr="Lin and Demner-Fushman (2006)" startWordPosition="2272" endWordPosition="2275">or the Other questions, nine sets of judgments were elicited from eight judges (the primary assessor who originally created the nuggets later annotated the nuggets once again). Each assessor was asked to assign the vital/okay label in a rapid fashion, without giving each decision much thought. Figure 3 gives an example of the multiple judgments for nuggets in Series 147. There is variation in notions of importance not only between different assessors, but also for a single assessor over time. 3.2 Results After the human annotation process, nugget pyramids were built in the manner described by Lin and Demner-Fushman (2006). Two scores were computed for each run submitted to the TREC 2006 main QA task: one based on the vital/okay judgments of the primary assessor (which we call the binary Fscore) and one based on the nugget pyramids (the pyramid F-score). The characteristics of the pyramid method can be inferred by comparing these two sets of scores. Figure 4 plots the average binary and average pyramid F-scores for each run (which represents average performance across all series). Even though the nugget pyramid does not represent any single real user (a point we return to later), pyramid Fscores do correlate hi</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>J. Lin and D. Demner-Fushman. 2006. Will pyramids built of nuggets topple over? In Proc. of HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sormunen</author>
</authors>
<title>Liberal relevance criteria of TREC—counting on negligible documents?</title>
<date>2002</date>
<booktitle>In Proc. of SIGIR</booktitle>
<contexts>
<context position="19686" citStr="Sormunen, 2002" startWordPosition="3210" endWordPosition="3211">ll known that there are large individual differences in notions of relevance. IR researchers believe that human idiosyncrasies are an inescapable fact present in any system designed for human users, and hence any attempt to remove those elements in the evaluation setup is actually undesirable. It is the responsibility of researchers to develop systems that are robust and flexible. This premise, however, does not mean that IR evaluation results are unstable or unreliable. Analyses have shown that despite large variations in human opinions, system rankings are remarkably stable (Voorhees, 2000; Sormunen, 2002)—that is, one can usually be confident about system comparisons. The philosophy in IR sharply contrasts with work in NLP annotation tasks such as parsing, word sense disambiguation, and semantic role labeling—where researchers strive for high levels of interannotator agreement, often through elaborate guidelines. The difference in philosophies arises because unlike these NLP annotation tasks, where the products are used primarily by other NLP system components, IR (and likewise QA) is an end-user task. These systems are intended for real world use. Since people differ, systems must be able to </context>
</contexts>
<marker>Sormunen, 2002</marker>
<rawString>E. Sormunen. 2002. Liberal relevance criteria of TREC—counting on negligible documents? In Proc. of SIGIR 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Variations in relevance judgments and the measurement of retrieval effectiveness.</title>
<date>2000</date>
<journal>IP&amp;M,</journal>
<volume>36</volume>
<issue>5</issue>
<contexts>
<context position="19669" citStr="Voorhees, 2000" startWordPosition="3208" endWordPosition="3209"> though it is well known that there are large individual differences in notions of relevance. IR researchers believe that human idiosyncrasies are an inescapable fact present in any system designed for human users, and hence any attempt to remove those elements in the evaluation setup is actually undesirable. It is the responsibility of researchers to develop systems that are robust and flexible. This premise, however, does not mean that IR evaluation results are unstable or unreliable. Analyses have shown that despite large variations in human opinions, system rankings are remarkably stable (Voorhees, 2000; Sormunen, 2002)—that is, one can usually be confident about system comparisons. The philosophy in IR sharply contrasts with work in NLP annotation tasks such as parsing, word sense disambiguation, and semantic role labeling—where researchers strive for high levels of interannotator agreement, often through elaborate guidelines. The difference in philosophies arises because unlike these NLP annotation tasks, where the products are used primarily by other NLP system components, IR (and likewise QA) is an end-user task. These systems are intended for real world use. Since people differ, systems</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>E. Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval effectiveness. IP&amp;M, 36(5):697–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>The philosophy of information retrieval evaluation.</title>
<date>2002</date>
<booktitle>In Proc. of CLEF Workshop.</booktitle>
<contexts>
<context position="18959" citStr="Voorhees, 2002" startWordPosition="3098" endWordPosition="3099"> guar0 772 antee that the method for computing those weights then compute the mean across those scores. We deactually captures any aspect of real user behavior. fine the macro-averaged binary F-score over a set It can be argued that the binary F-score is more re- A = jai,..., aN} of N assessors as: alistic since it reflects the opinion of a real user (the primary assessor), whereas the pyramid F-score tries to model the opinion of a mythical average user. Although this point may seem somewhat counterintuitive, it represents a well-established tradition in the information retrieval literature (Voorhees, 2002). In document retrieval, for example, relevance judgments are provided by a single assessor—even though it is well known that there are large individual differences in notions of relevance. IR researchers believe that human idiosyncrasies are an inescapable fact present in any system designed for human users, and hence any attempt to remove those elements in the evaluation setup is actually undesirable. It is the responsibility of researchers to develop systems that are robust and flexible. This premise, however, does not mean that IR evaluation results are unstable or unreliable. Analyses hav</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E. Voorhees. 2002. The philosophy of information retrieval evaluation. In Proc. of CLEF Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proc. of TREC</booktitle>
<contexts>
<context position="7835" citStr="Voorhees, 2003" startWordPosition="1241" endWordPosition="1242">res of all questions. The metric is a weighted harmonic mean between nugget precision and nugget recall, where recall is heavily favored (controlled by the Q parameter, usually set to three). Nugget recall is calculated solely on vital nuggets, while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. In an 769 earlier pilot study, researchers discovered that it was not possible for assessors to consistently enumerate the total set of nuggets contained in an answer, which corresponds to the denominator in a precision calculation (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 2.2 The Pyramid Extension The vital/okay distinction has been identified as a weakness in the TREC nugget-based evaluation methodology (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005; Lin and DemnerFushman, 2006). There do not appear to be any reliable indicators for predicting nugget importance, which makes it challenging to develop algorithms sensitive to this consideration. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the a</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E. Voorhees. 2003. Overview of the TREC 2003 question answering track. In Proc. of TREC 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Using question series to evaluate question answering system effectiveness.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<contexts>
<context position="8978" citStr="Voorhees, 2005" startWordPosition="1417" endWordPosition="1418">ems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to be zero. A binary distinction on nugget importance is insufficient to discriminate between the quality of runs that return no vital nuggets but different numbers of okay nuggets. Also, a score distribution heavily skewed towards zero makes meta-analyses of evaluation stability difficult to perform (Voorhees, 2005). The pyramid extension (Lin and DemnerFushman, 2006) was proposed to address the issues mentioned above. The idea was relatively simple: by soliciting vital/okay judgments from multiple assessors (after the list of nuggets has been produced by a primary assessor), it is possible to define nugget importance with greater granularity. Each nugget is assigned a weight between zero and one that is proportional to the number of assessors who judged it to be vital. Nugget recall from Figure 1 can be redefined to incorporate these weights: EmEA wm � EnEV wn Where A is the set of reference nuggets tha</context>
</contexts>
<marker>Voorhees, 2005</marker>
<rawString>E. Voorhees. 2005. Using question series to evaluate question answering system effectiveness. In Proc. of HLT/EMNLP 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>