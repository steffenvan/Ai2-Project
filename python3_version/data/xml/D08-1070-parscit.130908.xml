<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000097">
<title confidence="0.987999">
Learning with Probabilistic Features for Improved Pipeline Models
</title>
<author confidence="0.74548">
Razvan C. Bunescu
</author>
<affiliation confidence="0.7839825">
School of EECS
Ohio University
</affiliation>
<address confidence="0.831923">
Athens, OH 45701
</address>
<email confidence="0.998716">
bunescu@ohio.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997021875">
We present a novel learning framework for
pipeline models aimed at improving the com-
munication between consecutive stages in a
pipeline. Our method exploits the confidence
scores associated with outputs at any given
stage in a pipeline in order to compute prob-
abilistic features used at other stages down-
stream. We describe a simple method of in-
tegrating probabilistic features into the linear
scoring functions used by state of the art ma-
chine learning algorithms. Experimental eval-
uation on dependency parsing and named en-
tity recognition demonstrate the superiority of
our approach over the baseline pipeline mod-
els, especially when upstream stages in the
pipeline exhibit low accuracy.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99952152">
Machine learning algorithms are used extensively
in natural language processing. Applications range
from fundamental language tasks such as part of
speech (POS) tagging or syntactic parsing, to higher
level applications such as information extraction
(IE), semantic role labeling (SRL), or question an-
swering (QA). Learning a model for a particular lan-
guage processing problem often requires the output
from other natural language tasks. Syntactic pars-
ing and dependency parsing usually start with a tex-
tual input that is tokenized, split in sentences and
POS tagged. In information extraction, named en-
tity recognition (NER), coreference resolution, and
relation extraction (RE) have been shown to benefit
from features that use POS tags and syntactic depen-
dencies. Similarly, most SRL approaches assume
a parse tree representation of the input sentences.
The common practice in modeling such dependen-
cies is to use a pipeline organization, in which the
output of one task is fed as input to the next task
in the sequence. One advantage of this model is
that it is very simple to implement; it also allows
for a modular approach to natural language process-
ing. The key disadvantage is that errors propagate
between stages in the pipeline, significantly affect-
ing the quality of the final results. One solution
is to solve the tasks jointly, using the principled
framework of probabilistic graphical models. Sut-
ton et al. (2004) use factorial Conditional Random
Fields (CRFs) (Lafferty et al., 2001) to jointly pre-
dict POS tags and segment noun phrases, improving
on the cascaded models that perform the two tasks
in sequence. Wellner et al. (2004) describe a CRF
model that integrates the tasks of citation segmen-
tation and citation matching. Their empirical results
show the superiority of the integrated model over the
pipeline approach. While more accurate than their
pipeline analogues, probabilistic graphical models
that jointly solve multiple natural language tasks are
generally more demanding in terms of finding the
right representations, the associated inference algo-
rithms and their computational complexity. Recent
negative results on the integration of syntactic pars-
ing with SRL (Sutton and McCallum, 2005) provide
additional evidence for the difficulty of this general
approach. When dependencies between the tasks
can be formulated in terms of constraints between
their outputs, a simpler approach is to solve the tasks
separately and integrate the constraints in a linear
programming formulation, as proposed by Roth and
</bodyText>
<page confidence="0.968945">
670
</page>
<note confidence="0.962375">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999165">
Yih (2004) for the simultaneous learning of named
entities and relations between them. More recently,
Finkel et al. (2006) model the linguistic pipelines
as Bayesian networks on which they perform Monte
Carlo inference in order to find the most likely out-
put for the final stage in the pipeline.
In this paper, we present a new learning method
for pipeline models that mitigates the problem of er-
ror propagation between the tasks. Our method ex-
ploits the probabilities output by any given stage in
the pipeline as weights for the features used at other
stages downstream. We show a simple method of
integrating probabilistic features into linear scoring
functions, which makes our approach applicable to
state of the art machine learning algorithms such as
CRFs and Support Vector Machines (Vapnik, 1998;
Sch¨olkopf and Smola, 2002). Experimental results
on dependency parsing and named entity recogni-
tion show useful improvements over the baseline
pipeline models, especially when the basic pipeline
components exhibit low accuracy.
</bodyText>
<sectionHeader confidence="0.788481" genericHeader="method">
2 Learning with Probabilistic Features
</sectionHeader>
<bodyText confidence="0.999969611111111">
We consider that the task is to learn a mapping from
inputs x ∈ X to outputs y ∈ Y(x). Each input
x is also associated with a different set of outputs
z ∈ Z(x) for which we are given a probabilistic
confidence measure p(z|x). In a pipeline model, z
would correspond to the annotations performed on
the input x by all stages in the pipeline other than
the stage that produces y. For example, in the case
of dependency parsing, x is a sequence of words, y
is a set of word-word dependencies, z is a sequence
of POS tags, and p(z|x) is a measure of the confi-
dence that the POS tagger has in the output z. Let
φ be a representation function that maps an exam-
ple (x, y, z) to a feature vector φ(x, y, z) ∈ Rd, and
w ∈ Rd a parameter vector. Equations (1) and (2)
below show the traditional method for computing
the optimal output y� in a pipeline model, assuming
a linear scoring function defined by w and φ.
</bodyText>
<equation confidence="0.99364525">
P(x) = argmax w · φ(x, y, z(x)) (1)
y∈Y(x)
z(x) = argmax p(z|x) (2)
z∈Z(x)
</equation>
<bodyText confidence="0.9998048">
The weight vector w is learned by optimizing a pre-
defined objective function on a training dataset.
In the model above, only the best annotation z�
produced by upstream stages is used for determining
the optimal output y. However, z� may be an incor-
rect annotation, while the correct annotation may be
ignored because it was assigned a lower confidence
value. We propose exploiting all possible annota-
tions and their probabilities as illustrated in the new
model below:
</bodyText>
<equation confidence="0.9996935">
y(x) = argmax w · ψ(x, y) (3)
y∈Y(x)
ψ(x, y) = � p(z|x) · φ(x, y, z) (4)
z∈Z(x)
</equation>
<bodyText confidence="0.999860666666667">
In most cases, directly computing ψ(x, y) is unfeasi-
ble, due to a large number of annotations in Z(x). In
our dependency parsing example, Z(x) contains all
possible POS taggings of sentence x; consequently
its cardinality is exponential in the length of the sen-
tence. A more efficient way of computing ψ(x, y)
can be designed based on the observation that most
components φi of the original feature vector φ utilize
only a limited amount of evidence from the example
(x, y, z). We define (x, y, z) ∈ Fi(x, y, z) to cap-
ture the actual evidence from (x, y, z) that is used by
one instance of feature function φi. We call (x, y, z)
a feature instance of φi in the example (x, y, z).
Correspondingly, Fi(x, y, z) is the set of all fea-
ture instances of φi in example (x, y, z). Usually,
φi(x, y, z) is set to be equal with the number of in-
stances of φi in example (x, y, z), i.e. φi(x, y, z) =
|Fi(x, y, z)|. Table 1 illustrates three feature in-
stances (x, y, z) generated by three typical depen-
dency parsing features in the example from Figure 1.
Because the same feature may be instantiated multi-
</bodyText>
<equation confidence="0.7677528">
φ1 : DT → NN φ2 : NNS → thought φ3 : be ← in
y 10 →11 NNS2 2 →4 7 ←9 in9
z� DT10 NN11 thought4 be7
x�
|Fz |O(|x|2) O(|x|) O(1)
</equation>
<tableCaption confidence="0.998073">
Table 1: Feature instances.
</tableCaption>
<bodyText confidence="0.9988415">
ple times in the same example, the components of
each feature instance are annotated with their po-
sitions relative to the example. Given these defi-
nitions, the feature vector ψ(x, y) from (4) can be
</bodyText>
<page confidence="0.998732">
671
</page>
<figureCaption confidence="0.998585">
Figure 1: Dependency Parsing Example.
</figureCaption>
<bodyText confidence="0.989049">
rewritten in a component-wise manner as follows:
</bodyText>
<equation confidence="0.986497833333333">
ψ(x, y) = [ψ1(x, y) ... ψd(x, y)] (5)
ψi(x, y) = X p(z|x) · φi(x, y, z)
z∈Z(x)
X= p(z|x) · |Fi(x, y, z)|
z∈Z(x)
X= X p(z|x)
z∈Z(x) (˜x,˜y,˜z)∈Fi(x,y,z)
X= X p(z|x)
(˜x,˜y,˜z)∈Fi(x,y,Z(x)) z∈Z(x),z⊇˜z
where Fi(x, y, Z(x)) stands for:
Fi(x, y, Z(x)) = [ Fi(x, y, z)
z∈Z(x)
</equation>
<bodyText confidence="0.937861">
We introduce p(˜z|x) to denote the expectation:
</bodyText>
<equation confidence="0.9941272">
p(˜z|x) = X p(z|x)
z∈Z(x),z⊇˜z
Then ψi(x, y) can be written compactly as:
ψi(x, y) = X p(˜z|x) (6)
(˜x,˜y,˜z)∈Fi(x,y,Z(x))
</equation>
<bodyText confidence="0.999937333333333">
The total number of terms in (6) is equal with the
number of instantiations of feature φi in the exam-
ple (x, y) across all possible annotations z ∈ Z(x),
i.e. |Fi(x, y, Z(x))|. Usually this is significantly
smaller than the exponential number of terms in (4).
The actual number of terms depends on the particu-
lar feature used to generate them, as illustrated in the
last row of Table 1 for the three features used in de-
pendency parsing. The overall time complexity for
calculating ψ(x, y) also depends on the time com-
plexity needed to compute the expectations p(˜z|x).
When z is a sequence, p(˜z|x) can be computed ef-
ficiently using a constrained version of the forward-
backward algorithm (to be described in Section 3).
When z is a tree, p(˜z|x) will be computed using a
constrained version of the CYK algorithm (to be de-
scribed in Section 4).
The time complexity can be further reduced if in-
stead of ψ(x, y) we use its subcomponent ˆψ(x, y)
that is calculated based only on instances that appear
in the optimal annotation ˆz:
</bodyText>
<equation confidence="0.997439666666667">
ˆψ(x, y) = [ˆψ1(x, y) ... ˆψd(x, y)] (7)
ˆψi(x, y) = X p(˜z|x) (8)
(˜x,˜y,˜z)∈Fi(x,y,ˆz)
</equation>
<bodyText confidence="0.998747">
The three models are summarized in Table 2 below.
In the next two sections we illustrate their applica-
</bodyText>
<equation confidence="0.996177588235294">
M1 ˆy(x) = argmax w · φ(x, y)
φ(x, y) = y∈Y(x)
ˆz(x) = φ(x, y, ˆz(x))
argmaxp(z|x)
z∈Z(x)
ˆy(x) = argmaxw · ψ(x, y)
y∈Y(x)
M2 ψ(x, y) = [ψ1(x, y) ... ψd(x, y)]
ψi(x, y) = X p(˜z|x)
(˜x,˜y,˜z)∈Fi(x,y,Z(x))
ˆy(x) = argmaxw · ˆψ(x, y)
y∈Y(x)
M3 ˆψ(x, = ˆψd(x,
y) [ˆψ1(x, y) ... y)]
ˆψi = X p(˜z|x)
(x, y)
(˜x,˜y,˜z)∈Fi(x,y,ˆz)
</equation>
<tableCaption confidence="0.94592">
Table 2: Three Pipeline Models.
</tableCaption>
<bodyText confidence="0.997852">
tion to two common tasks in language processing:
dependency parsing and named entity recognition.
</bodyText>
<sectionHeader confidence="0.989904" genericHeader="method">
3 Dependency Parsing Pipeline
</sectionHeader>
<bodyText confidence="0.9971245">
In a traditional dependency parsing pipeline (model
M1 in Table 2), an input sentence x is first aug-
</bodyText>
<equation confidence="0.9945155">
X=
z∈Z(x)
p(z|x) X 1
(˜x,˜y,˜z)∈Fi(x,y,z)
</equation>
<page confidence="0.969846">
672
</page>
<bodyText confidence="0.999931363636364">
mented with a POS tagging z(x), and then pro-
cessed by a dependency parser in order to obtain
a dependency structure y(x). To evaluate the new
pipeline models we use MSTPARSER1, a linearly
scored dependency parser developed by McDonald
et al. (2005). Following the edge based factorization
method of Eisner (1996), the score of a dependency
tree in the first order version is defined as the sum of
the scores of all edges in the tree. Equivalently, the
feature vector of a dependency tree is defined as the
sum of the feature vectors of all edges in the tree:
</bodyText>
<equation confidence="0.983404">
M1: φ(x, y) = X φ(x, u—*v, z(x))
u→vEy
M2: ψ(x, y) = X ψ(x, u—*v)
u→vEy
M3: �ψ(x, y) = X �ψ(x, u—*v)
u→vEy
</equation>
<bodyText confidence="0.999122142857143">
For each edge u —* v E y, MSTPARSER generates
features based on a set of feature templates that take
into account the words and POS tags at positions u,
v, and their left and right neighbors uf1, v f1. For
example, a particular feature template T used inside
MSTPARSER generates the following POS bigram
features:
</bodyText>
<equation confidence="0.887485666666667">
� 1, if (zu,zv) = (t1,t2)
φi(x,u—*v, z) =
0, otherwise
</equation>
<bodyText confidence="0.99996775">
where t1, t2 E P are the two POS tags associated
with feature index i. By replacing y with u —* v in
the feature expressions from Table 2, we obtain the
following formulations:
</bodyText>
<equation confidence="0.99756225">
M (x u—&gt;v)=�0,
1, if (iu, zv)=(t1,t2)
Ml: �i otherwise
M2:ψi(x, u—*v) =p(�z=(t1, t2)|x)
</equation>
<bodyText confidence="0.932104285714286">
M3: �ψi(x, u—*v)= fp(�z=(t1,t2)|x),if (iu, 4)=(t1,t2)
where, following the notation from Section 2,
z� = (zu, zv) is the actual evidence from z that is
used by feature i, and z� is the top scoring annotation
produced by the POS tagger. The implementation in
MSTPARSER corresponds to the traditional pipeline
model M1. Given a method for computing feature
</bodyText>
<footnote confidence="0.578555">
1URL: http://sourceforge.net/projects/mstparser
</footnote>
<bodyText confidence="0.999097842105263">
probabilities p(z = (t1, t2)|x), it is straightforward
to modify MSTPARSER to implement models M2
and M3 – we simply replace the feature vectors φ
with ψ and ψ� respectively. As mentioned in Sec-
tion 2, the time complexity of computing the fea-
ture vectors ψ in model M2 depends on the com-
plexity of the actual evidence z� used by the fea-
tures. For example, the feature template T used
above is based on the POS tags at both ends of a de-
pendency edge, consequently it would generate |P|2
features in model M2 for any given edge u —* v.
There are however feature templates used in MST-
PARSER that are based on the POS tags of up to 4
tokens in the input sentence, which means that for
each edge they would generate |P|4 ~ 4.5M fea-
tures. Whether using all these probabilistic features
is computationally feasible or not also depends on
the time complexity of computing the confidence
measure p(z|x) associated with each feature.
</bodyText>
<subsectionHeader confidence="0.997177">
3.1 Probabilistic POS features
</subsectionHeader>
<bodyText confidence="0.974218357142857">
The new pipeline models M2 and M3 require an
annotation model that, at a minimum, facilitates
the computation of probabilistic confidence values
for each output. We chose to use linear chain
CRFs (Lafferty et al., 2001) since CRFs can be eas-
ily modified to compute expectations of the type
p(z|x), as needed by M2 and M3.
The CRF tagger was implemented in MAL-
LET (McCallum, 2002) using the original feature
templates from (Ratnaparkhi, 1996). The model
was trained on sections 2–21 from the English Penn
Treebank (Marcus et al., 1993). When tested on sec-
tion 23, the CRF tagger obtains 96.25% accuracy,
which is competitive with more finely tuned systems
such as Ratnaparkhi’s MaxEnt tagger.
We have also implemented in MALLET a con-
strained version of the forward-backward procedure
that allows computing feature probabilities p(z|x).
If z� = (ti1ti2...tik) specifies the tags at k positions
in the sentence, then the procedure recomputes the
α parameters for all positions between i1 and ik by
constraining the state transitions to pass through the
specified tags at the k positions. A similar approach
was used by Culotta et al. in (2004) in order to asso-
ciate confidence values with sequences of contigu-
ous tokens identified by a CRF model as fields in an
information extraction task. The constrained proce-
0, otherwise
</bodyText>
<page confidence="0.990254">
673
</page>
<bodyText confidence="0.9989303">
dure requires (ik − i1)|P|2 = O(N|P|2) multipli-
cations in an order 1 Markov model, where N is the
length of the sentence. Because MSTPARSER uses
an edge based factorization of the scoring function,
the constrained forward procedure will need to be
run for each feature template, for each pair of tokens
in the input sentence x. If the evidence z� required by
the feature template T constrains the tags at k posi-
tions, then the total time complexity for computing
the probabilistic features p(z|x) generated by T is:
</bodyText>
<equation confidence="0.99944">
O(N3|P|k+2)=O(N|P|2) · O(N2) · O(|P|k) (9)
</equation>
<bodyText confidence="0.941132428571429">
As mentioned earlier, some feature templates used
in the dependency parser constrain the POS tags at 4
positions, leading to a O(N3|P|6) time complexity
for a length N sentence. Experimental runs on the
same machine that was used for CRF training show
that such a time complexity is not yet feasible, espe-
cially because of the large size of P (46 POS tags).
In order to speed up the computation of probabilis-
tic features, we made the following two approxima-
tions:
1. Instead of using the constrained forward-
backward procedure, we enforce an indepen-
dence assumption between tags at different po-
sitions and rewrite p(z = (ti1ti2...tik)|x) as:
</bodyText>
<equation confidence="0.988042333333333">
k
p(ti1ti2...tik|x) ≈ H p(tij|x)
j=1
</equation>
<bodyText confidence="0.999384666666667">
The marginal probabilities p(tij|x) are easily
computed using the original forward and back-
ward parameters as:
</bodyText>
<equation confidence="0.988861">
αij(tij|x)βij(tij|x)
p(tij|x) = Z(x)
</equation>
<bodyText confidence="0.999200586206897">
This approximation eliminates the factor
O(N|P|2) from the time complexity in (9).
2. If any of the marginal probabilities p(tij|x) is
less than a predefined threshold (τ|P|)−1, we
set p(z|x) to 0. When τ ≥ 1, the method is
guaranteed to consider at least the most proba-
ble state when computing the probabilistic fea-
tures. Looking back at Equation (4), this is
equivalent with summing feature vectors only
over the most probable annotations z E i(x).
The approximation effectively replaces the fac-
tor O(|P|k) in (9) with a quasi-constant factor.
The two approximations lead to an overall time com-
plexity of O(N2) for computing the probabilistic
features associated with any feature template T , plus
O(N|P|2) for the unconstrained forward-backward
procedure. We will use M2 to refer to the model
M2 that incorporates the two approximations. The
independence assumption from the first approxima-
tion can be relaxed without increasing the asymp-
totic time complexity by considering as independent
only chunks of contiguous POS tags that are at least
a certain number of tokens apart. Consequently,
the probability of the tag sequence will be approxi-
mated with the product of the probabilities of the tag
chunks, where the exact probability of each chunk
is computed in constant time with the constrained
forward-backward procedure. We will use M2 to
refer to the resulting model.
</bodyText>
<subsectionHeader confidence="0.997081">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999564647058824">
MSTPARSER was trained on sections 2–21 from the
WSJ Penn Treebank, using the gold standard POS
tagging. The parser was then evaluated on section
23, using the POS tagging output by the CRF tagger.
For model M1 we need only the best output from
the POS tagger. For models M2 and M2 we com-
pute the probability associated with each feature us-
ing the corresponding approximations, as described
in the previous section. In model M2 we consider
as independent only chunks of POS tags that are 4
tokens or more apart. If the distance between the
chunks is less than 4 tokens, the probability for the
entire tag sequence in the feature is computed ex-
actly using the constrained forward-backward pro-
cedure. Table 3 shows the accuracy obtained by
models M1, M�2(τ) and M″2 (τ) for various values
of the threshold parameter τ. The accuracy is com-
</bodyText>
<table confidence="0.974519">
M1 M2(1) M2(2) M2(4) M″2 (4)
88.51 88.66 88.67 88.67 88.70
</table>
<tableCaption confidence="0.999204">
Table 3: Dependency parsing results.
</tableCaption>
<bodyText confidence="0.957962">
puted over unlabeled dependencies i.e. the percent-
age of words for which the parser has correctly iden-
tified the parent in the dependency tree. The pipeline
</bodyText>
<page confidence="0.99706">
674
</page>
<figureCaption confidence="0.999274">
Figure 2: Named Entity Recognition Example.
</figureCaption>
<bodyText confidence="0.9980275">
model M′2 that uses probabilistic features outper-
forms the traditional pipeline model M1. As ex-
pected, M′′
2 performs slightly better than M′2, due
to a more exact computation of feature probabilities.
Overall, only by using the probabilities associated
with the POS features, we achieve an absolute er-
ror reduction of 0.19%, in a context where the POS
stage in the pipeline already has a very high accu-
racy of 96.25%. We expect probabilistic features to
yield a more substantial improvement in cases where
the pipeline model contains less accurate upstream
stages. Such a case is that of NER based on a com-
bination of POS and dependency parsing features.
</bodyText>
<sectionHeader confidence="0.995536" genericHeader="method">
4 Named Entity Recognition Pipeline
</sectionHeader>
<bodyText confidence="0.99993331707317">
In Named Entity Recognition (NER), the task is to
identify textual mentions of predefined types of en-
tities. Traditionally, NER is modeled as a sequence
classification problem: each token in the input sen-
tence is tagged as being either inside (I) or outside
(O) of an entity mention. Most sequence tagging
approaches use the words and the POS tags in a
limited neighborhood of the current sentence posi-
tion in order to compute the corresponding features.
We augment these flat features with a set of tree
features that are computed based on the words and
POS tags found in the proximity of the current to-
ken in the dependency tree of the sentence. We
argue that such dependency tree features are better
at capturing predicate-argument relationships, espe-
cially when they span long stretches of text. Figure 2
shows a sentence x together with its POS tagging z1,
dependency links z2, and an output tagging y. As-
suming the task is to recognize mentions of people,
the word sailors needs to be tagged as inside. If we
extracted only flat features using a symmetric win-
dow of size 3, the relationship between sailors and
thought would be missed. This relationship is use-
ful, since an agent of the predicate thought is likely
to be a person entity. On the other hand, the nodes
sailors and thought are adjacent in the dependency
tree of the sentence. Therefore, their relationship
can be easily captured as a dependency tree feature
using the same window size.
For every token position, we generate flat features
by considering all unigrams, bigrams and trigrams
that start with the current token and extend either to
the left or to the right. Similarly, we generate tree
features by considering all unigrams, bigrams and
trigrams that start with the current token and extend
in any direction in the undirected version of the de-
pendency tree. The tree features are also augmented
with the actual direction of the dependency arcs be-
tween the tokens. If we use only words to create
n-gram features, the token sailors will be associated
with the following features:
</bodyText>
<listItem confidence="0.9969222">
• Flat: sailors, the sailors, (S) the sailors,
sailors mistakenly, sailors mistakenly thought.
• Tree: sailors, sailors ← the, sailors →
thought, sailors → thought ← must, sailors →
thought ← mistakenly.
</listItem>
<bodyText confidence="0.988755357142857">
We also allow n-grams to use word classes such as
POS tags and any of the following five categories:
(1C) for tokens consisting of one capital letter, (AC)
for tokens containing only capital letters, (FC) for
tokens that start with a capital letter, followed by
small letters, (CD) for tokens containing at least one
digit, and (CRT) for the current token.
The set of features can then be defined as a Carte-
sian product over word classes, as illustrated in Fig-
ure 3 for the original tree feature sailors → thought
← mistakenly. In this case, instead of one com-
pletely lexicalized feature, the model will consider
12 different features such as sailors → VBD ← RB,
NNS → thought ← RB, or NNS → VBD ← RB.
</bodyText>
<page confidence="0.992623">
675
</page>
<figure confidence="0.976454">
 r VBD 1 r RB 1
thought × × mistakenly J
</figure>
<figureCaption confidence="0.999547">
Figure 3: Dependency tree features.
</figureCaption>
<bodyText confidence="0.98941975">
The pipeline model M2 uses features that appear
in all possible annotations z = hz1, z2i, where z1
and z2 are the POS tagging and the dependency
parse respectively. If the corresponding evidence is
</bodyText>
<equation confidence="0.9544355">
z� = h�z1, z2i, then:
p(�z|x) = p(�z2|�z1, x)p(�z1|x)
</equation>
<bodyText confidence="0.987504111111111">
For example, NNS2 → thought4 ← RB3 is a feature
instance for the token sailors in the annotations from
Figure 2. This can be construed as having been gen-
erated by a feature template T that outputs the POS
tag ti at the current position, the word xj that is the
parent of xi in the dependency tree, and the POS tag
tk of another dependent of xj (i.e. ti → xj ← tk).
The probability p(z|x) for this type of features can
then be written as:
</bodyText>
<equation confidence="0.62741">
p(�z|x) = p(i→j ←k|ti, tk, x) · p(ti, tk|x)
</equation>
<bodyText confidence="0.959030666666667">
The two probability factors can be computed exactly
as follows:
1. The M2 model for dependency parsing from
Section 3 is used to compute the probabilistic
features ψ(x, u → v|ti, tk) by constraining the
POS annotations to pass through tags ti and tk
at positions i and k. The total time complexity
for this step is O(N3|P|k+2).
2. Having access to ψ(x, u → v|ti, tk), the factor
p(i→j←k|ti, tk, x) can be computed in O(N3)
time using a constrained version of Eisner’s al-
gorithm, as will be explained in Section 4.1.
</bodyText>
<listItem confidence="0.5975405">
3. As described in Section 3.1, computing the
expectation p(ti, tk|x) takes O(N|P2|) time
using the constrained forward-backward algo-
rithm.
</listItem>
<bodyText confidence="0.995575285714286">
The current token position i can have a total of N
values, while j and k can be any positions other
than i. Also, ti and tk can be any POS tag from
P. Consequently, the feature template T induces
O(N3|P|2) feature instances. Overall, the time
complexity for computing the feature instances gen-
erated by T is O(N6|P|k+4), as results from:
</bodyText>
<equation confidence="0.994174">
O(N3|P|2) · (O(N3|P|k+2) + O(N3) + O(N|P|2))
</equation>
<bodyText confidence="0.964411">
While still polynomial, this time complexity is fea-
sible only for small values of N. In general, the time
complexity for computing probabilistic features in
the full model M2 increases with both the number
of stages in the pipeline and the complexity of the
features.
Motivated by efficiency, we decided to use the
pipeline model M3 in which probabilities are com-
puted only over features that appear in the top scor-
ing annotation z� = hz1, z2i, where z1 and z2 repre-
sent the best POS tagging, and the best dependency
parse respectively. In order to further speed up the
computation of probabilistic features, we made the
following approximations:
1. We consider the POS tagging and the depen-
dency parse independent and rewrite p(z|x) as:
</bodyText>
<equation confidence="0.983646">
p(�z|x) = p(�z1, z2|x) ≈ p(�z1|x)p(z2|x)
</equation>
<bodyText confidence="0.957745">
2. We enforce an independence assumption be-
tween POS tags. Thus, if z1 = htilti2...tiki
specifies the tags at k positions in the sentence,
then p(z1|x) is rewritten as:
</bodyText>
<equation confidence="0.983694333333333">
k
p(tilti2...tik|x) ≈ Y p(tij|x)
j=1
</equation>
<bodyText confidence="0.6024705">
3. We also enforce a similar independence as-
sumption between dependency links. Thus, if
z2 = hu1 → v1...uk → vki specifies k depen-
dency links, then p(z2|x) is rewritten as:
</bodyText>
<equation confidence="0.964236">
p(u1→v1...uk →vk|x) ≈ Yk p(ul →vl|x)
l=1
</equation>
<bodyText confidence="0.998705">
For example, the probability p(z|x) of the feature
instance NNS2 → thought4 ← RB3 is approximated
as:
</bodyText>
<equation confidence="0.999936333333333">
p(�z|x) ≈ p(�z1|x) · p(�z2|x)
p(z1|x) ≈ p(t2 =NNS|x) · p(t3 =RB|x)
p(�z2|x) ≈ p(2→4|x) · p(3→4|x)
</equation>
<bodyText confidence="0.994163">
We will use M′3 to refer to the resulting model.
</bodyText>
<figure confidence="0.9045418">
hCRTi
NNS
sailors


</figure>
<page confidence="0.990146">
676
</page>
<subsectionHeader confidence="0.996837">
4.1 Probabilistic Dependency Features
</subsectionHeader>
<bodyText confidence="0.999529363636364">
The probabilistic POS features p(ti|x) are computed
using the forward-backward procedure in CRFs, as
described in Section 3.1. To completely specify the
pipeline model for NER, we also need an efficient
method for computing the probabilistic dependency
features p(u → v|x), where u → v is a dependency
edge between positions u and v in the sentence x.
MSTPARSER is a large-margin method that com-
putes an unbounded score s(x, y) for any given sen-
tence x and dependency structure y ∈ Y(x) using
the following edge-based factorization:
</bodyText>
<equation confidence="0.967507">
s(x, y) = � �s(x, u→v) = w O(x, u→v)
u→vEy u→vEy
</equation>
<bodyText confidence="0.9999777">
The following three steps describe a general method
for associating probabilities with output substruc-
tures. The method can be applied whenever a struc-
tured output is associated a score value that is un-
bounded in ][R, assuming that the score of the entire
output structure can be computed efficiently based
on a factorization into smaller substructures.
S1. Map the unbounded score s(x, y) from ][R
into [0, 1] using the softmax function (Bishop, 1995):
The normalized score n(x, y) preserves the ranking
given by the original score s(x, y). The normaliza-
tion constant at the denominator can be computed in
O(N3) time by replacing the max operator with the
sum operator inside Eisner’s chart parsing algorithm.
S2. Compute a normalized score for the sub-
structure by summing up the normalized scores of
all the complete structures that contain it. In our
model, dependency edges are substructures, while
dependency trees are complete structures. The nor-
malized score will then be computed as:
</bodyText>
<equation confidence="0.9465165">
n(x,u→v) = � n(x, y)
yEY(x),u→vEy
</equation>
<bodyText confidence="0.99959">
The sum can be computed in O(N3) time using a
constrained version of the algorithm that computes
the normalization constant in step S1. This con-
strained version of Eisner’s algorithm works in a
similar manner with the constrained forward back-
ward algorithm by restricting the dependency struc-
tures to contain a predefined edge or set of edges.
S3. Use the isotonic regression method of
Zadrozny and Elkan (2002) to map the normalized
scores n(x, u → v) into probabilities p(u → v|x). A
potential problem with the softmax function is that,
depending on the distribution of scores, the expo-
nential transform could dramatically overinflate the
higher scores. Isotonic regression, by redistributing
the normalized scores inside [0, 1], can alleviate this
problem.
</bodyText>
<subsectionHeader confidence="0.922205">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999925">
We test the pipeline model M′3 versus the traditional
model M1 on the task of detecting mentions of per-
son entities in the ACE dataset2. We use the standard
training – testing split of the ACE 2002 dataset in
which the training dataset is also augmented with the
documents from the ACE 2003 dataset. The com-
bined dataset contains 674 documents for training
and 97 for testing. We implemented the CRF model
in MALLET using three different sets of features:
Tree, Flat, and Full corresponding to the union of
all flat and tree features. The POS tagger and the de-
pendency parser were trained on sections 2-21 of the
Penn Treebank, followed by an isotonic regression
step on section 23 for the dependency parser. We
compute precision recall (PR) graphs by varying a
threshold on the token level confidence output by the
CRF tagger, and summarize the tagger performance
using the area under the curve. Table 4 shows the re-
sults obtained by the two models under the three fea-
ture settings. The model based on probabilistic fea-
</bodyText>
<table confidence="0.998758">
Model Tree Flat Full
M′3 76.78 77.02 77.96
M1 74.38 76.53 77.02
</table>
<tableCaption confidence="0.999812">
Table 4: Mention detection results.
</tableCaption>
<bodyText confidence="0.9996678">
tures consistently outperforms the traditional model,
especially when only tree features are used. Depen-
dency parsing is significantly less accurate than POS
tagging. Consequently, the improvement for the tree
based model is more substantial than for the flat
</bodyText>
<footnote confidence="0.686993">
2URL: http://www.nist.gov/speech/tests/ace
</footnote>
<equation confidence="0.836747333333333">
es(x,y)
n(x, y) = es(x,y)
EyEY(x)
</equation>
<page confidence="0.862336">
677
</page>
<figure confidence="0.986532">
Precision
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
</figure>
<figureCaption confidence="0.99985">
Figure 4: PR graphs for tree features.
</figureCaption>
<bodyText confidence="0.999984166666667">
model, confirming our expectation that probabilis-
tic features are more useful when upstream stages in
the pipeline are less accurate. Figure 4 shows the PR
curves obtained for the tree-based models, on which
we see a significant 5% improvement in precision
over a wide range of recall values.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999763125">
In terms of the target task – improving the perfor-
mance of linguistic pipelines – our research is most
related to the work of Finkel et al. (2006). In their
approach, output samples are drawn at each stage
in the pipeline conditioned on the samples drawn
at previous stages, and the final output is deter-
mined by a majority vote over the samples from
the final stage. The method needs very few sam-
ples for tasks such as textual entailment, where the
final outcome is binary, in agreement with a theo-
retical result on the rate of convergence of the vot-
ing Gibbs classifier due to Ng and Jordan (2001).
While their sampling method is inherently approx-
imate, our full pipeline model M2 is exact in the
sense that feature expectations are computed exactly
in polynomial time whenever the inference step at
each stage can be done in polynomial time, irrespec-
tive of the cardinality of the final output space. Also,
the pipeline models M2 and M3 and their more effi-
cient alternatives propagate uncertainty during both
training and testing through the vector of probabilis-
tic features, whereas the sampling method takes ad-
vantage of the probabilistic nature of the outputs
only during testing. Overall, the two approaches
can be seen as complementary. In order to be ap-
plicable with minimal engineering effort, the sam-
pling method needs NLP researchers to write pack-
ages that can generate samples from the posterior.
Similarly, the new pipeline models could be easily
applied in a diverse range of applications, assum-
ing researchers develop packages that can efficiently
compute marginals over output substructures.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989258064516">
We have presented a new, general method for im-
proving the communication between consecutive
stages in pipeline models. The method relies on
the computation of probabilities for count features,
which translates in adding a polynomial factor to the
overall time complexity of the pipeline whenever the
inference step at each stage is done in polynomial
time, which is the case for the vast majority of infer-
ence algorithms used in practical NLP applications.
We have also shown that additional independence
assumptions can make the approach more practical
by significantly reducing the time complexity. Ex-
isting learning based models can implement the new
method by replacing the original feature vector with
a more dense vector of probabilistic features3. It is
essential that every stage in the pipeline produces
probabilistic features, and to this end we have de-
scribed an effective method for associating proba-
bilities with output substructures.
We have shown for NER that simply using the
probabilities associated with features that appear
only in the top annotation can lead to useful im-
provements in performance, with minimal engineer-
ing effort. In future work we plan to empirically
evaluate NER with an approximate version of the
full model M2 which, while more demanding in
terms of time complexity, could lead to even more
significant gains in accuracy. We also intend to com-
prehensively evaluate the proposed scheme for com-
puting probabilities by experimenting with alterna-
tive normalization functions.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999562333333333">
We would like to thank Rada Mihalcea and the
anonymous reviewers for their insightful comments
and suggestions.
</bodyText>
<footnote confidence="0.743766">
3The Java source code will be released on my web page.
</footnote>
<figure confidence="0.997475307692308">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Probabilistic
Traditional
</figure>
<page confidence="0.988774">
678
</page>
<sectionHeader confidence="0.997747" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889649350649">
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recogntion. Oxford University Press.
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In Proceed-
ings of Human Language Technology Conference and
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), Boston, MA.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th Conference on Computational linguis-
tics, pages 340–345, Copenhagen, Denmark.
Jenny R. Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annota-
tion pipelines. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 618–626, Sydney, Australia.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of 18th International Conference on
Machine Learning (ICML-2001), pages 282–289,
Williamstown, MA.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313–330.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics (ACL-
05), pages 91–98, Ann Arbor, Michigan.
Andrew Y. Ng and Michael I. Jordan. 2001. Conver-
gence rates of the Voting Gibbs classifier, with appli-
cation to bayesian feature selection. In Proceedings of
18th International Conference on Machine Learning
(ICML-2001), pages 377–384, Williamstown, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part of speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-96), pages 133–141, Philadel-
phia, PA.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
In Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004),
pages 1–8, Boston, MA.
Bernhard Sch¨olkopf and Alexander J. Smola. 2002.
Learning with kernels - support vector machines, regu-
larization, optimization and beyond. MIT Press, Cam-
bridge, MA.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In CoNLL-05 Shared
Task.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of 21st In-
ternational Conference on Machine Learning (ICML-
2004), pages 783–790, Banff, Canada, July.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley &amp; Sons.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Proceedings of 20th
Conference on Uncertainty in Artificial Intelligence
(UAI-2004), Banff, Canada, July.
Bianca Zadrozny and Charles Elkan. 2002. Trans-
forming classifier scores into accurate multiclass prob-
ability estimates. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-2002), Ed-
monton, Alberta.
</reference>
<page confidence="0.999032">
679
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857495">
<title confidence="0.999844">Learning with Probabilistic Features for Improved Pipeline Models</title>
<author confidence="0.995515">C Razvan</author>
<affiliation confidence="0.971852">School of Ohio</affiliation>
<address confidence="0.917259">Athens, OH</address>
<email confidence="0.999492">bunescu@ohio.edu</email>
<abstract confidence="0.999322058823529">We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline. Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute probabilistic features used at other stages downstream. We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms. Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models, especially when upstream stages in the pipeline exhibit low accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recogntion.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="25975" citStr="Bishop, 1995" startWordPosition="4441" endWordPosition="4442">(x, y) for any given sentence x and dependency structure y ∈ Y(x) using the following edge-based factorization: s(x, y) = � �s(x, u→v) = w O(x, u→v) u→vEy u→vEy The following three steps describe a general method for associating probabilities with output substructures. The method can be applied whenever a structured output is associated a score value that is unbounded in ][R, assuming that the score of the entire output structure can be computed efficiently based on a factorization into smaller substructures. S1. Map the unbounded score s(x, y) from ][R into [0, 1] using the softmax function (Bishop, 1995): The normalized score n(x, y) preserves the ranking given by the original score s(x, y). The normalization constant at the denominator can be computed in O(N3) time by replacing the max operator with the sum operator inside Eisner’s chart parsing algorithm. S2. Compute a normalized score for the substructure by summing up the normalized scores of all the complete structures that contain it. In our model, dependency edges are substructures, while dependency trees are complete structures. The normalized score will then be computed as: n(x,u→v) = � n(x, y) yEY(x),u→vEy The sum can be computed in</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recogntion. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence estimation for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<location>Boston, MA.</location>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum. 2004. Confidence estimation for information extraction. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational linguistics,</booktitle>
<pages>340--345</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="10324" citStr="Eisner (1996)" startWordPosition="1756" endWordPosition="1757"> 2: Three Pipeline Models. tion to two common tasks in language processing: dependency parsing and named entity recognition. 3 Dependency Parsing Pipeline In a traditional dependency parsing pipeline (model M1 in Table 2), an input sentence x is first augX= z∈Z(x) p(z|x) X 1 (˜x,˜y,˜z)∈Fi(x,y,z) 672 mented with a POS tagging z(x), and then processed by a dependency parser in order to obtain a dependency structure y(x). To evaluate the new pipeline models we use MSTPARSER1, a linearly scored dependency parser developed by McDonald et al. (2005). Following the edge based factorization method of Eisner (1996), the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree. Equivalently, the feature vector of a dependency tree is defined as the sum of the feature vectors of all edges in the tree: M1: φ(x, y) = X φ(x, u—*v, z(x)) u→vEy M2: ψ(x, y) = X ψ(x, u—*v) u→vEy M3: �ψ(x, y) = X �ψ(x, u—*v) u→vEy For each edge u —* v E y, MSTPARSER generates features based on a set of feature templates that take into account the words and POS tags at positions u, v, and their left and right neighbors uf1, v f1. For example, a particular feature template T</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th Conference on Computational linguistics, pages 340–345, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>618--626</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3704" citStr="Finkel et al. (2006)" startWordPosition="560" endWordPosition="563">) provide additional evidence for the difficulty of this general approach. When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by Roth and 670 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679, Honolulu, October 2008.c�2008 Association for Computational Linguistics Yih (2004) for the simultaneous learning of named entities and relations between them. More recently, Finkel et al. (2006) model the linguistic pipelines as Bayesian networks on which they perform Monte Carlo inference in order to find the most likely output for the final stage in the pipeline. In this paper, we present a new learning method for pipeline models that mitigates the problem of error propagation between the tasks. Our method exploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream. We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art m</context>
<context position="29345" citStr="Finkel et al. (2006)" startWordPosition="4997" endWordPosition="5000">//www.nist.gov/speech/tests/ace es(x,y) n(x, y) = es(x,y) EyEY(x) 677 Precision 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Figure 4: PR graphs for tree features. model, confirming our expectation that probabilistic features are more useful when upstream stages in the pipeline are less accurate. Figure 4 shows the PR curves obtained for the tree-based models, on which we see a significant 5% improvement in precision over a wide range of recall values. 5 Related Work In terms of the target task – improving the performance of linguistic pipelines – our research is most related to the work of Finkel et al. (2006). In their approach, output samples are drawn at each stage in the pipeline conditioned on the samples drawn at previous stages, and the final output is determined by a majority vote over the samples from the final stage. The method needs very few samples for tasks such as textual entailment, where the final outcome is binary, in agreement with a theoretical result on the rate of convergence of the voting Gibbs classifier due to Ng and Jordan (2001). While their sampling method is inherently approximate, our full pipeline model M2 is exact in the sense that feature expectations are computed ex</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>Jenny R. Finkel, Christopher D. Manning, and Andrew Y. Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 618–626, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>282--289</pages>
<location>Williamstown, MA.</location>
<contexts>
<context position="2362" citStr="Lafferty et al., 2001" startWordPosition="363" endWordPosition="366">in modeling such dependencies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence. One advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language processing. The key disadvantage is that errors propagate between stages in the pipeline, significantly affecting the quality of the final results. One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models. Sutton et al. (2004) use factorial Conditional Random Fields (CRFs) (Lafferty et al., 2001) to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence. Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching. Their empirical results show the superiority of the integrated model over the pipeline approach. While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their com</context>
<context position="12889" citStr="Lafferty et al., 2001" startWordPosition="2210" endWordPosition="2213">feature templates used in MSTPARSER that are based on the POS tags of up to 4 tokens in the input sentence, which means that for each edge they would generate |P|4 ~ 4.5M features. Whether using all these probabilistic features is computationally feasible or not also depends on the time complexity of computing the confidence measure p(z|x) associated with each feature. 3.1 Probabilistic POS features The new pipeline models M2 and M3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output. We chose to use linear chain CRFs (Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(z|x), as needed by M2 and M3. The CRF tagger was implemented in MALLET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996). The model was trained on sections 2–21 from the English Penn Treebank (Marcus et al., 1993). When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger. We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probab</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 282–289, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13204" citStr="Marcus et al., 1993" startWordPosition="2264" endWordPosition="2267">onfidence measure p(z|x) associated with each feature. 3.1 Probabilistic POS features The new pipeline models M2 and M3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output. We chose to use linear chain CRFs (Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(z|x), as needed by M2 and M3. The CRF tagger was implemented in MALLET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996). The model was trained on sections 2–21 from the English Penn Treebank (Marcus et al., 1993). When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger. We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probabilities p(z|x). If z� = (ti1ti2...tik) specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions between i1 and ik by constraining the state transitions to pass through the specified tags at the k positions. A similar approach was used by Culotta et al. in </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="13049" citStr="McCallum, 2002" startWordPosition="2242" endWordPosition="2243">.5M features. Whether using all these probabilistic features is computationally feasible or not also depends on the time complexity of computing the confidence measure p(z|x) associated with each feature. 3.1 Probabilistic POS features The new pipeline models M2 and M3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output. We chose to use linear chain CRFs (Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(z|x), as needed by M2 and M3. The CRF tagger was implemented in MALLET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996). The model was trained on sections 2–21 from the English Penn Treebank (Marcus et al., 1993). When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger. We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probabilities p(z|x). If z� = (ti1ti2...tik) specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions betwe</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL05),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10260" citStr="McDonald et al. (2005)" startWordPosition="1745" endWordPosition="1748"> y) [ˆψ1(x, y) ... y)] ˆψi = X p(˜z|x) (x, y) (˜x,˜y,˜z)∈Fi(x,y,ˆz) Table 2: Three Pipeline Models. tion to two common tasks in language processing: dependency parsing and named entity recognition. 3 Dependency Parsing Pipeline In a traditional dependency parsing pipeline (model M1 in Table 2), an input sentence x is first augX= z∈Z(x) p(z|x) X 1 (˜x,˜y,˜z)∈Fi(x,y,z) 672 mented with a POS tagging z(x), and then processed by a dependency parser in order to obtain a dependency structure y(x). To evaluate the new pipeline models we use MSTPARSER1, a linearly scored dependency parser developed by McDonald et al. (2005). Following the edge based factorization method of Eisner (1996), the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree. Equivalently, the feature vector of a dependency tree is defined as the sum of the feature vectors of all edges in the tree: M1: φ(x, y) = X φ(x, u—*v, z(x)) u→vEy M2: ψ(x, y) = X ψ(x, u—*v) u→vEy M3: �ψ(x, y) = X �ψ(x, u—*v) u→vEy For each edge u —* v E y, MSTPARSER generates features based on a set of feature templates that take into account the words and POS tags at positions u, v, and their left and right n</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL05), pages 91–98, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Convergence rates of the Voting Gibbs classifier, with application to bayesian feature selection.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>377--384</pages>
<location>Williamstown, MA.</location>
<contexts>
<context position="29798" citStr="Ng and Jordan (2001)" startWordPosition="5079" endWordPosition="5082">lues. 5 Related Work In terms of the target task – improving the performance of linguistic pipelines – our research is most related to the work of Finkel et al. (2006). In their approach, output samples are drawn at each stage in the pipeline conditioned on the samples drawn at previous stages, and the final output is determined by a majority vote over the samples from the final stage. The method needs very few samples for tasks such as textual entailment, where the final outcome is binary, in agreement with a theoretical result on the rate of convergence of the voting Gibbs classifier due to Ng and Jordan (2001). While their sampling method is inherently approximate, our full pipeline model M2 is exact in the sense that feature expectations are computed exactly in polynomial time whenever the inference step at each stage can be done in polynomial time, irrespective of the cardinality of the final output space. Also, the pipeline models M2 and M3 and their more efficient alternatives propagate uncertainty during both training and testing through the vector of probabilistic features, whereas the sampling method takes advantage of the probabilistic nature of the outputs only during testing. Overall, the</context>
</contexts>
<marker>Ng, Jordan, 2001</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2001. Convergence rates of the Voting Gibbs classifier, with application to bayesian feature selection. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 377–384, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part of speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-96),</booktitle>
<pages>133--141</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13111" citStr="Ratnaparkhi, 1996" startWordPosition="2250" endWordPosition="2251">s is computationally feasible or not also depends on the time complexity of computing the confidence measure p(z|x) associated with each feature. 3.1 Probabilistic POS features The new pipeline models M2 and M3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output. We chose to use linear chain CRFs (Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(z|x), as needed by M2 and M3. The CRF tagger was implemented in MALLET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996). The model was trained on sections 2–21 from the English Penn Treebank (Marcus et al., 1993). When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger. We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probabilities p(z|x). If z� = (ti1ti2...tik) specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions between i1 and ik by constraining the state transitions to pass thr</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part of speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-96), pages 133–141, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<pages>1--8</pages>
<location>Boston, MA.</location>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 1–8, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with kernels - support vector machines, regularization, optimization and beyond.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>Bernhard Sch¨olkopf and Alexander J. Smola. 2002. Learning with kernels - support vector machines, regularization, optimization and beyond. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint parsing and semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoNLL-05 Shared Task.</booktitle>
<contexts>
<context position="3085" citStr="Sutton and McCallum, 2005" startWordPosition="471" endWordPosition="474">orm the two tasks in sequence. Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching. Their empirical results show the superiority of the integrated model over the pipeline approach. While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their computational complexity. Recent negative results on the integration of syntactic parsing with SRL (Sutton and McCallum, 2005) provide additional evidence for the difficulty of this general approach. When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by Roth and 670 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679, Honolulu, October 2008.c�2008 Association for Computational Linguistics Yih (2004) for the simultaneous learning of named entities and relations between them. More recently, F</context>
</contexts>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Charles Sutton and Andrew McCallum. 2005. Joint parsing and semantic role labeling. In CoNLL-05 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of 21st International Conference on Machine Learning (ICML2004),</booktitle>
<pages>783--790</pages>
<location>Banff, Canada,</location>
<contexts>
<context position="2291" citStr="Sutton et al. (2004)" startWordPosition="352" endWordPosition="356">arse tree representation of the input sentences. The common practice in modeling such dependencies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence. One advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language processing. The key disadvantage is that errors propagate between stages in the pipeline, significantly affecting the quality of the final results. One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models. Sutton et al. (2004) use factorial Conditional Random Fields (CRFs) (Lafferty et al., 2001) to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence. Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching. Their empirical results show the superiority of the integrated model over the pipeline approach. While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the r</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of 21st International Conference on Machine Learning (ICML2004), pages 783–790, Banff, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="4385" citStr="Vapnik, 1998" startWordPosition="674" endWordPosition="675">rform Monte Carlo inference in order to find the most likely output for the final stage in the pipeline. In this paper, we present a new learning method for pipeline models that mitigates the problem of error propagation between the tasks. Our method exploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream. We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art machine learning algorithms such as CRFs and Support Vector Machines (Vapnik, 1998; Sch¨olkopf and Smola, 2002). Experimental results on dependency parsing and named entity recognition show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy. 2 Learning with Probabilistic Features We consider that the task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y(x). Each input x is also associated with a different set of outputs z ∈ Z(x) for which we are given a probabilistic confidence measure p(z|x). In a pipeline model, z would correspond to the annotations performed on the input x by all stages in the</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>Andrew McCallum</author>
<author>Fuchun Peng</author>
<author>Michael Hay</author>
</authors>
<title>An integrated, conditional model of information extraction and coreference with application to citation matching.</title>
<date>2004</date>
<booktitle>In Proceedings of 20th Conference on Uncertainty in Artificial Intelligence (UAI-2004),</booktitle>
<location>Banff, Canada,</location>
<contexts>
<context position="2511" citStr="Wellner et al. (2004)" startWordPosition="389" endWordPosition="392"> advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language processing. The key disadvantage is that errors propagate between stages in the pipeline, significantly affecting the quality of the final results. One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models. Sutton et al. (2004) use factorial Conditional Random Fields (CRFs) (Lafferty et al., 2001) to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence. Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching. Their empirical results show the superiority of the integrated model over the pipeline approach. While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their computational complexity. Recent negative results on the integration of syntactic parsing with SRL (Sutton and McCallum, 2005) provide additional eviden</context>
</contexts>
<marker>Wellner, McCallum, Peng, Hay, 2004</marker>
<rawString>Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. 2004. An integrated, conditional model of information extraction and coreference with application to citation matching. In Proceedings of 20th Conference on Uncertainty in Artificial Intelligence (UAI-2004), Banff, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bianca Zadrozny</author>
<author>Charles Elkan</author>
</authors>
<title>Transforming classifier scores into accurate multiclass probability estimates.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002),</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="26960" citStr="Zadrozny and Elkan (2002)" startWordPosition="4600" endWordPosition="4603">mplete structures that contain it. In our model, dependency edges are substructures, while dependency trees are complete structures. The normalized score will then be computed as: n(x,u→v) = � n(x, y) yEY(x),u→vEy The sum can be computed in O(N3) time using a constrained version of the algorithm that computes the normalization constant in step S1. This constrained version of Eisner’s algorithm works in a similar manner with the constrained forward backward algorithm by restricting the dependency structures to contain a predefined edge or set of edges. S3. Use the isotonic regression method of Zadrozny and Elkan (2002) to map the normalized scores n(x, u → v) into probabilities p(u → v|x). A potential problem with the softmax function is that, depending on the distribution of scores, the exponential transform could dramatically overinflate the higher scores. Isotonic regression, by redistributing the normalized scores inside [0, 1], can alleviate this problem. 4.2 Experimental Results We test the pipeline model M′3 versus the traditional model M1 on the task of detecting mentions of person entities in the ACE dataset2. We use the standard training – testing split of the ACE 2002 dataset in which the trainin</context>
</contexts>
<marker>Zadrozny, Elkan, 2002</marker>
<rawString>Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002), Edmonton, Alberta.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>