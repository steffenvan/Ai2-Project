<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017869">
<title confidence="0.984682">
A fast and accurate method for detecting English-Japanese parallel texts
</title>
<author confidence="0.993467">
Ken’ichi Fukushima, Kenjiro Taura and Takashi Chikayama
</author>
<affiliation confidence="0.998274">
University of Tokyo
</affiliation>
<email confidence="0.961827">
ken@tkl.iis.u-tokyo.ac.jp
{tau,chikayama}@logos.ic.i.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961851851852">
Parallel corpus is a valuable resource used
in various fields of multilingual natural
language processing. One of the most
significant problems in using parallel cor-
pora is the lack of their availability. Re-
searchers have investigated approaches to
collecting parallel texts from the Web. A
basic component of these approaches is
an algorithm that judges whether a pair
of texts is parallel or not. In this paper,
we propose an algorithm that accelerates
this task without losing accuracy by pre-
processing a bilingual dictionary as well
as the collection of texts. This method
achieved 250,000 pairs/sec throughput on
a single CPU, with the best Fl score
of 0.960 for the task of detecting 200
Japanese-English translation pairs out of
40, 000. The method is applicable to texts
of any format, and not specific to HTML
documents labeled with URLs. We report
details of these preprocessing methods and
the fast comparison algorithm. To the
best of our knowledge, this is the first re-
ported experiment of extracting Japanese–
English parallel texts from a large corpora
based solely on linguistic content.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947311111112">
“Parallel text” is a pair of texts which is written
in different languages and is a translation of each
other. A compilation of parallel texts offered in a
serviceable form is called a “parallel corpus”. Par-
allel corpora are very valuable resources in various
fields of multilingual natural language processing
such as statistical machine translation (Brown et
al., 1990), cross-lingual IR (Chen and Nie, 2000),
and construction of dictionary (Nagao, 1996).
However, it is generally difficult to obtain paral-
lel corpora of enough quantity and quality. There
have only been a few varieties of parallel corpora.
In addition, their languages have been biased to-
ward English–French and their contents toward of-
ficial documents of governmental institutions or
software manuals. Therefore, it is often difficult
to find a parallel corpus that meets the needs of
specific researches.
To solve this problem, approaches to collect
parallel texts from the Web have been proposed.
In the Web space, all sorts of languages are used
though English is dominating, and the content of
the texts seems to be as diverse as all activities of
the human-beings. Therefore, this approach has a
potential to break the limitation in the use of par-
allel corpora.
Previous works successfully built parallel cor-
pora of interesting sizes. Most of them uti-
lized URL strings or HTML tags as a clue to ef-
ficiently find parallel documents (Yang and Li,
2002; Nadeau and Foster, 2004). Depending on
such information specific to webpages limits the
applicability of the methods. Even for webpages,
many parallel texts not conforming to the presup-
posed styles will be left undetected. In this work,
we have therefore decided to focus on a generally
applicable method, which is solely based on the
textual content of the documents. The main chal-
lenge then is how to make judgements fast.
Our proposed method utilizes a bilingual dictio-
nary which, for each word in tne language, gives
the list of translations in the other. The method
preprocesses both the bilingual dictionary and the
collection of texts to make a comparison of text
pairs in a subsequent stage faster. A comparison
</bodyText>
<page confidence="0.990201">
60
</page>
<bodyText confidence="0.956428">
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 60–67,
Sydney, July 2006. c�2006 Association for Computational Linguistics
of a text pair is carried out simply by compar-
ing two streams of integers without any dictionary
or table lookup, in time linear in the sum of the
two text sizes. With this method, we achieved
250,000 pairs/sec throughput on a single Xeon
CPU (2.4GHz). The best F1 score is 0.960, for a
dataset which includes 200 true pairs out of 40,000
candidate pairs. Further comments on these num-
bers are given in Section 4.
In addition, to the best of our knowledge,
this is the first reported experiment of extracitng
Japanese–English parallel texts using a method
solely based on their linguistic contents.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999863">
There have been several attempts to collect paral-
lel texts from the Web. We will mention two con-
trasting approaches among them.
</bodyText>
<subsectionHeader confidence="0.532308">
2.1 BITS
</subsectionHeader>
<bodyText confidence="0.999891888888889">
Ma and Liberman collected English–German par-
allel webpages (Ma and Liberman, 1999). They
began with a list of websites that belong to a do-
main accosiated with German–speaking areas and
searched for parallel webpages in these sites. For
each site, they downloaded a subset of the site
to investigate what language it is written in, and
then, downloaded all pages if it was proved to be
English–German bilingual. For each pair of En-
glish and German document, they judged whether
it is a mutual translation. They made a decision
in the following manner. First, they searched a
bilingual dictionary for all English–German word
pairs in the text pair. If a word pair is found in
the dictionary, it is recognized as an evidence of
translation. Finally, they divided the number of
recognized pairs by the sum of the length of the
two texts and regard this value as a score of trans-
lationality. When this score is greater than a given
threshold, the pair is judged as a mutual transla-
tion. They succeeded in creating about 63MB par-
allel corpus with 10 machines through 20 days.
The number of webpages is considered to have
increased far more rapidly than the performance of
computers in the past seven years. Therefore, we
think it is important to reduce the cost of calcula-
tion of a system.
</bodyText>
<subsectionHeader confidence="0.985235">
2.2 STRAND
</subsectionHeader>
<bodyText confidence="0.99998696">
If we simply make a dicision for all pairs in a col-
lection of texts, the calculation takes Q(n2) com-
parisons of text pairs where n is the number of
documents in the collection. In fact, most re-
searches utilize properties peculiar to certain par-
allel webpages to reduce the number of candidate
pairs in advance. Resnik and Smith focused on the
fact that a page pair tends to be a mutual transla-
tion when their URL strings meet a certain condi-
tion, and examined only page pairs which satisfy
it (Resnik and Smith, 2003). A URL string some-
times contains a substring which indicates the lan-
guage in which the page is written. For example,
a webpage written in Japanese sometimes have a
substring such as j, jp, jpn, n, euc or sjis in
its URL. They regard a pair ofpages as a candidate
when their URLs match completely after remov-
ing such language-specific substrings and, only for
these candidates, did they make a detailed com-
parison with bilingual dictionary. They were suc-
cessful in collecting 2190 parallel pairs from 8294
candidates. However, this URL condition seems
so strict for the purpose that they found 8294 can-
didate pairs from as much as 20 Tera bytes of web-
pages.
</bodyText>
<sectionHeader confidence="0.988138" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999114">
3.1 Problem settings
</subsectionHeader>
<bodyText confidence="0.99997764">
There are several evaluation criteria for parallel
text mining algorithms. They include accuracy,
execution speed, and generality. We say an algo-
rithm is general when it can be applied to texts of
any format, not only to webpages with associated
information specific to webpages (e.g., URLs and
tags). In this paper, we focus on developing a fast
and general algorithm for determining if a pair of
texts is parallel.
In general, there are two complementary ways
to improve the speed of parallel text mining. One
is to reduce the number of “candidate pairs” to be
compared. The other is to make a single compar-
ison of two texts faster. An example of the for-
mer is Resnik and Smith’s URL matching method,
which is able to mine parallel texts from a very
large corpora of Tera bytes. However, this ap-
proach is very specific to the Web and, even if we
restrict our interest to webpages, there may be a
significant number of parallel pages whose URLs
do not match the prescribed pattern and therefore
are filtered out. Our method is in the latter cat-
egory, and is generally applicable to texts of any
format. The approach depends only on the lin-
guistic content of texts. Reducing the number of
</bodyText>
<page confidence="0.99815">
61
</page>
<figureCaption confidence="0.999918">
Figure 1: Outline of the method
</figureCaption>
<bodyText confidence="0.999329615384615">
comparisons while maintaining the generality will
be one of our future works.
The outline of the method is as follows. First
we preprocess a bilingual dictionary and build a
mapping from words to integers, which we call
“semantic ID.” Texts are then preprocessed, con-
verting each word to its corresponding semantic
ID plus its position of the occurrence. Then we
compare all pairs of texts, using their converted
representations (Figure 1). Comparing a pair of
texts is fast because it is performed in time linear
in the length of the texts and does not need any
table lookup or string manipulation.
</bodyText>
<subsectionHeader confidence="0.999914">
3.2 Preprocessing a bilingual dictionary
</subsectionHeader>
<bodyText confidence="0.999992058823529">
We take only nouns into account in our algorithm.
For the language pair of English and Japanese,
a correspondence of parts of speech of a word
and its translation is not so clear and may make
the problem more difficult. A result was actually
worse when every open-class word was considered
than when only nouns were.
The first stage of the method is to assign an in-
teger called semantic ID to every word (in both
languages) that appears in a bilingual dictionary.
The goal is to assign the same ID to a pair of words
that are translations of each other. In an ideal situa-
tion where each word of one language corresponds
one-to-one with a word of the other language, all
you need to do is to assign differnt IDs to every
translational relationship between two words. The
main purpose of this conversion is to make a com-
parison of two texts in a subsequent stage faster.
However, it’s not exactly that simple. A word
very often has more than one words as its trans-
lation so the naive method described above is not
directly applicable. We devised an approximate
solution to address this complexity. We build
a bigraph whose nodes are words in the dictio-
nary and edges translational relationships between
them. This graph consists of many small con-
nected components, each representing a group of
words that are expected to have similar meanings.
We then make a mapping from a word to its se-
mantic ID. Two words are considered translations
of each other when they have the same semantic
ID.
This method causes a side-effect of connecting
two words not directly related in the dictionary. It
has both good and bad effects. A good effect is
that it may connect two words that do not explic-
itly appear as translations in the dictionary, but are
used as translations in practice (see section 4.3).
In other words, new translational word pairs are
detected. A bad effect, on the other hand, is that it
potentially connects many words that do not share
meanings at all. Figure 2 shows an actual exam-
ple of such an undesirable component observed in
our experiment. You can go from fruit to army
through several hops and these words are treated
as identical entity in subsequent steps of our tech-
nique. Futhermore, in the most extreme case, a
very large connected component can be created.
Table 1 shows the statistics of the component sizes
for the English-Japanese dictionary we have used
in our experiment (EDR Electronic Dictionary).
</bodyText>
<page confidence="0.997142">
62
</page>
<figureCaption confidence="0.999852">
Figure 2: Example of a undesirable graph
</figureCaption>
<bodyText confidence="0.999793777777778">
Most components are fairly small (&lt; 10 words).
The largest connected component, however, con-
sisted of 3563 nodes out of the total 28001 nodes
in the entire graph and 3943 edges out of 19413.
As we will see in the next section, this had a dev-
astating effect on the quality of judgement so we
clearly need a method that circumvents the sit-
uation. One possibility is to simply drop very
large components. Another is to divide the graph
</bodyText>
<tableCaption confidence="0.406438">
into small components. We have tried both ap-
proaches.
Table 1: Statistics of the component sizes
</tableCaption>
<figure confidence="0.8976823">
# of nodes # of components
2 6629
3 1498
4 463
5 212
6 125
7 69
8 44
9 32
10∼ 106
</figure>
<bodyText confidence="0.999892730769231">
For partitioning graphs, we used a very sim-
ple greedy method. Even though a more com-
plex method may be possible that takes advan-
tages of linguistic insights, this work uses a very
simple partitioning method that only looks at the
graph structure in this work. A graph is partitioned
into two parts having an equal number of nodes
and a partition is recursively performed until each
part becomes smaller than a given threshold. The
threshold is chosen so that it yields the best result
for a training set and then applied to a test data.
For each bisection, we begin with a random par-
tition and improves it by a local greedy search.
Given the current partition, it seeks a pair of nodes
which, if swapped, maximumly reduces the num-
ber of edges crossing the two parts. Ties are bro-
ken arbitrarily when there are many such pairs. If
no single swap reduces the number of edges across
parts, we simply stop (i.e., local search). A seman-
tic ID is then given to each part.
This process would lose connections between
words that are originally translations in the dictio-
nary but are separated by the partitioning. We will
describe a method to partially recover this loss in
the end of the next section, after describing how
texts are preprocessed.
</bodyText>
<subsectionHeader confidence="0.999518">
3.3 Preprocessing texts
</subsectionHeader>
<bodyText confidence="0.999990617647059">
Each text (document) is preprocessed as follows.
Texts are segmented into words and tagged with a
part-of-speech. Inflection problems are addressed
with lemmatization. Each word is converted into
the pair (nid, pos), where nid is the semantic ID of
the partition containing the word and pos its posi-
tion of occurrence. The position is normalized and
represented as a floating point number between 0.0
and 1.0. Any word which does not appear in the
dictionary is simply ignored. The position is used
to judge if words having an equal ID occur in sim-
ilar positions in both texts, so they suggest a trans-
lation.
After converting each word, all (nid, pos) pairs
are sorted first by their semantic IDs breaking ties
with positions. This sorting takes O(n log n) time
for a document of n words. This preprocessing
needs to be performed only once for each docu-
ment.
We recover the connections between word pairs
separated by the partitioning in the following man-
ner. Suppose words J and E are translations of
each other in the dictionary, J is in a partition
whose semantic ID is x and E in another partition
whose semantic ID is y. In this case, we translate
J into two elements x and y. This result is as if
two separate words, one in component x and an-
other in y, appeared in the original text, so it may
potentially have an undesirable side-effect on the
quality of judgement. It is therefore important to
keep the number of such pairs reasonably small.
We experimented with both cases, one in which
we recover separate connections and the other in
which we don’t.
</bodyText>
<subsectionHeader confidence="0.991511">
3.4 Comparing document pairs
</subsectionHeader>
<bodyText confidence="0.999958">
We judge if a text pair is likely to be a translation
by comparing two sequences obtained by the pre-
processing. We count the number of word pairs
</bodyText>
<page confidence="0.998845">
63
</page>
<bodyText confidence="0.99996644">
that have an equal semantic ID and whose posi-
tions are within a distance threshold. The best
threshold is chosen to yield the best result for a
training set and then applied to test set. This pro-
cess takes time linear in the length of texts since
the sequences are sorted. First, we set cursors
at the first element of each of the two sequences.
When the semantic IDs of the elements under the
cursors are equal and the difference between their
positions is within a threshold, we count them as
an evidence of translationality and move both cur-
sors forward. Otherwise, the cursor on the ele-
ment which is less according to the sorting cri-
teria is moved forward. In this step, we do not
perform any further search to determine if origi-
nal words of the elements were related directly in
the bilingual dictionary giving preference to speed
over accuracy. We repeat this operation until any
of the cursors reaches the end of the sequence. Fi-
nally, we divide the number of matching elements
by the sum of the lengths of the two documents.
We define this value as “tscore,” which stands for
translational score. At least one cursor moves af-
ter each comparison, so this algorithm finishes in
time linear in the length of the texts.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.969926">
4.1 Preparation
</subsectionHeader>
<bodyText confidence="0.999925619047619">
To evaluate our method, we used The EDR Elec-
tronic Dictionary1 for a bilingual dictionary and
Fry’s Japanese-English parallel web corpus (Fry,
2005) for sample data. In this experiment, we
considered only nouns (see section 3.2) and got
a graph which consists of 28001 nodes, 19413
edges and 9178 connected components of which
the largest has 3563 nodes and 3943 edges. Large
components including it need to be partitioned.
We conducted partitioning with differnt thresh-
olds and developed various word–ID mappings.
For each mapping, we made several variations in
two respect. One is whether cut connections are
recovered or not. The other is whether and how
many numerals, which can be easily utilized to
boost the vocaburary of the dictionary, are added
to a bilingual dictionary.
The parallel corpus we used had been collected
by Fry from four news sites. Most texts in the cor-
pus are news report on computer technology and
the rest is on various fields of science. A single
</bodyText>
<footnote confidence="0.776002">
1EDR Electronic Dictionary.
http://www2.nict.go.jp/kk/e416/EDR/
</footnote>
<bodyText confidence="0.999801705882353">
document is typically 1,000–6,000 bytes. He de-
tected parallel texts based only on HTML tags and
link structures, which depend on websites, with-
out looking at textual content, so there are many
false pairs in his corpus. Therefore, to evaluate
our method precisely, we used only 400 true par-
allel pairs that are randomly selected and checked
by human inspection. We divided them evenly and
randomly into two parts and use one half for a
training set and the other for a test set. In exper-
iments described in section 4.4 and 4.5, we used
other portion of the corpus to scale experiments.
For tokenization and pos-tagging, we used
MeCab2 to Japanese texts and SS Tagger3 to En-
glish texts. Because SS Tagger doesn’t act as lem-
matizer, we used morphstr() function in Word-
Net library4.
</bodyText>
<subsectionHeader confidence="0.9104475">
4.2 Effect of large components and a
partitioning
</subsectionHeader>
<bodyText confidence="0.99947088">
Figure 3 shows the results of experiments on sev-
eral conditions. There are three groups ofbars; (A)
treat every connected component equally regard-
less of its size, (B) simply drop the largest compo-
nent and (C) divide large components into smaller
parts. In each group, the upper bar corresponds
to the case the algorithm works without a distance
threshold and the lower with it (0.2). The figures
attached to each bar are the max Fl score, which
is a popular measure to evaluate a classification al-
gorithm, and indicate how accurately a method is
able to detect 200 true text pairs from the test set
of 40,000 pairs. We didn’t recover word connec-
tions broken in the partitioning step and didn’t add
any numerals to the vocabrary of the bilingual dic-
tionary this time.
The significant difference between (A) and (B)
clearly shows the devastating effect of large com-
ponents. The difference between (B) and (C)
shows that the accurary can be further improved
if large components are partitioned into small ones
in order to utilize as much information as possible.
In addtion, the accuracy consistently improves by
using the distance threshold.
Next, we determined the best word–ID mapping
</bodyText>
<footnote confidence="0.991467625">
2MeCab: Yet Another Part-of-Speech and Morphological
Analyzer.
http://mecab.sourceforge.jp/
3SS Tagger - a part-of-speech tagger for English.
http://www-tsujii.is.s.u-tokyo.ac.jp/
˜tsuruoka/postagger/
4WordNet - a lexical database for the English language.
http://wordnet.princeton.edu/
</footnote>
<page confidence="0.998538">
64
</page>
<figureCaption confidence="0.9999825">
Figure 4: The two word-matching policy
Figure 3: Effect of the graph partitioning
</figureCaption>
<bodyText confidence="0.988461">
and distance threshold and tested its performance
through a 2–fold cross validation. The best map-
ping among those was the one which
</bodyText>
<listItem confidence="0.997330833333333">
• divides a component recursively until the
number of nodes of each language becomes
no more than 30,
• does not recover connections that are cut in
the partitioning, and
• adds numerals from 0 to 999.
</listItem>
<bodyText confidence="0.991576333333333">
The best distance threshold was 0.2, and tscore
threshold 0.102. We tested this rule and thresholds
on the test set. The result was Fl = 0.960.
</bodyText>
<subsectionHeader confidence="0.9988">
4.3 Effect of false translation pairs
</subsectionHeader>
<bodyText confidence="0.999972375">
Our method of matching words differs from Ma
and Liberman’s one. While they only count word
pairs that directly appear in a bilingual dictionary,
we identify all words having the same seman-
tic ID. Potential merits and drawbacks to accu-
racy have been described in the section 3.2. We
compared the accuracy of the two algorithms to
investigate the effect of our approximate match-
ing. To this end, we implemented Ma and Liber-
man’s method with all other conditions and in-
put data being equal to the one in the last sec-
tion. We got max Fl = 0.933 as a result, which
is slightly worse than the figure reported in their
paper. Though it is difficult to conclude where
the difference stems from, there are several fac-
tors worth pointing out. First, our experiment is
done for English-Japanese, while Ma and Liber-
man’s experiment for English-German, which are
more similar than English and Japanese are. Sec-
ond, their data set contains much more true pairs
(240 out of 300) than our data set does (200 out of
40,000).
This number is also worse than that of our ex-
periment (Figure 4). This shows that, at least in
the experiment, our approach of identifying more
pairs than the original dictionary causes more
good effects than bad in total. We looked at word
pairs which are not matched in Ma and Liberman’s
method but in ours. While most of the pairs can be
hardly considered as a strict translation, some of
them are pairs practically used as translations. Ex-
amples of such pairs are shown in Figure 5.
</bodyText>
<figureCaption confidence="0.944122">
Figure 5: Word pairs not in the dictionary
</figureCaption>
<subsectionHeader confidence="0.995879">
4.4 Execution Speed
</subsectionHeader>
<bodyText confidence="0.999952533333334">
We have argued that the execution speed is a major
advantage of our method. We achieved 250,000
pairs/sec throughput on single Xeon (2.4GHz)
processor. It’s difficult to make a fair com-
parison of the execution speed because Ma and
Liberman’s paper does not describe enough de-
tails about their experimants other than processing
3145 websites with 10 sparc stations for 10 days.
Just for a rough estimate, we introduce some bold
assumptions. Say, there were a thousand pages for
each language in a website or, in other words, a
million page pairs, and the performance of proces-
sors has grown by 32 times in the past seven years,
our method works more than 40 times faster than
Ma and Liberman’s one. This difference seems
</bodyText>
<page confidence="0.999294">
65
</page>
<figureCaption confidence="0.999912">
Figure 6: A example of false–positive text pairs
</figureCaption>
<bodyText confidence="0.999918">
to be caused by a difference of the complexity
between the two algorithms. To the extent writ-
ten in their paper, Ma and Liberman calculated a
score of translationality by enumerating all com-
binations of two words within a distance threshold
and search a bilingual dictionary for each combi-
nation of words. This algorithm takes Q(n2) time
where n is the length of a text, while our method
takes O(n) time. In addition, our method doesn’t
need any string manipulation in the comparison
step.
</bodyText>
<subsectionHeader confidence="0.999341">
4.5 Analysis of miss detections
</subsectionHeader>
<bodyText confidence="0.99973625">
We analyzed text pairs for which judgements dif-
fer between Fry’s and ours.
Among pairs Fry determined as a translation,
we examined the 10 pairs ranked highest in our
algorithm. Two of them are in fact translations,
which were not detected by Fry’s method with-
out any linguistic information. The rest eight pairs
are not translations. Three of the eight pairs are
about bioscience, and a word “cell” occurred many
time (Figure 6). When words with an identical
semantic ID appear repeatedly in two texts being
compared, their distances are likely to be within a
distance threshold and the pair gets unreasonably
high tscore. Therefore, if we take the number of
each semantic ID in a text into account, we might
be able to improve the accuracy.
We performed the same examination on the 10
pairs ranked lowest among those Fry determined
not to be a translation. But no interesting feature
could be found at the moment.
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="conclusions">
5 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999989620689655">
In this paper, we proposed a fast and accurate
method for detecting parallel texts from a col-
lection. This method consists of major three
parts; preprocess a bilingual dictionary into word–
ID conversion rule, convert texts into ID se-
quences, compare sequences. With this method,
we achieved 250,000 pairs/sec on a single CPU
and best F1 score of 0.960. In addition, this
method utilizes only linguistic information of a
textual content so that it is generally applicable.
This means it can detect parallel documents in any
format. Furthermore, our method is independent
on languages in essence. It can be applied to any
pair of languages if a bilingual dictionary between
the languages are available (a general language
dictionary suffices.)
Our future study will include improving both
accuracy and speed while retaining the generail-
ity. For accuracy, as we described in Section 4.5,
tscore tends to increase when an identical semantic
ID appears many times in a text. We might be able
to deal with this problem by taking into account
the probability that the distance between words is
within a threshold. Large connected components
were partitioned by a very simple method at the
present work. More involved partitioning meth-
ods may improve the accuracy of the judgement.
For speed, reducing the number of comparisons is
the most important issue that needs be addressed.
</bodyText>
<page confidence="0.985583">
66
</page>
<sectionHeader confidence="0.99823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999740967741936">
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Comput. Linguist., 16(2):79–85.
Jiang Chen and Jian-Yun Nie. 2000. Automatic
construction of parallel english-chinese corpus for
cross-language information retrieval. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing, pages 21–28, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
John Fry. 2005. Assembling a parallel corpus from
RSS news feeds. In Workshop on Example-Based
Machine Translation, MT Summit X, Phuket, Thai-
land, September.
Xiaoyi Ma and Mark Liberman. 1999. BITS: A
method for bilingual text search over the web. In
Machine Translation Summit VII, September.
David Nadeau and George Foster. 2004. Real-time
identification of parallel texts from bilingual news-
feed. In Computational Linguistic in the North-East
(CLiNE 2004), pages 21–28.
Makoto Nagao, editor. 1996. Natural Language Pro-
cessing. Number 15 in Iwanami Software Science.
Iwanami Shoten. In Japanese.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Comput. Linguist., 29(3):349–380.
C. C. Yang and K. W. Li. 2002. Mining English/
Chinese parallel documents from the World Wide
Web. In Proceedings of the International World
Wide Web Conference, Honolulu, Hawaii, May.
</reference>
<page confidence="0.999505">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951591">
<title confidence="0.993494">A fast and accurate method for detecting English-Japanese parallel texts</title>
<author confidence="0.980525">Ken’ichi Fukushima</author>
<author confidence="0.980525">Kenjiro Taura</author>
<author confidence="0.980525">Takashi</author>
<affiliation confidence="0.992601">University of</affiliation>
<abstract confidence="0.999082892857143">Parallel corpus is a valuable resource used in various fields of multilingual natural language processing. One of the most significant problems in using parallel corpora is the lack of their availability. Researchers have investigated approaches to collecting parallel texts from the Web. A basic component of these approaches is an algorithm that judges whether a pair of texts is parallel or not. In this paper, we propose an algorithm that accelerates this task without losing accuracy by preprocessing a bilingual dictionary as well as the collection of texts. This method achieved 250,000 pairs/sec throughput on single CPU, with the best the task of detecting 200 Japanese-English translation pairs out of The method is applicable to texts of any format, and not specific to HTML documents labeled with URLs. We report details of these preprocessing methods and the fast comparison algorithm. To the best of our knowledge, this is the first reported experiment of extracting Japanese– English parallel texts from a large corpora based solely on linguistic content.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1715" citStr="Brown et al., 1990" startWordPosition="259" endWordPosition="262">ese preprocessing methods and the fast comparison algorithm. To the best of our knowledge, this is the first reported experiment of extracting Japanese– English parallel texts from a large corpora based solely on linguistic content. 1 Introduction “Parallel text” is a pair of texts which is written in different languages and is a translation of each other. A compilation of parallel texts offered in a serviceable form is called a “parallel corpus”. Parallel corpora are very valuable resources in various fields of multilingual natural language processing such as statistical machine translation (Brown et al., 1990), cross-lingual IR (Chen and Nie, 2000), and construction of dictionary (Nagao, 1996). However, it is generally difficult to obtain parallel corpora of enough quantity and quality. There have only been a few varieties of parallel corpora. In addition, their languages have been biased toward English–French and their contents toward official documents of governmental institutions or software manuals. Therefore, it is often difficult to find a parallel corpus that meets the needs of specific researches. To solve this problem, approaches to collect parallel texts from the Web have been proposed. I</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Comput. Linguist., 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Chen</author>
<author>Jian-Yun Nie</author>
</authors>
<title>Automatic construction of parallel english-chinese corpus for cross-language information retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>21--28</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1754" citStr="Chen and Nie, 2000" startWordPosition="265" endWordPosition="268">comparison algorithm. To the best of our knowledge, this is the first reported experiment of extracting Japanese– English parallel texts from a large corpora based solely on linguistic content. 1 Introduction “Parallel text” is a pair of texts which is written in different languages and is a translation of each other. A compilation of parallel texts offered in a serviceable form is called a “parallel corpus”. Parallel corpora are very valuable resources in various fields of multilingual natural language processing such as statistical machine translation (Brown et al., 1990), cross-lingual IR (Chen and Nie, 2000), and construction of dictionary (Nagao, 1996). However, it is generally difficult to obtain parallel corpora of enough quantity and quality. There have only been a few varieties of parallel corpora. In addition, their languages have been biased toward English–French and their contents toward official documents of governmental institutions or software manuals. Therefore, it is often difficult to find a parallel corpus that meets the needs of specific researches. To solve this problem, approaches to collect parallel texts from the Web have been proposed. In the Web space, all sorts of languages</context>
</contexts>
<marker>Chen, Nie, 2000</marker>
<rawString>Jiang Chen and Jian-Yun Nie. 2000. Automatic construction of parallel english-chinese corpus for cross-language information retrieval. In Proceedings of the sixth conference on Applied natural language processing, pages 21–28, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Fry</author>
</authors>
<title>Assembling a parallel corpus from RSS news feeds.</title>
<date>2005</date>
<booktitle>In Workshop on Example-Based Machine Translation, MT Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="16272" citStr="Fry, 2005" startWordPosition="2800" endWordPosition="2801">ngual dictionary giving preference to speed over accuracy. We repeat this operation until any of the cursors reaches the end of the sequence. Finally, we divide the number of matching elements by the sum of the lengths of the two documents. We define this value as “tscore,” which stands for translational score. At least one cursor moves after each comparison, so this algorithm finishes in time linear in the length of the texts. 4 Experiments 4.1 Preparation To evaluate our method, we used The EDR Electronic Dictionary1 for a bilingual dictionary and Fry’s Japanese-English parallel web corpus (Fry, 2005) for sample data. In this experiment, we considered only nouns (see section 3.2) and got a graph which consists of 28001 nodes, 19413 edges and 9178 connected components of which the largest has 3563 nodes and 3943 edges. Large components including it need to be partitioned. We conducted partitioning with differnt thresholds and developed various word–ID mappings. For each mapping, we made several variations in two respect. One is whether cut connections are recovered or not. The other is whether and how many numerals, which can be easily utilized to boost the vocaburary of the dictionary, are</context>
</contexts>
<marker>Fry, 2005</marker>
<rawString>John Fry. 2005. Assembling a parallel corpus from RSS news feeds. In Workshop on Example-Based Machine Translation, MT Summit X, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
<author>Mark Liberman</author>
</authors>
<title>BITS: A method for bilingual text search over the web. In</title>
<date>1999</date>
<booktitle>Machine Translation Summit VII,</booktitle>
<contexts>
<context position="4493" citStr="Ma and Liberman, 1999" startWordPosition="715" endWordPosition="718">throughput on a single Xeon CPU (2.4GHz). The best F1 score is 0.960, for a dataset which includes 200 true pairs out of 40,000 candidate pairs. Further comments on these numbers are given in Section 4. In addition, to the best of our knowledge, this is the first reported experiment of extracitng Japanese–English parallel texts using a method solely based on their linguistic contents. 2 Related Work There have been several attempts to collect parallel texts from the Web. We will mention two contrasting approaches among them. 2.1 BITS Ma and Liberman collected English–German parallel webpages (Ma and Liberman, 1999). They began with a list of websites that belong to a domain accosiated with German–speaking areas and searched for parallel webpages in these sites. For each site, they downloaded a subset of the site to investigate what language it is written in, and then, downloaded all pages if it was proved to be English–German bilingual. For each pair of English and German document, they judged whether it is a mutual translation. They made a decision in the following manner. First, they searched a bilingual dictionary for all English–German word pairs in the text pair. If a word pair is found in the dict</context>
</contexts>
<marker>Ma, Liberman, 1999</marker>
<rawString>Xiaoyi Ma and Mark Liberman. 1999. BITS: A method for bilingual text search over the web. In Machine Translation Summit VII, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>George Foster</author>
</authors>
<title>Real-time identification of parallel texts from bilingual newsfeed.</title>
<date>2004</date>
<booktitle>In Computational Linguistic in the North-East (CLiNE 2004),</booktitle>
<pages>21--28</pages>
<contexts>
<context position="2795" citStr="Nadeau and Foster, 2004" startWordPosition="436" endWordPosition="439">rpus that meets the needs of specific researches. To solve this problem, approaches to collect parallel texts from the Web have been proposed. In the Web space, all sorts of languages are used though English is dominating, and the content of the texts seems to be as diverse as all activities of the human-beings. Therefore, this approach has a potential to break the limitation in the use of parallel corpora. Previous works successfully built parallel corpora of interesting sizes. Most of them utilized URL strings or HTML tags as a clue to efficiently find parallel documents (Yang and Li, 2002; Nadeau and Foster, 2004). Depending on such information specific to webpages limits the applicability of the methods. Even for webpages, many parallel texts not conforming to the presupposed styles will be left undetected. In this work, we have therefore decided to focus on a generally applicable method, which is solely based on the textual content of the documents. The main challenge then is how to make judgements fast. Our proposed method utilizes a bilingual dictionary which, for each word in tne language, gives the list of translations in the other. The method preprocesses both the bilingual dictionary and the co</context>
</contexts>
<marker>Nadeau, Foster, 2004</marker>
<rawString>David Nadeau and George Foster. 2004. Real-time identification of parallel texts from bilingual newsfeed. In Computational Linguistic in the North-East (CLiNE 2004), pages 21–28.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Natural Language Processing. Number 15 in Iwanami Software Science. Iwanami Shoten. In Japanese.</booktitle>
<editor>Makoto Nagao, editor.</editor>
<marker>1996</marker>
<rawString>Makoto Nagao, editor. 1996. Natural Language Processing. Number 15 in Iwanami Software Science. Iwanami Shoten. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="6223" citStr="Resnik and Smith, 2003" startWordPosition="1024" endWordPosition="1027">even years. Therefore, we think it is important to reduce the cost of calculation of a system. 2.2 STRAND If we simply make a dicision for all pairs in a collection of texts, the calculation takes Q(n2) comparisons of text pairs where n is the number of documents in the collection. In fact, most researches utilize properties peculiar to certain parallel webpages to reduce the number of candidate pairs in advance. Resnik and Smith focused on the fact that a page pair tends to be a mutual translation when their URL strings meet a certain condition, and examined only page pairs which satisfy it (Resnik and Smith, 2003). A URL string sometimes contains a substring which indicates the language in which the page is written. For example, a webpage written in Japanese sometimes have a substring such as j, jp, jpn, n, euc or sjis in its URL. They regard a pair ofpages as a candidate when their URLs match completely after removing such language-specific substrings and, only for these candidates, did they make a detailed comparison with bilingual dictionary. They were successful in collecting 2190 parallel pairs from 8294 candidates. However, this URL condition seems so strict for the purpose that they found 8294 c</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Comput. Linguist., 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Yang</author>
<author>K W Li</author>
</authors>
<title>Mining English/ Chinese parallel documents from the World Wide Web.</title>
<date>2002</date>
<booktitle>In Proceedings of the International World Wide Web Conference,</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2769" citStr="Yang and Li, 2002" startWordPosition="432" endWordPosition="435"> find a parallel corpus that meets the needs of specific researches. To solve this problem, approaches to collect parallel texts from the Web have been proposed. In the Web space, all sorts of languages are used though English is dominating, and the content of the texts seems to be as diverse as all activities of the human-beings. Therefore, this approach has a potential to break the limitation in the use of parallel corpora. Previous works successfully built parallel corpora of interesting sizes. Most of them utilized URL strings or HTML tags as a clue to efficiently find parallel documents (Yang and Li, 2002; Nadeau and Foster, 2004). Depending on such information specific to webpages limits the applicability of the methods. Even for webpages, many parallel texts not conforming to the presupposed styles will be left undetected. In this work, we have therefore decided to focus on a generally applicable method, which is solely based on the textual content of the documents. The main challenge then is how to make judgements fast. Our proposed method utilizes a bilingual dictionary which, for each word in tne language, gives the list of translations in the other. The method preprocesses both the bilin</context>
</contexts>
<marker>Yang, Li, 2002</marker>
<rawString>C. C. Yang and K. W. Li. 2002. Mining English/ Chinese parallel documents from the World Wide Web. In Proceedings of the International World Wide Web Conference, Honolulu, Hawaii, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>