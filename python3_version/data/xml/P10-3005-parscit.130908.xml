<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014549">
<title confidence="0.979561">
Sentiment Translation through Lexicon Induction
</title>
<author confidence="0.997558">
Christian Scheible
</author>
<affiliation confidence="0.9980965">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.969556">
scheibcn@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.997109" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999740111111111">
The translation of sentiment information
is a task from which sentiment analy-
sis systems can benefit. We present a
novel, graph-based approach using Sim-
Rank, a well-established vertex similar-
ity algorithm to transfer sentiment infor-
mation between a source language and a
target language graph. We evaluate this
method in comparison with SO-PMI.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999877115384615">
Sentiment analysis is an important topic in compu-
tational linguistics that is of theoretical interest but
also implies many real-world applications. Usu-
ally, two aspects are of importance in sentiment
analysis. The first is the detection of subjectivity,
i.e. whether a text or an expression is meant to ex-
press sentiment at all; the second is the determina-
tion of sentiment orientation, i.e. what sentiment
is to be expressed in a structure that is considered
subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between languages. It is
built around an algorithm that has been success-
fully applied for the acquisition of bilingual lexi-
cons. One of the main benefits of the method is its
ability of handling sparse data well.
Our experiments are carried out using English
as a source language and German as a target lan-
guage.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999378153846154">
The translation of sentiment information has been
the topic of multiple publications.
Mihalcea et al. (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations, e.g., because irony is not
recognized.
Banea et al. (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classifier
using co-training. In co-training, one classifier
produces additional training data for a second clas-
sifier. In this case, an English classifier assists in
training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by (Hatzivassiloglou and McK-
eown, 1997). They construct graphs from coor-
dination data from large corpora based on the in-
tuition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster. The
authors note that some words cannot be assigned a
unique label since their sentiment depends on con-
</bodyText>
<page confidence="0.981164">
25
</page>
<note confidence="0.6025835">
Proceedings of the ACL 2010 Student Research Workshop, pages 25–30,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.994445888888889">
text.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure He assumes that
the sentiment orientation of a phrase can be deter-
mined by comparing its pointwise mutual infor-
mation with a positive (excellent) and a negative
phrase (poor). An introduction to SO-PMI is given
in Section 5.1
</bodyText>
<sectionHeader confidence="0.968366" genericHeader="method">
3 Bilingual Lexicon Induction
</sectionHeader>
<bodyText confidence="0.999981875">
Typical approaches to the induction of bilingual
lexicons involve gathering new information from
a small set of known identities between the lan-
guages which is called a seed lexicon and incor-
porating intralingual sources of information (e.g.
cooccurrence counts). Two examples of such
methods are a graph-based approach by Dorow et
al. (2009) and a vector-space based approach by
Rapp (1999). In this paper, we will employ the
graph-based method.
SimRank was first introduced by Jeh and
Widom (2002). It is an iterative algorithm that
measures the similarity between all vertices in a
graph. In SimRank, two nodes are similar if their
neighbors are similar. This defines a recursive pro-
cess that ends when the two nodes compared are
identical. As proposed by Dorow et al. (2009), we
will apply it to a graph G in which vertices repre-
sent words and edges represent relations between
words. SimRank will then yield similarity values
between vertices that indicate the degree of relat-
edness between them with regard to the property
encoded through the edges. For two nodes i and
j in G, similarity according to SimRank is defined
as
where N(x) is the neighborhood of x and c is
a weight factor that determines the influence of
neighbors that are farther away. The initial con-
dition for the recursion is sim(i, i) = 1.
Dorow et al. (2009) further propose the applica-
tion of the SimRank algorithm for the calculation
of similarities between a source graph S and a tar-
get graph T . Initially, some relations between the
two graphs need to be known. When operating on
word graphs, these can be taken from a bilingual
lexicon. This provides us with a framework for
the induction of a bilingual lexicon which can be
constructed based on the obtained similarity val-
ues between the vertices of the two graphs.
One problem of SimRank observed in experi-
ments by Laws et al. (2010) was that while words
with high similarity were semantically related,
they often were not exact translations of each
other but instead often fell into the categories of
hyponymy, hypernomy, holonymy, or meronymy.
However, this makes the similarity values appli-
cable for the translation of sentiment since it is a
property that does not depend on exact synonymy.
</bodyText>
<sectionHeader confidence="0.865161" genericHeader="method">
4 Sentiment Transfer
</sectionHeader>
<bodyText confidence="0.999763928571429">
Although unsupervised methods for the design of
sentiment analysis systems exist, any approach
can benefit from using resources that have been
established in other languages. The main problem
that we aim to deal with in this paper is the trans-
fer of such information between languages. The
SimRank lexicon induction method is suitable for
this purpose since it can produce useful similarity
values even with a small seed lexicon.
First, we build a graph for each language. The
vertices of these graphs will represent adjectives
while the edges are coordination relations between
these adjectives. An example for such a graph is
given in Figure 1.
</bodyText>
<figureCaption confidence="0.852169">
Figure 1: Sample graph showing English coordi-
nation relations.
</figureCaption>
<bodyText confidence="0.9998904">
The use of coordination information has been
shown to be beneficial for example in early work
by Hatzivassiloglou and McKeown (1997).
Seed links between those graphs will be taken
from a universal dictionary. Figure 2 shows an ex-
ample graph. Here, intralingual coordination rela-
tions are represented as black lines, seed relations
as solid grey lines, and relations that are induced
through SimRank as dashed grey lines.
After computing similarities in this graph, we
</bodyText>
<equation confidence="0.981832333333333">
�
sim(k, l),
|N(i)||N(j)
c
sim(i, j) =
k∈N(i),l∈N(j)
</equation>
<page confidence="0.990139">
26
</page>
<figureCaption confidence="0.9917645">
Figure 2: Sample graph showing English and German coordination relations. Solid black lines represent
coordinations, solid grey lines represent seed relations, and dashed grey lines show induced relations.
</figureCaption>
<bodyText confidence="0.85643">
need to obtain sentiment values. We will define
the sentiment score (sent) as
</bodyText>
<equation confidence="0.982285333333333">
�sent(nt) = simnorm(ns, nt) sent(ns),
nsES
simnorm(ns, nt) = EnsES sim(ns, nt).
</equation>
<bodyText confidence="0.999896142857143">
Normalization guarantees that all sentiment
scores lie within a specified range. Scores are not
a direct indicator for orientation since the similar-
ities still include a lot of noise. Therefore, we
interpret the scores by assigning each word to a
category by finding score thresholds between the
categories.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998399">
5.1 Baseline Method (SO-PMI)
</subsectionHeader>
<bodyText confidence="0.999755">
We will compare our method to the well-
established SO-PMI algorithm by Turney (2002)
to show an improvement over an unsupervised
method. The algorithm works with cooccurrence
counts on large corpora. To determine the seman-
tic orientation of a word w, the hits near positive
(Pwords) and negative (Nwords) seed words is
used. The SO-PMI equation is given as
</bodyText>
<equation confidence="0.5587166">
SO-PMI(word) =
HpwordEPwords hits(word NEAR pword)
7 7 HnwordENwords hits(word NEAR nword)
× HnwordENwords hits(nword)TT
\
</equation>
<subsectionHeader confidence="0.993315">
5.2 Data Acquisition
</subsectionHeader>
<bodyText confidence="0.999971514285714">
We used the English and German Wikipedia
branches as our corpora. We extracted coor-
dinations from the corpus using a simple CQP
pattern search (Christ et al., 1999). For our ex-
periments, we looked only at coordinations with
and. For the English corpus, we used the pattern
[pos = &amp;quot;JJ&amp;quot;] ([pos = &amp;quot;,&amp;quot;] [pos =
&amp;quot;JJ&amp;quot;])*([pos = &amp;quot;,&amp;quot;]? &amp;quot;and&amp;quot; [pos
= &amp;quot;JJ&amp;quot;])+, and for the German corpus, the
pattern [pos = &amp;quot;ADJ.*&amp;quot;] ([pos = &amp;quot;,&amp;quot;]
[pos = &amp;quot;ADJ.*&amp;quot;])* (&amp;quot;und&amp;quot; [pos =
&amp;quot;ADJ&amp;quot;])+ was used. This yielded 477,291 pairs
of coordinated English adjectives and 44,245
German pairs. We used the dict.cc dictionary1 as
a seed dictionary. It contained a total of 30,551
adjectives.
After building a graph out of this data as de-
scribed in Section 4, we apply the SimRank algo-
rithm using 7 iterations.
Data for the SO-PMI method had to be col-
lected from queries to search engines since the in-
formation available in the Wikipedia corpus was
too sparse. Since Google does not provide a sta-
ble NEAR operator, we used coordinations instead.
For each of the test words w and the SO-PMI seed
words s we made two queries +&amp;quot;w und s&amp;quot; and
+&amp;quot;s und w&amp;quot; to Google. The quotes and + were
added to ensure that no spelling correction or syn-
onym replacements took place. Since the original
experiments were designed for an English corpus,
a set of German seed words had to be constructed.
We chose gut, nett, richtig, sch¨on, ordentlich, an-
genehm, aufrichtig, gewissenhaft, and hervorra-
gend as positive seeds, and schlecht, teuer, falsch,
b¨ose, feindlich, verhasst, widerlich, fehlerhaft, and
</bodyText>
<footnote confidence="0.945373">
1http://www.dict.cc/
</footnote>
<bodyText confidence="0.9977376">
where nt is a node in the target graph T, and S
the source graph. This way, the sentiment score
of each node is an average over all nodes in S
weighted by their normalized similarity, simnorm.
We define the normalized similarity as
</bodyText>
<equation confidence="0.830063666666667">
sim(ns, nt)
(log2
llpwordEPwords hits(pword)
</equation>
<page confidence="0.986474">
27
</page>
<table confidence="0.999399166666667">
word value
strongpos 1.0
weakpos 0.5
neutral 0.0
weakneg −0.5
strongneg −1.0
</table>
<tableCaption confidence="0.999897">
Table 1: Assigned values for positivity labels
</tableCaption>
<bodyText confidence="0.999378555555555">
mangelhaft as negative seeds.
We constructed a test set by randomly selecting
200 German adjectives that occurred in a coordi-
nation in Wikipedia. We then eliminated adjec-
tives that we deemed uncommon or too difficult to
understand or that were mislabeled as adjectives.
This resulted in a 150 word test set. To deter-
mine the sentiment of these adjectives, we asked
9 human judges, all native German speakers, to
annotate them given the classes neutral, slightly
negative, very negative, slightly positive, and very
positive, reflecting the categories from the train-
ing data. In the annotation process, another 7 ad-
jectives had to be discarded because one or more
annotators marked them as unknown.
Since human judges tend to interpret scales
differently, we examine their agreement using
Kendall’s coefficient of concordance (W) includ-
ing correction for ties (Legendre, 2005) which
takes ranks into account. The agreement was cal-
culated as W = 0.674 with a significant confi-
dence (p &lt; .001), which is usually interpreted as
substantial agreement. Manual examination of the
data showed that most disagreement between the
annotators occurred with adjectives that are tied
to political implications, for example nuklear (nu-
clear).
</bodyText>
<subsectionHeader confidence="0.999186">
5.3 Sentiment Lexicon Induction
</subsectionHeader>
<bodyText confidence="0.999981777777778">
For our experiments, we used the polarity lexi-
con of Wilson et al. (2005). It includes annota-
tions of positivity in the form of the categories
neutral, weakly positive (weakpos), strongly posi-
tive (strongpos), weakly negative (weakneg), and
strongly positive (strongneg). In order to con-
duct arithmetic operations on these annotations,
mapped them to values from the interval [−1, 1]
by using the assignments given in Table 1.
</bodyText>
<sectionHeader confidence="0.8312" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.999939114285714">
To compare the two methods to the human raters,
we first reproduce the evaluation by Turney (2002)
and examine the correlation coefficients. Both
methods will be compared to an average over the
human rater values. These values are calculated
on values asserted based on Table 1. The corre-
lation coefficients between the automatic systems
and the human ratings, SO-PMI yields r = 0.551,
and SimRank yields r = 0.587 which are not sig-
nificantly different. This shows that SO and SR
have about the same performance on this broad
measure.
Since many adjectives do not express sentiment
at all, the correct categorization of neutral adjec-
tives is as important as the scalar rating. Thus,
we divide the adjectives into three categories –
positive, neutral, and negative. Due to disagree-
ments between the human judges there exists no
clear threshold between these categories. In order
to try different thresholds, we assume that senti-
ment is symmetrically distributed with mean 0 on
the human scores. For x E 1i2010 &lt; i &lt; 191, we
then assign word w with human rating score(w)
to negative if score(w) &lt; −x, to neutral if −x &lt;
score(w) &lt; x and to positive otherwise. This
gives us a three-category gold standard for each
x that is then the basis for computing evaluation
measures. Each category contains a certain per-
centile of the list of adjectives. By mapping these
percentiles to the rank-ordered scores for SO-PMI
and SimRank, we can create three-category par-
titions for them. For example if for x = 0.35
21% of the adjectives are negative, then the 21%
of adjectives with the lowest SO-PMI scores are
deemed to have been rated negative by SO-PMI.
</bodyText>
<figureCaption confidence="0.997259">
Figure 3: Macro- and micro-averaged Accuracy
</figureCaption>
<bodyText confidence="0.99783">
First, we will look at the macro- and micro-
averaged accuracies for both methods (cf. Fig-
ure 3). Overall, SimRank performs better for x
</bodyText>
<figure confidence="0.998171769230769">
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
x
Accuracy
0.8
0.6
0.4
0.2
0
1
SO-PMI (macro)
SimRank (macro)
SO-PMI (micro)
SimRank (micro)
</figure>
<page confidence="0.998405">
28
</page>
<bodyText confidence="0.9996912">
between 0.05 and 0.4 which is a plausible inter-
val for the neutral threshold on the human ratings.
The results diverge for very low and high values
of x, however these values can be considered un-
realistic since they implicate neutral areas that are
too small or too large. When comparing the ac-
curacies for each of the classes (cf. Figure 4), we
observe that in the aforementioned interval, Sim-
Rank has higher accuracy values than SO-PMI for
all of them.
</bodyText>
<figureCaption confidence="0.998303">
Figure 4: Accuracy for individual classes
</figureCaption>
<bodyText confidence="0.99890536">
Table 2 lists some interesting example words in-
cluding their human ratings and SO-PMI and Sim-
Rank scores which illustrate advantages and pos-
sible shortcomings of the two methods. The medi-
ans of SO-PMI and SimRank scores are −15.58
and −0.05, respectively. The mean values are
−9.57 for SO-PMI and 0.08 for SimRank, the
standard deviations are 13.75 and 0.22. SimRank
values range between −0.67 and 0.41, SO-PMI
ranges between −46.21 and 46.59. We will as-
sume that the medians mark the center of the set
of neutral adjectives.
Ausdrucksvoll receives a positive score from
SO-PMI which matches the human rating, how-
ever not from SimRank, which assigns a score
close to 0 and would likely be considered neutral.
This error can be explained by examining the sim-
ilarity distribution for ausdrucksvoll which reveals
that there are no nodes that are similar to this node,
which was most likely caused by its low degree.
Auferstanden (resurrected) is perceived as a posi-
tive adjective by the human judges, however it is
misclassified by SimRank as negative due to its
occurrence with words like gestorben (deceased)
and gekreuzigt (crucified) which have negative as-
</bodyText>
<table confidence="0.996314">
word (translation) SR SO judges
ausdrucksvoll (expressive) 0.069 22.93 0.39
grafisch (graphic) -0.050 -4.75 0.00
kriminell (criminal) -0.389 -15.98 -0.94
auferstanden (resurrected) -0.338 -10.97 0.34
</table>
<tableCaption confidence="0.7885655">
Table 2: Example adjectives including translation,
and their scores
</tableCaption>
<bodyText confidence="0.9958382">
sociations. This suggests that coordinations are
sometimes misleading and should not be used as
the only data source. Grafisch (graphics-related)
is an example for a neutral word misclassified by
SO-PMI due to its occurrence in positive contexts
on the web. Since SimRank is not restricted to re-
lations between an adjective and a seed word, all
adjective-adjective coordinations are used for the
estimation of a sentiment score. Kriminell is also
misclassified by SO-PMI for the same reason.
</bodyText>
<sectionHeader confidence="0.9886" genericHeader="conclusions">
6 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.999972375">
We presented a novel approach to the translation
of sentiment information that outperforms SO-
PMI, an established method. In particular, we
could show that SimRank outperforms SO-PMI
for values of the threshold x in an interval that
most likely leads to the correct separation of pos-
itive, neutral, and negative adjectives. We intend
to compare our system to other available work in
the future. In addition to our findings, we created
an initial gold standard set of sentiment-annotated
German adjectives that will be publicly available.
The two methods are very different in nature;
while SO-PMI is suitable for languages in which
very large corpora exist, this might not be the
case for knowledge-sparse languages. For some
German words (e.g. schwerstkrank (seriously
ill)), SO-PMI lacked sufficient results on the web
whereas SimRank correctly assigned negative sen-
timent. SimRank can leverage knowledge from
neighbor words to circumvent this problem. In
turn, this information can turn out to be mislead-
ing (cf. auferstanden). An advantage of our
method is that it uses existing resources from an-
other language and can thus be applied without
much knowledge about the target language. Our
future work will include a further examination of
the merits of its application for knowledge-sparse
languages.
The underlying graph structure provides a foun-
dation for many conceivable extensions. In this
paper, we presented a fairly simple experiment re-
stricted to adjectives only. However, the method
</bodyText>
<figure confidence="0.999757">
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
x
Accuracy
0.8
0.6
0.4
0.2
0
1
positive (SO-PMI)
positive (SimRank)
neutral (SO-PMI)
neutral (SimRank)
negative (SO-PMI)
negative (SimRank)
</figure>
<page confidence="0.995972">
29
</page>
<bodyText confidence="0.999968166666667">
is suitable to include arbitrary parts of speech as
well as phrases, as used by Turney (2002). An-
other conceivable application would be the direct
combination of the SimRank-based model with a
statistical model.
Currently, our input sentiment list exists only of
prior sentiment values, however work by Wilson
et al. (2009) has advanced the notion of contextual
polarity lists. The automatic translation of this in-
formation could be beneficial for sentiment analy-
sis in other languages.
Another important problem in sentiment anal-
ysis is the treatment of ambiguity. The senti-
ment expressed by a word or phrase is context-
dependent and is for example related to word sense
(Akkaya et al., 2009). Based on regularities in
graph structure and similarity, ambiguity resolu-
tion might become possible.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999937731707317">
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Sub-
jectivity Word Sense Disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 190–199.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 127–135, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
O. Christ, B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User’s Manual. University of
Stuttgart, March, 8:1999.
Beate Dorow, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 91–95, Athens, Greece, March. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the Association for Computational Linguistics,
pages 174–181, Madrid, Spain, July. Association for
Computational Linguistics.
Glen Jeh and Jennifer Widom. 2002. Simrank: a mea-
sure of structural-context similarity. In KDD ’02:
Proceedings of the eighth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 538–543, New York, NY, USA. ACM.
F. Laws, L. Michelbacher, B. Dorow, U. Heid, and
H. Sch¨utze. 2010. Building a Cross-lingual Re-
latedness Thesaurus Using a Graph Similarity Mea-
sure. Submitted on Nov 7, 2009, to the International
Conference on Language Resources and Evaluation
(LREC).
P. Legendre. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226–245.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 976–983, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519–526, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417–424, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235–
243, Suntec, Singapore, August. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 347–354, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an Explo-
ration of Features for Phrase-level Sentiment Analy-
sis. Computational Linguistics, 35(3):399–433.
</reference>
<page confidence="0.998813">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966538">
<title confidence="0.999955">Sentiment Translation through Lexicon Induction</title>
<author confidence="0.999985">Christian Scheible</author>
<affiliation confidence="0.99845">Institute for Natural Language Processing University of Stuttgart</affiliation>
<email confidence="0.987945">scheibcn@ims.uni-stuttgart.de</email>
<abstract confidence="0.9981194">The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using Sim- Rank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Akkaya</author>
<author>J Wiebe</author>
<author>R Mihalcea</author>
</authors>
<title>Subjectivity Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>190--199</pages>
<marker>Akkaya, Wiebe, Mihalcea, 2009</marker>
<rawString>C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjectivity Word Sense Disambiguation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>127--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2359" citStr="Banea et al. (2008)" startWordPosition="361" endWordPosition="364">has been the topic of multiple publications. Mihalcea et al. (2007) propose two methods for translating sentiment lexicons. The first method simply uses bilingual dictionaries to translate an English sentiment lexicon. A sentence-based classifier built with this list achieved high precision but low recall on a small Romanian test set. The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations, e.g., because irony is not recognized. Banea et al. (2008) use machine translation for multilingual sentiment analysis. Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in training a Chinese classifier. The induction of a sentiment lexicon is the s</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127–135, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Christ</author>
<author>B M Schulze</author>
<author>A Hofmann</author>
<author>E Koenig</author>
</authors>
<date>1999</date>
<booktitle>The IMS Corpus Workbench: Corpus Query Processor (CQP): User’s</booktitle>
<pages>8--1999</pages>
<institution>Manual. University of Stuttgart,</institution>
<contexts>
<context position="8953" citStr="Christ et al., 1999" startWordPosition="1423" endWordPosition="1426">ished SO-PMI algorithm by Turney (2002) to show an improvement over an unsupervised method. The algorithm works with cooccurrence counts on large corpora. To determine the semantic orientation of a word w, the hits near positive (Pwords) and negative (Nwords) seed words is used. The SO-PMI equation is given as SO-PMI(word) = HpwordEPwords hits(word NEAR pword) 7 7 HnwordENwords hits(word NEAR nword) × HnwordENwords hits(nword)TT \ 5.2 Data Acquisition We used the English and German Wikipedia branches as our corpora. We extracted coordinations from the corpus using a simple CQP pattern search (Christ et al., 1999). For our experiments, we looked only at coordinations with and. For the English corpus, we used the pattern [pos = &amp;quot;JJ&amp;quot;] ([pos = &amp;quot;,&amp;quot;] [pos = &amp;quot;JJ&amp;quot;])*([pos = &amp;quot;,&amp;quot;]? &amp;quot;and&amp;quot; [pos = &amp;quot;JJ&amp;quot;])+, and for the German corpus, the pattern [pos = &amp;quot;ADJ.*&amp;quot;] ([pos = &amp;quot;,&amp;quot;] [pos = &amp;quot;ADJ.*&amp;quot;])* (&amp;quot;und&amp;quot; [pos = &amp;quot;ADJ&amp;quot;])+ was used. This yielded 477,291 pairs of coordinated English adjectives and 44,245 German pairs. We used the dict.cc dictionary1 as a seed dictionary. It contained a total of 30,551 adjectives. After building a graph out of this data as described in Section 4, we apply the SimRank algorithm using 7 iterati</context>
</contexts>
<marker>Christ, Schulze, Hofmann, Koenig, 1999</marker>
<rawString>O. Christ, B.M. Schulze, A. Hofmann, and E. Koenig. 1999. The IMS Corpus Workbench: Corpus Query Processor (CQP): User’s Manual. University of Stuttgart, March, 8:1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Florian Laws</author>
<author>Lukas Michelbacher</author>
<author>Christian Scheible</author>
<author>Jason Utt</author>
</authors>
<title>A graphtheoretic algorithm for automatic extension of translation lexicons.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>91--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="4432" citStr="Dorow et al. (2009)" startWordPosition="684" endWordPosition="687">MI) synonymy measure He assumes that the sentiment orientation of a phrase can be determined by comparing its pointwise mutual information with a positive (excellent) and a negative phrase (poor). An introduction to SO-PMI is given in Section 5.1 3 Bilingual Lexicon Induction Typical approaches to the induction of bilingual lexicons involve gathering new information from a small set of known identities between the languages which is called a seed lexicon and incorporating intralingual sources of information (e.g. cooccurrence counts). Two examples of such methods are a graph-based approach by Dorow et al. (2009) and a vector-space based approach by Rapp (1999). In this paper, we will employ the graph-based method. SimRank was first introduced by Jeh and Widom (2002). It is an iterative algorithm that measures the similarity between all vertices in a graph. In SimRank, two nodes are similar if their neighbors are similar. This defines a recursive process that ends when the two nodes compared are identical. As proposed by Dorow et al. (2009), we will apply it to a graph G in which vertices represent words and edges represent relations between words. SimRank will then yield similarity values between ver</context>
</contexts>
<marker>Dorow, Laws, Michelbacher, Scheible, Utt, 2009</marker>
<rawString>Beate Dorow, Florian Laws, Lukas Michelbacher, Christian Scheible, and Jason Utt. 2009. A graphtheoretic algorithm for automatic extension of translation lexicons. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 91–95, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="3019" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="458" endWordPosition="462">for multilingual sentiment analysis. Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in training a Chinese classifier. The induction of a sentiment lexicon is the subject of early work by (Hatzivassiloglou and McKeown, 1997). They construct graphs from coordination data from large corpora based on the intuition that adjectives with the same sentiment orientation are likely to be coordinated. For example, fresh and delicious is more likely than rotten and delicious. They then apply a graph clustering algorithm to find groups of adjectives with the same orientation. Finally, they assign the same label to all adjectives that belong to the same cluster. The authors note that some words cannot be assigned a unique label since their sentiment depends on con25 Proceedings of the ACL 2010 Student Research Workshop, pages</context>
<context position="7182" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1147" endWordPosition="1150">deal with in this paper is the transfer of such information between languages. The SimRank lexicon induction method is suitable for this purpose since it can produce useful similarity values even with a small seed lexicon. First, we build a graph for each language. The vertices of these graphs will represent adjectives while the edges are coordination relations between these adjectives. An example for such a graph is given in Figure 1. Figure 1: Sample graph showing English coordination relations. The use of coordination information has been shown to be beneficial for example in early work by Hatzivassiloglou and McKeown (1997). Seed links between those graphs will be taken from a universal dictionary. Figure 2 shows an example graph. Here, intralingual coordination relations are represented as black lines, seed relations as solid grey lines, and relations that are induced through SimRank as dashed grey lines. After computing similarities in this graph, we � sim(k, l), |N(i)||N(j) c sim(i, j) = k∈N(i),l∈N(j) 26 Figure 2: Sample graph showing English and German coordination relations. Solid black lines represent coordinations, solid grey lines represent seed relations, and dashed grey lines show induced relations. ne</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 174–181, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Jeh</author>
<author>Jennifer Widom</author>
</authors>
<title>Simrank: a measure of structural-context similarity.</title>
<date>2002</date>
<booktitle>In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>538--543</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4589" citStr="Jeh and Widom (2002)" startWordPosition="710" endWordPosition="713">excellent) and a negative phrase (poor). An introduction to SO-PMI is given in Section 5.1 3 Bilingual Lexicon Induction Typical approaches to the induction of bilingual lexicons involve gathering new information from a small set of known identities between the languages which is called a seed lexicon and incorporating intralingual sources of information (e.g. cooccurrence counts). Two examples of such methods are a graph-based approach by Dorow et al. (2009) and a vector-space based approach by Rapp (1999). In this paper, we will employ the graph-based method. SimRank was first introduced by Jeh and Widom (2002). It is an iterative algorithm that measures the similarity between all vertices in a graph. In SimRank, two nodes are similar if their neighbors are similar. This defines a recursive process that ends when the two nodes compared are identical. As proposed by Dorow et al. (2009), we will apply it to a graph G in which vertices represent words and edges represent relations between words. SimRank will then yield similarity values between vertices that indicate the degree of relatedness between them with regard to the property encoded through the edges. For two nodes i and j in G, similarity acco</context>
</contexts>
<marker>Jeh, Widom, 2002</marker>
<rawString>Glen Jeh and Jennifer Widom. 2002. Simrank: a measure of structural-context similarity. In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538–543, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Laws</author>
<author>L Michelbacher</author>
<author>B Dorow</author>
<author>U Heid</author>
<author>H Sch¨utze</author>
</authors>
<title>Building a Cross-lingual Relatedness Thesaurus Using a Graph Similarity Measure.</title>
<date>2010</date>
<booktitle>the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<note>Submitted on Nov 7, 2009, to</note>
<marker>Laws, Michelbacher, Dorow, Heid, Sch¨utze, 2010</marker>
<rawString>F. Laws, L. Michelbacher, B. Dorow, U. Heid, and H. Sch¨utze. 2010. Building a Cross-lingual Relatedness Thesaurus Using a Graph Similarity Measure. Submitted on Nov 7, 2009, to the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Legendre</author>
</authors>
<title>Species associations: the Kendall coefficient of concordance revisited.</title>
<date>2005</date>
<journal>Journal of Agricultural Biological and Environment Statistics,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="11632" citStr="Legendre, 2005" startWordPosition="1871" endWordPosition="1872">jectives. This resulted in a 150 word test set. To determine the sentiment of these adjectives, we asked 9 human judges, all native German speakers, to annotate them given the classes neutral, slightly negative, very negative, slightly positive, and very positive, reflecting the categories from the training data. In the annotation process, another 7 adjectives had to be discarded because one or more annotators marked them as unknown. Since human judges tend to interpret scales differently, we examine their agreement using Kendall’s coefficient of concordance (W) including correction for ties (Legendre, 2005) which takes ranks into account. The agreement was calculated as W = 0.674 with a significant confidence (p &lt; .001), which is usually interpreted as substantial agreement. Manual examination of the data showed that most disagreement between the annotators occurred with adjectives that are tied to political implications, for example nuklear (nuclear). 5.3 Sentiment Lexicon Induction For our experiments, we used the polarity lexicon of Wilson et al. (2005). It includes annotations of positivity in the form of the categories neutral, weakly positive (weakpos), strongly positive (strongpos), weakl</context>
</contexts>
<marker>Legendre, 2005</marker>
<rawString>P. Legendre. 2005. Species associations: the Kendall coefficient of concordance revisited. Journal of Agricultural Biological and Environment Statistics, 10(2):226–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>976--983</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1807" citStr="Mihalcea et al. (2007)" startWordPosition="277" endWordPosition="280">be advantageous by making use of resources for a source language to improve the analysis of the target language. This paper presents an approach to the transfer of sentiment information between languages. It is built around an algorithm that has been successfully applied for the acquisition of bilingual lexicons. One of the main benefits of the method is its ability of handling sparse data well. Our experiments are carried out using English as a source language and German as a target language. 2 Related Work The translation of sentiment information has been the topic of multiple publications. Mihalcea et al. (2007) propose two methods for translating sentiment lexicons. The first method simply uses bilingual dictionaries to translate an English sentiment lexicon. A sentence-based classifier built with this list achieved high precision but low recall on a small Romanian test set. The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations, e.g., because irony is not recognized. Banea et al. (2008) use machine translation for multilingual sentim</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated english and german corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>College Park, Maryland, USA,</location>
<contexts>
<context position="4481" citStr="Rapp (1999)" startWordPosition="694" endWordPosition="695">ation of a phrase can be determined by comparing its pointwise mutual information with a positive (excellent) and a negative phrase (poor). An introduction to SO-PMI is given in Section 5.1 3 Bilingual Lexicon Induction Typical approaches to the induction of bilingual lexicons involve gathering new information from a small set of known identities between the languages which is called a seed lexicon and incorporating intralingual sources of information (e.g. cooccurrence counts). Two examples of such methods are a graph-based approach by Dorow et al. (2009) and a vector-space based approach by Rapp (1999). In this paper, we will employ the graph-based method. SimRank was first introduced by Jeh and Widom (2002). It is an iterative algorithm that measures the similarity between all vertices in a graph. In SimRank, two nodes are similar if their neighbors are similar. This defines a recursive process that ends when the two nodes compared are identical. As proposed by Dorow et al. (2009), we will apply it to a graph G in which vertices represent words and edges represent relations between words. SimRank will then yield similarity values between vertices that indicate the degree of relatedness bet</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 519–526, College Park, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="3726" citStr="Turney (2002)" startWordPosition="575" endWordPosition="576">adjectives with the same sentiment orientation are likely to be coordinated. For example, fresh and delicious is more likely than rotten and delicious. They then apply a graph clustering algorithm to find groups of adjectives with the same orientation. Finally, they assign the same label to all adjectives that belong to the same cluster. The authors note that some words cannot be assigned a unique label since their sentiment depends on con25 Proceedings of the ACL 2010 Student Research Workshop, pages 25–30, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics text. Turney (2002) suggests a corpus-based extraction method based on his pointwise mutual information (PMI) synonymy measure He assumes that the sentiment orientation of a phrase can be determined by comparing its pointwise mutual information with a positive (excellent) and a negative phrase (poor). An introduction to SO-PMI is given in Section 5.1 3 Bilingual Lexicon Induction Typical approaches to the induction of bilingual lexicons involve gathering new information from a small set of known identities between the languages which is called a seed lexicon and incorporating intralingual sources of information </context>
<context position="8372" citStr="Turney (2002)" startWordPosition="1333" endWordPosition="1334">w induced relations. need to obtain sentiment values. We will define the sentiment score (sent) as �sent(nt) = simnorm(ns, nt) sent(ns), nsES simnorm(ns, nt) = EnsES sim(ns, nt). Normalization guarantees that all sentiment scores lie within a specified range. Scores are not a direct indicator for orientation since the similarities still include a lot of noise. Therefore, we interpret the scores by assigning each word to a category by finding score thresholds between the categories. 5 Experiments 5.1 Baseline Method (SO-PMI) We will compare our method to the wellestablished SO-PMI algorithm by Turney (2002) to show an improvement over an unsupervised method. The algorithm works with cooccurrence counts on large corpora. To determine the semantic orientation of a word w, the hits near positive (Pwords) and negative (Nwords) seed words is used. The SO-PMI equation is given as SO-PMI(word) = HpwordEPwords hits(word NEAR pword) 7 7 HnwordENwords hits(word NEAR nword) × HnwordENwords hits(nword)TT \ 5.2 Data Acquisition We used the English and German Wikipedia branches as our corpora. We extracted coordinations from the corpus using a simple CQP pattern search (Christ et al., 1999). For our experimen</context>
<context position="12554" citStr="Turney (2002)" startWordPosition="2017" endWordPosition="2018">mplications, for example nuklear (nuclear). 5.3 Sentiment Lexicon Induction For our experiments, we used the polarity lexicon of Wilson et al. (2005). It includes annotations of positivity in the form of the categories neutral, weakly positive (weakpos), strongly positive (strongpos), weakly negative (weakneg), and strongly positive (strongneg). In order to conduct arithmetic operations on these annotations, mapped them to values from the interval [−1, 1] by using the assignments given in Table 1. 5.4 Results To compare the two methods to the human raters, we first reproduce the evaluation by Turney (2002) and examine the correlation coefficients. Both methods will be compared to an average over the human rater values. These values are calculated on values asserted based on Table 1. The correlation coefficients between the automatic systems and the human ratings, SO-PMI yields r = 0.551, and SimRank yields r = 0.587 which are not significantly different. This shows that SO and SR have about the same performance on this broad measure. Since many adjectives do not express sentiment at all, the correct categorization of neutral adjectives is as important as the scalar rating. Thus, we divide the a</context>
<context position="18723" citStr="Turney (2002)" startWordPosition="3032" endWordPosition="3033">l include a further examination of the merits of its application for knowledge-sparse languages. The underlying graph structure provides a foundation for many conceivable extensions. In this paper, we presented a fairly simple experiment restricted to adjectives only. However, the method 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 x Accuracy 0.8 0.6 0.4 0.2 0 1 positive (SO-PMI) positive (SimRank) neutral (SO-PMI) neutral (SimRank) negative (SO-PMI) negative (SimRank) 29 is suitable to include arbitrary parts of speech as well as phrases, as used by Turney (2002). Another conceivable application would be the direct combination of the SimRank-based model with a statistical model. Currently, our input sentiment list exists only of prior sentiment values, however work by Wilson et al. (2009) has advanced the notion of contextual polarity lists. The automatic translation of this information could be beneficial for sentiment analysis in other languages. Another important problem in sentiment analysis is the treatment of ambiguity. The sentiment expressed by a word or phrase is contextdependent and is for example related to word sense (Akkaya et al., 2009).</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>235--243</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="2689" citStr="Wan (2009)" startWordPosition="411" endWordPosition="412">e second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations, e.g., because irony is not recognized. Banea et al. (2008) use machine translation for multilingual sentiment analysis. Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in training a Chinese classifier. The induction of a sentiment lexicon is the subject of early work by (Hatzivassiloglou and McKeown, 1997). They construct graphs from coordination data from large corpora based on the intuition that adjectives with the same sentiment orientation are likely to be coordinated. For example, fresh and delicious is more likely than rotten and delicious. They then apply a graph </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 235– 243, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="12090" citStr="Wilson et al. (2005)" startWordPosition="1942" endWordPosition="1945">ges tend to interpret scales differently, we examine their agreement using Kendall’s coefficient of concordance (W) including correction for ties (Legendre, 2005) which takes ranks into account. The agreement was calculated as W = 0.674 with a significant confidence (p &lt; .001), which is usually interpreted as substantial agreement. Manual examination of the data showed that most disagreement between the annotators occurred with adjectives that are tied to political implications, for example nuklear (nuclear). 5.3 Sentiment Lexicon Induction For our experiments, we used the polarity lexicon of Wilson et al. (2005). It includes annotations of positivity in the form of the categories neutral, weakly positive (weakpos), strongly positive (strongpos), weakly negative (weakneg), and strongly positive (strongneg). In order to conduct arithmetic operations on these annotations, mapped them to values from the interval [−1, 1] by using the assignments given in Table 1. 5.4 Results To compare the two methods to the human raters, we first reproduce the evaluation by Turney (2002) and examine the correlation coefficients. Both methods will be compared to an average over the human rater values. These values are cal</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an Exploration of Features for Phrase-level Sentiment Analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing Contextual Polarity: an Exploration of Features for Phrase-level Sentiment Analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>