<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000148">
<title confidence="0.996697">
A Study of Two Graph Algorithms in Topic-driven Summarization
</title>
<author confidence="0.999499">
Vivi Nastase1 and Stan Szpakowicz1,2
</author>
<affiliation confidence="0.9802705">
1 School of Information Technology and Engineering,
University of Ottawa, Ottawa, Canada
2 Institute of Computer Science,
Polish Academy of Sciences, Warsaw, Poland
</affiliation>
<email confidence="0.99733">
{vnastase, szpak}@site.uottawa.ca
</email>
<sectionHeader confidence="0.997347" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933772727273">
We study how two graph algorithms ap-
ply to topic-driven summarization in the
scope of Document Understanding Con-
ferences. The DUC 2005 and 2006 tasks
were to summarize into 250 words a col-
lection of documents on a topic consist-
ing of a few statements or questions.
Our algorithms select sentences for ex-
traction. We measure their performance
on the DUC 2005 test data, using the Sum-
mary Content Units made available after
the challenge. One algorithm matches a
graph representing the entire topic against
each sentence in the collection. The
other algorithm checks, for pairs of open-
class words in the topic, whether they can
be connected in the syntactic graph of
each sentence. Matching performs bet-
ter than connecting words, but a combi-
nation of both methods works best. They
also both favour longer sentences, which
makes summaries more fluent.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997765625">
The DUC 2005 and 2006 summarization challenges
were motivated by the desire to make summariza-
tion relevant to real users. The task was focussed by
specifying an information need as a topic: one or a
few statements or questions (Dang, 2005). Systems
usually employ such data as a source of key words
or phrases which then help rank document sentences
by relevance to the topic.
We explore other information that can be ex-
tracted from a topic description. In particular, we
look at connections between open-class words. A
dependency parser, MiniPar (Lin, 1998), builds a
dependency relation graph for each sentence. We
apply such graphs in two ways. We match a graph
that covers the entire topic description against the
graph for each sentence in the collection. We also
extract all pairs of open-class words from the topic
description, and check whether they are connected
in the sentence graphs. Both methods let us rank
sentences; the top-ranking ones go into a summary
of at most 250 words. We evaluate the summaries
with the summary content units (SCU) data made
available after DUC 2005 (Nenkova and Passon-
neau, 2004; Copeck and Szpakowicz, 2005). The
experiments show that using more information than
just keywords leads to summaries with more SCUs
(total and unique) and higher SCU weight.
We present related work in section 2, and the data
and the representation we work with in section 3.
Section 4 shows the algorithms in more detail. We
describe the experiments and their results in section
5, and draw a few conclusions in section 6.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999895166666667">
Erkan and Radev (2004), Mihalcea (2004), Mihal-
cea and Tarau (2004) introduced graph methods
for summarization, word sense disambiguation and
other NLP applications.
The summarization graph-based systems imple-
ment a form of sentence ranking, based on the idea
of prestige or centrality in social networks. In this
case the network consists of sentences, and signifi-
cantly similar sentences are interconnected. Various
measures (such as node degree) help find the most
central sentences, or to score each sentence.
In topic-driven summarization, one or more sen-
tences or questions describe an information need
which the summaries must address. Previous sys-
tems extracted key words or phrases from topics
and used them to focus the summary (Fisher et al.,
2005).
Our experiments show that there is more to topics
than key words or phrases. We will experiment with
using grammatical dependency relations for the task
of extractive summarization.
In previous research, graph-matching using gram-
matical relations was used to detect textual entail-
ment (Haghighi et al., 2005).
</bodyText>
<sectionHeader confidence="0.993072" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.852742">
3.1 Topics
</subsectionHeader>
<bodyText confidence="0.99997875">
We work with a list of topics from the test data in
the DUC 2005 challenge. A topic has an identifier,
category (general/specific), title and a sequence of
statements or questions, for example:
</bodyText>
<page confidence="0.991922">
29
</page>
<bodyText confidence="0.341267">
Workshop on TextGraphs, at HLT-NAACL 2006, pages 29–32,
</bodyText>
<subsectionHeader confidence="0.262506">
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.777623285714286">
d307b
specific
New Hydroelectric Projects
What hydroelectric projects are planned
or in progress and what problems are
associated with them?
We apply MiniPar to the titles and contents
of the topics, and to all documents. The out-
put is post-processed to produce dependency pairs
only for open-class words. The dependency pairs
bypass prepositions and subordinators/coordinators
between clauses, linking the corresponding open-
class words. After post-processing, the topic will
be represented like this:
</bodyText>
<construct confidence="0.446865363636364">
QUESTION NUMBER: d307b
LIST OF WORDS:
associate, hydroelectric, in, plan,
problem, progress, project, new, them
LIST OF PAIRS:
relation(project, hydroelectric)
relation(project, new)
relation(associate, problem)
relation(plan, project)
relation(in, progress)
relation(associate, them)
</construct>
<bodyText confidence="0.9991593">
The parser does not always produce perfect
parses. In this example it did not associate the phrase
in progress with the noun projects, so we missed the
connection between projects and progress.
In the next step, we expand each open-class word
in the topic with all its WordNet synsets and one-step
hypernyms and hyponyms. We have two variants
of the topic file: with all open-class words from the
topic description Topicsall, and only with nouns and
verbs TopicsNV .
</bodyText>
<subsectionHeader confidence="0.997807">
3.2 Documents
</subsectionHeader>
<bodyText confidence="0.999927111111111">
For each topic, we summarize a collection of up to
50 news items. In our experiments, we build a file
with all documents for a given topic, one sentence
per line, cleaned of XML tags. We process each file
with MiniPar, and post-process the output similarly
to the topics. For documents we keep the list of de-
pendency relations but not a separate list of words.
This processing also gives one file per topic, each
sentence followed by its list of dependency relations.
</bodyText>
<subsectionHeader confidence="0.998926">
3.3 Summary Content Units
</subsectionHeader>
<bodyText confidence="0.999992615384615">
The DUC 2005 summary evaluation included an
analysis based on Summary Content Units. SCUs
are manually-selected topic-specific summary-
worthy phrases which the summarization systems
are expected to include in their output (Nenkova and
Passonneau, 2004; Copeck and Szpakowicz, 2005).
The SCUs for 20 of the test topics became available
after the challenge. We use the SCU data to mea-
sure the performance of our graph-matching and
path-search algorithms: the total number, weight
and number of unique SCUs per summary, and
the number of negative SCU sentences, explicitly
marked as not relevant to the summary.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="method">
4 Algorithms
</sectionHeader>
<subsectionHeader confidence="0.999635">
4.1 TopicHsentence graph matching (GM)
</subsectionHeader>
<bodyText confidence="0.997022458333333">
We treat a sentence and a topic as graphs. The nodes
are the open-class words in the sentence or topic (we
also refer to them as keywords), and the edges are
the dependency relations extracted from MiniPar’s
output. In order to maximize the matching score, we
replace a word wS in the sentence with wQ from the
query, if wS appears in the WordNet expansion of
words in wQ.
To score a match between a sentence and a graph,
we compute and then combine two partial scores:
SN (node match score) the node (keyword) overlap
between the two text units. A keyword count
is equal to the number of dependency pairs it
appears with in the document sentence;
SE (edge match score) the edge (dependency rela-
tion) overlap.
The overall score is S = SN +
WeightFactor * SE, where WeightFactor E
10, 1, 2, ...,15, 20, 50, 1001. Varying the weight
factor allows us to find various combinations
of node and edge score matches which work
best for sentence extraction in summarization.
When WeightFactor = 0, the sentence scores
correspond to keyword counts.
</bodyText>
<subsectionHeader confidence="0.991158">
4.2 Path search for topic keyword pairs (PS)
</subsectionHeader>
<bodyText confidence="0.998369285714286">
Here too we look at sentences as graphs. We only
take the list of words from the topic representation.
For each pair of those words, we check whether they
both appear in the sentence and are connected in
the sentence graph. We use the list of WordNet-
expanded terms again, to maximize matching. The
final score for the sentence has two components:
the node-match score SN, and SP, the number of
word pairs from the topic description connected by
a path in the sentence graph. The final score is
S = SN +WeightFactor*SP. WeightFactor, in
the same range as previously, is meant to boost the
contribution of the path score towards the final score
of the sentence.
</bodyText>
<sectionHeader confidence="0.990381" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.999532818181818">
We produce a summary for each topic and each ex-
perimental configuration. We take the most highly
ranked (complete) sentences for which the total
number of words does not exceed the 250-word
limit. Next, we gather SCU data for each sentence in
each summary from the SCU information files. For
a specific experimental configuration – topic repre-
sentation, graph algorithm – we produce summaries
for the 20 documents with the weight factor values
0, 1, 2, ...,15, 20, 50, 100. Each experimental con-
figuration generates 19 sets of average results, one
</bodyText>
<page confidence="0.975795">
30
</page>
<figure confidence="0.619658">
Graph match method
Path search method
</figure>
<figureCaption confidence="0.995072">
Figure 1: Average SCU weights for graph matching
</figureCaption>
<bodyText confidence="0.95445235">
(GM) and path search (PS) with different topic rep-
resentations
per weight factor. For one weight factor, we gen-
erate summaries for the 20 topics, and then average
their SCU statistics, including SCU weight, number
of unique SCUs and total number of SCUs. In the re-
sults which follow we present average SCU weight
per summary. The number of unique SCUs and
the number of SCUs closely follow the presented
graphs. The overlap of SCUs (number of SCUs
/ number of unique SCUs) reaches a maximum of
1.09. There was no explicit redundancy elimination,
mostly because the SCU overlap was so low.
We compare the performance of the two algo-
rithms, GM and PS, on the two topic representations
– with all open-class words and only with nouns and
verbs. Figure 1 shows the performance of the meth-
ods in terms of average SCU weights per summary
for each weight factor considered 1.
The results allow us to make several observations.
</bodyText>
<listItem confidence="0.962168375">
• Keyword-only match performs worse that ei-
ther GM or PS. The points corresponding to
keyword (node) match only are the points for
which the weight factor is 0. In this case the
dependency pairs match and paths found in the
graph do not contribute to the overall score.
• Both graph algorithms achieve better perfor-
mance for only the nouns and verbs from the
</listItem>
<tableCaption confidence="0.65565">
&apos;The summary statistics level off above a certain weight fac-
tor, so we include only the non-flat part of the graph.
</tableCaption>
<bodyText confidence="0.860288652173913">
topic than for all open-class words. If, how-
ever, the topic requests entities or events with
specific properties, described by adjectives or
adverbs, using only nouns and verbs may pro-
duce worse results.
• GM performs better than PS for both types of
topic descriptions. In other words, looking at
the same words that appear in the topic, con-
nected in the same way, leads to better results
than finding pairs of words that are “somehow”
connected.
• Higher performance for higher weight factors
further supports the point that looking for word
connections, instead of isolated words, helps
find sentences with information content more
related to the topic.
For the following set of experiments, we use the
topics with the word list containing only nouns and
verbs. We want to compare graph matching and
path search further. One issue that comes to mind is
whether a combination of the two methods will per-
form better than each of them individually. Figure 2
plots the average of SCU weights per summary.
</bodyText>
<figure confidence="0.7879505">
0 5 10 15 20
Pair/connections weight factor
</figure>
<figureCaption confidence="0.893888">
Figure 2: Graph matching, path search and their
combination
</figureCaption>
<bodyText confidence="0.9897999375">
We observe that the combination of graph match-
ing and path search gives better results than ei-
ther method alone. The sentence score com-
bines the number of edges matched and the num-
ber of connections found with equal weight fac-
tors for the edge match and path score. This
raises the question whether different weights for
the edge match and path would lead to better
scores. Figure 3 plots the results produced us-
ing the score computation formula 5 = 5N +
WeightFactorE * 5E + WeightFactorP * 5P,
where both WeightFactorE and WeightFactorP
are integers from 0 to 30.
The lowest scores are for the weight factors 0,
when sentence score depends only on the keyword
score. There is an increase in average SCU weights
</bodyText>
<figure confidence="0.998255846153846">
NV
all
0 5 10 15 20
Pair/connections weight factor
18
17
16
15
14
13
12
11
Average (positive) SCU weights/summary
10
NV
all
0 5 10 15 20
Pair/connections weight factor
16
15
14
13
12
11
Average (positive) SCU weights/summary
10
Average (positive) SCU weights/summary
19
18
17
16
15
14
13
12
11
GM &amp; PS
GM
PS
</figure>
<page confidence="0.996877">
31
</page>
<note confidence="0.587755">
Avg weight of SCUs
</note>
<figureCaption confidence="0.760279">
Figure 3: Graph match and path search combined
</figureCaption>
<bodyText confidence="0.989778">
with different weight factors
towards higher values of weight factors. A transpar-
ent view of the 3D graph shows that graph match has
higher peaks toward higher weight factors than path
search, and higher also than the situation when path
search and graph match have equal weights.
The only sentences in the given documents tagged
with SCU information are those which appeared in
the summaries generated by the competing teams
in 2005. Our results are therefore actually a lower
bound – more of the sentences selected may include
relevant information. A manual analysis of the sum-
maries generated using only keyword counts showed
that, for these summaries, the sentences not contain-
ing SCUs were not informative. We cannot check
this for all the summaries generated in these ex-
periments, because the number is very large, above
1000. An average summary had 8.24 sentences, with
3.19 sentences containing SCUs. We cannot say
much about the sentences that do not contain SCUs.
This may raise doubts about our results. Support
for the fact that the results reflect a real increase in
performance comes from the weights of the SCUs
added: the average SCU weight increases from 2.5
when keywords are used to 2.75 for path search al-
gorithm, and 2.91 for graph match and the combina-
tion of path search and graph match. This shows that
by increasing the weight of graph edges and paths
in the scoring of a sentence, the algorithm can pick
more and better SCUs, SCUs which more people see
as relevant to the topic. It would be certainly in-
teresting to have a way of assessing the “SCU-less”
sentences in the summary. We leave that for future
work, and possibly future developments in SCU an-
notation.
</bodyText>
<sectionHeader confidence="0.999666" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99990964">
We have studied how two algorithms influence sum-
marization by sentence extraction. They match the
topic description and sentences in a document. The
results show that using connections between the
words in the topic description improves the accu-
racy of sentence scoring compared to simple key-
word match. Finding connections between query
words in a sentence depends on finding the corre-
sponding words in the sentence. In our experiments,
we have used one-step extension in WordNet (along
IS-A links) to find such correspondences. It is, how-
ever, a limited solution, and better word matches
should be attempted, such as for example word sim-
ilarity scores in WordNet.
In summarization by sentence extraction, other
scores affect sentence ranking, for example position
in the document and paragraph or proximity to other
high-ranked sentences. We have analyzed the effect
of connections in isolation, to reduce the influence of
other factors. A summarization system would com-
bine all these scores, and possibly produce better re-
sults. Word connections or pairs could also be used
just as keywords were, as part of a feature descrip-
tion of documents, to be automatically ranked using
machine learning.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996769115384615">
Terry Copeck and Stan Szpakow-
icz. 2005. Leveraging pyramids.
http://duc.nist.gov/pubs/2005papers/uottawa.copeck2.pdf.
Hoa Trang Dang. 2005. Overview of DUC 2005.
http://duc.nist.gov/pubs/2005papers/OVERVIEW05.pdf.
G¨unes¸ Erkan and Dragomir Radev. 2004. LexRank: Graph-
based centrality as salience in text summarization. Journal
of Artificial Intelligence Research, (22).
Seeger Fisher, Brian Roark, Jianji Yang, and Bill
Hersh. 2005. Ogi/ohsu baseline query-directed
multi-document summarization system for duc-2005.
http://duc.nist.gov/pubs/2005papers/ohsu.seeger.pdf.
Aria Haghighi, Andrew Ng, and Christopher Manning. 2005.
Robust textual inference via graph matching. In Proc. of
HLT-EMNLP 2005, Vancouver, BC, Canada.
Dekang Lin. 1998. Dependency-based evaluation of MiniPar.
In Workshop on the Evaluation of Parsing Systems, Granada,
Spain.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order
into texts. In Proc. of EMNLP 2004, Barcelona, Spain.
Rada Mihalcea. 2004. Graph-based ranking algorithms for sen-
tence extraction, applied to text summarization. In Proc. of
ACL 2004, Barcelona, Spain.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. In
Proc. of NAACL-HLT 2004.
</reference>
<figure confidence="0.989329222222222">
20
0 5
10 15
Pair weight factor
15
10 Path weight factor
5
19.5
19
18.5
18
17.5
17
16.5
16
15.5
30
25
</figure>
<page confidence="0.975786">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827170">
<title confidence="0.999812">A Study of Two Graph Algorithms in Topic-driven Summarization</title>
<author confidence="0.994828">Stan</author>
<affiliation confidence="0.95771475">1School of Information Technology and University of Ottawa, Ottawa, 2Institute of Computer Polish Academy of Sciences, Warsaw,</affiliation>
<abstract confidence="0.999404">We study how two graph algorithms apply to topic-driven summarization in the scope of Document Understanding Conferences. The DUC 2005 and 2006 tasks were to summarize into 250 words a collection of documents on a topic consisting of a few statements or questions. Our algorithms select sentences for extraction. We measure their performance on the DUC 2005 test data, using the Summary Content Units made available after the challenge. One algorithm matches a graph representing the entire topic against each sentence in the collection. The other algorithm checks, for pairs of openclass words in the topic, whether they can be connected in the syntactic graph of each sentence. Matching performs better than connecting words, but a combination of both methods works best. They also both favour longer sentences, which makes summaries more fluent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Terry Copeck</author>
<author>Stan Szpakowicz</author>
</authors>
<date>2005</date>
<note>Leveraging pyramids. http://duc.nist.gov/pubs/2005papers/uottawa.copeck2.pdf.</note>
<contexts>
<context position="2321" citStr="Copeck and Szpakowicz, 2005" startWordPosition="372" endWordPosition="375">pendency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs. Both methods let us rank sentences; the top-ranking ones go into a summary of at most 250 words. We evaluate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The experiments show that using more information than just keywords leads to summaries with more SCUs (total and unique) and higher SCU weight. We present related work in section 2, and the data and the representation we work with in section 3. Section 4 shows the algorithms in more detail. We describe the experiments and their results in section 5, and draw a few conclusions in section 6. 2 Related work Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-b</context>
<context position="6189" citStr="Copeck and Szpakowicz, 2005" startWordPosition="972" endWordPosition="975">ntence per line, cleaned of XML tags. We process each file with MiniPar, and post-process the output similarly to the topics. For documents we keep the list of dependency relations but not a separate list of words. This processing also gives one file per topic, each sentence followed by its list of dependency relations. 3.3 Summary Content Units The DUC 2005 summary evaluation included an analysis based on Summary Content Units. SCUs are manually-selected topic-specific summaryworthy phrases which the summarization systems are expected to include in their output (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The SCUs for 20 of the test topics became available after the challenge. We use the SCU data to measure the performance of our graph-matching and path-search algorithms: the total number, weight and number of unique SCUs per summary, and the number of negative SCU sentences, explicitly marked as not relevant to the summary. 4 Algorithms 4.1 TopicHsentence graph matching (GM) We treat a sentence and a topic as graphs. The nodes are the open-class words in the sentence or topic (we also refer to them as keywords), and the edges are the dependency relations extracted from MiniPar’s output. In o</context>
</contexts>
<marker>Copeck, Szpakowicz, 2005</marker>
<rawString>Terry Copeck and Stan Szpakowicz. 2005. Leveraging pyramids. http://duc.nist.gov/pubs/2005papers/uottawa.copeck2.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<date>2005</date>
<journal>Overview of DUC</journal>
<note>http://duc.nist.gov/pubs/2005papers/OVERVIEW05.pdf.</note>
<contexts>
<context position="1410" citStr="Dang, 2005" startWordPosition="223" endWordPosition="224">opic against each sentence in the collection. The other algorithm checks, for pairs of openclass words in the topic, whether they can be connected in the syntactic graph of each sentence. Matching performs better than connecting words, but a combination of both methods works best. They also both favour longer sentences, which makes summaries more fluent. 1 Introduction The DUC 2005 and 2006 summarization challenges were motivated by the desire to make summarization relevant to real users. The task was focussed by specifying an information need as a topic: one or a few statements or questions (Dang, 2005). Systems usually employ such data as a source of key words or phrases which then help rank document sentences by relevance to the topic. We explore other information that can be extracted from a topic description. In particular, we look at connections between open-class words. A dependency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and </context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa Trang Dang. 2005. Overview of DUC 2005. http://duc.nist.gov/pubs/2005papers/OVERVIEW05.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>LexRank: Graphbased centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<contexts>
<context position="2753" citStr="Erkan and Radev (2004)" startWordPosition="447" endWordPosition="450"> a summary of at most 250 words. We evaluate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The experiments show that using more information than just keywords leads to summaries with more SCUs (total and unique) and higher SCU weight. We present related work in section 2, and the data and the representation we work with in section 3. Section 4 shows the algorithms in more detail. We describe the experiments and their results in section 5, and draw a few conclusions in section 6. 2 Related work Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summ</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir Radev. 2004. LexRank: Graphbased centrality as salience in text summarization. Journal of Artificial Intelligence Research, (22).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seeger Fisher</author>
<author>Brian Roark</author>
<author>Jianji Yang</author>
<author>Bill Hersh</author>
</authors>
<title>Ogi/ohsu baseline query-directed multi-document summarization system for duc-2005.</title>
<date>2005</date>
<note>http://duc.nist.gov/pubs/2005papers/ohsu.seeger.pdf.</note>
<contexts>
<context position="3489" citStr="Fisher et al., 2005" startWordPosition="560" endWordPosition="563">nd other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must address. Previous systems extracted key words or phrases from topics and used them to focus the summary (Fisher et al., 2005). Our experiments show that there is more to topics than key words or phrases. We will experiment with using grammatical dependency relations for the task of extractive summarization. In previous research, graph-matching using grammatical relations was used to detect textual entailment (Haghighi et al., 2005). 3 Data 3.1 Topics We work with a list of topics from the test data in the DUC 2005 challenge. A topic has an identifier, category (general/specific), title and a sequence of statements or questions, for example: 29 Workshop on TextGraphs, at HLT-NAACL 2006, pages 29–32, New York City, Ju</context>
</contexts>
<marker>Fisher, Roark, Yang, Hersh, 2005</marker>
<rawString>Seeger Fisher, Brian Roark, Jianji Yang, and Bill Hersh. 2005. Ogi/ohsu baseline query-directed multi-document summarization system for duc-2005. http://duc.nist.gov/pubs/2005papers/ohsu.seeger.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP 2005,</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="3799" citStr="Haghighi et al., 2005" startWordPosition="607" endWordPosition="610">) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must address. Previous systems extracted key words or phrases from topics and used them to focus the summary (Fisher et al., 2005). Our experiments show that there is more to topics than key words or phrases. We will experiment with using grammatical dependency relations for the task of extractive summarization. In previous research, graph-matching using grammatical relations was used to detect textual entailment (Haghighi et al., 2005). 3 Data 3.1 Topics We work with a list of topics from the test data in the DUC 2005 challenge. A topic has an identifier, category (general/specific), title and a sequence of statements or questions, for example: 29 Workshop on TextGraphs, at HLT-NAACL 2006, pages 29–32, New York City, June 2006. c�2006 Association for Computational Linguistics d307b specific New Hydroelectric Projects What hydroelectric projects are planned or in progress and what problems are associated with them? We apply MiniPar to the titles and contents of the topics, and to all documents. The output is post-processed t</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proc. of HLT-EMNLP 2005, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MiniPar.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1729" citStr="Lin, 1998" startWordPosition="275" endWordPosition="276">, which makes summaries more fluent. 1 Introduction The DUC 2005 and 2006 summarization challenges were motivated by the desire to make summarization relevant to real users. The task was focussed by specifying an information need as a topic: one or a few statements or questions (Dang, 2005). Systems usually employ such data as a source of key words or phrases which then help rank document sentences by relevance to the topic. We explore other information that can be extracted from a topic description. In particular, we look at connections between open-class words. A dependency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs. Both methods let us rank sentences; the top-ranking ones go into a summary of at most 250 words. We evaluate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The ex</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MiniPar. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2797" citStr="Mihalcea and Tarau (2004)" startWordPosition="453" endWordPosition="457">ate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The experiments show that using more information than just keywords leads to summaries with more SCUs (total and unique) and higher SCU weight. We present related work in section 2, and the data and the representation we work with in section 3. Section 4 shows the algorithms in more detail. We describe the experiments and their results in section 5, and draw a few conclusions in section 6. 2 Related work Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must address. Previous systems extract</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proc. of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proc. of ACL 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2770" citStr="Mihalcea (2004)" startWordPosition="451" endWordPosition="452">0 words. We evaluate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The experiments show that using more information than just keywords leads to summaries with more SCUs (total and unique) and higher SCU weight. We present related work in section 2, and the data and the representation we work with in section 3. Section 4 shows the algorithms in more detail. We describe the experiments and their results in section 5, and draw a few conclusions in section 6. 2 Related work Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must addres</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proc. of ACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL-HLT</booktitle>
<contexts>
<context position="2291" citStr="Nenkova and Passonneau, 2004" startWordPosition="367" endWordPosition="371">between open-class words. A dependency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs. Both methods let us rank sentences; the top-ranking ones go into a summary of at most 250 words. We evaluate the summaries with the summary content units (SCU) data made available after DUC 2005 (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The experiments show that using more information than just keywords leads to summaries with more SCUs (total and unique) and higher SCU weight. We present related work in section 2, and the data and the representation we work with in section 3. Section 4 shows the algorithms in more detail. We describe the experiments and their results in section 5, and draw a few conclusions in section 6. 2 Related work Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applicati</context>
<context position="6159" citStr="Nenkova and Passonneau, 2004" startWordPosition="968" endWordPosition="971">ents for a given topic, one sentence per line, cleaned of XML tags. We process each file with MiniPar, and post-process the output similarly to the topics. For documents we keep the list of dependency relations but not a separate list of words. This processing also gives one file per topic, each sentence followed by its list of dependency relations. 3.3 Summary Content Units The DUC 2005 summary evaluation included an analysis based on Summary Content Units. SCUs are manually-selected topic-specific summaryworthy phrases which the summarization systems are expected to include in their output (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The SCUs for 20 of the test topics became available after the challenge. We use the SCU data to measure the performance of our graph-matching and path-search algorithms: the total number, weight and number of unique SCUs per summary, and the number of negative SCU sentences, explicitly marked as not relevant to the summary. 4 Algorithms 4.1 TopicHsentence graph matching (GM) We treat a sentence and a topic as graphs. The nodes are the open-class words in the sentence or topic (we also refer to them as keywords), and the edges are the dependency relations extract</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In Proc. of NAACL-HLT 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>