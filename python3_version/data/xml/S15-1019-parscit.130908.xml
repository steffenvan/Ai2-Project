<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003888">
<title confidence="0.982273">
Competence in lexical semantics
</title>
<author confidence="0.992586">
Andr´as Kornai Judit ´Acs M´arton Makrai
</author>
<affiliation confidence="0.942735">
Institute for Computer Science Dept of Automation and Institute for Linguistics
Hungarian Academy of Sciences Applied Informatics, BUTE Hungarian Academy of Sciences
</affiliation>
<address confidence="0.7971635">
Kende u. 13-17 Magyar Tud´osok krt. 2 Bencz´ur u. 33
1111 Budapest, Hungary 1117 Budapest, Hungary 1068 Budapest, Hungary
</address>
<email confidence="0.994481">
andras@kornai.com judit@aut.bme.hu makrai@nytud.hu
</email>
<author confidence="0.995519">
D´avid Nemeskey Katalin Pajkossy G´abor Recski
</author>
<affiliation confidence="0.972326">
Faculty of Informatics Department of Algebra Institute for Linguistics
E¨otv¨os Lor´and University BUTE Hungarian Academy of Sciences
</affiliation>
<address confidence="0.744922">
P´azm´any P´eter s´et´any 1/C Egry J. u. 1 Bencz´ur u. 33
1117 Budapest, Hungary 1111 Budapest, Hungary 1068 Budapest, Hungary
</address>
<email confidence="0.996156">
nemeskeyd@gmail.com pajkossy@math.bme.hu recski@mokk.bme.hu
</email>
<sectionHeader confidence="0.99558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999730837837838">
We investigate from the competence stand-
point two recent models of lexical semantics,
algebraic conceptual representations and con-
tinuous vector models.
Characterizing what it means for a speaker to be
competent in lexical semantics remains perhaps the
most significant stumbling block in reconciling the
two main threads of semantics, Chomsky’s cogni-
tivism and Montague’s formalism. As Partee (1979)
already notes (see also Partee 2013), linguists as-
sume that people know their language and that their
brain is finite, while Montague assumed that words
are characterized by intensions, formal objects that
require an infinite amount of information to specify.
In this paper we investigate two recent models of
lexical semantics that rely exclusively on finite in-
formation objects: algebraic conceptual representa-
tions (ACR) (Wierzbicka, 1985; Kornai, 2010; Gor-
don et al., 2011), and continuous vector space (CVS)
models which assign to each word a point in finite-
dimensional Euclidean space (Bengio et al., 2003;
Turian et al., 2010; Pennington et al., 2014). After a
brief introduction to the philosophical background
of these and similar models, we address the hard
questions of competence, starting with learnability
in Section 2; the ability of finite networks or vectors
to replicate traditional notions of lexical relatedness
such as synonymy, antonymy, ambiguity, polysemy,
etc. in Section 3; the interface to compositional se-
mantics in Section 4; and language-specificity and
universality in Section 5. Our survey of the litera-
ture is far from exhaustive: both ACR and CVS have
deep roots, with significant precursors going back at
least to Quillian (1968) and Osgood et al. (1975) re-
spectively, but we put the emphasis on the compu-
tational experiments we ran (source code and lexica
available at github.com/kornai/4lang).
</bodyText>
<sectionHeader confidence="0.988263" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999861409090909">
In the eyes of many, Quine (1951) has demolished
the traditional analytic/synthetic distinction, relegat-
ing nearly all pre-Fregean accounts of word mean-
ing from Aristotle to Locke to the dustbin of his-
tory. The opposing view, articulated clearly in Grice
and Strawson (1956), is based on the empirical ob-
servation that people make the call rather uniformly
over novel examples, an argument whose import is
evident from the (at the time, still nascent) cogni-
tive perspective. Today, we may agree with Putnam
(1976):
‘Bachelor’ may be synonymous with ‘un-
married man’ but that cuts no philosophic
ice. ‘Chair’ may be synonymous with
‘moveable seat for one with back’ but that
bakes no philosophic bread and washes no
philosophic windows. It is the belief that
there are synonymies and analyticities of a
deeper nature - synonymies and analytici-
ties that cannot be discovered by the lex-
icographer or the linguist but only by the
philosopher - that is incorrect.
</bodyText>
<page confidence="0.982382">
165
</page>
<note confidence="0.9533205">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 165–175,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999922784615385">
Fortunately, one philosopher’s trash may just turn
out to be another linguist’s treasure. What Putnam
has demonstrated is that “a speaker can, by all rea-
sonable standards, be in command of a word like
water without being able to command the intension
that would represent the word in possible worlds se-
mantics” (Partee, 1979). Computational systems of
Knowledge Representation, starting with the Teach-
able Word Comprehender of Quillian (1968), and
culminating in the Deep Lexical Semantics of Hobbs
(2008), carried on this tradition of analyzing word
meaning in terms of ‘essential’ or ‘analytic’ compo-
nents.
A particularly important step in this direction
is the emergence of modern, computationally ori-
ented lexicographic work beginning with Collins-
COBUILD (Sinclair, 1987), the Longman Dictio-
nary of Contemporary English (LDOCE) (Bogu-
raev and Briscoe, 1989), WordNet (Miller, 1995),
FrameNet (Fillmore and Atkins, 1998), and Verb-
Net (Kipper et al., 2000). Both the network- and
the vector-based approach build on these efforts, but
through very different routes.
Traditional network theories of Knowledge Rep-
resentation tend to concentrate on nominal features
such as the IS A links (called hypernyms in Word-
Net) and treat the representation of verbs somewhat
haphazardly. The first systems with a well-defined
model of predication are the Conceptual Depen-
dency model of Schank (1972), the Natural Syntax
Metalanguage (NSM) of Wierzbicka (1985), and a
more elaborate deep lexical semantics system that is
still under construction by Hobbs and his coworkers
(Hobbs, 2008; Gordon et al., 2011). What we call al-
gebraic conceptual representation (ACR) is any such
theory encoded with colored directed edges between
the basic conceptual units. The algebraic approach
provides a better fit with functional programming
than the more declarative, automata-theoretic ap-
proach (Huet and Razet, 2008), and makes it possi-
ble to encode verbal subcategorization (case frame)
information that is at the heart of FrameNet and
VerbNet in addition to the standardly used nominal
features (Kornai, 2010).
Continuous vector space (CVS) is also not a sin-
gle model but a rich family of models, generally
based on what Baroni (2013) calls the distributional
hypothesis, that semantically similar items have sim-
ilar distribution. This idea, going back at least to
Firth (1957) is not at all trivial to defend, and not just
because defining ‘semantically similar’ is a chal-
lenging task: as we shall see, there are significant de-
sign choices involved in defining similarity of vec-
tors as well. To the extent CVS representations are
primarily used in artificial neural net models, it may
be helpful to consider the state of a network being
described by the vector whose nth coordinate gives
the activation level of the nth neuron. Under this
conception, the meaning of a word is simply the ac-
tivation pattern of the brain when the word is pro-
duced or perceived. Such vectors have very large
(1010) dimension so dimension reduction is called
for, but direct correlation between brain activation
patterns and the distribution of words has actually
been detected (Mitchell et al., 2008).
</bodyText>
<sectionHeader confidence="0.986129" genericHeader="introduction">
2 Learnability
</sectionHeader>
<bodyText confidence="0.999964166666667">
The key distinguishing feature between ‘explana-
tory’ or competence models and ‘descriptive’ or per-
formance models is that the former, but not the latter,
come complete with a learning algorithm (Chomsky,
1965). Although there is a wealth of data on chil-
dren’s acquisition of lexical entries (McKeown and
Curtis, 1987), neither cognitive nor formal seman-
tics have come close to formulating a robust theory
of acquisition, and for intensions, infinite informa-
tion objects encoding the meaning in the formal the-
ory, it is not at all clear whether such a learning al-
gorithm is even possible.
</bodyText>
<subsectionHeader confidence="0.996224">
2.1 The basic vocabulary
</subsectionHeader>
<bodyText confidence="0.999996142857143">
The idea that there is a small set of conceptual prim-
itives for building semantic representations has a
long history both in linguistics and AI as well as in
language teaching. The more theory-oriented sys-
tems, such as Conceptual Dependency and NSM as-
sume only a few dozen primitives, but have a disqui-
eting tendency to add new elements as time goes by
(Andrews, 2015). In contrast, the systems intended
for teaching and communication, such as Basic En-
glish (Ogden, 1944) start with at least a thousand
primitives, and assume that these need to be further
supplemented by technical terms from various do-
mains. Since the obvious learning algorithm based
on any such reductive system is one where the primi-
</bodyText>
<page confidence="0.997995">
166
</page>
<bodyText confidence="0.99919272881356">
tives are assumed universal (and possibly innate, see
Section 5), and the rest is learned by reduction to the
primitives, we performed a series of ‘ceiling’ exper-
iments aiming at a determination of how big the uni-
versal/innate component of the lexicon must be. A
trivial lower bound is given by the current size of the
NSM inventory, 65 (Andrews, 2015), but as long as
we don’t have the complete lexicon of at least one
language defined in NSM terms the reductivity of
the system remains in doubt.
For English, a Germanic language, the first prov-
ably reductive system is the Longman Defining Vo-
cabulary (LDV), some 2,200 items, which provide
a sufficient basis for defining all entries in LDOCE
(using English syntax in the definitions). Our work
started with a superset of the LDV that was obtained
by adding the most frequent words according to
the Google unigram count (Brants and Franz, 2006)
and the BNC, as well as the most frequent words
from a Slavic, a Finnougric, and a Romance lan-
guage (Polish, Hungarian, and Latin), and Whitney
(1885) to form the 4lang conceptual dictionary,
with the long-term design goal of eventually provid-
ing reductive definitions for the vocabularies of all
Old World languages. ´Acs et al. (2013) describes
how bindings in other languages can be created au-
tomatically and compares the reductive method to
the familiar term- and document-frequency based
searches for core vocabulary.
This superset of LDV, called ‘4lang’ in Table 1
below, can be considered a directed graph whose
nodes are the disambiguated concepts (with expo-
nents in four languages) and whose edges run from
each definiendum to every concept that appears in its
definition. Such a graph can have many cycles. Our
main interest is with selecting a defining set which
has the property that each word, including those that
appear in the definitions, can be defined in terms of
members of this set. Every word that is a true prim-
itive (has no definition, e.g. the basic terms of the
Schank and NSM systems) must be included in the
defining set, and to these we must add at least one
vertex from every directed cycle. Thus, the prob-
lem of finding a defining set is equivalent to find-
ing a feedback vertex set, (FVS) a problem already
proven NP-complete in Karp (1972). Since we can-
not run an exhaustive search, we use a heuristic al-
gorithm which searches for a defining set by gradu-
ally eliminating low-frequency nodes whose outgo-
ing arcs lead to not yet eliminated nodes, and make
no claim that the results in Table 1 are optimal, just
that they are typical of the reduction that can be ob-
tained by modest computation. We defer discussion
of the last line to Section 4, but note that the first line
already implies that a defining set of 1,008 concepts
will cover all senses of the high frequency items in
the major Western branches of IE, and to cover the
first (primary) sense of each word in LDOCE 361
words suffice.
</bodyText>
<table confidence="0.9951969">
Dictionary #words FVS
4lang (all senses) 31,192 1,008
4lang (first senses) 3,127 361
LDOCE (all senses) 79,414 1,061
LDOCE (first senses) 34,284 376
CED (all senses) 154,061 6,490
CED (first senses) 80,495 3,435
en.wiktionary (all senses) 369,281 2,504
en.wiktionary (first senses) 304,029 1,845
formal 2,754 129
</table>
<tableCaption confidence="0.9999">
Table 1: Properties of four different dictionaries
</tableCaption>
<bodyText confidence="0.999917434782609">
While a feedback vertex set is guaranteed to ex-
ist for any digraph (if all else fails, the entire set of
vertices will do), it is not guaranteed that there ex-
ists one that is considerably smaller than the entire
graph. (For random digraphs in general see Dutta
and Subramanian 2010, for highly symmetrical lat-
tices see Zhou 2013 ms.) In random digraphs under
relatively mild conditions on the proportion of edges
relative to nodes, Łuczak and Seierstad (2009) show
that a strong component essentially the size of the
entire graph will exist. Fortunately, digraphs built
on definitions are not at all behaving in a random
fashion, the strongly connected components are rel-
atively small, as Table 1 makes evident. For ex-
ample, in the English Wiktionary, 369,281 defini-
tions can be reduced to a core set of 2,504 defin-
ing words, and in CED we can find a defining set of
6,490 words, even though these dictionaries, unlike
LDOCE, were not built using an explicit defining
set. Since LDOCE pioneered the idea of actively
limiting the defining vocabulary, it is no great sur-
prise that it has a small feedback vertex set, though
everyday users of the LDV may be somewhat sur-
</bodyText>
<page confidence="0.988049">
167
</page>
<bodyText confidence="0.999977125">
prised that less than half (1,061 items) of the full
defining set (over 2,200 items) are needed.
We also experimented with an early (pre-
COBUILD) version of the Collins English Dictio-
nary (CED), as this is more representative of the tra-
ditional type of dictionaries which didn’t rely on a
defining vocabulary. In 154,061 definitions, 65,891
words are used, but only 15,464 of these are not
headwords in LDOCE. These words appear in less
than 10% of Collins definitions, meaning that using
LDOCE as an intermediary the LDV is already suffi-
cient for defining over 90% of the CED word senses.
An example of a CED defining word missing not just
from LDV but the entire LDOCE would be aigrette
‘a long plume worn on hats or as a headdress, esp.
one of long egret feathers’.
This number could be improved to about 93%
by detail parsing of the CED definitions. For ex-
ample, aigrette actually appears as crossreference
in the definition of egret, and deleting the cross-
reference would not alter the sense of egret being
defined. The remaining cases would require bet-
ter morphological parsing of latinate terms than we
currently have access to: for now, many definitions
cannot be automatically simplified because the sys-
tem is unaware that e.g. nitrobacterium is the singu-
lar of nitrobacteria. Manually spot-checking 2% of
the remaining CED words used in definitions found
over 75% latinate technical terms, but no instances
of undefinable non-technical senses that would re-
quire extending the LDV. This is not that every sense
of every nontechnical word of English is listed in
LDOCE, but inspecting even more comprehensive
dictionaries such as the Concise Oxford Dictionary
or Webster’s 3rd makes it clear that their definitions
use largely words which are themselves covered by
LDOCE. Thus, if we see a definition such as naph-
tha ‘kinds of inflammable oil got by dry distillation
of organic substances as coal, shale, or petroleum’
we can be nearly certain that words like inflammable
which are not part of the LDV will nevertheless be
definable in terms of it, in this case as ‘materials or
substances that will start to burn very easily’.
The reduction itself is not a trivial task, in that a sim-
plified definition of naphtha such as ‘kinds of oils
that will start to burn very easily and are produced by
dry distillation ...’ can eliminate inflammable only
if we notice that the ‘oil’ in the definition of naph-
</bodyText>
<figureCaption confidence="0.999995">
Figure 1: Original definition of naphtha
Figure 2: Reduced definition of naphtha
</figureCaption>
<bodyText confidence="0.999953666666667">
tha is the ‘material or substance’ in the definition of
inflammable. Similarly, we have to understand that
‘got’ was used in the sense obtained or produced,
that dry distillation is a single concept ‘the heating
of solid materials to produce gaseous products’ that
is not built compositionally from dry and distilla-
tion in spite of being written as two separate words,
and so forth. Automated detection and resolution
of these and similar issues remain challenging NLP
tasks, but from a competence perspective it is suffi-
cient to note that manual substitution is performed
effortlessly and near-uniformly by native speakers.
</bodyText>
<subsectionHeader confidence="0.99828">
2.2 Learnability in CVS semantics
</subsectionHeader>
<bodyText confidence="0.999982166666667">
The reductive theory of vocabulary acquisition is a
highly idealized one, for surely children don’t learn
the meaning of sharp by their parents telling them
it means ‘having a thin cutting edge or point’. Yet
it is clear that computers that lack a sensory sys-
tem that would deliver intense signals upon encoun-
</bodyText>
<page confidence="0.991829">
168
</page>
<bodyText confidence="0.999993033333333">
tering sharp objects can nevertheless acquire some-
thing of the meaning by pure deduction (assuming
also that they are programmed to know that cutting
one’s body will CAUSE PAIN) and further, the domi-
nant portion of the vocabulary is not connected to di-
rect sensory signals but is learned from context (see
Chapter 6 of McKeown and Curtis 1987).
This brings us to CVS semantics, where learning
theory is idealized in a very different way, by assum-
ing that the learner has access to very large corpora,
gigaword and beyond. We must agree with Miller
and Chomsky (1963) that in real life a child exposed
to a word every second would require over 30 years
to hear gigaword amounts, but we take this to be a
reflection of the weak inferencing ability of current
statistical models, for there is nothing in the argu-
ment that says that models that are more efficient in
extracting regularities can’t learn these from orders
of magnitude less data, especially as children are
known to acquire words based on a single exposure.
For now, such one shot learning remains something
of an ideal, in that CVS systems prune infrequent
words (Collobert et al., 2011; Mikolov et al., 2013a;
Luong et al., 2013), but it is clear that both CVS
and ACR have the beginnings of a feasible theory of
learning, while the classical theory of meaning pos-
tulates offers nothing of the sort, not even for the
handful of lexical items (tense and aspect markers
in particular, see Dowty 1979) where the underlying
logic has the resources to express these.
</bodyText>
<sectionHeader confidence="0.985602" genericHeader="method">
3 Lexical relatedness
</sectionHeader>
<bodyText confidence="0.999367857142858">
Ordinary dictionary definitions can be mined to re-
cover the conceptual entailments that are at the heart
of lexical semantic competence. Whatever naphtha
is, knowing that it is inflammable is sufficient for
knowing that it will start to burn easily. It is a major
NLP challenge to make this deduction (Dagan et al.
2006), but ACR can store the information trivially
and make the inference by spreading activation.
We implemented one variant of the ACR theory of
word meaning by a network of Eilenberg machines
(Eilenberg, 1974) corresponding to elements of the
reduced vocabulary. Eilenberg machines are a sim-
ple generalization of the better known finite state au-
tomata (FSA) and transducers (FSTs) that have be-
come standard since Koskenniemi (1983) in describ-
ing the rule-governed aspects of the lexicon, mor-
photactics and morphophonology (Huet and Razet,
2008; Kornai, 2010). The methods we use for defin-
ing word senses (concepts) are long familiar from
Knowledge Representation. We assume the reader is
familiar with the knowledge representation literature
(for a summary, see Brachman and Levesque 2004),
and describe only those parts of the system that dif-
fer from the mainstream assumptions. In particular,
we collapse attribution, unary predication, and TS A
links in a single link type ‘0’ (as in Figs. 1-2 above)
and have only two other kinds of links to distinguish
the arguments of transitive verbs, ‘1’ corresponding
to subject/agent; and ‘2’ to object/patient. The treat-
ment of other link types, be they construed as gram-
matical functions or as deep cases or even thematic
slots, is deferred to Section 4.
By creating graphs for all LDOCE headwords
based on dependency parses of their definitions (the
‘literal’ network of Table 1) using the unlexicalized
version of the Stanford Dependency Parser (Klein
and Manning, 2003), we obtained measures of lex-
ical relatedness by defining various similarity met-
rics over pairs of such graphs. The intuition under-
lying all these metrics is that two words are seman-
tically similar if their definitions overlap in (i) the
concepts present in their definitions (e.g. the def-
inition of both train and car will make reference
to the concept vehicle) and (ii) the binary relations
they take part in (e.g. both street and park are TN
town). While such a measure of semantic similar-
ity builds more on manual labor (already performed
by the lexicographers) than those gained from state-
of-the-art CVS systems, recently the results from
the ‘literal’ network have been used in a competi-
tive system for measuring semantic textual similar-
ity (Recski and ´Acs, 2015). In Section 4 we dis-
cuss the ‘formal’ network of Table 1 built directly
on the concept formulae. By spectral dimension re-
duction of the incidence matrix of this network we
can create an embedding that yields results on world
similarity tasks comparable to those obtained from
corpus-based embeddings (Makrai et al., 2013).
CVS models can be explicitly tested on their abil-
ity to recover synonymy by searching for the near-
est word in the sample (Mikolov et al., 2013b);
antonymy by reversing the sign of the vector (Zweig,
2014); and in general for all kinds of analogical
</bodyText>
<page confidence="0.995049">
169
</page>
<bodyText confidence="0.999913461538462">
statements such as king is to queen as man is to
woman by vector addition and subtraction (Mikolov
et al., 2013c); not to speak of cross-language
paraphrase/translation (Schwenk et al., 2012), long
viewed a key intermediary step toward explaining
competence in a foreign language.
Currently, CVS systems are clearly in the lead on
such tasks, and it is not clear what, if anything, can
be salvaged from the truth-conditional approach to
these matters. At the same time, the CVS approach
to quantifiers is not mature, and ACR theories sup-
port generics only. These may look like backward
steps, but keep in mind that our goal in compe-
tence modeling is to characterize everyday knowl-
edge, shared by all competent speakers of the lan-
guage, while quantifier and modal scope ambiguities
are something that ordinary speakers begin to appre-
ciate only after considerable schooling in these mat-
ters, with significant differences between the naive
(preschool) and the learned adult systems (´E. Kiss
et al., 2013). On the traditional account, only sub-
sumption (IS A or ‘0’) links can be easily recovered
from the meaning postulates, the cognitively central
similarity (as opposed to exact synonymy) relations
receive no treatment whatsoever, since similarity of
meaning postulates is undefined.
</bodyText>
<sectionHeader confidence="0.994201" genericHeader="method">
4 Lexical lookup
</sectionHeader>
<bodyText confidence="0.985894705882353">
The interaction with compositional semantics is a
key issue for any competence theory of lexical se-
mantics. In the classical formal system, this is han-
dled by a mechanism of lexical lookup that substi-
tutes the meaning postulates at the terminal nodes of
the derivation tree, at the price of introducing some
lexical redundancy rule that creates the intensional
meaning of each word, including the evidently non-
intensional ones, based on the meaning postulates
that encode the extensional meaning. (Ch. 19.2 of
Jacobson (2014) sketches an alternative treatment,
which keeps intensionality for the intended set of
cases.) While there are considerable technical diffi-
culties of formula manipulation involved, this is re-
ally one area where the classical theory shines as a
competence theory – we cannot even imagine to cre-
ate a learning algorithm that would cover the mean-
ing of infinitely many complex expressions unless
we had some means of combining the meanings of
the lexical entries.
CVS semantics offers several ways of combining
lexical entries, the simplest being simply adding the
vectors together (Mitchell and Lapata, 2008), but
the use of linear transformations (Lazaridou et al.,
2013) and tensor products (Smolensky, 1990) has
also been contemplated. Currently, an approach that
combines the vectors of the parts to form the vec-
tor of the whole by recurrent neural nets appears to
work best (Socher et al., 2013), but this is still an
area of intense research and it would be premature
to declare this method the winner. Here we concen-
trate on ACR, investigating the issue of the inventory
of graph edge colors on the same core vocabulary as
discussed above. The key technical problem is to
bring the variety of links between verbs and their
arguments under control: as Woods (1975) already
notes, the naive ACR theories are characterized by a
profusion of link types (graph edge colors).
We created a version of ACR that is limited to
three link types. Both the usual network represen-
tations (digraphs, as in Figs. 1 and 2 above) and
a more algebraic model composed of extended fi-
nite state automata (Eilenberg machines) are pro-
duced by parsing formulas defined by a formal
grammar summarized in Figure 3. For ease of read-
ing, in unary predication (e.g. mouse →− rodent)
0
we permit both prefix and suffix order, but with
different kinds of parens mouse[rodent] and
rodent(mouse); and we use infix notation (cow
MAKE milk) for transitives (cow ←− MAKE 2
</bodyText>
<equation confidence="0.733024">
1 →−
</equation>
<bodyText confidence="0.9960069375">
milk, link types ‘1’ and ‘2’).
The right column of Figure 3 shows the digraph
obtained from parsing the formula on the right hand
hand side of the grammar rules. There are no ‘3’
or higher links, as ditransitives like x give y to z are
decomposed at the semantic level into unary and bi-
nary atoms, in this case CAUSE and HAVE, ‘x cause
(z have y)’, see Kornai (2012) for further details. A
digraph representing the whole lexicon was built in
two steps: first, every clause in definitions was man-
ually translated to a formula (which in turns is au-
tomatically translated into a digraph), then the di-
graphs were connected by unifying nodes that have
the same label and no outgoing edges.
The amount of manual labor involved was con-
siderably lessened by the method of Section 3 that
</bodyText>
<page confidence="0.857627">
170
</page>
<equation confidence="0.998857333333333">
g(E) ... g(E)
E → En  |Ef
E → Cn(Ef) g(Ef)
0
V
g(Cn)
Ef → ACf g(Cf)
g(A(1)) g(A(2))
Ef → Cf’ g(Cf)
</equation>
<bodyText confidence="0.97894665">
finds the feedback vertex set, in that once such a
set is given, the rest could be built automatically.
This gives us a means of investigating the prevalence
of what would become different deep cases (colors,
link types) in other KR systems. Deep cases are dis-
tinguishers that mediate between the purely seman-
tic (theta) link types and the surface case/adposition
system. We have kept our system of deep cases
rather standard, both in the sense of representing a
common core among the many proposals starting
with Gruber (1965) and Fillmore (1968) and in the
sense of aiming at universality, a subject we defer to
the next section. The names and frequency of use in
the core vocabulary are given in Table 2. The results
are indicative of a primary (agent/patient, what we
denote ‘1’/‘2’), a secondary (DAT/REL/POSS), and
a tertiary (locative) layer in deep cases – how these
are mapped on language-specific (surface) cases will
be discussed in Section 5.
freq abbreviation comment
</bodyText>
<figure confidence="0.995230111111111">
487 AGT agent
388 PAT patient
34 DAT dative
82 REL root or adpositional object
70 POSS default for relational nouns
20 TO target of action
15 FROM source of action
3 AT location of action
0
Al
��
D → E(,E)* d
0
2
1
Al
g(A) d
Ef → CfA g(Cf)
2
�� ��
d
Ef → A(1)CfA(2) g(Cf)
��
1
��
2
1
g(A)
��
1
��
d
Ef → ’Cf g(Cf)
2
Al
d
</figure>
<equation confidence="0.898431631578947">
Ef → En
En → Cn
En → Cn[D] g(Cn)
0
V
g(D)
En → C(1)
n (C(2) g(C(2)
n ) n )
0
V
g(C(1)
n )
A → En  |[D]
Cn → AGT  |PAT  |POSS  |REL  |DAT  |FROM  |TO  |AT
Cn → root  |@url
E → &lt;C&gt;, A → &lt;A&gt;
Cn → [a-z-]+(/[0-9]+)
Cf → [A-Z ]+(/[0-9]+)
</equation>
<figureCaption confidence="0.97696">
Figure 3: The syntax of the definitions
</figureCaption>
<tableCaption confidence="0.570357">
Table 2: Deep cases
</tableCaption>
<bodyText confidence="0.9999576875">
To avoid problems with multiple word senses
and with constructional meaning (as in dry dis-
tillation or dry martini) we defined each entry in
this formal language (keeping different word senses
such as light/739 ‘the opposite of dark’ and
light/1381 ‘the opposite of heavy’ distinct by
disambiguation indexes) and built a graph directly
on the resulting conceptual network rather than the
original LDOCE definitions. The feedback ver-
tex set algorithm uroboros.py determined that
a core set of 129 concepts are sufficient to define
the others in the entire concept dictionary, and thus
for the entire LDOCE or similar dictionaries such as
CED or Webster’s 3rd. This upper bound is so close
to the NSM lower bound of 65 that a blow-by-blow
comparison would be justified.
</bodyText>
<page confidence="0.998586">
171
</page>
<sectionHeader confidence="0.994904" genericHeader="evaluation">
5 Universality
</sectionHeader>
<bodyText confidence="0.999990762500001">
The final issue one needs to investigate in assess-
ing the potential of any purported competence the-
ory is that of universality versus language particu-
larity. For CVS theories, this is rather easy: we have
one system of representation, finite dimensional vec-
tor spaces, which admits no typological variation,
let alone language-specific mechanisms – one size
fits all. As linguists, we see considerable variation
among the surface, and possibly even among the
deeper aspects of case linking (Smith, 1996), but as
computational modelers we lack, as of yet, a better
understanding of what corresponds to such mecha-
nisms within CVS semantics.
ACR systems are considerably more transparent
in this regard, and the kind of questions that we
would want to pose as linguists have direct re-
flexes in the formal system. Many of the original
theories of conceptual representation were English-
particular, sometimes to the point of being as naive
as the medieval theories of universal language (Eco,
1995). The most notable exception is NSM, clearly
developed with the native languages of Australia in
mind, and often exercised on Russian, Polish, and
other IE examples as well. Here we follow the spirit
of GFRG (Ranta, 2011) in assuming a common ab-
stract syntax for all languages. For case grammar
this requires some abstraction, for example English
NPs must also get case marked (an idea also present
in the ‘Case Theory’ of Government-Binding and re-
lated theories of transformational grammar). The
main difference between English and the overtly
case-marking languages such as Russian or Latin is
that in English we compute the cases from preposi-
tions and word order (position relative to the verb)
rather than from overt morphological marking as
standard. This way, the lexical entries can be kept
highly abstract, and for the most part, universal.
Thus the verb go will have a source and a goal.
For every language there is a langspec compo-
nent of the lexicon which stores e.g. for English the
information that source is expressed by the preposi-
tion from and destination by to. For Hungarian the
langspec file stores the information that source
can be linked by delative, elative, and ablative; goal
by illative, sublative, or terminative. Once this
kind of language-specific variation is factored out,
the go entry becomes before AT src, after
AT goal. The same technique is used to encode
both lexical entries and constructions in the sense
of Berkeley Construction Grammar (CxG, see Gold-
berg 1995).
Whether two constructions (in the same language
or two different languages) have to be coded by dif-
ferent deep cases is measured very badly, if at all,
by the standard test suits used e.g. in paraphrase de-
tection or question answering, and we would need
to invest serious effort in building new test suites.
For example, the system sketched above uses the
same deep case, REL, for linking objects that are
surface marked by quirky case and for arguments
of predicate nominals. Another example is the da-
tive/experiencer/beneficent family. Whether the ex-
periencer cases familiar from Korean and elsewhere
can be subsumed under the standard dative role (Fill-
more, 1968) is an open question, but one that can
at least be formulated in ACR. Currently we dis-
tinguish the dative DAT from possessive marking
POSS, generally not considered a true case but quite
prevalent in this function language after language:
consider English (the) root of a tree, or Polish ko-
rzen drzewa. This is in contrast to the less fre-
quent cases like (an excellent) occasion for mar-
tyrdom marked by obliques (here the preposition
for). What these nouns (occasion, condition, rea-
son, need) have in common is that the related word
is goal of the definiendum in some sense. In these
cases we use TO rather than POSS, a decision with
interesting ramifications elsewhere in the system,
but currently below the sensitivity of the standard
test sets.
</bodyText>
<sectionHeader confidence="0.995036" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999959">
It is not particularly surprising that both CVS and
ACR, originally designed as performance theories,
fare considerably better in the performance realm
than Montagovian semantics, especially as detailed
intensional lexica have never been crafted, and
Dowty (1979) remains, to this day, the path not taken
in formal semantics. It is only on the subdomain
of the logic puzzles involving Booleans and quan-
tification that Montagovian solutions showed any
promise, and these, with the exception of elemen-
tary negation, do not even appear in more down to
</bodyText>
<page confidence="0.995166">
172
</page>
<bodyText confidence="0.99998116">
earth evaluation sets such as (Weston et al., 2015).
The surprising conclusion of our work is that stan-
dard Montagovian semantics also falls short in the
competence realm, where the formal theory has long
been promoted as offering psychological reality.
We have compared CVS and ACR theories of lex-
ical semantics to the classical approach based on
meaning postulates by the usual criteria for compe-
tence theories. In Section 2 we have seen that both
ACR and CVS are better in terms of learnability than
the standard formal theory, and it is worth noting that
the number of ACR primitives, 129 in the version
implemented here, is less than the dimensions of
the best performing CVS embeddings, 150-300 af-
ter data compression by PCA or similar methods. In
Section 3 we have seen that lexical relatedness tasks
also favor ACR and CVS over the meaning postulate
approach (for a critical overview of meaning postu-
lates in model-theoretic semantics see Zimmermann
1999), and in Section 4 we have seen that composi-
tionality poses no problems for ACR. How compo-
sitional semantics is handled in CVS semantics re-
mains to be seen, but the problem is not a dearth of
plausible mechanisms, but rather an overabundance
of these.
</bodyText>
<sectionHeader confidence="0.987295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999821581395349">
The 4lang conceptual dictionary is the work of
many people over the years. The name is no longer
quite justified, in that natural language bindings, au-
tomatically generated and thus not entirely free of
errors and omissions, now exist for 50 languages
( ´Acs et al., 2013), many of them outside the Indoeu-
ropean and Finnougric families originally targeted.
More important, formal definitions of the concepts
that rely on less than a dozen theoretical primitives
(including the three link types) and only 129 core
concepts, are now available for all the concepts. So
far, only the theoretical underpinnings of this formal
system have been fully described in English (Kornai,
2010; Kornai, 2012), with many details presented
only in Hungarian (Kornai and Makrai, 2013), but
the formal definitions, and the parser that can build
both graphs and Eilenberg machines from these,
are now available as part of the kornai/4lang
github repository. These formal definitions were
written primarily by Makrai, with notable contribu-
tions by Recski and Nemeskey.
The system has been used as an experimental plat-
form for a variety of purposes, including quantita-
tive analysis of deep cases by Makrai, who devel-
oped the current version of the deep case system
with Nemeskey (Makrai, 2015); for defining lexi-
cal relatedness (Makrai et al., 2013; Recski and ´Acs,
2015); and in this paper, for finding the definitional
core, the feedback vertex set.
´Acs wrote the first version of the feedback ver-
tex set finder which was adapted to our data by
´Acs and Pajkossy, who also took part in the com-
putational experiments, including preprocessing the
data, adapting the vertex set finder, and running
the experiments. Recski created the pipeline in
the http://github/kornai/4lang reposi-
tory that builds formal definitions from English dic-
tionary entries. Kornai advised and wrote the paper.
We are grateful to Attila Zs´eder (HAS Linguistics
Institute) for writing the original parser for the for-
mal language of definitions and to Andr´as Gy´arf´as
(HAS R´enyi Institute) for help with feedback vertex
sets. Work supported by OTKA grant #82333.
</bodyText>
<sectionHeader confidence="0.998098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995915625">
Judit ´Acs, Katalin Pajkossy, and Andr´as Kornai. 2013.
Building basic vocabulary across 40 languages. In
Proceedings of the Sixth Workshop on Building and
Using Comparable Corpora, pages 52–58, Sofia, Bul-
garia, August. ACL.
Avery Andrews. 2015. Reconciling NSM and formal
semantics. ms, pages v2, jan 2015.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511–522.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155.
Branimir K. Boguraev and Edward J. Briscoe. 1989.
Computational Lexicography for Natural Language
Processing. Longman.
R.J. Brachman and H. Levesque. 2004. Knowledge Rep-
resentation and reasoning. Morgan Kaufman Elsevier,
Los Altos, CA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadelphia.
Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press.
</reference>
<page confidence="0.995513">
173
</page>
<reference confidence="0.999266859813084">
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research (JMLR).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges. Evalu-
ating Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, volume
3944 of LNCS, pages 177–190. Springer.
David Dowty. 1979. Word Meaning and Montague
Grammar. Reidel, Dordrecht.
Kunal Dutta and C. R. Subramanian. 2010. Induced
acyclic subgraphs in random digraphs: Improved
bounds. In Discrete Mathematics and Theoretical
Computer Science, pages 159–174.
Katalin ´E. Kiss, M´aty´as Ger¨ocs, and Tam´as Z´et´enyi.
2013. Preschoolers interpretation of doubly quantified
sentences. Acta Linguistica Hungarica, 60:143–171.
Umberto Eco. 1995. The Search for the Perfect Lan-
guage. Blackwell, Oxford.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines, volume A. Academic Press.
Charles Fillmore and Sue Atkins. 1998. Framenet and
lexicographic relevance. In Proceedings of the First
International Conference on Language Resources and
Evaluation, Granada, Spain.
Charles Fillmore. 1968. The case for case. In E. Bach
and R. Harms, editors, Universals in Linguistic The-
ory, pages 1–90. Holt and Rinehart, New York.
John R. Firth. 1957. A synopsis of linguistic theory. In
Studies in linguistic analysis, pages 1–32. Blackwell.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. Uni-
versity of Chicago Press.
Andrew S. Gordon, Jerry R. Hobbs, and Michael T. Cox.
2011. Anthropomorphic self-models for metareason-
ing agents. In Michael T. Cox and Anita Raja, editors,
Metareasoning: Thinking about Thinking, pages 295–
305. MIT Press.
Paul Grice and Peter Strawson. 1956. In defense of a
dogma. The Philosophical Review, 65:148–152.
Jeffrey Steven Gruber. 1965. Studies in lexical relations.
Ph.D. thesis, Massachusetts Institute of Technology.
J.R. Hobbs. 2008. Deep lexical semantics. Lecture
Notes in Computer Science, 4919:183.
G´erard Huet and Benoit Razet. 2008. Computing with
relational machines. In Tutorial at ICON, Dec 2008.
Pauline Jacobson. 2014. Compositional Semantics. Ox-
ford University Press.
Richard M. Karp. 1972. Reducibility among combinato-
rial problems. In R. Miller and J.W. Thatcher, editors,
Complexity of Computer Computations, pages 85–104.
Plenum Press, New York.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class based construction of a verb lexicon. In
AAAI-2000 Seventeenth National Conference on Arti-
ficial Intelligence, Austin, TX.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430.
Andr´as Kornai and M´arton Makrai. 2013. A 4lang fo-
galmi sz´ot´ar. In Attila Tan´acs and Veronika Vincze,
editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konfer-
encia, pages 62–70.
Andr´as Kornai. 2010. The algebra of lexical seman-
tics. In Christian Ebert, Gerhard J¨ager, and Jens
Michaelis, editors, Proceedings of the 11th Mathemat-
ics of Language Workshop, LNAI 6149, pages 174–
199. Springer.
Andr´as Kornai. 2012. Eliminating ditransitives. In Ph.
de Groote and M-J Nederhof, editors, Revised and Se-
lected Papers from the 15th and 16th Formal Grammar
Conferences, LNCS 7395, pages 243–261. Springer.
Kimmo Koskenniemi. 1983. Two-level model for mor-
phological analysis. In Proceedings of IJCAI-83,
pages 683–685.
Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli,
and Marco Baroni. 2013. Compositional-ly derived
representations of morphologically complex words in
distributional semantics. In ACL (1), pages 1517–
1526.
Tomasz Łuczak and Taral Guldahl Seierstad. 2009. The
critical behavior of random digraphs. Random Struc-
tures and Algorithms, 35:271–293.
Thang Luong, Richard Socher, and Christopher D. Man-
ning. 2013. Better word representations with re-
cursive neural networks for morphology. In CoNLL,
pages 104–113.
M´arton Makrai, D´avid M´ark Nemeskey, and Andr´as Ko-
rnai. 2013. Applicative structure in vector space
models. In Proceedings of the Workshop on Contin-
uous Vector Space Models and their Compositionality,
pages 59–63, Sofia, Bulgaria, August. ACL.
M´arton Makrai. 2015. Deep cases in the �4lang
conceptlexicon. In Attila Tancs, Viktor Varga,
and Veronika Vincze, editors, X. Magyar Szmtgpes
Nyelvszeti Konferencia (MSZNY 2014), pages 50–57
(in Hungarian), 387 (English abstract).
Margaret G. McKeown and Mary E. Curtis. 1987. The
nature of vocabulary acquisition. Lawrence Erlbaum
Associates.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Y. Bengio and Y. LeCun,
editors, Proc. ICLR 2013.
</reference>
<page confidence="0.985144">
174
</page>
<reference confidence="0.999845537735849">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositionality.
In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 26, pages 3111–
3119. Curran Associates, Inc.
Tomas Mikolov, Wen-tau Yih, and Zweig Geoffrey.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-HLT-
2013, pages 746–751.
George A. Miller and Noam Chomsky. 1963. Finitary
models of language users. In R.D. Luce, R.R. Bush,
and E. Galanter, editors, Handbook of Mathematical
Psychology, pages 419–491. Wiley.
George A. Miller. 1995. Wordnet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, Ohio. As-
sociation for Computational Linguistics.
T. M. Mitchell, S.V. Shinkareva, A. Carlson, K.M.
Chang, V.L. Malave, R.A. Mason, and M.A. Just.
2008. Predicting human brain activity associated with
the meanings of nouns. Science, 320(5880):1191.
C.K. Ogden. 1944. Basic English: A General Intro-
duction with Rules and Grammar. Psyche miniatures:
General Series. Kegan Paul, Trench, Trubner.
Charles E. Osgood, William S. May, and Murray S.
Miron. 1975. Cross Cultural Universals of Affective
Meaning. University of Illinois Press.
Barbara H. Partee. 1979. Semantics - mathematics or
psychology? In R. Bauerl, U. Egli, and A. von Ste-
chow, editors, Semantics from Different Points of View,
pages 1–14. Springer-Verlag, Berlin.
Barbara Partee. 2013. Changing perspectives on the
‘mathematics or psychology’ question. In Philosophy
Wkshp on “Semantics Mathematics or Psychology?”.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP 2014).
H. Putnam. 1976. Two dogmas revisited. printed in his
(1983) Realism and Reason, Philosophical Papers, 3.
M. Ross Quillian. 1968. Word concepts: A theory and
simulation of some basic semantic capabilities. Be-
havioral Science, 12:410–430.
Willard van Orman Quine. 1951. Two dogmas of em-
piricism. The Philosophical Review, 60:20–43.
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.
G´abor Recski and Judit ´Acs. 2015. MathLingBudapest:
Concept networks for semantic similarity. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015), Denver, CO, June. ACL.
Roger C. Schank. 1972. Conceptual dependency: A the-
ory of natural language understanding. Cognitive Psy-
chology, 3(4):552–631.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a gpu for statistical machine transla-
tion. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages
11–19. Association for Computational Linguistics.
John M. Sinclair. 1987. Looking up: an account of the
COBUILD project in lexical computing. Collins ELT.
Henry Smith. 1996. Restrictiveness in Case Theory.
Cambridge University Press.
Paul Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Artificial intelligence, 46(1):159–
216.
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. D. Man-
ning, and A. Y. Ng. 2013. Zero-shot learning through
cross-modal transfer. In International Conference on
Learning Representations (ICLR).
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for Computa-
tional Linguistics.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
arXiv:1502.05698.
William Dwight Whitney. 1885. The roots of the San-
skrit language. Transactions of the American Philo-
logical Association (1869-1896), 16:5–29.
Anna Wierzbicka. 1985. Lexicography and conceptual
analysis. Karoma, Ann Arbor.
William A. Woods. 1975. What’s in a link: Founda-
tions for semantic networks. Representation and Un-
derstanding: Studies in Cognitive Science, pages 35–
82.
Hai-Jun Zhou. 2013. Spin glass approach
to the feedback vertex set problem. ms,
arxiv.org/pdf/1307.6948v2.pdf.
Thomas E. Zimmermann. 1999. Meaning postulates and
the model-theoretic approach to natural language se-
mantics. Linguistics and Philosophy, 22:529–561.
Geoffrey Zweig. 2014. Explicit representation of
antonymy in language modeling. Technical report,
Microsoft Research.
</reference>
<page confidence="0.998709">
175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591244">
<title confidence="0.99666">Competence in lexical semantics</title>
<author confidence="0.998998">Andr´as Kornai Judit ´Acs M´arton Makrai</author>
<affiliation confidence="0.9722795">Institute for Computer Science Dept of Automation and Institute for Hungarian Academy of Sciences Applied Informatics, BUTE Hungarian Academy of Sciences</affiliation>
<address confidence="0.979976">Kende u. 13-17 Magyar Tud´osok krt. 2 Bencz´ur u. 33 1111 Budapest, Hungary 1117 Budapest, Hungary 1068 Budapest, Hungary</address>
<email confidence="0.96347">andras@kornai.comjudit@aut.bme.humakrai@nytud.hu</email>
<author confidence="0.955166">D´avid Nemeskey Katalin Pajkossy G´abor Recski</author>
<affiliation confidence="0.9620975">Faculty of Informatics Department of Algebra Institute for Linguistics E¨otv¨os Lor´and University BUTE Hungarian Academy of Sciences</affiliation>
<address confidence="0.974412">P´azm´any P´eter s´et´any 1/C Egry J. u. 1 Bencz´ur u. 33 1117 Budapest, Hungary 1111 Budapest, Hungary 1068 Budapest,</address>
<email confidence="0.99558">nemeskeyd@gmail.compajkossy@math.bme.hurecski@mokk.bme.hu</email>
<abstract confidence="0.9931965">We investigate from the competence standpoint two recent models of lexical semantics, algebraic conceptual representations and continuous vector models. Characterizing what it means for a speaker to be competent in lexical semantics remains perhaps the most significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003;</abstract>
<intro confidence="0.807171">Turian et al., 2010; Pennington et al., 2014). After a</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Judit ´Acs</author>
<author>Katalin Pajkossy</author>
<author>Andr´as Kornai</author>
</authors>
<title>Building basic vocabulary across 40 languages. In</title>
<date>2013</date>
<booktitle>Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,</booktitle>
<pages>52--58</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<marker>´Acs, Pajkossy, Kornai, 2013</marker>
<rawString>Judit ´Acs, Katalin Pajkossy, and Andr´as Kornai. 2013. Building basic vocabulary across 40 languages. In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora, pages 52–58, Sofia, Bulgaria, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avery Andrews</author>
</authors>
<title>Reconciling NSM and formal semantics. ms,</title>
<date>2015</date>
<pages>2</pages>
<contexts>
<context position="7897" citStr="Andrews, 2015" startWordPosition="1225" endWordPosition="1226"> close to formulating a robust theory of acquisition, and for intensions, infinite information objects encoding the meaning in the formal theory, it is not at all clear whether such a learning algorithm is even possible. 2.1 The basic vocabulary The idea that there is a small set of conceptual primitives for building semantic representations has a long history both in linguistics and AI as well as in language teaching. The more theory-oriented systems, such as Conceptual Dependency and NSM assume only a few dozen primitives, but have a disquieting tendency to add new elements as time goes by (Andrews, 2015). In contrast, the systems intended for teaching and communication, such as Basic English (Ogden, 1944) start with at least a thousand primitives, and assume that these need to be further supplemented by technical terms from various domains. Since the obvious learning algorithm based on any such reductive system is one where the primi166 tives are assumed universal (and possibly innate, see Section 5), and the rest is learned by reduction to the primitives, we performed a series of ‘ceiling’ experiments aiming at a determination of how big the universal/innate component of the lexicon must be.</context>
</contexts>
<marker>Andrews, 2015</marker>
<rawString>Avery Andrews. 2015. Reconciling NSM and formal semantics. ms, pages v2, jan 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
</authors>
<title>Composition in distributional semantics.</title>
<date>2013</date>
<journal>Language and Linguistics Compass,</journal>
<volume>7</volume>
<issue>10</issue>
<contexts>
<context position="5952" citStr="Baroni (2013)" startWordPosition="901" endWordPosition="902">t we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standardly used nominal features (Kornai, 2010). Continuous vector space (CVS) is also not a single model but a rich family of models, generally based on what Baroni (2013) calls the distributional hypothesis, that semantically similar items have similar distribution. This idea, going back at least to Firth (1957) is not at all trivial to defend, and not just because defining ‘semantically similar’ is a challenging task: as we shall see, there are significant design choices involved in defining similarity of vectors as well. To the extent CVS representations are primarily used in artificial neural net models, it may be helpful to consider the state of a network being described by the vector whose nth coordinate gives the activation level of the nth neuron. Under</context>
</contexts>
<marker>Baroni, 2013</marker>
<rawString>Marco Baroni. 2013. Composition in distributional semantics. Language and Linguistics Compass, 7(10):511–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1801" citStr="Bengio et al., 2003" startWordPosition="250" endWordPosition="253"> (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with signific</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir K Boguraev</author>
<author>Edward J Briscoe</author>
</authors>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<publisher>Longman.</publisher>
<contexts>
<context position="4600" citStr="Boguraev and Briscoe, 1989" startWordPosition="688" endWordPosition="692">and the intension that would represent the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more </context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Branimir K. Boguraev and Edward J. Briscoe. 1989. Computational Lexicography for Natural Language Processing. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brachman</author>
<author>H Levesque</author>
</authors>
<title>Knowledge Representation and reasoning.</title>
<date>2004</date>
<publisher>Morgan Kaufman Elsevier,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="18807" citStr="Brachman and Levesque 2004" startWordPosition="3068" endWordPosition="3071">network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word senses (concepts) are long familiar from Knowledge Representation. We assume the reader is familiar with the knowledge representation literature (for a summary, see Brachman and Levesque 2004), and describe only those parts of the system that differ from the mainstream assumptions. In particular, we collapse attribution, unary predication, and TS A links in a single link type ‘0’ (as in Figs. 1-2 above) and have only two other kinds of links to distinguish the arguments of transitive verbs, ‘1’ corresponding to subject/agent; and ‘2’ to object/patient. The treatment of other link types, be they construed as grammatical functions or as deep cases or even thematic slots, is deferred to Section 4. By creating graphs for all LDOCE headwords based on dependency parses of their definitio</context>
</contexts>
<marker>Brachman, Levesque, 2004</marker>
<rawString>R.J. Brachman and H. Levesque. 2004. Knowledge Representation and reasoning. Morgan Kaufman Elsevier, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<location>Philadelphia. Noam Chomsky.</location>
<contexts>
<context position="9133" citStr="Brants and Franz, 2006" startWordPosition="1433" endWordPosition="1436">wer bound is given by the current size of the NSM inventory, 65 (Andrews, 2015), but as long as we don’t have the complete lexicon of at least one language defined in NSM terms the reductivity of the system remains in doubt. For English, a Germanic language, the first provably reductive system is the Longman Defining Vocabulary (LDV), some 2,200 items, which provide a sufficient basis for defining all entries in LDOCE (using English syntax in the definitions). Our work started with a superset of the LDV that was obtained by adding the most frequent words according to the Google unigram count (Brants and Franz, 2006) and the BNC, as well as the most frequent words from a Slavic, a Finnougric, and a Romance language (Polish, Hungarian, and Latin), and Whitney (1885) to form the 4lang conceptual dictionary, with the long-term design goal of eventually providing reductive definitions for the vocabularies of all Old World languages. ´Acs et al. (2013) describes how bindings in other languages can be created automatically and compares the reductive method to the familiar term- and document-frequency based searches for core vocabulary. This superset of LDV, called ‘4lang’ in Table 1 below, can be considered a d</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia. Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="17301" citStr="Collobert et al., 2011" startWordPosition="2824" endWordPosition="2827">ust agree with Miller and Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficien</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="18020" citStr="Dagan et al. 2006" startWordPosition="2947" endWordPosition="2950">gs of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it will start to burn easily. It is a major NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word sense</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, volume 3944 of LNCS, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar.</title>
<date>1979</date>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="17614" citStr="Dowty 1979" startWordPosition="2882" endWordPosition="2883">efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it will start to burn easily. It is a major NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eil</context>
<context position="32143" citStr="Dowty (1979)" startWordPosition="5328" endWordPosition="5329">(here the preposition for). What these nouns (occasion, condition, reason, need) have in common is that the related word is goal of the definiendum in some sense. In these cases we use TO rather than POSS, a decision with interesting ramifications elsewhere in the system, but currently below the sensitivity of the standard test sets. 6 Conclusion It is not particularly surprising that both CVS and ACR, originally designed as performance theories, fare considerably better in the performance realm than Montagovian semantics, especially as detailed intensional lexica have never been crafted, and Dowty (1979) remains, to this day, the path not taken in formal semantics. It is only on the subdomain of the logic puzzles involving Booleans and quantification that Montagovian solutions showed any promise, and these, with the exception of elementary negation, do not even appear in more down to 172 earth evaluation sets such as (Weston et al., 2015). The surprising conclusion of our work is that standard Montagovian semantics also falls short in the competence realm, where the formal theory has long been promoted as offering psychological reality. We have compared CVS and ACR theories of lexical semanti</context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>David Dowty. 1979. Word Meaning and Montague Grammar. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kunal Dutta</author>
<author>C R Subramanian</author>
</authors>
<title>Induced acyclic subgraphs in random digraphs: Improved bounds.</title>
<date>2010</date>
<booktitle>In Discrete Mathematics and Theoretical Computer Science,</booktitle>
<pages>159--174</pages>
<contexts>
<context position="11803" citStr="Dutta and Subramanian 2010" startWordPosition="1892" endWordPosition="1895">VS 4lang (all senses) 31,192 1,008 4lang (first senses) 3,127 361 LDOCE (all senses) 79,414 1,061 LDOCE (first senses) 34,284 376 CED (all senses) 154,061 6,490 CED (first senses) 80,495 3,435 en.wiktionary (all senses) 369,281 2,504 en.wiktionary (first senses) 304,029 1,845 formal 2,754 129 Table 1: Properties of four different dictionaries While a feedback vertex set is guaranteed to exist for any digraph (if all else fails, the entire set of vertices will do), it is not guaranteed that there exists one that is considerably smaller than the entire graph. (For random digraphs in general see Dutta and Subramanian 2010, for highly symmetrical lattices see Zhou 2013 ms.) In random digraphs under relatively mild conditions on the proportion of edges relative to nodes, Łuczak and Seierstad (2009) show that a strong component essentially the size of the entire graph will exist. Fortunately, digraphs built on definitions are not at all behaving in a random fashion, the strongly connected components are relatively small, as Table 1 makes evident. For example, in the English Wiktionary, 369,281 definitions can be reduced to a core set of 2,504 defining words, and in CED we can find a defining set of 6,490 words, e</context>
</contexts>
<marker>Dutta, Subramanian, 2010</marker>
<rawString>Kunal Dutta and C. R. Subramanian. 2010. Induced acyclic subgraphs in random digraphs: Improved bounds. In Discrete Mathematics and Theoretical Computer Science, pages 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katalin ´E Kiss</author>
<author>M´aty´as Ger¨ocs</author>
<author>Tam´as Z´et´enyi</author>
</authors>
<title>Preschoolers interpretation of doubly quantified sentences. Acta Linguistica Hungarica,</title>
<date>2013</date>
<pages>60--143</pages>
<marker>Kiss, Ger¨ocs, Z´et´enyi, 2013</marker>
<rawString>Katalin ´E. Kiss, M´aty´as Ger¨ocs, and Tam´as Z´et´enyi. 2013. Preschoolers interpretation of doubly quantified sentences. Acta Linguistica Hungarica, 60:143–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umberto Eco</author>
</authors>
<title>The Search for the Perfect Language.</title>
<date>1995</date>
<location>Blackwell, Oxford.</location>
<contexts>
<context position="28950" citStr="Eco, 1995" startWordPosition="4803" endWordPosition="4804">we see considerable variation among the surface, and possibly even among the deeper aspects of case linking (Smith, 1996), but as computational modelers we lack, as of yet, a better understanding of what corresponds to such mechanisms within CVS semantics. ACR systems are considerably more transparent in this regard, and the kind of questions that we would want to pose as linguists have direct reflexes in the formal system. Many of the original theories of conceptual representation were Englishparticular, sometimes to the point of being as naive as the medieval theories of universal language (Eco, 1995). The most notable exception is NSM, clearly developed with the native languages of Australia in mind, and often exercised on Russian, Polish, and other IE examples as well. Here we follow the spirit of GFRG (Ranta, 2011) in assuming a common abstract syntax for all languages. For case grammar this requires some abstraction, for example English NPs must also get case marked (an idea also present in the ‘Case Theory’ of Government-Binding and related theories of transformational grammar). The main difference between English and the overtly case-marking languages such as Russian or Latin is that</context>
</contexts>
<marker>Eco, 1995</marker>
<rawString>Umberto Eco. 1995. The Search for the Perfect Language. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Eilenberg</author>
</authors>
<date>1974</date>
<booktitle>Automata, Languages, and Machines,</booktitle>
<volume>volume</volume>
<publisher>A. Academic Press.</publisher>
<contexts>
<context position="18227" citStr="Eilenberg, 1974" startWordPosition="2982" endWordPosition="2983">79) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it will start to burn easily. It is a major NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word senses (concepts) are long familiar from Knowledge Representation. We assume the reader is familiar with the knowledge representation literature (for a summary, see Brachman and Levesque 2004), and describe only </context>
</contexts>
<marker>Eilenberg, 1974</marker>
<rawString>Samuel Eilenberg. 1974. Automata, Languages, and Machines, volume A. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Sue Atkins</author>
</authors>
<title>Framenet and lexicographic relevance.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<location>Granada,</location>
<contexts>
<context position="4662" citStr="Fillmore and Atkins, 1998" startWordPosition="697" endWordPosition="700">lds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under co</context>
</contexts>
<marker>Fillmore, Atkins, 1998</marker>
<rawString>Charles Fillmore and Sue Atkins. 1998. Framenet and lexicographic relevance. In Proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory,</booktitle>
<pages>1--90</pages>
<editor>In E. Bach and R. Harms, editors,</editor>
<location>New York.</location>
<contexts>
<context position="26083" citStr="Fillmore (1968)" startWordPosition="4296" endWordPosition="4297">n(Ef) g(Ef) 0 V g(Cn) Ef → ACf g(Cf) g(A(1)) g(A(2)) Ef → Cf’ g(Cf) finds the feedback vertex set, in that once such a set is given, the rest could be built automatically. This gives us a means of investigating the prevalence of what would become different deep cases (colors, link types) in other KR systems. Deep cases are distinguishers that mediate between the purely semantic (theta) link types and the surface case/adposition system. We have kept our system of deep cases rather standard, both in the sense of representing a common core among the many proposals starting with Gruber (1965) and Fillmore (1968) and in the sense of aiming at universality, a subject we defer to the next section. The names and frequency of use in the core vocabulary are given in Table 2. The results are indicative of a primary (agent/patient, what we denote ‘1’/‘2’), a secondary (DAT/REL/POSS), and a tertiary (locative) layer in deep cases – how these are mapped on language-specific (surface) cases will be discussed in Section 5. freq abbreviation comment 487 AGT agent 388 PAT patient 34 DAT dative 82 REL root or adpositional object 70 POSS default for relational nouns 20 TO target of action 15 FROM source of action 3 </context>
<context position="31119" citStr="Fillmore, 1968" startWordPosition="5160" endWordPosition="5162">two different languages) have to be coded by different deep cases is measured very badly, if at all, by the standard test suits used e.g. in paraphrase detection or question answering, and we would need to invest serious effort in building new test suites. For example, the system sketched above uses the same deep case, REL, for linking objects that are surface marked by quirky case and for arguments of predicate nominals. Another example is the dative/experiencer/beneficent family. Whether the experiencer cases familiar from Korean and elsewhere can be subsumed under the standard dative role (Fillmore, 1968) is an open question, but one that can at least be formulated in ACR. Currently we distinguish the dative DAT from possessive marking POSS, generally not considered a true case but quite prevalent in this function language after language: consider English (the) root of a tree, or Polish korzen drzewa. This is in contrast to the less frequent cases like (an excellent) occasion for martyrdom marked by obliques (here the preposition for). What these nouns (occasion, condition, reason, need) have in common is that the related word is goal of the definiendum in some sense. In these cases we use TO </context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Charles Fillmore. 1968. The case for case. In E. Bach and R. Harms, editors, Universals in Linguistic Theory, pages 1–90. Holt and Rinehart, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory.</title>
<date>1957</date>
<booktitle>In Studies in linguistic analysis,</booktitle>
<pages>1--32</pages>
<publisher>Blackwell.</publisher>
<contexts>
<context position="6095" citStr="Firth (1957)" startWordPosition="922" endWordPosition="923">he algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standardly used nominal features (Kornai, 2010). Continuous vector space (CVS) is also not a single model but a rich family of models, generally based on what Baroni (2013) calls the distributional hypothesis, that semantically similar items have similar distribution. This idea, going back at least to Firth (1957) is not at all trivial to defend, and not just because defining ‘semantically similar’ is a challenging task: as we shall see, there are significant design choices involved in defining similarity of vectors as well. To the extent CVS representations are primarily used in artificial neural net models, it may be helpful to consider the state of a network being described by the vector whose nth coordinate gives the activation level of the nth neuron. Under this conception, the meaning of a word is simply the activation pattern of the brain when the word is produced or perceived. Such vectors have</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory. In Studies in linguistic analysis, pages 1–32. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions: A Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="30451" citStr="Goldberg 1995" startWordPosition="5050" endWordPosition="5052">d a goal. For every language there is a langspec component of the lexicon which stores e.g. for English the information that source is expressed by the preposition from and destination by to. For Hungarian the langspec file stores the information that source can be linked by delative, elative, and ablative; goal by illative, sublative, or terminative. Once this kind of language-specific variation is factored out, the go entry becomes before AT src, after AT goal. The same technique is used to encode both lexical entries and constructions in the sense of Berkeley Construction Grammar (CxG, see Goldberg 1995). Whether two constructions (in the same language or two different languages) have to be coded by different deep cases is measured very badly, if at all, by the standard test suits used e.g. in paraphrase detection or question answering, and we would need to invest serious effort in building new test suites. For example, the system sketched above uses the same deep case, REL, for linking objects that are surface marked by quirky case and for arguments of predicate nominals. Another example is the dative/experiencer/beneficent family. Whether the experiencer cases familiar from Korean and elsew</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Adele E. Goldberg. 1995. Constructions: A Construction Grammar Approach to Argument Structure. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Gordon</author>
<author>Jerry R Hobbs</author>
<author>Michael T Cox</author>
</authors>
<title>Anthropomorphic self-models for metareasoning agents.</title>
<date>2011</date>
<booktitle>Metareasoning: Thinking about Thinking,</booktitle>
<pages>295--305</pages>
<editor>In Michael T. Cox and Anita Raja, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1667" citStr="Gordon et al., 2011" startWordPosition="227" endWordPosition="231">significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specifici</context>
<context position="5334" citStr="Gordon et al., 2011" startWordPosition="802" endWordPosition="805">k- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under construction by Hobbs and his coworkers (Hobbs, 2008; Gordon et al., 2011). What we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standardly used nominal features (Kornai, 2010). Continuous vector space (CVS) is also not a single model but a rich family of models, generally based on </context>
</contexts>
<marker>Gordon, Hobbs, Cox, 2011</marker>
<rawString>Andrew S. Gordon, Jerry R. Hobbs, and Michael T. Cox. 2011. Anthropomorphic self-models for metareasoning agents. In Michael T. Cox and Anita Raja, editors, Metareasoning: Thinking about Thinking, pages 295– 305. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Grice</author>
<author>Peter Strawson</author>
</authors>
<title>In defense of a dogma. The Philosophical Review,</title>
<date>1956</date>
<pages>65--148</pages>
<contexts>
<context position="2908" citStr="Grice and Strawson (1956)" startWordPosition="422" endWordPosition="425">ity in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going back at least to Quillian (1968) and Osgood et al. (1975) respectively, but we put the emphasis on the computational experiments we ran (source code and lexica available at github.com/kornai/4lang). 1 Background In the eyes of many, Quine (1951) has demolished the traditional analytic/synthetic distinction, relegating nearly all pre-Fregean accounts of word meaning from Aristotle to Locke to the dustbin of history. The opposing view, articulated clearly in Grice and Strawson (1956), is based on the empirical observation that people make the call rather uniformly over novel examples, an argument whose import is evident from the (at the time, still nascent) cognitive perspective. Today, we may agree with Putnam (1976): ‘Bachelor’ may be synonymous with ‘unmarried man’ but that cuts no philosophic ice. ‘Chair’ may be synonymous with ‘moveable seat for one with back’ but that bakes no philosophic bread and washes no philosophic windows. It is the belief that there are synonymies and analyticities of a deeper nature - synonymies and analyticities that cannot be discovered by</context>
</contexts>
<marker>Grice, Strawson, 1956</marker>
<rawString>Paul Grice and Peter Strawson. 1956. In defense of a dogma. The Philosophical Review, 65:148–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Steven Gruber</author>
</authors>
<title>Studies in lexical relations.</title>
<date>1965</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="26063" citStr="Gruber (1965)" startWordPosition="4293" endWordPosition="4294">) E → En |Ef E → Cn(Ef) g(Ef) 0 V g(Cn) Ef → ACf g(Cf) g(A(1)) g(A(2)) Ef → Cf’ g(Cf) finds the feedback vertex set, in that once such a set is given, the rest could be built automatically. This gives us a means of investigating the prevalence of what would become different deep cases (colors, link types) in other KR systems. Deep cases are distinguishers that mediate between the purely semantic (theta) link types and the surface case/adposition system. We have kept our system of deep cases rather standard, both in the sense of representing a common core among the many proposals starting with Gruber (1965) and Fillmore (1968) and in the sense of aiming at universality, a subject we defer to the next section. The names and frequency of use in the core vocabulary are given in Table 2. The results are indicative of a primary (agent/patient, what we denote ‘1’/‘2’), a secondary (DAT/REL/POSS), and a tertiary (locative) layer in deep cases – how these are mapped on language-specific (surface) cases will be discussed in Section 5. freq abbreviation comment 487 AGT agent 388 PAT patient 34 DAT dative 82 REL root or adpositional object 70 POSS default for relational nouns 20 TO target of action 15 FROM</context>
</contexts>
<marker>Gruber, 1965</marker>
<rawString>Jeffrey Steven Gruber. 1965. Studies in lexical relations. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Deep lexical semantics.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>4919--183</pages>
<contexts>
<context position="4245" citStr="Hobbs (2008)" startWordPosition="639" endWordPosition="640">rence on Lexical and Computational Semantics (*SEM 2015), pages 165–175, Denver, Colorado, June 4–5, 2015. Fortunately, one philosopher’s trash may just turn out to be another linguist’s treasure. What Putnam has demonstrated is that “a speaker can, by all reasonable standards, be in command of a word like water without being able to command the intension that would represent the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowled</context>
</contexts>
<marker>Hobbs, 2008</marker>
<rawString>J.R. Hobbs. 2008. Deep lexical semantics. Lecture Notes in Computer Science, 4919:183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´erard Huet</author>
<author>Benoit Razet</author>
</authors>
<title>Computing with relational machines.</title>
<date>2008</date>
<booktitle>In Tutorial at ICON,</booktitle>
<contexts>
<context position="5632" citStr="Huet and Razet, 2008" startWordPosition="845" endWordPosition="848">dly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under construction by Hobbs and his coworkers (Hobbs, 2008; Gordon et al., 2011). What we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standardly used nominal features (Kornai, 2010). Continuous vector space (CVS) is also not a single model but a rich family of models, generally based on what Baroni (2013) calls the distributional hypothesis, that semantically similar items have similar distribution. This idea, going back at least to Firth (1957) is not at all trivial to defend, and not just because defining ‘semantically similar’ is a challenging task: as we shall see, there are </context>
<context position="18561" citStr="Huet and Razet, 2008" startWordPosition="3031" endWordPosition="3034"> easily. It is a major NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word senses (concepts) are long familiar from Knowledge Representation. We assume the reader is familiar with the knowledge representation literature (for a summary, see Brachman and Levesque 2004), and describe only those parts of the system that differ from the mainstream assumptions. In particular, we collapse attribution, unary predication, and TS A links in a single link type ‘0’ (as in Figs. 1-2 above) and have only two other kinds of links to distinguish the arguments of transitive verbs, ‘1’ corresponding to subject/agent; and ‘2’ to obj</context>
</contexts>
<marker>Huet, Razet, 2008</marker>
<rawString>G´erard Huet and Benoit Razet. 2008. Computing with relational machines. In Tutorial at ICON, Dec 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pauline Jacobson</author>
</authors>
<title>Compositional Semantics.</title>
<date>2014</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="22709" citStr="Jacobson (2014)" startWordPosition="3710" endWordPosition="3711">eatment whatsoever, since similarity of meaning postulates is undefined. 4 Lexical lookup The interaction with compositional semantics is a key issue for any competence theory of lexical semantics. In the classical formal system, this is handled by a mechanism of lexical lookup that substitutes the meaning postulates at the terminal nodes of the derivation tree, at the price of introducing some lexical redundancy rule that creates the intensional meaning of each word, including the evidently nonintensional ones, based on the meaning postulates that encode the extensional meaning. (Ch. 19.2 of Jacobson (2014) sketches an alternative treatment, which keeps intensionality for the intended set of cases.) While there are considerable technical difficulties of formula manipulation involved, this is really one area where the classical theory shines as a competence theory – we cannot even imagine to create a learning algorithm that would cover the meaning of infinitely many complex expressions unless we had some means of combining the meanings of the lexical entries. CVS semantics offers several ways of combining lexical entries, the simplest being simply adding the vectors together (Mitchell and Lapata,</context>
</contexts>
<marker>Jacobson, 2014</marker>
<rawString>Pauline Jacobson. 2014. Compositional Semantics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Karp</author>
</authors>
<title>Reducibility among combinatorial problems.</title>
<date>1972</date>
<journal>Complexity of Computer Computations,</journal>
<pages>85--104</pages>
<editor>In R. Miller and J.W. Thatcher, editors,</editor>
<publisher>Plenum Press,</publisher>
<location>New York.</location>
<contexts>
<context position="10504" citStr="Karp (1972)" startWordPosition="1670" endWordPosition="1671">ars in its definition. Such a graph can have many cycles. Our main interest is with selecting a defining set which has the property that each word, including those that appear in the definitions, can be defined in terms of members of this set. Every word that is a true primitive (has no definition, e.g. the basic terms of the Schank and NSM systems) must be included in the defining set, and to these we must add at least one vertex from every directed cycle. Thus, the problem of finding a defining set is equivalent to finding a feedback vertex set, (FVS) a problem already proven NP-complete in Karp (1972). Since we cannot run an exhaustive search, we use a heuristic algorithm which searches for a defining set by gradually eliminating low-frequency nodes whose outgoing arcs lead to not yet eliminated nodes, and make no claim that the results in Table 1 are optimal, just that they are typical of the reduction that can be obtained by modest computation. We defer discussion of the last line to Section 4, but note that the first line already implies that a defining set of 1,008 concepts will cover all senses of the high frequency items in the major Western branches of IE, and to cover the first (pr</context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Richard M. Karp. 1972. Reducibility among combinatorial problems. In R. Miller and J.W. Thatcher, editors, Complexity of Computer Computations, pages 85–104. Plenum Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In AAAI-2000 Seventeenth National Conference on Artificial Intelligence,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="4697" citStr="Kipper et al., 2000" startWordPosition="704" endWordPosition="707">nal systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under construction by Hobbs and his coworke</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class based construction of a verb lexicon. In AAAI-2000 Seventeenth National Conference on Artificial Intelligence, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="19536" citStr="Klein and Manning, 2003" startWordPosition="3188" endWordPosition="3191"> we collapse attribution, unary predication, and TS A links in a single link type ‘0’ (as in Figs. 1-2 above) and have only two other kinds of links to distinguish the arguments of transitive verbs, ‘1’ corresponding to subject/agent; and ‘2’ to object/patient. The treatment of other link types, be they construed as grammatical functions or as deep cases or even thematic slots, is deferred to Section 4. By creating graphs for all LDOCE headwords based on dependency parses of their definitions (the ‘literal’ network of Table 1) using the unlexicalized version of the Stanford Dependency Parser (Klein and Manning, 2003), we obtained measures of lexical relatedness by defining various similarity metrics over pairs of such graphs. The intuition underlying all these metrics is that two words are semantically similar if their definitions overlap in (i) the concepts present in their definitions (e.g. the definition of both train and car will make reference to the concept vehicle) and (ii) the binary relations they take part in (e.g. both street and park are TN town). While such a measure of semantic similarity builds more on manual labor (already performed by the lexicographers) than those gained from stateof-the</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
<author>M´arton Makrai</author>
</authors>
<title>A 4lang fogalmi sz´ot´ar.</title>
<date>2013</date>
<booktitle>In Attila Tan´acs and Veronika Vincze,</booktitle>
<pages>62--70</pages>
<editor>editors, IX. Magyar</editor>
<contexts>
<context position="34430" citStr="Kornai and Makrai, 2013" startWordPosition="5706" endWordPosition="5709">dings, automatically generated and thus not entirely free of errors and omissions, now exist for 50 languages ( ´Acs et al., 2013), many of them outside the Indoeuropean and Finnougric families originally targeted. More important, formal definitions of the concepts that rely on less than a dozen theoretical primitives (including the three link types) and only 129 core concepts, are now available for all the concepts. So far, only the theoretical underpinnings of this formal system have been fully described in English (Kornai, 2010; Kornai, 2012), with many details presented only in Hungarian (Kornai and Makrai, 2013), but the formal definitions, and the parser that can build both graphs and Eilenberg machines from these, are now available as part of the kornai/4lang github repository. These formal definitions were written primarily by Makrai, with notable contributions by Recski and Nemeskey. The system has been used as an experimental platform for a variety of purposes, including quantitative analysis of deep cases by Makrai, who developed the current version of the deep case system with Nemeskey (Makrai, 2015); for defining lexical relatedness (Makrai et al., 2013; Recski and ´Acs, 2015); and in this pa</context>
</contexts>
<marker>Kornai, Makrai, 2013</marker>
<rawString>Andr´as Kornai and M´arton Makrai. 2013. A 4lang fogalmi sz´ot´ar. In Attila Tan´acs and Veronika Vincze, editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia, pages 62–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
</authors>
<title>The algebra of lexical semantics.</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Mathematics of Language Workshop, LNAI 6149,</booktitle>
<pages>174--199</pages>
<editor>In Christian Ebert, Gerhard J¨ager, and Jens Michaelis, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1645" citStr="Kornai, 2010" startWordPosition="225" endWordPosition="226">haps the most significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; </context>
<context position="5827" citStr="Kornai, 2010" startWordPosition="878" endWordPosition="879"> lexical semantics system that is still under construction by Hobbs and his coworkers (Hobbs, 2008; Gordon et al., 2011). What we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standardly used nominal features (Kornai, 2010). Continuous vector space (CVS) is also not a single model but a rich family of models, generally based on what Baroni (2013) calls the distributional hypothesis, that semantically similar items have similar distribution. This idea, going back at least to Firth (1957) is not at all trivial to defend, and not just because defining ‘semantically similar’ is a challenging task: as we shall see, there are significant design choices involved in defining similarity of vectors as well. To the extent CVS representations are primarily used in artificial neural net models, it may be helpful to consider </context>
<context position="18576" citStr="Kornai, 2010" startWordPosition="3035" endWordPosition="3036"> NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word senses (concepts) are long familiar from Knowledge Representation. We assume the reader is familiar with the knowledge representation literature (for a summary, see Brachman and Levesque 2004), and describe only those parts of the system that differ from the mainstream assumptions. In particular, we collapse attribution, unary predication, and TS A links in a single link type ‘0’ (as in Figs. 1-2 above) and have only two other kinds of links to distinguish the arguments of transitive verbs, ‘1’ corresponding to subject/agent; and ‘2’ to object/patient. Th</context>
<context position="34342" citStr="Kornai, 2010" startWordPosition="5695" endWordPosition="5696">e years. The name is no longer quite justified, in that natural language bindings, automatically generated and thus not entirely free of errors and omissions, now exist for 50 languages ( ´Acs et al., 2013), many of them outside the Indoeuropean and Finnougric families originally targeted. More important, formal definitions of the concepts that rely on less than a dozen theoretical primitives (including the three link types) and only 129 core concepts, are now available for all the concepts. So far, only the theoretical underpinnings of this formal system have been fully described in English (Kornai, 2010; Kornai, 2012), with many details presented only in Hungarian (Kornai and Makrai, 2013), but the formal definitions, and the parser that can build both graphs and Eilenberg machines from these, are now available as part of the kornai/4lang github repository. These formal definitions were written primarily by Makrai, with notable contributions by Recski and Nemeskey. The system has been used as an experimental platform for a variety of purposes, including quantitative analysis of deep cases by Makrai, who developed the current version of the deep case system with Nemeskey (Makrai, 2015); for d</context>
</contexts>
<marker>Kornai, 2010</marker>
<rawString>Andr´as Kornai. 2010. The algebra of lexical semantics. In Christian Ebert, Gerhard J¨ager, and Jens Michaelis, editors, Proceedings of the 11th Mathematics of Language Workshop, LNAI 6149, pages 174– 199. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
</authors>
<title>Eliminating ditransitives.</title>
<date>2012</date>
<booktitle>Revised and Selected Papers from the 15th and 16th Formal Grammar Conferences, LNCS 7395,</booktitle>
<pages>243--261</pages>
<editor>In Ph. de Groote and M-J Nederhof, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="25021" citStr="Kornai (2012)" startWordPosition="4108" endWordPosition="4109">se of reading, in unary predication (e.g. mouse →− rodent) 0 we permit both prefix and suffix order, but with different kinds of parens mouse[rodent] and rodent(mouse); and we use infix notation (cow MAKE milk) for transitives (cow ←− MAKE 2 1 →− milk, link types ‘1’ and ‘2’). The right column of Figure 3 shows the digraph obtained from parsing the formula on the right hand hand side of the grammar rules. There are no ‘3’ or higher links, as ditransitives like x give y to z are decomposed at the semantic level into unary and binary atoms, in this case CAUSE and HAVE, ‘x cause (z have y)’, see Kornai (2012) for further details. A digraph representing the whole lexicon was built in two steps: first, every clause in definitions was manually translated to a formula (which in turns is automatically translated into a digraph), then the digraphs were connected by unifying nodes that have the same label and no outgoing edges. The amount of manual labor involved was considerably lessened by the method of Section 3 that 170 g(E) ... g(E) E → En |Ef E → Cn(Ef) g(Ef) 0 V g(Cn) Ef → ACf g(Cf) g(A(1)) g(A(2)) Ef → Cf’ g(Cf) finds the feedback vertex set, in that once such a set is given, the rest could be bu</context>
<context position="34357" citStr="Kornai, 2012" startWordPosition="5697" endWordPosition="5698">ame is no longer quite justified, in that natural language bindings, automatically generated and thus not entirely free of errors and omissions, now exist for 50 languages ( ´Acs et al., 2013), many of them outside the Indoeuropean and Finnougric families originally targeted. More important, formal definitions of the concepts that rely on less than a dozen theoretical primitives (including the three link types) and only 129 core concepts, are now available for all the concepts. So far, only the theoretical underpinnings of this formal system have been fully described in English (Kornai, 2010; Kornai, 2012), with many details presented only in Hungarian (Kornai and Makrai, 2013), but the formal definitions, and the parser that can build both graphs and Eilenberg machines from these, are now available as part of the kornai/4lang github repository. These formal definitions were written primarily by Makrai, with notable contributions by Recski and Nemeskey. The system has been used as an experimental platform for a variety of purposes, including quantitative analysis of deep cases by Makrai, who developed the current version of the deep case system with Nemeskey (Makrai, 2015); for defining lexical</context>
</contexts>
<marker>Kornai, 2012</marker>
<rawString>Andr´as Kornai. 2012. Eliminating ditransitives. In Ph. de Groote and M-J Nederhof, editors, Revised and Selected Papers from the 15th and 16th Formal Grammar Conferences, LNCS 7395, pages 243–261. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level model for morphological analysis.</title>
<date>1983</date>
<booktitle>In Proceedings of IJCAI-83,</booktitle>
<pages>683--685</pages>
<contexts>
<context position="18449" citStr="Koskenniemi (1983)" startWordPosition="3016" endWordPosition="3017">ence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it will start to burn easily. It is a major NLP challenge to make this deduction (Dagan et al. 2006), but ACR can store the information trivially and make the inference by spreading activation. We implemented one variant of the ACR theory of word meaning by a network of Eilenberg machines (Eilenberg, 1974) corresponding to elements of the reduced vocabulary. Eilenberg machines are a simple generalization of the better known finite state automata (FSA) and transducers (FSTs) that have become standard since Koskenniemi (1983) in describing the rule-governed aspects of the lexicon, morphotactics and morphophonology (Huet and Razet, 2008; Kornai, 2010). The methods we use for defining word senses (concepts) are long familiar from Knowledge Representation. We assume the reader is familiar with the knowledge representation literature (for a summary, see Brachman and Levesque 2004), and describe only those parts of the system that differ from the mainstream assumptions. In particular, we collapse attribution, unary predication, and TS A links in a single link type ‘0’ (as in Figs. 1-2 above) and have only two other kin</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level model for morphological analysis. In Proceedings of IJCAI-83, pages 683–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Marco Marelli</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Compositional-ly derived representations of morphologically complex words in distributional semantics.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>1517--1526</pages>
<contexts>
<context position="23379" citStr="Lazaridou et al., 2013" startWordPosition="3813" endWordPosition="3816"> intensionality for the intended set of cases.) While there are considerable technical difficulties of formula manipulation involved, this is really one area where the classical theory shines as a competence theory – we cannot even imagine to create a learning algorithm that would cover the meaning of infinitely many complex expressions unless we had some means of combining the meanings of the lexical entries. CVS semantics offers several ways of combining lexical entries, the simplest being simply adding the vectors together (Mitchell and Lapata, 2008), but the use of linear transformations (Lazaridou et al., 2013) and tensor products (Smolensky, 1990) has also been contemplated. Currently, an approach that combines the vectors of the parts to form the vector of the whole by recurrent neural nets appears to work best (Socher et al., 2013), but this is still an area of intense research and it would be premature to declare this method the winner. Here we concentrate on ACR, investigating the issue of the inventory of graph edge colors on the same core vocabulary as discussed above. The key technical problem is to bring the variety of links between verbs and their arguments under control: as Woods (1975) a</context>
</contexts>
<marker>Lazaridou, Marelli, Zamparelli, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly derived representations of morphologically complex words in distributional semantics. In ACL (1), pages 1517– 1526.</rawString>
</citation>
<citation valid="true">
<title>Tomasz Łuczak and Taral Guldahl Seierstad.</title>
<date>2009</date>
<pages>35--271</pages>
<contexts>
<context position="11981" citStr="(2009)" startWordPosition="1923" endWordPosition="1923">ktionary (all senses) 369,281 2,504 en.wiktionary (first senses) 304,029 1,845 formal 2,754 129 Table 1: Properties of four different dictionaries While a feedback vertex set is guaranteed to exist for any digraph (if all else fails, the entire set of vertices will do), it is not guaranteed that there exists one that is considerably smaller than the entire graph. (For random digraphs in general see Dutta and Subramanian 2010, for highly symmetrical lattices see Zhou 2013 ms.) In random digraphs under relatively mild conditions on the proportion of edges relative to nodes, Łuczak and Seierstad (2009) show that a strong component essentially the size of the entire graph will exist. Fortunately, digraphs built on definitions are not at all behaving in a random fashion, the strongly connected components are relatively small, as Table 1 makes evident. For example, in the English Wiktionary, 369,281 definitions can be reduced to a core set of 2,504 defining words, and in CED we can find a defining set of 6,490 words, even though these dictionaries, unlike LDOCE, were not built using an explicit defining set. Since LDOCE pioneered the idea of actively limiting the defining vocabulary, it is no </context>
</contexts>
<marker>2009</marker>
<rawString>Tomasz Łuczak and Taral Guldahl Seierstad. 2009. The critical behavior of random digraphs. Random Structures and Algorithms, 35:271–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology. In CoNLL,</title>
<date>2013</date>
<pages>104--113</pages>
<contexts>
<context position="17345" citStr="Luong et al., 2013" startWordPosition="2832" endWordPosition="2835">n real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it will start to burn eas</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In CoNLL, pages 104–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M´arton Makrai</author>
<author>D´avid M´ark Nemeskey</author>
<author>Andr´as Kornai</author>
</authors>
<title>Applicative structure in vector space models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>59--63</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="20628" citStr="Makrai et al., 2013" startWordPosition="3370" endWordPosition="3373"> of semantic similarity builds more on manual labor (already performed by the lexicographers) than those gained from stateof-the-art CVS systems, recently the results from the ‘literal’ network have been used in a competitive system for measuring semantic textual similarity (Recski and ´Acs, 2015). In Section 4 we discuss the ‘formal’ network of Table 1 built directly on the concept formulae. By spectral dimension reduction of the incidence matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, an</context>
<context position="34990" citStr="Makrai et al., 2013" startWordPosition="5797" endWordPosition="5800">tails presented only in Hungarian (Kornai and Makrai, 2013), but the formal definitions, and the parser that can build both graphs and Eilenberg machines from these, are now available as part of the kornai/4lang github repository. These formal definitions were written primarily by Makrai, with notable contributions by Recski and Nemeskey. The system has been used as an experimental platform for a variety of purposes, including quantitative analysis of deep cases by Makrai, who developed the current version of the deep case system with Nemeskey (Makrai, 2015); for defining lexical relatedness (Makrai et al., 2013; Recski and ´Acs, 2015); and in this paper, for finding the definitional core, the feedback vertex set. ´Acs wrote the first version of the feedback vertex set finder which was adapted to our data by ´Acs and Pajkossy, who also took part in the computational experiments, including preprocessing the data, adapting the vertex set finder, and running the experiments. Recski created the pipeline in the http://github/kornai/4lang repository that builds formal definitions from English dictionary entries. Kornai advised and wrote the paper. We are grateful to Attila Zs´eder (HAS Linguistics Institut</context>
</contexts>
<marker>Makrai, Nemeskey, Kornai, 2013</marker>
<rawString>M´arton Makrai, D´avid M´ark Nemeskey, and Andr´as Kornai. 2013. Applicative structure in vector space models. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 59–63, Sofia, Bulgaria, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M´arton Makrai</author>
</authors>
<title>Deep cases in the �4lang conceptlexicon.</title>
<date>2015</date>
<booktitle>Szmtgpes Nyelvszeti Konferencia (MSZNY 2014),</booktitle>
<pages>50--57</pages>
<editor>In Attila Tancs, Viktor Varga, and Veronika Vincze, editors, X. Magyar</editor>
<note>(in Hungarian), 387 (English abstract).</note>
<contexts>
<context position="34935" citStr="Makrai, 2015" startWordPosition="5790" endWordPosition="5791">nglish (Kornai, 2010; Kornai, 2012), with many details presented only in Hungarian (Kornai and Makrai, 2013), but the formal definitions, and the parser that can build both graphs and Eilenberg machines from these, are now available as part of the kornai/4lang github repository. These formal definitions were written primarily by Makrai, with notable contributions by Recski and Nemeskey. The system has been used as an experimental platform for a variety of purposes, including quantitative analysis of deep cases by Makrai, who developed the current version of the deep case system with Nemeskey (Makrai, 2015); for defining lexical relatedness (Makrai et al., 2013; Recski and ´Acs, 2015); and in this paper, for finding the definitional core, the feedback vertex set. ´Acs wrote the first version of the feedback vertex set finder which was adapted to our data by ´Acs and Pajkossy, who also took part in the computational experiments, including preprocessing the data, adapting the vertex set finder, and running the experiments. Recski created the pipeline in the http://github/kornai/4lang repository that builds formal definitions from English dictionary entries. Kornai advised and wrote the paper. We a</context>
</contexts>
<marker>Makrai, 2015</marker>
<rawString>M´arton Makrai. 2015. Deep cases in the �4lang conceptlexicon. In Attila Tancs, Viktor Varga, and Veronika Vincze, editors, X. Magyar Szmtgpes Nyelvszeti Konferencia (MSZNY 2014), pages 50–57 (in Hungarian), 387 (English abstract).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret G McKeown</author>
<author>Mary E Curtis</author>
</authors>
<title>The nature of vocabulary acquisition. Lawrence Erlbaum Associates.</title>
<date>1987</date>
<contexts>
<context position="7233" citStr="McKeown and Curtis, 1987" startWordPosition="1108" endWordPosition="1111">tivation pattern of the brain when the word is produced or perceived. Such vectors have very large (1010) dimension so dimension reduction is called for, but direct correlation between brain activation patterns and the distribution of words has actually been detected (Mitchell et al., 2008). 2 Learnability The key distinguishing feature between ‘explanatory’ or competence models and ‘descriptive’ or performance models is that the former, but not the latter, come complete with a learning algorithm (Chomsky, 1965). Although there is a wealth of data on children’s acquisition of lexical entries (McKeown and Curtis, 1987), neither cognitive nor formal semantics have come close to formulating a robust theory of acquisition, and for intensions, infinite information objects encoding the meaning in the formal theory, it is not at all clear whether such a learning algorithm is even possible. 2.1 The basic vocabulary The idea that there is a small set of conceptual primitives for building semantic representations has a long history both in linguistics and AI as well as in language teaching. The more theory-oriented systems, such as Conceptual Dependency and NSM assume only a few dozen primitives, but have a disquiet</context>
<context position="16496" citStr="McKeown and Curtis 1987" startWordPosition="2684" endWordPosition="2687">ry acquisition is a highly idealized one, for surely children don’t learn the meaning of sharp by their parents telling them it means ‘having a thin cutting edge or point’. Yet it is clear that computers that lack a sensory system that would deliver intense signals upon encoun168 tering sharp objects can nevertheless acquire something of the meaning by pure deduction (assuming also that they are programmed to know that cutting one’s body will CAUSE PAIN) and further, the dominant portion of the vocabulary is not connected to direct sensory signals but is learned from context (see Chapter 6 of McKeown and Curtis 1987). This brings us to CVS semantics, where learning theory is idealized in a very different way, by assuming that the learner has access to very large corpora, gigaword and beyond. We must agree with Miller and Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, es</context>
</contexts>
<marker>McKeown, Curtis, 1987</marker>
<rawString>Margaret G. McKeown and Mary E. Curtis. 1987. The nature of vocabulary acquisition. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proc. ICLR</booktitle>
<editor>In Y. Bengio and Y. LeCun, editors,</editor>
<contexts>
<context position="17323" citStr="Mikolov et al., 2013" startWordPosition="2828" endWordPosition="2831">d Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it </context>
<context position="20772" citStr="Mikolov et al., 2013" startWordPosition="3397" endWordPosition="3400">, recently the results from the ‘literal’ network have been used in a competitive system for measuring semantic textual similarity (Recski and ´Acs, 2015). In Section 4 we discuss the ‘formal’ network of Table 1 built directly on the concept formulae. By spectral dimension reduction of the incidence matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, and it is not clear what, if anything, can be salvaged from the truth-conditional approach to these matters. At the same time, the CVS approach to</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Y. Bengio and Y. LeCun, editors, Proc. ICLR 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="17323" citStr="Mikolov et al., 2013" startWordPosition="2828" endWordPosition="2831">d Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it </context>
<context position="20772" citStr="Mikolov et al., 2013" startWordPosition="3397" endWordPosition="3400">, recently the results from the ‘literal’ network have been used in a competitive system for measuring semantic textual similarity (Recski and ´Acs, 2015). In Section 4 we discuss the ‘formal’ network of Table 1 built directly on the concept formulae. By spectral dimension reduction of the incidence matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, and it is not clear what, if anything, can be salvaged from the truth-conditional approach to these matters. At the same time, the CVS approach to</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111– 3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Zweig Geoffrey</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT2013,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="17323" citStr="Mikolov et al., 2013" startWordPosition="2828" endWordPosition="2831">d Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., 2013a; Luong et al., 2013), but it is clear that both CVS and ACR have the beginnings of a feasible theory of learning, while the classical theory of meaning postulates offers nothing of the sort, not even for the handful of lexical items (tense and aspect markers in particular, see Dowty 1979) where the underlying logic has the resources to express these. 3 Lexical relatedness Ordinary dictionary definitions can be mined to recover the conceptual entailments that are at the heart of lexical semantic competence. Whatever naphtha is, knowing that it is inflammable is sufficient for knowing that it </context>
<context position="20772" citStr="Mikolov et al., 2013" startWordPosition="3397" endWordPosition="3400">, recently the results from the ‘literal’ network have been used in a competitive system for measuring semantic textual similarity (Recski and ´Acs, 2015). In Section 4 we discuss the ‘formal’ network of Table 1 built directly on the concept formulae. By spectral dimension reduction of the incidence matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, and it is not clear what, if anything, can be salvaged from the truth-conditional approach to these matters. At the same time, the CVS approach to</context>
</contexts>
<marker>Mikolov, Yih, Geoffrey, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Zweig Geoffrey. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT2013, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Noam Chomsky</author>
</authors>
<title>Finitary models of language users.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology,</booktitle>
<pages>419--491</pages>
<editor>In R.D. Luce, R.R. Bush, and E. Galanter, editors,</editor>
<publisher>Wiley.</publisher>
<contexts>
<context position="16719" citStr="Miller and Chomsky (1963)" startWordPosition="2723" endWordPosition="2726">y system that would deliver intense signals upon encoun168 tering sharp objects can nevertheless acquire something of the meaning by pure deduction (assuming also that they are programmed to know that cutting one’s body will CAUSE PAIN) and further, the dominant portion of the vocabulary is not connected to direct sensory signals but is learned from context (see Chapter 6 of McKeown and Curtis 1987). This brings us to CVS semantics, where learning theory is idealized in a very different way, by assuming that the learner has access to very large corpora, gigaword and beyond. We must agree with Miller and Chomsky (1963) that in real life a child exposed to a word every second would require over 30 years to hear gigaword amounts, but we take this to be a reflection of the weak inferencing ability of current statistical models, for there is nothing in the argument that says that models that are more efficient in extracting regularities can’t learn these from orders of magnitude less data, especially as children are known to acquire words based on a single exposure. For now, such one shot learning remains something of an ideal, in that CVS systems prune infrequent words (Collobert et al., 2011; Mikolov et al., </context>
</contexts>
<marker>Miller, Chomsky, 1963</marker>
<rawString>George A. Miller and Noam Chomsky. 1963. Finitary models of language users. In R.D. Luce, R.R. Bush, and E. Galanter, editors, Handbook of Mathematical Psychology, pages 419–491. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4624" citStr="Miller, 1995" startWordPosition="694" endWordPosition="695"> the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical s</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="23315" citStr="Mitchell and Lapata, 2008" startWordPosition="3803" endWordPosition="3806">2 of Jacobson (2014) sketches an alternative treatment, which keeps intensionality for the intended set of cases.) While there are considerable technical difficulties of formula manipulation involved, this is really one area where the classical theory shines as a competence theory – we cannot even imagine to create a learning algorithm that would cover the meaning of infinitely many complex expressions unless we had some means of combining the meanings of the lexical entries. CVS semantics offers several ways of combining lexical entries, the simplest being simply adding the vectors together (Mitchell and Lapata, 2008), but the use of linear transformations (Lazaridou et al., 2013) and tensor products (Smolensky, 1990) has also been contemplated. Currently, an approach that combines the vectors of the parts to form the vector of the whole by recurrent neural nets appears to work best (Socher et al., 2013), but this is still an area of intense research and it would be premature to declare this method the winner. Here we concentrate on ACR, investigating the issue of the inventory of graph edge colors on the same core vocabulary as discussed above. The key technical problem is to bring the variety of links be</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
<author>S V Shinkareva</author>
<author>A Carlson</author>
<author>K M Chang</author>
<author>V L Malave</author>
<author>R A Mason</author>
<author>M A Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<volume>320</volume>
<issue>5880</issue>
<contexts>
<context position="6899" citStr="Mitchell et al., 2008" startWordPosition="1056" endWordPosition="1059">ng similarity of vectors as well. To the extent CVS representations are primarily used in artificial neural net models, it may be helpful to consider the state of a network being described by the vector whose nth coordinate gives the activation level of the nth neuron. Under this conception, the meaning of a word is simply the activation pattern of the brain when the word is produced or perceived. Such vectors have very large (1010) dimension so dimension reduction is called for, but direct correlation between brain activation patterns and the distribution of words has actually been detected (Mitchell et al., 2008). 2 Learnability The key distinguishing feature between ‘explanatory’ or competence models and ‘descriptive’ or performance models is that the former, but not the latter, come complete with a learning algorithm (Chomsky, 1965). Although there is a wealth of data on children’s acquisition of lexical entries (McKeown and Curtis, 1987), neither cognitive nor formal semantics have come close to formulating a robust theory of acquisition, and for intensions, infinite information objects encoding the meaning in the formal theory, it is not at all clear whether such a learning algorithm is even possi</context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>T. M. Mitchell, S.V. Shinkareva, A. Carlson, K.M. Chang, V.L. Malave, R.A. Mason, and M.A. Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science, 320(5880):1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Ogden</author>
</authors>
<title>Basic English: A General Introduction with Rules and Grammar. Psyche miniatures: General Series. Kegan Paul,</title>
<date>1944</date>
<location>Trench, Trubner.</location>
<contexts>
<context position="8000" citStr="Ogden, 1944" startWordPosition="1241" endWordPosition="1242">coding the meaning in the formal theory, it is not at all clear whether such a learning algorithm is even possible. 2.1 The basic vocabulary The idea that there is a small set of conceptual primitives for building semantic representations has a long history both in linguistics and AI as well as in language teaching. The more theory-oriented systems, such as Conceptual Dependency and NSM assume only a few dozen primitives, but have a disquieting tendency to add new elements as time goes by (Andrews, 2015). In contrast, the systems intended for teaching and communication, such as Basic English (Ogden, 1944) start with at least a thousand primitives, and assume that these need to be further supplemented by technical terms from various domains. Since the obvious learning algorithm based on any such reductive system is one where the primi166 tives are assumed universal (and possibly innate, see Section 5), and the rest is learned by reduction to the primitives, we performed a series of ‘ceiling’ experiments aiming at a determination of how big the universal/innate component of the lexicon must be. A trivial lower bound is given by the current size of the NSM inventory, 65 (Andrews, 2015), but as lo</context>
</contexts>
<marker>Ogden, 1944</marker>
<rawString>C.K. Ogden. 1944. Basic English: A General Introduction with Rules and Grammar. Psyche miniatures: General Series. Kegan Paul, Trench, Trubner.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Osgood</author>
<author>William S May</author>
<author>Murray S Miron</author>
</authors>
<title>Cross Cultural Universals of Affective Meaning.</title>
<date>1975</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="2479" citStr="Osgood et al. (1975)" startWordPosition="356" endWordPosition="359">rief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going back at least to Quillian (1968) and Osgood et al. (1975) respectively, but we put the emphasis on the computational experiments we ran (source code and lexica available at github.com/kornai/4lang). 1 Background In the eyes of many, Quine (1951) has demolished the traditional analytic/synthetic distinction, relegating nearly all pre-Fregean accounts of word meaning from Aristotle to Locke to the dustbin of history. The opposing view, articulated clearly in Grice and Strawson (1956), is based on the empirical observation that people make the call rather uniformly over novel examples, an argument whose import is evident from the (at the time, still na</context>
</contexts>
<marker>Osgood, May, Miron, 1975</marker>
<rawString>Charles E. Osgood, William S. May, and Murray S. Miron. 1975. Cross Cultural Universals of Affective Meaning. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara H Partee</author>
</authors>
<title>Semantics - mathematics or psychology? In</title>
<date>1979</date>
<booktitle>Semantics from Different Points of View,</booktitle>
<pages>1--14</pages>
<editor>R. Bauerl, U. Egli, and A. von Stechow, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="1189" citStr="Partee (1979)" startWordPosition="157" endWordPosition="158">demy of Sciences P´azm´any P´eter s´et´any 1/C Egry J. u. 1 Bencz´ur u. 33 1117 Budapest, Hungary 1111 Budapest, Hungary 1068 Budapest, Hungary nemeskeyd@gmail.com pajkossy@math.bme.hu recski@mokk.bme.hu Abstract We investigate from the competence standpoint two recent models of lexical semantics, algebraic conceptual representations and continuous vector models. Characterizing what it means for a speaker to be competent in lexical semantics remains perhaps the most significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio </context>
<context position="4065" citStr="Partee, 1979" startWordPosition="614" endWordPosition="615">ynonymies and analyticities that cannot be discovered by the lexicographer or the linguist but only by the philosopher - that is incorrect. 165 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 165–175, Denver, Colorado, June 4–5, 2015. Fortunately, one philosopher’s trash may just turn out to be another linguist’s treasure. What Putnam has demonstrated is that “a speaker can, by all reasonable standards, be in command of a word like water without being able to command the intension that would represent the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), a</context>
</contexts>
<marker>Partee, 1979</marker>
<rawString>Barbara H. Partee. 1979. Semantics - mathematics or psychology? In R. Bauerl, U. Egli, and A. von Stechow, editors, Semantics from Different Points of View, pages 1–14. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
</authors>
<title>Changing perspectives on the ‘mathematics or psychology’ question.</title>
<date>2013</date>
<booktitle>In Philosophy Wkshp on “Semantics Mathematics or Psychology?”.</booktitle>
<contexts>
<context position="1226" citStr="Partee 2013" startWordPosition="163" endWordPosition="164">´any 1/C Egry J. u. 1 Bencz´ur u. 33 1117 Budapest, Hungary 1111 Budapest, Hungary 1068 Budapest, Hungary nemeskeyd@gmail.com pajkossy@math.bme.hu recski@mokk.bme.hu Abstract We investigate from the competence standpoint two recent models of lexical semantics, algebraic conceptual representations and continuous vector models. Characterizing what it means for a speaker to be competent in lexical semantics remains perhaps the most significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pe</context>
</contexts>
<marker>Partee, 2013</marker>
<rawString>Barbara Partee. 2013. Changing perspectives on the ‘mathematics or psychology’ question. In Philosophy Wkshp on “Semantics Mathematics or Psychology?”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="1848" citStr="Pennington et al., 2014" startWordPosition="258" endWordPosition="261">3), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going back at least to Quillian </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Putnam</author>
</authors>
<title>Two dogmas revisited. printed in his</title>
<date>1976</date>
<journal>Realism and Reason, Philosophical Papers,</journal>
<volume>3</volume>
<contexts>
<context position="3147" citStr="Putnam (1976)" startWordPosition="464" endWordPosition="465">onal experiments we ran (source code and lexica available at github.com/kornai/4lang). 1 Background In the eyes of many, Quine (1951) has demolished the traditional analytic/synthetic distinction, relegating nearly all pre-Fregean accounts of word meaning from Aristotle to Locke to the dustbin of history. The opposing view, articulated clearly in Grice and Strawson (1956), is based on the empirical observation that people make the call rather uniformly over novel examples, an argument whose import is evident from the (at the time, still nascent) cognitive perspective. Today, we may agree with Putnam (1976): ‘Bachelor’ may be synonymous with ‘unmarried man’ but that cuts no philosophic ice. ‘Chair’ may be synonymous with ‘moveable seat for one with back’ but that bakes no philosophic bread and washes no philosophic windows. It is the belief that there are synonymies and analyticities of a deeper nature - synonymies and analyticities that cannot be discovered by the lexicographer or the linguist but only by the philosopher - that is incorrect. 165 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 165–175, Denver, Colorado, June 4–5, 2015. Fortuna</context>
</contexts>
<marker>Putnam, 1976</marker>
<rawString>H. Putnam. 1976. Two dogmas revisited. printed in his (1983) Realism and Reason, Philosophical Papers, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ross Quillian</author>
</authors>
<title>Word concepts: A theory and simulation of some basic semantic capabilities.</title>
<date>1968</date>
<journal>Behavioral Science,</journal>
<pages>12--410</pages>
<contexts>
<context position="2454" citStr="Quillian (1968)" startWordPosition="353" endWordPosition="354">l., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going back at least to Quillian (1968) and Osgood et al. (1975) respectively, but we put the emphasis on the computational experiments we ran (source code and lexica available at github.com/kornai/4lang). 1 Background In the eyes of many, Quine (1951) has demolished the traditional analytic/synthetic distinction, relegating nearly all pre-Fregean accounts of word meaning from Aristotle to Locke to the dustbin of history. The opposing view, articulated clearly in Grice and Strawson (1956), is based on the empirical observation that people make the call rather uniformly over novel examples, an argument whose import is evident from t</context>
<context position="4182" citStr="Quillian (1968)" startWordPosition="629" endWordPosition="630">her - that is incorrect. 165 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 165–175, Denver, Colorado, June 4–5, 2015. Fortunately, one philosopher’s trash may just turn out to be another linguist’s treasure. What Putnam has demonstrated is that “a speaker can, by all reasonable standards, be in command of a word like water without being able to command the intension that would represent the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through</context>
</contexts>
<marker>Quillian, 1968</marker>
<rawString>M. Ross Quillian. 1968. Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12:410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willard van Orman Quine</author>
</authors>
<title>Two dogmas of empiricism.</title>
<date>1951</date>
<journal>The Philosophical Review,</journal>
<pages>60--20</pages>
<contexts>
<context position="2667" citStr="Quine (1951)" startWordPosition="388" endWordPosition="389">or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going back at least to Quillian (1968) and Osgood et al. (1975) respectively, but we put the emphasis on the computational experiments we ran (source code and lexica available at github.com/kornai/4lang). 1 Background In the eyes of many, Quine (1951) has demolished the traditional analytic/synthetic distinction, relegating nearly all pre-Fregean accounts of word meaning from Aristotle to Locke to the dustbin of history. The opposing view, articulated clearly in Grice and Strawson (1956), is based on the empirical observation that people make the call rather uniformly over novel examples, an argument whose import is evident from the (at the time, still nascent) cognitive perspective. Today, we may agree with Putnam (1976): ‘Bachelor’ may be synonymous with ‘unmarried man’ but that cuts no philosophic ice. ‘Chair’ may be synonymous with ‘mo</context>
</contexts>
<marker>Quine, 1951</marker>
<rawString>Willard van Orman Quine. 1951. Two dogmas of empiricism. The Philosophical Review, 60:20–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical Framework: Programming with Multilingual Grammars.</title>
<date>2011</date>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="29171" citStr="Ranta, 2011" startWordPosition="4840" endWordPosition="4841">such mechanisms within CVS semantics. ACR systems are considerably more transparent in this regard, and the kind of questions that we would want to pose as linguists have direct reflexes in the formal system. Many of the original theories of conceptual representation were Englishparticular, sometimes to the point of being as naive as the medieval theories of universal language (Eco, 1995). The most notable exception is NSM, clearly developed with the native languages of Australia in mind, and often exercised on Russian, Polish, and other IE examples as well. Here we follow the spirit of GFRG (Ranta, 2011) in assuming a common abstract syntax for all languages. For case grammar this requires some abstraction, for example English NPs must also get case marked (an idea also present in the ‘Case Theory’ of Government-Binding and related theories of transformational grammar). The main difference between English and the overtly case-marking languages such as Russian or Latin is that in English we compute the cases from prepositions and word order (position relative to the verb) rather than from overt morphological marking as standard. This way, the lexical entries can be kept highly abstract, and fo</context>
</contexts>
<marker>Ranta, 2011</marker>
<rawString>Aarne Ranta. 2011. Grammatical Framework: Programming with Multilingual Grammars. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´abor Recski</author>
<author>Judit ´Acs</author>
</authors>
<title>MathLingBudapest: Concept networks for semantic similarity.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<publisher>ACL.</publisher>
<location>Denver, CO,</location>
<marker>Recski, ´Acs, 2015</marker>
<rawString>G´abor Recski and Judit ´Acs. 2015. MathLingBudapest: Concept networks for semantic similarity. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>Conceptual dependency: A theory of natural language understanding.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5127" citStr="Schank (1972)" startWordPosition="772" endWordPosition="773">), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under construction by Hobbs and his coworkers (Hobbs, 2008; Gordon et al., 2011). What we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at </context>
</contexts>
<marker>Schank, 1972</marker>
<rawString>Roger C. Schank. 1972. Conceptual dependency: A theory of natural language understanding. Cognitive Psychology, 3(4):552–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a gpu for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21074" citStr="Schwenk et al., 2012" startWordPosition="3447" endWordPosition="3450">matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, and it is not clear what, if anything, can be salvaged from the truth-conditional approach to these matters. At the same time, the CVS approach to quantifiers is not mature, and ACR theories support generics only. These may look like backward steps, but keep in mind that our goal in competence modeling is to characterize everyday knowledge, shared by all competent speakers of the language, while quantifier and modal scope ambiguities are someth</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Sinclair</author>
</authors>
<title>Looking up: an account of the COBUILD project in lexical computing. Collins ELT.</title>
<date>1987</date>
<contexts>
<context position="4515" citStr="Sinclair, 1987" startWordPosition="678" endWordPosition="679"> standards, be in command of a word like water without being able to command the intension that would represent the word in possible worlds semantics” (Partee, 1979). Computational systems of Knowledge Representation, starting with the Teachable Word Comprehender of Quillian (1968), and culminating in the Deep Lexical Semantics of Hobbs (2008), carried on this tradition of analyzing word meaning in terms of ‘essential’ or ‘analytic’ components. A particularly important step in this direction is the emergence of modern, computationally oriented lexicographic work beginning with CollinsCOBUILD (Sinclair, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of S</context>
</contexts>
<marker>Sinclair, 1987</marker>
<rawString>John M. Sinclair. 1987. Looking up: an account of the COBUILD project in lexical computing. Collins ELT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Smith</author>
</authors>
<title>Restrictiveness in Case Theory.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="28461" citStr="Smith, 1996" startWordPosition="4723" endWordPosition="4724"> so close to the NSM lower bound of 65 that a blow-by-blow comparison would be justified. 171 5 Universality The final issue one needs to investigate in assessing the potential of any purported competence theory is that of universality versus language particularity. For CVS theories, this is rather easy: we have one system of representation, finite dimensional vector spaces, which admits no typological variation, let alone language-specific mechanisms – one size fits all. As linguists, we see considerable variation among the surface, and possibly even among the deeper aspects of case linking (Smith, 1996), but as computational modelers we lack, as of yet, a better understanding of what corresponds to such mechanisms within CVS semantics. ACR systems are considerably more transparent in this regard, and the kind of questions that we would want to pose as linguists have direct reflexes in the formal system. Many of the original theories of conceptual representation were Englishparticular, sometimes to the point of being as naive as the medieval theories of universal language (Eco, 1995). The most notable exception is NSM, clearly developed with the native languages of Australia in mind, and ofte</context>
</contexts>
<marker>Smith, 1996</marker>
<rawString>Henry Smith. 1996. Restrictiveness in Case Theory. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<pages>216</pages>
<contexts>
<context position="23417" citStr="Smolensky, 1990" startWordPosition="3820" endWordPosition="3821">.) While there are considerable technical difficulties of formula manipulation involved, this is really one area where the classical theory shines as a competence theory – we cannot even imagine to create a learning algorithm that would cover the meaning of infinitely many complex expressions unless we had some means of combining the meanings of the lexical entries. CVS semantics offers several ways of combining lexical entries, the simplest being simply adding the vectors together (Mitchell and Lapata, 2008), but the use of linear transformations (Lazaridou et al., 2013) and tensor products (Smolensky, 1990) has also been contemplated. Currently, an approach that combines the vectors of the parts to form the vector of the whole by recurrent neural nets appears to work best (Socher et al., 2013), but this is still an area of intense research and it would be premature to declare this method the winner. Here we concentrate on ACR, investigating the issue of the inventory of graph edge colors on the same core vocabulary as discussed above. The key technical problem is to bring the variety of links between verbs and their arguments under control: as Woods (1975) already notes, the naive ACR theories a</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1):159– 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>M Ganjoo</author>
<author>H Sridhar</author>
<author>O Bastani</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In International Conference on Learning Representations (ICLR).</booktitle>
<contexts>
<context position="23607" citStr="Socher et al., 2013" startWordPosition="3852" endWordPosition="3855"> imagine to create a learning algorithm that would cover the meaning of infinitely many complex expressions unless we had some means of combining the meanings of the lexical entries. CVS semantics offers several ways of combining lexical entries, the simplest being simply adding the vectors together (Mitchell and Lapata, 2008), but the use of linear transformations (Lazaridou et al., 2013) and tensor products (Smolensky, 1990) has also been contemplated. Currently, an approach that combines the vectors of the parts to form the vector of the whole by recurrent neural nets appears to work best (Socher et al., 2013), but this is still an area of intense research and it would be premature to declare this method the winner. Here we concentrate on ACR, investigating the issue of the inventory of graph edge colors on the same core vocabulary as discussed above. The key technical problem is to bring the variety of links between verbs and their arguments under control: as Woods (1975) already notes, the naive ACR theories are characterized by a profusion of link types (graph edge colors). We created a version of ACR that is limited to three link types. Both the usual network representations (digraphs, as in Fi</context>
</contexts>
<marker>Socher, Ganjoo, Sridhar, Bastani, Manning, Ng, 2013</marker>
<rawString>R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. D. Manning, and A. Y. Ng. 2013. Zero-shot learning through cross-modal transfer. In International Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1822" citStr="Turian et al., 2010" startWordPosition="254" endWordPosition="257"> (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-specificity and universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with significant precursors going </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Tomas Mikolov</author>
</authors>
<title>Towards ai-complete question answering: A set of prerequisite toy tasks.</title>
<date>2015</date>
<pages>1502--05698</pages>
<contexts>
<context position="32484" citStr="Weston et al., 2015" startWordPosition="5385" endWordPosition="5388">s. 6 Conclusion It is not particularly surprising that both CVS and ACR, originally designed as performance theories, fare considerably better in the performance realm than Montagovian semantics, especially as detailed intensional lexica have never been crafted, and Dowty (1979) remains, to this day, the path not taken in formal semantics. It is only on the subdomain of the logic puzzles involving Booleans and quantification that Montagovian solutions showed any promise, and these, with the exception of elementary negation, do not even appear in more down to 172 earth evaluation sets such as (Weston et al., 2015). The surprising conclusion of our work is that standard Montagovian semantics also falls short in the competence realm, where the formal theory has long been promoted as offering psychological reality. We have compared CVS and ACR theories of lexical semantics to the classical approach based on meaning postulates by the usual criteria for competence theories. In Section 2 we have seen that both ACR and CVS are better in terms of learnability than the standard formal theory, and it is worth noting that the number of ACR primitives, 129 in the version implemented here, is less than the dimensio</context>
</contexts>
<marker>Weston, Bordes, Chopra, Mikolov, 2015</marker>
<rawString>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv:1502.05698.</rawString>
</citation>
<citation valid="true">
<title>The roots of the Sanskrit language.</title>
<date>1869</date>
<journal>Transactions of the American Philological Association</journal>
<pages>16--5</pages>
<marker>1869</marker>
<rawString>William Dwight Whitney. 1885. The roots of the Sanskrit language. Transactions of the American Philological Association (1869-1896), 16:5–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Wierzbicka</author>
</authors>
<title>Lexicography and conceptual analysis.</title>
<date>1985</date>
<location>Karoma, Ann Arbor.</location>
<contexts>
<context position="1631" citStr="Wierzbicka, 1985" startWordPosition="223" endWordPosition="224">antics remains perhaps the most significant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is finite, while Montague assumed that words are characterized by intensions, formal objects that require an infinite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on finite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in finitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of finite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics </context>
<context position="5187" citStr="Wierzbicka (1985)" startWordPosition="780" endWordPosition="781">) (Boguraev and Briscoe, 1989), WordNet (Miller, 1995), FrameNet (Fillmore and Atkins, 1998), and VerbNet (Kipper et al., 2000). Both the network- and the vector-based approach build on these efforts, but through very different routes. Traditional network theories of Knowledge Representation tend to concentrate on nominal features such as the IS A links (called hypernyms in WordNet) and treat the representation of verbs somewhat haphazardly. The first systems with a well-defined model of predication are the Conceptual Dependency model of Schank (1972), the Natural Syntax Metalanguage (NSM) of Wierzbicka (1985), and a more elaborate deep lexical semantics system that is still under construction by Hobbs and his coworkers (Hobbs, 2008; Gordon et al., 2011). What we call algebraic conceptual representation (ACR) is any such theory encoded with colored directed edges between the basic conceptual units. The algebraic approach provides a better fit with functional programming than the more declarative, automata-theoretic approach (Huet and Razet, 2008), and makes it possible to encode verbal subcategorization (case frame) information that is at the heart of FrameNet and VerbNet in addition to the standar</context>
</contexts>
<marker>Wierzbicka, 1985</marker>
<rawString>Anna Wierzbicka. 1985. Lexicography and conceptual analysis. Karoma, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>What’s in a link: Foundations for semantic networks.</title>
<date>1975</date>
<booktitle>Representation and Understanding: Studies in Cognitive Science,</booktitle>
<pages>35--82</pages>
<contexts>
<context position="23977" citStr="Woods (1975)" startWordPosition="3919" endWordPosition="3920"> et al., 2013) and tensor products (Smolensky, 1990) has also been contemplated. Currently, an approach that combines the vectors of the parts to form the vector of the whole by recurrent neural nets appears to work best (Socher et al., 2013), but this is still an area of intense research and it would be premature to declare this method the winner. Here we concentrate on ACR, investigating the issue of the inventory of graph edge colors on the same core vocabulary as discussed above. The key technical problem is to bring the variety of links between verbs and their arguments under control: as Woods (1975) already notes, the naive ACR theories are characterized by a profusion of link types (graph edge colors). We created a version of ACR that is limited to three link types. Both the usual network representations (digraphs, as in Figs. 1 and 2 above) and a more algebraic model composed of extended finite state automata (Eilenberg machines) are produced by parsing formulas defined by a formal grammar summarized in Figure 3. For ease of reading, in unary predication (e.g. mouse →− rodent) 0 we permit both prefix and suffix order, but with different kinds of parens mouse[rodent] and rodent(mouse); </context>
</contexts>
<marker>Woods, 1975</marker>
<rawString>William A. Woods. 1975. What’s in a link: Foundations for semantic networks. Representation and Understanding: Studies in Cognitive Science, pages 35– 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Jun Zhou</author>
</authors>
<title>Spin glass approach to the feedback vertex set problem. ms,</title>
<date>2013</date>
<pages>1307--6948</pages>
<contexts>
<context position="11850" citStr="Zhou 2013" startWordPosition="1902" endWordPosition="1903">1 LDOCE (all senses) 79,414 1,061 LDOCE (first senses) 34,284 376 CED (all senses) 154,061 6,490 CED (first senses) 80,495 3,435 en.wiktionary (all senses) 369,281 2,504 en.wiktionary (first senses) 304,029 1,845 formal 2,754 129 Table 1: Properties of four different dictionaries While a feedback vertex set is guaranteed to exist for any digraph (if all else fails, the entire set of vertices will do), it is not guaranteed that there exists one that is considerably smaller than the entire graph. (For random digraphs in general see Dutta and Subramanian 2010, for highly symmetrical lattices see Zhou 2013 ms.) In random digraphs under relatively mild conditions on the proportion of edges relative to nodes, Łuczak and Seierstad (2009) show that a strong component essentially the size of the entire graph will exist. Fortunately, digraphs built on definitions are not at all behaving in a random fashion, the strongly connected components are relatively small, as Table 1 makes evident. For example, in the English Wiktionary, 369,281 definitions can be reduced to a core set of 2,504 defining words, and in CED we can find a defining set of 6,490 words, even though these dictionaries, unlike LDOCE, we</context>
</contexts>
<marker>Zhou, 2013</marker>
<rawString>Hai-Jun Zhou. 2013. Spin glass approach to the feedback vertex set problem. ms, arxiv.org/pdf/1307.6948v2.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas E Zimmermann</author>
</authors>
<title>Meaning postulates and the model-theoretic approach to natural language semantics. Linguistics and Philosophy,</title>
<date>1999</date>
<pages>22--529</pages>
<contexts>
<context position="33396" citStr="Zimmermann 1999" startWordPosition="5539" endWordPosition="5540"> meaning postulates by the usual criteria for competence theories. In Section 2 we have seen that both ACR and CVS are better in terms of learnability than the standard formal theory, and it is worth noting that the number of ACR primitives, 129 in the version implemented here, is less than the dimensions of the best performing CVS embeddings, 150-300 after data compression by PCA or similar methods. In Section 3 we have seen that lexical relatedness tasks also favor ACR and CVS over the meaning postulate approach (for a critical overview of meaning postulates in model-theoretic semantics see Zimmermann 1999), and in Section 4 we have seen that compositionality poses no problems for ACR. How compositional semantics is handled in CVS semantics remains to be seen, but the problem is not a dearth of plausible mechanisms, but rather an overabundance of these. Acknowledgments The 4lang conceptual dictionary is the work of many people over the years. The name is no longer quite justified, in that natural language bindings, automatically generated and thus not entirely free of errors and omissions, now exist for 50 languages ( ´Acs et al., 2013), many of them outside the Indoeuropean and Finnougric famil</context>
</contexts>
<marker>Zimmermann, 1999</marker>
<rawString>Thomas E. Zimmermann. 1999. Meaning postulates and the model-theoretic approach to natural language semantics. Linguistics and Philosophy, 22:529–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
</authors>
<title>Explicit representation of antonymy in language modeling.</title>
<date>2014</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="20834" citStr="Zweig, 2014" startWordPosition="3409" endWordPosition="3410">competitive system for measuring semantic textual similarity (Recski and ´Acs, 2015). In Section 4 we discuss the ‘formal’ network of Table 1 built directly on the concept formulae. By spectral dimension reduction of the incidence matrix of this network we can create an embedding that yields results on world similarity tasks comparable to those obtained from corpus-based embeddings (Makrai et al., 2013). CVS models can be explicitly tested on their ability to recover synonymy by searching for the nearest word in the sample (Mikolov et al., 2013b); antonymy by reversing the sign of the vector (Zweig, 2014); and in general for all kinds of analogical 169 statements such as king is to queen as man is to woman by vector addition and subtraction (Mikolov et al., 2013c); not to speak of cross-language paraphrase/translation (Schwenk et al., 2012), long viewed a key intermediary step toward explaining competence in a foreign language. Currently, CVS systems are clearly in the lead on such tasks, and it is not clear what, if anything, can be salvaged from the truth-conditional approach to these matters. At the same time, the CVS approach to quantifiers is not mature, and ACR theories support generics </context>
</contexts>
<marker>Zweig, 2014</marker>
<rawString>Geoffrey Zweig. 2014. Explicit representation of antonymy in language modeling. Technical report, Microsoft Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>