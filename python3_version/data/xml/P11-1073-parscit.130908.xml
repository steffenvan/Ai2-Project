<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015123">
<title confidence="0.99724">
Computing and Evaluating Syntactic Complexity Features for
Automated Scoring of Spontaneous Non-Native Speech
</title>
<author confidence="0.996163">
Miao Chen Klaus Zechner
</author>
<affiliation confidence="0.997985">
School of Information Studies NLP &amp; Speech Group
Syracuse University Educational Testing Service
</affiliation>
<address confidence="0.795418">
Syracuse, NY, USA Princeton, NJ, USA
</address>
<email confidence="0.999175">
mchen14@syr.edu kzechner@ets.org
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921291666667">
This paper focuses on identifying, extracting
and evaluating features related to syntactic
complexity of spontaneous spoken responses as
part of an effort to expand the current feature
set of an automated speech scoring system in
order to cover additional aspects considered
important in the construct of communicative
competence.
Our goal is to find effective features, se-
lected from a large set of features proposed
previously and some new features designed in
analogous ways from a syntactic complexity
perspective that correlate well with human rat-
ings of the same spoken responses, and to build
automatic scoring models based on the most
promising features by using machine learning
methods.
On human transcriptions with manually
annotated clause and sentence boundaries, our
best scoring model achieves an overall Pearson
correlation with human rater scores of r=0.49
on an unseen test set, whereas correlations of
models using sentence or clause boundaries
from automated classifiers are around r=0.2.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998593564102564">
Past efforts directed at automated scoring of
speech have used mainly features related to fluen
cy (e.g., speaking rate, length and distribution of
pauses), pronunciation (e.g., using log-likelihood
scores from the acoustic model of an Automatic
Speech Recognition (ASR) system), or prosody
(e.g., information related to pitch contours or syl-
lable stress) (e.g., Bernstein, 1999; Bernstein et
al., 2000; Bernstein et al., 2010; Cucchiarini et al.,
1997; Cucchiarini et al., 2000; Franco et al., 2000a;
Franco et al., 2000b; Zechner et al., 2007, Zechner
et al., 2009).
While this approach is a good match to most of
the important properties related to low entropy
speech (i.e., speech which is highly predictable),
such as reading a passage aloud, it lacks many im-
portant aspects of spontaneous speech which are
relevant to be evaluated both by a human rater and
an automated scoring system. Examples of such
aspects of speech, which are considered part of the
construct1 of “communicative competence (Bach-
man, 1990), include grammatical accuracy, syntac-
tic complexity, vocabulary diversity, and aspects of
spoken discourse structure, e.g., coherence and
cohesion. These different aspects of speaking pro-
ficiency are often highly correlated in a non-native
speaker (Xi and Mollaun, 2006; Bernstein et al.,
2010), and so scoring models built solely on fea-
tures of fluency and pronunciation may achieve
reasonably high correlations with holistic human
rater scores. However, it is important to point out
that such systems would still be unable to assess
many important aspects of the speaking construct
and therefore cannot be seen as ideal from a validi-
ty point of view.2
The purpose of this paper is to address one of
these important aspects of spoken language in
more detail, namely syntactic complexity. This
paper can be seen as a first step toward including
</bodyText>
<footnote confidence="0.9827522">
1 A construct is a set of knowledge, skills, and abilities
measured by a test.
2 “Construct validity” refers to the extent that a test measures
what it is designed to measure, in this case, communicative
competence via speaking.
</footnote>
<page confidence="0.851737">
722
</page>
<note confidence="0.9806415">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722–731,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.929656443478261">
features related to this part of the speaking con- 2 Related Work
struct into an already existing automated speech
scoring system for spontaneous speech which so
far mostly uses features related to fluency and pro-
nunciation (Zechner et al., 2009).
We use data from the speaking section of the
TOEFL® Practice Online (TPO) test, which is a
low stakes practice test for non-native speakers
where they are asked to provide six spontaneous
speech samples of about one minute in length each
in response to a variety of prompts. Some prompts
may be simple questions, and others may involve
reading or listening to passages first and then ans-
wering related questions. All responses were
scored holistically by human raters according to
pre-defined scoring rubrics (i.e., specific scoring
guidelines) on a scale of 1 to 4, 4 being the highest
proficiency level.
In our automated scoring system, the first com-
ponent is an ASR system that decodes the digitized
speech sample, generating a time-annotated hypo-
thesis for every response. Next, fluency and pro-
nunciation features are computed based on the
ASR output hypotheses, and finally a multiple re-
gression scoring model, trained on human rater
scores, computes the score for a given spoken re-
sponse (see Zechner et al. (2009) for more details).
We conducted the study in three steps: (1) finding
important measures of syntactic complexity from
second language acquisition (SLA) and English
language learning (ELL) literature, and extending
this feature set based on our observations of the
TPO data in analogous ways; (2) computing fea-
tures based on transcribed speech responses and
selecting features with highest correlations to hu-
man rater scores, also considering their compara-
tive values for native speakers taking the same test;
and (3) building scoring models for the selected
sub-set of the features to generate a proficiency
score for each speaker, using all six responses of
that speaker.
In the remainder of the paper, we will address
related work in syntactic complexity (Section 2),
introduce the speech data sets of our study (Section
3), describe the methods we used for feature ex-
traction (Section 4), provide the experiment design
and results (Section 5), analyze and discuss the
results in Section 6, before concluding the paper
(Section 7).
723
2.1 Literature on Syntactic Complexity
Syntactic complexity is defined as “the range of
forms that surface in language production and the
degree of sophistication of such forms” (Ortega,
2003). It is an important factor in the second lan-
guage assessment construct as described in Bach-
man’s (1990) conceptual model of language
ability, and therefore is often used as an index of
language proficiency and development status of L2
learners. Various studies have proposed and inves-
tigated measures of syntactic complexity as well as
examined its predictiveness for language profi-
ciency, in both L2 writing and speaking settings,
which will be reviewed respectively.
Writing
Wolfe-Quintero et al. (1998) reviewed a number of
grammatical complexity measures in L2 writing
from thirty-nine studies, and their usage for pre-
dicting language proficiency was discussed. Some
examples of syntactic complexity measures are:
mean number of clauses per T-unit3 , mean length
of clauses, mean number of verbs per sentence, etc.
The various measures can be grouped into two cat-
egories: (1) clauses, sentences, and T-units in
terms of each other; and (2) specific grammatical
structures (e.g., passives, nominals) in relation to
clauses, sentences, or T-units (Wolfe-Quintero et
al., 1998). Three primary methods of calculating
syntactic complexity measures are frequency, ratio,
and index, where frequency is the count of occur-
rences of a specific grammatical structure, ratio is
the number of one type of unit divided by the total
number of another unit, and index is computing
numeric scores by specific formulae (Wolfe-
Quintero et al., 1998). For example, the measure
“mean number of clauses per T-unit” is obtained
by using the ratio calculation method and the
clause and T-unit grammatical structures. Some
structures such as clauses and T-units only need
shallow linguistic processing to acquire, while
some require parsing. There are numerous combi-
nations for measures and we need empirical evi-
3 T-units are defined as “shortest grammatically allowable
sentences into which (writing can be split) or minimally
terminable units” (Hunt, 1965:20).
dence to select measures with the highest perfor-
mance.
There have been a series of empirical studies
examining the relationship of syntactic complexity
measures to L2 proficiency using real-world data
(Cooper, 1976; Larsen-Freeman, 1978; Perkins,
1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003;
Lu, 2010). The studies investigate measures that
highly correlate with proficiency levels or distin-
guish between different proficiency levels. Many
T-unit related measures were identified as statisti-
cally significant indicators to L2 proficiency, such
as mean length of T-unit (Henry, 1996; Lu, 2010),
mean number of clauses per T-unit (Cooper, 1976;
Lu, 2010), mean number of complex nominals per
T-unit (Lu, 2010), or the mean number of error-
free T-units per sentence (Ho-Peng, 1983). Other
significant measures are mean length of clause (Lu,
2010), or frequency of passives in composition
(Kameen, 1979).
</bodyText>
<subsectionHeader confidence="0.737314">
Speaking
</subsectionHeader>
<bodyText confidence="0.999627276595745">
Syntactic complexity analysis in speech mainly
inherits measures from the writing domain, and the
abovementioned measures can be employed in the
same way on speech transcripts for complexity
computation. A series of studies have examined
relations between the syntactic complexity of
speech and the speakers’ holistic speaking profi-
ciency levels (Halleck, 1995; Bernstein et al.,
2010; Iwashita, 2006). Three objective measures of
syntactic complexity, including mean T-unit
length, mean error-free T-unit length, and percent
of error-free T-units were found to correlate with
holistic evaluations of speakers in Halleck (1995).
Iwashita’s (2006) study on Japanese L2 speakers
found that length-based complexity features (i.e.,
number of T-units and number of clauses per T-
unit) are good predictors for oral proficiency. In
studies directly employing syntactic complexity
measures in other contexts, ratio-based measures
are frequently used. Examples are mean length of
utterance (Condouris et al., 2003), word count or
tree depth (Roll et al., 2007), or mean length of T-
units and mean number of clauses per T-unit
(Bernstein et al., 2010). Frequency-based measures
were used less, such as number of full phrases in
Roll et al. (2007).
The speaking output is usually less clean than
writing data (e.g., considering disfluencies such as
false starts, repetitions, filled pauses etc.). There-
fore we may need to remove these disfluencies first
before computing syntactic complexity features.
Also, importantly, ASR output does not contain
interpunctuation but both for sentential-based fea-
tures as well as for parser-based features, the
boundaries of clauses and sentences need to be
known. For this purpose, we will use automated
classifiers that are trained to predict clause and
sentence boundaries, as described in Chen et al.
(2010). With previous studies providing us a rich
pool of complexity features, additionally we also
develop features analogous to the ones from the
literature, mostly by using different calculation
methods. For instance, the frequency of Preposi-
tional Phrases (PPs) is a feature from the literature,
and we add some variants such as number of PPs
per clause as a new feature to our extended feature
set.
</bodyText>
<subsectionHeader confidence="0.99901">
2.2 Devising the Initial Feature Set
</subsectionHeader>
<bodyText confidence="0.999988217391304">
Through this literature review, we identified some
important features that were frequently used in
previous studies in both L2 speaking and writing,
such as length of sentences and number of clauses
per sentence. In addition, we also collected candi-
date features that were less frequently mentioned
in the literature, in order to start with a larger field
of potential candidate features. We further ex-
tended the feature set by inspecting our data, de-
scribed in the following section, and created
suitable additional features by means of analogy.
This process resulted in a set of 91 features, 11 of
which are related to clausal and sentential unit
measurements (frequency-based) and 80 to mea-
surements within such units (ratio-based). From
the perspective of extracting measures, in our study,
some measures can be computed using only clause
and sentence boundary information, and some can
be derived only if the spoken responses are syntac-
tically parsed. In our feature set, there are two
types of features: clause and sentence boundary
based (26 in total) and parsing based (65). The fea-
tures will be described in detail in Section 4.
</bodyText>
<sectionHeader confidence="0.997305" genericHeader="introduction">
3 Data
</sectionHeader>
<bodyText confidence="0.9997682">
Our data set contains (1) 1,060 non-native speech
responses of 189 speakers from the TPO test (NN
set), and (2) 100 responses from 48 native speakers
that took the same test (Nat set). All responses
were verbatim transcribed manually and scored
</bodyText>
<page confidence="0.98851">
724
</page>
<bodyText confidence="0.999874909090909">
holistically by human raters. (We only made use of
the scores for the non-native data set in this study,
since we purposefully selected speakers with per-
fect or near perfect scores for the Nat set from a
larger native speech data set.) As mentioned above,
there are four proficiency levels for human scoring,
levels 1 to 4, with higher levels indicating better
speaking proficiency.
The NN set was randomly partitioned into a
training (NN-train) and a test set with 760 and 300
responses, respectively, and no speaker overlap.
</bodyText>
<table confidence="0.995982054054054">
Data Res- Speakers Responses per
Set ponses Speaker
(average)
NN- 760 137 5.55
train
Description: used to train sentence and
clause boundary detectors, evaluate fea-
tures and train scoring models
NN- 300 52 5.77
test-1-
Hum
Description: human transcriptions and
annotations of sentence and clause boun-
daries
NN- 300 52 5.77
test-2-
CB
Description: human transcriptions, au-
tomatically predicted clause boundaries
NN- 300 52 5.77
test-3-
SB
Description: human transcriptions, au-
tomatically predicted sentence bounda-
ries
NN- 300 52 5.77
test-4-
ASR-
CB
Description: ASR hypotheses, automati-
cally predicted clause boundaries
NN- 300 52 5.77
test-5-
ASR-
SB
Description: ASR hypotheses, automati-
cally predicted sentence boundaries
</table>
<tableCaption confidence="0.999875">
Table 1. Overview of non-native data sets.
</tableCaption>
<bodyText confidence="0.997286614035088">
A second version of the test set contains ASR
hypotheses instead of human transcriptions. The
word error rate (WER4) on this data set is 50.5%.
4 Word error rate (WER) is the ratio of errors from a string
between the ASR hypothesis and the reference transcript,
where the sum of substitutions, insertions, and deletions is
We used a total of five variants of the test sets, as
described in Table 1. Sets 1-3 are based on human
transcriptions, whereas sets 4 and 5 are based on
ASR output. Further, set 1 contains human anno-
tated clause and sentence boundaries, whereas the
other 4 sets have clause or sentence boundaries
predicted by a classifier.
All human transcribed files from the NN data
set were annotated for clause boundaries, clause
types, and disfluencies by human annotators (see
Chen et al. (2010)).
For the Nat data set, all of the 100 transcribed
responses were annotated in the same manner by a
human annotator. They are not used for any train-
ing purposes but serve as a comparative reference
for syntactic complexity features derived from the
non-native corpus.
The NN-train set was used both for training
clause and sentence boundary classifiers, as well as
for feature selection and training of the scoring
models. The two boundary detectors were machine
learning based Hidden Markov Models, trained by
using a language model derived from the 760 train-
ing files which had sentence and clause boundary
labels (NN-train; see also Chen et al. (2010)).
Since a speaker’s response to a single test item
can be quite short (fewer than 100 words in many
cases), it may contain only very few syntactic
complexity features we are looking for. (Note that
much of the previous work focused on written lan-
guage with much longer texts to be considered.)
However, if we aggregate responses of a single
speaker, we have a better chance of finding a larger
number of syntactic complexity features in the ag-
gregated file. Therefore we joined files from the
same speaker to one file for the training set and the
five test sets, resulting in 52 aggregated files in
each test set. Accordingly, we averaged the re-
sponse scores of a single speaker to obtain the total
speaker score to be used later in scoring model
training and evaluation (Section 5).5
While disfluencies were used for the training of
the boundary detectors, they were removed after-
wards from the annotated data sets to obtain a tran-
divided by the length of the reference. To obtain WER in
percent, this ratio is multiplied by 100.0.
5 Although in most operational settings, features are derived
from single responses, this may not be true in all cases.
Furthermore, scores of multiple responses are often combined
for score reporting, which would make such an approach
easier to implement and argue for operationally.
</bodyText>
<page confidence="0.991895">
725
</page>
<bodyText confidence="0.999643">
scription which is “cleaner” and lends itself better
to most of the feature extraction methods we use.
</bodyText>
<sectionHeader confidence="0.990522" genericHeader="method">
4 Feature Extraction
</sectionHeader>
<subsectionHeader confidence="0.984362">
4.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.999982707317073">
As mentioned in Section 2, we gathered 91 candi-
date syntactic complexity features based on our
literature review as initial feature set, which is
grouped into two categories: (1) Clause and sen-
tence Boundary based features (CB features); and
(2) Parse Tree based features (PT features). Clause
based features are based on both clause boundaries
and clause types and can be generated from human
clause annotations, e.g., “frequency of adjective
clauses6 per one thousand words”, “mean number
of dependent clauses per clause”, etc. Parse tree
based features refer to features that are generated
from parse trees and cannot be extracted from hu-
man annotated clauses directly.
We first selected features showing high correla-
tion to human assigned scores. In this process the
CB features were computed from human labeled
clause boundaries in transcripts for best accuracy,
and PT features were calculated from using parsing
and other tools because we did not have human
parse tree annotations for our data.
We used the Stanford Parser (Klein and Man-
ning, 2003) in conjunction with the Stanford Tre-
gex package (Levy and Andrew, 2006) which
supports using rules to extract specific configura-
tions from parse trees, in a package put together by
Lu (Lu, 2011). When given a sentence, the Stan-
ford Parser outputs its grammatical structure by
grouping words (and phrases) in a tree structure
and identifies grammatical roles of words and
phrases.
Tregex is a tree query tool that takes Stanford
parser trees as input and queries the trees to find
subtrees that meet specific rules written in Tregex
syntax (Levy and Andrew, 2006). It uses relational
operators regulated by Tregex, for example, “A &lt;&lt;
B” stands for “subtree A dominates subtree B”.
The operators primarily function in subtree prece-
dence, dominance, negation, regular expression,
tree node identity, headship, or variable groups,
among others (Levy and Andrew, 2006).
</bodyText>
<footnote confidence="0.939421">
6 An adjective clause is a clause that functions as an adjective
in modifying a noun. E.g., “This cat is a cat that is difficult to
deal with.”
</footnote>
<bodyText confidence="0.9900978">
Lu’s tool (Lu, 2011), built upon the Stanford
Parser and Tregex, does syntactic complexity anal-
ysis given textual data. Lu’s tool contributed 8 of
the initial CB features and 6 of the initial PT fea-
tures, and we computed the remaining CB and PT
features using Perl scripts, the Stanford Parser, and
Tregex.
Table 2 lists the sub-set of 17 features (out of 91
features total) that were used for building the scor-
ing models described later (Section 5).
</bodyText>
<subsectionHeader confidence="0.983804">
4.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.964245625">
We determined the importance of the features by
computing each feature’s correlation with human
raters’ proficiency scores based on the training set
NN-train. We also used criteria related to the
speaking construct, comparisons with native
speaker data, and feature inter-correlations. While
approaches coming from a pure machine learning
perspective would likely use the entire feature pool
as input for a classifier, our goal here is to obtain
an initial feature set by judicious and careful fea-
ture selection that can withstand the scrutiny of
construct validity in assessment development.
As noted earlier, the disfluencies in the training set
had been removed to obtain a “cleaner” text that
looks somewhat more akin to a written passage and
is easier to process by NLP modules such as pars-
ers and part-of-speech (POS) taggers.7 The ex-
tracted features partly were taken directly from
proposals in the literature and partly were slightly
modified to fit our clause annotation scheme. In
order to have a unified framework for computing
syntactic complexity features, we used a combina-
tion of the Stanford Parser and Tregex for compu-
ting both clause- and sentence-based features as
well as parse-tree-based features, i.e., we did not
make use of the human clause boundary label an-
notations here. The only exception to this
7 We are aware that disfluencies can provide valuable clues
about spoken proficiency in and of themselves; however, this
study is focused exclusively on syntactic complexity analysis,
and in this context, disfluencies would distort the picture
considerably due to the introduction of parsing errors, e.g.
</bodyText>
<page confidence="0.99528">
726
</page>
<table confidence="0.999888277777778">
Name Type8 Meaning Correlation Regression
MLS CB Mean length of sentences 0.329 0.101
MLT CB Mean length of T-units 0.300 -0.059
DC/C CB Mean number of dependent clauses per clause 0.291 2.873
SSfreq CB Frequency of simple sentences per 1000 words -.0242 0.001
MLSS CB Mean length of simple sentences 0.255 0.040
ADJCfreq CB Frequency of adjective clauses per 1000 words 0.253 0.004
Ffreq CB Frequency of fragments per 1000 words -0.386 -0.057
MLCC CB Mean length of coordinate clauses 0.224 0.017
CT/T PT Mean number of complex T-units per T-unit 0.248 0.908
PP_ling/S PT Mean number of linguistically meaningful prepositional phrases (PP) per sentence9 0.310 0.423
NP/S PT Mean number of noun phrases (NP) per sentence 0.244 -0.411
CN/S PT Mean number of complex nominal per sentence 0.325 0.653
VB _ling/T PT Mean number of linguistically meaningful10 verb phrases per T-unit 0.273 -0.780
PAS/S PT Mean number of passives per sentence 0.260 1.520
DI/T PT Mean number of dependent infinitives per T-unit 0.325 1.550
MLev PT Mean number of parsing tree levels per sentence 0.306 -0.134
MPSam PT Mean P-based Sampson11 per sentence 0.254 0.234
</table>
<tableCaption confidence="0.99982">
Table 2. List of syntactic complexity features selected to be included in building the scoring models.
</tableCaption>
<bodyText confidence="0.9658504">
is that we are using human clause and sentence
labels to create a candidate set for the clause boun-
dary features evaluated by the Stanford Parser and
Tregex, as explained in the following subsection.
8 Feature type: CB=Clause boundary based feature type,
PT=Parse tree based feature type
9A “linguistically meaningful PP” (PP_ling) is defined as a PP
immediately dominated by another PP in cases where a
preposition contains a noun such as “in spite of” or “in front
of”. An example would be “she stood in front of a house”
where “in front of a house” would be parsed as two embedded
PPs but only the top PP would be counted in this case.
10 A “linguistically meaningful VP” (VP_ling) is defined as a
verb phrase immediately dominated by a clausal phrase, in
order to avoid VPs embedded in another VP, e.g., &amp;quot;should go
to work&amp;quot; is identified as one VP instead of two embedded
VPs.
11 The “P-based Sampson” is a raw production-based measure
(Sampson, 1997), defined as &amp;quot;proportion of the daughters of a
nonterminal node which are themselves nonterminal and
nonrightmost, averaged over the nonterminals of a sentence&amp;quot;.
Clause and Sentence based Features (CB fea-
tures)
Firstly, we extracted all 26 initial CB features di-
rectly from human annotated data of NN-train, us-
ing information from the clause and sentence type
labels. The reasoning behind this was to create an
initial pool of clause-based features that reflects
the distribution of clauses and sentences as accu-
rately as possible, even though we did not plan to
use this extraction method operationally, where the
parser decides on clause and sentence types. After
computing the values of each CB feature, we cal-
culated correlations between each feature and hu-
man-rated scores. Then we created an initial CB
feature pool by selecting features that met two cri-
teria: (1) the absolute Pearson correlation coeffi-
cient with human scores was larger than 0.2; and
(2) the mean value of the feature on non-native
speakers was at least 20% lower than that for na-
</bodyText>
<page confidence="0.992732">
727
</page>
<bodyText confidence="0.999950454545455">
tive speakers in case of positive correlation and at
least by 20% higher than for native speakers in
case of negative correlation, using the Nat data set
for the latter criterion. Note that all of these fea-
tures were computed without using a parser. This
resulted in 13 important features.
Secondly, Tregex rules were developed based
on Lu’s tool to extract these 13 CB features from
parsing results where the parser is provided with
one sentence at a time. By applying the same selec-
tion criteria as before, except for allowing for cor-
relations above 0.1 and giving preference to
linguistically more meaningful features, we found
8 features that matched our criteria:
MLS, MLT, DC/C, SSfreq, MLSS, ADJCfreq,
Ffreq, MLCC
All 28 pairwise inter-correlations between these
8 features were computed and inspected to avoid
including features with high inter-correlations in
the scoring model. Since we did not find any inter-
correlations larger than 0.9, the features were con-
sidered moderately independent and none of them
were removed from this set so it also maintains
linguistic richness for the feature set.
Due to the importance of T-units in complexity
analysis, we briefly introduce how we obtain them
from annotations. Three types of clauses labeled in
our transcript can serve as T-units, including sim-
ple sentences, independent clauses, and conjunct
(coordination) clauses. These clauses were identi-
fied in the human-annotated text and extracted as
T-units in this phase. T-units in parse trees are
identified using rules in Lu’s tool.
</bodyText>
<subsectionHeader confidence="0.545435">
Parse Tree based Features (PT features)
</subsectionHeader>
<bodyText confidence="0.999947692307692">
We evaluated 65 features in total and selected fea-
tures with highest importance using the following
two criteria (which are very similar as before): (1)
the absolute Pearson correlation coefficient with
human scores is larger than 0.2; and (2) the feature
mean value on native speakers (Nat) is higher than
on score 4 for non-native speakers in case of posi-
tive correlation, or lower for negative correlation.
20 of 65 features were found to meet the require-
ments.
Next, we examined inter-correlations between
these features and found some correlations larger
than 0.85.12 For each feature pair exhibiting high
inter-correlation, we removed one feature accord-
ing to the criterion that the removed feature should
be linguistically less meaningful than the remain-
ing one. After this filtering, the 9 remaining PT
features are:
CT/T, PP_ling/S, NP/S, CN/S, VP_ling/T, PAS/S,
DI/T, MLev, MPSam
In summary, as a result of the feature selection
process, a total of 17 features were identified as
important features to be used in scoring models for
predicting speakers’ proficiency scores. Among
them 8 are clause boundary based and the other 9
are parse tree based.
</bodyText>
<sectionHeader confidence="0.9945" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99988329032258">
In the previous section, we identified 17 syntactic
features that show promising correlations with hu-
man rater speaking proficiency scores. These fea-
tures as well as the human-rated scores will be
used to build scoring models by using machine
learning methods. As introduced in Section 3, we
have one training set (N=137 speakers with all of
their responses combined) for model building and
five testing sets (N=52 for each of them) for evalu-
ation.
The publicly available machine learning pack-
age Weka was used in our experiments (Hall et al.
2009). We experimented with two algorithms in
Weka: multiple regression (called “LinearRegres-
sion” in Weka) and decision tree (called “M5P”in
Weka). The score values to be predicted are real
numbers (i.e., non-integer), because we have to
compute the average score of one speaker’s res-
ponses. Our initial runs showed that decision tree
models were consistently outperformed by mul-
tiple regression (MR) models and thus decided to
only focus on MR models henceforth.
We set the “AttributeSelectionMethod” parame-
ter in Weka’s LinearRegression algorithm to all 3
of its possible values in turn: (Model-1) M5 me-
thod; (Model-2) no attribute selection; and (Model-
3) greedy method. The resulting three multiple re-
gression models were then tested against the five
testing sets. Overall, correlations for all models for
the NN-test-1-Hum set were between 0.45 and
0.49, correlations for sets NN-test-2-CB and NN-
</bodyText>
<footnote confidence="0.804671333333333">
12 The reason for using a lower threshold than above was to
obtain a roughly equal number of CB and PT features in the
end.
</footnote>
<page confidence="0.99453">
728
</page>
<bodyText confidence="0.839035625">
test-3-SB (human transcript based, and using au-
tomated boundaries) around 0.2, and for sets NN-
test-4-ASR-CB and NN-test-5-ASR-SB (ASR hy-
potheses, and using automated boundaries), the
correlations were not significant. Model-2 (using
all 17 features) had the highest correlation on NN-
test-1-Hum and we provide correlation results of
this model in Table 3.
</bodyText>
<table confidence="0.999688857142857">
Test set Correlation Correlation significance
coefficient (p &lt; 0.05)
NN-test-1-Hum 0.488 Significant
NN-test-2-CB 0.220 Significant
NN-test-3-SB 0.170 Significant
NN-test-4-ASR-CB -0.025 Not significant
NN-test-5-ASR-SB -0.013 Not significant
</table>
<tableCaption confidence="0.986334">
Table 3. Multiple regression model testing results for
Model-2.
</tableCaption>
<sectionHeader confidence="0.996419" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997686369230769">
As we can see from the result table (Table 3) in the
previous section, using only syntactic complexity
features, based on clausal or parse tree information
derived from human transcriptions of spoken test
responses, can predict holistic human rater scores
for combined speaker responses over a whole test
with an overall correlation of r=0.49. While this is
a promising result for this study with a focus on a
broad spectrum of syntactic complexity features,
the results also show significant limitations for an
immediate operational use of such features. First,
the imperfect prediction of clause and sentence
boundaries by the two automatic classifiers causes
a substantial degradation of scoring model perfor-
mance to about r=0.2, and secondly, the rather high
error rate of the ASR system (50.5%) does not al-
low for the computation of features that would re-
sult in any significant correlation with human
scores. We want to note here that while ASR sys-
tems can be found that exhibit WERs below 10%
for certain tasks, such as restricted dictation in
low-noise environments by native speakers, our
ASR task is significantly harder in several ways:
(1) we have to recognize non-native speak-
ers’rresponses where speakers have a number of
different native language backgrounds; (2) the pro-
ficiency level of the test takers varies widely; and
(3) the responses are spontaneous and uncon-
strained in terms of vocabulary.
As for the automatic clause and sentence boun-
dary classifiers, we can observe (in Table 4) that
although the sentence boundary classifier has a
slightly higher F-score than the clause boundary
classifier, errors in sentence boundary detection
have more negative effects on the accuracy of
score prediction than those made by the clause
boundary classifier. In fact, the lower F-score of
the latter is mainly due to its lower precision which
indicates that there are more spurious clause boun-
daries in its output which apparently cause little
harm to the feature extraction processes.
Among the 17 final features, 3 of them are fre-
quency-based and the remaining 14 are ratio-
based, which mirrors our findings from previous
work that frequency features have been used less
successfully than ratio features. As for ratio fea-
tures, 5 of them are grammatical structure counts
against sentence units, 4 are counts against T-units,
and only 1 is based on counts against clause units.
The feature set covers a wide range of grammatical
structures, such as T-units, verb phrases, noun
phrases, complex nominals, adjective clauses,
coordinate clauses, prepositional phrases, etc.
While this wide coverage provides for richness of
the construct of syntactic complexity, some of the
features exhibit relatively high correlation with
each other which reduces their overall contribu-
tions to the scoring model’s performance.
Going through the workflow of our system, we
find at least five major stages that can generate
errors which in turn can adversely affect feature
computation and scoring model building. Errors
may appear in each stage of our workflow, passing
or even enlarging their effects from previous stages
to later stages:
</bodyText>
<listItem confidence="0.9734658">
1) grammatical errors by the speakers (test takers);
2) errors by the ASR system;
3) sentence/clause boundary detection errors;
4) parser errors; and
5) rule extraction errors.
</listItem>
<bodyText confidence="0.999012666666667">
In future work we will need to address each er-
ror source to obtain a higher overall system per-
formance.
</bodyText>
<page confidence="0.993836">
729
</page>
<table confidence="0.99541525">
Classifier Accu- Preci- Re- F score
racy sion call
Clause boundary 0.954 0.721 0.748 0.734
Sentence boundary 0.975 0.811 0.755 0.782
</table>
<tableCaption confidence="0.965416">
Table 4. Performance of clause and sentence boundary
detectors.
</tableCaption>
<sectionHeader confidence="0.981202" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999983680851064">
In this paper, we investigated associations between
speakers’ syntactic complexity features and their
speaking proficiency scores provided by human
raters. By exploring empirical evidence from non-
native and native speakers’ data sets of spontane-
ous speech test responses, we identified 17 features
related to clause types and parse trees as effective
predictors of human speaking scores. The features
were implemented based on Lu’s L2 Syntactic
Complexity Analyzer toolkit (Lu, 2011) to be au-
tomatically extracted from human or ASR tran-
scripts. Three multiple regression models were
built from non-native speech training data with
different parameter setup and were tested against
five testing sets with different preprocessing steps.
The best model used the complete set of 17 fea-
tures and exhibited a correlation with human
scores of r=0.49 on human transcripts with boun-
dary annotations.
When using automated classifiers to predict
clause or sentence boundaries, correlations with
human scores are around r=0.2. Our experiments
indicate that by enhancing the accuracy of the two
main automated preprocessing components, name-
ly ASR and automatic sentence and clause boun-
dary detectors, scoring model performance will
increase substantially, as well. Furthermore, this
result demonstrates clearly that syntactic complexi-
ty features can be devised that are able to predict
human speaking proficiency scores.
Since this is a preliminary study, there is ample
space to improve all major stages in the feature
extraction process. The errors listed in the previous
section are potential working directions for prepro-
cessing enhancements prior to machine learning.
Among the five types of errors, we can work on
improving the accuracy of the speech recognizer,
sentence and clause boundary detectors, parser,
and feature extraction rules; as for the grammatical
errors produced by test takers, we are envisioning
to automatically identify and correct such errors.
We will further experiment with syntactic com-
plexity measures to balance construct richness and
model simplicity. Furthermore, we can also expe-
riment with additional types of machine learning
models and tune parameters to derive scoring
models with better performance.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99998275">
The authors wish to thank Lei Chen and Su-Youn
Yoon for their help with the sentence and clause
boundary classifiers. We also would like to thank
our colleagues Jill Burstein, Keelan Evanini, Yoko
Futagi, Derrick Higgins, Nitin Madnani, and Joel
Tetreault, as well as the four anonymous ACL re-
viewers for their valuable and helpful feedback and
comments on our paper.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996008">
Bachman, L.F. (1990). Fundamental considerations in
language testing. Oxford: Oxford University Press.
Bernstein, J. (1999). PhonePass testing: Structure and
construct. Menlo Park, CA: Ordinate Corporation.
Bernstein, J., DeJong, J., Pisoni, D. &amp; Townshend, B.
(2000). Two experiments in automatic scoring of
spoken language proficiency. Proceedings of In-
STILL 2000, Dundee, Scotland.
Bernstein, J., Cheng, J., &amp; Suzuki, M. (2010). Fluency
and structural complexity as predictors of L2 oral
proficiency. Proceedings of Interspeech 2010, Tokyo,
Japan, September.
Chen, L., Tetreault, J. &amp; Xi, X. (2010). Towards using
structural events to assess non-native speech.
NAACL-HLT 2010. 5th Workshop on Innovative
Use of NLP for Building Educational Applications,
Los Angeles, CA, June.
Condouris, K., Meyer, E. &amp; Tagger-Flusberg, H.
(2003). The relationship between standardized meas-
ures of language and measures of spontaneous speech
in children with autism. American Journal of Speech-
Language Pathology, 12(3), 349-358.
Cooper, T.C. (1976). Measuring written syntactic pat-
terns of second language learners of German. The
Journal of Educational Research, 69(5), 176-183.
Cucchiarini, C., Strik, H. &amp; Boves, L. (1997). Automat-
ic evaluation of Dutch pronunciation by using speech
recognition technology. IEEE Automatic Speech
Recognition and Understanding Workshop, Santa
Barbara, CA.
</reference>
<page confidence="0.963401">
730
</page>
<reference confidence="0.999924714285714">
Cucchiarini, C., Strik, H. &amp; Boves, L. (2000). Quantita-
tive assessment of second language learners&apos; fluency
by means of automatic speech recognition technolo-
gy. Journal of the Acoustical Society of America,
107, 989-999.
Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R.
&amp; Butzberger, J. (2000a). The SRI EduSpeak system:
Recognition and pronunciation scoring for language
learning. Proceedings of InSTiLL-2000 (Intelligent
Speech Technology in Language Learning), Dundee,
Scotland.
Franco, H., Neumeyer, L., Digalakis, V. &amp; Ronen, O.
(2000b). Combination of machine scores for auto-
matic grading of pronunciation quality. Speech
Communication, 30, 121-130.
Hall, M., Frank, E., Holmes, G., Pfahringer, B.,
Reutemann, P. &amp; Witten, I.H. (2009). The WEKA
Data Mining Software: An Update. SIGKDD Explo-
rations, 11(1).
Halleck, G.B. (1995). Assessing oral proficiency: A
comparison of holistic and objective measures. The
Modern Language Journal, 79(2), 223-234.
Henry, K. (1996). Early L2 writing development: A
study of autobiographical essays by university-level
students on Russian. The Modern Language Journal,
80(3), 309-326.
Ho-Peng, L. (1983). Using T-unit measures to assess
writing proficiency of university ESL students.
RELC Journal, 14(2), 35-43.
Hunt, K. (1965). Grammatical structures written at three
grade levels. NCTE Research report No.3. Cham-
paign, IL: NCTE.
Iwashita, N. (2006). Syntactic complexity measures and
their relations to oral proficiency in Japanese as a
foreign language. Language Assessment Quarterly,
3(20), 151-169.
Kameen, P.T. (1979). Syntactic skill and ESL writing
quality. In C. Yorio, K. Perkins, &amp; J. Schachter
(Eds.), On TESOL ’79: The learner in focus (pp.343-
364). Washington, D.C.: TESOL.
Klein, D. &amp; Manning, C.D. (2003). Fast exact inference
with a factored model for a natural language parsing.
In S.Becker, S. Thrun &amp; K. Obermayer (Eds.), Ad-
vances in Neural Information Processing Systems 15
(pp.3-10). Cambridge, MA: MIT Press.
Larsen-Freeman, D. (1978). An ESL index of develop-
ment. Teachers of English to Speakers of Other Lan-
guages Quarterly, 12(4), 439-448.
Levy, R. &amp; Andrew, G. (2006). Tregex and Tsurgeon:
Tools for querying and manipulating tree data struc-
tures. Proceedings of the Fifth International Confe-
rence on Language Resources and Evaluation.
Lu, X. (2010). Automatic analysis of syntactic complex-
ity in second language writing. International Journal
of Corpus Linguistics, 15(4), 474-496.
Lu, X. (2011). L2 Syntactic Complexity Analyzer. Re-
trieved from
http://www.personal.psu.edu/xxl13/downloads/l2sca.
html
Ortega, L. (2003). Syntactic complexity measures and
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguis-
tics, 24(4), 492-518.
Perkins, K. (1980). Using objective methods of attained
writing proficiency to discriminate among holistic
evaluations. Teachers of English to Speakers of Oth-
er Languages Quarterly, 14(1), 61-69.
Roll, M., Frid, J. &amp; Horne, M. (2007). Measuring syn-
tactic complexity in spontaneous spoken Swedish.
Language and Speech, 50(2), 227-245.
Sampson, G. (1997). Depth in English grammar. Journal
of Linguistics, 33, 131-151.
Wolfe-Quintero, K., Inagaki, S. &amp; Kim, H. Y. (1998).
Second language development in writing: Measures
of fluency, accuracy, &amp; complexity. Honolulu, HI:
University of Hawaii Press.
Xi, X., &amp; Mollaun, P. (2006). Investigating the utility
of analytic scoring for the TOEFL® Academic
Speaking Test (TAST). TOEFL iBT Research Re-
port No. TOEFLiBT-01.
Zechner, K., Higgins, D. &amp; Xi, X. (2007). SpeechRa-
ter(SM): A construct-driven approach to score spon-
taneous non-native speech. Proceedings of the 2007
Workshop of the International Speech Communica-
tion Association (ISCA) Special Interest Group on
Speech and Language Technology in Education
(SLaTE), Farmington, PA, October.
Zechner, K., Higgins, D., Xi, X, &amp; Williamson, D.M.
(2009). Automatic scoring of non-native spontaneous
speech in tests of spoken English. Speech Communi-
cation, 51 (10), October.
</reference>
<page confidence="0.998003">
731
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439986">
<title confidence="0.9971585">Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</title>
<author confidence="0.999903">Miao Chen Klaus Zechner</author>
<affiliation confidence="0.9966155">School of Information Studies NLP &amp; Speech Group Syracuse University Educational Testing Service</affiliation>
<address confidence="0.892918">Syracuse, NY, USA Princeton, NJ, USA</address>
<email confidence="0.979689">mchen14@syr.edukzechner@ets.org</email>
<abstract confidence="0.98009836">This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. Our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. On human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L F Bachman</author>
</authors>
<title>Fundamental considerations in language testing.</title>
<date>1990</date>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="2350" citStr="Bachman, 1990" startWordPosition="351" endWordPosition="353">; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and aspects of spoken discourse structure, e.g., coherence and cohesion. These different aspects of speaking proficiency are often highly correlated in a non-native speaker (Xi and Mollaun, 2006; Bernstein et al., 2010), and so scoring models built solely on features of fluency and pronunciation may achieve reasonably high correlations with holistic human rater scores. However, it is important to point out that such systems would still be unable to assess many important aspects of the speaking construct and therefore c</context>
</contexts>
<marker>Bachman, 1990</marker>
<rawString>Bachman, L.F. (1990). Fundamental considerations in language testing. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
</authors>
<title>PhonePass testing: Structure and construct. Menlo Park, CA: Ordinate Corporation.</title>
<date>1999</date>
<contexts>
<context position="1712" citStr="Bernstein, 1999" startWordPosition="246" endWordPosition="247">coring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “co</context>
</contexts>
<marker>Bernstein, 1999</marker>
<rawString>Bernstein, J. (1999). PhonePass testing: Structure and construct. Menlo Park, CA: Ordinate Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J DeJong</author>
<author>D Pisoni</author>
<author>B Townshend</author>
</authors>
<title>Two experiments in automatic scoring of spoken language proficiency.</title>
<date>2000</date>
<booktitle>Proceedings of InSTILL</booktitle>
<location>Dundee, Scotland.</location>
<contexts>
<context position="1736" citStr="Bernstein et al., 2000" startWordPosition="248" endWordPosition="251">eves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (</context>
</contexts>
<marker>Bernstein, DeJong, Pisoni, Townshend, 2000</marker>
<rawString>Bernstein, J., DeJong, J., Pisoni, D. &amp; Townshend, B. (2000). Two experiments in automatic scoring of spoken language proficiency. Proceedings of InSTILL 2000, Dundee, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J Cheng</author>
<author>M Suzuki</author>
</authors>
<title>Fluency and structural complexity as predictors of L2 oral proficiency.</title>
<date>2010</date>
<booktitle>Proceedings of Interspeech 2010,</booktitle>
<location>Tokyo, Japan,</location>
<contexts>
<context position="1760" citStr="Bernstein et al., 2010" startWordPosition="252" endWordPosition="255">correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include </context>
<context position="9355" citStr="Bernstein et al., 2010" startWordPosition="1438" endWordPosition="1441">mplex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kameen, 1979). Speaking Syntactic complexity analysis in speech mainly inherits measures from the writing domain, and the abovementioned measures can be employed in the same way on speech transcripts for complexity computation. A series of studies have examined relations between the syntactic complexity of speech and the speakers’ holistic speaking proficiency levels (Halleck, 1995; Bernstein et al., 2010; Iwashita, 2006). Three objective measures of syntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic evaluations of speakers in Halleck (1995). Iwashita’s (2006) study on Japanese L2 speakers found that length-based complexity features (i.e., number of T-units and number of clauses per Tunit) are good predictors for oral proficiency. In studies directly employing syntactic complexity measures in other contexts, ratio-based measures are frequently used. Examples are mean length of utterance (Con</context>
</contexts>
<marker>Bernstein, Cheng, Suzuki, 2010</marker>
<rawString>Bernstein, J., Cheng, J., &amp; Suzuki, M. (2010). Fluency and structural complexity as predictors of L2 oral proficiency. Proceedings of Interspeech 2010, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>J Tetreault</author>
<author>X Xi</author>
</authors>
<title>Towards using structural events to assess non-native speech.</title>
<date>2010</date>
<booktitle>NAACL-HLT 2010. 5th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<location>Los Angeles, CA,</location>
<contexts>
<context position="10805" citStr="Chen et al. (2010)" startWordPosition="1657" endWordPosition="1660">Roll et al. (2007). The speaking output is usually less clean than writing data (e.g., considering disfluencies such as false starts, repetitions, filled pauses etc.). Therefore we may need to remove these disfluencies first before computing syntactic complexity features. Also, importantly, ASR output does not contain interpunctuation but both for sentential-based features as well as for parser-based features, the boundaries of clauses and sentences need to be known. For this purpose, we will use automated classifiers that are trained to predict clause and sentence boundaries, as described in Chen et al. (2010). With previous studies providing us a rich pool of complexity features, additionally we also develop features analogous to the ones from the literature, mostly by using different calculation methods. For instance, the frequency of Prepositional Phrases (PPs) is a feature from the literature, and we add some variants such as number of PPs per clause as a new feature to our extended feature set. 2.2 Devising the Initial Feature Set Through this literature review, we identified some important features that were frequently used in previous studies in both L2 speaking and writing, such as length o</context>
<context position="14734" citStr="Chen et al. (2010)" startWordPosition="2297" endWordPosition="2300">from a string between the ASR hypothesis and the reference transcript, where the sum of substitutions, insertions, and deletions is We used a total of five variants of the test sets, as described in Table 1. Sets 1-3 are based on human transcriptions, whereas sets 4 and 5 are based on ASR output. Further, set 1 contains human annotated clause and sentence boundaries, whereas the other 4 sets have clause or sentence boundaries predicted by a classifier. All human transcribed files from the NN data set were annotated for clause boundaries, clause types, and disfluencies by human annotators (see Chen et al. (2010)). For the Nat data set, all of the 100 transcribed responses were annotated in the same manner by a human annotator. They are not used for any training purposes but serve as a comparative reference for syntactic complexity features derived from the non-native corpus. The NN-train set was used both for training clause and sentence boundary classifiers, as well as for feature selection and training of the scoring models. The two boundary detectors were machine learning based Hidden Markov Models, trained by using a language model derived from the 760 training files which had sentence and clause</context>
</contexts>
<marker>Chen, Tetreault, Xi, 2010</marker>
<rawString>Chen, L., Tetreault, J. &amp; Xi, X. (2010). Towards using structural events to assess non-native speech. NAACL-HLT 2010. 5th Workshop on Innovative Use of NLP for Building Educational Applications, Los Angeles, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Condouris</author>
<author>E Meyer</author>
<author>H Tagger-Flusberg</author>
</authors>
<title>The relationship between standardized measures of language and measures of spontaneous speech in children with autism.</title>
<date>2003</date>
<journal>American Journal of SpeechLanguage Pathology,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>349--358</pages>
<contexts>
<context position="9975" citStr="Condouris et al., 2003" startWordPosition="1525" endWordPosition="1528">010; Iwashita, 2006). Three objective measures of syntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic evaluations of speakers in Halleck (1995). Iwashita’s (2006) study on Japanese L2 speakers found that length-based complexity features (i.e., number of T-units and number of clauses per Tunit) are good predictors for oral proficiency. In studies directly employing syntactic complexity measures in other contexts, ratio-based measures are frequently used. Examples are mean length of utterance (Condouris et al., 2003), word count or tree depth (Roll et al., 2007), or mean length of Tunits and mean number of clauses per T-unit (Bernstein et al., 2010). Frequency-based measures were used less, such as number of full phrases in Roll et al. (2007). The speaking output is usually less clean than writing data (e.g., considering disfluencies such as false starts, repetitions, filled pauses etc.). Therefore we may need to remove these disfluencies first before computing syntactic complexity features. Also, importantly, ASR output does not contain interpunctuation but both for sentential-based features as well as f</context>
</contexts>
<marker>Condouris, Meyer, Tagger-Flusberg, 2003</marker>
<rawString>Condouris, K., Meyer, E. &amp; Tagger-Flusberg, H. (2003). The relationship between standardized measures of language and measures of spontaneous speech in children with autism. American Journal of SpeechLanguage Pathology, 12(3), 349-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Cooper</author>
</authors>
<title>Measuring written syntactic patterns of second language learners of German.</title>
<date>1976</date>
<journal>The Journal of Educational Research,</journal>
<volume>69</volume>
<issue>5</issue>
<pages>176--183</pages>
<contexts>
<context position="8273" citStr="Cooper, 1976" startWordPosition="1281" endWordPosition="1282">od and the clause and T-unit grammatical structures. Some structures such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are m</context>
</contexts>
<marker>Cooper, 1976</marker>
<rawString>Cooper, T.C. (1976). Measuring written syntactic patterns of second language learners of German. The Journal of Educational Research, 69(5), 176-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cucchiarini</author>
<author>H Strik</author>
<author>L Boves</author>
</authors>
<title>Automatic evaluation of Dutch pronunciation by using speech recognition technology. IEEE Automatic Speech Recognition and Understanding Workshop,</title>
<date>1997</date>
<location>Santa Barbara, CA.</location>
<contexts>
<context position="1786" citStr="Cucchiarini et al., 1997" startWordPosition="256" endWordPosition="259">ater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, synt</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 1997</marker>
<rawString>Cucchiarini, C., Strik, H. &amp; Boves, L. (1997). Automatic evaluation of Dutch pronunciation by using speech recognition technology. IEEE Automatic Speech Recognition and Understanding Workshop, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cucchiarini</author>
<author>H Strik</author>
<author>L Boves</author>
</authors>
<title>Quantitative assessment of second language learners&apos; fluency by means of automatic speech recognition technology.</title>
<date>2000</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>107</volume>
<pages>989--999</pages>
<contexts>
<context position="1812" citStr="Cucchiarini et al., 2000" startWordPosition="260" endWordPosition="263">n unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabula</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2000</marker>
<rawString>Cucchiarini, C., Strik, H. &amp; Boves, L. (2000). Quantitative assessment of second language learners&apos; fluency by means of automatic speech recognition technology. Journal of the Acoustical Society of America, 107, 989-999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Franco</author>
<author>V Abrash</author>
<author>K Precoda</author>
<author>H Bratt</author>
<author>R Rao</author>
<author>J Butzberger</author>
</authors>
<title>The SRI EduSpeak system: Recognition and pronunciation scoring for language learning.</title>
<date>2000</date>
<booktitle>Proceedings of InSTiLL-2000 (Intelligent Speech Technology in Language Learning),</booktitle>
<location>Dundee, Scotland.</location>
<contexts>
<context position="1833" citStr="Franco et al., 2000" startWordPosition="264" endWordPosition="267"> correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and asp</context>
</contexts>
<marker>Franco, Abrash, Precoda, Bratt, Rao, Butzberger, 2000</marker>
<rawString>Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R. &amp; Butzberger, J. (2000a). The SRI EduSpeak system: Recognition and pronunciation scoring for language learning. Proceedings of InSTiLL-2000 (Intelligent Speech Technology in Language Learning), Dundee, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Franco</author>
<author>L Neumeyer</author>
<author>V Digalakis</author>
<author>O Ronen</author>
</authors>
<title>Combination of machine scores for automatic grading of pronunciation quality.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>30</volume>
<pages>121--130</pages>
<contexts>
<context position="1833" citStr="Franco et al., 2000" startWordPosition="264" endWordPosition="267"> correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and asp</context>
</contexts>
<marker>Franco, Neumeyer, Digalakis, Ronen, 2000</marker>
<rawString>Franco, H., Neumeyer, L., Digalakis, V. &amp; Ronen, O. (2000b). Combination of machine scores for automatic grading of pronunciation quality. Speech Communication, 30, 121-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="27600" citStr="Hall et al. 2009" startWordPosition="4405" endWordPosition="4408">ased and the other 9 are parse tree based. 5 Experiments and Results In the previous section, we identified 17 syntactic features that show promising correlations with human rater speaking proficiency scores. These features as well as the human-rated scores will be used to build scoring models by using machine learning methods. As introduced in Section 3, we have one training set (N=137 speakers with all of their responses combined) for model building and five testing sets (N=52 for each of them) for evaluation. The publicly available machine learning package Weka was used in our experiments (Hall et al. 2009). We experimented with two algorithms in Weka: multiple regression (called “LinearRegression” in Weka) and decision tree (called “M5P”in Weka). The score values to be predicted are real numbers (i.e., non-integer), because we have to compute the average score of one speaker’s responses. Our initial runs showed that decision tree models were consistently outperformed by multiple regression (MR) models and thus decided to only focus on MR models henceforth. We set the “AttributeSelectionMethod” parameter in Weka’s LinearRegression algorithm to all 3 of its possible values in turn: (Model-1) M5 m</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P. &amp; Witten, I.H. (2009). The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G B Halleck</author>
</authors>
<title>Assessing oral proficiency: A comparison of holistic and objective measures.</title>
<date>1995</date>
<journal>The Modern Language Journal,</journal>
<volume>79</volume>
<issue>2</issue>
<pages>223--234</pages>
<contexts>
<context position="9331" citStr="Halleck, 1995" startWordPosition="1436" endWordPosition="1437">an number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kameen, 1979). Speaking Syntactic complexity analysis in speech mainly inherits measures from the writing domain, and the abovementioned measures can be employed in the same way on speech transcripts for complexity computation. A series of studies have examined relations between the syntactic complexity of speech and the speakers’ holistic speaking proficiency levels (Halleck, 1995; Bernstein et al., 2010; Iwashita, 2006). Three objective measures of syntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic evaluations of speakers in Halleck (1995). Iwashita’s (2006) study on Japanese L2 speakers found that length-based complexity features (i.e., number of T-units and number of clauses per Tunit) are good predictors for oral proficiency. In studies directly employing syntactic complexity measures in other contexts, ratio-based measures are frequently used. Examples are mean </context>
</contexts>
<marker>Halleck, 1995</marker>
<rawString>Halleck, G.B. (1995). Assessing oral proficiency: A comparison of holistic and objective measures. The Modern Language Journal, 79(2), 223-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Henry</author>
</authors>
<title>Early L2 writing development: A study of autobiographical essays by university-level students on Russian.</title>
<date>1996</date>
<journal>The Modern Language Journal,</journal>
<volume>80</volume>
<issue>3</issue>
<pages>309--326</pages>
<contexts>
<context position="8338" citStr="Henry, 1996" startWordPosition="1289" endWordPosition="1290">es such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in comp</context>
</contexts>
<marker>Henry, 1996</marker>
<rawString>Henry, K. (1996). Early L2 writing development: A study of autobiographical essays by university-level students on Russian. The Modern Language Journal, 80(3), 309-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ho-Peng</author>
</authors>
<title>Using T-unit measures to assess writing proficiency of university ESL students.</title>
<date>1983</date>
<journal>RELC Journal,</journal>
<volume>14</volume>
<issue>2</issue>
<pages>35--43</pages>
<contexts>
<context position="8325" citStr="Ho-Peng, 1983" startWordPosition="1287" endWordPosition="1288">. Some structures such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of pas</context>
</contexts>
<marker>Ho-Peng, 1983</marker>
<rawString>Ho-Peng, L. (1983). Using T-unit measures to assess writing proficiency of university ESL students. RELC Journal, 14(2), 35-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hunt</author>
</authors>
<title>Grammatical structures written at three grade levels.</title>
<date>1965</date>
<booktitle>NCTE Research report No.3.</booktitle>
<publisher>NCTE.</publisher>
<location>Champaign, IL:</location>
<contexts>
<context position="8053" citStr="Hunt, 1965" startWordPosition="1249" endWordPosition="1250">ber of another unit, and index is computing numeric scores by specific formulae (WolfeQuintero et al., 1998). For example, the measure “mean number of clauses per T-unit” is obtained by using the ratio calculation method and the clause and T-unit grammatical structures. Some structures such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010</context>
</contexts>
<marker>Hunt, 1965</marker>
<rawString>Hunt, K. (1965). Grammatical structures written at three grade levels. NCTE Research report No.3. Champaign, IL: NCTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Iwashita</author>
</authors>
<title>Syntactic complexity measures and their relations to oral proficiency in Japanese as a foreign language.</title>
<date>2006</date>
<journal>Language Assessment Quarterly,</journal>
<volume>3</volume>
<issue>20</issue>
<pages>151--169</pages>
<contexts>
<context position="9372" citStr="Iwashita, 2006" startWordPosition="1442" endWordPosition="1443">t (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kameen, 1979). Speaking Syntactic complexity analysis in speech mainly inherits measures from the writing domain, and the abovementioned measures can be employed in the same way on speech transcripts for complexity computation. A series of studies have examined relations between the syntactic complexity of speech and the speakers’ holistic speaking proficiency levels (Halleck, 1995; Bernstein et al., 2010; Iwashita, 2006). Three objective measures of syntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic evaluations of speakers in Halleck (1995). Iwashita’s (2006) study on Japanese L2 speakers found that length-based complexity features (i.e., number of T-units and number of clauses per Tunit) are good predictors for oral proficiency. In studies directly employing syntactic complexity measures in other contexts, ratio-based measures are frequently used. Examples are mean length of utterance (Condouris et al., 20</context>
</contexts>
<marker>Iwashita, 2006</marker>
<rawString>Iwashita, N. (2006). Syntactic complexity measures and their relations to oral proficiency in Japanese as a foreign language. Language Assessment Quarterly, 3(20), 151-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P T Kameen</author>
</authors>
<title>Syntactic skill and ESL writing quality. In</title>
<date>1979</date>
<publisher>TESOL.</publisher>
<location>Washington, D.C.:</location>
<contexts>
<context position="8960" citStr="Kameen, 1979" startWordPosition="1383" endWordPosition="1384"> 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kameen, 1979). Speaking Syntactic complexity analysis in speech mainly inherits measures from the writing domain, and the abovementioned measures can be employed in the same way on speech transcripts for complexity computation. A series of studies have examined relations between the syntactic complexity of speech and the speakers’ holistic speaking proficiency levels (Halleck, 1995; Bernstein et al., 2010; Iwashita, 2006). Three objective measures of syntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic eva</context>
</contexts>
<marker>Kameen, 1979</marker>
<rawString>Kameen, P.T. (1979). Syntactic skill and ESL writing quality. In C. Yorio, K. Perkins, &amp; J. Schachter (Eds.), On TESOL ’79: The learner in focus (pp.343-364). Washington, D.C.: TESOL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for a natural language parsing.</title>
<date>2003</date>
<booktitle>In S.Becker, S. Thrun &amp; K. Obermayer (Eds.), Advances in Neural Information Processing Systems</booktitle>
<volume>15</volume>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="17907" citStr="Klein and Manning, 2003" startWordPosition="2822" endWordPosition="2826">of adjective clauses6 per one thousand words”, “mean number of dependent clauses per clause”, etc. Parse tree based features refer to features that are generated from parse trees and cannot be extracted from human annotated clauses directly. We first selected features showing high correlation to human assigned scores. In this process the CB features were computed from human labeled clause boundaries in transcripts for best accuracy, and PT features were calculated from using parsing and other tools because we did not have human parse tree annotations for our data. We used the Stanford Parser (Klein and Manning, 2003) in conjunction with the Stanford Tregex package (Levy and Andrew, 2006) which supports using rules to extract specific configurations from parse trees, in a package put together by Lu (Lu, 2011). When given a sentence, the Stanford Parser outputs its grammatical structure by grouping words (and phrases) in a tree structure and identifies grammatical roles of words and phrases. Tregex is a tree query tool that takes Stanford parser trees as input and queries the trees to find subtrees that meet specific rules written in Tregex syntax (Levy and Andrew, 2006). It uses relational operators regula</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. &amp; Manning, C.D. (2003). Fast exact inference with a factored model for a natural language parsing. In S.Becker, S. Thrun &amp; K. Obermayer (Eds.), Advances in Neural Information Processing Systems 15 (pp.3-10). Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Larsen-Freeman</author>
</authors>
<title>An ESL index of development.</title>
<date>1978</date>
<journal>Teachers of English to Speakers of Other Languages Quarterly,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>439--448</pages>
<contexts>
<context position="8295" citStr="Larsen-Freeman, 1978" startWordPosition="1283" endWordPosition="1284">use and T-unit grammatical structures. Some structures such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (</context>
</contexts>
<marker>Larsen-Freeman, 1978</marker>
<rawString>Larsen-Freeman, D. (1978). An ESL index of development. Teachers of English to Speakers of Other Languages Quarterly, 12(4), 439-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: Tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="17979" citStr="Levy and Andrew, 2006" startWordPosition="2835" endWordPosition="2838">lauses per clause”, etc. Parse tree based features refer to features that are generated from parse trees and cannot be extracted from human annotated clauses directly. We first selected features showing high correlation to human assigned scores. In this process the CB features were computed from human labeled clause boundaries in transcripts for best accuracy, and PT features were calculated from using parsing and other tools because we did not have human parse tree annotations for our data. We used the Stanford Parser (Klein and Manning, 2003) in conjunction with the Stanford Tregex package (Levy and Andrew, 2006) which supports using rules to extract specific configurations from parse trees, in a package put together by Lu (Lu, 2011). When given a sentence, the Stanford Parser outputs its grammatical structure by grouping words (and phrases) in a tree structure and identifies grammatical roles of words and phrases. Tregex is a tree query tool that takes Stanford parser trees as input and queries the trees to find subtrees that meet specific rules written in Tregex syntax (Levy and Andrew, 2006). It uses relational operators regulated by Tregex, for example, “A &lt;&lt; B” stands for “subtree A dominates sub</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Levy, R. &amp; Andrew, G. (2006). Tregex and Tsurgeon: Tools for querying and manipulating tree data structures. Proceedings of the Fifth International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<pages>474--496</pages>
<contexts>
<context position="8363" citStr="Lu, 2010" startWordPosition="1293" endWordPosition="1294">its only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kameen, 1979). S</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4), 474-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Lu</author>
</authors>
<date>2011</date>
<booktitle>L2 Syntactic Complexity Analyzer. Retrieved from http://www.personal.psu.edu/xxl13/downloads/l2sca. html</booktitle>
<contexts>
<context position="18102" citStr="Lu, 2011" startWordPosition="2858" endWordPosition="2859">man annotated clauses directly. We first selected features showing high correlation to human assigned scores. In this process the CB features were computed from human labeled clause boundaries in transcripts for best accuracy, and PT features were calculated from using parsing and other tools because we did not have human parse tree annotations for our data. We used the Stanford Parser (Klein and Manning, 2003) in conjunction with the Stanford Tregex package (Levy and Andrew, 2006) which supports using rules to extract specific configurations from parse trees, in a package put together by Lu (Lu, 2011). When given a sentence, the Stanford Parser outputs its grammatical structure by grouping words (and phrases) in a tree structure and identifies grammatical roles of words and phrases. Tregex is a tree query tool that takes Stanford parser trees as input and queries the trees to find subtrees that meet specific rules written in Tregex syntax (Levy and Andrew, 2006). It uses relational operators regulated by Tregex, for example, “A &lt;&lt; B” stands for “subtree A dominates subtree B”. The operators primarily function in subtree precedence, dominance, negation, regular expression, tree node identit</context>
<context position="33429" citStr="Lu, 2011" startWordPosition="5313" endWordPosition="5314">boundary 0.975 0.811 0.755 0.782 Table 4. Performance of clause and sentence boundary detectors. 7 Conclusion and Future Work In this paper, we investigated associations between speakers’ syntactic complexity features and their speaking proficiency scores provided by human raters. By exploring empirical evidence from nonnative and native speakers’ data sets of spontaneous speech test responses, we identified 17 features related to clause types and parse trees as effective predictors of human speaking scores. The features were implemented based on Lu’s L2 Syntactic Complexity Analyzer toolkit (Lu, 2011) to be automatically extracted from human or ASR transcripts. Three multiple regression models were built from non-native speech training data with different parameter setup and were tested against five testing sets with different preprocessing steps. The best model used the complete set of 17 features and exhibited a correlation with human scores of r=0.49 on human transcripts with boundary annotations. When using automated classifiers to predict clause or sentence boundaries, correlations with human scores are around r=0.2. Our experiments indicate that by enhancing the accuracy of the two m</context>
</contexts>
<marker>Lu, 2011</marker>
<rawString>Lu, X. (2011). L2 Syntactic Complexity Analyzer. Retrieved from http://www.personal.psu.edu/xxl13/downloads/l2sca. html</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ortega</author>
</authors>
<title>Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing.</title>
<date>2003</date>
<journal>Applied Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>492--518</pages>
<contexts>
<context position="6117" citStr="Ortega, 2003" startWordPosition="951" endWordPosition="952"> each speaker, using all six responses of that speaker. In the remainder of the paper, we will address related work in syntactic complexity (Section 2), introduce the speech data sets of our study (Section 3), describe the methods we used for feature extraction (Section 4), provide the experiment design and results (Section 5), analyze and discuss the results in Section 6, before concluding the paper (Section 7). 723 2.1 Literature on Syntactic Complexity Syntactic complexity is defined as “the range of forms that surface in language production and the degree of sophistication of such forms” (Ortega, 2003). It is an important factor in the second language assessment construct as described in Bachman’s (1990) conceptual model of language ability, and therefore is often used as an index of language proficiency and development status of L2 learners. Various studies have proposed and investigated measures of syntactic complexity as well as examined its predictiveness for language proficiency, in both L2 writing and speaking settings, which will be reviewed respectively. Writing Wolfe-Quintero et al. (1998) reviewed a number of grammatical complexity measures in L2 writing from thirty-nine studies, </context>
<context position="8352" citStr="Ortega, 2003" startWordPosition="1291" endWordPosition="1292">auses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or frequency of passives in composition (Kamee</context>
</contexts>
<marker>Ortega, 2003</marker>
<rawString>Ortega, L. (2003). Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing. Applied Linguistics, 24(4), 492-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Perkins</author>
</authors>
<title>Using objective methods of attained writing proficiency to discriminate among holistic evaluations.</title>
<date>1980</date>
<journal>Teachers of English to Speakers of Other Languages Quarterly,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>61--69</pages>
<contexts>
<context position="8310" citStr="Perkins, 1980" startWordPosition="1285" endWordPosition="1286">ical structures. Some structures such as clauses and T-units only need shallow linguistic processing to acquire, while some require parsing. There are numerous combinations for measures and we need empirical evi3 T-units are defined as “shortest grammatically allowable sentences into which (writing can be split) or minimally terminable units” (Hunt, 1965:20). dence to select measures with the highest performance. There have been a series of empirical studies examining the relationship of syntactic complexity measures to L2 proficiency using real-world data (Cooper, 1976; Larsen-Freeman, 1978; Perkins, 1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; Lu, 2010). The studies investigate measures that highly correlate with proficiency levels or distinguish between different proficiency levels. Many T-unit related measures were identified as statistically significant indicators to L2 proficiency, such as mean length of T-unit (Henry, 1996; Lu, 2010), mean number of clauses per T-unit (Cooper, 1976; Lu, 2010), mean number of complex nominals per T-unit (Lu, 2010), or the mean number of errorfree T-units per sentence (Ho-Peng, 1983). Other significant measures are mean length of clause (Lu, 2010), or f</context>
</contexts>
<marker>Perkins, 1980</marker>
<rawString>Perkins, K. (1980). Using objective methods of attained writing proficiency to discriminate among holistic evaluations. Teachers of English to Speakers of Other Languages Quarterly, 14(1), 61-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Roll</author>
<author>J Frid</author>
<author>M Horne</author>
</authors>
<title>Measuring syntactic complexity in spontaneous spoken Swedish.</title>
<date>2007</date>
<journal>Language and Speech,</journal>
<volume>50</volume>
<issue>2</issue>
<pages>227--245</pages>
<contexts>
<context position="10021" citStr="Roll et al., 2007" startWordPosition="1534" endWordPosition="1537">yntactic complexity, including mean T-unit length, mean error-free T-unit length, and percent of error-free T-units were found to correlate with holistic evaluations of speakers in Halleck (1995). Iwashita’s (2006) study on Japanese L2 speakers found that length-based complexity features (i.e., number of T-units and number of clauses per Tunit) are good predictors for oral proficiency. In studies directly employing syntactic complexity measures in other contexts, ratio-based measures are frequently used. Examples are mean length of utterance (Condouris et al., 2003), word count or tree depth (Roll et al., 2007), or mean length of Tunits and mean number of clauses per T-unit (Bernstein et al., 2010). Frequency-based measures were used less, such as number of full phrases in Roll et al. (2007). The speaking output is usually less clean than writing data (e.g., considering disfluencies such as false starts, repetitions, filled pauses etc.). Therefore we may need to remove these disfluencies first before computing syntactic complexity features. Also, importantly, ASR output does not contain interpunctuation but both for sentential-based features as well as for parser-based features, the boundaries of cl</context>
</contexts>
<marker>Roll, Frid, Horne, 2007</marker>
<rawString>Roll, M., Frid, J. &amp; Horne, M. (2007). Measuring syntactic complexity in spontaneous spoken Swedish. Language and Speech, 50(2), 227-245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>Depth in English grammar.</title>
<date>1997</date>
<journal>Journal of Linguistics,</journal>
<volume>33</volume>
<pages>131--151</pages>
<contexts>
<context position="23226" citStr="Sampson, 1997" startWordPosition="3699" endWordPosition="3700">d as a PP immediately dominated by another PP in cases where a preposition contains a noun such as “in spite of” or “in front of”. An example would be “she stood in front of a house” where “in front of a house” would be parsed as two embedded PPs but only the top PP would be counted in this case. 10 A “linguistically meaningful VP” (VP_ling) is defined as a verb phrase immediately dominated by a clausal phrase, in order to avoid VPs embedded in another VP, e.g., &amp;quot;should go to work&amp;quot; is identified as one VP instead of two embedded VPs. 11 The “P-based Sampson” is a raw production-based measure (Sampson, 1997), defined as &amp;quot;proportion of the daughters of a nonterminal node which are themselves nonterminal and nonrightmost, averaged over the nonterminals of a sentence&amp;quot;. Clause and Sentence based Features (CB features) Firstly, we extracted all 26 initial CB features directly from human annotated data of NN-train, using information from the clause and sentence type labels. The reasoning behind this was to create an initial pool of clause-based features that reflects the distribution of clauses and sentences as accurately as possible, even though we did not plan to use this extraction method operationa</context>
</contexts>
<marker>Sampson, 1997</marker>
<rawString>Sampson, G. (1997). Depth in English grammar. Journal of Linguistics, 33, 131-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wolfe-Quintero</author>
<author>S Inagaki</author>
<author>H Y Kim</author>
</authors>
<date>1998</date>
<booktitle>Second language development in writing: Measures of fluency, accuracy, &amp; complexity.</booktitle>
<publisher>University of Hawaii Press.</publisher>
<location>Honolulu, HI:</location>
<contexts>
<context position="6623" citStr="Wolfe-Quintero et al. (1998)" startWordPosition="1027" endWordPosition="1030"> as “the range of forms that surface in language production and the degree of sophistication of such forms” (Ortega, 2003). It is an important factor in the second language assessment construct as described in Bachman’s (1990) conceptual model of language ability, and therefore is often used as an index of language proficiency and development status of L2 learners. Various studies have proposed and investigated measures of syntactic complexity as well as examined its predictiveness for language proficiency, in both L2 writing and speaking settings, which will be reviewed respectively. Writing Wolfe-Quintero et al. (1998) reviewed a number of grammatical complexity measures in L2 writing from thirty-nine studies, and their usage for predicting language proficiency was discussed. Some examples of syntactic complexity measures are: mean number of clauses per T-unit3 , mean length of clauses, mean number of verbs per sentence, etc. The various measures can be grouped into two categories: (1) clauses, sentences, and T-units in terms of each other; and (2) specific grammatical structures (e.g., passives, nominals) in relation to clauses, sentences, or T-units (Wolfe-Quintero et al., 1998). Three primary methods of </context>
</contexts>
<marker>Wolfe-Quintero, Inagaki, Kim, 1998</marker>
<rawString>Wolfe-Quintero, K., Inagaki, S. &amp; Kim, H. Y. (1998). Second language development in writing: Measures of fluency, accuracy, &amp; complexity. Honolulu, HI: University of Hawaii Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xi</author>
<author>P Mollaun</author>
</authors>
<title>Investigating the utility of analytic scoring for the TOEFL® Academic Speaking Test (TAST).</title>
<date>2006</date>
<tech>TOEFL iBT Research Report No. TOEFLiBT-01.</tech>
<contexts>
<context position="2620" citStr="Xi and Mollaun, 2006" startWordPosition="387" endWordPosition="390">eech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and aspects of spoken discourse structure, e.g., coherence and cohesion. These different aspects of speaking proficiency are often highly correlated in a non-native speaker (Xi and Mollaun, 2006; Bernstein et al., 2010), and so scoring models built solely on features of fluency and pronunciation may achieve reasonably high correlations with holistic human rater scores. However, it is important to point out that such systems would still be unable to assess many important aspects of the speaking construct and therefore cannot be seen as ideal from a validity point of view.2 The purpose of this paper is to address one of these important aspects of spoken language in more detail, namely syntactic complexity. This paper can be seen as a first step toward including 1 A construct is a set o</context>
</contexts>
<marker>Xi, Mollaun, 2006</marker>
<rawString>Xi, X., &amp; Mollaun, P. (2006). Investigating the utility of analytic scoring for the TOEFL® Academic Speaking Test (TAST). TOEFL iBT Research Report No. TOEFLiBT-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>D Higgins</author>
<author>X Xi</author>
</authors>
<title>SpeechRater(SM): A construct-driven approach to score spontaneous non-native speech.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Workshop of the International Speech Communication Association (ISCA) Special Interest Group on Speech and Language Technology in Education (SLaTE),</booktitle>
<location>Farmington, PA,</location>
<contexts>
<context position="1878" citStr="Zechner et al., 2007" startWordPosition="272" endWordPosition="275">ause boundaries from automated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and aspects of spoken discourse structure, e.g., coh</context>
</contexts>
<marker>Zechner, Higgins, Xi, 2007</marker>
<rawString>Zechner, K., Higgins, D. &amp; Xi, X. (2007). SpeechRater(SM): A construct-driven approach to score spontaneous non-native speech. Proceedings of the 2007 Workshop of the International Speech Communication Association (ISCA) Special Interest Group on Speech and Language Technology in Education (SLaTE), Farmington, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>D Higgins</author>
<author>X Xi</author>
<author>D M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken English.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<contexts>
<context position="1901" citStr="Zechner et al., 2009" startWordPosition="276" endWordPosition="279">utomated classifiers are around r=0.2. 1 Introduction Past efforts directed at automated scoring of speech have used mainly features related to fluen cy (e.g., speaking rate, length and distribution of pauses), pronunciation (e.g., using log-likelihood scores from the acoustic model of an Automatic Speech Recognition (ASR) system), or prosody (e.g., information related to pitch contours or syllable stress) (e.g., Bernstein, 1999; Bernstein et al., 2000; Bernstein et al., 2010; Cucchiarini et al., 1997; Cucchiarini et al., 2000; Franco et al., 2000a; Franco et al., 2000b; Zechner et al., 2007, Zechner et al., 2009). While this approach is a good match to most of the important properties related to low entropy speech (i.e., speech which is highly predictable), such as reading a passage aloud, it lacks many important aspects of spontaneous speech which are relevant to be evaluated both by a human rater and an automated scoring system. Examples of such aspects of speech, which are considered part of the construct1 of “communicative competence (Bachman, 1990), include grammatical accuracy, syntactic complexity, vocabulary diversity, and aspects of spoken discourse structure, e.g., coherence and cohesion. Th</context>
<context position="3864" citStr="Zechner et al., 2009" startWordPosition="588" endWordPosition="591">d abilities measured by a test. 2 “Construct validity” refers to the extent that a test measures what it is designed to measure, in this case, communicative competence via speaking. 722 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722–731, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics features related to this part of the speaking con- 2 Related Work struct into an already existing automated speech scoring system for spontaneous speech which so far mostly uses features related to fluency and pronunciation (Zechner et al., 2009). We use data from the speaking section of the TOEFL® Practice Online (TPO) test, which is a low stakes practice test for non-native speakers where they are asked to provide six spontaneous speech samples of about one minute in length each in response to a variety of prompts. Some prompts may be simple questions, and others may involve reading or listening to passages first and then answering related questions. All responses were scored holistically by human raters according to pre-defined scoring rubrics (i.e., specific scoring guidelines) on a scale of 1 to 4, 4 being the highest proficiency</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Zechner, K., Higgins, D., Xi, X, &amp; Williamson, D.M. (2009). Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communication, 51 (10), October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>