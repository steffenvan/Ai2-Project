<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.071523">
<title confidence="0.962912">
How Text Segmentation Algorithms Gain from Topic Models
</title>
<author confidence="0.894249">
Martin Riedl and Chris Biemann
</author>
<affiliation confidence="0.8506215">
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universit¨at Darmstadt
</affiliation>
<address confidence="0.955766">
Hochschulstrasse 10, D-64289 Darmstadt, Germany
</address>
<email confidence="0.997593">
riedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866083333333">
This paper introduces a general method to in-
corporate the LDA Topic Model into text seg-
mentation algorithms. We show that seman-
tic information added by Topic Models signifi-
cantly improves the performance of two word-
based algorithms, namely TextTiling and C99.
Additionally, we introduce the new TopicTil-
ing algorithm that is designed to take better
advantage of topic information. We show con-
sistent improvements over word-based meth-
ods and achieve state-of-the art performance
on a standard dataset.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949551724138">
Texts are often structured into segments to ease un-
derstanding and readability of texts. Knowing about
sentence boundaries is advantageous for natural lan-
guage processing (NLP) tasks such as summariza-
tion or indexing. While many genres such as en-
cyclopedia entries or scientific articles follow rather
formal conventions of breaking up a text into mean-
ingful units, there are plenty of electronically avail-
able texts without defined segments, e.g. web doc-
uments. Text segmentation is the task of automati-
cally segmenting texts into parts. Viewing a well-
written text as sequence of subtopics and assuming
that subtopics correspond to segments, a segmenta-
tion algorithm needs to find changes of subtopics to
identify the natural division of an unstructured text.
In this work, we utilize semantic information
from Topic Models (TMs) to inform text segmen-
tation algorithms. For this, we compare two early
word-based algorithms with their topic-based vari-
ants, and construct our own algorithm called Topic-
Tiling. We show that using topics estimated by La-
tent Dirichlet Allocation (LDA) in lieu of words sub-
stantially improves earlier segmentation algorithms.
In comparison to TextTiling (TT), neither smoothing
nor a blocksize or window size is needed. TT using
TMs and our own algorithm improve on the state-of-
the-art for a standard dataset, while being conceptu-
ally simpler and computationally more efficient than
other topic-based segmentation algorithms.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999841625">
Based on the observation of Halliday and Hasan
(1976) that the density of coherence relations is
higher within segments than between segments,
most algorithms compute a coherence score to mea-
sure the difference of textual units for informing
a segmentation decision. TextTiling (TT) (Hearst,
1994) relies on the simplest coherence relation –
word repetition – and computes similarities between
textual units based on the similarities of word space
vectors. With C99 (Choi, 2000) an algorithm was
introduced that uses a matrix-based ranking and a
clustering approach in order to relate the most sim-
ilar textual units and to cluster groups of consecu-
tive units into segments. Both TT and C99 charac-
terize textual units by the words they contain. Gal-
ley et al. (2003) showed that using TF-IDF term
weights in the term vector improves the performance
of TT. Proposals using Dynamic Programming (DP)
are given in (Utiyama and Isahara, 2001; Fragkou et
al., 2004). Related to our work are the approaches
described in (Misra et al., 2009; Sun et al., 2008):
here, TMs are also used to alleviate the sparsity of
word vectors. Misra et al. (2009) extended the DP
algorithm U00 from Utiyama and Isahara (2001) us-
</bodyText>
<page confidence="0.907048333333333">
553
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999944421052632">
ing TMs. At this, the topic assignments have to be
inferred for each possible segment, resulting in high
computational cost. In addition to these linear topic
segmentation algorithms, there are hierarchical seg-
mentation algorithms, see (Yaari, 1997; Hsueh et al.,
2006; Eisenstein, 2009).
For topic modeling, we use the widely applied
LDA (Blei et al., 2003). This generative probabilis-
tic model uses a training corpus of documents to cre-
ate document-topic and topic-word distributions and
is parameterized by the number of topics N as well
as by two hyperparameters. To generate a document
d the topic proportions are drawn using a Dirichlet
distribution with hyperparameter a. Adjacent for
each word i a topic zdi is chosen according to a
multinomial distribution using hyperparameter Qzdi .
Unseen documents can be annotated with an existing
TM using Bayesian inference methods (here: Gibbs
sampling).
</bodyText>
<sectionHeader confidence="0.978528" genericHeader="method">
3 Method: From Words to Topics
</sectionHeader>
<bodyText confidence="0.9999434">
The underlying mechanism described here is very
simple: Instead of using words directly as features
to characterize textual units, we use the topic IDs
assigned by Bayesian inference. LDA inference as-
signs a topic ID to each word in the test document
in each inference iteration step, based on a TM es-
timated on a training corpus. We use the topic ID,
lastly assigned to each word. This might lead to in-
stabilities as a word with high probabilities for sev-
eral topics could be assigned to different topics in
different inference iterations. To avoid these insta-
bilities, we save all topic IDs assigned to a word for
each inference iteration. Finally, the most frequent
topic ID is assigned to each word. This mechanism
we call the mode method. Both word replacements
can be applied to most segmentation algorithms.
In this work, we use this general setup to imple-
ment topic-based versions of TT and C99 and de-
velop a new TextTiling-based method called Topic-
Tiling.
</bodyText>
<sectionHeader confidence="0.981542" genericHeader="method">
4 Topic-based Segmentation Algorithms
</sectionHeader>
<subsectionHeader confidence="0.998595">
4.1 TextTiling using Topic Models
</subsectionHeader>
<bodyText confidence="0.999993470588235">
In TextTiling (TT) (Hearst, 1994) using topic IDs
(TTLDA), a document D, which is subject to seg-
mentation, is represented as a sequence of n topic
IDs1. TT splits the document into topic-sequences,
instead of sentences, where each sequence consists
of w topic IDs. To calculate the similarity between
two topic-sequences, called sequence-gap, TT uses
k topic-sequences, named block, to the left and to
the right of the sequence gap. This parameter k de-
fines the so-called blocksize. The cosine similarity
is applied to computed a similarity score based on
the topic frequency of the adjacent blocks at each
sequence-gap. A value close to 1 indicates a high
similarity among two blocks, a value close to zero
denotes a low similarity. Then for each sequence-
gap a depth score di is calculated for describing the
sharpness of a gap, by di = 1/2(hl(i)−si+hr(i)−
si). The function hl(i) returns the highest similarity
score on the left side of the sequence-gap index i that
does not increase and hr(i) returns the highest score
on the right side. Then all local maxima positions
are searched based on the depth scores.
In the next step, these obtained maxima scores are
sorted. If the number of segments n is given as input
parameter, the n highest depth scores are used, oth-
erwise a cut-off function is used that applies a seg-
ment only if the depth score is larger than p − a/2,
where mean p and the standard deviation a are cal-
culated based on the entirety of depth scores. As TT
calculates the depth on every topic-sequence using
the highest gap, this could lead to a segmentation
in the middle of a sentence. To avoid this, a final
step ensures that the segmentation is positioned at
the nearest sentence boundary.
</bodyText>
<subsectionHeader confidence="0.990716">
4.2 C99 using Topic Models
</subsectionHeader>
<bodyText confidence="0.99496975">
For the C99 algorithm (Choi, 2000), named
(C99LDA) when using topic IDs, the text is divided
into minimal units on sentence boundaries. A sim-
ilarity matrix 5mxm is computed, where m denotes
the number of units (sentences). Every element sij
is calculated using the cosine similarity between unit
i and j. Next, a rank matrix R is computed to im-
prove the contrast of 5: Each element rij contains
the number of neighbors of sij that have lower simi-
larity scores then sij itself. In a final step a top-down
clustering algorithm is performed to split the docu-
ment into m segments B = bi, ... , bm. This algo-
</bodyText>
<footnote confidence="0.97786">
1words instead of topic IDs are utilized in the original ap-
proach.
</footnote>
<page confidence="0.996202">
554
</page>
<bodyText confidence="0.9998495">
rithm starts with the whole document considered as
one segment and splits off segments until the stop
criteria are met, e.g. the number of segments or a
similarity threshold.
</bodyText>
<subsectionHeader confidence="0.998258">
4.3 TopicTiling
</subsectionHeader>
<bodyText confidence="0.999974041666667">
TopicTiling is a new TextTiling-based algorithm and
is adjusted to use TMs. As we have found in data
analysis, it is frequently the case that a topic dom-
inates within a sampling unit (sentence), and that
units from the same segment frequently are domi-
nated by the same topic. In contrast to word-based
representations, we expect no need to face sparsity
issues that require smoothing methods (see TT) and
ranking methods (see C99), which allows us to sim-
plify the algorithm. Initially, the document is split
into minimal units on sentence boundaries. To mea-
sure the coherence between units, the cosine similar-
ity (vector dot product) between two adjacent sen-
tences is computed. Each sentences s is represented
as a N-dimensional vector, where N is the number
of topics defined in the TMs. The i-th element of the
vector contains the number of times the i-th topic
is observed in the sentence. In comparison to TT
we search all local minima based on these similar-
ity scores and calculate for these positions the depth
score as described in TT. If the number of segments
is known in advance, the segments of the n-highest
depth-scores are used, otherwise the cut-off score
criteria used in TT is adapted.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999932730769231">
As laid out in Section 3, a LDA Model is estimated
on a training dataset and used for inference on the
test set. To ensure that we do no use informa-
tion from the test set, we perform a 10-fold Cross
Validation (CV) for all reported results. To reduce
the variance of the shown results, derived by the ran-
dom nature of sampling and inference, the results
for each fold are calculated 30 times using different
LDA models.
The LDA model is trained with N=100 top-
ics, 500 sampling iterations and symmetric hy-
perparameters as recommended by Griffiths and
Steyvers (2004)(α=50/N and 0=0.01), using JGibb-
sLda (Phan and Nguyen, 2007). For the annota-
tion of unseen data with topic information, we use
LDA inference, sampling 100 iterations. Inference
is executed sentence-wise, since sentences form the
minimal unit of our segmentation algorithms and we
cannot use document information in the test setting.
The performance of the algorithms is measured us-
ing Pk and WindowDiff (WD) metrics (Beeferman
et al., 1999; Pevzner and Hearst, 2002). The C99 al-
gorithm is initialized with a 11×11 ranking mask, as
recommended in Choi (2000). TT is configured ac-
cording to Choi (2000) with sequence length w=20
and block size k=6.
</bodyText>
<subsectionHeader confidence="0.996816">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.99996894117647">
For evaluation, we rely on the Choi data set (Choi,
2000), which has been used in several other text seg-
mentation approaches to ensure comparability. This
data set is generated artificially using the Brown cor-
pus and consists of 700 documents. Each docu-
ment consists of 10 segments. For its generation,
3–11 sentences are sequentially extracted from a
randomly selected document and merged together.
While our CV evaluation setting is designed to avoid
using the same documents for training and testing,
this cannot be guaranteed as the segments within the
documents generated by Choi are included in sev-
eral documents. This problem also occurs in other
approaches, but has not be described in (Fragkou et
al., 2004; Misra et al., 2009; Galley et al., 2003),
where parts or the whole dataset are used for train-
ing either TF-IDF values or topic models.
</bodyText>
<subsectionHeader confidence="0.773162">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999878357142857">
For the experiments the C99 and TT implementa-
tions2 are executed in two settings: using words and
using topics. When using words, TT and C99 use
stemmed words and filter out words using a stop-
word list. C99 additional removes words using pre-
defined regular expressions. In the case of topic IDs,
no stopword filtering was deemed necessary. Table
1 shows the result of the different algorithms with all
combination of provided segment number and using
the mode method.
We note that WD values are always higher than
the Pk values, and these measures are highly corre-
lated. First we discuss results for the setting with
number of segments provided (see column 2-5 of
</bodyText>
<footnote confidence="0.802383">
2We use the implementations by Choi available at http:
//code.google.com/p/uima-text-segmenter/.
</footnote>
<page confidence="0.993367">
555
</page>
<table confidence="0.9995595">
Method Segments provided Segments unprovided
mode=false mode=true mode=false mode=true
Pk WD Pk WD Pk WD Pk WD
C99 11.20 12.07 12.73 14.57
C99LDA 4.16 4.89 2.67 3.08 8.69 10.52 3.24 4.08
TT 44.48 47.11 49.51 66.16
TTLDA 1.85 2.10 1.04 1.18 16.41 21.40 2.89 3.67
TopicTiling 2.65 3.02 2.12 2.42 4.12 5.75 2.30 3.08
TopicTiling 1.50 1.72 1.06 1.21 3.24 4.58 1.39 1.84
(filtered)
</table>
<tableCaption confidence="0.941214833333333">
Table 1: Results by segment length for TT with
words and topics (TTLDA), C99 with words and topics
(C99LDA) and TopicTiling using all sentences and using
only sentences with more then 5 word tokens (filtered).
Table 1). A significant improvement for C99 and
TT can be achieved when using topic IDs. In case
</tableCaption>
<bodyText confidence="0.999264696969697">
of C99LDA, the error rate is at least halved and for
TTLDA the error rate is reduced by a factor of 20.
Using the most frequent topic ID assigned during
the Bayesian inference (mode method) reduces the
error rates further for the TM-based approaches, as
the probability for randomly assigned topic IDs is
decreased. The newly introduced algorithm Top-
icTiling as described above does not improve over
TTLDA. Analysis revealed that the Choi corpus in-
cludes also captions and other “non-sentences” that
are marked as sentences, which causes TopicTil-
ing to introduce false positive segments since the
topic vectors are too sparse for these short “non-
sentences”. We therefore filter out “sentences” with
less than 5 words (see bottom line in Table 1).
This leads to errors values that are close to the re-
sults achieved with TTLDA when the mode is used.
When the number of segments is not given in ad-
vance (see columns 6-9 in Table 1), we again ob-
serve significantly better results comparing topic-
based methods to word-based methods. But the er-
ror rates of TTLDA are unexpectedly high when the
mode method is not used. We discovered in data
analysis that TT estimates too many segments, as the
topic ID distributions between adjacent sentences
within a segment are often too diverse, especially
in face of random fluctuations from the topic assign-
ments. Estimating the number of segments is better
achieved using TopicTiling instead of TTLDA.
In Table 2, we compare TTLDA, C99LDA and
our TopicTiling algorithm to other published results
on the same dataset. We can see that all introduced
topic-based methods outperform the yet best pub-
</bodyText>
<table confidence="0.6675049">
Method Segments
provided unprovided
TT 44.48 49.51
C99 11.20 12.73
U00 (Utiyama and Isahara, 2001) 9 10
F04 (Fragkou et al., 2004) 5.39
M09 (Misra et al., 2009) 2.73
C99LDA (mode = true) 2.67 3.24
TTLDA (mode=true) 1.04 2.89
TopicTiling (mode=true, filtered) 1.06 1.39
</table>
<tableCaption confidence="0.974736">
Table 2: List of lowest Pk values for the Choi data set for
different algorithms in the literature.
</tableCaption>
<bodyText confidence="0.999433266666667">
lished M09 algorithm (Misra et al., 2009). The
improvements of C99, TTLDA and TopicTiling in
comparison to M09 are significant3.
TopicTiling and TTLDA are computationally
more efficient than M09. Whereas our linear method
has a complexity of O(T) (T is the number of
sentences), dynamic algorithms like M09 have a
complexity of O(T2) (cf. Fragkou et al. (2004)),
which also applies to the number of topic inference
runs. When the number of segments is not given
in advance, TopicTiling outperforms TTLDA sig-
nificantly. As an additional benefit, TopicTiling is
even simpler than TT, as no smoothing parameter is
needed and the depth scores are only calculated for
the minima of the similarity scores.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976916666667">
The method introduced in this paper shows that us-
ing semantic information, provided by TMs, can im-
prove existing algorithm significantly. This is at-
tested modifying the algorithm TT and C99. With
TopicTiling a new simplistic topic based algorithm
is developed that can produce state-of-the-art results
based on the Choi corpus and outperform TTLDA
when the number of segments is unknown. Addi-
tionally this method is computationally more effi-
cient in comparison to other topic based segmenta-
tion algorithms. Another contribution is the mode
method for stabilizing topic ID assignments.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999659">
This work has been supported by LOEWE as part of
the research center “Digital Humanities”. We would
like to thank the anonymous reviewers for their com-
ments, which truly helped to improve the paper.
</bodyText>
<footnote confidence="0.82948">
3using a one sampled t-test with α = 0.05
</footnote>
<page confidence="0.996075">
556
</page>
<sectionHeader confidence="0.989819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765703125">
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine learning, 34(1):177–210.
David M. Blei, Andrew Y Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, pages 26–33,
Seattle, WA, USA.
Jacob Eisenstein. 2009. Hierarchical text segmenta-
tion from multi-scale lexical cohesion. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 353–
361, Boulder, CO, USA.
Pavlina Fragkou, Vassilios Petridis, and Athanasios Ke-
hagias. 2004. A Dynamic Programming Algorithm
for Linear Text Segmentation. Journal of Intelligent
Information Systems, 23(2):179–197.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation
of multi-party conversation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1, pages 562–569, Sapporo,
Japan.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
M A K Halliday and Ruqaiya Hasan. 1976. Cohesion in
English, volume 1 of English Language Series. Long-
man.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd annual
meeting on Association for Computational Linguistics,
pages 9–16, Las Cruces, NM, USA.
P.-Y. Hsueh, J. D. Moore, and S. Renals. 2006. Auto-
matic segmentation of multiparty dialogue. AMI-156.
Hemant Misra, Joemon M Jose, and Olivier Capp´e. 2009.
Text Segmentation via Topic Modeling : An Analyti-
cal Study. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, pages
1553–1556, Hong Kong.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistic, 28(1):19–36.
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. Gibb-
sLDA++: A C/C++ implementation of latent Dirichlet
allocation (LDA). http://jgibblda.sourceforge.net/.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher ker-
nel. Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Human
Language Technologies, pages 269–272.
Masao Utiyama and Hitoshi Isahara. 2001. A statisti-
cal model for domain-independent text segmentation.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 499–506,
Toulouse, France.
Yaakov Yaari. 1997. Segmentation of expository texts
by hierarchical agglomerative clustering. In Proceed-
ings of the Conference on Recent Advances in Natural
Language Processing, Tzigov Chark, Bulgaria.
</reference>
<page confidence="0.997346">
557
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739426">
<title confidence="0.999792">How Text Segmentation Algorithms Gain from Topic Models</title>
<author confidence="0.798631">Riedl</author>
<affiliation confidence="0.860958">Ubiquitous Knowledge Processing Computer Science Department, Technische Universit¨at</affiliation>
<address confidence="0.988365">Hochschulstrasse 10, D-64289 Darmstadt, Germany</address>
<email confidence="0.997193">riedl@ukp.informatik.tu-darmstadt.de,biem@cs.tu-darmstadt.de</email>
<abstract confidence="0.999598692307692">This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="10520" citStr="Beeferman et al., 1999" startWordPosition="1717" endWordPosition="1720"> 30 times using different LDA models. The LDA model is trained with N=100 topics, 500 sampling iterations and symmetric hyperparameters as recommended by Griffiths and Steyvers (2004)(α=50/N and 0=0.01), using JGibbsLda (Phan and Nguyen, 2007). For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000) with sequence length w=20 and block size k=6. 5.1 Data Set For evaluation, we rely on the Choi data set (Choi, 2000), which has been used in several other text segmentation approaches to ensure comparability. This data set is generated artificially using the Brown corpus and consists of 700 documents. Each document consists of 10 segments. For its generation, 3–11 sentences are sequentially extracted from a randomly selected document </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine learning, 34(1):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4079" citStr="Blei et al., 2003" startWordPosition="618" endWordPosition="621">om Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter a. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter Qzdi . Unseen documents can be annotated with an existing TM using Bayesian inference methods (here: Gibbs sampling). 3 Method: From Words to Topics The underlying mechani</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>26--33</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="2769" citStr="Choi, 2000" startWordPosition="409" endWordPosition="410">, while being conceptually simpler and computationally more efficient than other topic-based segmentation algorithms. 2 Related Work Based on the observation of Halliday and Hasan (1976) that the density of coherence relations is higher within segments than between segments, most algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to </context>
<context position="7469" citStr="Choi, 2000" startWordPosition="1194" endWordPosition="1195"> sorted. If the number of segments n is given as input parameter, the n highest depth scores are used, otherwise a cut-off function is used that applies a segment only if the depth score is larger than p − a/2, where mean p and the standard deviation a are calculated based on the entirety of depth scores. As TT calculates the depth on every topic-sequence using the highest gap, this could lead to a segmentation in the middle of a sentence. To avoid this, a final step ensures that the segmentation is positioned at the nearest sentence boundary. 4.2 C99 using Topic Models For the C99 algorithm (Choi, 2000), named (C99LDA) when using topic IDs, the text is divided into minimal units on sentence boundaries. A similarity matrix 5mxm is computed, where m denotes the number of units (sentences). Every element sij is calculated using the cosine similarity between unit i and j. Next, a rank matrix R is computed to improve the contrast of 5: Each element rij contains the number of neighbors of sij that have lower similarity scores then sij itself. In a final step a top-down clustering algorithm is performed to split the document into m segments B = bi, ... , bm. This algo1words instead of topic IDs are</context>
<context position="10638" citStr="Choi (2000)" startWordPosition="1739" endWordPosition="1740">meters as recommended by Griffiths and Steyvers (2004)(α=50/N and 0=0.01), using JGibbsLda (Phan and Nguyen, 2007). For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000) with sequence length w=20 and block size k=6. 5.1 Data Set For evaluation, we rely on the Choi data set (Choi, 2000), which has been used in several other text segmentation approaches to ensure comparability. This data set is generated artificially using the Brown corpus and consists of 700 documents. Each document consists of 10 segments. For its generation, 3–11 sentences are sequentially extracted from a randomly selected document and merged together. While our CV evaluation setting is designed to avoid using the same documents for training and te</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 26–33, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Hierarchical text segmentation from multi-scale lexical cohesion.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>353--361</pages>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="4008" citStr="Eisenstein, 2009" startWordPosition="607" endWordPosition="608"> of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter a. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter Qzdi . Unseen documents can be annotated with an existing TM using Bayesian inference methods (here:</context>
</contexts>
<marker>Eisenstein, 2009</marker>
<rawString>Jacob Eisenstein. 2009. Hierarchical text segmentation from multi-scale lexical cohesion. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 353– 361, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavlina Fragkou</author>
</authors>
<title>Vassilios Petridis, and Athanasios Kehagias.</title>
<date>2004</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>23</volume>
<issue>2</issue>
<marker>Fragkou, 2004</marker>
<rawString>Pavlina Fragkou, Vassilios Petridis, and Athanasios Kehagias. 2004. A Dynamic Programming Algorithm for Linear Text Segmentation. Journal of Intelligent Information Systems, 23(2):179–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Fosler-Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>562--569</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3055" citStr="Galley et al. (2003)" startWordPosition="457" endWordPosition="461">ost algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada,</context>
<context position="11498" citStr="Galley et al., 2003" startWordPosition="1881" endWordPosition="1884">mparability. This data set is generated artificially using the Brown corpus and consists of 700 documents. Each document consists of 10 segments. For its generation, 3–11 sentences are sequentially extracted from a randomly selected document and merged together. While our CV evaluation setting is designed to avoid using the same documents for training and testing, this cannot be guaranteed as the segments within the documents generated by Choi are included in several documents. This problem also occurs in other approaches, but has not be described in (Fragkou et al., 2004; Misra et al., 2009; Galley et al., 2003), where parts or the whole dataset are used for training either TF-IDF values or topic models. 5.2 Results For the experiments the C99 and TT implementations2 are executed in two settings: using words and using topics. When using words, TT and C99 use stemmed words and filter out words using a stopword list. C99 additional removes words using predefined regular expressions. In the case of topic IDs, no stopword filtering was deemed necessary. Table 1 shows the result of the different algorithms with all combination of provided segment number and using the mode method. We note that WD values ar</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1, pages 562–569, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="10081" citStr="Griffiths and Steyvers (2004)" startWordPosition="1649" endWordPosition="1652"> the cut-off score criteria used in TT is adapted. 5 Evaluation As laid out in Section 3, a LDA Model is estimated on a training dataset and used for inference on the test set. To ensure that we do no use information from the test set, we perform a 10-fold Cross Validation (CV) for all reported results. To reduce the variance of the shown results, derived by the random nature of sampling and inference, the results for each fold are calculated 30 times using different LDA models. The LDA model is trained with N=100 topics, 500 sampling iterations and symmetric hyperparameters as recommended by Griffiths and Steyvers (2004)(α=50/N and 0=0.01), using JGibbsLda (Phan and Nguyen, 2007). For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000)</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<journal>of English Language Series. Longman.</journal>
<booktitle>Cohesion in English,</booktitle>
<volume>1</volume>
<contexts>
<context position="2344" citStr="Halliday and Hasan (1976)" startWordPosition="343" endWordPosition="346">rly word-based algorithms with their topic-based variants, and construct our own algorithm called TopicTiling. We show that using topics estimated by Latent Dirichlet Allocation (LDA) in lieu of words substantially improves earlier segmentation algorithms. In comparison to TextTiling (TT), neither smoothing nor a blocksize or window size is needed. TT using TMs and our own algorithm improve on the state-ofthe-art for a standard dataset, while being conceptually simpler and computationally more efficient than other topic-based segmentation algorithms. 2 Related Work Based on the observation of Halliday and Hasan (1976) that the density of coherence relations is higher within segments than between segments, most algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M A K Halliday and Ruqaiya Hasan. 1976. Cohesion in English, volume 1 of English Language Series. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<location>Las Cruces, NM, USA.</location>
<contexts>
<context position="2588" citStr="Hearst, 1994" startWordPosition="381" endWordPosition="382"> In comparison to TextTiling (TT), neither smoothing nor a blocksize or window size is needed. TT using TMs and our own algorithm improve on the state-ofthe-art for a standard dataset, while being conceptually simpler and computationally more efficient than other topic-based segmentation algorithms. 2 Related Work Based on the observation of Halliday and Hasan (1976) that the density of coherence relations is higher within segments than between segments, most algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) ar</context>
<context position="5727" citStr="Hearst, 1994" startWordPosition="891" endWordPosition="892">ral topics could be assigned to different topics in different inference iterations. To avoid these instabilities, we save all topic IDs assigned to a word for each inference iteration. Finally, the most frequent topic ID is assigned to each word. This mechanism we call the mode method. Both word replacements can be applied to most segmentation algorithms. In this work, we use this general setup to implement topic-based versions of TT and C99 and develop a new TextTiling-based method called TopicTiling. 4 Topic-based Segmentation Algorithms 4.1 TextTiling using Topic Models In TextTiling (TT) (Hearst, 1994) using topic IDs (TTLDA), a document D, which is subject to segmentation, is represented as a sequence of n topic IDs1. TT splits the document into topic-sequences, instead of sentences, where each sequence consists of w topic IDs. To calculate the similarity between two topic-sequences, called sequence-gap, TT uses k topic-sequences, named block, to the left and to the right of the sequence gap. This parameter k defines the so-called blocksize. The cosine similarity is applied to computed a similarity score based on the topic frequency of the adjacent blocks at each sequence-gap. A value clos</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 9–16, Las Cruces, NM, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-Y Hsueh</author>
<author>J D Moore</author>
<author>S Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<pages>156</pages>
<contexts>
<context position="3989" citStr="Hsueh et al., 2006" startWordPosition="603" endWordPosition="606">leviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter a. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter Qzdi . Unseen documents can be annotated with an existing TM using Bayesian infer</context>
</contexts>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>P.-Y. Hsueh, J. D. Moore, and S. Renals. 2006. Automatic segmentation of multiparty dialogue. AMI-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hemant Misra</author>
<author>Joemon M Jose</author>
<author>Olivier Capp´e</author>
</authors>
<title>Text Segmentation via Topic Modeling : An Analytical Study.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1553--1556</pages>
<location>Hong Kong.</location>
<marker>Misra, Jose, Capp´e, 2009</marker>
<rawString>Hemant Misra, Joemon M Jose, and Olivier Capp´e. 2009. Text Segmentation via Topic Modeling : An Analytical Study. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 1553–1556, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistic,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="10547" citStr="Pevzner and Hearst, 2002" startWordPosition="1721" endWordPosition="1724">t LDA models. The LDA model is trained with N=100 topics, 500 sampling iterations and symmetric hyperparameters as recommended by Griffiths and Steyvers (2004)(α=50/N and 0=0.01), using JGibbsLda (Phan and Nguyen, 2007). For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000) with sequence length w=20 and block size k=6. 5.1 Data Set For evaluation, we rely on the Choi data set (Choi, 2000), which has been used in several other text segmentation approaches to ensure comparability. This data set is generated artificially using the Brown corpus and consists of 700 documents. Each document consists of 10 segments. For its generation, 3–11 sentences are sequentially extracted from a randomly selected document and merged together. While </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistic, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
<author>Cam-Tu Nguyen</author>
</authors>
<title>GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA).</title>
<date>2007</date>
<note>http://jgibblda.sourceforge.net/.</note>
<contexts>
<context position="10141" citStr="Phan and Nguyen, 2007" startWordPosition="1658" endWordPosition="1661">laid out in Section 3, a LDA Model is estimated on a training dataset and used for inference on the test set. To ensure that we do no use information from the test set, we perform a 10-fold Cross Validation (CV) for all reported results. To reduce the variance of the shown results, derived by the random nature of sampling and inference, the results for each fold are calculated 30 times using different LDA models. The LDA model is trained with N=100 topics, 500 sampling iterations and symmetric hyperparameters as recommended by Griffiths and Steyvers (2004)(α=50/N and 0=0.01), using JGibbsLda (Phan and Nguyen, 2007). For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000) with sequence length w=20 and block size k=6. 5.1 Data Set </context>
</contexts>
<marker>Phan, Nguyen, 2007</marker>
<rawString>Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA). http://jgibblda.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Sun</author>
<author>Runxin Li</author>
<author>Dingsheng Luo</author>
<author>Xihong Wu</author>
</authors>
<title>Text segmentation with LDA-based Fisher kernel.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies,</booktitle>
<pages>269--272</pages>
<contexts>
<context position="3340" citStr="Sun et al., 2008" startWordPosition="506" endWordPosition="509">s of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation</context>
</contexts>
<marker>Sun, Li, Luo, Wu, 2008</marker>
<rawString>Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu. 2008. Text segmentation with LDA-based Fisher kernel. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies, pages 269–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>499--506</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3225" citStr="Utiyama and Isahara, 2001" startWordPosition="485" endWordPosition="488">he simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high</context>
<context position="14782" citStr="Utiyama and Isahara, 2001" startWordPosition="2430" endWordPosition="2433"> not used. We discovered in data analysis that TT estimates too many segments, as the topic ID distributions between adjacent sentences within a segment are often too diverse, especially in face of random fluctuations from the topic assignments. Estimating the number of segments is better achieved using TopicTiling instead of TTLDA. In Table 2, we compare TTLDA, C99LDA and our TopicTiling algorithm to other published results on the same dataset. We can see that all introduced topic-based methods outperform the yet best pubMethod Segments provided unprovided TT 44.48 49.51 C99 11.20 12.73 U00 (Utiyama and Isahara, 2001) 9 10 F04 (Fragkou et al., 2004) 5.39 M09 (Misra et al., 2009) 2.73 C99LDA (mode = true) 2.67 3.24 TTLDA (mode=true) 1.04 2.89 TopicTiling (mode=true, filtered) 1.06 1.39 Table 2: List of lowest Pk values for the Choi data set for different algorithms in the literature. lished M09 algorithm (Misra et al., 2009). The improvements of C99, TTLDA and TopicTiling in comparison to M09 are significant3. TopicTiling and TTLDA are computationally more efficient than M09. Whereas our linear method has a complexity of O(T) (T is the number of sentences), dynamic algorithms like M09 have a complexity of O</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 499–506, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Yaari</author>
</authors>
<title>Segmentation of expository texts by hierarchical agglomerative clustering.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing, Tzigov Chark,</booktitle>
<contexts>
<context position="3969" citStr="Yaari, 1997" startWordPosition="601" endWordPosition="602">so used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter a. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter Qzdi . Unseen documents can be annotated with an existing TM </context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proceedings of the Conference on Recent Advances in Natural Language Processing, Tzigov Chark, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>