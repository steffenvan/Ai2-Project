<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.105131">
<title confidence="0.980242">
Jane: Open Source Machine Translation System Combination
</title>
<author confidence="0.986212">
Markus Freitag1 and Matthias Huck2 and Hermann Ney1
</author>
<affiliation confidence="0.834591">
1 Lehrstuhl f¨ur Informatik 6 2 School of Informatics
Computer Science Department University of Edinburgh
RWTH Aachen University 10 Crichton Street
D-52056 Aachen, Germany Edinburgh EH8 9AB, UK
</affiliation>
<email confidence="0.992808">
{freitag,ney}@cs.rwth-aachen.de mhuck@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999604739130435">
Different machine translation engines can
be remarkably dissimilar not only with re-
spect to their technical paradigm, but also
with respect to the translation output they
yield. System combination is a method for
combining the output of multiple machine
translation engines in order to take benefit
of the strengths of each of the individual
engines.
In this work we introduce a novel system
combination implementation which is in-
tegrated into Jane, RWTH’s open source
statistical machine translation toolkit. On
the most recent Workshop on Statisti-
cal Machine Translation system combi-
nation shared task, we achieve improve-
ments of up to 0.7 points in BLEU over
the best system combination hypotheses
which were submitted for the official eval-
uation. Moreover, we enhance our sys-
tem combination pipeline with additional
n-gram language models and lexical trans-
lation models.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829">
We present a novel machine translation system
combination framework which has been imple-
mented and released as part of the most recent ver-
sion of the Jane toolkit.1 Our system combina-
tion framework has already been applied success-
fully for joining the outputs of different individual
machine translation engines from several project
partners within large-scale projects like Quaero
(Peitz and others, 2013), EU-BRIDGE (Freitag
and others, 2013), and DARPA BOLT. The com-
bined translation is typically of better quality than
</bodyText>
<footnote confidence="0.996258333333333">
1Jane is publicly available under an open source non-
commercial license and can be downloaded from http://
www.hltpr.rwth-aachen.de/jane/.
</footnote>
<bodyText confidence="0.99987896969697">
any of the individual hypotheses. The source code
of our framework has now been released to the
public.
We focus on system combination via confusion
network decoding. This basically means that we
align all input hypotheses from individual machine
translation (MT) engines together and extract a
combination as a new output. For our baseline
algorithm we only need the first best translation
from each of the different MT engines, without
any additional information. Supplementary to the
baseline models integrated into our framework, we
optionally allow for utilization of n-gram language
models and IBM-1 lexicon models (Brown et al.,
1993), both trained on additional training corpora
that might be at hand.
We evaluate the Jane system combination
framework on the latest official Workshop on
Statistical Machine Translation (WMT) system
combination shared task (Callison-Burch et al.,
2011). Many state-of-the-art MT system combi-
nation toolkits have been evaluated on this task,
which allows us to directly compare the results ob-
tained with our novel Jane system combination
framework with the best known results obtained
with other toolkits.
The paper is structured as follows: We com-
mence with giving a brief outline of some related
work (Section 2). In Section 3 we describe the
techniques which are implemented in the Jane
MT system combination framework. The exper-
imental results are presented and analyzed in Sec-
tion 4. We conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999578">
The first application of system combination to MT
has been presented by Bangalore et al. (2001).
They used a multiple string alignment (MSA) ap-
proach to align the hypotheses together and built
a confusion network from which the system com-
bination output is determined using majority vot-
</bodyText>
<page confidence="0.990819">
29
</page>
<note confidence="0.9603635">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999862">
Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path.
</figureCaption>
<bodyText confidence="0.999464777777778">
ing and an additional language model. Matusov
et al. (2006) proposed an alignment based on the
GIZA++ toolkit which introduced word reordering
not present in MSA, and Sim et al. (2007) used
alignments produced by TER scoring (Snover et
al., 2006). Extensions of the last two are based on
hidden Markov models (He et al., 2008), inversion
transduction grammars (Karakos et al., 2008), or
METEOR (Heafield and Lavie, 2010).
</bodyText>
<sectionHeader confidence="0.882859" genericHeader="method">
3 The Jane MT System Combination
Framework
</sectionHeader>
<bodyText confidence="0.99999375">
In this section we describe the techniques for MT
system combination which we implemented in the
Jane toolkit.2 We first address the generation of a
confusion network from the input translations. For
that we need a pairwise alignment between all in-
put hypotheses. We then present word reordering
mechanisms, the baseline models, and additional
advanced models which can be applied for system
combination using Jane. The system combina-
tion decoding step basically involves determining
the shortest path through the confusion network
based on several model scores from this network.
</bodyText>
<subsectionHeader confidence="0.994979">
3.1 Confusion Network
</subsectionHeader>
<bodyText confidence="0.999519666666667">
A confusion network represents all different com-
bined translations we can generate from the set of
provided input hypotheses. Figure 1 depicts an ex-
ample of a confusion network. A word alignment
between all pairs of input hypotheses is required
for generating a confusion network. For conve-
nience, we first select one of the input hypotheses
as the primary hypothesis. The primary hypothesis
then determines the word order and all remaining
hypotheses are word-to-word aligned to the given
word order.
To generate a meaningful confusion network,
we should adopt an alignment which only al-
lows to switch between words which are syn-
onyms, misspellings, morphological variants or
on a higher level paraphrases of the words from
the primary hypothesis. In this work we use
METEOR alignments. METEOR (Denkowski
</bodyText>
<footnote confidence="0.897601666666667">
2Practical usage aspects are explained in the man-
ual: http://www.hltpr.rwth-aachen.de/jane/
manual.pdf
</footnote>
<bodyText confidence="0.999942363636364">
and Lavie, 2011) was originally designed to re-
order a translation for scoring and has a high pre-
cision. The recall is lower because synonyms
which are not in the METEOR database or punc-
tuation marks like “!” and “?” are not aligned
to each other. For our purposes, we augment the
METEOR paraphrase table with entries like “.|!”,
“.|?”, or “the|a”.
Figure 2 shows an example METEOR hypothe-
sis alignment. The primary hypothesis “isolated
cdna lib” determines the word order. An entry
“a|b” means that word “a” from a secondary hy-
pothesis has been aligned to word “b” from the
primary one. “*EPS*” is the empty word and
thus an entry “*EPS*|b” means that no word could
be aligned to the primary hypothesis word “b”.
“a|*EPS*” means that the word “a” has not been
aligned to any word from the primary hypothesis.
After producing the alignment information, we
can build the confusion network. Now, we are able
to not only extract the original primary hypoth-
esis from the confusion network but also switch
words from the primary hypothesis to words from
any secondary hypothesis (also the empty word)
or insert words or sequences of words.
In the final confusion network, we do not stick
to one hypothesis as the primary system. For m in-
put hypotheses we build m different confusion net-
works, each having a different system as primary
system. The final confusion network is a union of
all m networks.3
The most straightforward way to obtain a com-
bined hypothesis from a confusion network is to
extract it via majority voting. For example, in
the first column in Figure 3, “the” has been seen
three times, but the translation options “a” and
“an” have each been seen only once. By means
of a straight majority vote we would extract “the”.
As the different single system translations are of
varying utility for system combination, we assign
a system weight to each input hypothesis. The sys-
tem weights are set by optimizing scaling factors
for binary system voting features (cf. Section 3.3).
We employ some more weighted baseline features
</bodyText>
<footnote confidence="0.94662">
3Jane’s implementation for building confusion networks
is based on the OpenFST library (Allauzen et al., 2007).
</footnote>
<page confidence="0.983508">
30
</page>
<table confidence="0.840957357142857">
the|*EPS* isolated|isolated cdna|cdna *EPS*|lib
a|*EPS* isolated|isolated cdna|cdna lib|lib
an|*EPS* isolated|isolated cdna|cdna lib|lib
the|*EPS* *EPS*|isolated cdna|cdna *EPS*|lib
the|*EPS* *EPS*|isolated cdna|cdna lib|lib
Figure 2: Alignment result after running
METEOR. *EPS* denotes the empty word.
*EPS* isolated cdna lib
the isolated cdna *EPS*
a isolated cdna lib
an isolated cdna lib
the *EPS* cdna lib
the *EPS* cdna *EPS*
the isolated cdna lib
</table>
<figureCaption confidence="0.9994865">
Figure 3: Majority vote on aligned words. The last
line is the system combination output.
</figureCaption>
<bodyText confidence="0.9989275">
and additional models (cf. Section 3.4) in the deci-
sion process. In Figure 1 we scored the confusion
network with some system weights and used the
shortest path algorithm to find the hypothesis with
the highest score (the hypothesis along the path
highlighted in red).
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Word Reordering
</subsectionHeader>
<bodyText confidence="0.9987610625">
Many words from secondary hypotheses can be
unaligned as they have no connection to any words
of the primary hypothesis. However, words from
different secondary systems could be related to
each other. In order to account for these relations
and to give the words from the secondary hypothe-
ses a higher chance to be present in the combined
output, we introduce some simple word reordering
mechanisms.
We rank the hypotheses according to a language
model trained on all input hypotheses. We initial-
ize the confusion network with the sentence from
the primary system. During the generation of the
confusion network we align the hypotheses con-
secutively into the confusion network via the fol-
lowing procedure:
</bodyText>
<listItem confidence="0.961132">
• If a word wi from hypothesis A has a relation
to a word vj of the primary hypothesis, we
insert it as a new translation alternative to vj.
• If wi has no relation to the primary, but to
a word uk from a secondary hypothesis in
the confusion network, we insert wi as a new
translation alternative to uk.
• Otherwise we insert wi in front of the previ-
ous inserted word wi_1 of hypothesis A. The
new position gets an epsilon arc for the pri-
mary and all unrelated secondary systems.
</listItem>
<subsectionHeader confidence="0.991472">
3.3 Baseline Models
</subsectionHeader>
<bodyText confidence="0.998429307692308">
Once we have the final confusion network, we
want to adopt models which are valuable features
to score the different translation options. In our
implementation we use the following set of stan-
dard models:
m binary system voting features For each word
the voting feature for system i (1 &lt; i &lt; m) is 1
iff the word is from system i, otherwise 0.
Binary primary system feature A feature that
marks the primary hypothesis.
LM feature 3-gram language model trained on
the input hypotheses.
Word penalty Counts the number of words.
</bodyText>
<subsectionHeader confidence="0.967413">
3.4 Additional Models
</subsectionHeader>
<bodyText confidence="0.999628">
The Jane system combination toolkit also pro-
vides the possibility to utilize some additional
models for system combination. For the current
release we integrated the optional usage of the fol-
lowing additional models:
</bodyText>
<listItem confidence="0.923251">
Big LM A big language model trained on larger
monolingual target-side corpora.
IBM-1 Source-to-target and target-to-source
IBM-1 lexical translation models obtained
from bilingual training data.
</listItem>
<sectionHeader confidence="0.990251" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999921578947369">
All experiments are conducted on the latest offi-
cial WMT system combination shared task.4 We
exclusively employ resources which were permit-
ted for the constrained track of the task in all our
setups. The big LM was trained on News Com-
mentary and Europarl data. As tuning set we
used newssyscombtune2011, as test set we used
newssyscombtest2011. Feature weights have been
optimized with MERT (Och, 2003). Table 1 con-
tains the empirical results (truecase). For all four
language pairs we achieve improvements over the
best 2011 evaluation system combination submis-
sion either in BLEU or TER. We get the highest
improvement of 0.7 points in BLEU for es—*en
when adding both the big LM and IBM-1 features.
Adding the big LM over the baseline enhances
the translation quality for all four language pairs.
Adding IBM-1 lexicon models on top of the big
LM is of marginal or no benefit for most language
</bodyText>
<footnote confidence="0.99941025">
4The most recent system combination shared task that
has been organized as part of the WMT evaluation cam-
paign took place in 2011. http://www.statmt.org/
wmt11/system-combination-task.html
</footnote>
<page confidence="0.999858">
31
</page>
<tableCaption confidence="0.999704">
Table 1: Experimental results on the WMT system combination tasks (newssyscombtest2011).
</tableCaption>
<table confidence="0.999467714285714">
system cz→en de→en es→en fr→en
BLEU TER BLEU TER BLEU TER BLEU TER
best single system 28.7 53.4 23.0 59.5 28.9 51.2 29.4 52.0
best 2011 evaluation syscomb 28.8 55.2 25.1 57.4 32.4 49.9 31.3 50.1
Jane syscomb baseline 28.8 53.6 24.7 57.6 32.7 50.3 31.3 50.3
Jane syscomb + big LM 29.0 54.5 25.0 57.3 32.9 50.3 31.4 50.0
Jane syscomb + big LM + IBM-1 29.0 54.5 25.0 57.3 33.1 50.0 31.5 50.1
</table>
<bodyText confidence="0.79817">
pairs, but at least provides slight improvements for
es→en.
</bodyText>
<sectionHeader confidence="0.998453" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999841">
RWTH’s open source machine translation toolkit
Jane now includes a state-of-the-art system com-
bination framework. We found that the Jane sys-
tem combination performs on a similar level or
better than the best evaluation system combina-
tion submissions on all WMT 2011 system com-
bination shared task language pairs (with English
as target language). We furthermore presented the
effects of integrating a big n-gram language model
and of lexical features from IBM-1 models.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998641">
This material is based upon work supported by
the DARPA BOLT project under Contract No.
HR0011- 12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 287658.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891524590164">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. In Jan Holub and Jan Zd´arek, edi-
tors, Implementation and Application of Automata,
volume 4783 of Lecture Notes in Computer Science,
pages 11–23. Springer Berlin Heidelberg.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing Consensus Translation
from Multiple Machine Translation Systems. In
Proc. of ASRU, pages 351–354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proc. of WMT, pages 22–64.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proc. of WMT, pages 85–91.
Markus Freitag et al. 2013. EU-BRIDGE MT: Text
Translation of Talks in the EU-BRIDGE Project. In
Proc. of IWSLT.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-
HMM-based Hypothesis Alignment for Combining
Outputs from Machine Translation Systems. In
Proc. of EMNLP, pages 98–107.
Kenneth Heafield and Alon Lavie. 2010. Combining
Machine Translation Output with Open Source: The
Carnegie Mellon Multi-Engine Machine Translation
Scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27–36.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. of ACL: Short Papers, pages 81–84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proc. of EACL, pages
33–40.
Franz J. Och. 2003. Minimum Error Rate Training for
Statistical Machine Translation. In Proc. of ACL,
pages 160–167.
Stephan Peitz et al. 2013. Joint WMT 2013 Submis-
sion of the QUAERO Project. In Proc. of WMT,
pages 185–192.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007.
Consensus Network Decoding for Statistical Ma-
chine Translation System Combination. In
Proc. of ICASSP, volume 4, pages 105–108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA, pages 223–231.
</reference>
<page confidence="0.9993">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325789">
<title confidence="0.941718">Jane: Open Source Machine Translation System Combination</title>
<affiliation confidence="0.7419825">f¨ur Informatik 6 of Informatics Computer Science Department University of Edinburgh</affiliation>
<address confidence="0.9829075">RWTH Aachen University 10 Crichton Street D-52056 Aachen, Germany Edinburgh EH8 9AB, UK</address>
<email confidence="0.997906">mhuck@inf.ed.ac.uk</email>
<abstract confidence="0.99856475">Different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they combination a method for combining the output of multiple machine translation engines in order to take benefit of the strengths of each of the individual engines. In this work we introduce a novel system combination implementation which is ininto RWTH’s open source statistical machine translation toolkit. On most recent on Statisti- Machine Translation combination shared task, we achieve improveof up to 0.7 points in the best system combination hypotheses which were submitted for the official evaluation. Moreover, we enhance our system combination pipeline with additional language models and lexical translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Wojciech Skut, and Mehryar Mohri.</title>
<date>2007</date>
<booktitle>Implementation and Application of Automata,</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<editor>In Jan Holub and Jan Zd´arek, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="8137" citStr="Allauzen et al., 2007" startWordPosition="1278" endWordPosition="1281">For example, in the first column in Figure 3, “the” has been seen three times, but the translation options “a” and “an” have each been seen only once. By means of a straight majority vote we would extract “the”. As the different single system translations are of varying utility for system combination, we assign a system weight to each input hypothesis. The system weights are set by optimizing scaling factors for binary system voting features (cf. Section 3.3). We employ some more weighted baseline features 3Jane’s implementation for building confusion networks is based on the OpenFST library (Allauzen et al., 2007). 30 the|*EPS* isolated|isolated cdna|cdna *EPS*|lib a|*EPS* isolated|isolated cdna|cdna lib|lib an|*EPS* isolated|isolated cdna|cdna lib|lib the|*EPS* *EPS*|isolated cdna|cdna *EPS*|lib the|*EPS* *EPS*|isolated cdna|cdna lib|lib Figure 2: Alignment result after running METEOR. *EPS* denotes the empty word. *EPS* isolated cdna lib the isolated cdna *EPS* a isolated cdna lib an isolated cdna lib the *EPS* cdna lib the *EPS* cdna *EPS* the isolated cdna lib Figure 3: Majority vote on aligned words. The last line is the system combination output. and additional models (cf. Section 3.4) in the dec</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. In Jan Holub and Jan Zd´arek, editors, Implementation and Application of Automata, volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing Consensus Translation from Multiple Machine Translation Systems.</title>
<date>2001</date>
<booktitle>In Proc. of ASRU,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="3489" citStr="Bangalore et al. (2001)" startWordPosition="527" endWordPosition="530"> been evaluated on this task, which allows us to directly compare the results obtained with our novel Jane system combination framework with the best known results obtained with other toolkits. The paper is structured as follows: We commence with giving a brief outline of some related work (Section 2). In Section 3 we describe the techniques which are implemented in the Jane MT system combination framework. The experimental results are presented and analyzed in Section 4. We conclude the paper in Section 5. 2 Related Work The first application of system combination to MT has been presented by Bangalore et al. (2001). They used a multiple string alignment (MSA) approach to align the hypotheses together and built a confusion network from which the system combination output is determined using majority vot29 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing Consensus Translation from Multiple Machine Translation Systems. In Proc. of ASRU, pages 351–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2555" citStr="Brown et al., 1993" startWordPosition="377" endWordPosition="380">hypotheses. The source code of our framework has now been released to the public. We focus on system combination via confusion network decoding. This basically means that we align all input hypotheses from individual machine translation (MT) engines together and extract a combination as a new output. For our baseline algorithm we only need the first best translation from each of the different MT engines, without any additional information. Supplementary to the baseline models integrated into our framework, we optionally allow for utilization of n-gram language models and IBM-1 lexicon models (Brown et al., 1993), both trained on additional training corpora that might be at hand. We evaluate the Jane system combination framework on the latest official Workshop on Statistical Machine Translation (WMT) system combination shared task (Callison-Burch et al., 2011). Many state-of-the-art MT system combination toolkits have been evaluated on this task, which allows us to directly compare the results obtained with our novel Jane system combination framework with the best known results obtained with other toolkits. The paper is structured as follows: We commence with giving a brief outline of some related wor</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar F Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proc. of WMT,</booktitle>
<pages>22--64</pages>
<contexts>
<context position="2807" citStr="Callison-Burch et al., 2011" startWordPosition="413" endWordPosition="416">ngines together and extract a combination as a new output. For our baseline algorithm we only need the first best translation from each of the different MT engines, without any additional information. Supplementary to the baseline models integrated into our framework, we optionally allow for utilization of n-gram language models and IBM-1 lexicon models (Brown et al., 1993), both trained on additional training corpora that might be at hand. We evaluate the Jane system combination framework on the latest official Workshop on Statistical Machine Translation (WMT) system combination shared task (Callison-Burch et al., 2011). Many state-of-the-art MT system combination toolkits have been evaluated on this task, which allows us to directly compare the results obtained with our novel Jane system combination framework with the best known results obtained with other toolkits. The paper is structured as follows: We commence with giving a brief outline of some related work (Section 2). In Section 3 we describe the techniques which are implemented in the Jane MT system combination framework. The experimental results are presented and analyzed in Section 4. We conclude the paper in Section 5. 2 Related Work The first app</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proc. of WMT, pages 22–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>85--91</pages>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proc. of WMT, pages 85–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
</authors>
<date>2013</date>
<booktitle>EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In Proc. of IWSLT.</booktitle>
<marker>Freitag, 2013</marker>
<rawString>Markus Freitag et al. 2013. EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>IndirectHMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="4344" citStr="He et al., 2008" startWordPosition="661" endWordPosition="664"> Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations. For that we need a pairwise alignment between all input hypotheses. We then present word reordering mechanisms, the baseline models, and additional advanced models which can be applied for system combination using Jane. The system combination decoding step basicall</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. IndirectHMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems. In Proc. of EMNLP, pages 98–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Alon Lavie</author>
</authors>
<title>Combining Machine Translation Output with Open Source: The Carnegie Mellon Multi-Engine Machine Translation Scheme. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<pages>93--27</pages>
<contexts>
<context position="4438" citStr="Heafield and Lavie, 2010" startWordPosition="674" endWordPosition="677"> pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations. For that we need a pairwise alignment between all input hypotheses. We then present word reordering mechanisms, the baseline models, and additional advanced models which can be applied for system combination using Jane. The system combination decoding step basically involves determining the shortest path through the confusion network based on several model </context>
</contexts>
<marker>Heafield, Lavie, 2010</marker>
<rawString>Kenneth Heafield and Alon Lavie. 2010. Combining Machine Translation Output with Open Source: The Carnegie Mellon Multi-Engine Machine Translation Scheme. The Prague Bulletin of Mathematical Linguistics, 93:27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using ITG-based alignments.</title>
<date>2008</date>
<booktitle>In Proc. of ACL: Short Papers,</booktitle>
<pages>81--84</pages>
<contexts>
<context position="4400" citStr="Karakos et al., 2008" startWordPosition="668" endWordPosition="671">ion for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations. For that we need a pairwise alignment between all input hypotheses. We then present word reordering mechanisms, the baseline models, and additional advanced models which can be applied for system combination using Jane. The system combination decoding step basically involves determining the shortest path through the con</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using ITG-based alignments. In Proc. of ACL: Short Papers, pages 81–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="4077" citStr="Matusov et al. (2006)" startWordPosition="615" endWordPosition="618">sented by Bangalore et al. (2001). They used a multiple string alignment (MSA) approach to align the hypotheses together and built a confusion network from which the system combination output is determined using majority vot29 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment. In Proc. of EACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11568" citStr="Och, 2003" startWordPosition="1838" endWordPosition="1839">ig language model trained on larger monolingual target-side corpora. IBM-1 Source-to-target and target-to-source IBM-1 lexical translation models obtained from bilingual training data. 4 Experimental Results All experiments are conducted on the latest official WMT system combination shared task.4 We exclusively employ resources which were permitted for the constrained track of the task in all our setups. The big LM was trained on News Commentary and Europarl data. As tuning set we used newssyscombtune2011, as test set we used newssyscombtest2011. Feature weights have been optimized with MERT (Och, 2003). Table 1 contains the empirical results (truecase). For all four language pairs we achieve improvements over the best 2011 evaluation system combination submission either in BLEU or TER. We get the highest improvement of 0.7 points in BLEU for es—*en when adding both the big LM and IBM-1 features. Adding the big LM over the baseline enhances the translation quality for all four language pairs. Adding IBM-1 lexicon models on top of the big LM is of marginal or no benefit for most language 4The most recent system combination shared task that has been organized as part of the WMT evaluation camp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Peitz</author>
</authors>
<date>2013</date>
<booktitle>Joint WMT 2013 Submission of the QUAERO Project. In Proc. of WMT,</booktitle>
<pages>185--192</pages>
<marker>Peitz, 2013</marker>
<rawString>Stephan Peitz et al. 2013. Joint WMT 2013 Submission of the QUAERO Project. In Proc. of WMT, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus Network Decoding for Statistical Machine Translation System Combination.</title>
<date>2007</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<volume>4</volume>
<pages>105--108</pages>
<contexts>
<context position="4202" citStr="Sim et al. (2007)" startWordPosition="636" endWordPosition="639">t a confusion network from which the system combination output is determined using majority vot29 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations. For that we need a pairwise alignment between all input hypotheses. We then present word reordering mechanisms, the baselin</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland. 2007. Consensus Network Decoding for Statistical Machine Translation System Combination. In Proc. of ICASSP, volume 4, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciula</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="4264" citStr="Snover et al., 2006" startWordPosition="646" endWordPosition="649">put is determined using majority vot29 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path. ing and an additional language model. Matusov et al. (2006) proposed an alignment based on the GIZA++ toolkit which introduced word reordering not present in MSA, and Sim et al. (2007) used alignments produced by TER scoring (Snover et al., 2006). Extensions of the last two are based on hidden Markov models (He et al., 2008), inversion transduction grammars (Karakos et al., 2008), or METEOR (Heafield and Lavie, 2010). 3 The Jane MT System Combination Framework In this section we describe the techniques for MT system combination which we implemented in the Jane toolkit.2 We first address the generation of a confusion network from the input translations. For that we need a pairwise alignment between all input hypotheses. We then present word reordering mechanisms, the baseline models, and additional advanced models which can be applied </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciula, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciula, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of AMTA, pages 223–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>