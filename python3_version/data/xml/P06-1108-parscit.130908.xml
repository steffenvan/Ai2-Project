<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001005">
<title confidence="0.996753">
Event Extraction in a Plot Advice Agent
</title>
<author confidence="0.999279">
Harry Halpin
</author>
<affiliation confidence="0.9986185">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.884395666666667">
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
</address>
<email confidence="0.999032">
H.Halpin@ed.ac.uk
</email>
<author confidence="0.992283">
Johanna D. Moore
</author>
<affiliation confidence="0.9982455">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.884318">
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
</address>
<email confidence="0.999125">
J.Moore@ed.ac.uk
</email>
<sectionHeader confidence="0.993919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999683555555555">
In this paper we present how the auto-
matic extraction of events from text can
be used to both classify narrative texts ac-
cording to plot quality and produce advice
in an interactive learning environment in-
tended to help students with story writing.
We focus on the story rewriting task, in
which an exemplar story is read to the stu-
dents and the students rewrite the story in
their own words. The system automati-
cally extracts events from the raw text, for-
malized as a sequence of temporally or-
dered predicate-arguments. These events
are given to a machine-learner that pro-
duces a coarse-grained rating of the story.
The results of the machine-learner and the
extracted events are then used to generate
fine-grained advice for the students.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999169529411765">
In this paper we investigate how features of a text
discovered via automatic event extraction can be
used in both natural language understanding and
advice generation in the domain of narrative in-
struction. The background application is a fully
automated plot analysis agent to improve the writ-
ing of students could be used by current nar-
rative tutoring systems (Robertson and Wiemer-
Hastings, 2002). As shown by participatory de-
sign studies, teachers are interested in a plot anal-
ysis agent that can give online natural language
advice and many students enjoy feedback from an
automated agent (Robertson and Cross, 2003). We
use automatic event extraction to create a story-
independent automated agent that can both ana-
lyze the plot of a story and generate appropriate
advice.
</bodyText>
<subsectionHeader confidence="0.996967">
1.1 The Story Rewriting Task
</subsectionHeader>
<bodyText confidence="0.999960083333333">
A task used in schools is the story rewriting task,
where a story, the exemplar story, is read to the
students, and afterwards the story is rewritten by
each student, providing a corpus of rewritten sto-
ries. This task tests the students ability to both
listen and write, while removing from the student
the cognitive load needed to generate a new plot.
This task is reminiscent of the well-known “War
of the Ghosts” experiment used in psychology for
studying memory (Bartlett, 1932) and related to
work in fields such as summarization (Lemaire et
al., 2005) and narration (Halpin et al., 2004).
</bodyText>
<subsectionHeader confidence="0.970666">
1.2 Agent Design
</subsectionHeader>
<bodyText confidence="0.999952181818182">
The goal of the agent is to classify each of the
rewritten stories for overall plot quality. This
rating can be used to give “coarse-grained” gen-
eral advice. The agent should then provide “fine-
grained” specific advice to the student on how their
plot could be improved. The agent should be able
to detect if the story should be re-read or a human
teacher summoned to help the student.
To accomplish this task, we extract events that
represent the entities and their actions in the plot
from both the exemplar and the rewritten stories.
A plot comparison algorithm checks for the pres-
ence or absence of events from the exemplar story
in each rewritten story. The results of this algo-
rithm will be used by a machine-learner to clas-
sify each story for overall plot quality and provide
general “canned” advice to the student. The fea-
tures statistically shared by “excellent” stories rep-
resent the important events of the exemplar story.
The results of a search for these important events
in a rewritten story provides the input needed by
templates to generate specific advice for a student.
</bodyText>
<page confidence="0.968843">
857
</page>
<note confidence="0.834132">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 857–864,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.956581" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999885375">
In order to train our agent, we collected a corpus
of 290 stories from primary schools based on two
different exemplar stories. The first is an episode
of “The Wonderful Adventures of Nils” by Selma
Lagerloff (160 stories) and the second a re-telling
of “The Treasure Thief” by Herodotus (130 sto-
ries). These will be referred to as the “Adventure”
and “Thief” corpora.
</bodyText>
<subsectionHeader confidence="0.990589">
2.1 Rating
</subsectionHeader>
<bodyText confidence="0.999123666666667">
An experienced teacher, Rater A, designed a rating
scheme equivalent to those used in schools. The
scheme rates the stories as follows:
</bodyText>
<listItem confidence="0.971691259259259">
1. Excellent: An excellent story shows that
the student has “read beyond the lines” and
demonstrates a deep understanding of the
story, using inference to grasp points that
may not have been explicit in the story. The
student should be able to retrieve all the im-
portant links, and not all the details, but the
right details.
2. Good: A good story shows that the student
understood the story and has “read between
the lines.” The student recalls the main events
and links in the plot. However, the student
shows no deep understanding of the plot and
does not make use of inference. This can of-
ten be detected by the student leaving out an
important link or emphasizing the wrong de-
tails.
3. Fair: A fair story shows that student has
listened to the story but not understood the
story, and so is only trying to repeat what they
have heard. This is shown by the fact that the
fair story is missing multiple important links
in the story, including a possibly vital part of
the story.
4. Poor: A poor story shows the student has had
trouble listening to the story. The poor story
is missing a substantial amount of the plot,
</listItem>
<bodyText confidence="0.937232285714286">
with characters left out and events confused.
The student has trouble connecting the parts
of the story.
To check the reliability of the rating scheme,
two other teachers (Rater B and Rater C) rated
subsets (82 and 68 respectively) of each of the cor-
pora. While their absolute agreement with Rater A
</bodyText>
<table confidence="0.9970286">
Class Adventure Thief
1 (Excellent) .231 .146
2 (Good) .300 .377
3 (Fair) .156 .292
4 (Poor) .313 .185
</table>
<tableCaption confidence="0.999785">
Table 1: Probability Distribution of Ratings
</tableCaption>
<bodyText confidence="0.999451416666667">
makes the task appear subjective (58% for B and
53% for C), their relative agreement was high, as
almost all disagreements were by one level in the
rating scheme. Therefore we use Cronbach’s α
and &apos;rb instead of Cohen’s or Fleiss’ r. to take into
account the fact that our scale is ordinal. Between
Rater A and B there was a Cronbach’s α statistic
of .90 and a Kendall’s &apos;rb statistic of .74. Between
Rater B and C there was a Cronbach’s α statis-
tic of .87 and Kendall’s &apos;rb statistic of .67. These
statistics show the rating scheme to be reliable and
the distribution of plot ratings are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.998398">
2.2 Linguistic Issues
</subsectionHeader>
<bodyText confidence="0.999942">
One challenge facing this task is the ungrammati-
cal and highly irregular text produced by the stu-
dents. Many stories consist of one long run-on
sentence. This leads a traditional parsing system
with a direct mapping from the parse tree to a se-
mantic representation to fail to achieve a parse on
35% percent of the stories, and as such could not
be used (Bos et al., 2004). The stories exhibit fre-
quent use of reported speech and the switching
from first-person to third-person within a single
sentence. Lastly, the use of incorrect spelling e.g.,
“stalk” for “stork” appearing in multiple stories
in the corpus, the consistent usage of homonyms
such as “there” for “their,” and the invention of
words (“torlix”), all prove to be frequent.
</bodyText>
<sectionHeader confidence="0.967748" genericHeader="method">
3 Plot Analysis
</sectionHeader>
<bodyText confidence="0.999760083333333">
To automatically rate student writing many tutor-
ing systems use Latent Semantic Analysis, a vari-
ation on the “bag-of-words” technique that uses
dimensionality reduction (Graesser et al., 2000).
We hypothesize that better results can be achieved
using a “representational” account that explicitly
represents each event in the plot. These semantic
relationships are important in stories, e.g., “The
thief jumped on the donkey” being distinctly dif-
ferent from “The donkey jumped on the thief.”
What characters participate in an action matter,
since “The king stole the treasure” reveals a major
</bodyText>
<page confidence="0.99711">
858
</page>
<bodyText confidence="0.998207">
misunderstanding while “The thief stole the trea-
sure” shows a correct interpretation by the student.
</bodyText>
<subsectionHeader confidence="0.999527">
3.1 Stories as Events
</subsectionHeader>
<bodyText confidence="0.999992086956522">
We represent a story as a sequence of events,
p1...ph, represented as a list of predicate-
arguments, similar to the event calculus (Mueller,
2003). Our predicate-argument structure is a mini-
mal subset of first-order logic (no quantifiers), and
so is compatible with case-frame and dependency
representations. Every event has a predicate (func-
tion) p that has one or more arguments, n1...na.
In the tradition of Discourse Representation The-
ory (Kamp and Reyle, 1993), our current predi-
cate argument structure could be converted auto-
matically to first order logic by using a default
existential quantification over the predicates and
joining them conjunctively. Predicate names are
often verbs, while their arguments are usually, al-
though not exclusively, nouns or adjectives. When
describing a set of events in the story, a superscript
is used to keep the arguments in an event distinct,
as n25 is argument 2 in event 5. The same argument
name may appear in multiple events. The plot of
any given story is formalized as an event structure
composed of h events in a partial order, with the
partial order denoting their temporal order:
</bodyText>
<equation confidence="0.968049">
p1 (1 2 a 2 4 c
</equation>
<bodyText confidence="0.937049">
n1, n1 , ...n1), ...., ph (nh, nh...nh)
An example from the “Thief” exemplar story is
“The Queen nagged the king to build a treasure
chamber. The king decided to have a treasure
chamber.” This can be represented by an event
structure as:
</bodyText>
<equation confidence="0.98565">
nag(king, queen)
build(chamber)
decide(king)
have(chamber)
</equation>
<bodyText confidence="0.999926">
Note due the ungrammatical corpus we cannot at
this time extract neo-Davidsonian events. A sen-
tence maps onto one, multiple, or no events. A
unique name and closed-world assumption is en-
forced, although for purposes of comparing event
we compare membership of argument and predi-
cate names in WordNet synsets in addition to exact
name matches (Fellbaum, 1998).
</bodyText>
<sectionHeader confidence="0.972062" genericHeader="method">
4 Extracting Events
</sectionHeader>
<bodyText confidence="0.999978613636364">
Paralleling work in summarization, it is hypothe-
sized that the quality of a rewritten story can be
defined by the presence or absence of “seman-
tic content units” that are crucial details of the
text that may have a variety of syntactic forms
(Nenkova and Passonneau, 2004). We further hy-
pothesize these can be found in chunks of the
text automatically identified by a chunker, and we
can represent these units as predicate-arguments in
our event structure. The event structure of each
story is automatically extracted using an XML-
based pipeline composed of NLP processing mod-
ules, and unlike other story systems, extract full
events instead of filling in a frame of a story script
(Riloff, 1999). Using the latest version of the
Language Technology Text Tokenization Toolkit
(Grover et al., 2000), words are tokenized and sen-
tence boundaries detected. Words are given part-
of-speech tags by a maximum entropy tagger from
the toolkit. We do not attempt to obtain a full parse
of the sentence due to the highly irregular nature
of the sentences. Pronouns are resolved using a
rule-based reimplementation of the CogNIAC al-
gorithm (Baldwin, 1997) and sentences are lem-
matized and chunked using the Cass Chunker (Ab-
ney, 1995). It was felt the chunking method would
be the only feasible way to retrieve portions of the
sentences that may contain complete “semantic
content units” from the ungrammatical and irregu-
lar text. The application of a series of rules, mainly
mapping verbs to predicate names and nouns to
arguments, to the results of the chunker produces
events from chunks as described in our previous
work (McNeill et al., 2006). The accuracy of our
rule-set was developed by using the grammatical
exemplar stories as a testbed, and a blind judge
found they produced 68% interpretable or “sen-
sible” events given the ungrammatical text. Stu-
dents usually use the present or past tense exclu-
sively throughout the story and events are usually
presented in order of occurrence. An inspection
of our corpus showed 3% of stories in our corpus
seemed to get the order of events wrong (Hick-
mann, 2003).
</bodyText>
<subsectionHeader confidence="0.999471">
4.1 Comparing Stories
</subsectionHeader>
<bodyText confidence="0.99996">
Since the student is rewriting the story using their
own words, a certain variance from the plot of the
exemplar story should be expected and even re-
warded. Extra statements that may be true, but
are not explicitly stated in the story, can be in-
ferred by the students. Statements that are true
but are not highly relevant to the course of the
</bodyText>
<page confidence="0.993538">
859
</page>
<bodyText confidence="0.999982405405406">
plot can likewise be left out. Word similarity
must be taken into account, so that “The king is
protecting his gold” can be recognized as “The
pharaoh guarded the treasure.” Characters change
in context, as one character that is described as
the “younger brother” is from the viewpoint of his
mother “the younger son.” So, building a model
from the events of two stories and simply check-
ing equivalence can not be used for comparison,
since a wide variety of partial equivalence must be
taken into account.
Instead of using absolute measures of equiva-
lence based on model checking or measures based
on word distribution, we compare each story on
the basis of the presence or absence of events. This
approach takes advantage of WordNet to define
synonym matching and uses the relational struc-
ture of the events to allow partial matching of
predicate functions and arguments. The events
of the exemplar story are assumed to be correct,
and they are searched for in the rewritten story in
the order in which they occur in the exemplar. If
an event is matched (including using WordNet),
then in turn each of the arguments attempts to be
matched.
This algorithm is given more formally in Fig-
ure 1. The complete event structure from the ex-
emplar story, E, and the complete event structure
from the rewritten story R, with each individual
event predicate name labelled as e and r respec-
tively, and their arguments labelled as n in either
Ne and Nr. SYN(x) is the synset of the term x,
including hypernyms and hyponyms except upper
ontology ones. The results of the algorithm are
stored in binary vector F with index i. 1 denotes
an exact match or WordNet synset match, and 0 a
failure to find any match.
</bodyText>
<sectionHeader confidence="0.70502" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999906916666667">
As a baseline system LSA produces a similar-
ity score for each rewritten story by comparing it
to the exemplar, this score is used as a distance
metric for a k-Nearest Neighbor classifier (Deer-
wester et al., 1990). The parameters for LSA were
empirically determined to be a dimensionality of
200 over the semantic space given by the rec-
ommended reading list for American 6th graders
(Landauer and Dumais, 1997). These parameters
resulted in the LSA similarity score having a Pear-
son’s correlation of -.520 with Rater A. k was
found to be optimal at 9.
</bodyText>
<table confidence="0.84941085">
Algorithm 4.1: PLOTCOMPARE(E, R)
i 0
f ;
fore E E
do for r E R
if e = SYN(r)
then fz 1
else fz 0
for ne E Ne
for nr E Nr
if ne = SYN(nr)
then fz 1
else fz 0
i = i + 1
Figure 1: Plot Comparison Algorithm
Classifier Corpus Features % Correct
k-NN Adventure LSA 47.5
Naive Bayes Adventure PLOT 55.6
k-NN Thief LSA 41.2
Naive Bayes Thief PLOT 45.4
</table>
<tableCaption confidence="0.99762">
Table 2: Machine-Learning Results
</tableCaption>
<bodyText confidence="0.999988791666667">
The results of the plot comparison algorithm
were given as features to machine-learners, with
results produced using ten-fold cross-validation.
A Naive Bayes learner discovers the different sta-
tistical distributions of events for each rating. The
results for both the “Adventure” and “Thief” sto-
ries are displayed in Table 2. “PLOT” means the
results of the Plot Comparison Algorithm were
used as features for the machine-learner while
”LSA” means the similarity scores for Latent Se-
mantic Analysis were used instead. Note that the
same machine-learner could not be used to judge
the effect of LSA and PLOT since LSA scores are
real numbers and PLOT a set of features encoded
as binary vectors.
The results do not seem remarkable at first
glance. However, recall that the human raters had
an average of 56% agreement on story ratings, and
in that light the Naive Bayes learner approaches
the performance of human raters. Surprisingly,
when the LSA score is used as a feature in addition
to the results of the plot comparison algorithm for
the Naive Bayes learners, there is no further im-
provement. This shows features given by the event
</bodyText>
<figure confidence="0.936647333333333">
do {
do I
do I
</figure>
<page confidence="0.946124">
860
</page>
<table confidence="0.9994536">
Class 1 2 3 4
1 (Excellent) 14 22 0 1
2 (Good) 5 36 0 7
3 (Fair) 3 20 0 2
4 (Poor) 0 11 0 39
</table>
<tableCaption confidence="0.946568">
Table 3: Naive Bayes Confusion Matrix: “Ad-
venture”
</tableCaption>
<table confidence="0.9999134">
Class Precision Recall
Excellent .64 .38
Good .40 .75
Fair .00 .00
Poor .80 .78
</table>
<tableCaption confidence="0.999731">
Table 4: Naive Bayes Results: “Adventure”
</tableCaption>
<bodyText confidence="0.998778">
structure better characterize plot structure than the
word distribution. Unlike previous work, the use
of both the plot comparison results and LSA did
not improve performance for Naive Bayes, so the
results of using Naive Bayes with both are not re-
ported (Halpin et al., 2004).
The results for the “Adventure” corpus are in
general better than the results for the “Thief” cor-
pus. However, this is due to the “Thief” corpus
being smaller and having an infrequent number of
“Excellent” and “Poor” stories, as shown in Table
1. In the “Thief” corpus the learner simply col-
lapses most stories into “Good,” resulting in very
poor performance. Another factor may be that the
“Thief” story was more complex than the “Adven-
ture” story, featuring 9 characters over 5 scenes, as
opposed to the “Adventure” corpus that featured 4
characters over 2 scenes.
For the “Adventure” corpus, the Naive Bayes
classifier produces the best results, as detailed in
Table 4 and the confusion matrix in Figure 3. A
close inspection of the results shows that in the
“Adventure Corpus” the “Poor” and “Good” sto-
ries are classified in general fairly well by the
Naive Bayes learner, while some of the “Excel-
lent” stories are classified as correctly. A signifi-
cant number of both “Excellent” and most “Fair”
stories are classified as “Good.” The “Fair” cate-
gory, due to its small size in the training corpus,
has disappeared. No “Poor” stories are classified
as “Excellent,” and no “Excellent” stories are clas-
sified as “Poor.” The increased difficulty in distin-
guishing “Excellent” stories from “Good” stories
is likely due to the use of inference by “Excellent”
stories, which our system does not use. An inspec-
tion of the rating scale’s wording reveals the sim-
ilarity in wording between the “Fair” and “Good”
ratings. This may explain the lack of “Fair” sto-
ries in the corpus and therefore the inability of
machine-learners to recognize them. As given by
a survey of five teachers experienced in using the
story rewriting task in schools, this level of perfor-
mance is not ideal but acceptable to teachers.
Our technique is also shown to be easily
portable over different domains where a teacher
can annotate around one hundred sample stories
using our scale, although performance seems to
suffer the more complex a story is. Since the Naive
Bayes classifier is fast (able to classify stories in
only a few seconds) and the entire algorithm from
training to advice generation (as detailed below)
is fully automatic once a small training corpus has
been produced, this technique can be used in real-
life tutoring systems and easily ported to other sto-
ries.
</bodyText>
<sectionHeader confidence="0.994219" genericHeader="method">
5 Automated Advice
</sectionHeader>
<bodyText confidence="0.999827545454545">
The plot analysis agent is not meant to give the
students grades for their stories, but instead use
the automatic ratings as an intermediate step to
produce advice, like other hybrid tutoring systems
(Rose et al., 2002). The advice that the agent can
generate from the automatic rating classification
is limited to coarse-grained general advice. How-
ever, by inspecting the results of the plot com-
parison algorithm, our agent is capable of giving
detailed fine-grained specific advice from the re-
lationships of the events in the story. One tutor-
ing system resembling ours is the WRITE sys-
tem, but we differ from it by using event struc-
ture to represent the information in the system,
instead of using rhetorical features (Burstein et
al., 2003). In this regards it more closely resem-
bles the physics tutoring system WHY-ATLAS, al-
though we deal with narrative stories of a longer
length than physics essays. The WHY-ATLAS
physics tutor identifies missing information in the
explanations of students using theorem-proving
(Rose et al., 2002).
</bodyText>
<subsectionHeader confidence="0.997748">
5.1 Advice Generation Algorithm
</subsectionHeader>
<bodyText confidence="0.99989325">
Different types of stories need different amounts
of advice. An “Excellent” story needs less ad-
vice than a “Good” story. One advice statement is
“general,” while the rest are specific. The system
</bodyText>
<page confidence="0.99552">
861
</page>
<bodyText confidence="0.999975862745098">
produces a total of seven advice statements for a
“Poor” story, and two less statements for each rat-
ing level above “Poor.”
With the aid of a teacher, a number of “canned”
text statements offering general advice were cre-
ated for each rating class. These include state-
ments such as “It’s very good! I only have a few
pointers“ for a “Good” story and “Let’s get help
from the teacher” for “Poor” story. The advice
generation begins by randomly selecting a state-
ment suitable for the rating of the story. Those
students whose stories are rated “Poor” are asked
if they would like to re-read the story and ask a
teacher for help.
The generation of specific advice uses the re-
sults of the plot-comparison algorithm to produce
specific advice. A number of advice templates
were produced, and the results of the Advice Gen-
eration Algorithm fill in the needed values of the
template. The 0 most frequent events in “Excel-
lent” stories are called the Important Event Struc-
ture, which represents the “important” events in
the story in temporal order. Empirical experiments
led us 0 = 10 for the “Adventure” story, but for
longer stories like the “Thief” story a larger 0
would be appropriate. These events correspond to
the ones given the highest weights by the Naive
Bayes algorithm. For each event in the event struc-
ture of a rewritten story, a search for a match in
the important event structure is taken. If a pred-
icate name match is found in the important event
structure, the search continues to attempt to match
the arguments. If the event and the arguments do
not match, advice is generated using the structure
of the “important” event that it cannot find in the
rewritten story.
This advice may use both the predicate name
and its arguments, such as “Did the stork fly?”
from fly(stork). If an argument is missing, the ad-
vice may be about only the argument(s), like “Can
you tell me more about the stork?” If the event is
out of order, advice is given to the student to cor-
rect the order, as in “I think something with the
stork happened earlier in the story.”
This algorithm is formalized in Figure 2, with
all variables being the same as in the Plot Anal-
ysis Algorithm, except that W is the Important
Event Structure composed of events w with the
set of arguments N,,,. M is a binary vector used
to store the success of a match with index i. The
ADV function, given an event, generates one ad-
</bodyText>
<figure confidence="0.969402235294118">
Algorithm
5.1: ADVICEGENERATE(W, R)
for w E W
M = 0
i = 0
for r E R
if w = r or SYN(r)
then mi = 1
else mi = 0
i = i+ 1
for n,,, E N,,,
for n, E N,
if n,,, = SYN(n,) or n,
then mi 1
else mi 0
i = i + 1
ADV (w, M)
</figure>
<figureCaption confidence="0.999304">
Figure 2: Advice Generation Algorithm
</figureCaption>
<bodyText confidence="0.999673923076923">
vice statement to be given to the student.
An element of randomization was used to gen-
erate a diversity of types of answers. An ad-
vice generation function (ADV ) takes an impor-
tant event (w) and its binary matching vector (M)
and generates an advice statement for w. Per im-
portant event this advice generation function is pa-
rameterized so that it has a 10% chance of deliver-
ing advice based on the entire event, 20% chance
of producing advice that dealt with temporal or-
der (these being parameters being found ideal af-
ter testing the algorithm), and otherwise produces
advice based on the arguments.
</bodyText>
<subsectionHeader confidence="0.998536">
5.2 Advice Evaluation
</subsectionHeader>
<bodyText confidence="0.9995182">
The plot advice algorithm is run using a randomly
selected corpus of 20 stories, 5 from each plot rat-
ing level using the “Adventure Corpus.” This pro-
duced matching advice for each story, for a total
of 80 advice statements.
</bodyText>
<subsectionHeader confidence="0.994317">
5.3 Advice Rating
</subsectionHeader>
<bodyText confidence="0.995735">
An advice rating scheme was developed to rate the
advice produced in consultation with a teacher.
</bodyText>
<listItem confidence="0.99257175">
1. Excellent: The advice was suitable for the
story, and helped the student gain insight into
the story.
2. Good: The advice was suitable for the story,
</listItem>
<figure confidence="0.998247727272727">
do �
������������������������� �
��������������������������
do �
���������������� �
�����������������
do I
�
��� �
����
do
</figure>
<page confidence="0.976607">
862
</page>
<table confidence="0.982848">
Rating % Given
Excellent 0
Good 35
Fair 60
Poor 5
</table>
<tableCaption confidence="0.73706">
Table 5: Advice Rating Results
and would help the student.
</tableCaption>
<listItem confidence="0.8829875">
3. Fair: The advice was suitable, but should
have been phrased differently.
4. Poor: The advice really didn’t make sense
and would only confuse the student further.
</listItem>
<bodyText confidence="0.999995633333333">
Before testing the system on students, it was de-
cided to have teachers evaluate how well the ad-
vice given by the system corresponded to the ad-
vice they would give in response to a story. A
teacher read each story and the advice. They then
rated the advice using the advice rating scheme.
Each story was rated for its overall advice quality,
and then each advice statement was given com-
ments by the teacher, such that we could derive
how each individual piece of advice contributed
to the global rating. Some of the general “coarse-
grained” advice was “Good! You got all the main
parts of the story” for an “Excellent” story, “Let’s
make it even better!” for a “Good” story, and
“Reading the story again with a teacher would be
help!” for a “Poor” story. Sometimes the ad-
vice generation algorithm was remarkably accu-
rate. In one story the connection between a curse
being lifted by the possession of a coin by the
character Nils was left out by a student. The ad-
vice generation algorithm produced the following
useful advice statement: “Tell me more about the
curse and Nils.” Occasionally an automatically ex-
tracted event that is difficult to interpret by a hu-
man or simply incorrectly is extracted. This in turn
can cause advice that does not make any sense
can be produced, such as “Tell me more about a
spot?”. Qualitative analysis showed that “missing
important advice” to be the most significant prob-
lem, followed by “nonsensical advice.”
</bodyText>
<sectionHeader confidence="0.851371" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.999953130434783">
The results are given in Table 5. The majority of
the advice was rated overall as “fair.” Only one
story was given “poor” advice, and a few were
given “good” advice. However, most advice rated
as “good” was the advice generated by “excel-
lent” stories, which generate less advice than other
types of stories. “Poor” stories were given almost
entirely “fair” advice, although once “poor” ad-
vice was generated. In general, the teacher found
“coarse-grained” advice to be very useful, and was
very pleased that the agent could detect when the
student needed to re-read the story and when a stu-
dent did not need to write any more. In some cases
the specific advice was shown to help provide a
“crucial detail” and help “elicit a fact.” The advice
was often “repetitive” and ”badly phrased.” The
specific advice came under criticism for often not
“being directed enough” and for being “too literal”
and not “inferential enough.” The rater noticed
that “The program can not differentiate between
an unfinished story...and one that is confused.” and
that “Some why, where and how questions could
be used” in the advice.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999986033333334">
Since the task involved a fine-grained analysis of
the rewritten story, the use of events that take plot
structure into account made sense regardless of
its performance. The use of events as structured
features in a machine-learning classifier outper-
formed a classifier that relied on a unstructured
“bag-of-words” as features. The system achieved
close to human performance on rating the stories.
Since each of the events used as a feature in the
machine-learner corresponds to a particular event
in the story, the features are easily interpretable by
other components in the system and interpretable
by humans. This allows these events to be used
in a template-driven system to generate advice for
students based on the structure of their plot.
Extracting events from text is fraught with er-
ror, particularly in the ungrammatical and infor-
mal domain used in this experiment. This is often
a failure of our system to detect semantic content
units through either not including them in chunks
or only partially including a single unit in a chunk.
Chunking also has difficulty dealing with preposi-
tions, embedded speech, semantic role labels, and
complex sentences correctly. Improvement in our
ability to retrieve semantics would help both story
classification and advice generation.
Advice generation was impaired by the abil-
ity to produce directed questions from the events
using templates. This is because while our sys-
tem could detect important events and their or-
</bodyText>
<page confidence="0.99576">
863
</page>
<bodyText confidence="0.999935928571429">
der, it could not make explicit their connection
through inference. Given the lack of a large-scale
open-source accessible “common-sense” knowl-
edge base and the difficulty in extracting infer-
ential statements from raw text, further progress
using inference will be difficult. Progress in ei-
ther making it easier for a teacher to make explicit
the important inferences in the text or improved
methodology to learn inferential knowledge from
the text would allow further progress. Tantaliz-
ingly, this ability for a reader to use “inference to
grasp points that may not have been explicit in the
story” is given as the hallmark of truly understand-
ing a story by teachers.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875588888889">
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax. In
Jennifer Cole, Georgia Green, and Jerry Morgan,
editors, Computational Linguistics and the Founda-
tions ofLinguistic Theory, pages 145–164.
Breck Baldwin. 1997. CogNIAC : A High Precision
Pronoun Resolution Engine.
F.C. Bartlett. 1932. Remembering. Cambridge Uni-
versity Press, Cambridge.
Johan Bos, Stephen Clark, Mark Steedman, James Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING ’04). Geneva,
Switzerland.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE Stuff: Automatic Identification
of Discourse Structure in Student Essays. IEEE In-
telligent Systems, pages 32–39.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
For Information Science, (41):391–407.
Christine Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings,
D. Harter, and N. Person. 2000. Using latent se-
mantic analysis to evaluate the contributions of stu-
dents in autotutor. Interactive Learning Environ-
ments, 8:149–169.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT - A Flexible Tokenisa-
tion Tool. In Proceedings of the Second Language
Resources and Evaluation Conference.
Harry Halpin, Johanna Moore, and Judy Robertson.
2004. Automatic analysis ofplot for story rewriting.
In In Proceedings of Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Maya Hickmann. 2003. Children’s Discourse: per-
son, space and time across language. Cambridge
University Press, Cambridge, UK.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic.
Thomas. Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and repre-
sentation of knowledge. Psychological Review.
B. Lemaire, S. Mandin, P. Dessus, and G. Denhire.
2005. Computational cognitive models of summa-
rization assessment skills. In In Proceedings of the
27th Annual Meeting of the Cognitive Science Soci-
ety, Stressa, Italy.
Fiona McNeill, Harry Halpin, Ewan Klein, and Alan
Bundy. 2006. Merging stories with shallow seman-
tics. In Proceedings of the Knowledge Representa-
tion and Reasoning for Language Processing Work-
shop at the European Association for Computational
Linguistics, Genoa, Italy.
Erik T. Mueller. 2003. Story understanding through
multi-representation model construction. In Graeme
Hirst and Sergei Nirenburg, editors, Text Meaning:
Proceedings of the HLT-NAACL 2003 Workshop,
pages 46–53, East Stroudsburg, PA. Association for
Computational Linguistics.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In In Proceedings of the Joint Con-
ference of the North American Association for Com-
putational Linguistics and Human Language Tech-
nologies. Boston, USA.
E. Riloff. 1999. Information extraction as a step-
ping stone toward story understanding. In Ash-
win Ram and Kenneth Moorman, editors, Computa-
tional Models of Reading and Understanding. MIT
Press.
Judy Robertson and Beth Cross. 2003. Children’s
perceptions about writing with their teacher and the
StoryStation learning environment. Narrative and
Interactive Learning Environments: Special Issue
of International Journal of Continuing Engineering
Education and Life-long Learning.
Judy Robertson and Peter Wiemer-Hastings. 2002.
Feedback on children’s stories via multiple interface
agents. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. VanLehn. 2002. A hybrid language
understanding approach for robust selection of tutor-
ing goals. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
</reference>
<page confidence="0.998637">
864
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425494">
<title confidence="0.999191">Event Extraction in a Plot Advice Agent</title>
<author confidence="0.999921">Harry Halpin</author>
<affiliation confidence="0.903741666666667">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.970773">Edinburgh, EH8 9LW Scotland, UK</address>
<email confidence="0.984998">H.Halpin@ed.ac.uk</email>
<author confidence="0.998718">Johanna D Moore</author>
<affiliation confidence="0.903694666666667">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.9719255">Edinburgh, EH8 9LW Scotland, UK</address>
<email confidence="0.99709">J.Moore@ed.ac.uk</email>
<abstract confidence="0.99872">In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing. We focus on the story rewriting task, in which an exemplar story is read to the students and the students rewrite the story in their own words. The system automatically extracts events from the raw text, formalized as a sequence of temporally ordered predicate-arguments. These events are given to a machine-learner that produces a coarse-grained rating of the story. The results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Chunks and dependencies: Bringing processing evidence to bear on syntax.</title>
<date>1995</date>
<booktitle>Computational Linguistics and the Foundations ofLinguistic Theory,</booktitle>
<pages>145--164</pages>
<editor>In Jennifer Cole, Georgia Green, and Jerry Morgan, editors,</editor>
<contexts>
<context position="11008" citStr="Abney, 1995" startWordPosition="1844" endWordPosition="1846">tems, extract full events instead of filling in a frame of a story script (Riloff, 1999). Using the latest version of the Language Technology Text Tokenization Toolkit (Grover et al., 2000), words are tokenized and sentence boundaries detected. Words are given partof-speech tags by a maximum entropy tagger from the toolkit. We do not attempt to obtain a full parse of the sentence due to the highly irregular nature of the sentences. Pronouns are resolved using a rule-based reimplementation of the CogNIAC algorithm (Baldwin, 1997) and sentences are lemmatized and chunked using the Cass Chunker (Abney, 1995). It was felt the chunking method would be the only feasible way to retrieve portions of the sentences that may contain complete “semantic content units” from the ungrammatical and irregular text. The application of a series of rules, mainly mapping verbs to predicate names and nouns to arguments, to the results of the chunker produces events from chunks as described in our previous work (McNeill et al., 2006). The accuracy of our rule-set was developed by using the grammatical exemplar stories as a testbed, and a blind judge found they produced 68% interpretable or “sensible” events given the</context>
</contexts>
<marker>Abney, 1995</marker>
<rawString>Steven Abney. 1995. Chunks and dependencies: Bringing processing evidence to bear on syntax. In Jennifer Cole, Georgia Green, and Jerry Morgan, editors, Computational Linguistics and the Foundations ofLinguistic Theory, pages 145–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
</authors>
<title>CogNIAC : A High Precision Pronoun Resolution Engine.</title>
<date>1997</date>
<contexts>
<context position="10930" citStr="Baldwin, 1997" startWordPosition="1831" endWordPosition="1832">XMLbased pipeline composed of NLP processing modules, and unlike other story systems, extract full events instead of filling in a frame of a story script (Riloff, 1999). Using the latest version of the Language Technology Text Tokenization Toolkit (Grover et al., 2000), words are tokenized and sentence boundaries detected. Words are given partof-speech tags by a maximum entropy tagger from the toolkit. We do not attempt to obtain a full parse of the sentence due to the highly irregular nature of the sentences. Pronouns are resolved using a rule-based reimplementation of the CogNIAC algorithm (Baldwin, 1997) and sentences are lemmatized and chunked using the Cass Chunker (Abney, 1995). It was felt the chunking method would be the only feasible way to retrieve portions of the sentences that may contain complete “semantic content units” from the ungrammatical and irregular text. The application of a series of rules, mainly mapping verbs to predicate names and nouns to arguments, to the results of the chunker produces events from chunks as described in our previous work (McNeill et al., 2006). The accuracy of our rule-set was developed by using the grammatical exemplar stories as a testbed, and a bl</context>
</contexts>
<marker>Baldwin, 1997</marker>
<rawString>Breck Baldwin. 1997. CogNIAC : A High Precision Pronoun Resolution Engine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C Bartlett</author>
</authors>
<title>Remembering.</title>
<date>1932</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2349" citStr="Bartlett, 1932" startWordPosition="382" endWordPosition="383">yindependent automated agent that can both analyze the plot of a story and generate appropriate advice. 1.1 The Story Rewriting Task A task used in schools is the story rewriting task, where a story, the exemplar story, is read to the students, and afterwards the story is rewritten by each student, providing a corpus of rewritten stories. This task tests the students ability to both listen and write, while removing from the student the cognitive load needed to generate a new plot. This task is reminiscent of the well-known “War of the Ghosts” experiment used in psychology for studying memory (Bartlett, 1932) and related to work in fields such as summarization (Lemaire et al., 2005) and narration (Halpin et al., 2004). 1.2 Agent Design The goal of the agent is to classify each of the rewritten stories for overall plot quality. This rating can be used to give “coarse-grained” general advice. The agent should then provide “finegrained” specific advice to the student on how their plot could be improved. The agent should be able to detect if the story should be re-read or a human teacher summoned to help the student. To accomplish this task, we extract events that represent the entities and their acti</context>
</contexts>
<marker>Bartlett, 1932</marker>
<rawString>F.C. Bartlett. 1932. Remembering. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING ’04).</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="6861" citStr="Bos et al., 2004" startWordPosition="1170" endWordPosition="1173">c of .74. Between Rater B and C there was a Cronbach’s α statistic of .87 and Kendall’s &apos;rb statistic of .67. These statistics show the rating scheme to be reliable and the distribution of plot ratings are given in Table 1. 2.2 Linguistic Issues One challenge facing this task is the ungrammatical and highly irregular text produced by the students. Many stories consist of one long run-on sentence. This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al., 2004). The stories exhibit frequent use of reported speech and the switching from first-person to third-person within a single sentence. Lastly, the use of incorrect spelling e.g., “stalk” for “stork” appearing in multiple stories in the corpus, the consistent usage of homonyms such as “there” for “their,” and the invention of words (“torlix”), all prove to be frequent. 3 Plot Analysis To automatically rate student writing many tutoring systems use Latent Semantic Analysis, a variation on the “bag-of-words” technique that uses dimensionality reduction (Graesser et al., 2000). We hypothesize that be</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In In Proceedings of the 20th International Conference on Computational Linguistics (COLING ’04). Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<date>2003</date>
<booktitle>Finding the WRITE Stuff: Automatic Identification of Discourse Structure in Student Essays. IEEE Intelligent Systems,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="19706" citStr="Burstein et al., 2003" startWordPosition="3356" endWordPosition="3359"> as an intermediate step to produce advice, like other hybrid tutoring systems (Rose et al., 2002). The advice that the agent can generate from the automatic rating classification is limited to coarse-grained general advice. However, by inspecting the results of the plot comparison algorithm, our agent is capable of giving detailed fine-grained specific advice from the relationships of the events in the story. One tutoring system resembling ours is the WRITE system, but we differ from it by using event structure to represent the information in the system, instead of using rhetorical features (Burstein et al., 2003). In this regards it more closely resembles the physics tutoring system WHY-ATLAS, although we deal with narrative stories of a longer length than physics essays. The WHY-ATLAS physics tutor identifies missing information in the explanations of students using theorem-proving (Rose et al., 2002). 5.1 Advice Generation Algorithm Different types of stories need different amounts of advice. An “Excellent” story needs less advice than a “Good” story. One advice statement is “general,” while the rest are specific. The system 861 produces a total of seven advice statements for a “Poor” story, and two</context>
</contexts>
<marker>Burstein, Marcu, Knight, 2003</marker>
<rawString>Jill Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE Stuff: Automatic Identification of Discourse Structure in Student Essays. IEEE Intelligent Systems, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society For Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="14177" citStr="Deerwester et al., 1990" startWordPosition="2394" endWordPosition="2398">tten story R, with each individual event predicate name labelled as e and r respectively, and their arguments labelled as n in either Ne and Nr. SYN(x) is the synset of the term x, including hypernyms and hyponyms except upper ontology ones. The results of the algorithm are stored in binary vector F with index i. 1 denotes an exact match or WordNet synset match, and 0 a failure to find any match. 4.2 Results As a baseline system LSA produces a similarity score for each rewritten story by comparing it to the exemplar, this score is used as a distance metric for a k-Nearest Neighbor classifier (Deerwester et al., 1990). The parameters for LSA were empirically determined to be a dimensionality of 200 over the semantic space given by the recommended reading list for American 6th graders (Landauer and Dumais, 1997). These parameters resulted in the LSA similarity score having a Pearson’s correlation of -.520 with Rater A. k was found to be optimal at 9. Algorithm 4.1: PLOTCOMPARE(E, R) i 0 f ; fore E E do for r E R if e = SYN(r) then fz 1 else fz 0 for ne E Ne for nr E Nr if ne = SYN(nr) then fz 1 else fz 0 i = i + 1 Figure 1: Plot Comparison Algorithm Classifier Corpus Features % Correct k-NN Adventure LSA 47</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society For Information Science, (41):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9767" citStr="Fellbaum, 1998" startWordPosition="1639" endWordPosition="1640">An example from the “Thief” exemplar story is “The Queen nagged the king to build a treasure chamber. The king decided to have a treasure chamber.” This can be represented by an event structure as: nag(king, queen) build(chamber) decide(king) have(chamber) Note due the ungrammatical corpus we cannot at this time extract neo-Davidsonian events. A sentence maps onto one, multiple, or no events. A unique name and closed-world assumption is enforced, although for purposes of comparing event we compare membership of argument and predicate names in WordNet synsets in addition to exact name matches (Fellbaum, 1998). 4 Extracting Events Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of “semantic content units” that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). We further hypothesize these can be found in chunks of the text automatically identified by a chunker, and we can represent these units as predicate-arguments in our event structure. The event structure of each story is automatically extracted using an XMLbased pipeline composed of NLP processing module</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
<author>P Wiemer-Hastings</author>
<author>K Wiemer-Hastings</author>
<author>D Harter</author>
<author>N Person</author>
</authors>
<title>Using latent semantic analysis to evaluate the contributions of students in autotutor. Interactive Learning Environments,</title>
<date>2000</date>
<pages>8--149</pages>
<contexts>
<context position="7437" citStr="Graesser et al., 2000" startWordPosition="1259" endWordPosition="1262">d as such could not be used (Bos et al., 2004). The stories exhibit frequent use of reported speech and the switching from first-person to third-person within a single sentence. Lastly, the use of incorrect spelling e.g., “stalk” for “stork” appearing in multiple stories in the corpus, the consistent usage of homonyms such as “there” for “their,” and the invention of words (“torlix”), all prove to be frequent. 3 Plot Analysis To automatically rate student writing many tutoring systems use Latent Semantic Analysis, a variation on the “bag-of-words” technique that uses dimensionality reduction (Graesser et al., 2000). We hypothesize that better results can be achieved using a “representational” account that explicitly represents each event in the plot. These semantic relationships are important in stories, e.g., “The thief jumped on the donkey” being distinctly different from “The donkey jumped on the thief.” What characters participate in an action matter, since “The king stole the treasure” reveals a major 858 misunderstanding while “The thief stole the treasure” shows a correct interpretation by the student. 3.1 Stories as Events We represent a story as a sequence of events, p1...ph, represented as a l</context>
</contexts>
<marker>Graesser, Wiemer-Hastings, Wiemer-Hastings, Harter, Person, 2000</marker>
<rawString>A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, D. Harter, and N. Person. 2000. Using latent semantic analysis to evaluate the contributions of students in autotutor. Interactive Learning Environments, 8:149–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Colin Matheson</author>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
</authors>
<title>LT TTT - A Flexible Tokenisation Tool.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="10585" citStr="Grover et al., 2000" startWordPosition="1772" endWordPosition="1775">al details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). We further hypothesize these can be found in chunks of the text automatically identified by a chunker, and we can represent these units as predicate-arguments in our event structure. The event structure of each story is automatically extracted using an XMLbased pipeline composed of NLP processing modules, and unlike other story systems, extract full events instead of filling in a frame of a story script (Riloff, 1999). Using the latest version of the Language Technology Text Tokenization Toolkit (Grover et al., 2000), words are tokenized and sentence boundaries detected. Words are given partof-speech tags by a maximum entropy tagger from the toolkit. We do not attempt to obtain a full parse of the sentence due to the highly irregular nature of the sentences. Pronouns are resolved using a rule-based reimplementation of the CogNIAC algorithm (Baldwin, 1997) and sentences are lemmatized and chunked using the Cass Chunker (Abney, 1995). It was felt the chunking method would be the only feasible way to retrieve portions of the sentences that may contain complete “semantic content units” from the ungrammatical </context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. LT TTT - A Flexible Tokenisation Tool. In Proceedings of the Second Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Halpin</author>
<author>Johanna Moore</author>
<author>Judy Robertson</author>
</authors>
<title>Automatic analysis ofplot for story rewriting. In</title>
<date>2004</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2460" citStr="Halpin et al., 2004" startWordPosition="399" endWordPosition="402">1 The Story Rewriting Task A task used in schools is the story rewriting task, where a story, the exemplar story, is read to the students, and afterwards the story is rewritten by each student, providing a corpus of rewritten stories. This task tests the students ability to both listen and write, while removing from the student the cognitive load needed to generate a new plot. This task is reminiscent of the well-known “War of the Ghosts” experiment used in psychology for studying memory (Bartlett, 1932) and related to work in fields such as summarization (Lemaire et al., 2005) and narration (Halpin et al., 2004). 1.2 Agent Design The goal of the agent is to classify each of the rewritten stories for overall plot quality. This rating can be used to give “coarse-grained” general advice. The agent should then provide “finegrained” specific advice to the student on how their plot could be improved. The agent should be able to detect if the story should be re-read or a human teacher summoned to help the student. To accomplish this task, we extract events that represent the entities and their actions in the plot from both the exemplar and the rewritten stories. A plot comparison algorithm checks for the pr</context>
<context position="16592" citStr="Halpin et al., 2004" startWordPosition="2833" endWordPosition="2836">urther improvement. This shows features given by the event do { do I do I 860 Class 1 2 3 4 1 (Excellent) 14 22 0 1 2 (Good) 5 36 0 7 3 (Fair) 3 20 0 2 4 (Poor) 0 11 0 39 Table 3: Naive Bayes Confusion Matrix: “Adventure” Class Precision Recall Excellent .64 .38 Good .40 .75 Fair .00 .00 Poor .80 .78 Table 4: Naive Bayes Results: “Adventure” structure better characterize plot structure than the word distribution. Unlike previous work, the use of both the plot comparison results and LSA did not improve performance for Naive Bayes, so the results of using Naive Bayes with both are not reported (Halpin et al., 2004). The results for the “Adventure” corpus are in general better than the results for the “Thief” corpus. However, this is due to the “Thief” corpus being smaller and having an infrequent number of “Excellent” and “Poor” stories, as shown in Table 1. In the “Thief” corpus the learner simply collapses most stories into “Good,” resulting in very poor performance. Another factor may be that the “Thief” story was more complex than the “Adventure” story, featuring 9 characters over 5 scenes, as opposed to the “Adventure” corpus that featured 4 characters over 2 scenes. For the “Adventure” corpus, the</context>
</contexts>
<marker>Halpin, Moore, Robertson, 2004</marker>
<rawString>Harry Halpin, Johanna Moore, and Judy Robertson. 2004. Automatic analysis ofplot for story rewriting. In In Proceedings of Empirical Methods in Natural Language Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maya Hickmann</author>
</authors>
<title>Children’s Discourse: person, space and time across language.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="11885" citStr="Hickmann, 2003" startWordPosition="1992" endWordPosition="1994">dicate names and nouns to arguments, to the results of the chunker produces events from chunks as described in our previous work (McNeill et al., 2006). The accuracy of our rule-set was developed by using the grammatical exemplar stories as a testbed, and a blind judge found they produced 68% interpretable or “sensible” events given the ungrammatical text. Students usually use the present or past tense exclusively throughout the story and events are usually presented in order of occurrence. An inspection of our corpus showed 3% of stories in our corpus seemed to get the order of events wrong (Hickmann, 2003). 4.1 Comparing Stories Since the student is rewriting the story using their own words, a certain variance from the plot of the exemplar story should be expected and even rewarded. Extra statements that may be true, but are not explicitly stated in the story, can be inferred by the students. Statements that are true but are not highly relevant to the course of the 859 plot can likewise be left out. Word similarity must be taken into account, so that “The king is protecting his gold” can be recognized as “The pharaoh guarded the treasure.” Characters change in context, as one character that is </context>
</contexts>
<marker>Hickmann, 2003</marker>
<rawString>Maya Hickmann. 2003. Children’s Discourse: person, space and time across language. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic.</title>
<date>1993</date>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="8428" citStr="Kamp and Reyle, 1993" startWordPosition="1413" endWordPosition="1416">the treasure” reveals a major 858 misunderstanding while “The thief stole the treasure” shows a correct interpretation by the student. 3.1 Stories as Events We represent a story as a sequence of events, p1...ph, represented as a list of predicatearguments, similar to the event calculus (Mueller, 2003). Our predicate-argument structure is a minimal subset of first-order logic (no quantifiers), and so is compatible with case-frame and dependency representations. Every event has a predicate (function) p that has one or more arguments, n1...na. In the tradition of Discourse Representation Theory (Kamp and Reyle, 1993), our current predicate argument structure could be converted automatically to first order logic by using a default existential quantification over the predicates and joining them conjunctively. Predicate names are often verbs, while their arguments are usually, although not exclusively, nouns or adjectives. When describing a set of events in the story, a superscript is used to keep the arguments in an event distinct, as n25 is argument 2 in event 5. The same argument name may appear in multiple events. The plot of any given story is formalized as an event structure composed of h events in a p</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review.</journal>
<contexts>
<context position="14374" citStr="Landauer and Dumais, 1997" startWordPosition="2427" endWordPosition="2430">rnyms and hyponyms except upper ontology ones. The results of the algorithm are stored in binary vector F with index i. 1 denotes an exact match or WordNet synset match, and 0 a failure to find any match. 4.2 Results As a baseline system LSA produces a similarity score for each rewritten story by comparing it to the exemplar, this score is used as a distance metric for a k-Nearest Neighbor classifier (Deerwester et al., 1990). The parameters for LSA were empirically determined to be a dimensionality of 200 over the semantic space given by the recommended reading list for American 6th graders (Landauer and Dumais, 1997). These parameters resulted in the LSA similarity score having a Pearson’s correlation of -.520 with Rater A. k was found to be optimal at 9. Algorithm 4.1: PLOTCOMPARE(E, R) i 0 f ; fore E E do for r E R if e = SYN(r) then fz 1 else fz 0 for ne E Ne for nr E Nr if ne = SYN(nr) then fz 1 else fz 0 i = i + 1 Figure 1: Plot Comparison Algorithm Classifier Corpus Features % Correct k-NN Adventure LSA 47.5 Naive Bayes Adventure PLOT 55.6 k-NN Thief LSA 41.2 Naive Bayes Thief PLOT 45.4 Table 2: Machine-Learning Results The results of the plot comparison algorithm were given as features to machine-l</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas. Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lemaire</author>
<author>S Mandin</author>
<author>P Dessus</author>
<author>G Denhire</author>
</authors>
<title>Computational cognitive models of summarization assessment skills. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Stressa, Italy.</location>
<contexts>
<context position="2424" citStr="Lemaire et al., 2005" startWordPosition="393" endWordPosition="396">y and generate appropriate advice. 1.1 The Story Rewriting Task A task used in schools is the story rewriting task, where a story, the exemplar story, is read to the students, and afterwards the story is rewritten by each student, providing a corpus of rewritten stories. This task tests the students ability to both listen and write, while removing from the student the cognitive load needed to generate a new plot. This task is reminiscent of the well-known “War of the Ghosts” experiment used in psychology for studying memory (Bartlett, 1932) and related to work in fields such as summarization (Lemaire et al., 2005) and narration (Halpin et al., 2004). 1.2 Agent Design The goal of the agent is to classify each of the rewritten stories for overall plot quality. This rating can be used to give “coarse-grained” general advice. The agent should then provide “finegrained” specific advice to the student on how their plot could be improved. The agent should be able to detect if the story should be re-read or a human teacher summoned to help the student. To accomplish this task, we extract events that represent the entities and their actions in the plot from both the exemplar and the rewritten stories. A plot co</context>
</contexts>
<marker>Lemaire, Mandin, Dessus, Denhire, 2005</marker>
<rawString>B. Lemaire, S. Mandin, P. Dessus, and G. Denhire. 2005. Computational cognitive models of summarization assessment skills. In In Proceedings of the 27th Annual Meeting of the Cognitive Science Society, Stressa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiona McNeill</author>
<author>Harry Halpin</author>
<author>Ewan Klein</author>
<author>Alan Bundy</author>
</authors>
<title>Merging stories with shallow semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the Knowledge Representation and Reasoning for Language Processing Workshop at the European Association for Computational Linguistics,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="11421" citStr="McNeill et al., 2006" startWordPosition="1912" endWordPosition="1915">egular nature of the sentences. Pronouns are resolved using a rule-based reimplementation of the CogNIAC algorithm (Baldwin, 1997) and sentences are lemmatized and chunked using the Cass Chunker (Abney, 1995). It was felt the chunking method would be the only feasible way to retrieve portions of the sentences that may contain complete “semantic content units” from the ungrammatical and irregular text. The application of a series of rules, mainly mapping verbs to predicate names and nouns to arguments, to the results of the chunker produces events from chunks as described in our previous work (McNeill et al., 2006). The accuracy of our rule-set was developed by using the grammatical exemplar stories as a testbed, and a blind judge found they produced 68% interpretable or “sensible” events given the ungrammatical text. Students usually use the present or past tense exclusively throughout the story and events are usually presented in order of occurrence. An inspection of our corpus showed 3% of stories in our corpus seemed to get the order of events wrong (Hickmann, 2003). 4.1 Comparing Stories Since the student is rewriting the story using their own words, a certain variance from the plot of the exemplar</context>
</contexts>
<marker>McNeill, Halpin, Klein, Bundy, 2006</marker>
<rawString>Fiona McNeill, Harry Halpin, Ewan Klein, and Alan Bundy. 2006. Merging stories with shallow semantics. In Proceedings of the Knowledge Representation and Reasoning for Language Processing Workshop at the European Association for Computational Linguistics, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik T Mueller</author>
</authors>
<title>Story understanding through multi-representation model construction.</title>
<date>2003</date>
<booktitle>Text Meaning: Proceedings of the HLT-NAACL 2003 Workshop,</booktitle>
<pages>46--53</pages>
<editor>In Graeme Hirst and Sergei Nirenburg, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>East Stroudsburg, PA.</location>
<contexts>
<context position="8109" citStr="Mueller, 2003" startWordPosition="1366" endWordPosition="1367"> a “representational” account that explicitly represents each event in the plot. These semantic relationships are important in stories, e.g., “The thief jumped on the donkey” being distinctly different from “The donkey jumped on the thief.” What characters participate in an action matter, since “The king stole the treasure” reveals a major 858 misunderstanding while “The thief stole the treasure” shows a correct interpretation by the student. 3.1 Stories as Events We represent a story as a sequence of events, p1...ph, represented as a list of predicatearguments, similar to the event calculus (Mueller, 2003). Our predicate-argument structure is a minimal subset of first-order logic (no quantifiers), and so is compatible with case-frame and dependency representations. Every event has a predicate (function) p that has one or more arguments, n1...na. In the tradition of Discourse Representation Theory (Kamp and Reyle, 1993), our current predicate argument structure could be converted automatically to first order logic by using a default existential quantification over the predicates and joining them conjunctively. Predicate names are often verbs, while their arguments are usually, although not exclu</context>
</contexts>
<marker>Mueller, 2003</marker>
<rawString>Erik T. Mueller. 2003. Story understanding through multi-representation model construction. In Graeme Hirst and Sergei Nirenburg, editors, Text Meaning: Proceedings of the HLT-NAACL 2003 Workshop, pages 46–53, East Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method. In</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference of the North American Association for Computational Linguistics and Human Language Technologies.</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="10061" citStr="Nenkova and Passonneau, 2004" startWordPosition="1687" endWordPosition="1690">cal corpus we cannot at this time extract neo-Davidsonian events. A sentence maps onto one, multiple, or no events. A unique name and closed-world assumption is enforced, although for purposes of comparing event we compare membership of argument and predicate names in WordNet synsets in addition to exact name matches (Fellbaum, 1998). 4 Extracting Events Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of “semantic content units” that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). We further hypothesize these can be found in chunks of the text automatically identified by a chunker, and we can represent these units as predicate-arguments in our event structure. The event structure of each story is automatically extracted using an XMLbased pipeline composed of NLP processing modules, and unlike other story systems, extract full events instead of filling in a frame of a story script (Riloff, 1999). Using the latest version of the Language Technology Text Tokenization Toolkit (Grover et al., 2000), words are tokenized and sentence boundaries detected. Words are given part</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In In Proceedings of the Joint Conference of the North American Association for Computational Linguistics and Human Language Technologies. Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Information extraction as a stepping stone toward story understanding.</title>
<date>1999</date>
<booktitle>In Ashwin Ram and Kenneth Moorman, editors, Computational Models of Reading and Understanding.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10484" citStr="Riloff, 1999" startWordPosition="1759" endWordPosition="1760">ten story can be defined by the presence or absence of “semantic content units” that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). We further hypothesize these can be found in chunks of the text automatically identified by a chunker, and we can represent these units as predicate-arguments in our event structure. The event structure of each story is automatically extracted using an XMLbased pipeline composed of NLP processing modules, and unlike other story systems, extract full events instead of filling in a frame of a story script (Riloff, 1999). Using the latest version of the Language Technology Text Tokenization Toolkit (Grover et al., 2000), words are tokenized and sentence boundaries detected. Words are given partof-speech tags by a maximum entropy tagger from the toolkit. We do not attempt to obtain a full parse of the sentence due to the highly irregular nature of the sentences. Pronouns are resolved using a rule-based reimplementation of the CogNIAC algorithm (Baldwin, 1997) and sentences are lemmatized and chunked using the Cass Chunker (Abney, 1995). It was felt the chunking method would be the only feasible way to retrieve</context>
</contexts>
<marker>Riloff, 1999</marker>
<rawString>E. Riloff. 1999. Information extraction as a stepping stone toward story understanding. In Ashwin Ram and Kenneth Moorman, editors, Computational Models of Reading and Understanding. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judy Robertson</author>
<author>Beth Cross</author>
</authors>
<title>Children’s perceptions about writing with their teacher and the StoryStation learning environment.</title>
<date>2003</date>
<journal>Narrative and Interactive Learning Environments: Special Issue of International Journal of Continuing Engineering Education and Life-long Learning.</journal>
<contexts>
<context position="1682" citStr="Robertson and Cross, 2003" startWordPosition="267" endWordPosition="270">ction In this paper we investigate how features of a text discovered via automatic event extraction can be used in both natural language understanding and advice generation in the domain of narrative instruction. The background application is a fully automated plot analysis agent to improve the writing of students could be used by current narrative tutoring systems (Robertson and WiemerHastings, 2002). As shown by participatory design studies, teachers are interested in a plot analysis agent that can give online natural language advice and many students enjoy feedback from an automated agent (Robertson and Cross, 2003). We use automatic event extraction to create a storyindependent automated agent that can both analyze the plot of a story and generate appropriate advice. 1.1 The Story Rewriting Task A task used in schools is the story rewriting task, where a story, the exemplar story, is read to the students, and afterwards the story is rewritten by each student, providing a corpus of rewritten stories. This task tests the students ability to both listen and write, while removing from the student the cognitive load needed to generate a new plot. This task is reminiscent of the well-known “War of the Ghosts”</context>
</contexts>
<marker>Robertson, Cross, 2003</marker>
<rawString>Judy Robertson and Beth Cross. 2003. Children’s perceptions about writing with their teacher and the StoryStation learning environment. Narrative and Interactive Learning Environments: Special Issue of International Journal of Continuing Engineering Education and Life-long Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judy Robertson</author>
<author>Peter Wiemer-Hastings</author>
</authors>
<title>Feedback on children’s stories via multiple interface agents.</title>
<date>2002</date>
<booktitle>In International Conference on Intelligent Tutoring Systems,</booktitle>
<location>Biarritz, France.</location>
<marker>Robertson, Wiemer-Hastings, 2002</marker>
<rawString>Judy Robertson and Peter Wiemer-Hastings. 2002. Feedback on children’s stories via multiple interface agents. In International Conference on Intelligent Tutoring Systems, Biarritz, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rose</author>
<author>D Bhembe</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>K VanLehn</author>
</authors>
<title>A hybrid language understanding approach for robust selection of tutoring goals.</title>
<date>2002</date>
<booktitle>In International Conference on Intelligent Tutoring Systems,</booktitle>
<location>Biarritz, France.</location>
<contexts>
<context position="19182" citStr="Rose et al., 2002" startWordPosition="3269" endWordPosition="3272">mance seems to suffer the more complex a story is. Since the Naive Bayes classifier is fast (able to classify stories in only a few seconds) and the entire algorithm from training to advice generation (as detailed below) is fully automatic once a small training corpus has been produced, this technique can be used in reallife tutoring systems and easily ported to other stories. 5 Automated Advice The plot analysis agent is not meant to give the students grades for their stories, but instead use the automatic ratings as an intermediate step to produce advice, like other hybrid tutoring systems (Rose et al., 2002). The advice that the agent can generate from the automatic rating classification is limited to coarse-grained general advice. However, by inspecting the results of the plot comparison algorithm, our agent is capable of giving detailed fine-grained specific advice from the relationships of the events in the story. One tutoring system resembling ours is the WRITE system, but we differ from it by using event structure to represent the information in the system, instead of using rhetorical features (Burstein et al., 2003). In this regards it more closely resembles the physics tutoring system WHY-</context>
</contexts>
<marker>Rose, Bhembe, Roque, Siler, Srivastava, VanLehn, 2002</marker>
<rawString>C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivastava, and K. VanLehn. 2002. A hybrid language understanding approach for robust selection of tutoring goals. In International Conference on Intelligent Tutoring Systems, Biarritz, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>