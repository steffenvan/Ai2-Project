<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.982564">
Lattice Parsing to Integrate Speech Recognition and Rule-Based Machine
Translation
</title>
<author confidence="0.937509">
Seljuk Köprü
</author>
<affiliation confidence="0.693287333333333">
AppTek, Inc.
METU Technopolis
Ankara, Turkey
</affiliation>
<email confidence="0.994129">
skopru@apptek.com
</email>
<author confidence="0.968814">
Adnan Yazıcı
</author>
<affiliation confidence="0.903862666666667">
Department of Computer Engineering
Middle East Technical University
Ankara, Turkey
</affiliation>
<email confidence="0.9968">
yazici@metu.edu.tr
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999225">
In this paper, we present a novel approach
to integrate speech recognition and rule-
based machine translation by lattice pars-
ing. The presented approach is hybrid
in two senses. First, it combines struc-
tural and statistical methods for language
modeling task. Second, it employs a
chart parser which utilizes manually cre-
ated syntax rules in addition to scores ob-
tained after statistical processing during
speech recognition. The employed chart
parser is a unification-based active chart
parser. It can parse word graphs by using a
mixed strategy instead of being bottom-up
or top-down only. The results are reported
based on word error rate on the NIST
HUB-1 word-lattices. The presented ap-
proach is implemented and compared with
other syntactic language modeling tech-
niques.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902865384616">
The integration of speech and language technolo-
gies plays an important role in speech to text
translation. This paper describes a unification-
based active chart parser and how it is utilized
for language modeling in speech recognition or
speech translation. The fundamental idea behind
the proposed solution is to combine the strengths
of unification-based chart parsing and statistical
language modeling. In the solution, all sentence
hypotheses, which are represented in word-lattice
format at the end of automatic speech recognition
(ASR), are parsed simultaneously. The chart is
initialized with the lattice and it is processed un-
til the first sentence hypothesis is selected by the
parser. The parser also utilizes the scores assigned
to words during the speech recognition process.
This leads to a hybrid solution.
An important benefit of this approach is that it
allows one to make use of the available grammars
and parsers for language modeling task. So as to
be used for this task, syntactic analyzer compo-
nents developed for a rule-based machine trans-
lation (RBMT) system are modified. In speech
translation (ST), this approach leads to a perfect
integration of the ASR and RBMT components.
Language modeling effort in ASR and syntac-
tic analysis effort in RBMT are overlapped and
merged into a single task. Its advantages are
twofold. First, this allows us to avoid unnecessary
duplication of similar jobs. Secondly, by using the
available components, we avoid the difficulty of
building a syntactic language model all from the
beginning.
There are two basic methods that are being
used to integrate ASR and rule-based MT systems:
First-best method and the N-best list method. Both
techniques are motivated from a software engi-
neering perspective. In the First-best approach
(Figure 1.a), the ASR module sends a single rec-
ognized text to the MT component to translate.
Any ambiguity existing in the recognition process
is resolved inside the ASR. In contrast to the First-
best approach, in the N-best List approach (Figure
1.b); the ASR outputs N possible recognition hy-
potheses to be evaluated by the MT component.
The MT picks the first hypothesis and translates it
if it is grammatically correct. Otherwise, it moves
to the second hypothesis and so on. If none of the
available hypotheses are syntactically correct, then
it translates the first one.
We propose a new method to couple ASR and
rule-based MT system as an alternative to the ap-
</bodyText>
<note confidence="0.9230495">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 469–477,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999482">
469
</page>
<bodyText confidence="0.999388375">
proaches mentioned above. Figure 1 represents
the two currently in-use coupling methods fol-
lowed by the new approach we introduce (Fig-
ure 1.c). In the newly proposed technique, which
we call the N-best word graph approach, the ASR
module outputs a word graph containing all N-best
hypotheses. The MT component parses the word
graph, thus, all possible hypotheses at one time.
</bodyText>
<figure confidence="0.9885035">
Recognized Text
1. Recognized Text
...
N. Recognized Text
</figure>
<figureCaption confidence="0.9981845">
Figure 1: ASR and rule-based MT coupling: a)
First-best b) N-best list c) N-best word graph.
</figureCaption>
<bodyText confidence="0.9999617625">
While integrating the SR system with the rule-
based MT system, this study uses word graphs
and chart parsing with new extensions. Parsing of
word lattices has been a topic of research over the
past decade. The idea of chart parsing the word
graph in SR systems has been previously used
in different studies in order to resolve ambigu-
ity. Tomita (1986) introduced the concept of word-
lattice parsing for the purpose of speech recogni-
tion and used an LR parser. Next, Paeseler (1988)
used a chart parser to process word-lattices. How-
ever, to the best of our knowledge, the specific
method for chart parsing a word graph introduced
in this paper has not been previously used for cou-
pling purposes.
Recent studies point out the importance of uti-
lizing word graphs in speech tasks (Dyer et al.,
2008). Previous work on language modeling can
be classified according to whether a system uses
purely statistical methods or whether it uses them
in combination with syntactic methods. In this pa-
per, the focus is on systems that contain syntactic
approaches. In general, these language modeling
approaches try to parse the ASR output in word-
lattice format in order to choose the most prob-
able hypothesis. Chow and Roukos (1989) used
a unification-based CYK parser for the purpose of
speech understanding. Chien et al. (1990) and We-
ber (1994) utilized probabilistic context free gram-
mars in conjunction with unification grammars to
chart-parse a word-lattice. There are various dif-
ferences between the work of Chien et al. (1990)
and Weber (1994) and the work presented in this
paper. First, in the previously mentioned studies,
the chart is populated with the same word graph
that comes from the speech recognizer without any
pruning, whereas in our approach the word graph
is reduced to an acceptable size. Otherwise, the
efficiency becomes a big challenge because the
search space introduced by a chart with over thou-
sands of initial edges can easily be beyond current
practical limits. Another important difference in
our approach is the modification of the chart pars-
ing algorithm to eliminate spurious parses.
Ney (1991) deals with the use of probabilis-
tic CYK parser for continous speech recognition
task. Stolcke (1995) summarizes extensively their
approach to utilize probabilistic Earley parsing.
Chappelier et al. (1999) gives an overview of dif-
ferent approaches to integrate linguistic models
into speech recognition systems. They also re-
search various techniques of producing sets of hy-
potheses that contain more “semantic” variabil-
ity than the commonly used ones. Some of the
recent studies about structural language model-
ing extract a list of N-best hypotheses using an
N-gram and then apply structural methods to de-
cide on the best hypothesis (Chelba, 2000; Roark,
2001). This contrasts with the approach presented
in this study where, instead of a single sentence,
the word-lattice is parsed. Parsing all sentence hy-
potheses simultaneously enables a reduction in the
number of edges produced during the parsing pro-
cess. This is because the shared word hypothe-
ses are processed only once compared to the N-
best list approach, where the shared words are pro-
cessed each time they occur in a hypothesis. Sim-
ilar to the current work, other studies parse the
whole word-lattice without extracting a list (Hall,
2005). A significant distinction between the work
of Hall (2005) and our study is the parsing algo-
rithm. In contrast to our chart parsing approach
augmented by unification based feature structures,
Charniak parser is used in Hall (2005)’s along with
PCFG.
The rest of the paper is organized as follows:
In the following section, an overview of the pro-
posed language model is presented. Next, in Sec-
tion 3, the parsing process of the word-lattice is
described in detail. Section 4 describes the exper-
</bodyText>
<figure confidence="0.997843166666667">
Speech
Recognizer
Speech
Recognizer
Speech
Recognizer
Rule−based Target Text
MT
Rule−based Target Text
MT Target Text
Rule−based
MT
</figure>
<page confidence="0.997227">
470
</page>
<bodyText confidence="0.967528466666666">
iments and reports the obtained results. Finally,
Section 5 concludes the paper.
2 Hybrid language modeling
The general architecture of the system is depicted
in Figure 2. The HTK toolkit (Woodland, 2000)
word-lattice file format is used as the default file
format in the proposed solution. The word-lattice
output from ASR is converted into a finite state
machine (FSM). This conversion enables us to
benefit from standard theory and algorithms on
FSMs. In the converted FSM, non-determinism is
removed and it is minimized by eliminating redun-
dant nodes and arcs. Next, the chart is initialized
with the deterministic and minimal FSM. Finally,
this chart is used in the structural analysis.
</bodyText>
<figure confidence="0.483585">
Speech
ASR
Word Graph
FSM Conversion
FSM
Minimization
Minimized FSM
Initialization
Initial Chart
</figure>
<figureCaption confidence="0.998161">
Figure 2: The hybrid language model architecture.
</figureCaption>
<bodyText confidence="0.99996">
Structural analysis of the word-lattice is accom-
plished in two consecutive tasks. First, morpho-
logical analysis is performed on the word level and
any information carried by the word is extracted
to be used in the following stages. Second, syn-
tactic analysis is performed on the sentence level.
The syntactic analyzer consists of a chart parser in
which the rules modeling the language grammar
are augmented with functional expressions.
</bodyText>
<sectionHeader confidence="0.960369" genericHeader="method">
3 Word Graph Processing
</sectionHeader>
<bodyText confidence="0.999975384615385">
The word graphs produced by an ASR are beyond
the limits of a unification-based chart parser. A
small-sized lattice from the NIST HUB-1 data set
(Pallett et al., 1994) can easily contain a couple of
hundred states and more than one thousand arcs.
The largest lattice in the same data set has 25 000
states and almost 1 million arcs. No unification-
based chart parser is capable of coping with an in-
put of this size. It is unpractical and unreasonable
to parse the lattice in the same form as it is output
from the ASR. Instead, the word graph is pruned
to a reasonable size so that it can be parsed accord-
ing to acceptable time and memory limitations.
</bodyText>
<subsectionHeader confidence="0.996397">
3.1 Word graph to FSM conversion
</subsectionHeader>
<bodyText confidence="0.999980487804878">
The pruning process starts by converting the time-
state lattice to a finite state machine. This way,
algorithms and data structures for FSMs are uti-
lized in the following processing steps. Each word
in the time-state lattice corresponds to a state node
in the new FSM. The time slot information is also
dropped in the recently built automata. The links
between the words in the lattice are mapped as the
FSM arcs.
In the original representation, the word labels
in the time-state lattices are on the nodes, and the
acoustic scores and the statistical language model
scores are on the arcs. Similarly, the words are
also on the nodes. This representation does not fit
into the chart definition where the words are on
the arcs. Therefore, the FSM is converted to an
arc labeled FSM. The conversion is accomplished
by moving back the word label on a state to the
incoming arcs. The weights on the arcs represent
the negative logarithms of probabilities. In order
to find the weight of a path in the FSM, all weights
on the arcs existing on that path are added up.
The resulting FSM contains redundant arcs that
are inherited from the word graph. Many arcs cor-
respond to the same word with a different score.
The FSM is nondeterministic because, at a given
state, there are different alternative arcs with the
same word label. Before parsing the converted
FSM, it is essential to find an equivalent finite au-
tomata that is deterministic and that has as few
nodes as possible. This way, the work necessary
during parsing is reduced and efficient processing
is ensured.
The minimization process serves to shrink down
the FSM to an equivalent automata with a suitable
size for parsing. However, it is usually the case
that the size is not small enough to meet the time
and memory limitations in parsing. N-best list se-
lection can be regarded as the last step in constrict-
ing the size. A subset of possible hypotheses is se-
lected among many that are contained in the mini-
</bodyText>
<figure confidence="0.989676285714286">
Selected Hypothesis
Morphology Rules
Syntax Rules
Morphological Analysis
Chart w/ feature structures
Chart Parsing
Lexicon
</figure>
<page confidence="0.99495">
471
</page>
<bodyText confidence="0.978503">
mized FSM. The selection mechanism favors only
the best hypotheses according to the scores present
in the FSM arcs.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Chart parsing
</subsectionHeader>
<bodyText confidence="0.999985918918919">
The parsing engine implemented for this work is
an active chart parser similar to the one described
in Kay (1986). The language grammar that is pro-
cessed by the parser can be designed top-down,
bottom-up or in a combined manner. It employs
an agenda to store the edges prior to inserting to
the chart. Edges are defined to be either complete
or incomplete. Incomplete edges describe the rule
state where one or more syntactic categories are
expected to be matched. An incomplete edge be-
comes complete if all syntactic categories on the
right-hand-side of the rule are matched.
Parsing starts from the rules that are associ-
ated to the lexical entries. This corresponds to
the bottom-up parsing strategy. Moreover, pars-
ing also starts from the rules that build the final
symbol in the grammar. This corresponds to the
top-down parsing strategy. Bottom-up rules and
top-down rules differ in that the former contains
a non-terminal that is marked as the trigger or
central element on the left-hand-side of the rule.
This central element is the starting point for the
execution of the bottom-up rule. After the cen-
tral element is matched, the extension continues
in a bidirectional manner to complete the missing
constituents. Bottom-up incomplete edges are de-
scribed with double-dotted rules to keep track of
the beginning and end of the matched fragment.
The anticipated edges are first inserted into the
agenda. Edges popped out from the agenda are
processed with the fundamental rule of chart pars-
ing. The agenda allows the reorganization of the
edge processing order. After the application of the
fundamental rule, new edges are predicted accord-
ing to either bottom-up or top-down parsing strat-
egy. This strategy is determined by how the cur-
rent edge has been created.
</bodyText>
<subsectionHeader confidence="0.996887">
3.3 Chart initialization
</subsectionHeader>
<bodyText confidence="0.999679935483871">
The chart initialization procedure creates from an
input FSM, which is derived from the ASR word
lattice, a valid chart that can be parsed in an active
chart parser. The initialization starts with filling
in the distance value for each node. The distance
of a node in the FSM is defined as the number of
nodes on the longest path from the start state to
the current state. After the distance value is set
for all nodes in the FSM, an edge is created for
each arc. The edge structure contains the start and
end values in addition to the weight and label data
fields. These position values represent the edge
location relative to the beginning of the chart. The
starting and ending node information for the arc is
also copied to the edge. This node information is
later utilized in chart parsing to eliminate spurious
parses. The number of edges in the chart equals to
the number of edges in the input FSM at the end
of initialization.
Consider the simple FSM F1 depicted in Fig-
ure 3, the corresponding two-dimensional chart
and the related hypotheses. The chart is populated
with the converted word graph before parsing be-
gins. Words in the same column can be regarded
as a single lexical entry with different senses (e.g.,
‘boy’ and ‘boycott’ in column 2). Words span-
ning more than one column can be regarded as id-
iomatic entries (e.g. ‘escalated’ from column 3
to 5). Merged cells in the chart (e.g., ‘the’ and
‘yesterday’ at columns 1 and 6, respectively) are
shared in both sentence hypotheses.
</bodyText>
<figure confidence="0.944517666666667">
F1:
Chart:
0 1 2 3 4 5 6
Hypotheses:
• The boy goes to school yesterday
• The boycott escalated yesterday
</figure>
<figureCaption confidence="0.9766655">
Figure 3: Sample FSM F4, the corresponding
chart and the hypotheses.
</figureCaption>
<subsectionHeader confidence="0.871596">
3.4 Extended Chart Parsing
</subsectionHeader>
<bodyText confidence="0.999856857142857">
In a standard active chart parser, the chart depicted
in Figure 3 could produce some spurious parses.
For example, both of the complete edges in the ini-
tial chart at location [1-2] (i.e. ‘boy’ and ‘boycott)
can be combined with the word ‘goes’, although
‘boycott goes’ is not allowed in the original word
graph. We have eliminated these kinds of spuri-
</bodyText>
<figure confidence="0.999356578947368">
boy
goes to
5 6 7
school
boycott
yesterday
3
4
2
escalated
the
0 1
0 the1
1 boycott 2
1 boy 5
5 goes 6 6 to7
2 escalated 3
7 school 3
3 yesterday 4
</figure>
<page confidence="0.994072">
472
</page>
<bodyText confidence="0.999992032258065">
ous parses by making use of the arcstart and ar-
cfinish values. These labels indicate the starting
and ending node identifiers of the path spanned by
the edge in subject. The application of this idea
is illustrated in Figure 4. Different from the orig-
inal implementation of the fundamental rule, the
procedure has the additional parameters to define
starting and ending node identifiers. Before creat-
ing a new incomplete edge, it is checked whether
the node identifiers match or not.
When we consider the chart given in Figure 3,
‘1 boycott 2’ and ‘5 goes,,’ cannot be combined ac-
cording to the new fundamental rule in a parse
tree because the ending node id, i.e. 2, of the for-
mer does not match the starting node id, i.e. 5,
of the latter. In another example, ‘,, the1’ can be
combined with both ‘1 boy5’ and ‘1 boycott 2’ be-
cause their respective node identifiers match. Af-
ter the two edges, ‘boycott’ and ‘escalated’, are
combined and a new edge is generated, the start-
ing node identifiers for the entire edge will be as
in ‘1 boycott escalated 3’.
The utilization of the node identifiers enables
the two-dimensional modeling of a word graph in
a chart. This extension to chart parsing makes
the current approach word-graph based rather than
confusion-network based. Parse trees that con-
flict with the input word graph are blocked and all
the processing resources are dedicated to proper
edges. The chart parsing algorithm is listed in Fig-
ure 4.
</bodyText>
<subsectionHeader confidence="0.942328">
3.5 Unification-based chart parsing
</subsectionHeader>
<bodyText confidence="0.9998645">
The grammar rules are implemented using Lexical
Functional Grammar (LFG) paradigm. The pri-
mary data structure to represent the features and
values is a directed acyclic graph (dag). The sys-
tem also includes an expressive Boolean formal-
ism, used to represent functional equations to ac-
cess, inspect or modify features or feature sets in
the dag. Complex feature structures (e.g. lists,
sets, strings, and conglomerate lists) can be asso-
ciated with lexical entries and grammatical cate-
gories using inheritance operations. Unification is
used as the fundamental mechanism to integrate
information from lexical entries into larger gram-
matical constituents.
The constituent structure (c-structure) repre-
sents the composition of syntactic constituents for
a phrase. It is the term used for parse tree in
LFG. The functional structure (f-structure) is the
</bodyText>
<table confidence="0.868803466666667">
input: grammar, word−graph
output: chart
algorithm CHART−PARSE (grammar, word−graph )
INITIALIZE (chart, agenda, word−graph )
while agenda i s not empty
edge — POP ( agenda )
PROCESS−EDGE (edge)
end while
end algorithm
procedure PROCESS−EDGE (A —. B • α • C, [j, k], [ns, ne])
PUSH ( chart , A —. B • α • C, [j, k], [ns, ne] )
FUNDAMENTAL−RULE (A —. B • α • C, [j, k], [ns, ne])
PREDICT (A —. B • α • C, [j, k], [ns, ne])
end procedure
procedure FUNDAMENTAL−RULE (A — .B • α • C, [j, k], [ns, ne])
</table>
<equation confidence="0.969045303030303">
i f B = βD // edge i s incomplete
for each (D —. •δ•, [i, j], [nr, ns]) in chart
PUSH (agenda, (A —. β • Dα • C, [i, k], [nr, ne]) )
end for
end i f
i f C = Dγ // edge i s incomplete
for each (D —. •δ•, [k, l], [ne, nf ]) in chart
PUSH (agenda, (A —. B • αD•γ, [j, l], [ns, nf ]) )
end for
end i f
i f B i s null and C i s null // edge i s complete
for each (D —. βA • γ • δ, [k, l], [ne, nf ]) in chart
PUSH (agenda, (D —. β • Aγ • δ, [j, l], [ns, nf ]) )
end for
for each (D —. β • γ • Aδ, [i, j], [nr, ns]) in chart
PUSH (agenda, (D —. β • γA • δ, [i, k], [nr, ne]) )
end for
end i f
end procedure
procedure PREDICT (A —. B • α • C, [j, k], [ns, ne])
i f B i s null and C i s null // edge i s complete
for each D —. βAγ in grammar where A i s trigger
PUSH (agenda , (D —. β • A • γ, [j, k], [ns, ne]) )
end for
else
i f B = βD // edge i s incomplete
for each D —. γ in grammar
PUSH (agenda , (D —. γ•, [j, j], [ns, ns]) )
end for
end i f
i f C = Dγ // edge i s incomplete
for each D —. γ in grammar
PUSH (agenda, (D —. •γ, [k, k], [ne, ne]) )
</equation>
<tableCaption confidence="0.91572475">
end for
end i f
end i f
end procedure
</tableCaption>
<figureCaption confidence="0.9371">
Figure 4: Extended chart parsing algorithm used
</figureCaption>
<bodyText confidence="0.964214066666667">
to parse word graphs. Fundamental rule and pre-
dict procedures are updated to handle word graphs
in a bidirectional manner.
representation of grammatical functions in LFG.
Attribute-value-matrices are used to describe f-
structures. A sample c-structure and the corre-
sponding f-structures in English are shown in Fig-
ure 5. For simplicity, many details and feature val-
ues are not given. The dag containing the infor-
mation originated from the lexicon and the infor-
mation extracted from morphological analysis is
shown on the leaf levels of the parse tree in Figure
5. The final dag corresponding to the root node is
built during the parsing process in cascaded unifi-
cation operations specified in the grammar rules.
</bodyText>
<page confidence="0.998707">
473
</page>
<figure confidence="0.9495666">
np vp
pro v pp
p np
det n
he looked after the kids
</figure>
<figureCaption confidence="0.9879685">
Figure 5: The c-structure and the associated f-
structures.
</figureCaption>
<subsectionHeader confidence="0.997999">
3.6 Parse evaluation and recovery
</subsectionHeader>
<bodyText confidence="0.971055228070175">
After all rules are executed and no more edges are
left in the agenda, the chart parsing process ends
and parse evaluation begins. The chart is searched
for complete edges with the final symbol of the
grammar (e.g. SBAR) as their category. Any such
edge spanning the entire input represents the full
parse. If there is no such edge then the parse re-
covery process takes control.
If the input sentence is ambiguous, then, at the
end of parsing, there will multiple parse trees in
the chart that span the entire input. Similarly,
a grammar built with insufficient constraints can
lead to multiple parse trees. In this case, all possi-
ble edges are evaluated for completeness and co-
herence (Bresnan, 1982) starting from the edge
with the highest weight. A parse tree is complete
if all the functional roles (SUBJ, OBJ, SCOMP etc.)
governed by the verb are actually present in the c-
structure; it is coherent if all the functional roles
present are actually governed by the verb. The
parse tree that is evaluated as complete and co-
herent and has the highest weight is selected for
further processing.
In general, a parsing process is said to be suc-
cessful if a parse tree can be built according to the
input sentence. The building of the parse tree fails
when the sentence is ungrammatical. For the goal
of MT, however, a parse tree is required for the
transfer stage and the generation stage even if the
input is not grammatical. Therefore, for any input
sentence, a corresponding parse tree is built at the
end of parsing.
If parsing fails, i.e. if all rules are exhausted and
no successful parse tree has been produced, then
the system tries to recover from the failure by cre-
ating a tree like structure. Appropriate complete
edges in the chart are used for this purpose. The
idea is to piece together all partial parses for the
input sentence, so that the number of constituent
edges is minimum and the score of the final tree is
maximum. While selecting the constituents, over-
lapping edges are not chosen.
The recovery process functions as follows:
• The whole chart is traversed and a complete
edge is inserted into a candidate list if it has
the highest score for that start-end position.
If two edges have the same score, then the
farthest one to the leaf level is preferred.
• The candidate list is traversed and a com-
bination with the minimum number of con-
stituents is selected. The edges with the
widest span get into the winning combina-
tion.
• The c-structures and f-structures of the edges
in the winning combination are joined into a
whole c-structure and f-structure which rep-
resent the final parse tree for the input.
</bodyText>
<sectionHeader confidence="0.998516" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999755833333333">
The experiments carried out in this paper are run
on word graphs based on 1993 benchmark tests for
the ARPA spoken language program (Pallett et al.,
1994). In the large-vocabulary continuous speech
recognition (CSR) tests reported by Pallett et al.
(1994), Wall Street Journal-based CSR corpus ma-
terial was made use of. Those tests intended to
measure basic speaker-independent performance
on a 64K-word read-speech test set which con-
sists of 213 utterances. Each of the 10 different
speakers provided 20 to 23 utterances. An acous-
tic model and a trigram language model is trained
using Wall Street Journal data by Chelba (2000)
who also generated the 213 word graphs used in
the current experiments. The word graphs, re-
ferred as HUB-1 data set, contain both the acous-
tic scores and the trigram language model scores.
Previously, the same data set was used in other
</bodyText>
<figure confidence="0.996819173913044">
s
⎡ ⎤
cat s
⎢ ⎥
⎢ ⎢form ‘look’ ⎥ ⎥
⎢ ⎥
⎢ ⎢tense past ⎥ ⎥
⎢ ⎡form ‘he’⎤ ⎥
⎢ ⎥
⎢subj ⎣⎦ ⎥
⎢proper plus ⎥
⎢ ⎢�orm
‘kids’⎤
obleak def plus J ⎥
⎣ ⎦
pform ‘after’
⎡ ⎤ [cat v Icat prep] cat det ⎡ ⎤
cat pro tense pas t ⎣⎦ cat n
⎢ ⎢ ⎢proper plus def plus ⎢ ⎥
⎢ ⎢ ⎢case nom ⎢ ⎢proper minus ⎥ ⎥
⎢ ⎢ ⎣num sg ⎦ ⎢ ⎥
person 3 ⎢ ⎣num pl ⎦ ⎥
person 3
</figure>
<page confidence="0.580594">
474
</page>
<figure confidence="0.999292222222222">
29.64 �
10.00
�
�
�
�
�
�
�
�
�
�
� �
� � �
�
� � � � � � � � �
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
β
</figure>
<figureCaption confidence="0.995828">
Figure 6: WER for HUB-1 first-best hypotheses
</figureCaption>
<bodyText confidence="0.652847571428571">
obtained using different language-model scaling
factors and α = 1. The unsteadiness of the WER
for β = 10 needs further investigation.
WER
13.32
studies (Chelba, 2000; Roark, 2001; Hall, 2005)
for language modeling task in ASR.
</bodyText>
<subsectionHeader confidence="0.998961">
4.1 N-best list pruning
</subsectionHeader>
<bodyText confidence="0.999597461538462">
The 213 word graphs in the HUB-1 data set are
pruned as described in Section 3 in order to pre-
pare them for chart parsing. AT&amp;T toolkit (Mohri
et al., 1998) is used for determinization and min-
imization of the word graphs and for n-best path
extraction. Prior to feeding in the word graphs to
the FSM tools, the acoustic model and the trigram
language model in the original lattices are com-
bined into a single score using Equation 1, where
S represents the combined score of an arc, A is
the acoustic model (AM) score, L is the language
model (LM) score, α is the AM scale factor and β
is the LM scale factor.
</bodyText>
<equation confidence="0.986402">
S = α A + β L (1)
</equation>
<bodyText confidence="0.999240071428572">
Figure 6 depicts the word error rates for the
first-best hypotheses obtained heuristically by us-
ing α = 1 and β values from 1 to 25. The low-
est WER (13.32) is achieved when α is set to 1
and β to 15. This result is close with the findings
from Hall (2005) who reported to use 16 as the LM
scale factor for the same data set. WER score for
LM-only was 26.8 where in comparison the AM-
only score was 29.64. The results imply that the
language model has more predicting power over
the acoustic model in the HUB-1 lattices. For the
rest of the experiments, we used 1 and 15 as the
acoustic model and language model scale factors,
respectively.
</bodyText>
<subsectionHeader confidence="0.999223">
4.2 Word graph accuracy
</subsectionHeader>
<bodyText confidence="0.994864571428571">
Using the scale factors found in the previous sec-
tion we built N-best word graphs for different N
values. In order to measure the word graph ac-
curacy we constructed the FSM for reference hy-
potheses, FR,f, and we took the intersection of all
the word graphs with the reference FSM. Table 1
lists the word graph accuracy rate for different N
values. For example, an accuracy rate of 30.98 de-
notes that 66 word graphs out of 213 contain the
correct sentences. The accuracy rate for the origi-
nal word graphs in the data set (last row in Table 1)
is 66.67 which indicates that only 142 out of 213
contain the reference sentence. That is, in 71 of the
instances, the reference sentence is not included in
</bodyText>
<tableCaption confidence="0.9880965">
Table 1: Word graph accuracy for different N val-
ues in the data set with 213 word graphs.
</tableCaption>
<table confidence="0.998924714285714">
N Accuracy N Accuracy
1 30.98 60 59.15
10 51.17 70 59.15
20 56.34 80 59.15
30 58.22 90 60.10
40 59.15 100 60.10
50 59.15 full 66.67
</table>
<bodyText confidence="0.986109">
the untouched word graph. The accurate rates ex-
press the maximum sentence error rate (SER) that
can be achieved for the data set.
</bodyText>
<subsectionHeader confidence="0.997895">
4.3 Linguistic Resources
</subsectionHeader>
<bodyText confidence="0.999768666666667">
The English grammar used in the chart parser con-
tained 20 morphology analysis rules and 225 syn-
tax analysis rules. All the rules and the unification
constraints are implemented in LFG formalism.
The number of rules to model the language gram-
mar is quite few compared to probabilistic CFGs
which contain more than 10 000 rules. The mono-
lingual analysis lexicon consists of 40 000 lexical
entries.
</bodyText>
<subsectionHeader confidence="0.994141">
4.4 Chart parsing experiment
</subsectionHeader>
<bodyText confidence="0.999948">
We conducted experiments to compare the per-
formance for N-best list parsing and N-best word
graph parsing. Compared to the N-best list ap-
proach, in N-best word graph parsing approach,
the shared edges are processed only once for all
hypotheses. This saves a lot on the number of
</bodyText>
<page confidence="0.999433">
475
</page>
<tableCaption confidence="0.983553">
Table 2: Number of complete and incomplete
edges generated for the NIST HUB-1 data set us-
ing different approaches.
</tableCaption>
<table confidence="0.999028166666667">
Approach Hypotheses Complete Incomplete
edges edges
N-best list 4869 798 K 12.125 M
1 164 2490
N-best 4869 150.8 K 1.662 M
word graph 1 31 341
</table>
<bodyText confidence="0.999447761904762">
complete and incomplete edges generated during
parsing. Hence, the overall processing time re-
quired to analyze the hypotheses are reduced. In
an N-best list approach, where each hypothesis is
processed separately in the analyzer, there are dif-
ferent charts and different parsing instances for
each sentence hypothesis. Shared words in dif-
ferent sentences are parsed repeatedly and same
edges will be created at each instance.
Table 2 represents the number of complete and
incomplete edges generated for the NIST HUB-1
data set. For each hypothesis, 164 complete edges
and 2490 incomplete edges are generated on the
average in the N-best list approach. In the N-best
word graph approach, the average number of com-
plete edges and incomplete edges reduced to 31
and 341, respectively. The decrease is 81.1% in
complete edges and 86.3% in incomplete edges for
the NIST HUB-1 data set. The profit introduced
in the number of edges by using the N-best word
graph approach is immense.
</bodyText>
<subsectionHeader confidence="0.979084">
4.5 Language modeling experiment
</subsectionHeader>
<bodyText confidence="0.899186666666667">
In order to compare this approach to previous
language modeling approaches we used the same
data set. Table 3 lists the WER for the NIST
HUB-1 data set for different approaches includ-
ing ours. The N-best word graph approach pre-
sented in this paper scored 12.6 WER and still
needs some improvements. The English analy-
sis grammar that was used in the experiments was
designed to parse typed text containing punctua-
tion information. The speech data, however, does
not contain any punctuation. Therefore, the gram-
mar has to be adjusted accordingly to improve the
WER. Another common source of error in parsing
is because of unnormalized text.
Table 3: WER taken from Hall and Johnson
(2003) for various language models on HUB-1 lat-
tices in addition to our approach presented in the
fifth row.
</bodyText>
<table confidence="0.957762">
Model WER
Charniak Parser (Charniak, 2001) 11.8
Attention Shifting 11.9
(Hall and Johnson, 2004)
PCFG (Hall, 2005) 12.0
A* decoding (Xu et al., 2002) 12.3
N-best word graph (this study) 12.6
PCFG (Roark, 2001) 12.7
PCFG (Hall and Johnson, 2004) 13.0
40m-word trigram 13.7
(Hall and Johnson, 2003)
PCFG (Hall and Johnson, 2003) 15.5
</table>
<sectionHeader confidence="0.999257" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999988652173913">
The primary aim of this research was to propose
a new and efficient method for integrating an SR
system with an MT system employing a chart
parser. The main idea is to populate the initial
chart parser with the word graph that comes out
of the SR component.
This paper presents an attempt to blend statisti-
cal SR systems with rule-based MT systems. The
goal of the final assembly of these two compo-
nents was to achieve an enhanced Speech Transla-
tion (ST) system. Specifically, we propose to parse
the word graph generated by the SR module inside
the rule-based parser. This approach can be gener-
alized to any MT system employing chart parsing
in its analysis stage. In addition to utilizing rule-
based MT in ST, this study used word graphs and
chart parsing with new extensions.
For further improvement of the overall system,
our future studies include the following: 1. Ad-
justing the English syntax analysis rules to handle
spoken text which does not include any punctua-
tion. 2. Normalization of the word arcs in the in-
put lattice to match words in the analysis lexicon.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998172333333333">
Thanks to Jude Miller and Mirna Miller for pro-
viding the Arabic reference translations. We also
thank Brian Roark and Keith Hall for providing
the test data, and Nagendra Goel, Cem Boz¸sahin,
Ay¸senur Birtürk and Tolga Qilo˘glu for their valu-
able comments.
</bodyText>
<page confidence="0.998866">
476
</page>
<sectionHeader confidence="0.995896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930553398059">
J. Bresnan. 1982. Control and complementation. In
J. Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 282–390. MIT Press,
Cambridge, MA.
J.-C. Chappelier, M. Rajman, R. Aragues, and
A. Rozenknop. 1999. Lattice parsing for speech
recognition. In TALN’99, pages 95–104.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting on Association for Computational Linguis-
tics. Association for Computational Linguistics.
Ciprian Chelba. 2000. Exploiting Syntactic Structure
for Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee. 1990.
An augmented chart data structure with efficient
word lattice parsing scheme in speech recognition
applications. In Proceedings of the 13th conference
on Computational linguistics, pages 60–65, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee.
1993. A best-first language processing model in-
tegrating the unification grammar and markov lan-
guage model for speech recognition applications.
IEEE Transactions on Speech and Audio Process-
ing, 1(2):221–240.
Yen-Lu Chow and Salim Roukos. 1989. Speech
understanding using a unification grammar. In
ICAASP’89: Proc. of the International Conference
on Acoustics, Speech, and Signal Processing, pages
727–730. IEEE.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012–
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Keith Hall and Mark Johnson. 2003. Language mod-
elling using efficient best-first bottom-up parsing.
In ASR’03: IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 507–512.
IEEE.
Keith Hall and Mark Johnson. 2004. Attention shifting
for parsing speech. In ACL ’04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 40, Morristown, NJ, USA.
Association for Computational Linguistics.
Keith Hall. 2005. Best-First Word Lattice Parsing:
Techniques for Integrated Syntax Language Model-
ing. Ph.D. thesis, Brown University.
Martin Kay. 1986. Algorithm schemata and data struc-
tures in syntactic processing. Readings in natural
language processing, pages 35–70.
C. D. Manning and H. Schütze. 2000. Foundations of
Statistical Natural Language Processing. The MIT
Press.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 1998. A rational design for a weighted finite-
state transducer library. In WIA ’97: Revised Pa-
pers from the Second International Workshop on Im-
plementing Automata, pages 144–158, London, UK.
Springer-Verlag.
Hermann Ney. 1991. Dynamic programming pars-
ing for context-free grammars in continuous speech
recognition. IEEE Transactions on Signal Process-
ing, 39(2):336–340.
A. Paeseler. 1988. Modification of Earley’s algo-
rithm for speech recognition. In Proceedings of
the NATO Advanced Study Institute on Recent ad-
vances in speech understanding and dialog systems,
pages 465–472, New York, NY, USA. Springer-
Verlag New York, Inc.
David S. Pallett, Jonathan G. Fiscus, William M.
Fisher, John S. Garofolo, Bruce A. Lund, and
Mark A. Przybocki. 1994. In HLT ’94: Proceedings
of the workshop on Human Language Technology,
pages 49–74, Morristown, NJ, USA. Association for
Computational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.
Masaru Tomita. 1986. An efficient word lattice pars-
ing algorithm for continuous speech recognition.
Acoustics, Speech, and Signal Processing, IEEE In-
ternational Conference on ICASSP ’86., 11:1569–
1572.
Hans Weber. 1994. Time synchronous chart parsing of
speech integrating unification grammars with statis-
tics. In Proceedings of the Eighth Twente Workshop
on Language Technology, pages 107–119.
Phil Woodland. 2000. HTK Speech Recognition
Toolkit. Cambridge University Engineering Depart-
ment, http://htk.eng.cam.ac.uk.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.
A study on richer syntactic dependencies for struc-
tured language modeling. In ACL ’02: Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 191–198. Association
for Computational Linguistics.
</reference>
<page confidence="0.998427">
477
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573932">
<title confidence="0.9981625">Lattice Parsing to Integrate Speech Recognition and Rule-Based Machine Translation</title>
<author confidence="0.94934">Seljuk Köprü</author>
<affiliation confidence="0.8808435">AppTek, Inc. METU Technopolis</affiliation>
<address confidence="0.973225">Ankara, Turkey</address>
<email confidence="0.999756">skopru@apptek.com</email>
<author confidence="0.990799">Adnan Yazıcı</author>
<affiliation confidence="0.989769">Department of Computer Engineering Middle East Technical University</affiliation>
<address confidence="0.999206">Ankara, Turkey</address>
<email confidence="0.993514">yazici@metu.edu.tr</email>
<abstract confidence="0.992872047619048">In this paper, we present a novel approach to integrate speech recognition and rulebased machine translation by lattice parsing. The presented approach is hybrid in two senses. First, it combines structural and statistical methods for language modeling task. Second, it employs a chart parser which utilizes manually created syntax rules in addition to scores obtained after statistical processing during speech recognition. The employed chart parser is a unification-based active chart parser. It can parse word graphs by using a mixed strategy instead of being bottom-up or top-down only. The results are reported based on word error rate on the NIST HUB-1 word-lattices. The presented approach is implemented and compared with other syntactic language modeling techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>Control and complementation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>282--390</pages>
<editor>In J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21903" citStr="Bresnan, 1982" startWordPosition="3770" endWordPosition="3771">g process ends and parse evaluation begins. The chart is searched for complete edges with the final symbol of the grammar (e.g. SBAR) as their category. Any such edge spanning the entire input represents the full parse. If there is no such edge then the parse recovery process takes control. If the input sentence is ambiguous, then, at the end of parsing, there will multiple parse trees in the chart that span the entire input. Similarly, a grammar built with insufficient constraints can lead to multiple parse trees. In this case, all possible edges are evaluated for completeness and coherence (Bresnan, 1982) starting from the edge with the highest weight. A parse tree is complete if all the functional roles (SUBJ, OBJ, SCOMP etc.) governed by the verb are actually present in the cstructure; it is coherent if all the functional roles present are actually governed by the verb. The parse tree that is evaluated as complete and coherent and has the highest weight is selected for further processing. In general, a parsing process is said to be successful if a parse tree can be built according to the input sentence. The building of the parse tree fails when the sentence is ungrammatical. For the goal of </context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>J. Bresnan. 1982. Control and complementation. In J. Bresnan, editor, The Mental Representation of Grammatical Relations, pages 282–390. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition. In</title>
<date>1999</date>
<booktitle>TALN’99,</booktitle>
<pages>95--104</pages>
<contexts>
<context position="6550" citStr="Chappelier et al. (1999)" startWordPosition="1044" endWordPosition="1047">ognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the efficiency becomes a big challenge because the search space introduced by a chart with over thousands of initial edges can easily be beyond current practical limits. Another important difference in our approach is the modification of the chart parsing algorithm to eliminate spurious parses. Ney (1991) deals with the use of probabilistic CYK parser for continous speech recognition task. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. Chappelier et al. (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems. They also research various techniques of producing sets of hypotheses that contain more “semantic” variability than the commonly used ones. Some of the recent studies about structural language modeling extract a list of N-best hypotheses using an N-gram and then apply structural methods to decide on the best hypothesis (Chelba, 2000; Roark, 2001). This contrasts with the approach presented in this study where, instead of a single sentence, the word-lattice is parsed. Parsing all sentence </context>
</contexts>
<marker>Chappelier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J.-C. Chappelier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In TALN’99, pages 95–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30654" citStr="Charniak, 2001" startWordPosition="5376" endWordPosition="5377">pproach presented in this paper scored 12.6 WER and still needs some improvements. The English analysis grammar that was used in the experiments was designed to parse typed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The main idea is to populate the initial chart parser with the word graph that comes out of the SR component. This paper presents an attempt to blen</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="6991" citStr="Chelba, 2000" startWordPosition="1117" endWordPosition="1118">ic CYK parser for continous speech recognition task. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. Chappelier et al. (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems. They also research various techniques of producing sets of hypotheses that contain more “semantic” variability than the commonly used ones. Some of the recent studies about structural language modeling extract a list of N-best hypotheses using an N-gram and then apply structural methods to decide on the best hypothesis (Chelba, 2000; Roark, 2001). This contrasts with the approach presented in this study where, instead of a single sentence, the word-lattice is parsed. Parsing all sentence hypotheses simultaneously enables a reduction in the number of edges produced during the parsing process. This is because the shared word hypotheses are processed only once compared to the Nbest list approach, where the shared words are processed each time they occur in a hypothesis. Similar to the current work, other studies parse the whole word-lattice without extracting a list (Hall, 2005). A significant distinction between the work o</context>
<context position="24472" citStr="Chelba (2000)" startWordPosition="4215" endWordPosition="4216"> carried out in this paper are run on word graphs based on 1993 benchmark tests for the ARPA spoken language program (Pallett et al., 1994). In the large-vocabulary continuous speech recognition (CSR) tests reported by Pallett et al. (1994), Wall Street Journal-based CSR corpus material was made use of. Those tests intended to measure basic speaker-independent performance on a 64K-word read-speech test set which consists of 213 utterances. Each of the 10 different speakers provided 20 to 23 utterances. An acoustic model and a trigram language model is trained using Wall Street Journal data by Chelba (2000) who also generated the 213 word graphs used in the current experiments. The word graphs, referred as HUB-1 data set, contain both the acoustic scores and the trigram language model scores. Previously, the same data set was used in other s ⎡ ⎤ cat s ⎢ ⎥ ⎢ ⎢form ‘look’ ⎥ ⎥ ⎢ ⎥ ⎢ ⎢tense past ⎥ ⎥ ⎢ ⎡form ‘he’⎤ ⎥ ⎢ ⎥ ⎢subj ⎣⎦ ⎥ ⎢proper plus ⎥ ⎢ ⎢�orm ‘kids’⎤ obleak def plus J ⎥ ⎣ ⎦ pform ‘after’ ⎡ ⎤ [cat v Icat prep] cat det ⎡ ⎤ cat pro tense pas t ⎣⎦ cat n ⎢ ⎢ ⎢proper plus def plus ⎢ ⎥ ⎢ ⎢ ⎢case nom ⎢ ⎢proper minus ⎥ ⎥ ⎢ ⎢ ⎣num sg ⎦ ⎢ ⎥ person 3 ⎢ ⎣num pl ⎦ ⎥ person 3 474 29.64 � 10.00 � � � � � </context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee-Feng Chien</author>
<author>K J Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>An augmented chart data structure with efficient word lattice parsing scheme in speech recognition applications.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th conference on Computational linguistics,</booktitle>
<pages>60--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5544" citStr="Chien et al. (1990)" startWordPosition="886" endWordPosition="889">Recent studies point out the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination with syntactic methods. In this paper, the focus is on systems that contain syntactic approaches. In general, these language modeling approaches try to parse the ASR output in wordlattice format in order to choose the most probable hypothesis. Chow and Roukos (1989) used a unification-based CYK parser for the purpose of speech understanding. Chien et al. (1990) and Weber (1994) utilized probabilistic context free grammars in conjunction with unification grammars to chart-parse a word-lattice. There are various differences between the work of Chien et al. (1990) and Weber (1994) and the work presented in this paper. First, in the previously mentioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the efficiency becomes a big challenge because the search space introduced by a chart with over thousands</context>
</contexts>
<marker>Chien, Chen, Lee, 1990</marker>
<rawString>Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee. 1990. An augmented chart data structure with efficient word lattice parsing scheme in speech recognition applications. In Proceedings of the 13th conference on Computational linguistics, pages 60–65, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee-Feng Chien</author>
<author>K J Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>A best-first language processing model integrating the unification grammar and markov language model for speech recognition applications.</title>
<date>1993</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>1--2</pages>
<marker>Chien, Chen, Lee, 1993</marker>
<rawString>Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee. 1993. A best-first language processing model integrating the unification grammar and markov language model for speech recognition applications. IEEE Transactions on Speech and Audio Processing, 1(2):221–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yen-Lu Chow</author>
<author>Salim Roukos</author>
</authors>
<title>Speech understanding using a unification grammar. In</title>
<date>1989</date>
<booktitle>ICAASP’89: Proc. of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>727--730</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5447" citStr="Chow and Roukos (1989)" startWordPosition="871" endWordPosition="874">t parsing a word graph introduced in this paper has not been previously used for coupling purposes. Recent studies point out the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination with syntactic methods. In this paper, the focus is on systems that contain syntactic approaches. In general, these language modeling approaches try to parse the ASR output in wordlattice format in order to choose the most probable hypothesis. Chow and Roukos (1989) used a unification-based CYK parser for the purpose of speech understanding. Chien et al. (1990) and Weber (1994) utilized probabilistic context free grammars in conjunction with unification grammars to chart-parse a word-lattice. There are various differences between the work of Chien et al. (1990) and Weber (1994) and the work presented in this paper. First, in the previously mentioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the effi</context>
</contexts>
<marker>Chow, Roukos, 1989</marker>
<rawString>Yen-Lu Chow and Salim Roukos. 1989. Speech understanding using a unification grammar. In ICAASP’89: Proc. of the International Conference on Acoustics, Speech, and Signal Processing, pages 727–730. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5025" citStr="Dyer et al., 2008" startWordPosition="802" endWordPosition="805">esearch over the past decade. The idea of chart parsing the word graph in SR systems has been previously used in different studies in order to resolve ambiguity. Tomita (1986) introduced the concept of wordlattice parsing for the purpose of speech recognition and used an LR parser. Next, Paeseler (1988) used a chart parser to process word-lattices. However, to the best of our knowledge, the specific method for chart parsing a word graph introduced in this paper has not been previously used for coupling purposes. Recent studies point out the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination with syntactic methods. In this paper, the focus is on systems that contain syntactic approaches. In general, these language modeling approaches try to parse the ASR output in wordlattice format in order to choose the most probable hypothesis. Chow and Roukos (1989) used a unification-based CYK parser for the purpose of speech understanding. Chien et al. (1990) and Weber (1994) utilized probabilistic context free grammars in conjunction wit</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012– 1020, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Language modelling using efficient best-first bottom-up parsing.</title>
<date>2003</date>
<booktitle>In ASR’03: IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>507--512</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="30509" citStr="Hall and Johnson (2003)" startWordPosition="5350" endWordPosition="5353"> approaches we used the same data set. Table 3 lists the WER for the NIST HUB-1 data set for different approaches including ours. The N-best word graph approach presented in this paper scored 12.6 WER and still needs some improvements. The English analysis grammar that was used in the experiments was designed to parse typed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The</context>
</contexts>
<marker>Hall, Johnson, 2003</marker>
<rawString>Keith Hall and Mark Johnson. 2003. Language modelling using efficient best-first bottom-up parsing. In ASR’03: IEEE Workshop on Automatic Speech Recognition and Understanding, pages 507–512. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Attention shifting for parsing speech.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="30708" citStr="Hall and Johnson, 2004" startWordPosition="5382" endWordPosition="5385">R and still needs some improvements. The English analysis grammar that was used in the experiments was designed to parse typed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The main idea is to populate the initial chart parser with the word graph that comes out of the SR component. This paper presents an attempt to blend statistical SR systems with rule-based MT systems. T</context>
</contexts>
<marker>Hall, Johnson, 2004</marker>
<rawString>Keith Hall and Mark Johnson. 2004. Attention shifting for parsing speech. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 40, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
</authors>
<title>Best-First Word Lattice Parsing: Techniques for Integrated Syntax Language Modeling.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="7545" citStr="Hall, 2005" startWordPosition="1209" endWordPosition="1210">al methods to decide on the best hypothesis (Chelba, 2000; Roark, 2001). This contrasts with the approach presented in this study where, instead of a single sentence, the word-lattice is parsed. Parsing all sentence hypotheses simultaneously enables a reduction in the number of edges produced during the parsing process. This is because the shared word hypotheses are processed only once compared to the Nbest list approach, where the shared words are processed each time they occur in a hypothesis. Similar to the current work, other studies parse the whole word-lattice without extracting a list (Hall, 2005). A significant distinction between the work of Hall (2005) and our study is the parsing algorithm. In contrast to our chart parsing approach augmented by unification based feature structures, Charniak parser is used in Hall (2005)’s along with PCFG. The rest of the paper is organized as follows: In the following section, an overview of the proposed language model is presented. Next, in Section 3, the parsing process of the word-lattice is described in detail. Section 4 describes the experSpeech Recognizer Speech Recognizer Speech Recognizer Rule−based Target Text MT Rule−based Target Text MT </context>
<context position="25420" citStr="Hall, 2005" startWordPosition="4445" endWordPosition="4446">s ⎥ ⎢ ⎢�orm ‘kids’⎤ obleak def plus J ⎥ ⎣ ⎦ pform ‘after’ ⎡ ⎤ [cat v Icat prep] cat det ⎡ ⎤ cat pro tense pas t ⎣⎦ cat n ⎢ ⎢ ⎢proper plus def plus ⎢ ⎥ ⎢ ⎢ ⎢case nom ⎢ ⎢proper minus ⎥ ⎥ ⎢ ⎢ ⎣num sg ⎦ ⎢ ⎥ person 3 ⎢ ⎣num pl ⎦ ⎥ person 3 474 29.64 � 10.00 � � � � � � � � � � � � � � � � � � � � � � � � � 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 β Figure 6: WER for HUB-1 first-best hypotheses obtained using different language-model scaling factors and α = 1. The unsteadiness of the WER for β = 10 needs further investigation. WER 13.32 studies (Chelba, 2000; Roark, 2001; Hall, 2005) for language modeling task in ASR. 4.1 N-best list pruning The 213 word graphs in the HUB-1 data set are pruned as described in Section 3 in order to prepare them for chart parsing. AT&amp;T toolkit (Mohri et al., 1998) is used for determinization and minimization of the word graphs and for n-best path extraction. Prior to feeding in the word graphs to the FSM tools, the acoustic model and the trigram language model in the original lattices are combined into a single score using Equation 1, where S represents the combined score of an arc, A is the acoustic model (AM) score, L is the language mode</context>
<context position="30726" citStr="Hall, 2005" startWordPosition="5387" endWordPosition="5388">ments. The English analysis grammar that was used in the experiments was designed to parse typed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The main idea is to populate the initial chart parser with the word graph that comes out of the SR component. This paper presents an attempt to blend statistical SR systems with rule-based MT systems. The goal of the fin</context>
</contexts>
<marker>Hall, 2005</marker>
<rawString>Keith Hall. 2005. Best-First Word Lattice Parsing: Techniques for Integrated Syntax Language Modeling. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Algorithm schemata and data structures in syntactic processing.</title>
<date>1986</date>
<booktitle>Readings in natural language processing,</booktitle>
<pages>35--70</pages>
<contexts>
<context position="12495" citStr="Kay (1986)" startWordPosition="2042" endWordPosition="2043"> not small enough to meet the time and memory limitations in parsing. N-best list selection can be regarded as the last step in constricting the size. A subset of possible hypotheses is selected among many that are contained in the miniSelected Hypothesis Morphology Rules Syntax Rules Morphological Analysis Chart w/ feature structures Chart Parsing Lexicon 471 mized FSM. The selection mechanism favors only the best hypotheses according to the scores present in the FSM arcs. 3.2 Chart parsing The parsing engine implemented for this work is an active chart parser similar to the one described in Kay (1986). The language grammar that is processed by the parser can be designed top-down, bottom-up or in a combined manner. It employs an agenda to store the edges prior to inserting to the chart. Edges are defined to be either complete or incomplete. Incomplete edges describe the rule state where one or more syntactic categories are expected to be matched. An incomplete edge becomes complete if all syntactic categories on the right-hand-side of the rule are matched. Parsing starts from the rules that are associated to the lexical entries. This corresponds to the bottom-up parsing strategy. Moreover, </context>
</contexts>
<marker>Kay, 1986</marker>
<rawString>Martin Kay. 1986. Algorithm schemata and data structures in syntactic processing. Readings in natural language processing, pages 35–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<marker>Manning, Schütze, 2000</marker>
<rawString>C. D. Manning and H. Schütze. 2000. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>A rational design for a weighted finitestate transducer library.</title>
<date>1998</date>
<booktitle>In WIA ’97: Revised Papers from the Second International Workshop on Implementing Automata,</booktitle>
<pages>144--158</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="25636" citStr="Mohri et al., 1998" startWordPosition="4484" endWordPosition="4487">n 3 ⎢ ⎣num pl ⎦ ⎥ person 3 474 29.64 � 10.00 � � � � � � � � � � � � � � � � � � � � � � � � � 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 β Figure 6: WER for HUB-1 first-best hypotheses obtained using different language-model scaling factors and α = 1. The unsteadiness of the WER for β = 10 needs further investigation. WER 13.32 studies (Chelba, 2000; Roark, 2001; Hall, 2005) for language modeling task in ASR. 4.1 N-best list pruning The 213 word graphs in the HUB-1 data set are pruned as described in Section 3 in order to prepare them for chart parsing. AT&amp;T toolkit (Mohri et al., 1998) is used for determinization and minimization of the word graphs and for n-best path extraction. Prior to feeding in the word graphs to the FSM tools, the acoustic model and the trigram language model in the original lattices are combined into a single score using Equation 1, where S represents the combined score of an arc, A is the acoustic model (AM) score, L is the language model (LM) score, α is the AM scale factor and β is the LM scale factor. S = α A + β L (1) Figure 6 depicts the word error rates for the first-best hypotheses obtained heuristically by using α = 1 and β values from 1 to </context>
</contexts>
<marker>Mohri, Pereira, Riley, 1998</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1998. A rational design for a weighted finitestate transducer library. In WIA ’97: Revised Papers from the Second International Workshop on Implementing Automata, pages 144–158, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
</authors>
<title>Dynamic programming parsing for context-free grammars in continuous speech recognition.</title>
<date>1991</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="6345" citStr="Ney (1991)" startWordPosition="1017" endWordPosition="1018">et al. (1990) and Weber (1994) and the work presented in this paper. First, in the previously mentioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the efficiency becomes a big challenge because the search space introduced by a chart with over thousands of initial edges can easily be beyond current practical limits. Another important difference in our approach is the modification of the chart parsing algorithm to eliminate spurious parses. Ney (1991) deals with the use of probabilistic CYK parser for continous speech recognition task. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. Chappelier et al. (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems. They also research various techniques of producing sets of hypotheses that contain more “semantic” variability than the commonly used ones. Some of the recent studies about structural language modeling extract a list of N-best hypotheses using an N-gram and then apply structural methods </context>
</contexts>
<marker>Ney, 1991</marker>
<rawString>Hermann Ney. 1991. Dynamic programming parsing for context-free grammars in continuous speech recognition. IEEE Transactions on Signal Processing, 39(2):336–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Paeseler</author>
</authors>
<title>Modification of Earley’s algorithm for speech recognition.</title>
<date>1988</date>
<booktitle>In Proceedings of the NATO Advanced Study Institute on Recent advances in speech understanding and dialog systems,</booktitle>
<pages>465--472</pages>
<publisher>SpringerVerlag</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4711" citStr="Paeseler (1988)" startWordPosition="749" endWordPosition="750"> 1. Recognized Text ... N. Recognized Text Figure 1: ASR and rule-based MT coupling: a) First-best b) N-best list c) N-best word graph. While integrating the SR system with the rulebased MT system, this study uses word graphs and chart parsing with new extensions. Parsing of word lattices has been a topic of research over the past decade. The idea of chart parsing the word graph in SR systems has been previously used in different studies in order to resolve ambiguity. Tomita (1986) introduced the concept of wordlattice parsing for the purpose of speech recognition and used an LR parser. Next, Paeseler (1988) used a chart parser to process word-lattices. However, to the best of our knowledge, the specific method for chart parsing a word graph introduced in this paper has not been previously used for coupling purposes. Recent studies point out the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination with syntactic methods. In this paper, the focus is on systems that contain syntactic approaches. In general, these language model</context>
</contexts>
<marker>Paeseler, 1988</marker>
<rawString>A. Paeseler. 1988. Modification of Earley’s algorithm for speech recognition. In Proceedings of the NATO Advanced Study Institute on Recent advances in speech understanding and dialog systems, pages 465–472, New York, NY, USA. SpringerVerlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Pallett</author>
<author>Jonathan G Fiscus</author>
<author>William M Fisher</author>
<author>John S Garofolo</author>
<author>Bruce A Lund</author>
<author>Mark A Przybocki</author>
</authors>
<date>1994</date>
<booktitle>In HLT ’94: Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>49--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9642" citStr="Pallett et al., 1994" startWordPosition="1541" endWordPosition="1544">ral analysis of the word-lattice is accomplished in two consecutive tasks. First, morphological analysis is performed on the word level and any information carried by the word is extracted to be used in the following stages. Second, syntactic analysis is performed on the sentence level. The syntactic analyzer consists of a chart parser in which the rules modeling the language grammar are augmented with functional expressions. 3 Word Graph Processing The word graphs produced by an ASR are beyond the limits of a unification-based chart parser. A small-sized lattice from the NIST HUB-1 data set (Pallett et al., 1994) can easily contain a couple of hundred states and more than one thousand arcs. The largest lattice in the same data set has 25 000 states and almost 1 million arcs. No unificationbased chart parser is capable of coping with an input of this size. It is unpractical and unreasonable to parse the lattice in the same form as it is output from the ASR. Instead, the word graph is pruned to a reasonable size so that it can be parsed according to acceptable time and memory limitations. 3.1 Word graph to FSM conversion The pruning process starts by converting the timestate lattice to a finite state ma</context>
<context position="23998" citStr="Pallett et al., 1994" startWordPosition="4138" endWordPosition="4141">-end position. If two edges have the same score, then the farthest one to the leaf level is preferred. • The candidate list is traversed and a combination with the minimum number of constituents is selected. The edges with the widest span get into the winning combination. • The c-structures and f-structures of the edges in the winning combination are joined into a whole c-structure and f-structure which represent the final parse tree for the input. 4 Experiments The experiments carried out in this paper are run on word graphs based on 1993 benchmark tests for the ARPA spoken language program (Pallett et al., 1994). In the large-vocabulary continuous speech recognition (CSR) tests reported by Pallett et al. (1994), Wall Street Journal-based CSR corpus material was made use of. Those tests intended to measure basic speaker-independent performance on a 64K-word read-speech test set which consists of 213 utterances. Each of the 10 different speakers provided 20 to 23 utterances. An acoustic model and a trigram language model is trained using Wall Street Journal data by Chelba (2000) who also generated the 213 word graphs used in the current experiments. The word graphs, referred as HUB-1 data set, contain </context>
</contexts>
<marker>Pallett, Fiscus, Fisher, Garofolo, Lund, Przybocki, 1994</marker>
<rawString>David S. Pallett, Jonathan G. Fiscus, William M. Fisher, John S. Garofolo, Bruce A. Lund, and Mark A. Przybocki. 1994. In HLT ’94: Proceedings of the workshop on Human Language Technology, pages 49–74, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="7005" citStr="Roark, 2001" startWordPosition="1119" endWordPosition="1120">for continous speech recognition task. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. Chappelier et al. (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems. They also research various techniques of producing sets of hypotheses that contain more “semantic” variability than the commonly used ones. Some of the recent studies about structural language modeling extract a list of N-best hypotheses using an N-gram and then apply structural methods to decide on the best hypothesis (Chelba, 2000; Roark, 2001). This contrasts with the approach presented in this study where, instead of a single sentence, the word-lattice is parsed. Parsing all sentence hypotheses simultaneously enables a reduction in the number of edges produced during the parsing process. This is because the shared word hypotheses are processed only once compared to the Nbest list approach, where the shared words are processed each time they occur in a hypothesis. Similar to the current work, other studies parse the whole word-lattice without extracting a list (Hall, 2005). A significant distinction between the work of Hall (2005) </context>
<context position="25407" citStr="Roark, 2001" startWordPosition="4443" endWordPosition="4444">⎥ ⎢proper plus ⎥ ⎢ ⎢�orm ‘kids’⎤ obleak def plus J ⎥ ⎣ ⎦ pform ‘after’ ⎡ ⎤ [cat v Icat prep] cat det ⎡ ⎤ cat pro tense pas t ⎣⎦ cat n ⎢ ⎢ ⎢proper plus def plus ⎢ ⎥ ⎢ ⎢ ⎢case nom ⎢ ⎢proper minus ⎥ ⎥ ⎢ ⎢ ⎣num sg ⎦ ⎢ ⎥ person 3 ⎢ ⎣num pl ⎦ ⎥ person 3 474 29.64 � 10.00 � � � � � � � � � � � � � � � � � � � � � � � � � 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 β Figure 6: WER for HUB-1 first-best hypotheses obtained using different language-model scaling factors and α = 1. The unsteadiness of the WER for β = 10 needs further investigation. WER 13.32 studies (Chelba, 2000; Roark, 2001; Hall, 2005) for language modeling task in ASR. 4.1 N-best list pruning The 213 word graphs in the HUB-1 data set are pruned as described in Section 3 in order to prepare them for chart parsing. AT&amp;T toolkit (Mohri et al., 1998) is used for determinization and minimization of the word graphs and for n-best path extraction. Prior to feeding in the word graphs to the FSM tools, the acoustic model and the trigram language model in the original lattices are combined into a single score using Equation 1, where S represents the combined score of an arc, A is the acoustic model (AM) score, L is the </context>
<context position="30821" citStr="Roark, 2001" startWordPosition="5404" endWordPosition="5405">ed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The main idea is to populate the initial chart parser with the word graph that comes out of the SR component. This paper presents an attempt to blend statistical SR systems with rule-based MT systems. The goal of the final assembly of these two components was to achieve an enhanced Speech Translation (ST) system. </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="6446" citStr="Stolcke (1995)" startWordPosition="1033" endWordPosition="1034">tioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the efficiency becomes a big challenge because the search space introduced by a chart with over thousands of initial edges can easily be beyond current practical limits. Another important difference in our approach is the modification of the chart parsing algorithm to eliminate spurious parses. Ney (1991) deals with the use of probabilistic CYK parser for continous speech recognition task. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. Chappelier et al. (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems. They also research various techniques of producing sets of hypotheses that contain more “semantic” variability than the commonly used ones. Some of the recent studies about structural language modeling extract a list of N-best hypotheses using an N-gram and then apply structural methods to decide on the best hypothesis (Chelba, 2000; Roark, 2001). This contrasts with the approach presen</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An efficient word lattice parsing algorithm for continuous speech recognition.</title>
<date>1986</date>
<booktitle>Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’86.,</booktitle>
<pages>11--1569</pages>
<contexts>
<context position="4582" citStr="Tomita (1986)" startWordPosition="727" endWordPosition="728">ining all N-best hypotheses. The MT component parses the word graph, thus, all possible hypotheses at one time. Recognized Text 1. Recognized Text ... N. Recognized Text Figure 1: ASR and rule-based MT coupling: a) First-best b) N-best list c) N-best word graph. While integrating the SR system with the rulebased MT system, this study uses word graphs and chart parsing with new extensions. Parsing of word lattices has been a topic of research over the past decade. The idea of chart parsing the word graph in SR systems has been previously used in different studies in order to resolve ambiguity. Tomita (1986) introduced the concept of wordlattice parsing for the purpose of speech recognition and used an LR parser. Next, Paeseler (1988) used a chart parser to process word-lattices. However, to the best of our knowledge, the specific method for chart parsing a word graph introduced in this paper has not been previously used for coupling purposes. Recent studies point out the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination w</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Masaru Tomita. 1986. An efficient word lattice parsing algorithm for continuous speech recognition. Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’86., 11:1569– 1572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Weber</author>
</authors>
<title>Time synchronous chart parsing of speech integrating unification grammars with statistics.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eighth Twente Workshop on Language Technology,</booktitle>
<pages>107--119</pages>
<contexts>
<context position="5561" citStr="Weber (1994)" startWordPosition="891" endWordPosition="893"> the importance of utilizing word graphs in speech tasks (Dyer et al., 2008). Previous work on language modeling can be classified according to whether a system uses purely statistical methods or whether it uses them in combination with syntactic methods. In this paper, the focus is on systems that contain syntactic approaches. In general, these language modeling approaches try to parse the ASR output in wordlattice format in order to choose the most probable hypothesis. Chow and Roukos (1989) used a unification-based CYK parser for the purpose of speech understanding. Chien et al. (1990) and Weber (1994) utilized probabilistic context free grammars in conjunction with unification grammars to chart-parse a word-lattice. There are various differences between the work of Chien et al. (1990) and Weber (1994) and the work presented in this paper. First, in the previously mentioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size. Otherwise, the efficiency becomes a big challenge because the search space introduced by a chart with over thousands of initial edges</context>
</contexts>
<marker>Weber, 1994</marker>
<rawString>Hans Weber. 1994. Time synchronous chart parsing of speech integrating unification grammars with statistics. In Proceedings of the Eighth Twente Workshop on Language Technology, pages 107–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Woodland</author>
</authors>
<title>HTK Speech Recognition Toolkit.</title>
<date>2000</date>
<institution>Cambridge University Engineering Department,</institution>
<location>http://htk.eng.cam.ac.uk.</location>
<contexts>
<context position="8379" citStr="Woodland, 2000" startWordPosition="1343" endWordPosition="1344"> Hall (2005)’s along with PCFG. The rest of the paper is organized as follows: In the following section, an overview of the proposed language model is presented. Next, in Section 3, the parsing process of the word-lattice is described in detail. Section 4 describes the experSpeech Recognizer Speech Recognizer Speech Recognizer Rule−based Target Text MT Rule−based Target Text MT Target Text Rule−based MT 470 iments and reports the obtained results. Finally, Section 5 concludes the paper. 2 Hybrid language modeling The general architecture of the system is depicted in Figure 2. The HTK toolkit (Woodland, 2000) word-lattice file format is used as the default file format in the proposed solution. The word-lattice output from ASR is converted into a finite state machine (FSM). This conversion enables us to benefit from standard theory and algorithms on FSMs. In the converted FSM, non-determinism is removed and it is minimized by eliminating redundant nodes and arcs. Next, the chart is initialized with the deterministic and minimal FSM. Finally, this chart is used in the structural analysis. Speech ASR Word Graph FSM Conversion FSM Minimization Minimized FSM Initialization Initial Chart Figure 2: The h</context>
</contexts>
<marker>Woodland, 2000</marker>
<rawString>Phil Woodland. 2000. HTK Speech Recognition Toolkit. Cambridge University Engineering Department, http://htk.eng.cam.ac.uk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>191--198</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30761" citStr="Xu et al., 2002" startWordPosition="5392" endWordPosition="5395">ammar that was used in the experiments was designed to parse typed text containing punctuation information. The speech data, however, does not contain any punctuation. Therefore, the grammar has to be adjusted accordingly to improve the WER. Another common source of error in parsing is because of unnormalized text. Table 3: WER taken from Hall and Johnson (2003) for various language models on HUB-1 lattices in addition to our approach presented in the fifth row. Model WER Charniak Parser (Charniak, 2001) 11.8 Attention Shifting 11.9 (Hall and Johnson, 2004) PCFG (Hall, 2005) 12.0 A* decoding (Xu et al., 2002) 12.3 N-best word graph (this study) 12.6 PCFG (Roark, 2001) 12.7 PCFG (Hall and Johnson, 2004) 13.0 40m-word trigram 13.7 (Hall and Johnson, 2003) PCFG (Hall and Johnson, 2003) 15.5 5 Conclusions The primary aim of this research was to propose a new and efficient method for integrating an SR system with an MT system employing a chart parser. The main idea is to populate the initial chart parser with the word graph that comes out of the SR component. This paper presents an attempt to blend statistical SR systems with rule-based MT systems. The goal of the final assembly of these two components</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 191–198. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>