<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.989809">
Appropriately Handled Prosodic Breaks Help PCFG Parsing
</title>
<author confidence="0.999617">
Zhongqiang Huang1, Mary Harper1,2
</author>
<affiliation confidence="0.9636662">
1Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD USA
2Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
</affiliation>
<email confidence="0.997737">
{zqhuang,mharper}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.995623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786230769231">
This paper investigates using prosodic infor-
mation in the form of ToBI break indexes for
parsing spontaneous speech. We revisit two
previously studied approaches, one that hurt
parsing performance and one that achieved
minor improvements, and propose a new
method that aims to better integrate prosodic
breaks into parsing. Although these ap-
proaches can improve the performance of ba-
sic probabilistic context free grammar (PCFG)
parsers, they all fail to produce fine-grained
PCFG models with latent annotations (PCFG-
LA) (Matsuzaki et al., 2005; Petrov and Klein,
2007) that perform significantly better than the
baseline PCFG-LA model that does not use
break indexes, partially due to mis-alignments
between automatic prosodic breaks and true
phrase boundaries. We propose two alterna-
tive ways to restrict the search space of the
prosodically enriched parser models to the n-
best parses from the baseline PCFG-LA parser
to avoid egregious parses caused by incor-
rect breaks. Our experiments show that all
of the prosodically enriched parser models can
then achieve significant improvement over the
baseline PCFG-LA parser.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999255">
Speech conveys more than a sequence of words to
a listener. An important additional type of informa-
tion that phoneticians investigate is called prosody,
which includes phenomena such as pauses, pitch,
energy, duration, grouping, and emphasis. For a
review of the role of prosody in processing spo-
ken language, see (Cutler et al., 1997). Prosody
can help with the disambiguation of lexical meaning
</bodyText>
<page confidence="0.990565">
37
</page>
<bodyText confidence="0.999955694444444">
(via accents and tones) and sentence type (e.g., yes-
no question versus statement), provide discourse-
level information like focus, prominence, and dis-
course segment, and help a listener to discern a
speaker’s emotion or hesitancy, etc. Prosody often
draws a listener’s attention to important information
through contrastive pitch or duration patterns associ-
ated words or phrases. In addition, prosodic cues can
help one to segment speech into chunks that are hy-
pothesized to have a hierarchical structure, although
not necessarily identical to that of syntax. This sug-
gests that prosodic cues may help in the parsing of
speech inputs, the topic of this paper.
Prosodic information such as pause length, du-
ration of words and phones, pitch contours, en-
ergy contours, and their normalized values have
been used in speech processing tasks like sentence
boundary detection (Liu et al., 2005). In contrast,
other researchers use linguistic encoding schemes
like ToBI (Silverman et al., 1992), which encodes
tones, the degree of juncture between words, and
prominence symbolically. For example, a simplified
ToBI encoding scheme uses the symbol 4 for ma-
jor intonational breaks, p for hesitation, and 1 for all
other breaks (Dreyer and Shafran, 2007). In the lit-
erature, there have been several attempts to integrate
prosodic information to improve parse accuracy of
speech transcripts. These studies have used either
quantized acoustic measurements of prosody or au-
tomatically detected break indexes.
Gregory et al. (2004) attempted to integrate quan-
tized prosodic features as additional tokens in the
same manner that punctuation marks are added
into text. Although punctuation marks can signif-
icantly improve parse accuracy of newswire text,
the quantized prosodic tokens were found harm-
</bodyText>
<note confidence="0.6374655">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37–45,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999839371428571">
ful to parse accuracy when inserted into human-
generated speech transcripts of the Switchboard cor-
pus. The authors hypothesized that the inserted
pseudo-punctuation break n-gram dependencies in
the parser model, leading to lower accuracies. How-
ever, another possible cause is that the prosody has
not been effectively utilized due to the fact that
it is overloaded; it not only provides information
about phrases, but also about the state of the speaker
and his/her sentence planning process. Hence, the
prosodic information may at times be more harmful
than helpful to parsing performance.
In a follow-on experiment, Kahn et al. (2005), in-
stead of using raw quantized prosodic features, used
three classes of automatically detected ToBI break
indexes (1, 4, or p) and their posteriors. Rather than
directly incorporating the breaks into the parse trees,
they used the breaks to generate additional features
for re-ranking the n-best parse trees from a gener-
ative parsing model trained without prosody. They
were able to obtain a significant 0.6% improvement
on Switchboard over the generative parser, and a
more modest 0.1% to 0.2% improvement over the
reranking model that also utilizes syntactic features.
Dreyer and Shafran (2007) added prosodic breaks
into a generative parsing model with latent vari-
ables. They utilized three classes of ToBI break in-
dexes (1, 4, and p), automatically predicted by the
approach described in (Dreyer and Shafran, 2007;
Harper et al., 2005). Breaks were modeled as a se-
quence of observations parallel to the sentence and
each break was generated by the preterminal of the
preceding word, assuming that the observation of a
break, b, was conditionally independent of its pre-
ceding word, w, given preterminal X:
</bodyText>
<equation confidence="0.999864">
P(w,b|X) = P(w|X)P(b|X) (1)
</equation>
<bodyText confidence="0.999860289473684">
Their approach has advantages over (Gregory et al.,
2004) in that it does not break n-gram dependencies
in parse modeling. It also has disadvantages in that
the breaks are modeled by preterminals rather than
higher level nonterminals, and thus cannot directly
affect phrasing in a basic PCFG grammar. How-
ever, they addressed this independence drawback by
splitting each nonterminal into latent tags so that the
impact of prosodic breaks could be percolated into
the phrasing process through the interaction of la-
tent tags. They achieved a minor 0.2% improvement
over their baseline model without prosodic cues and
also found that prosodic breaks can be used to build
more compact grammars.
In this paper, we re-investigate the models of
(Gregory et al., 2004) and (Dreyer and Shafran,
2007), and propose a new way of modeling that
can potentially address the shortcomings of the two
previous approaches. We also attribute part of the
failure or ineffectiveness of the previously investi-
gated approaches to errors in the quantized prosodic
tokens or automatic break indexes, which are pre-
dicted based only on acoustic cues and could mis-
align with phrase boundaries. We illustrate that
these prosodically enriched models are in fact highly
effective if we systematically eliminate bad phrase
and hesitation breaks given their projection onto the
reference parse trees. Inspired by this, we pro-
pose two alternative rescoring methods to restrict
the search space of the prosodically enriched parser
models to the n-best parses from the baseline PCFG-
LA parser to avoid egregious parse trees. The effec-
tiveness of our rescoring method suggests that the
reranking approach of (Kahn et al., 2005) was suc-
cessful not only because of their prosodic feature de-
sign, but also because they restrict the search space
for reranking to n-best lists generated by a syntactic
model alone.
</bodyText>
<sectionHeader confidence="0.993485" genericHeader="introduction">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.963217913043478">
Due to our goal of investigating the effect of
prosodic information on the accuracy of state of the
art parsing of conversational speech, we utilize both
Penn Switchboard (Godfrey et al., 1992) and Fisher
treebanks (Harper et al., 2005; Bies et al., 2006), for
which we also had automatically generated break in-
dexes from (Dreyer and Shafran, 2007; Harper et al.,
2005)1. The Fisher treebank is a higher quality pars-
ing resource than Switchboard due to its greater use
of audio and refined specifications for sentence seg-
mentation and disfluency markups, and so we utilize
its eval set for our parser evaluation; the first 1,020
trees (7,184 words) were used for development and
the remaining 3,917 trees (29,173 words) for eval-
uation. We utilized the Fisher dev1 and dev2 sets
containing 16,519 trees (112,717 words) as the main
training data source and used the Penn Switchboard
1A small fraction of words in the Switchboard treebank
could not be aligned with the break indexes that were produced
based on a later refinement of the transcription. We chose not
to alter the Switchboard treebank, so in cases of missing break
values, we heuristically added break *1* to words in the middle
of a sentence and *4* to words that end a sentence.
</bodyText>
<page confidence="0.998604">
38
</page>
<bodyText confidence="0.9999625">
treebank containing 110,504 trees (837,863 words)
as an additional training source to evaluate the ef-
fect of training data size on parsing performance.
The treebank trees are normalized by downcasing
all terminal strings and deleting punctuation, empty
nodes, and nonterminal-yield unary rules that are not
related to edits.
We will compare2 three prosodically enriched
PCFG models described in the next section, with a
baseline PCFG parser. We will also utilize a state
of the art PCFG-LA parser (Petrov and Klein, 2007;
Huang and Harper, 2009) to examine the effect of
prosodic enrichment3. Unlike (Kahn et al., 2005),
we do not remove EDITED regions prior to parsing
because parsing of EDITED regions is likely to ben-
efit from prosodic information. Also, parses from all
models are compared with the gold standard parses
in the Fisher evaluation set using SParseval bracket
scoring (Harper et al., 2005; Roark et al., 2006)
without flattening the EDITED constituents.
</bodyText>
<sectionHeader confidence="0.90079" genericHeader="method">
3 Methods of Integrating Breaks
</sectionHeader>
<bodyText confidence="0.921934034482759">
Rather than using quantized raw acoustic features as
in (Gregory et al., 2004), we use automatically gen-
erated ToBI break indexes as in (Dreyer and Shafran,
2007; Kahn et al., 2005) as the prosodic cues, and
investigate three alternative methods of modeling
prosodic breaks. Figure 1 shows parse trees for the
four models for processing the spontaneous speech
transcription she’s she would do, where the speaker
hesitated after saying she’s and then resumed with
another utterance she would do. Each word input
into the parser has an associated break index repre-
sented by the symbol 1, 4, or p enclosed in asterisks
indicating the break after the word. The automat-
ically detected break *4* after the contraction is a
strong indicator of an intonational phrase boundary
that might provide helpful information for parsing if
modeled appropriately. Figure 1 (a) shows the ref-
erence parse tree (thus the name REGULAR) where
the break indexes are not utilized.
The first method to incorporate break indexes,
BRKINSERT, shown in Figure 1 (b), treats the *p*
and *4* breaks as tokens, placing them under the
2We use Bikel’s randomized parsing evaluation comparator
to determine the significance (p &lt; 0.005) of the difference be-
tween two parsers’ outputs.
3Due to the randomness of parameter initialization in the
learning of PCFG-LA models with increasing numbers of latent
tags, we train each latent variable grammar with 10 different
seeds and report the average F score on the evaluation set.
</bodyText>
<figure confidence="0.998665">
(a) REGULAR (b) BRKINSERT
(c) BRKPOS (d) BRKPHRASE
</figure>
<figureCaption confidence="0.999993">
Figure 1: Modeling Methods
</figureCaption>
<bodyText confidence="0.99990376">
highest nonterminal nodes so that the order of words
and breaks remain unchanged in the terminals. This
is similar to (Gregory et al., 2004), except that auto-
matically generated ToBI breaks are used rather than
quantized raw prosodic tokens.
The second method, BRKPOS, shown in Fig-
ure 1 (c), treats breaks as a sequence of observa-
tions parallel to the words in the sentence as in
(Dreyer and Shafran, 2007). The dotted edges in
Figure 1 (c) represent the relation between pretermi-
nals and prosodic breaks, and we call them prosodic
rewrites, with analogy to grammar rewrites and lex-
ical rewrites. The generation of words and prosodic
breaks is assumed to be conditionally independent
given the preterminal, as in Equation 1.
The third new method, BRKPHRASE, shown in
Figure 1 (d), also treats breaks as a sequence of ob-
servations parallel to the sentence; however, rather
than associating the prosodic breaks with the preter-
minals, each is generated by the highest nonterminal
(including preterminal) in the parse tree that covers
the preceding word as the right-most terminal. The
observation of break, b, is assumed to be condition-
ally independent of grammar or lexical rewrite, r,
given the nonterminal X:
</bodyText>
<equation confidence="0.992979">
P(r, b|X) = P(r|X)P(b|X) (2)
</equation>
<bodyText confidence="0.999728125">
The relation is indicated by the dotted edges in Fig-
ure 1 (d), and it is also called a prosodic rewrite.
The potential advantage of BRKPHRASE is that it
does not break or fragment n-gram dependencies of
the grammar rewrites, as in the BRKINSERT method,
and it directly models the dependency between
breaks and phrases, which the BRKPOS method ex-
plicitly lacks.
</bodyText>
<sectionHeader confidence="0.968622" genericHeader="method">
4 Model Training
</sectionHeader>
<bodyText confidence="0.999917">
Since automatically generated prosodic breaks are
incorporated into the parse trees deterministi-
</bodyText>
<figure confidence="0.999613553846154">
*1*
’s *4* she *1*would*1* *4*
do
S
EDITED
S
EDITED
S
NP
PRP
she
VP
S
BREAK VP BREAK
VP
NP
VP
NP
VP
NP
VP
VBZ
PRP
MD
VB
PRP
VBZ
PRP
MD VB
she *1* ’s *4* she *1* would do
*1* *4*
she
*1*
’s *4* she *1*would*1* *4*
do
*1*
’s *4* she *1*would*1* *4*
do
S
S
EDITED
EDITED
S
NP
PRP
she
VP
S
VP
VP
NP
VP
NP
VP
NP
VP
VBZ
PRP
MD
VB
PRP
VBZ
PRP
MD
VB
</figure>
<page confidence="0.995957">
39
</page>
<bodyText confidence="0.999872962962963">
cally for all of the three enrichment methods
(BRKINSERT, BRKPOS, and BRKPHRASE), train-
ing a basic PCFG is straightforward; we simply pull
the counts of grammar rules, lexical rewrites, or
prosodic rewrites from the treebank and normalize
them to obtain their probabilities.
As is well known in the parsing community, the
basic PCFG does not provide state-of-the-art per-
formance due to its strong independence assump-
tions. We can relax these assumptions by explicitly
incorporating more information into the conditional
history, as in Charniak’s parser (Charniak, 2000);
however, this would require sophisticated engineer-
ing efforts to decide what to include in the history
and how to smooth probabilities appropriately due
to data sparsity. In this paper, we utilize PCFG-LA
models (Matsuzaki et al., 2005; Petrov and Klein,
2007) that split each nonterminal into a set of latent
tags and learn complex dependencies among the la-
tent tags automatically during training. The result-
ing model is still a PCFG, but it is probabilistically
context free on the latent tags, and the interaction
among the latent tags is able to implicitly capture
higher order dependencies among the original non-
terminals and observations. We follow the approach
in (Huang and Harper, 2009) to train the PCFG-LA
models.
</bodyText>
<sectionHeader confidence="0.995485" genericHeader="method">
5 Parsing
</sectionHeader>
<bodyText confidence="0.999883888888889">
In a basic PCFG without latent variables, the goal
of maximum probability parsing is to find the most
likely parse tree given a sentence based on the gram-
mar. Suppose our grammar is binarized (so it con-
tains only unary and binary grammar rules). Given
an input sentence wn1 = w1, w2, · · · , wn, the inside
probability, P(i, j, X), of the most likely sub-tree
that is rooted at nonterminal X and generates sub-
sequence wji can be computed recursively by:
</bodyText>
<equation confidence="0.999190666666667">
P(i, j, X) = max(max P(i, j, Y )P(X —* Y ),
Y
P(i, k,Y)P(k + 1, j, Z)P(X —* Y Z)) (3)
</equation>
<bodyText confidence="0.998489833333334">
Backtracing the search process then returns the most
likely parse tree for the REGULAR grammar.
The same parsing algorithm can be directly ap-
plied to the BRKINSERT grammar given that the
break indexes are inserted appropriately into the in-
put sentence as additional tokens. Minor modifica-
tion is needed to extend the same parsing algorithm
to the BRKPOS grammar. The only difference is that
the inside probability of a preterminal is set accord-
ing to Equation 1. The rest of the algorithm proceeds
as in Equation 3.
However, parsing with the BRKPHRASE grammar
is more complicated because whether a nonterminal
generates a break or not is determined by whether
it is the highest nonterminal that covers the preced-
ing word as its right-most terminal. In this case,
the input observation also contains a sequence of
break indexes bn1 = b1, b2, · · · , bn that is parallel
to the input sentence wn1 = w1, w2, · · · , wn. Let
P(i, j, X, 0) be the probability of the most likely
sub-tree rooted at nonterminal X over span (i, j)
that generates word sequence wji , as well as break
index sequence bj−1
i , excluding bj. According to
the independence assumption in Equation 2, with
the addition of prosodic edge X —* bj, the same
sub-tree also has the highest probability, denoted by
P(i, j, X, 1), of generating word sequence wji to-
gether with the break index sequence bji . Thus we
have:
</bodyText>
<equation confidence="0.983887">
P(i, j, X,1) = P(i, j, X, 0)P(bjJX) (4)
</equation>
<bodyText confidence="0.925570481481482">
The structural constraint that a break index is only
generated by the highest nonterminal that covers
the preceding word as the right-most terminal en-
ables a dynamic programming algorithm to compute
P(i, j, X, 0) and thus P(i, j, X,1) efficiently. If the
sub-tree (without the prosodic edge that generates
bj) over span (i, j) is constructed from a unary rule
rewrite X —* Y , then the root nonterminal Y of
some best sub-tree over the same span (i, j) can not
generate break bj because it has a higher nontermi-
nal X that also covers word wj as its right-most ter-
minal. If the sub-tree is constructed from a binary
rule rewrite X —* Y Z, then the root nonterminal Y
of some best sub-tree over some span (i, k) will gen-
erate break bk because Y is the highest nonterminal
that covers word wk as the right-most terminal4. In
contrast, the root nonterminal Z of some best sub-
tree over some span (k+1, j) can not generate break
bj because Z has a higher nonterminal X that also
covers word wj as its right-most terminal. Hence,
4Use of left-branching is required for the BRKPHRASE
method to ensure that the prosodic breaks are associated with
the original nonterminals, not intermediate nonterminals in-
troduced by binarization. Binarization is needed for efficient
parametrization of PCFG-LA models and left- versus right-
branching binarization does not significantly affect model per-
formance; hence, we use left-branching for all models.
</bodyText>
<equation confidence="0.5841998">
max
i&lt;k&lt;j,Y,Z
40
P(i, j, X,1) and P(i, j, X, 0) can be computed re-
cursively by Equation 4 above and Equation 5 be-
low:
P(i, j, X, 0) = max(max P(i, j, Y, 0)P(X → Y ),
Y
max P(i, k, Y, 1)P(k + 1, j, Z, 0)P(X → Y Z)) (5)
i&lt;k&lt;j,Y,Z
</equation>
<bodyText confidence="0.999028933333333">
Although dynamic programming algorithms exist
for maximum probability decoding of basic PCFGs
without latent annotations for all four methods, it is
an NP hard problem to find the most likely parse tree
using PCFG-LA models. Several alternative decod-
ing algorithms have been proposed in the literature
for parsing with latent variable grammars. We use
the best performing max-rule-product decoding al-
gorithm, which searches for the best parse tree that
maximizes the product of the posterior rule (either
grammar, lexical, or prosodic) probabilities, as de-
scribed in (Petrov and Klein, 2007) for our models
with latent annotations and extend the dynamic pars-
ing algorithm described in Equation 5 for the BRK-
PHRASE grammar with latent annotations.
</bodyText>
<sectionHeader confidence="0.993403" genericHeader="method">
6 Results on the Fisher Corpus
</sectionHeader>
<subsectionHeader confidence="0.995875">
6.1 Prosodically Enriched Models
</subsectionHeader>
<bodyText confidence="0.999936">
Table 1 reports the parsing accuracy of the four basic
PCFGs without latent annotations when trained on
the Fisher training data. All of the grammars have a
low F score of around 65% due to the overly strong
and incorrect independence assumptions. We ob-
serve that the BRKPHRASE grammar benefits most
from breaks, significantly improving the baseline
accuracy from 64.9% to 67.2%, followed by the
BRKINSERT grammar, which at 66.2% achieves a
smaller improvement. The BRKPOS grammar ben-
efits the least among the three because breaks are
attached to the preterminals and thus have less im-
pact on phrasing due to the independence assump-
tions in the basic PCFG. In contrast, both the BRK-
PHRASE and BRKINSERT methods directly model
the relationship between breaks and phrase bound-
aries through governing nonterminals; however, the
BRKPHRASE method does not directly change any
of the grammar rules in contrast to the BRKINSERT
method that more or less breaks n-gram dependen-
cies and fragments rule probabilities.
The bars labeled DIRECT in Figure 2 report the
parsing performance of the four PCFG-LA models
trained on Fisher. The introduction of latent anno-
tations significantly boosts parsing accuracies, pro-
viding relative improvements ranging from 16.8%
</bodyText>
<table confidence="0.4409835">
BRKPOS BRKPHRASE
66.2 65.2
</table>
<tableCaption confidence="0.989096666666667">
Table 1: Fisher evaluation parsing results for the basic
PCFGs without latent annotations trained on the Fisher
training set.
</tableCaption>
<bodyText confidence="0.964442216216216">
up to 19.0% when trained on Fisher training data
due to the fact that the PCFG-LA models are able
to automatically learn more complex dependencies
not captured by basic PCFGs.
Figure 2: Parsing results on the Fisher evaluation set
of the PCFG-LA models trained on the Fisher training
data. The DIRECT bars represent direct parsing results for
models trained and evaluated on the original data, ORA-
CLE bars for models trained and evaluated on the modi-
fied oracle data (see Subsection 6.2), and the ORACLE-
RESCORE and DIRECTRESCORE bars for results of the
two rescoring approaches (described in Subsection 6.3)
on the original evaluation data.
However, the prosodically enriched methods do
not significantly improve upon the REGULAR base-
line after the introduction of latent annotations. The
BRKPHRASE method only achieves a minor in-
significant 0.1% improvement over the REGULAR
baseline; whereas, the BRKINSERT method is a sig-
nificant 0.7% worse than the baseline. Similar re-
sults for BRKINSERT were reported in (Gregory et
al., 2004), where they attributed the degradation to
the fact that the insertion of the prosodic “punctua-
tion” breaks the n-gram dependencies. Another pos-
sible cause is that the insertion of “bad” breaks that
do not align with true phrase boundaries hurts per-
formance more than the benefits gained from “good”
breaks due to the tightly integrated relationship be-
tween phrases and breaks. For the BRKPOS method,
the impact of break indexes is implicitly percolated
to the nonterminals through the interaction among
latent tags, as discussed in (Dreyer and Shafran,
2007), and its performance may thus be less affected
by the “bad” breaks. With latent annotations (in con-
trast to the basic PCFG), the model is now signif-
icantly better than BRKINSERT and is on par with
BRKPHRASE.
</bodyText>
<figure confidence="0.98929747826087">
REGULAR BRKINSERT BRKPOS BRKPHRASE
DIRECT ORACLERESCORE
ORACLE DIRECTRESCORE
85.5
84.5
83.5
82.5
83.9
83.2
84.2 84.2
84.4
84.0
85.0
84.5
84.7
84.0
85.1
84.7
84.8
REGULAR
BRKINSERT
64.9
67.2
</figure>
<page confidence="0.994733">
41
</page>
<subsectionHeader confidence="0.999539">
6.2 Models with Oracle Breaks
</subsectionHeader>
<bodyText confidence="0.999959">
In order to determine whether “bad” breaks limit
the improvements in parsing performance from
prosodic enrichment, we conducted a simple ora-
cle experiment where all *p* and *4* breaks that
did not align with phrase boundaries in the tree-
bank were systematically converted to *1* breaks5.
When trained and evaluated on this modified ora-
cle data, all three prosodically enriched latent vari-
able models improve by about 1% and were then
able to achieve significant improvements over the
REGULAR PCFG-LA baseline, as shown by the bars
labeled ORACLE in Figure 2. It should be noted,
however, that the BRKINSERT method is much less
effective than the other two methods in the oracle
experiment, suggesting that broken n-gram depen-
dencies affect the model in addition to the erroneous
breaks.
</bodyText>
<subsectionHeader confidence="0.998655">
6.3 N-Best Re-Scoring
</subsectionHeader>
<bodyText confidence="0.999962">
As mentioned previously, prosody does not only
provide information about phrases, but also about
the state of the speaker and his/her sentence plan-
ning process. Given that our break detector uti-
lizes only acoustic knowledge to predict breaks, the
recognized *p* and *4* breaks may not correctly
reflect hesitations and phrase boundaries. Incor-
rectly recognized breaks could hurt parsing more
than the benefit brought from the correctly recog-
nized breaks, as demonstrated by superior perfor-
mance of the prosodically enhanced models in the
oracle experiment. We next describe two alternative
methods to make better use of automatic breaks.
In the first approach, which is called ORACLE-
RESCORE, we train the prosodically enhanced
grammars on cleaned-up break-annotated training
data, where misclassified *p* and *4* breaks are
converted to *1* breaks (as in the oracle experi-
ment). If these grammars were used to directly parse
the test sentences with automatically detected (un-
modified) breaks, the results would be quite poor
due to mismatch between the training and testing
conditions. However, we can automatically bias
against potentially misclassified *p* and *4* breaks
if we utilize information provided by n-best parses
from the baseline REGULAR PCFG-LA grammar.
</bodyText>
<footnote confidence="0.5168155">
5Other sources of errors include misclassification of *p*
breaks as *1* or *4* and misclassification of *4* breaks as *1*
or *p*. Although these errors are not repaired in the oracle ex-
periment, fixing them could potentially provide greater gains.
</footnote>
<bodyText confidence="0.999424543478261">
For each hypothesized parse tree in the n-best list,
the *p* and *4* breaks that do not align with the
phrase boundaries of the hypothesized parse tree are
converted to *1* breaks, and then a new score is
computed using the product of posterior rule proba-
bilities6, as in the max-rule-product criterion, for the
hypothesized parse tree using the grammars trained
on the cleaned-up training data. In this approach,
we convert the posterior probability, P(T |W, B),
of parse tree T given words W and breaks B
to P(B′|W, B)P(T |W, B′), where B′ is the new
break sequence constrained by T, and simplify it to
P(T |W, B′), assuming that conversions to a new se-
quence of breaks as constrained by a hypothesized
parse tree are equally probable given the original se-
quence of breaks. We consider this to be a reason-
able assumption for a small n-best (n = 50) list with
reasonably good quality.
In the second approach, called DIRECTRESCORE,
we train the prosodically enhanced PCFG-LA mod-
els using unmodified, automatic breaks, and then
use them to rescore the n-best lists produced by
the REGULAR PCFG-LA model to avoid the poorer
parse trees caused by fully trusting automatic break
indexes. The size of the n-best list should not be too
small or too large, or the results would be like di-
rectly parsing with REGULAR when n = 1 or with
the prosodically enriched model when n —* oc.
The ORACLERESCORE and DIRECTRESCORE
bars in Figure 2 report the performance of the
prosodically enriched models with the correspond-
ing rescoring method. Both methods use the same
50-best lists produced by the baseline REGULAR
PCFG-LA model using the max-rule-product cri-
terion. Both rescoring methods produce signifi-
cant improvements in the performance of all three
prosodically enriched PCFG-LA models. The pre-
viously ineffective (0.7% worse than REGULAR)
BRKINSERT PCFG-LA model is now 0.3% and
0.5% better than the REGULAR baseline using
the ORACLERESCORE and DIRECTRESCORE ap-
proaches, respectively. The best performing BRK-
POS and BRKPHRASE rescoring models are 0.6-
0.9% better than the REGULAR baseline. It is in-
teresting to note that rescoring with models trained
on cleaned up prosodic breaks is somewhat poorer
</bodyText>
<footnote confidence="0.99703325">
6The product of posterior rule probabilities of a parse tree
is more suitable for rescoring than the joint probability of the
parse tree and the observables (words and breaks) because the
breaks are possibly different for different trees.
</footnote>
<page confidence="0.99754">
42
</page>
<figure confidence="0.351230666666667">
F(BRKPHRASE+DIRECTRESCORE)-F(RECULAR)
than models trained using all automatic breaks.
7 Models with Augmented Training Data
</figure>
<bodyText confidence="0.997464352941177">
Figure 3 reports the evaluation results for mod-
els that are trained on the combination of Fisher
and Switchboard training data. With the additional
Switchboard training data, the nonterminals can be
split into more fine-grained latent tags, enabling the
learning of deeper dependencies without over-fitting
the limited sized Fisher training data. This improved
all models by at least 2.6% absolute. Note also that
the patterns observed for models trained using the
larger training set are quite similar to those from us-
ing the smaller training set in Figure 2. The prosod-
ically enriched models all benefit significantly from
the oracle breaks and from the rescoring methods.
The BRKPOS and BRKPHRASE methods, with the
additional training data, also achieve significant im-
provements over the REGULAR baseline without
rescoring.
</bodyText>
<figureCaption confidence="0.996603333333333">
Figure 3: Parsing results on the Fisher evaluation set of
the PCFG-LA models trained on the Fisher+Switchboard
training data.
</figureCaption>
<sectionHeader confidence="0.99201" genericHeader="method">
8 Error Analysis
</sectionHeader>
<bodyText confidence="0.996844235294117">
In this section, we compare the errors of the
BRKPHRASE PCFG-LA model and the DIRECT-
RESCORE approach for that model to each other and
to the baseline PCFG-LA model without prosodic
breaks. All models are trained and tested on Fisher
as in Section 6. The results using other prosodically
enhanced PCFG-LA models and their rescoring al-
ternatives show similar patterns.
Figure 4 depicts the difference in F scores be-
tween BRKPHRASE and REGULAR and between
BRKPHRASE+DIRECTRESCORE and REGULAR on
a tree-by-tree basis in a 2D plot. Each quad-
rant also contains +/– signs roughly describing how
much BRKPHRASE+DIRECTRESCORE is better (+)
or worse (–) than BRKPHRASE and a pair of num-
bers (a, b), in which a represents the percentage of
sentences in that quadrant containing *p* or *4*
</bodyText>
<figure confidence="0.866484">
-20 -15 -10 -5 0 5 10 15 20
F(BRKPHRA9E)-F(REGULAR)
</figure>
<figureCaption confidence="0.683106375">
Figure 4: 2D plot of the difference in F scores be-
tween BRKPHRASE and REGULAR and between BRK-
PHRASE+DIRECTRESCORE and REGULAR, on a tree-
by-tree basis, where each dot represents a test sentence.
Each quadrant also contains +/– signs roughly describ-
ing how much BRKPHRASE+DIRECTRESCORE is better
(+) or worse (–) than BRKPHRASE and a pair of numbers
(a, b), in which a represents the percentage of sentences
</figureCaption>
<bodyText confidence="0.97964564516129">
in that quadrant containing *p* or *4* breaks that do not
align with true phrase boundaries, and b represents the
percentage of such *p* and *4* breaks among the total
number of *p* and *4* breaks in that quadrant.
breaks that do not align with true phrase bound-
aries, and b represents the percentage of such *p*
and *4* breaks among the total number of *p* and
*4* breaks in that quadrant.
Each dot in the top-right quadrant represents a
test sentence for which both BRKPHRASE and BRK-
PHRASE+DIRECTRESCORE produce better trees
than the baseline REGULAR PCFG-LA model. The
BRKPHRASE+DIRECTRESCORE approach is on av-
erage slightly worse than the BRKPHRASE method
(hence the single minus sign), although it also often
produces better parses than BRKPHRASE alone. In
contrast, the BRKPHRASE+DIRECTRESCORE ap-
proach on average makes many fewer errors than
BRKPHRASE (hence ++) as can be observed in the
bottom-left quadrant, where both approaches pro-
duce worse parse trees than the REGULAR base-
line. The most interesting quadrant is on the top-left
where the BRKPHRASE approach always produces
worse parses than the REGULAR baseline while the
BRKPHRASE+DIRECTRESCORE approach is able
to avoid these errors while producing better parses
than the baseline (hence +++). Although the BRK-
PHRASE+DIRECTRESCORE approach can also pro-
duce worse parses than REGULAR, as in the bottom-
right quadrant (hence –––), altogether the quad-
rants suggest that, by restricting the search space
</bodyText>
<figure confidence="0.994043685714286">
REGULAR BRKINSERT BRKPOS BRKPHRASE
DIRECT ORACLERESCORE
ORACLE DIRECTRESCORE
88
87
86
85
86.5
86.3
87.4
86.8
87.1
86.8
87.7
87.2 87.3
86.8
87.5
87.2 87.3
20
15
10
5
0
-5
-10
-15
-20
+++
(70.2%, 30.0%)
++
(48.2%, 27.6%)
-
(47.2%, 25.3%)
---
(66.7%, 28.1%)
</figure>
<page confidence="0.998754">
43
</page>
<table confidence="0.998270666666667">
REGULAR BRKPHRASE BRKPHRASE
+DIRECTRESCORE
NP 90.4 90.4 90.9
VP 84.7 84.7 85.6
S 84.4 84.3 85.2
INTJ 93.0 93.4 93.4
PP 76.5 76.7 77.9
EDITED 60.4 62.2 63.3
SBAR 67.2 67.0 68.8
</table>
<bodyText confidence="0.9995415">
to the n-best lists produced by the baseline REG-
ULAR parser, the BRKPHRASE+DIRECTRESCORE
approach is able to avoid many bad parses trees
at the expense of somewhat poorer parses in cases
when BRKPHRASE is able to benefit from the full
search space.
The reader should note that the top-left quadrant
of Figure 4 has the highest percentage (70.2%) of
sentences with “bad” *p* and *4* breaks and the
highest percentage (30.0%) of such “bad” breaks
among all breaks. This evidence supports our argu-
ment that “bad” breaks are harmful to parsing per-
formance and some parse errors caused by mislead-
ing breaks can be resolved by limiting the search
space of the prosodically enriched models to the
n-best lists produced by the baseline REGULAR
parser. However, the significant presence of “bad”
breaks in the top-right quadrant also suggests that
the prosodically enriched models are able to pro-
duce better parses than the baseline despite the pres-
ence of “bad” breaks, probably because the models
are trained on the mixture of both “good” and “bad”
breaks and are able to somehow learn to use “good”
breaks while avoiding being misled by “bad” breaks.
</bodyText>
<tableCaption confidence="0.974286666666667">
Table 2: F scores of the seven most frequent non-
terminals of the REGULAR, BRKPHRASE, and BRK-
PHRASE+DIRECTRESCORE models.
</tableCaption>
<bodyText confidence="0.997636681818182">
Table 2 reports the F scores of the seven most fre-
quent phrases for the REGULAR, BRKPHRASE, and
BRKPHRASE+DIRECTRESCORE methods trained
on Fisher. When comparing the BRKPHRASE
method to REGULAR, the break indexes help to im-
prove the score for edits most, followed by inter-
jections and prepositional phrases; however, they do
not improve the accuracy of any of the other phrases.
The BRKPHRASE+DIRECTRESCORE approach ob-
tains improvements on all of the major phrases.
Figure 5 (a) shows a reference parse tree of a
test sentence. The REGULAR approach correctly
parses the first half (omitted) of the sentence but
it fails to correctly interpret the second half (as
shown). The BRKPHRASE approach, in contrast,
is misguided by the incorrectly classified inter-
ruption point *p* after word “has”, and so pro-
duces an incorrect parse early in the sentence. The
BRKPHRASE+DIRECTRESCORE approach is able
to provide the correct tree given the n-best list pro-
duced by the REGULAR approach, despite the break
index errors.
</bodyText>
<figure confidence="0.999297">
(a) Reference, BRKPHRASE+DIRECTRESCORE
(b) REGULAR (c) BRKPHRASE
</figure>
<figureCaption confidence="0.8031015">
Figure 5: Parses for like*1* has*P* anything*1* like*1*
affected*1* you*4* personally*4* or*1* anything*4*
</figureCaption>
<sectionHeader confidence="0.996884" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999996409090909">
We have investigated using prosodic information in
the form of automatically detected ToBI break in-
dexes for parsing spontaneous speech by compar-
ing three prosodic enrichment methods. Although
prosodic enrichment improves the basic PCFGs, that
performance gain disappears when latent variables
are used, partly due to the impact of misclassified
(“bad”) breaks that are assigned to words that do not
occur at phrase boundaries. However, we find that
by simply restricting the search space of the three
prosodically enriched latent variable parser models
to the n-best parses from the baseline PCFG-LA
parser, all of them attain significant improvements.
Our analysis more fully explains the positive results
achieved by (Kahn et al., 2005) from reranking with
prosodic features and suggests that the hypothesis
that inserted prosodic punctuation breaks n-gram de-
pendencies only partially explains the negative re-
sults of (Gregory et al., 2004). Our findings from
the oracle experiment suggest that integrating ToBI
classification with syntactic parsing should increase
the accuracy of both tasks.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988965">
We would like to thank Izhak Shafran for providing
break indexes for Fisher and Switchboard and for
</bodyText>
<page confidence="0.99654">
44
</page>
<bodyText confidence="0.999968666666667">
comments on an earlier draft of this paper. This re-
search was supported in part by NSF IIS-0703859.
Opinions, findings, and recommendations expressed
in this paper are those of the authors and do not nec-
essarily reflect the views of the funding agency or
the institutions where the work was completed.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928387755102">
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
Event Detection. Technical report, Johns Hopkins
University.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Sparseval: Evaluation metrics for pars-
ing speech. In LREC.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
</reference>
<page confidence="0.999392">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761361">
<title confidence="0.999071">Appropriately Handled Prosodic Breaks Help PCFG Parsing</title>
<author confidence="0.971848">Mary</author>
<affiliation confidence="0.978561">for Computational Linguistics and Information Institute for Advanced Computer University of Maryland, College Park, MD Language Technology Center of</affiliation>
<address confidence="0.882979">Johns Hopkins University, Baltimore, MD USA</address>
<abstract confidence="0.99764037037037">This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFG- LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Stephanie Strassel</author>
<author>Haejoong Lee</author>
<author>Kazuaki Maeda</author>
<author>Seth Kulick</author>
<author>Yang Liu</author>
<author>Mary Harper</author>
<author>Matthew Lease</author>
</authors>
<title>Linguistic resources for speech parsing.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="7814" citStr="Bies et al., 2006" startWordPosition="1206" endWordPosition="1209">he baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1. The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation. We utilized the Fisher dev1 and dev2 sets containing 16,519 trees (112,717 words) as the main training data source and </context>
</contexts>
<marker>Bies, Strassel, Lee, Maeda, Kulick, Liu, Harper, Lease, 2006</marker>
<rawString>Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki Maeda, Seth Kulick, Yang Liu, Mary Harper, and Matthew Lease. 2006. Linguistic resources for speech parsing. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14000" citStr="Charniak, 2000" startWordPosition="2244" endWordPosition="2245">P VP NP VP VBZ PRP MD VB PRP VBZ PRP MD VB 39 cally for all of the three enrichment methods (BRKINSERT, BRKPOS, and BRKPHRASE), training a basic PCFG is straightforward; we simply pull the counts of grammar rules, lexical rewrites, or prosodic rewrites from the treebank and normalize them to obtain their probabilities. As is well known in the parsing community, the basic PCFG does not provide state-of-the-art performance due to its strong independence assumptions. We can relax these assumptions by explicitly incorporating more information into the conditional history, as in Charniak’s parser (Charniak, 2000); however, this would require sophisticated engineering efforts to decide what to include in the history and how to smooth probabilities appropriately due to data sparsity. In this paper, we utilize PCFG-LA models (Matsuzaki et al., 2005; Petrov and Klein, 2007) that split each nonterminal into a set of latent tags and learn complex dependencies among the latent tags automatically during training. The resulting model is still a PCFG, but it is probabilistically context free on the latent tags, and the interaction among the latent tags is able to implicitly capture higher order dependencies amo</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
<author>Delphine Dahan</author>
<author>Wilma v an Donselaar</author>
</authors>
<title>Prosody in comprehension of spoken language: A literature review. Language and Speech.</title>
<date>1997</date>
<contexts>
<context position="1850" citStr="Cutler et al., 1997" startWordPosition="267" endWordPosition="270"> parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser. 1 Introduction Speech conveys more than a sequence of words to a listener. An important additional type of information that phoneticians investigate is called prosody, which includes phenomena such as pauses, pitch, energy, duration, grouping, and emphasis. For a review of the role of prosody in processing spoken language, see (Cutler et al., 1997). Prosody can help with the disambiguation of lexical meaning 37 (via accents and tones) and sentence type (e.g., yesno question versus statement), provide discourselevel information like focus, prominence, and discourse segment, and help a listener to discern a speaker’s emotion or hesitancy, etc. Prosody often draws a listener’s attention to important information through contrastive pitch or duration patterns associated words or phrases. In addition, prosodic cues can help one to segment speech into chunks that are hypothesized to have a hierarchical structure, although not necessarily ident</context>
</contexts>
<marker>Cutler, Dahan, Donselaar, 1997</marker>
<rawString>Anne Cutler, Delphine Dahan, and Wilma v an Donselaar. 1997. Prosody in comprehension of spoken language: A literature review. Language and Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Izhak Shafran</author>
</authors>
<title>Exploiting prosody for PCFGs with latent annotations.</title>
<date>2007</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="3157" citStr="Dreyer and Shafran, 2007" startWordPosition="472" endWordPosition="475">f speech inputs, the topic of this paper. Prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used in speech processing tasks like sentence boundary detection (Liu et al., 2005). In contrast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break indexes. Gregory et al. (2004) attempted to integrate quantized prosodic features as additional tokens in the same manner that punctuation marks are added into text. Although punctuation marks can significantly improve parse accuracy of newswire text, the quantized prosodic tokens were found harmHuman Language Technologies: The 2010 Annual Conference of t</context>
<context position="5126" citStr="Dreyer and Shafran (2007)" startWordPosition="771" endWordPosition="774">eriment, Kahn et al. (2005), instead of using raw quantized prosodic features, used three classes of automatically detected ToBI break indexes (1, 4, or p) and their posteriors. Rather than directly incorporating the breaks into the parse trees, they used the breaks to generate additional features for re-ranking the n-best parse trees from a generative parsing model trained without prosody. They were able to obtain a significant 0.6% improvement on Switchboard over the generative parser, and a more modest 0.1% to 0.2% improvement over the reranking model that also utilizes syntactic features. Dreyer and Shafran (2007) added prosodic breaks into a generative parsing model with latent variables. They utilized three classes of ToBI break indexes (1, 4, and p), automatically predicted by the approach described in (Dreyer and Shafran, 2007; Harper et al., 2005). Breaks were modeled as a sequence of observations parallel to the sentence and each break was generated by the preterminal of the preceding word, assuming that the observation of a break, b, was conditionally independent of its preceding word, w, given preterminal X: P(w,b|X) = P(w|X)P(b|X) (1) Their approach has advantages over (Gregory et al., 2004) i</context>
<context position="6457" citStr="Dreyer and Shafran, 2007" startWordPosition="987" endWordPosition="990">ks are modeled by preterminals rather than higher level nonterminals, and thus cannot directly affect phrasing in a basic PCFG grammar. However, they addressed this independence drawback by splitting each nonterminal into latent tags so that the impact of prosodic breaks could be percolated into the phrasing process through the interaction of latent tags. They achieved a minor 0.2% improvement over their baseline model without prosodic cues and also found that prosodic breaks can be used to build more compact grammars. In this paper, we re-investigate the models of (Gregory et al., 2004) and (Dreyer and Shafran, 2007), and propose a new way of modeling that can potentially address the shortcomings of the two previous approaches. We also attribute part of the failure or ineffectiveness of the previously investigated approaches to errors in the quantized prosodic tokens or automatic break indexes, which are predicted based only on acoustic cues and could misalign with phrase boundaries. We illustrate that these prosodically enriched models are in fact highly effective if we systematically eliminate bad phrase and hesitation breaks given their projection onto the reference parse trees. Inspired by this, we pr</context>
<context position="7906" citStr="Dreyer and Shafran, 2007" startWordPosition="1221" endWordPosition="1224">coring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1. The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation. We utilized the Fisher dev1 and dev2 sets containing 16,519 trees (112,717 words) as the main training data source and used the Penn Switchboard 1A small fraction of words in the Switchboard treebank could not b</context>
<context position="9968" citStr="Dreyer and Shafran, 2007" startWordPosition="1559" endWordPosition="1562">2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investigate three alternative methods of modeling prosodic breaks. Figure 1 shows parse trees for the four models for processing the spontaneous speech transcription she’s she would do, where the speaker hesitated after saying she’s and then resumed with another utterance she would do. Each word input into the parser has an associated break index represented by the symbol 1, 4, or p enclosed in asterisks indicating the break after the word. The automatically detected break *4* after the contraction is a strong indicator of an intonational phrase b</context>
<context position="11780" citStr="Dreyer and Shafran, 2007" startWordPosition="1856" endWordPosition="1859"> numbers of latent tags, we train each latent variable grammar with 10 different seeds and report the average F score on the evaluation set. (a) REGULAR (b) BRKINSERT (c) BRKPOS (d) BRKPHRASE Figure 1: Modeling Methods highest nonterminal nodes so that the order of words and breaks remain unchanged in the terminals. This is similar to (Gregory et al., 2004), except that automatically generated ToBI breaks are used rather than quantized raw prosodic tokens. The second method, BRKPOS, shown in Figure 1 (c), treats breaks as a sequence of observations parallel to the words in the sentence as in (Dreyer and Shafran, 2007). The dotted edges in Figure 1 (c) represent the relation between preterminals and prosodic breaks, and we call them prosodic rewrites, with analogy to grammar rewrites and lexical rewrites. The generation of words and prosodic breaks is assumed to be conditionally independent given the preterminal, as in Equation 1. The third new method, BRKPHRASE, shown in Figure 1 (d), also treats breaks as a sequence of observations parallel to the sentence; however, rather than associating the prosodic breaks with the preterminals, each is generated by the highest nonterminal (including preterminal) in th</context>
<context position="22166" citStr="Dreyer and Shafran, 2007" startWordPosition="3625" endWordPosition="3628">ilar results for BRKINSERT were reported in (Gregory et al., 2004), where they attributed the degradation to the fact that the insertion of the prosodic “punctuation” breaks the n-gram dependencies. Another possible cause is that the insertion of “bad” breaks that do not align with true phrase boundaries hurts performance more than the benefits gained from “good” breaks due to the tightly integrated relationship between phrases and breaks. For the BRKPOS method, the impact of break indexes is implicitly percolated to the nonterminals through the interaction among latent tags, as discussed in (Dreyer and Shafran, 2007), and its performance may thus be less affected by the “bad” breaks. With latent annotations (in contrast to the basic PCFG), the model is now significantly better than BRKINSERT and is on par with BRKPHRASE. REGULAR BRKINSERT BRKPOS BRKPHRASE DIRECT ORACLERESCORE ORACLE DIRECTRESCORE 85.5 84.5 83.5 82.5 83.9 83.2 84.2 84.2 84.4 84.0 85.0 84.5 84.7 84.0 85.1 84.7 84.8 REGULAR BRKINSERT 64.9 67.2 41 6.2 Models with Oracle Breaks In order to determine whether “bad” breaks limit the improvements in parsing performance from prosodic enrichment, we conducted a simple oracle experiment where all *p*</context>
</contexts>
<marker>Dreyer, Shafran, 2007</marker>
<rawString>Markus Dreyer and Izhak Shafran. 2007. Exploiting prosody for PCFGs with latent annotations. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="7752" citStr="Godfrey et al., 1992" startWordPosition="1195" endWordPosition="1198">e prosodically enriched parser models to the n-best parses from the baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1. The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation. We utilized the Fisher dev1 and dev2 sets containing 16,5</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle L Gregory</author>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>Sentence-internal prosody does not help parsing the way punctuation does.</title>
<date>2004</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3430" citStr="Gregory et al. (2004)" startWordPosition="512" endWordPosition="515">trast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break indexes. Gregory et al. (2004) attempted to integrate quantized prosodic features as additional tokens in the same manner that punctuation marks are added into text. Although punctuation marks can significantly improve parse accuracy of newswire text, the quantized prosodic tokens were found harmHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37–45, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ful to parse accuracy when inserted into humangenerated speech transcripts of the Switchboard corpus. The authors hypothesized that the i</context>
<context position="5724" citStr="Gregory et al., 2004" startWordPosition="869" endWordPosition="872">eyer and Shafran (2007) added prosodic breaks into a generative parsing model with latent variables. They utilized three classes of ToBI break indexes (1, 4, and p), automatically predicted by the approach described in (Dreyer and Shafran, 2007; Harper et al., 2005). Breaks were modeled as a sequence of observations parallel to the sentence and each break was generated by the preterminal of the preceding word, assuming that the observation of a break, b, was conditionally independent of its preceding word, w, given preterminal X: P(w,b|X) = P(w|X)P(b|X) (1) Their approach has advantages over (Gregory et al., 2004) in that it does not break n-gram dependencies in parse modeling. It also has disadvantages in that the breaks are modeled by preterminals rather than higher level nonterminals, and thus cannot directly affect phrasing in a basic PCFG grammar. However, they addressed this independence drawback by splitting each nonterminal into latent tags so that the impact of prosodic breaks could be percolated into the phrasing process through the interaction of latent tags. They achieved a minor 0.2% improvement over their baseline model without prosodic cues and also found that prosodic breaks can be used</context>
<context position="9885" citStr="Gregory et al., 2004" startWordPosition="1545" endWordPosition="1548">ze a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investigate three alternative methods of modeling prosodic breaks. Figure 1 shows parse trees for the four models for processing the spontaneous speech transcription she’s she would do, where the speaker hesitated after saying she’s and then resumed with another utterance she would do. Each word input into the parser has an associated break index represented by the symbol 1, 4, or p enclosed in asterisks indicating the break after the word. The automatically detecte</context>
<context position="11514" citStr="Gregory et al., 2004" startWordPosition="1810" endWordPosition="1813">them under the 2We use Bikel’s randomized parsing evaluation comparator to determine the significance (p &lt; 0.005) of the difference between two parsers’ outputs. 3Due to the randomness of parameter initialization in the learning of PCFG-LA models with increasing numbers of latent tags, we train each latent variable grammar with 10 different seeds and report the average F score on the evaluation set. (a) REGULAR (b) BRKINSERT (c) BRKPOS (d) BRKPHRASE Figure 1: Modeling Methods highest nonterminal nodes so that the order of words and breaks remain unchanged in the terminals. This is similar to (Gregory et al., 2004), except that automatically generated ToBI breaks are used rather than quantized raw prosodic tokens. The second method, BRKPOS, shown in Figure 1 (c), treats breaks as a sequence of observations parallel to the words in the sentence as in (Dreyer and Shafran, 2007). The dotted edges in Figure 1 (c) represent the relation between preterminals and prosodic breaks, and we call them prosodic rewrites, with analogy to grammar rewrites and lexical rewrites. The generation of words and prosodic breaks is assumed to be conditionally independent given the preterminal, as in Equation 1. The third new m</context>
<context position="21607" citStr="Gregory et al., 2004" startWordPosition="3536" endWordPosition="3539">odels trained and evaluated on the modified oracle data (see Subsection 6.2), and the ORACLERESCORE and DIRECTRESCORE bars for results of the two rescoring approaches (described in Subsection 6.3) on the original evaluation data. However, the prosodically enriched methods do not significantly improve upon the REGULAR baseline after the introduction of latent annotations. The BRKPHRASE method only achieves a minor insignificant 0.1% improvement over the REGULAR baseline; whereas, the BRKINSERT method is a significant 0.7% worse than the baseline. Similar results for BRKINSERT were reported in (Gregory et al., 2004), where they attributed the degradation to the fact that the insertion of the prosodic “punctuation” breaks the n-gram dependencies. Another possible cause is that the insertion of “bad” breaks that do not align with true phrase boundaries hurts performance more than the benefits gained from “good” breaks due to the tightly integrated relationship between phrases and breaks. For the BRKPOS method, the impact of break indexes is implicitly percolated to the nonterminals through the interaction among latent tags, as discussed in (Dreyer and Shafran, 2007), and its performance may thus be less af</context>
</contexts>
<marker>Gregory, Johnson, Charniak, 2004</marker>
<rawString>Michelle L. Gregory, Mark Johnson, and Eugene Charniak. 2004. Sentence-internal prosody does not help parsing the way punctuation does. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary P Harper</author>
<author>Bonnie J Dorr</author>
<author>John Hale</author>
<author>Brian Roark</author>
<author>Izhak Shafran</author>
<author>Matthew Lease</author>
<author>Yang Liu</author>
<author>Matthew Snover</author>
<author>Lisa Yung</author>
<author>Anna Krasnyanskaya</author>
<author>Robin Stewart</author>
</authors>
<title>Johns Hopkins Summer Workshop Final Report on Parsing and Spoken Structural Event Detection.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="5369" citStr="Harper et al., 2005" startWordPosition="811" endWordPosition="814">ey used the breaks to generate additional features for re-ranking the n-best parse trees from a generative parsing model trained without prosody. They were able to obtain a significant 0.6% improvement on Switchboard over the generative parser, and a more modest 0.1% to 0.2% improvement over the reranking model that also utilizes syntactic features. Dreyer and Shafran (2007) added prosodic breaks into a generative parsing model with latent variables. They utilized three classes of ToBI break indexes (1, 4, and p), automatically predicted by the approach described in (Dreyer and Shafran, 2007; Harper et al., 2005). Breaks were modeled as a sequence of observations parallel to the sentence and each break was generated by the preterminal of the preceding word, assuming that the observation of a break, b, was conditionally independent of its preceding word, w, given preterminal X: P(w,b|X) = P(w|X)P(b|X) (1) Their approach has advantages over (Gregory et al., 2004) in that it does not break n-gram dependencies in parse modeling. It also has disadvantages in that the breaks are modeled by preterminals rather than higher level nonterminals, and thus cannot directly affect phrasing in a basic PCFG grammar. H</context>
<context position="7794" citStr="Harper et al., 2005" startWordPosition="1202" endWordPosition="1205"> n-best parses from the baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1. The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation. We utilized the Fisher dev1 and dev2 sets containing 16,519 trees (112,717 words) as the main train</context>
<context position="9709" citStr="Harper et al., 2005" startWordPosition="1518" endWordPosition="1521">y rules that are not related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investigate three alternative methods of modeling prosodic breaks. Figure 1 shows parse trees for the four models for processing the spontaneous speech transcription she’s she would do, where the speaker hesitated after saying she’s and then resumed with another utterance she would do. Each wo</context>
</contexts>
<marker>Harper, Dorr, Hale, Roark, Shafran, Lease, Liu, Snover, Yung, Krasnyanskaya, Stewart, 2005</marker>
<rawString>Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark, Izhak Shafran, Matthew Lease, Yang Liu, Matthew Snover, Lisa Yung, Anna Krasnyanskaya, and Robin Stewart. 2005. 2005 Johns Hopkins Summer Workshop Final Report on Parsing and Spoken Structural Event Detection. Technical report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages. In EMNLP.</title>
<date>2009</date>
<contexts>
<context position="9349" citStr="Huang and Harper, 2009" startWordPosition="1459" endWordPosition="1462">to words in the middle of a sentence and *4* to words that end a sentence. 38 treebank containing 110,504 trees (837,863 words) as an additional training source to evaluate the effect of training data size on parsing performance. The treebank trees are normalized by downcasing all terminal strings and deleting punctuation, empty nodes, and nonterminal-yield unary rules that are not related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreye</context>
<context position="14697" citStr="Huang and Harper, 2009" startWordPosition="2354" endWordPosition="2357">what to include in the history and how to smooth probabilities appropriately due to data sparsity. In this paper, we utilize PCFG-LA models (Matsuzaki et al., 2005; Petrov and Klein, 2007) that split each nonterminal into a set of latent tags and learn complex dependencies among the latent tags automatically during training. The resulting model is still a PCFG, but it is probabilistically context free on the latent tags, and the interaction among the latent tags is able to implicitly capture higher order dependencies among the original nonterminals and observations. We follow the approach in (Huang and Harper, 2009) to train the PCFG-LA models. 5 Parsing In a basic PCFG without latent variables, the goal of maximum probability parsing is to find the most likely parse tree given a sentence based on the grammar. Suppose our grammar is binarized (so it contains only unary and binary grammar rules). Given an input sentence wn1 = w1, w2, · · · , wn, the inside probability, P(i, j, X), of the most likely sub-tree that is rooted at nonterminal X and generates subsequence wji can be computed recursively by: P(i, j, X) = max(max P(i, j, Y )P(X —* Y ), Y P(i, k,Y)P(k + 1, j, Z)P(X —* Y Z)) (3) Backtracing the sear</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective use of prosody in parsing conversational speech.</title>
<date>2005</date>
<booktitle>In EMNLP-HLT.</booktitle>
<contexts>
<context position="4528" citStr="Kahn et al. (2005)" startWordPosition="678" endWordPosition="681">racy when inserted into humangenerated speech transcripts of the Switchboard corpus. The authors hypothesized that the inserted pseudo-punctuation break n-gram dependencies in the parser model, leading to lower accuracies. However, another possible cause is that the prosody has not been effectively utilized due to the fact that it is overloaded; it not only provides information about phrases, but also about the state of the speaker and his/her sentence planning process. Hence, the prosodic information may at times be more harmful than helpful to parsing performance. In a follow-on experiment, Kahn et al. (2005), instead of using raw quantized prosodic features, used three classes of automatically detected ToBI break indexes (1, 4, or p) and their posteriors. Rather than directly incorporating the breaks into the parse trees, they used the breaks to generate additional features for re-ranking the n-best parse trees from a generative parsing model trained without prosody. They were able to obtain a significant 0.6% improvement on Switchboard over the generative parser, and a more modest 0.1% to 0.2% improvement over the reranking model that also utilizes syntactic features. Dreyer and Shafran (2007) a</context>
<context position="7355" citStr="Kahn et al., 2005" startWordPosition="1129" endWordPosition="1132">es, which are predicted based only on acoustic cues and could misalign with phrase boundaries. We illustrate that these prosodically enriched models are in fact highly effective if we systematically eliminate bad phrase and hesitation breaks given their projection onto the reference parse trees. Inspired by this, we propose two alternative rescoring methods to restrict the search space of the prosodically enriched parser models to the n-best parses from the baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1. The Fisher treebank is a</context>
<context position="9423" citStr="Kahn et al., 2005" startWordPosition="1471" endWordPosition="1474">reebank containing 110,504 trees (837,863 words) as an additional training source to evaluate the effect of training data size on parsing performance. The treebank trees are normalized by downcasing all terminal strings and deleting punctuation, empty nodes, and nonterminal-yield unary rules that are not related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investig</context>
<context position="34790" citStr="Kahn et al., 2005" startWordPosition="5656" endWordPosition="5659">taneous speech by comparing three prosodic enrichment methods. Although prosodic enrichment improves the basic PCFGs, that performance gain disappears when latent variables are used, partly due to the impact of misclassified (“bad”) breaks that are assigned to words that do not occur at phrase boundaries. However, we find that by simply restricting the search space of the three prosodically enriched latent variable parser models to the n-best parses from the baseline PCFG-LA parser, all of them attain significant improvements. Our analysis more fully explains the positive results achieved by (Kahn et al., 2005) from reranking with prosodic features and suggests that the hypothesis that inserted prosodic punctuation breaks n-gram dependencies only partially explains the negative results of (Gregory et al., 2004). Our findings from the oracle experiment suggest that integrating ToBI classification with syntactic parsing should increase the accuracy of both tasks. Acknowledgments We would like to thank Izhak Shafran for providing break indexes for Fisher and Switchboard and for 44 comments on an earlier draft of this paper. This research was supported in part by NSF IIS-0703859. Opinions, findings, and</context>
</contexts>
<marker>Kahn, Lease, Charniak, Johnson, Ostendorf, 2005</marker>
<rawString>Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective use of prosody in parsing conversational speech. In EMNLP-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2801" citStr="Liu et al., 2005" startWordPosition="417" endWordPosition="420">ttention to important information through contrastive pitch or duration patterns associated words or phrases. In addition, prosodic cues can help one to segment speech into chunks that are hypothesized to have a hierarchical structure, although not necessarily identical to that of syntax. This suggests that prosodic cues may help in the parsing of speech inputs, the topic of this paper. Prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used in speech processing tasks like sentence boundary detection (Liu et al., 2005). In contrast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break i</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="922" citStr="Matsuzaki et al., 2005" startWordPosition="123" endWordPosition="126">pkins University, Baltimore, MD USA {zqhuang,mharper}@umiacs.umd.edu Abstract This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFGLA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser. 1 Introduction Speech </context>
<context position="14237" citStr="Matsuzaki et al., 2005" startWordPosition="2279" endWordPosition="2282">, or prosodic rewrites from the treebank and normalize them to obtain their probabilities. As is well known in the parsing community, the basic PCFG does not provide state-of-the-art performance due to its strong independence assumptions. We can relax these assumptions by explicitly incorporating more information into the conditional history, as in Charniak’s parser (Charniak, 2000); however, this would require sophisticated engineering efforts to decide what to include in the history and how to smooth probabilities appropriately due to data sparsity. In this paper, we utilize PCFG-LA models (Matsuzaki et al., 2005; Petrov and Klein, 2007) that split each nonterminal into a set of latent tags and learn complex dependencies among the latent tags automatically during training. The resulting model is still a PCFG, but it is probabilistically context free on the latent tags, and the interaction among the latent tags is able to implicitly capture higher order dependencies among the original nonterminals and observations. We follow the approach in (Huang and Harper, 2009) to train the PCFG-LA models. 5 Parsing In a basic PCFG without latent variables, the goal of maximum probability parsing is to find the mos</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="947" citStr="Petrov and Klein, 2007" startWordPosition="127" endWordPosition="130">ore, MD USA {zqhuang,mharper}@umiacs.umd.edu Abstract This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFGLA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser. 1 Introduction Speech conveys more than a seque</context>
<context position="9324" citStr="Petrov and Klein, 2007" startWordPosition="1455" endWordPosition="1458">tically added break *1* to words in the middle of a sentence and *4* to words that end a sentence. 38 treebank containing 110,504 trees (837,863 words) as an additional training source to evaluate the effect of training data size on parsing performance. The treebank trees are normalized by downcasing all terminal strings and deleting punctuation, empty nodes, and nonterminal-yield unary rules that are not related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI b</context>
<context position="14262" citStr="Petrov and Klein, 2007" startWordPosition="2283" endWordPosition="2286">rom the treebank and normalize them to obtain their probabilities. As is well known in the parsing community, the basic PCFG does not provide state-of-the-art performance due to its strong independence assumptions. We can relax these assumptions by explicitly incorporating more information into the conditional history, as in Charniak’s parser (Charniak, 2000); however, this would require sophisticated engineering efforts to decide what to include in the history and how to smooth probabilities appropriately due to data sparsity. In this paper, we utilize PCFG-LA models (Matsuzaki et al., 2005; Petrov and Klein, 2007) that split each nonterminal into a set of latent tags and learn complex dependencies among the latent tags automatically during training. The resulting model is still a PCFG, but it is probabilistically context free on the latent tags, and the interaction among the latent tags is able to implicitly capture higher order dependencies among the original nonterminals and observations. We follow the approach in (Huang and Harper, 2009) to train the PCFG-LA models. 5 Parsing In a basic PCFG without latent variables, the goal of maximum probability parsing is to find the most likely parse tree given</context>
<context position="18951" citStr="Petrov and Klein, 2007" startWordPosition="3115" endWordPosition="3118"> Z, 0)P(X → Y Z)) (5) i&lt;k&lt;j,Y,Z Although dynamic programming algorithms exist for maximum probability decoding of basic PCFGs without latent annotations for all four methods, it is an NP hard problem to find the most likely parse tree using PCFG-LA models. Several alternative decoding algorithms have been proposed in the literature for parsing with latent variable grammars. We use the best performing max-rule-product decoding algorithm, which searches for the best parse tree that maximizes the product of the posterior rule (either grammar, lexical, or prosodic) probabilities, as described in (Petrov and Klein, 2007) for our models with latent annotations and extend the dynamic parsing algorithm described in Equation 5 for the BRKPHRASE grammar with latent annotations. 6 Results on the Fisher Corpus 6.1 Prosodically Enriched Models Table 1 reports the parsing accuracy of the four basic PCFGs without latent annotations when trained on the Fisher training data. All of the grammars have a low F score of around 65% due to the overly strong and incorrect independence assumptions. We observe that the BRKPHRASE grammar benefits most from breaks, significantly improving the baseline accuracy from 64.9% to 67.2%, </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mary Harper</author>
<author>Yang Liu</author>
<author>Robin Stewart</author>
<author>Matthew Lease</author>
<author>Matthew Snover</author>
<author>Izhak Shafran</author>
<author>Bonnie J Dorr</author>
<author>John Hale</author>
<author>Anna Krasnyanskaya</author>
<author>Lisa Yung</author>
</authors>
<title>Sparseval: Evaluation metrics for parsing speech.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="9730" citStr="Roark et al., 2006" startWordPosition="1522" endWordPosition="1525">related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3. Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investigate three alternative methods of modeling prosodic breaks. Figure 1 shows parse trees for the four models for processing the spontaneous speech transcription she’s she would do, where the speaker hesitated after saying she’s and then resumed with another utterance she would do. Each word input into the par</context>
</contexts>
<marker>Roark, Harper, Liu, Stewart, Lease, Snover, Shafran, Dorr, Hale, Krasnyanskaya, Yung, 2006</marker>
<rawString>Brian Roark, Mary Harper, Yang Liu, Robin Stewart, Matthew Lease, Matthew Snover, Izhak Shafran, Bonnie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa Yung. 2006. Sparseval: Evaluation metrics for parsing speech. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Silverman</author>
<author>Mary Beckman</author>
<author>John Pitrelli</author>
<author>Mari Ostendorf</author>
<author>Colin Wightman</author>
<author>Patti Price</author>
<author>Janet Pierrehumbert</author>
<author>Julia Hirshberg</author>
</authors>
<title>ToBI: A standard for labeling English prosody.</title>
<date>1992</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="2900" citStr="Silverman et al., 1992" startWordPosition="431" endWordPosition="434">ords or phrases. In addition, prosodic cues can help one to segment speech into chunks that are hypothesized to have a hierarchical structure, although not necessarily identical to that of syntax. This suggests that prosodic cues may help in the parsing of speech inputs, the topic of this paper. Prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used in speech processing tasks like sentence boundary detection (Liu et al., 2005). In contrast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break indexes. Gregory et al. (2004) attempted to integrate quantized prosodic features as additional toke</context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrehumbert, Hirshberg, 1992</marker>
<rawString>Kim Silverman, Mary Beckman, John Pitrelli, Mari Ostendorf, Colin Wightman, Patti Price, Janet Pierrehumbert, and Julia Hirshberg. 1992. ToBI: A standard for labeling English prosody. In ICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>