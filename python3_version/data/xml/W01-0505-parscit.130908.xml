<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.995866">
Improving Lexical Mapping Model of English-Korean Bitext
Using Structural Features
</title>
<author confidence="0.992728">
Seonho Kimt and Juntae Yoont and Mansuk Songt
</author>
<affiliation confidence="0.991556">
tDept. of Computer Science, Yonsei University,
</affiliation>
<address confidence="0.740268">
Seoul 120-749, Korea
</address>
<email confidence="0.920044">
shkim,mssong@december.yonsei.ac.kr
</email>
<author confidence="0.4126625">
NLP Lab., Daumsoft Inc., Kangnam-gu, Samsung-dong,
Yungjeon Bldg., Seoul 135-728, Korea
</author>
<email confidence="0.992521">
jtyoon@daumsoft.com
</email>
<sectionHeader confidence="0.979774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837315789474">
The problem of finding lexical alignments for given
sentence pairs is computationally expensive. Fur-
thermore, it is much difficult to find lexical align-
ments between Korean and English since they have
considerably different syntactic structures and the
coverage of word-for-word correspondences is low.
This paper presents a method for extracting struc-
tural features which can reduce mapping space by
allowing only probable alignments. We describe how
the features improve the performance of the lexical
alignment model. The structural features provide
the information for the correspondences of parts-of-
speech (POS) sequences which are useful in transla-
tion. Based on maximum entropy (ME) concept, the
structural features are incrementally selected, which
are later embedded in the lexical alignment model.
It turns out that the features help get better lex-
ical alignments of Korean and English by offering
linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.996295" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964993294117647">
Aligned bitexts are useful for the derivation of
bilingual lexical resources which are used for ma-
chine translation and cross languages information
retrieval. Thus, a lot of approaches have been sug-
gested to find sets of corresponding word tokens
(Brown et al., 1993; Berger et al., 1996; Melamed,
1997), phrase (Shin et al., 1996), noun phrase (Ku-
piec, 1993), and collocation (Smadja et al., 1996) in
a bitext.
Some works have used lexical association measures
for finding word correspondences (Gale and Church,
1991; Fung and Church, 1994). However, the associ-
ation measures can be misled in cases where a word
in a source language frequently co-occurs with more
than one word in a target language or in cases of
indirect association&apos; (Melamed, 1997).
In other works, iterative parameter re-estimation
&apos;Suppose that Uk and vk are indeed mutual translation
and Uk and Uk-ki often co-occur in text. Then vk and 24+1
will also co-occur more than expected by chance , which is
represented as indirect association.
techniques based on IBM model 1-5 2 have been
employed (Brown et al., 1993). They were usually
incorporated in the EM algorithm (Brown et al.,
1993; Kupiec, 1993; Tillmann and Ney, 2000; Och
et al., 2000). However, we are often faced with some
difficulties as follows, when the IBM model-based
approaches are directly applied to the alignment,
especially on bitext involving a less closely related
language pair.
1. It needs excessive iteration time for parame-
ter estimation and high decoding complexity.
Thus most systems assumed one-to-one corre-
spondence to reduce computational complex-
ity. However, word sequences are not trans-
lated literally word for word. For example, in
cases of collocations, compound nouns, and am-
biguous words with different meaning depen-
dent on the context, they require phrase-level
correspondences.
2. Most systems use little or no linguistic knowl-
edge to structure the underlying models. The
distortion probability and the fertility probabil-
ity for finding word correspondences is a weak
description for word order change between lan-
guages and 1:n mapping modeling. As the re-
sult, lots of ungrammatical sentences and too
many parameters to be estimated are allowed.
In addition, many words are aligned to the
empty word due to the overfitting problem (Och
et al., 2000).
</bodyText>
<listItem confidence="0.929998666666667">
3. In order to estimate parameters properly, it re-
quires a very large volume of bilingual aligned
text.
</listItem>
<bodyText confidence="0.840719">
In this paper, we use structural correspondences to
</bodyText>
<page confidence="0.855514">
2
7/1
</page>
<note confidence="0.369915">
P(f, ale) = Hn(Oilei)Ht(fjlea,)d(jlajâ€ž/)
</note>
<footnote confidence="0.9986244">
n is the fertility probability that an English word generates
7/ French words, t is the word alignment probability that the
English word e generates the French word f, and d is the dis-
tortion probability that an English word in a certain position
will generate a French word in a certain position.
</footnote>
<table confidence="0.999117875">
English Korean
can/MD eu1/ENTR1 sa/NNDE ss/AJ
da/ENTE
World/NNP Cup/NNP weoideakeop/NNIN1
to/TO take/VBP off/RP iryugha/VB neart/ENTR1
MD:modal auxiliary, NNP:proper noun, VBP:verb, RP:particle
NNDE:dependent noun, AJ:adjective ENTE: final ending,
NNIN1:proper noun, VB:verb, ENTR1:adnominal ending
</table>
<tableCaption confidence="0.9174765">
Table 1: Lexical differences between English and Ko-
rean
</tableCaption>
<bodyText confidence="0.877763775">
overcome the above problem in bilingual text align-
ment.
2 The problems of lexical alignment
in Korean and English bitext
Although one-to-one correspondence assumption has
been shown to give highly accurate results in closely
related language pair (Melamed, 1997), it does not
fit in structurally different language pair such as Ko-
rean and English. According to Shin et al. (1996),
the coverage of one-to-one correspondence in Korean
and English bitext is approximately 34% and many-
to-many mappings exceed 40% in lexical alignments.
Besides, Korean and English show much difference
in terms of unit of mutual translation as shown in
Table 1.
Thus, we use structural or linguistic information
to find accurate lexical alignments of bitext. In gen-
eral, structural information can be represented by
the form of bilingual correspondence of phrase struc-
ture (Watanabe et al., 2000) or dependency struc-
ture (Yamamoto, 2000).
To find these structural correspondences between
a language pair, unification-based grammar (Mat-
sumoto, 1993) or bilingual grammar (Wu, 1997) can
be used. That is, if we try to find structural match of
bitext, syntactic analysis of each text should be done
first. However, syntactic analysis of each text(or bi-
text) is also a hard and computationally complicated
problem.
In this paper, we propose POS sequence feature.
We consider correspondences between POS tag se-
quences of bitext as the structural information which
is important in determining structures and reduc-
ing unnecessary parameters of a statistical alignment
model. We induce the correspondences(structural
features) by a feature selection method based on the
ME framework. Finally, the structural features are
embedded in a lexical alignment model of English
and Korean bitext. Our model offers the following
advantages:
</bodyText>
<listItem confidence="0.9537588">
1. It can overcome the limitation of word-for-word
correspondences.
2. The model can take advantage of the explicit
introduction of some knowledge about the lan-
guage. Therefore, it can reduce a lot of param-
</listItem>
<bodyText confidence="0.59209025">
eters in statistical machine translation by elim-
inating syntactically unlikely alignments. The
lexical alignment model iterates only over a sub-
set of probable alignments.
</bodyText>
<listItem confidence="0.926676">
3. It is possible to estimate the probability of lex-
ical alignment in a relatively small size of cor-
pora.
4. The structural information is helpful in the con-
struction of a bilingual grammar.
3 Overview of the model
</listItem>
<bodyText confidence="0.991459620689656">
In general, there exist constaints among POS se-
quences mapping when aligning bitext. For instance,
in many cases a noun in Korean is translated to a
noun or a noun preceded by an article in English.
This POS constraint would be useful information in
alignment of a closely related pair as well as a less
closely related language pair.
Some approaches design an alignment algorithm
that maximizes the number of matching POS in
aligned segments (Papageorgiou et al., 1994). We
also assume that the mapping information of POS
sequences of a language pair is useful in a model of
statistical machine translation.
If there are similarities between corresponding
POS sequences in bitext, the structural feature
would be easily computed or identified. However,
a POS sequence in English often correspond to a to-
tally different POS sequence in Korean as shown in
the following example:
can/MD eui/ENTR1 sa/NNDE1 &apos;iss/AJMA da/ENTE
It is caused by the discrepancy between two lan-
guages.
Korean is an agglutinative language. A sentence
in Korean consists of a series of syntactic units called
eojeol. An eojeol delimited by whitespace is often
composed of a content word and function words.
Tense markers, clausal connectives, particles and so
forth are contained in an eojeol. Thus, one or more
words in English often correspond to an eojeol, i.e.
a couple of morphemes. For instance, a phrase, &apos;to
the school&apos;, in English corresponds to an eojeol &amp;quot;d4-
2___(haggyo-ro, school-to)&apos; in Korean.
Thus, we need a method for mapping POS se-
quences of bitext. In this paper, the correspondences
of POS tag sequences are obtained by automatic fea-
ture selection based on the ME framework. Here, a
feature is defined as a correspondence of POS tag
sequences in bitext.
The outline of finding correspondences of POS tag
sequences is as follows: First, our model starts with
initial features obtained by a supervision step. The
initial features are extracted from a small portion
of bitext, which of weights are trained by the IIS
algorithm (Berger et al., 1996; Pietra et al., 1997).
These are called initial active features.
In the next step, a feature pool is constructed from
training data. At this time, only features giving a
large gain to the model are selected. The final out-
put of feature selection is the set of active features
and the correspondences of POS tag sequences that
are represented with conditional probabilities.
The resulted features are used for the parameters
of bilingual text alignment. In the process, we look
at the words to ensure a correct alignment. That is,
the POS sequences are in fact encoding particular
words.
The underlying process is as follows:
Input: a set L of POS-labeled sentence pairs.
</bodyText>
<listItem confidence="0.997876117647059">
1. Make a set .F of correspondences of tag se-
quences, (te, tk) from a small portion of L by
hand.
2. Set .F into a set of initial active features, A.
3. Compute the weights of the initial active fea-
tures A using HS algorithm.
4. Create a feature pool P which is a set of possi-
ble combinations of tag sequences from sentence
pairs.
5. Filter P using frequency counts and similarity
with A.
6. Compute the approximate gains of the features
in P.
7. Select features(Ai) with large gain values, and
add A.
8. Compute the lexical alignment of bitext using
p(tk Ite) where (te, tk) E A.
</listItem>
<sectionHeader confidence="0.92913" genericHeader="method">
4 Feature Selection
</sectionHeader>
<bodyText confidence="0.999993">
As mentioned above, features which represent map-
pings of POS sequences in two languages are auto-
matically learned from bitext. The learning consists
of two steps: (1) supervised step and (2) unsuper-
vised feature selection.
In the supervised step, we manually aligned En-
glish and Korean texts. From the manually aligned
text, we consturct a feature pool which has initial
POS sequence correspondence. In the unsupervised
feature selection, the POS sequence correspondeces
are added to the feature pool.
</bodyText>
<subsectionHeader confidence="0.990462">
4.1 Supervision Step
</subsectionHeader>
<bodyText confidence="0.995320210526316">
In the supervision step, a small portion of bi-
text is tagged using Brill&apos;s tagger (Brill, 1995) and
`MORANY&apos; (Yoon et. al., 1999), each of which
is for English and Korean tagging respectively. We
manually aligned each sentence pair in the bitext
and collected their correspondences of tag sequences.
For simplicity, we adjusted some part of Brill&apos;s tag
set.
First of all, we classified sentential patterns of En-
glish and Korean. Then, we aligned English and
Korean sentence pairs on the basis of the patterns.
Also, we made tag sequence construction rules with
respect to each language by analysis of the cases
where the words of one unit are separated and ad-
jacent. The rules are used when making a feature
pool.
After collecting the correspondences of POS tag
sequences, we used them as a set of initial active
features, A.
</bodyText>
<subsectionHeader confidence="0.998888">
4.2 Construction of a Feature Pool
</subsectionHeader>
<bodyText confidence="0.998132444444444">
In this chapter, we describe how a feature pool is
constructed. Our task is to construct a probabil-
ity model p that produces a corresponding Korean
tag sequence &amp;quot;Tic for a given English tag sequence
J. As features to represent the model, we use co-
occurrence information of POS tag sequences.
Let Ts be all possible correspondences of tag se-
quences for a specific sentence pair, S. We then
define a feature function (or a feature) as follows:
</bodyText>
<equation confidence="0.984357333333333">
1 x = te &amp; y = tk Sz
fte,t,(x,y) = pair(te,tk) E TS
0 otherwise
</equation>
<bodyText confidence="0.9999678">
A feature fte,t,, which indicates co-occurrence be-
tween tags appearing in Ts, expresses information
for predicting that an English POS tag sequence te
maps into a Korean POS tag sequence tk â€¢
In order to make a feature pool, given a sentence
pair, we first construct all possible combinations of
English POS tag sequences and Korean POS tag se-
quences. Among them, only features fte,t, that sat-
isfy the following two conditions are added into the
feature pool P.
</bodyText>
<listItem confidence="0.99929325">
â€¢ count(fte,t,)&gt; 15
â€¢ there exist tk such that (te, tk.) E A and the
similarity value (simply counting of same tag)
between tk and tkx is greater than 0.6
</listItem>
<subsectionHeader confidence="0.973909">
4.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999622714285714">
Since the set of P is too vast, a feature selection pro-
cess is needed to select useful features. For this, each
feature is evaluated according to how much they con-
tribute to the likelihood of training data, which is
called gain.
Before explaining feature gain and a process of
feature selection, we will give a brief introduction of
ME. In ME, a feature gives information to a proba-
bility model and it has a weight.
Thus, the model is constrained by features we de-
fined. In general weights of the features are trained
by the improved iterative scaling (IIS) algorithm
that minimizes the Kullback-Leibler divergence be-
tween the model and the empirical distribution of
the training data.
In fact, our model reduces to a simple type of
probability model that can be derived simply from
a ratio counts since the features do not overlap.
Let pA be the optimal model constrained by a set
of initial active features A, then it can be represented
in (1).
</bodyText>
<equation confidence="0.9999635">
pA(ylx) = LAyix)eE fi (x,Y) (1)
zA(x) = Ep(yix)eE
</equation>
<bodyText confidence="0.999952545454545">
The weights (A) of active features are computed by
the IIS before feature selection.
Let ,41, be AU L, and pAL be the optimal model
in the space of probability distribution after adding
feature L. The model p.,611 contains another param-
eter a, in addition to the parameters given by active
features, which is a weight for the feature L. In order
to make it tractable to compute feature selection, we
assume that the addition of a feature L affects only
the single parameter a. The only parameter which
distinguishes models of (2) is a.
</bodyText>
<equation confidence="0.999280333333333">
1 (2)
pAf, = zct(x)PA(Yix)eaL(x&apos;Y)
Z(x) =EPA(Yix)ectMx&apos;Y)
</equation>
<bodyText confidence="0.9996204">
The improvement (gain) of a model after adding a
single feature L can be estimated by measuring dif-
ference of maximum log-likelihood between L(pAL)
and L(pA). We denote the gain for feature L by
A(,41), which is represented in (3).
</bodyText>
<equation confidence="0.99267225">
A (AL) maxcEGAf, (a) (3)
GAfi (a) = L(PAf) L(PA)
log PAâ€˜f
PA
</equation>
<bodyText confidence="0.997569">
The following algorithm is used to compute the
gain of the model with respect to L. We adopted
Berger&apos;s method for computing gains (Berger et
al., 1996). For the details, the reader is referred
to (Berger et al., 1996; Pietra et al., 1997).
</bodyText>
<listItem confidence="0.998536">
1. Let r = {
2. Set a() = 0
3. Repeat the following until GAL (an) has con-
verged:
Compute anÂ±i from an using
</listItem>
<equation confidence="0.948382777777778">
anÂ±i = an + 1 log (1 G GA:tf: (0))
Compute GAL (an+i) using
GAL (a) = â€”E.73(x)logz(x)+c(fi)
GiAfi(a) = .13(M â€” Ex /3(x)m(x) ,
CA.fi (a) = â€” E. i)(x)p.Afi â€”m(x))2 Ix)
where a = an+i, Afi = Au
M(x) Pc:kfi(filx)
P.Afi(filX) = PAfi (Y1x)fi(x,Y)
4. Set â€” AL(AL) GAL (an)
</equation>
<bodyText confidence="0.998959875">
This algorithm is iteratively computed using Net-
won&apos;s method. With the gain value, we can recog-
nize importance of a feature, i.e. how much the fea-
ture accords with the model. As a result, we can se-
lect useful features with high gain values and obtain
their conditional probabilities. Put another way, we
can get the correspondence probabilities of POS tag
sequences.
</bodyText>
<sectionHeader confidence="0.991303" genericHeader="method">
5 lexical alignment
</sectionHeader>
<bodyText confidence="0.99975">
In this section, we describe how the correspondence
probabilities between bilingual POS sequences is em-
bedded in lexical alignment. In Korean-English ma-
chine translation, a translation system finds the cor-
responding sentence e given a Korean sentence k.
The fundamental equation of machine translation
can be represented in (4), where the random vari-
ables K and E are a Korean sentence and a En-
glish sentence making up a translation, and the ran-
dom variable A is an alignment between them. The
equation is related with a language model probabil-
ity, P(e), a translation model probability P(kle). In
this paper, we are interested in estimating the trans-
lation model probability P(kle).
</bodyText>
<equation confidence="0.993416">
P(elk) = P(e)P(kle)
p(k)
= arg nTx P(e)P(kle) (4)
</equation>
<bodyText confidence="0.99974025">
In this work, we modified the IBM 1 model using
the structural information for estimating translation
probabilities. The translation probability of a spe-
cific sentence pair, k and e is
</bodyText>
<equation confidence="0.983747333333333">
m
P(kle) = e t(kpjlepi)P(Ckp repi ) (5)
j=1 j=1
</equation>
<bodyText confidence="0.961529333333333">
In (5), an English sentence e has / possible
phrases, epiep2...e and a Korean sentence k has
m possible phrases, kpikp2...kpm and the phrases of
e have corresponding sequences of POS categories,
Cep1Cep2...C, , and the phrases of k have the se-
quences of PPOS categories Ckp1Ckp2...Ckpm,. E is
</bodyText>
<table confidence="0.982937571428572">
1 if ii(f) PAW
â€”1 otherwise
English Korean
Train Sentences 30,000
Words/Eojeol 282,967 201,771
Vocabulary 47,647 80,360
Morph Types 52,197 57,800
</table>
<tableCaption confidence="0.974152">
Table 2: Training corpus size
</tableCaption>
<table confidence="0.999526333333333">
English words Korean morphemes Ratio
1 1 31.2
1 2 22.4
1 3 13.6
2 1 6.9
etc etc 25.9
</table>
<tableCaption confidence="0.999938">
Table 3: The result of alignment unit
</tableCaption>
<bodyText confidence="0.9896635625">
a small, fixed normalizing number (Brown et al.,
1993).
Note that the probability P(Ckp, rep) is pre-
computed by the feature selection process and only
lexical alignment parameter t(kpj lep,) can be esti-
mated by the EM algorithm using sentence pairs of
bitext, (k(s), e(s)), s = 1,..., S. According to the
equation (4), we can eliminate the combination that
P(Ckp, &apos;Cep) is zero. Thus, the model iterates only
over a subset of probable alignments.
To estimate probability t(kplep), the expected
number of times(fractional counts) that the phrase
ep connects to kp in the translation (kle) is used.
The count denoted by e(kplep; k, e) can be expressed
in (7) using (6) and translation probabilities are rees-
timated by (8).
</bodyText>
<equation confidence="0.999074384615385">
P(ale, =
P(k, ale) P(k, ale)
(6)
P(kle) E P(k, ale)
711 1
e(kplep; k, e) =
j=1 i=1
t(kplep)P(ckplcep)
=
t(kplepOP(ckplcepi)+...+t(kplepi)P(ckplcepi)
t(kplep) = sc(kplep; k,
E E e(kp iep; k(s), e(s))
kp s=1
</equation>
<sectionHeader confidence="0.960959" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999942214285714">
We present results tested on English-Korean bitext
that is extracted from the web site of &apos;Korea Times&apos;
and a magazine for English learning (Table 2).
We manually aligned 700 POS-tagged sentence
pairs to obtain initial parameters for correspon-
dences of tag sequences. As shown in Table 3,
the coverage of word-for-word correspondences in
English-Korean bitext was only 31.2%.
As a result, 1,483 correspondences of tag se-
quences were collected from the manually aligned
bitext (Table 4).
The correspondences were used as initial active
features and the weights of the initial active fea-
tures were computed by ITS algorithm. Table 5
</bodyText>
<table confidence="0.59312825">
set description disjoint features
A active features 1,483
P filtered feature pool 8,147
N selected new features 702
</table>
<tableCaption confidence="0.983698">
Table 4: Features(correspondences of Tag Sequence)
</tableCaption>
<bodyText confidence="0.5893925">
shows weights A of the initial active features such
that f+
</bodyText>
<subsubsectionHeader confidence="0.519137">
,BEP-FJJ,tk E A.
</subsubsectionHeader>
<bodyText confidence="0.999748111111111">
Through the process of feature pool construction,
8,147 features(tag sequence correspondences) were
selected from the 23,000 sentence pairs. Since we
used the filtering method and tag sequence construc-
tion rules, the size of the feature pool was not large.
In the feature selection step, we chose useful fea-
tures with the gain threshold of 0.008. As a result of
feature selection, we obtained the probability model
that given a specific English POS tag sequence, a
corresponding Korean POS tag sequence happens to
occur.
Table 6 shows some examples of conditional prob-
abilities. The table shows that the determiner of
English is generally translated into NULL or adnom-
inal word in Korean. The conditional probabilities
regarding the correspondences of POS tag sequences
were used as known parameters of the lexical align-
ment model.
</bodyText>
<subsectionHeader confidence="0.998787">
Effect of Smoothing
</subsectionHeader>
<bodyText confidence="0.997883666666667">
As mentioned before, in previous IBM-based mod-
els, many words are aligned to the empty word and
rare words are mis-aligned. Thus, we tested if the
structural features are effective in smoothing. For
this, the accuracy of lexical correspondences was
evaluated both on low frequency words and high fre-
</bodyText>
<footnote confidence="0.732108307692308">
t e p(tkite)
tk
&apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 NNIN2 0.524131
&apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 ,
+++++++
H H H H H H (1)
ANDEÂ±NNIN2 0.15161
ANNUÂ±NNDE2 0.091036
NNIN2+PPCA1 0.063515
NNIN2+NNIN2 0.058322
NNIN2+PPAU 0.05768
ADCO 0.049622
etc
</footnote>
<tableCaption confidence="0.981727">
Table 6: Examples of conditional probability
</tableCaption>
<table confidence="0.999889176470588">
fte,tk Ai p(tk Ite) e k
fBEPÂ±JJ,VBMAÂ±ENC03Â±AXÂ±ENTE 10.1369 0.4247 are-Fprepared junbidoi-Feo-Fiss-Fda
fBEPÂ±JJ,V BMA 8.8520 0.1180 is-Fcareful ju&apos;yiha
fBEPÂ±JJ,AJMA 8.6787 0.0996 am-Fhealthy geongangha
fBEPÂ±JJ,AJMAÂ±ENTE 8.2628 0.0655 is-Fnew syaelob-Fda
fBEPÂ±JJ,VBMAÂ±ENTE 7.2379 0.0236 am-Fsure hwagsinha-Fbnida
fBEPÂ±JJ,NNIN2+CO 7.1372 0.0210 am-Frich buja-Fi
fBEPÂ±JJ,NNIN2+COÂ±VBMA 6.9909 0.0183 is-Fselfish igijeog-Fi-Fdoi
fBEPÂ±JJ,NNIN2+PPCA1+VBMAÂ±ENTE 6.8402 0.0157 is-Fpatriotic &apos;aegugja-Fga-Fdoi-Fda
fBEPÂ±JJ,NNIN2+COÂ±ENTE 6.8308 0.0156 is-Freasonable habligeog-Fi-Fda
fBEPÂ±JJ,NNIN2+PPCA2+AXÂ±ENTE 6.4256 0.0105 is-Frephrehensible binanbad-Feul-Fmanha-Fda
fBEPÂ±JJ,NNIN2+PPCA1+VBMA 6.4250 0.0105 am-Fhelpful doum-Fi-Fdoi-Fda
BE:be verb (present tense), DT:determiner, JJ:adjective(ordinal), NN:common noun, RB :adverb
AJMA:adjective, VBMA:verb, AX:auxilary verb ENC03:auxilary ending, ENTE:final ending
ANCO:configurative adnominal, ANDE: demonstrative adnominal , ANNU:numeral adnominal
NNIN2:common noun, NNDE2:unit-dependent noun, CO:copular
PPCA1:nominative postposition, PPCA2:accusative postposition, PPAU:auxilary postposition
</table>
<tableCaption confidence="0.99009">
Table 5: Examples of active features
</tableCaption>
<table confidence="0.997870125">
frequency English Korean Accuracy
Words Eojeols
(Total)
4 100(2149) 100(3317) 70.3%
50,-400 100(712) 100(486) 78.9%
the number of parameters
IBM 1 14,776
Our Model 1,344
</table>
<tableCaption confidence="0.999951">
Table 8: Problem Space
Table 7: Evaluation Vocabulary
</tableCaption>
<bodyText confidence="0.9919981">
quency words.
Table 7 shows the accuracy of some results ob-
tained by lexical alignment. In the training bitext,
English 2149 words and Korean 3317 eojeols with low
frequency 4 and English 712 words and Korean 486
eojeols with high frequency (50-300) were found.
For an evaluation, 100 words and 100 eojeols were se-
lected out of them. The alignments of the words (eo-
jeols) were evaluated. As a result, it turns out that
the structural features are effective in rare words.
</bodyText>
<subsectionHeader confidence="0.953217">
Effect of reducing a parameter space
</subsectionHeader>
<bodyText confidence="0.998869882352941">
Another advantage of the use of structural fea-
tures is to reduce parameter space of the lexical
alignment model. To show the impacts on the size of
the parameter space reduced by our model, we com-
pared the results from our model with those from
the IBM 1-model presented by Brown et al. (1993)
on 100 sentence pairs. The number of parameters
obtained means the counts of possible lexical map-
pings.
As shown in Table 8, the number of prameters
were drastically reduced in our model. Considering
the complexity of another models (IBM 2-5) it is
obvious that they have more parameters.
Effect of improving the results of lexical
alignments
For evaluating the efficacy of the structural fea-
tures, we compared the results with IBM model 1.
</bodyText>
<sectionHeader confidence="0.436169" genericHeader="evaluation">
Accuracy
</sectionHeader>
<table confidence="0.5393115">
IBM 1 59.8
Our Model 73.7
</table>
<tableCaption confidence="0.982849">
Table 9: Accuracy of n:1 and 1:1 alignments
</tableCaption>
<bodyText confidence="0.999907555555556">
As described before, only n(English):1 and 1:1 map-
ping are possible in IBM model since one-to-one cor-
respondence is assumed. Thus, we selected only n:1
and 1:1 alignments out of the alignment results. For
comparison, we investigated the alignments of the
English 100 words with high frequency, which were
explained above.
Table 9 shows the accuracy of lexical alignments.
It is shown that the structural features have an effect
on the alignment even though the amount of data
investigated is small.
However, the overall accuracy of the alignment is
somewhat low, which is mainly due to the small size
of training samples. Table 10 shows some results of
mutual translation. We see a considerable improve-
ment when allowing for structural features (corre-
spondences of POS tag sequences) in lexical align-
ments.
</bodyText>
<subsectionHeader confidence="0.735868">
Error Analysis
</subsectionHeader>
<bodyText confidence="0.999508">
Except the errors by the incorrect parameter esti-
mation, most errors of correspondences of POS tag
sequences are caused by POS tagging errors. In ad-
dition, the correspondences of adverbs turn out to be
</bodyText>
<table confidence="0.989434714285714">
Korean English Probability
jeongbu government 0.7312
jeongbu the government 0.2012
jeongbu republic 0.0328
jeongbu authorities 0.0171
jeongbu officials 0.0119
jeongbu Chinese 0.0041
</table>
<tableCaption confidence="0.99747">
Table 10: Examples of alignment
</tableCaption>
<bodyText confidence="0.9824795">
sometimes erroneous, which is due to the fact that
the position of adverb can be moved quite free.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989904761905">
Because of the considerable difference between En-
glish and Korean, computation cost of lexical align-
ment is very high. One solution of the problem is to
provide mapping information of syntactic structure
between the two languages.
In this paper, we defined the structural feature as
the correspondence of POS tag sequences and pre-
sented a method for extraction of structural features
for Korean-English bilingual alignment. Firstly, the
initial active features were collected from a small
size of manually aligned bitext, which are trained by
IIS which is a training algorithm for ME. Secondly,
extracted from training data, the features giving a
large gain were added to the set of active features.
Furthermore, the features extracted were tested
for lexical alignment of bitext. The experiment
showed that the features are helpful for reducing
the mapping space in alignment. We expect that
the alignment can be more accurate and efficient by
combining the structural features with translation
lexicon in the future.
</bodyText>
<sectionHeader confidence="0.999068" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995587253012048">
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-73.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4):543-565.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
A. P. Dempster, N. M. Laird and D. B. Rubin.
1976. Maximum likelihood from incomplete data
via the EM algorithm. The Royal Statistics Soci-
ety, 39(B) 205-237.
Pascale Fung and Kenneth W. Church. 1994. Kvec:
A New Approach for Aligning Parallel Texts In
Proceedings of COLING 94, 1096-1102.
William A. Gale and Kenneth W. Church. 1991.
Identifying Word Correspondance in Parallel Text
In Proceedings of the DARPA NLP Workshop
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual cor-
pora. Computational Linguistics, 19:75-102.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition MIT Press.
Julian Kupiec. 1993. An algorithm for finding noun
phrase correspondences in bilingual corpora. In
Proceedings of ACL 31, 17-22.
Yuji Matsumoto. 1993. Structural matching of par-
allel texts. In Proceedings of the 31st Annual
Meeting of the ACL, 23-30.
I. Dan Melamed. 1997. A word-to-word model of
translation equivalence. In Proceedings of ACL
35/EACL 8, 16-23.
Franz Josef Och and Hermann Ney. 2000. Improv-
ing Statistical Alignment Models. In Proceedings
of ACL 38, 440-447.
H. Papageorgiou, L. Cranias and S. Piperidis. 1994.
Automatic Alignment in Parallel Corpora. In Pro-
ceedings of ACL 32 (Student Session).
Stephen A. Della Pietra, Vincent J. Della Pietra,
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(4):380-393.
Kengo Sato 1998. Maximum Entropy Model Learn-
ing of the Translation Rules. In Proceedings of
ACL 36/COLING, 1171-1175.
Jung H. Shin, Young S. Han, and Key-Sun
Choi. 1996. Bilingual knowledge acquisition from
Korean-English parallel corpus using alignment
method. In Proceedings of COLING 96.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach.
Computational Linguistics, 22(1):1-38.
Christoph Tillmann and Hermann Ney. 2000 Word
Re-ordering and DP-based Search in Statistical
Machine Translation. In Proceedings of COLING
2000.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in machine translation. In Proceedings
of ACL 36/COLING
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2000. Finding Structural Correspondences
from Bilingual Parsed Corpus for Corpus-based
Translation. In Proceedings of COLING 2000.
Dekai Wu 1997. Stochastic Inversion Transduction
Grammar and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23-3, pp. 377-
403.
Kaoru Yamamoto and Yuji Matsumoto. 2000. Ac-
quisition of Phrase-level Bilingual Correspondence
using Dependency Structure. In Proceedings of
COLING 2000.
Juntae Yoon, Chunghee Lee, Seonho Kim, and Man-
suk Song. 1999. Morphological Analyzer of Yon-
sei University., Morany: Morphological Analysis
based on Large Lexical Database Extracted from
Corpus. In Proceedings of Hangule and Korean
Information Processing Workshop, pp.92-98.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191839">
<title confidence="0.9938435">Improving Lexical Mapping Model of English-Korean Using Structural Features</title>
<author confidence="0.67327">Kimt Yoont</author>
<note confidence="0.569877">tDept. of Computer Science, Yonsei Seoul 120-749, NLP Lab., Daumsoft Inc., Kangnam-gu, Yungjeon Bldg., Seoul 135-728,</note>
<email confidence="0.999157">jtyoon@daumsoft.com</email>
<abstract confidence="0.99917085">The problem of finding lexical alignments for given sentence pairs is computationally expensive. Furthermore, it is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of word-for-word correspondences is low. This paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments. We describe how the features improve the performance of the lexical alignment model. The structural features provide the information for the correspondences of parts-ofspeech (POS) sequences which are useful in translation. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1581" citStr="Berger et al., 1996" startWordPosition="223" endWordPosition="226">OS) sequences which are useful in translation. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occ</context>
<context position="8896" citStr="Berger et al., 1996" startWordPosition="1381" endWordPosition="1384">ponds to an eojeol &amp;quot;d4- 2___(haggyo-ro, school-to)&apos; in Korean. Thus, we need a method for mapping POS sequences of bitext. In this paper, the correspondences of POS tag sequences are obtained by automatic feature selection based on the ME framework. Here, a feature is defined as a correspondence of POS tag sequences in bitext. The outline of finding correspondences of POS tag sequences is as follows: First, our model starts with initial features obtained by a supervision step. The initial features are extracted from a small portion of bitext, which of weights are trained by the IIS algorithm (Berger et al., 1996; Pietra et al., 1997). These are called initial active features. In the next step, a feature pool is constructed from training data. At this time, only features giving a large gain to the model are selected. The final output of feature selection is the set of active features and the correspondences of POS tag sequences that are represented with conditional probabilities. The resulted features are used for the parameters of bilingual text alignment. In the process, we look at the words to ensure a correct alignment. That is, the POS sequences are in fact encoding particular words. The underlyi</context>
<context position="14734" citStr="Berger et al., 1996" startWordPosition="2399" endWordPosition="2402">ition of a feature L affects only the single parameter a. The only parameter which distinguishes models of (2) is a. 1 (2) pAf, = zct(x)PA(Yix)eaL(x&apos;Y) Z(x) =EPA(Yix)ectMx&apos;Y) The improvement (gain) of a model after adding a single feature L can be estimated by measuring difference of maximum log-likelihood between L(pAL) and L(pA). We denote the gain for feature L by A(,41), which is represented in (3). A (AL) maxcEGAf, (a) (3) GAfi (a) = L(PAf) L(PA) log PAâ€˜f PA The following algorithm is used to compute the gain of the model with respect to L. We adopted Berger&apos;s method for computing gains (Berger et al., 1996). For the details, the reader is referred to (Berger et al., 1996; Pietra et al., 1997). 1. Let r = { 2. Set a() = 0 3. Repeat the following until GAL (an) has converged: Compute anÂ±i from an using anÂ±i = an + 1 log (1 G GA:tf: (0)) Compute GAL (an+i) using GAL (a) = â€”E.73(x)logz(x)+c(fi) GiAfi(a) = .13(M â€” Ex /3(x)m(x) , CA.fi (a) = â€” E. i)(x)p.Afi â€”m(x))2 Ix) where a = an+i, Afi = Au M(x) Pc:kfi(filx) P.Afi(filX) = PAfi (Y1x)fi(x,Y) 4. Set â€” AL(AL) GAL (an) This algorithm is iteratively computed using Netwon&apos;s method. With the gain value, we can recognize importance of a feature, i.e. how mu</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="10787" citStr="Brill, 1995" startWordPosition="1707" endWordPosition="1708">tioned above, features which represent mappings of POS sequences in two languages are automatically learned from bitext. The learning consists of two steps: (1) supervised step and (2) unsupervised feature selection. In the supervised step, we manually aligned English and Korean texts. From the manually aligned text, we consturct a feature pool which has initial POS sequence correspondence. In the unsupervised feature selection, the POS sequence correspondeces are added to the feature pool. 4.1 Supervision Step In the supervision step, a small portion of bitext is tagged using Brill&apos;s tagger (Brill, 1995) and `MORANY&apos; (Yoon et. al., 1999), each of which is for English and Korean tagging respectively. We manually aligned each sentence pair in the bitext and collected their correspondences of tag sequences. For simplicity, we adjusted some part of Brill&apos;s tag set. First of all, we classified sentential patterns of English and Korean. Then, we aligned English and Korean sentence pairs on the basis of the patterns. Also, we made tag sequence construction rules with respect to each language by analysis of the cases where the words of one unit are separated and adjacent. The rules are used when maki</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1560" citStr="Brown et al., 1993" startWordPosition="219" endWordPosition="222">of parts-ofspeech (POS) sequences which are useful in translation. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk a</context>
<context position="17198" citStr="Brown et al., 1993" startWordPosition="2828" endWordPosition="2831"> e has / possible phrases, epiep2...e and a Korean sentence k has m possible phrases, kpikp2...kpm and the phrases of e have corresponding sequences of POS categories, Cep1Cep2...C, , and the phrases of k have the sequences of PPOS categories Ckp1Ckp2...Ckpm,. E is 1 if ii(f) PAW â€”1 otherwise English Korean Train Sentences 30,000 Words/Eojeol 282,967 201,771 Vocabulary 47,647 80,360 Morph Types 52,197 57,800 Table 2: Training corpus size English words Korean morphemes Ratio 1 1 31.2 1 2 22.4 1 3 13.6 2 1 6.9 etc etc 25.9 Table 3: The result of alignment unit a small, fixed normalizing number (Brown et al., 1993). Note that the probability P(Ckp, rep) is precomputed by the feature selection process and only lexical alignment parameter t(kpj lep,) can be estimated by the EM algorithm using sentence pairs of bitext, (k(s), e(s)), s = 1,..., S. According to the equation (4), we can eliminate the combination that P(Ckp, &apos;Cep) is zero. Thus, the model iterates only over a subset of probable alignments. To estimate probability t(kplep), the expected number of times(fractional counts) that the phrase ep connects to kp in the translation (kle) is used. The count denoted by e(kplep; k, e) can be expressed in (</context>
<context position="22566" citStr="Brown et al. (1993)" startWordPosition="3624" endWordPosition="3627">lish 712 words and Korean 486 eojeols with high frequency (50-300) were found. For an evaluation, 100 words and 100 eojeols were selected out of them. The alignments of the words (eojeols) were evaluated. As a result, it turns out that the structural features are effective in rare words. Effect of reducing a parameter space Another advantage of the use of structural features is to reduce parameter space of the lexical alignment model. To show the impacts on the size of the parameter space reduced by our model, we compared the results from our model with those from the IBM 1-model presented by Brown et al. (1993) on 100 sentence pairs. The number of parameters obtained means the counts of possible lexical mappings. As shown in Table 8, the number of prameters were drastically reduced in our model. Considering the complexity of another models (IBM 2-5) it is obvious that they have more parameters. Effect of improving the results of lexical alignments For evaluating the efficacy of the structural features, we compared the results with IBM model 1. Accuracy IBM 1 59.8 Our Model 73.7 Table 9: Accuracy of n:1 and 1:1 alignments As described before, only n(English):1 and 1:1 mapping are possible in IBM mode</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm. The Royal Statistics Society,</title>
<date>1976</date>
<volume>39</volume>
<pages>205--237</pages>
<marker>Dempster, Laird, Rubin, 1976</marker>
<rawString>A. P. Dempster, N. M. Laird and D. B. Rubin. 1976. Maximum likelihood from incomplete data via the EM algorithm. The Royal Statistics Society, 39(B) 205-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kenneth W Church</author>
</authors>
<title>Kvec: A New Approach for Aligning Parallel Texts In</title>
<date>1994</date>
<booktitle>Proceedings of COLING 94,</booktitle>
<pages>1096--1102</pages>
<contexts>
<context position="1835" citStr="Fung and Church, 1994" startWordPosition="263" endWordPosition="266">lignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-occur more than expected by chance , which is represented as indirect association. techniques based on IBM model 1-5 2 have been employed (Brown et al., 1993). They were usually incorporated in the EM algorithm (</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>Pascale Fung and Kenneth W. Church. 1994. Kvec: A New Approach for Aligning Parallel Texts In Proceedings of COLING 94, 1096-1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Identifying Word Correspondance in Parallel Text</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA NLP Workshop</booktitle>
<contexts>
<context position="1811" citStr="Gale and Church, 1991" startWordPosition="259" endWordPosition="262">lp get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-occur more than expected by chance , which is represented as indirect association. techniques based on IBM model 1-5 2 have been employed (Brown et al., 1993). They were usually incorporat</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. Identifying Word Correspondance in Parallel Text In Proceedings of the DARPA NLP Workshop</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--75</pages>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19:75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="1653" citStr="Kupiec, 1993" startWordPosition="236" endWordPosition="238">oncept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-occur more than expected by ch</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of ACL 31, 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Structural matching of parallel texts.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="5490" citStr="Matsumoto, 1993" startWordPosition="831" endWordPosition="833">respondence in Korean and English bitext is approximately 34% and manyto-many mappings exceed 40% in lexical alignments. Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1. Thus, we use structural or linguistic information to find accurate lexical alignments of bitext. In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency structure (Yamamoto, 2000). To find these structural correspondences between a language pair, unification-based grammar (Matsumoto, 1993) or bilingual grammar (Wu, 1997) can be used. That is, if we try to find structural match of bitext, syntactic analysis of each text should be done first. However, syntactic analysis of each text(or bitext) is also a hard and computationally complicated problem. In this paper, we propose POS sequence feature. We consider correspondences between POS tag sequences of bitext as the structural information which is important in determining structures and reducing unnecessary parameters of a statistical alignment model. We induce the correspondences(structural features) by a feature selection method</context>
</contexts>
<marker>Matsumoto, 1993</marker>
<rawString>Yuji Matsumoto. 1993. Structural matching of parallel texts. In Proceedings of the 31st Annual Meeting of the ACL, 23-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A word-to-word model of translation equivalence.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL 35/EACL 8,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1597" citStr="Melamed, 1997" startWordPosition="227" endWordPosition="228">re useful in translation. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then</context>
<context position="4725" citStr="Melamed, 1997" startWordPosition="714" endWordPosition="715">lish Korean can/MD eu1/ENTR1 sa/NNDE ss/AJ da/ENTE World/NNP Cup/NNP weoideakeop/NNIN1 to/TO take/VBP off/RP iryugha/VB neart/ENTR1 MD:modal auxiliary, NNP:proper noun, VBP:verb, RP:particle NNDE:dependent noun, AJ:adjective ENTE: final ending, NNIN1:proper noun, VB:verb, ENTR1:adnominal ending Table 1: Lexical differences between English and Korean overcome the above problem in bilingual text alignment. 2 The problems of lexical alignment in Korean and English bitext Although one-to-one correspondence assumption has been shown to give highly accurate results in closely related language pair (Melamed, 1997), it does not fit in structurally different language pair such as Korean and English. According to Shin et al. (1996), the coverage of one-to-one correspondence in Korean and English bitext is approximately 34% and manyto-many mappings exceed 40% in lexical alignments. Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1. Thus, we use structural or linguistic information to find accurate lexical alignments of bitext. In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe </context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A word-to-word model of translation equivalence. In Proceedings of ACL 35/EACL 8, 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improving Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL 38,</booktitle>
<pages>440--447</pages>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improving Statistical Alignment Models. In Proceedings of ACL 38, 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Papageorgiou</author>
<author>L Cranias</author>
<author>S Piperidis</author>
</authors>
<title>Automatic Alignment in Parallel Corpora.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL 32</booktitle>
<location>(Student Session).</location>
<contexts>
<context position="7328" citStr="Papageorgiou et al., 1994" startWordPosition="1125" endWordPosition="1128">ignment in a relatively small size of corpora. 4. The structural information is helpful in the construction of a bilingual grammar. 3 Overview of the model In general, there exist constaints among POS sequences mapping when aligning bitext. For instance, in many cases a noun in Korean is translated to a noun or a noun preceded by an article in English. This POS constraint would be useful information in alignment of a closely related pair as well as a less closely related language pair. Some approaches design an alignment algorithm that maximizes the number of matching POS in aligned segments (Papageorgiou et al., 1994). We also assume that the mapping information of POS sequences of a language pair is useful in a model of statistical machine translation. If there are similarities between corresponding POS sequences in bitext, the structural feature would be easily computed or identified. However, a POS sequence in English often correspond to a totally different POS sequence in Korean as shown in the following example: can/MD eui/ENTR1 sa/NNDE1 &apos;iss/AJMA da/ENTE It is caused by the discrepancy between two languages. Korean is an agglutinative language. A sentence in Korean consists of a series of syntactic u</context>
</contexts>
<marker>Papageorgiou, Cranias, Piperidis, 1994</marker>
<rawString>H. Papageorgiou, L. Cranias and S. Piperidis. 1994. Automatic Alignment in Parallel Corpora. In Proceedings of ACL 32 (Student Session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="8918" citStr="Pietra et al., 1997" startWordPosition="1385" endWordPosition="1388">4- 2___(haggyo-ro, school-to)&apos; in Korean. Thus, we need a method for mapping POS sequences of bitext. In this paper, the correspondences of POS tag sequences are obtained by automatic feature selection based on the ME framework. Here, a feature is defined as a correspondence of POS tag sequences in bitext. The outline of finding correspondences of POS tag sequences is as follows: First, our model starts with initial features obtained by a supervision step. The initial features are extracted from a small portion of bitext, which of weights are trained by the IIS algorithm (Berger et al., 1996; Pietra et al., 1997). These are called initial active features. In the next step, a feature pool is constructed from training data. At this time, only features giving a large gain to the model are selected. The final output of feature selection is the set of active features and the correspondences of POS tag sequences that are represented with conditional probabilities. The resulted features are used for the parameters of bilingual text alignment. In the process, we look at the words to ensure a correct alignment. That is, the POS sequences are in fact encoding particular words. The underlying process is as follo</context>
<context position="14821" citStr="Pietra et al., 1997" startWordPosition="2415" endWordPosition="2418">inguishes models of (2) is a. 1 (2) pAf, = zct(x)PA(Yix)eaL(x&apos;Y) Z(x) =EPA(Yix)ectMx&apos;Y) The improvement (gain) of a model after adding a single feature L can be estimated by measuring difference of maximum log-likelihood between L(pAL) and L(pA). We denote the gain for feature L by A(,41), which is represented in (3). A (AL) maxcEGAf, (a) (3) GAfi (a) = L(PAf) L(PA) log PAâ€˜f PA The following algorithm is used to compute the gain of the model with respect to L. We adopted Berger&apos;s method for computing gains (Berger et al., 1996). For the details, the reader is referred to (Berger et al., 1996; Pietra et al., 1997). 1. Let r = { 2. Set a() = 0 3. Repeat the following until GAL (an) has converged: Compute anÂ±i from an using anÂ±i = an + 1 log (1 G GA:tf: (0)) Compute GAL (an+i) using GAL (a) = â€”E.73(x)logz(x)+c(fi) GiAfi(a) = .13(M â€” Ex /3(x)m(x) , CA.fi (a) = â€” E. i)(x)p.Afi â€”m(x))2 Ix) where a = an+i, Afi = Au M(x) Pc:kfi(filx) P.Afi(filX) = PAfi (Y1x)fi(x,Y) 4. Set â€” AL(AL) GAL (an) This algorithm is iteratively computed using Netwon&apos;s method. With the gain value, we can recognize importance of a feature, i.e. how much the feature accords with the model. As a result, we can select useful features with </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen A. Della Pietra, Vincent J. Della Pietra, John D. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kengo Sato</author>
</authors>
<title>Maximum Entropy Model Learning of the Translation Rules.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL 36/COLING,</booktitle>
<pages>1171--1175</pages>
<marker>Sato, 1998</marker>
<rawString>Kengo Sato 1998. Maximum Entropy Model Learning of the Translation Rules. In Proceedings of ACL 36/COLING, 1171-1175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung H Shin</author>
<author>Young S Han</author>
<author>Key-Sun Choi</author>
</authors>
<title>Bilingual knowledge acquisition from Korean-English parallel corpus using alignment method.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96.</booktitle>
<contexts>
<context position="1625" citStr="Shin et al., 1996" startWordPosition="230" endWordPosition="233">. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-oc</context>
<context position="4842" citStr="Shin et al. (1996)" startWordPosition="733" endWordPosition="736">gha/VB neart/ENTR1 MD:modal auxiliary, NNP:proper noun, VBP:verb, RP:particle NNDE:dependent noun, AJ:adjective ENTE: final ending, NNIN1:proper noun, VB:verb, ENTR1:adnominal ending Table 1: Lexical differences between English and Korean overcome the above problem in bilingual text alignment. 2 The problems of lexical alignment in Korean and English bitext Although one-to-one correspondence assumption has been shown to give highly accurate results in closely related language pair (Melamed, 1997), it does not fit in structurally different language pair such as Korean and English. According to Shin et al. (1996), the coverage of one-to-one correspondence in Korean and English bitext is approximately 34% and manyto-many mappings exceed 40% in lexical alignments. Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1. Thus, we use structural or linguistic information to find accurate lexical alignments of bitext. In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency structure (Yamamoto, 2000). To find these structural correspondences between a language p</context>
</contexts>
<marker>Shin, Han, Choi, 1996</marker>
<rawString>Jung H. Shin, Young S. Han, and Key-Sun Choi. 1996. Bilingual knowledge acquisition from Korean-English parallel corpus using alignment method. In Proceedings of COLING 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1692" citStr="Smadja et al., 1996" startWordPosition="241" endWordPosition="244">are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge. 1 Introduction Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval. Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), and collocation (Smadja et al., 1996) in a bitext. Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994). However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-occur more than expected by chance , which is represented as indirect</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word Re-ordering and DP-based Search in Statistical Machine Translation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="2491" citStr="Tillmann and Ney, 2000" startWordPosition="373" endWordPosition="376">s can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997). In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Uk-ki often co-occur in text. Then vk and 24+1 will also co-occur more than expected by chance , which is represented as indirect association. techniques based on IBM model 1-5 2 have been employed (Brown et al., 1993). They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000). However, we are often faced with some difficulties as follows, when the IBM model-based approaches are directly applied to the alignment, especially on bitext involving a less closely related language pair. 1. It needs excessive iteration time for parameter estimation and high decoding complexity. Thus most systems assumed one-to-one correspondence to reduce computational complexity. However, word sequences are not translated literally word for word. For example, in cases of collocations, compound nouns, and ambiguous words with different meaning dependent on the context, </context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2000 Word Re-ordering and DP-based Search in Statistical Machine Translation. In Proceedings of COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Modeling with structures in machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL 36/COLING</booktitle>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1998. Modeling with structures in machine translation. In Proceedings of ACL 36/COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
<author>Sadao Kurohashi</author>
<author>Eiji Aramaki</author>
</authors>
<title>Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="5338" citStr="Watanabe et al., 2000" startWordPosition="810" endWordPosition="813">ed, 1997), it does not fit in structurally different language pair such as Korean and English. According to Shin et al. (1996), the coverage of one-to-one correspondence in Korean and English bitext is approximately 34% and manyto-many mappings exceed 40% in lexical alignments. Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1. Thus, we use structural or linguistic information to find accurate lexical alignments of bitext. In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency structure (Yamamoto, 2000). To find these structural correspondences between a language pair, unification-based grammar (Matsumoto, 1993) or bilingual grammar (Wu, 1997) can be used. That is, if we try to find structural match of bitext, syntactic analysis of each text should be done first. However, syntactic analysis of each text(or bitext) is also a hard and computationally complicated problem. In this paper, we propose POS sequence feature. We consider correspondences between POS tag sequences of bitext as the structural information which is important in determining structure</context>
</contexts>
<marker>Watanabe, Kurohashi, Aramaki, 2000</marker>
<rawString>Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki. 2000. Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation. In Proceedings of COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammar and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="5522" citStr="Wu, 1997" startWordPosition="837" endWordPosition="838">t is approximately 34% and manyto-many mappings exceed 40% in lexical alignments. Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1. Thus, we use structural or linguistic information to find accurate lexical alignments of bitext. In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency structure (Yamamoto, 2000). To find these structural correspondences between a language pair, unification-based grammar (Matsumoto, 1993) or bilingual grammar (Wu, 1997) can be used. That is, if we try to find structural match of bitext, syntactic analysis of each text should be done first. However, syntactic analysis of each text(or bitext) is also a hard and computationally complicated problem. In this paper, we propose POS sequence feature. We consider correspondences between POS tag sequences of bitext as the structural information which is important in determining structures and reducing unnecessary parameters of a statistical alignment model. We induce the correspondences(structural features) by a feature selection method based on the ME framework. Fina</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu 1997. Stochastic Inversion Transduction Grammar and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23-3, pp. 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquisition of Phrase-level Bilingual Correspondence using Dependency Structure.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Yamamoto, Matsumoto, 2000</marker>
<rawString>Kaoru Yamamoto and Yuji Matsumoto. 2000. Acquisition of Phrase-level Bilingual Correspondence using Dependency Structure. In Proceedings of COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juntae Yoon</author>
<author>Chunghee Lee</author>
<author>Seonho Kim</author>
<author>Mansuk Song</author>
</authors>
<title>Morphological Analyzer of Yonsei University., Morany: Morphological Analysis based on Large Lexical Database Extracted from Corpus.</title>
<date>1999</date>
<booktitle>In Proceedings of Hangule and Korean Information Processing Workshop,</booktitle>
<pages>92--98</pages>
<marker>Yoon, Lee, Kim, Song, 1999</marker>
<rawString>Juntae Yoon, Chunghee Lee, Seonho Kim, and Mansuk Song. 1999. Morphological Analyzer of Yonsei University., Morany: Morphological Analysis based on Large Lexical Database Extracted from Corpus. In Proceedings of Hangule and Korean Information Processing Workshop, pp.92-98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>