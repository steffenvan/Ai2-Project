<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004637">
<title confidence="0.997693">
Intrinsic vs. Extrinsic Evaluation Measures for
Referring Expression Generation
</title>
<author confidence="0.989991">
Anja Belz
</author>
<affiliation confidence="0.9933385">
Natural Language Technology Group
University of Brighton
</affiliation>
<address confidence="0.991887">
Brighton BN2 4GJ, UK
</address>
<email confidence="0.999005">
a.s.belz@brighton.ac.uk
</email>
<author confidence="0.993322">
Albert Gatt
</author>
<affiliation confidence="0.99738">
Department of Computing Science
University of Aberdeen
</affiliation>
<address confidence="0.959642">
Aberdeen AB24 3UE, UK
</address>
<email confidence="0.998684">
a.gatt@abdn.ac.uk
</email>
<sectionHeader confidence="0.99584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9914534">
In this paper we present research in which we
apply (i) the kind of intrinsic evaluation met-
rics that are characteristic of current compara-
tive HLT evaluation, and (ii) extrinsic, human
task-performance evaluations more in keeping
with NLG traditions, to 15 systems implement-
ing a language generation task. We analyse
the evaluation results and find that there are no
significant correlations between intrinsic and
extrinsic evaluation measures for this task.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906769230769">
In recent years, NLG evaluation has taken on a more
comparative character. NLG now has evaluation re-
sults for comparable, but independently developed
systems, including results for systems that regener-
ate the Penn Treebank (Langkilde, 2002) and sys-
tems that generate weather forecasts (Belz and Re-
iter, 2006). The growing interest in comparative
evaluation has also resulted in a tentative interest in
shared-task evaluation events, which led to the first
such event for NLG (the Attribute Selection for Gen-
eration of Referring Expressions, or ASGRE, Chal-
lenge) in 2007 (Belz and Gatt, 2007), with a second
event (the Referring Expression Generation, or REG,
Challenge) currently underway.
In HLT in general, comparative evaluations (and
shared-task evaluation events in particular) are dom-
inated by intrinsic evaluation methodologies, in con-
trast to the more extrinsic evaluation traditions of
NLG. In this paper, we present research in which we
applied both intrinsic and extrinsic evaluation meth-
ods to the same task, in order to shed light on how
the two correlate for NLG tasks. The results show a
surprising lack of correlation between the two types
of measures, suggesting that intrinsic metrics and
extrinsic methods can represent two very different
views of how well a system performs.
</bodyText>
<sectionHeader confidence="0.548032" genericHeader="method">
2 Task, Data and Systems
</sectionHeader>
<bodyText confidence="0.999891407407407">
Referring expression generation (REG) is concerned
with the generation of expressions that describe en-
tities in a given piece of discourse. REG research
goes back at least to the 1980s (Appelt, Grosz, Joshi,
McDonald and others), but the field as it is today
was shaped in particular by Dale and Reiter’s work
(Dale, 1989; Dale and Reiter, 1995). REG tends to be
divided into the stages of attribute selection (select-
ing properties of entities) and realisation (convert-
ing selected properties into word strings). Attribute
selection in its standard formulation was the shared
task in the ASGRE Challenge: given an intended ref-
erent (‘target’) and the other domain entities (‘dis-
tractors’) each with possible attributes, select a set
of attributes for the target referent.
The ASGRE data (which is now publicly available)
consists of all 780 singular items in the TUNA corpus
(Gatt et al., 2007) in two subdomains, consisting of
descriptions of furniture and people. Each data item
is a paired attribute set (as derived from a human-
produced RE) and domain representation (target and
distractor entities represented as possible attributes
and values).
ASGRE participants were asked to submit the out-
puts produced by their systems for an unseen test
data set. The outputs from 15 of these systems,
shown in the left column of Table 1, were used in
</bodyText>
<page confidence="0.983229">
197
</page>
<reference confidence="0.222065">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 197–200,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999753333333333">
the experiments reported below. Systems differed
in terms of whether they were trainable, performed
exhaustive search and hardwired use of certain at-
tributes types, among other algorithmic properties
(see the ASGRE papers for full details). In the case
of one system (IS-FBS), a buggy version was origi-
nally submitted and used in Exp 1. It was replaced in
Exp 2 by a corrected version; the former is marked
by a * in what follows.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="method">
3 Evaluation Methods
</sectionHeader>
<bodyText confidence="0.986531925">
1. Extrinsic evaluation measures: We conducted
two task-performance evaluation experiments (the
first was part of the ASGRE Challenge, the second
is new), in which participants identified the referent
denoted by a description by clicking on a picture in
a visual display of target and distractor entities. To
enable subjects to read the outputs of peer systems,
we converted them from the attribute-value format
described above to something more readable, using
a simple attribute-to-word converter.
Both experiments used a Repeated Latin Squares
design, and involved 30 participants and 2,250 indi-
vidual trials (see Belz &amp; Gatt (2007) for full details).
In Exp 1, subjects were shown the domain on
the same screen as the description. Two depen-
dent measures were used: (i) combined reading and
identification time (RIT), measured from the point at
which the description and pictures appeared on the
screen to the point at which a picture was selected
by mouse-click; and (ii) error rate (ER-1).
In Exp 2, subjects first read the description and
then initiated the presentation of domain entities.
We computed: (i) reading time (RT), measured from
the presentation of a description to the point where
a subject requested the presentation of the domain;
(ii) identification time (IT), measured from the pre-
sentation of the domain to the point where a subject
clicked on a picture; and (iii) error rate (ER-2).
2. REG-specific intrinsic measures: Unique-
ness is the proportion of attribute sets generated by
a system which identify the referent uniquely (i.e.
none of the distractors). Minimality is the propor-
tion of attribute sets which are minimal as well as
unique (i.e. there is no smaller unique set of at-
tributes). These measures were included because
they are commonly named as desiderata for attribute
selection algorithms in the REG field (Dale, 1989).
The minimality check used in this paper treats refer-
ent type as a simple attribute, as the ASGRE systems
tended to do.1
</bodyText>
<listItem confidence="0.958881333333333">
3. Set-similarity measures: The Dice similarity
coefficient computes the similarity between a peer
attribute set A1 and a (human-produced) reference
</listItem>
<bodyText confidence="0.9023695">
attribute set A2 as 2 !
IAi ,nA1 J . MASI (Passonneau,
2006) is similar but biased in favour of similarity
where one set is a subset of the other.
</bodyText>
<listItem confidence="0.6945616">
4. String-similarity measures: In order to apply
string-similarity metrics, peer and reference outputs
were converted to word-strings by the method de-
scribed under 1 above. String-edit distance (SE) is
straightforward Levenshtein distance with a substi-
</listItem>
<bodyText confidence="0.871990384615385">
tution cost of 2 and insertion/deletion cost of 1. We
also used the version of string-edit distance (‘SEB’)
of Bangalore et al. (2000) which normalises for
length. BLEU computes the proportion of word n-
grams (n ≤ 4 is standard) that a peer output shares
with several reference outputs. The NIST MT eval-
uation metric (Doddington, 2002) is an adaptation
of BLEU which gives more importance to less fre-
quent (hence more informative) n-grams. We also
used two versions of the ROUGE metric (Lin and
Hovy, 2003), ROUGE-2 and ROUGE-SU4 (based on
non-contiguous, or ‘skip’, n-grams), which were of-
ficial scores in the DUC 2005 summarization task.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999560384615385">
Results for all evaluation measures and all systems
are shown in Table 1. Uniqueness results are not
included, as all systems scored 100%.
We ran univariate analyses of variance (ANOVAs)
using SYSTEM as the independent variable (15
levels), testing its effect on the extrinsic task-
performance measures. For error rate (ER), we used
a Kruskal-Wallis ranks test to compare identifica-
tion accuracy rates across systems2. The main effect
of SYSTEM was significant on RIT (F(14,2249) =
6.401, p &lt; .001), RT (F(14,2249) = 2.56, p &lt;
.01), and IT (F(14,2249) = 1.93, p &lt; .01). In nei-
ther experiment was there a significant effect on ER.
</bodyText>
<footnote confidence="0.994166">
1As a consequence, the Minimality results we report here
look different from those in the ASGRE report.
2A non-paramteric test was more appropriate given the large
number of zero values in ER proportions, and a high dependency
of variance on the mean.
</footnote>
<page confidence="0.98996">
198
</page>
<table confidence="0.9999025">
extrinsic REG string-similarity set-similarity
RIT RT IT ER-1 ER-2 Min RSU4 R-2 NIST BLEU SE SEB Dice MASI
CAM-B 2784.80 1309.07 1952.39 9.33 5.33 8.11 .673 .647 2.70 .309 4.42 .307 .620 .403
CAM-BU 2659.37 1251.32 1877.95 9.33 4 10.14 .663 .638 2.61 .317 4.23 .359 .630 .420
CAM-T 2626.02 1475.31 1978.24 10 5.33 0 .698 .723 3.50 .415 3.67 .496 .725 .560
CAM-TU 2572.82 1297.37 1809.04 8.67 4 0 .677 .691 3.28 .407 3.71 .494 .721 .557
DIT-DS 2785.40 1304.12 1859.25 10.67 2 0 .651 .679 4.23 .457 3.55 .525 .750 .595
GR-FP 2724.56 1382.04 2053.33 8.67 3.33 4.73 .65 .649 3.24 .358 3.87 .441 .689 .480
GR-SC 2811.09 1349.05 1899.59 11.33 2 4.73 .644 .644 2.42 .305 4 .431 .671 .466
IS-FBN 3570.90 1837.55 2188.92 15.33 6 1.35 .771 .772 4.75 .521 3.15 .438 .770 .601
IS-FBS – 1461.45 2181.88 – 7.33 100 .485 .448 2.11 .166 5.53 .089 .368 .182
*IS-FBS 4008.99 – – 10 – 39.86 – – – – – – .527 .281
IS-IAC 2844.17 1356.15 1973.19 8.67 6 0 .612 .623 3.77 .442 3.43 .559 .746 .597
NIL 1960.31 1482.67 1960.31 10 5.33 20.27 .525 .509 3.32 .32 4.12 .447 .625 .477
T-AS+ 2652.85 1321.20 1817.30 9.33 4.67 0 .671 .684 2.62 .298 4.24 .37 .660 .452
T-AS 2864.93 1229.42 1766.35 10 4.67 0 .683 .692 2.99 .342 4.10 .393 .645 .422
T-RS+ 2759.76 1278.01 1814.93 6.67 1.33 0 .677 .697 2.85 .303 4.32 .36 .669 .459
T-RS 2514.37 1255.28 1866.94 8.67 4.67 0 .694 .711 3.16 .341 4.18 .383 .655 .432
</table>
<tableCaption confidence="0.994946">
Table 1: Results for all systems and evaluation measures (ER-1 = error rate in Exp 1, ER-2 = error rate in Exp 2). (R =
ROUGE; system IDs as in the ASGRE papers, except GR = GRAPH; T = TITCH).
</tableCaption>
<bodyText confidence="0.977106785714286">
Table 2 shows correlations between the automatic
metrics and the task-performance measures from
Exp 1. RIT and ER-1 are not included because of
the presence of *IS-FBS in Exp 1 (but see individual
results below). For reasons of space, we refer the
reader to the table for individual correlation results.
We also computed correlations between the task-
performance measures across the two experiments
(leaving out the IS-FBS system). Correlation be-
tween RIT and RT was .827**; between RIT and IT
.675**; and there was no significant correlation be-
tween the error rates. The one difference evident
between RT and IT is that ER correlates only with IT
(not RT) in Exp 2 (see Table 2).
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99998840625">
In Table 2, the four broad types of metrics we have
investigated (task-performance, REG-specific, string
similarity, set similarity) are indicated by vertical
and horizontal lines. The results within each of the
resulting boxes are very homogeneous. There are
significant (and mostly strong) correlations not only
among the string-similarity metrics and among the
set-similarities, but also across the two types. There
are also significant correlations between the three
task-performance measures.
However, the correlation figures between the task-
performance measures and all others are weak and
not significant. The one exception is the correlation
between NIST and RT which is actually in the wrong
direction (better NIST implies worse reading times).
This is an unambiguous result and it shows clearly
that similarity to human-produced reference texts is
not necessarily indicative of quality as measured by
human task performance.
The emergence of comparative evaluation in NLG
raises the broader question of how systems that gen-
erate language should be compared. In MT and sum-
marisation it is more or less taken as read that sys-
tems which generate more human-like language are
better systems. However, it has not been shown
that more human-like outputs result in better per-
formance from an extrinsic perspective. Intuitively,
it might be expected that higher humanlikeness en-
tails better task-performance (here, shorter read-
ing/identification times, lower error). The lack of
significant covariation between intrinsic and extrin-
sic measures in our experiments suggests otherwise.
</bodyText>
<sectionHeader confidence="0.996468" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99988375">
Our aim in this paper was to shed light on how
the intrinsic evaluation methodologies that dominate
current comparative HLT evaluations correlate with
human task-performance evaluations more in keep-
ing with NLG traditions. We used the data and sys-
tems from the recent ASGRE Challenge, and com-
pared a total of 17 different evaluation methods for
15 different systems implementing the ASGRE task.
Our most striking result is that none of the met-
rics that assess humanlikeness correlate with any of
the task-performance measures, while strong corre-
lations are observed within the two classes of mea-
</bodyText>
<page confidence="0.994513">
199
</page>
<table confidence="0.999826714285714">
extrinsic REG string-similarity set-similarity
RT IT ER-2 Min R-SU4 R-2 NIST BLEU SE SEB Dice MASI
RT 1 .8** .46 .18 .10 .05 .54* .39 -.30 .02 .12 .23
IT .8** 1 .59* .56* -.24 -.33 .22 .04 .09 -.31 -.28 -.17
ER-2 .46 .59* 1 .51 -.29 -.36 .03 -.08 .22 -.34 -.39 -.29
Min .18 .56* .51 1 -.76** -.81** -.46 -.66** .79** -.8** -.90** -.79**
R-SU4 .10 -.24 -.29 -.76** 1 .98** .45 .63* -.63* .42 .72** .57*
R-2 .05 -.33 -.36 -.81** .98** 1 .51 .68** -.69** .53* .78** .65**
NIST .54* .22 .03 -.46 .45 .51 1 .94** -.84** .68** .74** .82**
BLEU .39 .04 -.08 -.66** .63* .68** .94** 1 -.96** .82** .89** .93**
SE -.30 .09 .22 .79** -.63* -.69** -.84** -.96** 1 -.92** -.96** -.97**
SEB .02 -.31 -.34 -.8** .42 .53* .68** .82** -.92** 1 .92** .95**
Dice .12 -.28 -.39 -.90** .72** .78** .74** .89** -.96** .92** 1 .97**
MASI .23 -.17 -.29 -.79** .57* .65** .82** .93** -.97** .95** .97** 1
</table>
<tableCaption confidence="0.995189">
Table 2: Pairwise correlations between all automatic measures and the task-performance results from Exp 2. (* =
significant at .05; ** at .01). R = ROUGE.
</tableCaption>
<bodyText confidence="0.999854333333333">
sures – intrinsic and extrinsic. Somewhat worry-
ingly, our results show that a system’s ability to pro-
duce human-like outputs may be completely unre-
lated to its effect on human task-performance.
Our main conclusions for REG evaluation are that
we need to be cautious in relying on humanlikeness
as a quality criterion, and that we leave extrinsic
evaluation behind at our peril as we move towards
more comparative forms of evaluation.
Given that the intrinsic metrics that dominate in
competetive HLT evaluations are not assessed in
terms of correlation with extrinsic notions of qual-
ity, our results sound a more general note of caution
about using intrinsic measures (and humanlikeness
metrics in particular) without extrinsic validation.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999995111111111">
We gratefully acknowledge the contribution made to
the evaluations by the faculty and staff at Brighton
University who participated in the identification ex-
periments. Thanks are also due to Robert Dale, Kees
van Deemter, Ielka van der Sluis and the anonymous
reviewers for very helpful comments. The biggest
contribution was, of course, made by the participants
in the ASGRE Challenge who created the systems in-
volved in the evaluations.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999195394736842">
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings of
the 1st International Conference on Natural Language
Generation (INLG ’00), pages 1–8.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of the 2nd UCNLG Workshop: Language
Generation and Machine Translation (UCNLG+MT),
pages 75–83.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proc. EACL’06,
pages 313–320.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233–263.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the 11th European Workshop on Natural Language
Generation (ENLG’07), pages 49–56.
I. Langkilde. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence
generator. In Proceedings of the 2nd International
Natural Language Generation Conference (INLG ’02).
C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL 2003, pages 71–78.
R. Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. In Proceedings of the 5th Language Re-
sources and Evaluation Converence (LREC’06).
</reference>
<page confidence="0.996573">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962167">
<title confidence="0.997055">Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation</title>
<author confidence="0.997424">Anja Belz</author>
<affiliation confidence="0.999015">Natural Language Technology Group University of Brighton</affiliation>
<address confidence="0.999653">Brighton BN2 4GJ, UK</address>
<email confidence="0.992233">a.s.belz@brighton.ac.uk</email>
<author confidence="0.999239">Albert Gatt</author>
<affiliation confidence="0.999973">Department of Computing Science University of Aberdeen</affiliation>
<address confidence="0.998548">Aberdeen AB24 3UE, UK</address>
<email confidence="0.996747">a.gatt@abdn.ac.uk</email>
<abstract confidence="0.998661636363636">In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparaand (ii) extrinsic, human task-performance evaluations more in keeping to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>197--200</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 197–200,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Natural Language Generation (INLG ’00),</booktitle>
<pages>1--8</pages>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics for generation. In Proceedings of the 1st International Conference on Natural Language Generation (INLG ’00), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Gatt</author>
</authors>
<title>The attribute selection for GRE challenge: Overview and evaluation results.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd UCNLG Workshop: Language Generation and Machine Translation (UCNLG+MT),</booktitle>
<pages>75--83</pages>
<contexts>
<context position="1374" citStr="Belz and Gatt, 2007" startWordPosition="199" endWordPosition="202">or this task. 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG. In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how the two correlate for NLG tasks. The results show a surprising lack of correlation between the two types of measures, suggesting that intrin</context>
</contexts>
<marker>Belz, Gatt, 2007</marker>
<rawString>A. Belz and A. Gatt. 2007. The attribute selection for GRE challenge: Overview and evaluation results. In Proceedings of the 2nd UCNLG Workshop: Language Generation and Machine Translation (UCNLG+MT), pages 75–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proc. EACL’06,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="1091" citStr="Belz and Reiter, 2006" startWordPosition="153" endWordPosition="157">nd (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG. In this </context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proc. EACL’06, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2448" citStr="Dale and Reiter, 1995" startWordPosition="372" endWordPosition="375"> on how the two correlate for NLG tasks. The results show a surprising lack of correlation between the two types of measures, suggesting that intrinsic metrics and extrinsic methods can represent two very different views of how well a system performs. 2 Task, Data and Systems Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse. REG research goes back at least to the 1980s (Appelt, Grosz, Joshi, McDonald and others), but the field as it is today was shaped in particular by Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995). REG tends to be divided into the stages of attribute selection (selecting properties of entities) and realisation (converting selected properties into word strings). Attribute selection in its standard formulation was the shared task in the ASGRE Challenge: given an intended referent (‘target’) and the other domain entities (‘distractors’) each with possible attributes, select a set of attributes for the target referent. The ASGRE data (which is now publicly available) consists of all 780 singular items in the TUNA corpus (Gatt et al., 2007) in two subdomains, consisting of descriptions of f</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2424" citStr="Dale, 1989" startWordPosition="370" endWordPosition="371">o shed light on how the two correlate for NLG tasks. The results show a surprising lack of correlation between the two types of measures, suggesting that intrinsic metrics and extrinsic methods can represent two very different views of how well a system performs. 2 Task, Data and Systems Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse. REG research goes back at least to the 1980s (Appelt, Grosz, Joshi, McDonald and others), but the field as it is today was shaped in particular by Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995). REG tends to be divided into the stages of attribute selection (selecting properties of entities) and realisation (converting selected properties into word strings). Attribute selection in its standard formulation was the shared task in the ASGRE Challenge: given an intended referent (‘target’) and the other domain entities (‘distractors’) each with possible attributes, select a set of attributes for the target referent. The ASGRE data (which is now publicly available) consists of all 780 singular items in the TUNA corpus (Gatt et al., 2007) in two subdomains, consist</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>R. Dale. 1989. Cooking up referring expressions. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology.</booktitle>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>I van der Sluis</author>
<author>K van Deemter</author>
</authors>
<title>Evaluating algorithms for the generation of referring expressions using a balanced corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG’07),</booktitle>
<pages>49--56</pages>
<marker>Gatt, van der Sluis, van Deemter, 2007</marker>
<rawString>A. Gatt, I. van der Sluis, and K. van Deemter. 2007. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG’07), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG ’02).</booktitle>
<contexts>
<context position="1023" citStr="Langkilde, 2002" startWordPosition="144" endWordPosition="145">at are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in c</context>
</contexts>
<marker>Langkilde, 2002</marker>
<rawString>I. Langkilde. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proceedings of the 2nd International Natural Language Generation Conference (INLG ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>71--78</pages>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. HLT-NAACL 2003, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
</authors>
<title>Measuring agreement on setvalued items (MASI) for semantic and pragmatic annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Language Resources and Evaluation Converence (LREC’06).</booktitle>
<marker>Passonneau, 2006</marker>
<rawString>R. Passonneau. 2006. Measuring agreement on setvalued items (MASI) for semantic and pragmatic annotation. In Proceedings of the 5th Language Resources and Evaluation Converence (LREC’06).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>