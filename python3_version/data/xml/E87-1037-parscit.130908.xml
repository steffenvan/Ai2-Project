<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<title confidence="0.998464">
A Comparison of Rule-Invocation Strategies
in Context-Free Chart Parsing
</title>
<author confidence="0.969495">
Mats Wiren
</author>
<affiliation confidence="0.9917985">
Department of Computer and Information Science
LinkOping University
</affiliation>
<address confidence="0.718509">
S-581 83 LinkOping, Sweden
</address>
<sectionHeader confidence="0.941039" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823733333333">
Currently several grammatical formalisms converge
towards being declarative and towards utilizing
context-free phrase-structure grammar as a back-
bone, e.g. LFG and PATR-II. Typically the pro-
cessing of these formalisms is organized within a
chart-parsing framework. The declarative charac-
ter of the formalisms makes it important to decide
upon an overall optimal control strategy on the part
of the processor. In particular, this brings the rule-
invocation strategy into critical focus: to gain max-
imal processing efficiency, one has to determine the
best way of putting the rules to use. The aim of this
paper is to provide a survey and a practical compari-
son of fundamental rule-invocation strategies within
context-free chart parsing.
</bodyText>
<sectionHeader confidence="0.817223" genericHeader="keywords">
1 Background
and Introduction
</sectionHeader>
<bodyText confidence="0.999890294117647">
An apparent tendency in computational linguistics
during the last few years has been towards declara-
tive grammar formalisms. This tendency has mani-
fested itself with respect to linguistic tools, perhaps
seen most clearly in the evolution from ATNs with
their strongly procedural grammars to PATR-II in
its various incarnations (Shieber et al. 1983, Kart-
tunen 1986), and to logic-based formalisms such as
DCG (Pereira and Warren 1980). It has also man-
ifested itself in linguistic theories, where there has
been a development from systems employing sequen-
tial derivations in the analysis of sentence struc-
tures to systems like LFG and GPSG which estab-
lish relations among the elements of a sentence in an
order-independent and also direction-independent
way. For example, phenomena such as rule order-
ing simply do not arise in these theories.
</bodyText>
<subsectionHeader confidence="0.649474">
This research has been supported by the National Swedish
Board for Technical Development.
</subsectionHeader>
<bodyText confidence="0.999623911764706">
In addition, declarative formalisms are, in princi-
ple, processor-independent. Procedural formalisms,
although possibly highly standardized (like Woods&apos;
ATN formalism), typically make references to an
(abstract) machine.
By virtue of this, it is possible for grammar writ-
ers to concentrate on linguistic issues, leaving aside
questions of how to express their descriptions in a
way which provides for efficient execution by the pro-
cessor at hand.
Processing efficiency instead becomes an issue for
the designer of the processor, who has to find an
overall &amp;quot;optimal&amp;quot; control strategy for the processing
of the grammar. In particular (and also because of
the potentially very large number of rules in realis-
tic natural-language systems), this brings the rule-
invocation strategy&apos; into critical focus: to gain max-
imal processing efficiency, one has to determine the
best way of putting the rules to use.2
This paper focuses on rule-invocation strategies
from the perspective of (context-free) chart parsing
(Kay 1973, 1982; Kaplan 1973).
Context-free phrase-structure grammar is of in-
terest here in particular because it is utilized as
the backbone of many declarative formalisms. The
chart-parsing framework is of interest in this connec-
tion because, being a &amp;quot;higher-order algorithm&amp;quot; (Kay
1982:329), it lends itself easily to the processing of
different grammatical formalisms. At the same time
it is of course a natural test bed for experiments with
various control strategies.
Previously a number of comparisons of rule-
invocation strategies in this or in similar settings
have been reported:
</bodyText>
<footnote confidence="0.857210571428571">
1This term seems to have been coined by Thompson
(1981). Basically, it refers to the spectrum between top-down
and bottom-up processing of the grammar rules.
2The other principal control-strategy dimension, the search
stndegy (depth-first vs. breadth-first), is irrelevant for the effi-
ciency in chart parsing since it only affects the order in which
successive (partial) analyses are developed.
</footnote>
<page confidence="0.997589">
226
</page>
<bodyText confidence="0.999851538461539">
Kay (1982) is the principal source, providing a
very general exposition of the control strategies and
data structures involved in chart parsing. In con-
sidering the efficiency question, Kay favours a &amp;quot;di-
rected&amp;quot; bottom-up strategy (cf. section 2.2.3).
Thompson (1981) is another fundamental source,
though he discusses the effects of various rule-
invocation strategies mainly from the perspective of
GPSG parsing which is not the main point here.
Kilbury (1985) presents a left-corner strategy, ar-
guing that with respect to natural-language gram-
mars it will generally outperform the top-down
(Earley-style) strategy.
Wang (1985) discusses Kilbury&apos;s and Earley&apos;s al-
gorithms, favouring the latter because of the ineffi-
cient way in which bottom-up algorithms deal with
rules with right common factors. Neither Wang nor
Kilbury considers the natural approach to overcom-
ing this problem, viz, top-down filtering (cf. section
2.2.3).
As for empirical studies, Slocum (1981) is a rich
source. Among many other things, he provides some
performance data regarding top-down filtering.
Pratt (1975) reports on a successful augmentation
of a bottom-up chart-like parser with a top-down
filter.
Tomita (1985, 1986) introduces a very efficient,
extended LR-parsing algorithm that can deal with
full context-free languages. Based on empirical com-
parisons, Tomita shows his algorithm to be superior
to Earley&apos;s algorithm and also to a modified ver-
sion thereof (corresponding here to &apos;selective top-
down&amp;quot;; cf. section 2.1.2). Thus, with respect to
raw efficiency, it seems clear that Tomita&apos;s algorithm
is superior to comparable chart-parsing algorithms.
However, a chart-parsing framework does have its
advantages, particularly in its flexibility and open-
endedness.
The contribution this paper makes is:
</bodyText>
<listItem confidence="0.996796571428571">
• to survey fundamental strategies for rule-
invocation within a context-free chart-parsing
framework; in particular
• to specify &amp;quot;directed&amp;quot; versions of Kilbury&apos;s strat-
egy; and
• to provide a practical comparison of the strate-
gies based on empirical results.
</listItem>
<sectionHeader confidence="0.96588" genericHeader="method">
2 A Survey of
</sectionHeader>
<subsectionHeader confidence="0.934759">
Rule-Invocation Strategies
</subsectionHeader>
<bodyText confidence="0.998820571428571">
This section surveys the fundamental rule-invocation
strategies in context-free chart parsing.&apos; In a chart-
parsing framework, different rule-invocation strate-
gies correspond to different conditions for and ways
of predicting new edges4. This section will therefore
in effect constitute a survey of different methods for
predicting new edges.
</bodyText>
<subsectionHeader confidence="0.992124">
2.1 Top-Down Strategies
</subsectionHeader>
<bodyText confidence="0.999895333333333">
The principle of top-down parsing is to use the rules
of the grammar to generate a sentence that matches
the one being analyzed.
</bodyText>
<subsectionHeader confidence="0.946798">
2.1.1 Top-Down
</subsectionHeader>
<bodyText confidence="0.9836295">
A strategy for top-down chart parsing5 is given be-
low. Assume a context-free grammar G. Also, we
make the usual assumption that G is cycle-free, i.e.,
it does not contain derivations of the form A1 A2,
</bodyText>
<equation confidence="0.9790755">
A2 —■ A3, , Ai Al.
Strategy 16 (TD)
</equation>
<bodyText confidence="0.983075857142857">
Whenever an active edge is added to the chart,
if its first required constituent is C, then add an
empty active C edge for every rule in G which
expands C.7
This principle will apply to itself recursively, en-
suring that all subsidiary active edges also get pro-
duced.
</bodyText>
<subsectionHeader confidence="0.703278">
2.1.2 Selective Top-Down
</subsectionHeader>
<bodyText confidence="0.997862555555555">
Realistic natural-language grammars are likely to be
highly branching. A weak point of the &amp;quot;normal&amp;quot;
top-down strategy above will then be the excessive
number of predictions typically made: in the begin-
ning of a phrase new edges will be introduced for
all constituents, and constituents within those con-
stituents, that the phrase can possibly start with.
One way of limiting the number of predictions
is by making the strategy &apos;selective&amp;quot; (Griffiths
</bodyText>
<footnote confidence="0.8880345625">
31 assume a basic familiarity with chart parsing. For an
excellent introduction, see Thompson and Ritchie (1984).
4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to
&amp;quot;items&amp;quot; in Aho and Ullman (1972:320).
6Top-down (context-free) chart parsing is sometimes called
&amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way
in which Earley&apos;s algorithm (Earley 1970) works. It should
be pointed out that the parse-forest representation employed
here does not suffer from the kind of defect claimed by Tomita
(1985:762, 1986:74) to result from Earley&apos;s algorithm.
6This formulation is equivalent to the one in Thompson
(1981:4).
7Note that in order to handle left-recursive rules without
going into an infinite loop, this strategy needs a redundancy
check which prevents more than one identical active edge from
being added to the chart.
</footnote>
<page confidence="0.989931">
227
</page>
<bodyText confidence="0.99922825">
and Petrick 1965:291): by looking at the cate-
gory/categories of the next word, it is possible to rule
out some proposed edges that are known not to com-
bine with the corresponding inactive edge(s). Given
that top-down chart parsing starts with a scanning
phase, the adoption of this filter is straightforward.
The strategy makes use of a reachability relation
2 where ARB holds if there exists some derivation
from A to B such that B is the first element in a
string dominated by A. Given preterminal look-
ahead symbol(s) pi corresponding to the next word,
the processor can then ask if the first required con-
stituent of a predicted active edge (say, C) can some-
how start with (some) pi. In practice, the relation is
implemented as a precompiled table. Determining if
holds can then be made very fast and in constant
time. (Cf. Pratt 1975:424.)
The strategy presented here corresponds to Kay&apos;s
&amp;quot;directed top-down&amp;quot; strategy (Kay 1982:338) and
can be specified in the following manner.
</bodyText>
<subsectionHeader confidence="0.445537">
Strategy 2 (TD,)
</subsectionHeader>
<bodyText confidence="0.9999333">
Let r(X) be the first required constituent of the
(active) edge X. Let v be the vertex to which
the active edge about to be proposed extends.
Let pi,..., py, be the preterminal categories of
the edges extending from v that correspond to
the next word. — Whenever an active edge
is added to the chart, if its first required con-
stituent is C, then for every rule in G which
expands C add an empty active C edge if for
some j r(C) = pi or r(C)Rpi.
</bodyText>
<subsectionHeader confidence="0.999027">
2.2 Bottom-Up Strategies
</subsectionHeader>
<bodyText confidence="0.999879785714286">
The principle of bottom-up parsing is to reduce a
sequence of phrases whose types match the right-
hand side of a grammar rule to a phrase of the type
of the left-hand side of the rule. To make a reduction
possible, all the right-hand-side phrases have to be
present. This can be ensured by matching from right
to left in the right-hand side of the grammar rule;
this is for example the case with the Cocke-Kasami-
Younger algorithm (Aho and Ullman 1972).
A problem with this approach is that the analy-
sis of the first part of a phrase has no influence on
the analysis of the latter parts until the results from
them are combined. This problem can be met by
adopting left-corner parsing.
</bodyText>
<subsectionHeader confidence="0.872272">
2.2.1 Left Corner
</subsectionHeader>
<bodyText confidence="0.999288285714286">
Left-corner parsing is a bottom-up technique where
the right-hand-side symbols of the rules are matched
from left to right.&apos; Once the left-corner symbol has
been found, the grammar rule can be used to predict
what may come next.
A basic strategy for left-corner chart parsing is
given below.
</bodyText>
<subsectionHeader confidence="0.652986">
Strategy 39 (LC)
</subsectionHeader>
<bodyText confidence="0.998178428571429">
Whenever an inactive edge is added to the
chart, if its category is T, then for every rule in
G with T as left-corner symbol add an empty
active edgeP
Note that this strategy will make &amp;quot;minimal&amp;quot; pre-
dictions, i.e., it will only predict the next higher-level
phrases which a given constituent can begin.
</bodyText>
<subsubsectionHeader confidence="0.672009">
2.2.2 Left Corner it la Kilbury
</subsubsectionHeader>
<bodyText confidence="0.99976296875">
Kilbury (1985) presents a modified left-corner strat-
egy. Basically it amounts to this: instead of predict-
ing empty active edges, edges which subsume the
inactive edge that provoked the new edge are pre-
dicted. A predicted new edge may then be either
active or inactive depending on the contents of the
inactive edge and on what is required by the new
edge.
This strategy has two clear advantages: First, it
saves many edges compared to the &amp;quot;normal&apos; left cor-
ner because it never produces empty active edges.
Secondly (and not pointed out by Kilbury), the usual
redundancy check is not needed here since the strat-
egy itself avoids the risk of predicting more than one
identical edge. The reason for this is that a predicted
edge always subsumes the triggering (inactive) edge.
Since the triggering edge is guaranteed to be unique,
the subsuming edge will also be unique. By virtue
of this, Kilbury&apos;s prediction strategy is actually the
simplest of all the strategies considered here.
The price one has to pay for this is that rules
with empty-string productions (or c-productions, i.e.
rules of the form A e), cannot be handled. This
might look like a serious limitation since most cur-
rent linguistic theories (e.g., LFG, GPSG) make ex-
plicit use of c-productions, typically for the handling
of gaps. On the other hand, context-free gram-
mars can be converted into grammars without e-
productions (Aho and Ulhnan 1972:150).
In practice however, e-productions can be han-
dled in various ways which circumvent the prob-
lem. For example, Karttunen&apos;s D-PATR system
</bodyText>
<footnote confidence="0.998567571428572">
8The left corner of a rule is the leftmost symbol of its right-
hand side.
8This formulation is again equivalent to the one in Thomp-
son (1981:4). Thompson however refers to it as &amp;quot;bottom-up&amp;quot;.
181n this case, left-recursive rules will not lead to infinite
loops. The redundancy check is still needed to prevent super-
fluous analyses from being generated, though.
</footnote>
<page confidence="0.994057">
228
</page>
<bodyText confidence="0.994737916666667">
does not allow empty productions. Instead, it takes
care of fillers and gaps through a &amp;quot;threading&amp;quot; tech-
nique (Karttunen 1986:77). Indeed, the system has
been successfully used for writing LFG-style gram-
mars (e.g., Dyvik 1986).
Kilbury&apos;s left-corner strategy can be specified in
the following manner.
Strategy 4 (LCK)
Whenever an inactive edge is added to the
chart, if its category is T, then for every rule
in G with T as left-corner symbol add an edge
that subsumes the T edge.
</bodyText>
<subsubsectionHeader confidence="0.809364">
2.2.3 Top-Down Filtering
</subsubsectionHeader>
<bodyText confidence="0.994687016666667">
As often pointed out, bottom-up and left-corner
strategies encounter problems with sets of rules like
A —■ BC and A C (right common factors). For
example, assuming standard grammar rules, when
parsing the phrase &amp;quot;the birds fly&amp;quot; an unwanted sen-
tence &amp;quot;birds fly&amp;quot; will be discovered.
This problem can be met by adopting top-down
filtering, a technique which can be seen as the
dual of the selective top-down strategy. Descrip-
tions of top-down filtering are given for example in
Kay (1982) (&amp;quot;directed bottom-up parsing&amp;quot;) and in
Slocum (1981:2). Also, the &amp;quot;oracle&amp;quot; used by Pratt
(1975:424) is a top-down filter.
Essentially top-down filtering is like running a top-
down parser in parallel with a bottom-up parser.
The (simulated) top-down parser rejects some of the
edges that the bottom-up parser proposes, viz, those
that the former would not discover. The additional
question that the top-down filter asks is then: is
there any place in a higher-level structure for the
phrase about to be built by the bottom-up parser?
On the chart, this corresponds to asking if any (ac-
tive) edge ending in the starting vertex of the pro-
posed edge needs this this kind of edge, directly or
indirectly. The procedure for computing the answer
to this again makes use of the reachability relation
(cf. section 2.1.2).&amp;quot;
Adding top-down filtering to the LC strategy
above produces the following strategy.
Strategy 5 (Let)
Let v be the vertex from which the triggering
edge T extends. Let A1, , Am be the ac-
tive edges incident to v, and let r(Ai) be their
liKilbury (1985:10) actually makes use of a similar rela-
tion encoding the left-branchings of the grammar (the &amp;quot;first-
relation&amp;quot;), but he uses it only for speeding up grammar-rule
access (by indexing rules from left corners) and not for the
purpose of filtering out unwanted edges.
respective first required constituents. — When-
ever an inactive edge is added to the chart, if its
category is T, then for every rule C in G with
T as left-corner symbol add an empty active C
edge if for some i r(Ai) = C or r(A)RC.
Analogously, adding top-down filtering to Kil-
bury&apos;s strategy LCK results in the following.
Strategy 6 (LCKt)
(Same preconditions as above.) — Whenever
an inactive edge is added to the chart, if its
category is T, then for every rule C in G with
T as left-corner symbol add a C edge subsuming
the T edge if for some i r(Ai) = C or r(A)RC.
One of the advantages with chart parsing is direc-
tion independence: the words of a sentence do not
have to be parsed strictly from left to right but can
be parsed in any order. Although this is still possible
using top-down filtering, processing becomes some-
what less straightforward (cf. Kay 1982:352). The
simplest way of meeting this problem, and also the
solution adopted here, is to presuppose left-to-right
parsing.
</bodyText>
<subsubsectionHeader confidence="0.636807">
2.2.4 Selectivity
</subsubsectionHeader>
<bodyText confidence="0.987076454545455">
By again adopting a kind of lookahead and by uti-
lizing the reachability relation R, it is possible to
limit the number of edges built even further. This
lookahead can be realized by performing a dictionary
lookup of the words before actually building the cor-
responding inactive edges, storing the results in a
table. Being analogous to the filter used in the di-
rected top-down strategy, this filter makes sure that
a predicted edge can somehow be extended given the
category/categories of the next word. Note that this
filter only affects active predicted edges.
Adding selectivity to Kilbury&apos;s strategy LCK re-
sults in the following.
Strategy 7 (LCK.)
Let pl, pn be the categories of the word cor-
responding to the preterminal edges extending
from the vertex to which the T edge is incident.
Let r(C) be defined as above. — Whenever an
inactive edge is added to the chart, if its cate-
gory is T, then for every rule C in G with T as
left-corner symbol add a C edge subsuming the
T edge if for some j r(C) = pi or r(C)p1.
</bodyText>
<subsectionHeader confidence="0.886509">
2.2.5 Top-Down Filtering and Selectivity
</subsectionHeader>
<bodyText confidence="0.9995725">
The final step is to combine the two previous strate-
gies to arrive at a maximally directed version of Kil-
</bodyText>
<page confidence="0.994989">
229
</page>
<bodyText confidence="0.9431596">
bury&apos;s strategy. Again, left-to-right processing is
presupposed.
Strategy 8 (LCK8t)
Let r(A), r(C), and pi be defined analogously
to the previous. — Whenever an inactive edge is
added to the chart, if its category is T, then for
every rule C in G with T as left-corner symbol
add a C edge subsuming the T edge if for some i
= C or r(Ai)2C and for some j r(C) = pi
or r(C)p1.
</bodyText>
<sectionHeader confidence="0.993925" genericHeader="method">
3 Empirical Results
</sectionHeader>
<bodyText confidence="0.999951428571429">
In order to assess the practical behaviour of the
strategies discussed above, a test bench was devel-
oped where it was made possible in effect to switch
between eight different parsers corresponding to the
eight strategies above, and also between different
grammars, dictionaries, and sentence sets.
Several experiments were conducted along the
way. The test grammars used were first partly based
on a Swedish D-PATR grammar by Merkel (1986).
Later on, I decided to use (some of) the data com-
piled by Tomita (1986) for the testings of his ex-
tended LR parser.
This section presents the results of the latter ex-
periments.
</bodyText>
<subsectionHeader confidence="0.999346">
3.1 Grammars and Sentence Sets
</subsectionHeader>
<bodyText confidence="0.982648678571429">
The three grammars and two sentence sets used in
these experiments have been obtained from Masaru
Tomita and can be found in his book (Tomita 1986).
Grammars I and II are toy grammars consisting
of 8 and 43 rules, respectively. Grammar III with
224 rules is constructed to fit sentence set I which is
a collection of 40 sentences collected from authentic
texts. (Grammar IV with 394 rules was not used
here.)
Because grammar III contains one empty produc-
tion, not all sentences of sentence set I will be cor-
rectly parsed by Kilbury&apos;s algorithm. For the pur-
pose of these experiments, I collected 21 sentences
out of the sentence set. This reduced set will hence-
forth be referred to as sentence set 1.12 The sen-
tences in this set vary in length between 1 and 27
words.
Sentence set II was made systematically from the
schema
noun verb det noun (prep det noun)&apos;&apos;.
&amp;quot;The sentences in the set are 1-3, 9, 13-15, 19-25, 29, and
35-40 (cf. Tomita 1986:152).
An example of a sentence with this structure is &amp;quot;I
saw the man in the park with a telescope...&amp;quot;. In
these experiments n = 1, , 7 was used.
The dictionary was constructed from the category
sequences given by Tomita together with the sen-
tences (Tomita 1986 pp. 185-189).
</bodyText>
<subsectionHeader confidence="0.998754">
3.2 Efficiency Measures
</subsectionHeader>
<bodyText confidence="0.999985833333333">
A reasonable efficiency measure in chart parsing is
the number of edges produced. The motivation for
this is that the working of a chart parser is tightly
centered around the production and manipulation
of edges, and that much of its work can somehow
be reduced to this. For example, a measure of the
amount of work done at each vertex by the procedure
which implements &amp;quot;the fundamental rule&amp;quot; (Thomp-
son 1981:2) can be expressed as the product of the
number of incoming active edges and the number of
outgoing inactive edges. In addition, the number of
chart edges produced is a measure which is indepen-
dent of implementation and machine.
On the other hand, the number of edges does not
give any indication of the overhead costs involved in
various strategies. Hence I also provide figures of
the parsing times, albeit with a warning for taking
them too seriously.13
The experiments were run on Xerox 1186 Lisp ma-
chines. The time measures were obtained using the
Interlisp-D function TIMEALL. The time figures be-
low give the CPU time in seconds (garbage-collection
time and swapping time not included; the latter was
however almost non-existent).
</bodyText>
<subsectionHeader confidence="0.993457">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.912576157894737">
This section presents the results of the experiments.
In the tables, the fourth column gives the accumu-
lated number of edges over the sentence set. The sec-
ond and third columns give the corresponding num-
bers of active and inactive edges, respectively. The
fifth column gives the accumulated CPU time in sec-
onds. The last column gives the rank of the strate-
gies with respect to the number of edges produced
and, in parentheses, with respect to time consumed
(if differing from the former).
Table 1 shows the results of the first experiment:
running grammar I (8 rules) with sentence set II (7
sentences). There were 625 parses for every strategy
(1, 2, 5, 14, 42, 132, and 429).
13The parsers are experimental in character and were not
coded for maximal efficiency. For example, edges at a given
vertex are being searched linearly. On the other hand, gram-
mar rules (like reachability relations) are indexed through pre-
compiled hashtables.
</bodyText>
<page confidence="0.991722">
230
</page>
<tableCaption confidence="0.7923235">
Table 1
Experiment 1: Grammar I, sentence set II
</tableCaption>
<table confidence="0.979836452380953">
Strategy Active Inactive Total Time Rank
TD 1628 3496 5124 62 6
TD. 1579 3496 5075 58 4 (5)
LC 3104 3967 7071 79 8
LCt 1579 3496 5075 57 4
LCK 2873 3967 6840 64 7
LCK. 697 3967 4664 47 2 (3)
LCKt 1460 3496 4956 45 3 (2)
LCKat 527 3496 4023 40 1
Table 2
Experiment 2: Grammar II, sentence set II
Strategy Active Inactive Total Time Rank
TD 5015 2675 7690 121 6
TD. 3258 2675 5933 78 4
LC 7232 5547 12779 192 8
LC t 3237 2675 5912 132 3 (7)
LCK 6154 5547 11701 117 7 (5)
LCK. 1283 5547 6830 70 5 (2)
LCKt 2719 2675 5394 74 2 (3)
LCK.t 915 2675 3590 41 1
Table 3
Experiment 3: Grammar III, sentence set II
Strategy Active Inactive Total Time Rank
TD 13676 5278 18954 910 6 (5)
TD. 9301 5278 14579 765 4
LC 19522 7980 27502 913 8 (6)
LC t 9301 5278 14579 2604 4 (8)
LCK 18227 7980 26207 731 7 (3)
LCK. 1359 7980 9339 482 2
LCKt 8748 5278 14026 1587 3 (7)
LCK„t 718 5278 5996 352 1
Table 4
Experiment 4: Grammar III, sentence set I
Strategy Active Inactive Total Time Rank
TD 30403 8376 38779 1524 6 (4)
TD. 14389 8376 23215 1172 4 (2)
LC 42959 19451 62410 2759 8 (6)
LC t 14714 8376 23090 5843 3 (8)
LCK 38040 19451 57491 1961 7 (5)
LCK. 3845 19451 23296 1410 5 (3)
LCKt 12856 8376 21232 3898 2 (7)
LCK.t 1265 8376 9641 1019 1
</table>
<bodyText confidence="0.983844181818182">
Table 2 shows the results of the second experi-
ment: grammar II with sentence set II. This gram-
mar handles PP attachment in a way different from
grammars I and III which leads to fewer parses: 322
for every strategy.
Table 3 shows the results of the third experiment:
grammar III (224 rules) with sentence set II. Again,
there were 625 parses for every strategy.
Table 4 shows the results of the fourth experiment:
running grammar III with sentence set I (21 sen-
tences). There were 885 parses for every strategy.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999700530303031">
This section summarizes and discusses the results of
the experiments.
As for the three undirected methods, and with
respect to the number of edges produced, the top-
down (Earley-style) strategy performs best while the
standard left-corner strategy is the worst alternative.
Kilbury&apos;s strategy, by saving active looping edges,
produces somewhat fewer edges than the standard
left-corner strategy. More apparent is its time ad-
vantage, due to the basic simplicity of the strategy.
For example, it outperforms the top-down strategy
in experiments 2 and 3.
Results like those above are of course strongly
grammar dependent. If, for example, the branching
factor of the grammar increases, top-down overpre-
dictions will soon dominate superfluous bottom-up
substring generation. This was clearly seen in some
of the early experiments not showed here. In cases
like this, bottom-up parsing becomes advantageous
and, in particular, Kilbury&apos;s strategy will outper-
form the two others.
Thus, although Wang (1985:7) seems to be right in
claiming that &amp;quot;...Earley&apos;s algorithm is better than
Kilbury&apos;s in general.&amp;quot;, in practice this can often be
different (as Wang himself recognizes). Incidentally,
Wang&apos;s own example (:4), aimed at showing that Kil-
bury&apos;s algorithm handles right recursion worse than
Earley&apos;s algorithm, illustrates this:
Assume a grammar with rules S Ac, A --+ aA,
A and a sentence &amp;quot;a aaab c&amp;quot; to be parsed.
Here a bottom-up parser such as Kilbury&apos;s will ob-
viously do some useless work in predicting several
unwanted S edges. But even so the top-down over-
predictions will actually dominate: the Earley-style
strategy gives 16 active and 12 inactive edges, to-
talling 28 edges, whereas Kilbury&apos;s strategy gives 9
and 16, respectively, totalling 25 edges.
The directed methods — those based on selectiv-
ity or top-down filtering — reduce the number of
edges very significantly. The selectivity filter here
turned out to be much more time efficient, though.
Selectivity testing is also basically a simple opera-
tion, seldom involving more than a few lookups (de-
pending on the degree of lexical ambiguity).
Paradoxically, the effect of top-down filtering was
to degrade time performance as the grammars grew
larger. To a large extent this is likely to have
been caused by implementation idiosyncrasies: ac-
tive edges incident to a vertex were searched linearly;
when the number of edges increases, this gets very
costly. After all, top-down filtering is generally con-
sidered beneficial (e.g. Slocum 1981:4).
The maximally directed strategy — Kilbury&apos;s al-
gorithm with selectivity and top-down filtering —
remained the most efficient one throughout all the
experiments, both with respect to edges produced
and time consumed (but more so with respect to the
former). Top-down filtering did not degrade time
performance quite as much in this case, presumably
because of the great number of active edges cut off
by the selectivity filter.
Finally, it should be mentioned that bottom-up
parsing enjoys a special advantage not shown here,
namely in being able to detect ungrammatical sen-
tences much more effectively than top-down meth-
ods (cf. Kay 1982:342).
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99927545">
This paper has surveyed the fundamental rule-
invocation strategies in context-free chart parsing.
In order to arrive at some quantitative measure
of their performance characteristics, the strategies
have been implemented and tested empirically. The
experiments clearly indicate that it is possible to
significantly increase efficiency in chart parsing by
fine-tuning the rule-invocation strategy. Fine-tuning
however also requires that the characteristics of the
grammars to be used are borne in mind. Never-
theless, the experiments indicate that in general di-
rected methods are to be preferred to undirected
methods; that top-down is the best undirected strat-
egy; that Kilbury&apos;s original algorithm is not in itself
a very good candidate, but that its directed versions
— in particular the one with both selectivity and
top-down filtering — are very promising.
Future work along these lines is planned to involve
application of (some of) the strategies above within
a unification-based parsing system.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999130375">
I would like to thank Lars Ahrenberg, Nils Dahlback,
Arne Jonsson, Magnus Merkel, Ivan Rankin, and an
anonymous referee for the very helpful comments
they have made on various drafts of this paper. In
addition I am indebted to Masaru Tomita for pro-
viding me with his test grammars and sentences, and
to Martin Kay for comments in connection with my
presentation.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997836717948718">
Aho, Alfred V. and Jeffrey D. Ullman (1972). The
Theory of Parsing, Translation, and Compiling.
Volume I: Parsing. Prentice-Hall, Englewood Cliffs,
New Jersey.
Dyvik, Helge (1986). Aspects of Unification-Based
Chart Parsing. Ms. Department of Linguistics and
Phonetics, University of Bergen, Bergen, Norway.
Earley, Jay (1970). An Efficient Context-Free
Parsing Algorithm. Communications of the ACM
13(2):94-102.
Griffiths, T. V. and Stanley R. Petrick (1965).
On the Relative Efficiences of Context-Free Gram-
mar Recognisers. Communications of the ACM
8(5):289-300.
Kaplan, Ronald M. (1973). A General Syntactic
Processor. In: Randall Rustin, ed., Natural Lan-
guage Processing. Algorithmics Press, New York,
New York: 193-241.
Karttunen, Lauri (1986). D-PATR: A Develop-
ment Environment for Unification-Based Grammars.
Proc. 11th COLING, Bonn, Federal Republic of Ger-
many: 74-80.
Kay, Martin (1973). The MIND System. In: Ran-
dall Rustin, ed., Natural Language Processing. Al-
gorithmics Press, New York, New York: 155-188.
Kay, Martin (1982). Algorithm Schemata and Data
Structures in Syntactic Processing. In: Sture Allen,
ed., Text Processing. Proceedings of Nobel Sympo-
sium 51. Almqvist &amp; Wiksell International, Stock-
holm, Sweden: 327-358. Also: CSL-80-12, Xerox
PARC, Palo Alto, California.
Kilbury, James (1985). Chart Parsing and the
Earley Algorithm. KIT-Report 24, Projektgruppe
Kunstliche Intelligens und Textverstehen, Techni-
ache Universitat Berlin, West Berlin. Also in:
U. Klenk, ed. (1985), Kontextfreie Syntaxen und
verwandte Systeme. Vortrcige eines Kolloquiums
in Grand Ventron im Oktober, 1984. Niemeyer,
Tiibingen, Federal Republic of Germany.
</reference>
<page confidence="0.934633">
232
</page>
<reference confidence="0.999723727272728">
Merkel, Magnus (1986). A Swedish Grammar in
D-PATR. Experiences of Working with D-PATR.
Research report LiTH-IDA-R-86-31, Department of
Computer and Information Science, Linkiiping Uni-
versity, Link&apos;aping, Sweden.
Pereira, Fernando C. N. and David H. D. Warren
(1980). Definite Clause Grammars for Language
Analysis—A Survey of the Formalism and a Com-
parison with Augmented Transition Networks. Ar-
tificial Intelligence 13(3):231-278.
Pratt, Vaughan R. (1975). LINGOL — A Progress
Report. Proc. 4th IJCAI, Tbilisi, Georgia, USSR:
422-428.
Shieber, Stuart M., Hans Uszkoreit, Fernando C. N.
Pereira, Jane J. Robinson, and Mabry Tyson (1983).
The Formalism and Implementation of PATR-II. In:
Barbara Grosz and Mark Stickel, eds., Research on
Interactive Acquisition and Use of Knowledge. SRI
Final Report 1894, SRI International, Menlo Park,
California.
Slocum, Jonathan (1981). A Practical Comparison
of Parsing Strategies. Proc. 19th ACL, Stanford,
California: 1-6.
Thompson, Henry (1981). Chart Parsing and Rule
Schemata in GPSG. Research Paper No. 165, De-
partment of Artificial Intelligence, University of Ed-
inburgh, Edinburgh, Scotland. Also in: Proc. 19th
ACL, Stanford, California: 167-172.
Thompson, Henry and Graeme Ritchie (1984). Im-
plementing Natural Language Parsers. In: Tim
O&apos;Shea and Marc Eisenstadt, Artificial Intelligence:
Tools, Techniques, and Applications. Harper Sc Row,
New York, New York: 245-300.
Tomita, Masaru (1985). An Efficient Context-free
Parsing Algorithm For Natural Languages. Proc.
9th IJCAI, Los Angeles, California: 756-764.
Tomita, Masaru (1986). Efficient Parsing for Nat-
ural Language. A Fast Algorithm for Practical Sys-
tems. Kluwer Academic Publishers, Norwell, Mas-
sachusetts.
Wang, Weiguo (1985). Computational Linguistics
Technical Notes No. 2. Technical Report 85/013,
Computer Science Department, Boston University,
Boston, Massachusetts.
</reference>
<page confidence="0.998956">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981269">
<title confidence="0.9996655">A Comparison of Rule-Invocation Strategies in Context-Free Chart Parsing</title>
<author confidence="0.999253">Mats Wiren</author>
<affiliation confidence="0.9999735">Department of Computer and Information Science LinkOping University</affiliation>
<address confidence="0.984395">S-581 83 LinkOping, Sweden</address>
<abstract confidence="0.9999570625">Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g. LFG and PATR-II. Typically the processing of these formalisms is organized within a chart-parsing framework. The declarative character of the formalisms makes it important to decide upon an overall optimal control strategy on the part of the processor. In particular, this brings the ruleinvocation strategy into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing.</abstract>
<intro confidence="0.99895">1 Background</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Volume I: Parsing. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="7643" citStr="Aho and Ullman (1972" startWordPosition="1166" endWordPosition="1169">rammars are likely to be highly branching. A weak point of the &amp;quot;normal&amp;quot; top-down strategy above will then be the excessive number of predictions typically made: in the beginning of a phrase new edges will be introduced for all constituents, and constituents within those constituents, that the phrase can possibly start with. One way of limiting the number of predictions is by making the strategy &apos;selective&amp;quot; (Griffiths 31 assume a basic familiarity with chart parsing. For an excellent introduction, see Thompson and Ritchie (1984). 4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to &amp;quot;items&amp;quot; in Aho and Ullman (1972:320). 6Top-down (context-free) chart parsing is sometimes called &amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way in which Earley&apos;s algorithm (Earley 1970) works. It should be pointed out that the parse-forest representation employed here does not suffer from the kind of defect claimed by Tomita (1985:762, 1986:74) to result from Earley&apos;s algorithm. 6This formulation is equivalent to the one in Thompson (1981:4). 7Note that in order to handle left-recursive rules without going into an infinite loop, this strategy needs a redundancy check which prevents more than one identical ac</context>
<context position="10207" citStr="Aho and Ullman 1972" startWordPosition="1606" endWordPosition="1609"> if its first required constituent is C, then for every rule in G which expands C add an empty active C edge if for some j r(C) = pi or r(C)Rpi. 2.2 Bottom-Up Strategies The principle of bottom-up parsing is to reduce a sequence of phrases whose types match the righthand side of a grammar rule to a phrase of the type of the left-hand side of the rule. To make a reduction possible, all the right-hand-side phrases have to be present. This can be ensured by matching from right to left in the right-hand side of the grammar rule; this is for example the case with the Cocke-KasamiYounger algorithm (Aho and Ullman 1972). A problem with this approach is that the analysis of the first part of a phrase has no influence on the analysis of the latter parts until the results from them are combined. This problem can be met by adopting left-corner parsing. 2.2.1 Left Corner Left-corner parsing is a bottom-up technique where the right-hand-side symbols of the rules are matched from left to right.&apos; Once the left-corner symbol has been found, the grammar rule can be used to predict what may come next. A basic strategy for left-corner chart parsing is given below. Strategy 39 (LC) Whenever an inactive edge is added to t</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman (1972). The Theory of Parsing, Translation, and Compiling. Volume I: Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helge Dyvik</author>
</authors>
<title>Aspects of Unification-Based Chart Parsing.</title>
<date>1986</date>
<institution>Ms. Department of Linguistics and Phonetics, University of Bergen,</institution>
<location>Bergen,</location>
<contexts>
<context position="13242" citStr="Dyvik 1986" startWordPosition="2120" endWordPosition="2121">ttunen&apos;s D-PATR system 8The left corner of a rule is the leftmost symbol of its righthand side. 8This formulation is again equivalent to the one in Thompson (1981:4). Thompson however refers to it as &amp;quot;bottom-up&amp;quot;. 181n this case, left-recursive rules will not lead to infinite loops. The redundancy check is still needed to prevent superfluous analyses from being generated, though. 228 does not allow empty productions. Instead, it takes care of fillers and gaps through a &amp;quot;threading&amp;quot; technique (Karttunen 1986:77). Indeed, the system has been successfully used for writing LFG-style grammars (e.g., Dyvik 1986). Kilbury&apos;s left-corner strategy can be specified in the following manner. Strategy 4 (LCK) Whenever an inactive edge is added to the chart, if its category is T, then for every rule in G with T as left-corner symbol add an edge that subsumes the T edge. 2.2.3 Top-Down Filtering As often pointed out, bottom-up and left-corner strategies encounter problems with sets of rules like A —■ BC and A C (right common factors). For example, assuming standard grammar rules, when parsing the phrase &amp;quot;the birds fly&amp;quot; an unwanted sentence &amp;quot;birds fly&amp;quot; will be discovered. This problem can be met by adopting top</context>
</contexts>
<marker>Dyvik, 1986</marker>
<rawString>Dyvik, Helge (1986). Aspects of Unification-Based Chart Parsing. Ms. Department of Linguistics and Phonetics, University of Bergen, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM</journal>
<pages>13--2</pages>
<contexts>
<context position="7604" citStr="Earley (1970)" startWordPosition="1160" endWordPosition="1161">own Realistic natural-language grammars are likely to be highly branching. A weak point of the &amp;quot;normal&amp;quot; top-down strategy above will then be the excessive number of predictions typically made: in the beginning of a phrase new edges will be introduced for all constituents, and constituents within those constituents, that the phrase can possibly start with. One way of limiting the number of predictions is by making the strategy &apos;selective&amp;quot; (Griffiths 31 assume a basic familiarity with chart parsing. For an excellent introduction, see Thompson and Ritchie (1984). 4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to &amp;quot;items&amp;quot; in Aho and Ullman (1972:320). 6Top-down (context-free) chart parsing is sometimes called &amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way in which Earley&apos;s algorithm (Earley 1970) works. It should be pointed out that the parse-forest representation employed here does not suffer from the kind of defect claimed by Tomita (1985:762, 1986:74) to result from Earley&apos;s algorithm. 6This formulation is equivalent to the one in Thompson (1981:4). 7Note that in order to handle left-recursive rules without going into an infinite loop, this strategy needs a redundancy check wh</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay (1970). An Efficient Context-Free Parsing Algorithm. Communications of the ACM 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T V Griffiths</author>
<author>Stanley R Petrick</author>
</authors>
<title>On the Relative Efficiences of Context-Free Grammar Recognisers.</title>
<date>1965</date>
<journal>Communications of the ACM</journal>
<pages>8--5</pages>
<marker>Griffiths, Petrick, 1965</marker>
<rawString>Griffiths, T. V. and Stanley R. Petrick (1965). On the Relative Efficiences of Context-Free Grammar Recognisers. Communications of the ACM 8(5):289-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>A General Syntactic Processor.</title>
<date>1973</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>193--241</pages>
<editor>In: Randall Rustin, ed.,</editor>
<publisher>Algorithmics Press,</publisher>
<location>New York, New York:</location>
<contexts>
<context position="2916" citStr="Kaplan 1973" startWordPosition="440" endWordPosition="441"> by the processor at hand. Processing efficiency instead becomes an issue for the designer of the processor, who has to find an overall &amp;quot;optimal&amp;quot; control strategy for the processing of the grammar. In particular (and also because of the potentially very large number of rules in realistic natural-language systems), this brings the ruleinvocation strategy&apos; into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use.2 This paper focuses on rule-invocation strategies from the perspective of (context-free) chart parsing (Kay 1973, 1982; Kaplan 1973). Context-free phrase-structure grammar is of interest here in particular because it is utilized as the backbone of many declarative formalisms. The chart-parsing framework is of interest in this connection because, being a &amp;quot;higher-order algorithm&amp;quot; (Kay 1982:329), it lends itself easily to the processing of different grammatical formalisms. At the same time it is of course a natural test bed for experiments with various control strategies. Previously a number of comparisons of ruleinvocation strategies in this or in similar settings have been reported: 1This term seems to have been coined by T</context>
</contexts>
<marker>Kaplan, 1973</marker>
<rawString>Kaplan, Ronald M. (1973). A General Syntactic Processor. In: Randall Rustin, ed., Natural Language Processing. Algorithmics Press, New York, New York: 193-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>D-PATR: A Development Environment for Unification-Based Grammars.</title>
<date>1986</date>
<booktitle>Proc. 11th COLING,</booktitle>
<pages>74--80</pages>
<location>Bonn, Federal Republic of</location>
<contexts>
<context position="1318" citStr="Karttunen 1986" startWordPosition="193" endWordPosition="195">iciency, one has to determine the best way of putting the rules to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing. 1 Background and Introduction An apparent tendency in computational linguistics during the last few years has been towards declarative grammar formalisms. This tendency has manifested itself with respect to linguistic tools, perhaps seen most clearly in the evolution from ATNs with their strongly procedural grammars to PATR-II in its various incarnations (Shieber et al. 1983, Karttunen 1986), and to logic-based formalisms such as DCG (Pereira and Warren 1980). It has also manifested itself in linguistic theories, where there has been a development from systems employing sequential derivations in the analysis of sentence structures to systems like LFG and GPSG which establish relations among the elements of a sentence in an order-independent and also direction-independent way. For example, phenomena such as rule ordering simply do not arise in these theories. This research has been supported by the National Swedish Board for Technical Development. In addition, declarative formalis</context>
<context position="13141" citStr="Karttunen 1986" startWordPosition="2105" endWordPosition="2106">ice however, e-productions can be handled in various ways which circumvent the problem. For example, Karttunen&apos;s D-PATR system 8The left corner of a rule is the leftmost symbol of its righthand side. 8This formulation is again equivalent to the one in Thompson (1981:4). Thompson however refers to it as &amp;quot;bottom-up&amp;quot;. 181n this case, left-recursive rules will not lead to infinite loops. The redundancy check is still needed to prevent superfluous analyses from being generated, though. 228 does not allow empty productions. Instead, it takes care of fillers and gaps through a &amp;quot;threading&amp;quot; technique (Karttunen 1986:77). Indeed, the system has been successfully used for writing LFG-style grammars (e.g., Dyvik 1986). Kilbury&apos;s left-corner strategy can be specified in the following manner. Strategy 4 (LCK) Whenever an inactive edge is added to the chart, if its category is T, then for every rule in G with T as left-corner symbol add an edge that subsumes the T edge. 2.2.3 Top-Down Filtering As often pointed out, bottom-up and left-corner strategies encounter problems with sets of rules like A —■ BC and A C (right common factors). For example, assuming standard grammar rules, when parsing the phrase &amp;quot;the bi</context>
</contexts>
<marker>Karttunen, 1986</marker>
<rawString>Karttunen, Lauri (1986). D-PATR: A Development Environment for Unification-Based Grammars. Proc. 11th COLING, Bonn, Federal Republic of Germany: 74-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>The MIND System. In:</title>
<date>1973</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>155--188</pages>
<editor>Randall Rustin, ed.,</editor>
<publisher>Algorithmics Press,</publisher>
<location>New York, New York:</location>
<contexts>
<context position="2896" citStr="Kay 1973" startWordPosition="437" endWordPosition="438">icient execution by the processor at hand. Processing efficiency instead becomes an issue for the designer of the processor, who has to find an overall &amp;quot;optimal&amp;quot; control strategy for the processing of the grammar. In particular (and also because of the potentially very large number of rules in realistic natural-language systems), this brings the ruleinvocation strategy&apos; into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use.2 This paper focuses on rule-invocation strategies from the perspective of (context-free) chart parsing (Kay 1973, 1982; Kaplan 1973). Context-free phrase-structure grammar is of interest here in particular because it is utilized as the backbone of many declarative formalisms. The chart-parsing framework is of interest in this connection because, being a &amp;quot;higher-order algorithm&amp;quot; (Kay 1982:329), it lends itself easily to the processing of different grammatical formalisms. At the same time it is of course a natural test bed for experiments with various control strategies. Previously a number of comparisons of ruleinvocation strategies in this or in similar settings have been reported: 1This term seems to h</context>
</contexts>
<marker>Kay, 1973</marker>
<rawString>Kay, Martin (1973). The MIND System. In: Randall Rustin, ed., Natural Language Processing. Algorithmics Press, New York, New York: 155-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<date>1982</date>
<booktitle>Algorithm Schemata and Data Structures in Syntactic Processing. In: Sture Allen, ed., Text Processing. Proceedings of Nobel Symposium 51. Almqvist &amp; Wiksell International, Stockholm, Sweden: 327-358. Also: CSL-80-12, Xerox PARC,</booktitle>
<location>Palo Alto, California.</location>
<contexts>
<context position="3174" citStr="Kay 1982" startWordPosition="478" endWordPosition="479">mber of rules in realistic natural-language systems), this brings the ruleinvocation strategy&apos; into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use.2 This paper focuses on rule-invocation strategies from the perspective of (context-free) chart parsing (Kay 1973, 1982; Kaplan 1973). Context-free phrase-structure grammar is of interest here in particular because it is utilized as the backbone of many declarative formalisms. The chart-parsing framework is of interest in this connection because, being a &amp;quot;higher-order algorithm&amp;quot; (Kay 1982:329), it lends itself easily to the processing of different grammatical formalisms. At the same time it is of course a natural test bed for experiments with various control strategies. Previously a number of comparisons of ruleinvocation strategies in this or in similar settings have been reported: 1This term seems to have been coined by Thompson (1981). Basically, it refers to the spectrum between top-down and bottom-up processing of the grammar rules. 2The other principal control-strategy dimension, the search stndegy (depth-first vs. breadth-first), is irrelevant for the efficiency in char</context>
<context position="9218" citStr="Kay 1982" startWordPosition="1424" endWordPosition="1425">makes use of a reachability relation 2 where ARB holds if there exists some derivation from A to B such that B is the first element in a string dominated by A. Given preterminal lookahead symbol(s) pi corresponding to the next word, the processor can then ask if the first required constituent of a predicted active edge (say, C) can somehow start with (some) pi. In practice, the relation is implemented as a precompiled table. Determining if holds can then be made very fast and in constant time. (Cf. Pratt 1975:424.) The strategy presented here corresponds to Kay&apos;s &amp;quot;directed top-down&amp;quot; strategy (Kay 1982:338) and can be specified in the following manner. Strategy 2 (TD,) Let r(X) be the first required constituent of the (active) edge X. Let v be the vertex to which the active edge about to be proposed extends. Let pi,..., py, be the preterminal categories of the edges extending from v that correspond to the next word. — Whenever an active edge is added to the chart, if its first required constituent is C, then for every rule in G which expands C add an empty active C edge if for some j r(C) = pi or r(C)Rpi. 2.2 Bottom-Up Strategies The principle of bottom-up parsing is to reduce a sequence of</context>
<context position="14007" citStr="Kay (1982)" startWordPosition="2250" endWordPosition="2251">y is T, then for every rule in G with T as left-corner symbol add an edge that subsumes the T edge. 2.2.3 Top-Down Filtering As often pointed out, bottom-up and left-corner strategies encounter problems with sets of rules like A —■ BC and A C (right common factors). For example, assuming standard grammar rules, when parsing the phrase &amp;quot;the birds fly&amp;quot; an unwanted sentence &amp;quot;birds fly&amp;quot; will be discovered. This problem can be met by adopting top-down filtering, a technique which can be seen as the dual of the selective top-down strategy. Descriptions of top-down filtering are given for example in Kay (1982) (&amp;quot;directed bottom-up parsing&amp;quot;) and in Slocum (1981:2). Also, the &amp;quot;oracle&amp;quot; used by Pratt (1975:424) is a top-down filter. Essentially top-down filtering is like running a topdown parser in parallel with a bottom-up parser. The (simulated) top-down parser rejects some of the edges that the bottom-up parser proposes, viz, those that the former would not discover. The additional question that the top-down filter asks is then: is there any place in a higher-level structure for the phrase about to be built by the bottom-up parser? On the chart, this corresponds to asking if any (active) edge ending</context>
<context position="16199" citStr="Kay 1982" startWordPosition="2635" endWordPosition="2636">n filtering to Kilbury&apos;s strategy LCK results in the following. Strategy 6 (LCKt) (Same preconditions as above.) — Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add a C edge subsuming the T edge if for some i r(Ai) = C or r(A)RC. One of the advantages with chart parsing is direction independence: the words of a sentence do not have to be parsed strictly from left to right but can be parsed in any order. Although this is still possible using top-down filtering, processing becomes somewhat less straightforward (cf. Kay 1982:352). The simplest way of meeting this problem, and also the solution adopted here, is to presuppose left-to-right parsing. 2.2.4 Selectivity By again adopting a kind of lookahead and by utilizing the reachability relation R, it is possible to limit the number of edges built even further. This lookahead can be realized by performing a dictionary lookup of the words before actually building the corresponding inactive edges, storing the results in a table. Being analogous to the filter used in the directed top-down strategy, this filter makes sure that a predicted edge can somehow be extended g</context>
<context position="26836" citStr="Kay 1982" startWordPosition="4487" endWordPosition="4488">egy — Kilbury&apos;s algorithm with selectivity and top-down filtering — remained the most efficient one throughout all the experiments, both with respect to edges produced and time consumed (but more so with respect to the former). Top-down filtering did not degrade time performance quite as much in this case, presumably because of the great number of active edges cut off by the selectivity filter. Finally, it should be mentioned that bottom-up parsing enjoys a special advantage not shown here, namely in being able to detect ungrammatical sentences much more effectively than top-down methods (cf. Kay 1982:342). 5 Conclusion This paper has surveyed the fundamental ruleinvocation strategies in context-free chart parsing. In order to arrive at some quantitative measure of their performance characteristics, the strategies have been implemented and tested empirically. The experiments clearly indicate that it is possible to significantly increase efficiency in chart parsing by fine-tuning the rule-invocation strategy. Fine-tuning however also requires that the characteristics of the grammars to be used are borne in mind. Nevertheless, the experiments indicate that in general directed methods are to </context>
</contexts>
<marker>Kay, 1982</marker>
<rawString>Kay, Martin (1982). Algorithm Schemata and Data Structures in Syntactic Processing. In: Sture Allen, ed., Text Processing. Proceedings of Nobel Symposium 51. Almqvist &amp; Wiksell International, Stockholm, Sweden: 327-358. Also: CSL-80-12, Xerox PARC, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Kilbury</author>
</authors>
<title>Chart Parsing and the Earley Algorithm. KIT-Report 24, Projektgruppe Kunstliche Intelligens und Textverstehen, Techniache Universitat Berlin, West Berlin. Also</title>
<date>1985</date>
<booktitle>Kontextfreie Syntaxen und verwandte Systeme. Vortrcige eines Kolloquiums in Grand Ventron im Oktober,</booktitle>
<editor>in: U. Klenk, ed.</editor>
<contexts>
<context position="4332" citStr="Kilbury (1985)" startWordPosition="654" endWordPosition="655">breadth-first), is irrelevant for the efficiency in chart parsing since it only affects the order in which successive (partial) analyses are developed. 226 Kay (1982) is the principal source, providing a very general exposition of the control strategies and data structures involved in chart parsing. In considering the efficiency question, Kay favours a &amp;quot;directed&amp;quot; bottom-up strategy (cf. section 2.2.3). Thompson (1981) is another fundamental source, though he discusses the effects of various ruleinvocation strategies mainly from the perspective of GPSG parsing which is not the main point here. Kilbury (1985) presents a left-corner strategy, arguing that with respect to natural-language grammars it will generally outperform the top-down (Earley-style) strategy. Wang (1985) discusses Kilbury&apos;s and Earley&apos;s algorithms, favouring the latter because of the inefficient way in which bottom-up algorithms deal with rules with right common factors. Neither Wang nor Kilbury considers the natural approach to overcoming this problem, viz, top-down filtering (cf. section 2.2.3). As for empirical studies, Slocum (1981) is a rich source. Among many other things, he provides some performance data regarding top-do</context>
<context position="11116" citStr="Kilbury (1985)" startWordPosition="1767" endWordPosition="1768">e where the right-hand-side symbols of the rules are matched from left to right.&apos; Once the left-corner symbol has been found, the grammar rule can be used to predict what may come next. A basic strategy for left-corner chart parsing is given below. Strategy 39 (LC) Whenever an inactive edge is added to the chart, if its category is T, then for every rule in G with T as left-corner symbol add an empty active edgeP Note that this strategy will make &amp;quot;minimal&amp;quot; predictions, i.e., it will only predict the next higher-level phrases which a given constituent can begin. 2.2.2 Left Corner it la Kilbury Kilbury (1985) presents a modified left-corner strategy. Basically it amounts to this: instead of predicting empty active edges, edges which subsume the inactive edge that provoked the new edge are predicted. A predicted new edge may then be either active or inactive depending on the contents of the inactive edge and on what is required by the new edge. This strategy has two clear advantages: First, it saves many edges compared to the &amp;quot;normal&apos; left corner because it never produces empty active edges. Secondly (and not pointed out by Kilbury), the usual redundancy check is not needed here since the strategy </context>
<context position="15069" citStr="Kilbury (1985" startWordPosition="2430" endWordPosition="2431">a higher-level structure for the phrase about to be built by the bottom-up parser? On the chart, this corresponds to asking if any (active) edge ending in the starting vertex of the proposed edge needs this this kind of edge, directly or indirectly. The procedure for computing the answer to this again makes use of the reachability relation (cf. section 2.1.2).&amp;quot; Adding top-down filtering to the LC strategy above produces the following strategy. Strategy 5 (Let) Let v be the vertex from which the triggering edge T extends. Let A1, , Am be the active edges incident to v, and let r(Ai) be their liKilbury (1985:10) actually makes use of a similar relation encoding the left-branchings of the grammar (the &amp;quot;firstrelation&amp;quot;), but he uses it only for speeding up grammar-rule access (by indexing rules from left corners) and not for the purpose of filtering out unwanted edges. respective first required constituents. — Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add an empty active C edge if for some i r(Ai) = C or r(A)RC. Analogously, adding top-down filtering to Kilbury&apos;s strategy LCK results in the following. Strategy 6 (LC</context>
</contexts>
<marker>Kilbury, 1985</marker>
<rawString>Kilbury, James (1985). Chart Parsing and the Earley Algorithm. KIT-Report 24, Projektgruppe Kunstliche Intelligens und Textverstehen, Techniache Universitat Berlin, West Berlin. Also in: U. Klenk, ed. (1985), Kontextfreie Syntaxen und verwandte Systeme. Vortrcige eines Kolloquiums in Grand Ventron im Oktober, 1984. Niemeyer, Tiibingen, Federal Republic of Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Merkel</author>
</authors>
<title>A Swedish Grammar in D-PATR. Experiences of Working with D-PATR.</title>
<date>1986</date>
<tech>Research report LiTH-IDA-R-86-31,</tech>
<institution>Department of Computer and Information Science, Linkiiping University, Link&apos;aping, Sweden.</institution>
<contexts>
<context position="18346" citStr="Merkel (1986)" startWordPosition="3011" endWordPosition="3012">en for every rule C in G with T as left-corner symbol add a C edge subsuming the T edge if for some i = C or r(Ai)2C and for some j r(C) = pi or r(C)p1. 3 Empirical Results In order to assess the practical behaviour of the strategies discussed above, a test bench was developed where it was made possible in effect to switch between eight different parsers corresponding to the eight strategies above, and also between different grammars, dictionaries, and sentence sets. Several experiments were conducted along the way. The test grammars used were first partly based on a Swedish D-PATR grammar by Merkel (1986). Later on, I decided to use (some of) the data compiled by Tomita (1986) for the testings of his extended LR parser. This section presents the results of the latter experiments. 3.1 Grammars and Sentence Sets The three grammars and two sentence sets used in these experiments have been obtained from Masaru Tomita and can be found in his book (Tomita 1986). Grammars I and II are toy grammars consisting of 8 and 43 rules, respectively. Grammar III with 224 rules is constructed to fit sentence set I which is a collection of 40 sentences collected from authentic texts. (Grammar IV with 394 rules w</context>
</contexts>
<marker>Merkel, 1986</marker>
<rawString>Merkel, Magnus (1986). A Swedish Grammar in D-PATR. Experiences of Working with D-PATR. Research report LiTH-IDA-R-86-31, Department of Computer and Information Science, Linkiiping University, Link&apos;aping, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<pages>13--3</pages>
<contexts>
<context position="1387" citStr="Pereira and Warren 1980" startWordPosition="203" endWordPosition="206">les to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing. 1 Background and Introduction An apparent tendency in computational linguistics during the last few years has been towards declarative grammar formalisms. This tendency has manifested itself with respect to linguistic tools, perhaps seen most clearly in the evolution from ATNs with their strongly procedural grammars to PATR-II in its various incarnations (Shieber et al. 1983, Karttunen 1986), and to logic-based formalisms such as DCG (Pereira and Warren 1980). It has also manifested itself in linguistic theories, where there has been a development from systems employing sequential derivations in the analysis of sentence structures to systems like LFG and GPSG which establish relations among the elements of a sentence in an order-independent and also direction-independent way. For example, phenomena such as rule ordering simply do not arise in these theories. This research has been supported by the National Swedish Board for Technical Development. In addition, declarative formalisms are, in principle, processor-independent. Procedural formalisms, a</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando C. N. and David H. D. Warren (1980). Definite Clause Grammars for Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence 13(3):231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaughan R Pratt</author>
</authors>
<title>LINGOL — A Progress Report.</title>
<date>1975</date>
<booktitle>Proc. 4th IJCAI,</booktitle>
<pages>422--428</pages>
<location>Tbilisi, Georgia, USSR:</location>
<contexts>
<context position="4958" citStr="Pratt (1975)" startWordPosition="746" endWordPosition="747">ft-corner strategy, arguing that with respect to natural-language grammars it will generally outperform the top-down (Earley-style) strategy. Wang (1985) discusses Kilbury&apos;s and Earley&apos;s algorithms, favouring the latter because of the inefficient way in which bottom-up algorithms deal with rules with right common factors. Neither Wang nor Kilbury considers the natural approach to overcoming this problem, viz, top-down filtering (cf. section 2.2.3). As for empirical studies, Slocum (1981) is a rich source. Among many other things, he provides some performance data regarding top-down filtering. Pratt (1975) reports on a successful augmentation of a bottom-up chart-like parser with a top-down filter. Tomita (1985, 1986) introduces a very efficient, extended LR-parsing algorithm that can deal with full context-free languages. Based on empirical comparisons, Tomita shows his algorithm to be superior to Earley&apos;s algorithm and also to a modified version thereof (corresponding here to &apos;selective topdown&amp;quot;; cf. section 2.1.2). Thus, with respect to raw efficiency, it seems clear that Tomita&apos;s algorithm is superior to comparable chart-parsing algorithms. However, a chart-parsing framework does have its a</context>
<context position="9124" citStr="Pratt 1975" startWordPosition="1412" endWordPosition="1413">sing starts with a scanning phase, the adoption of this filter is straightforward. The strategy makes use of a reachability relation 2 where ARB holds if there exists some derivation from A to B such that B is the first element in a string dominated by A. Given preterminal lookahead symbol(s) pi corresponding to the next word, the processor can then ask if the first required constituent of a predicted active edge (say, C) can somehow start with (some) pi. In practice, the relation is implemented as a precompiled table. Determining if holds can then be made very fast and in constant time. (Cf. Pratt 1975:424.) The strategy presented here corresponds to Kay&apos;s &amp;quot;directed top-down&amp;quot; strategy (Kay 1982:338) and can be specified in the following manner. Strategy 2 (TD,) Let r(X) be the first required constituent of the (active) edge X. Let v be the vertex to which the active edge about to be proposed extends. Let pi,..., py, be the preterminal categories of the edges extending from v that correspond to the next word. — Whenever an active edge is added to the chart, if its first required constituent is C, then for every rule in G which expands C add an empty active C edge if for some j r(C) = pi or r</context>
<context position="14101" citStr="Pratt (1975" startWordPosition="2264" endWordPosition="2265"> edge. 2.2.3 Top-Down Filtering As often pointed out, bottom-up and left-corner strategies encounter problems with sets of rules like A —■ BC and A C (right common factors). For example, assuming standard grammar rules, when parsing the phrase &amp;quot;the birds fly&amp;quot; an unwanted sentence &amp;quot;birds fly&amp;quot; will be discovered. This problem can be met by adopting top-down filtering, a technique which can be seen as the dual of the selective top-down strategy. Descriptions of top-down filtering are given for example in Kay (1982) (&amp;quot;directed bottom-up parsing&amp;quot;) and in Slocum (1981:2). Also, the &amp;quot;oracle&amp;quot; used by Pratt (1975:424) is a top-down filter. Essentially top-down filtering is like running a topdown parser in parallel with a bottom-up parser. The (simulated) top-down parser rejects some of the edges that the bottom-up parser proposes, viz, those that the former would not discover. The additional question that the top-down filter asks is then: is there any place in a higher-level structure for the phrase about to be built by the bottom-up parser? On the chart, this corresponds to asking if any (active) edge ending in the starting vertex of the proposed edge needs this this kind of edge, directly or indirec</context>
</contexts>
<marker>Pratt, 1975</marker>
<rawString>Pratt, Vaughan R. (1975). LINGOL — A Progress Report. Proc. 4th IJCAI, Tbilisi, Georgia, USSR: 422-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Hans Uszkoreit</author>
<author>Fernando C N Pereira</author>
<author>Jane J Robinson</author>
<author>Mabry Tyson</author>
</authors>
<title>The Formalism and Implementation of PATR-II.</title>
<date>1983</date>
<booktitle>Research on Interactive Acquisition and Use of Knowledge. SRI Final Report 1894, SRI International,</booktitle>
<editor>In: Barbara Grosz and Mark Stickel, eds.,</editor>
<location>Menlo Park, California.</location>
<contexts>
<context position="1301" citStr="Shieber et al. 1983" startWordPosition="189" endWordPosition="192">aximal processing efficiency, one has to determine the best way of putting the rules to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing. 1 Background and Introduction An apparent tendency in computational linguistics during the last few years has been towards declarative grammar formalisms. This tendency has manifested itself with respect to linguistic tools, perhaps seen most clearly in the evolution from ATNs with their strongly procedural grammars to PATR-II in its various incarnations (Shieber et al. 1983, Karttunen 1986), and to logic-based formalisms such as DCG (Pereira and Warren 1980). It has also manifested itself in linguistic theories, where there has been a development from systems employing sequential derivations in the analysis of sentence structures to systems like LFG and GPSG which establish relations among the elements of a sentence in an order-independent and also direction-independent way. For example, phenomena such as rule ordering simply do not arise in these theories. This research has been supported by the National Swedish Board for Technical Development. In addition, dec</context>
</contexts>
<marker>Shieber, Uszkoreit, Pereira, Robinson, Tyson, 1983</marker>
<rawString>Shieber, Stuart M., Hans Uszkoreit, Fernando C. N. Pereira, Jane J. Robinson, and Mabry Tyson (1983). The Formalism and Implementation of PATR-II. In: Barbara Grosz and Mark Stickel, eds., Research on Interactive Acquisition and Use of Knowledge. SRI Final Report 1894, SRI International, Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Slocum</author>
</authors>
<title>A Practical Comparison of Parsing Strategies.</title>
<date>1981</date>
<booktitle>Proc. 19th ACL,</booktitle>
<pages>1--6</pages>
<location>Stanford, California:</location>
<contexts>
<context position="4838" citStr="Slocum (1981)" startWordPosition="728" endWordPosition="729">ion strategies mainly from the perspective of GPSG parsing which is not the main point here. Kilbury (1985) presents a left-corner strategy, arguing that with respect to natural-language grammars it will generally outperform the top-down (Earley-style) strategy. Wang (1985) discusses Kilbury&apos;s and Earley&apos;s algorithms, favouring the latter because of the inefficient way in which bottom-up algorithms deal with rules with right common factors. Neither Wang nor Kilbury considers the natural approach to overcoming this problem, viz, top-down filtering (cf. section 2.2.3). As for empirical studies, Slocum (1981) is a rich source. Among many other things, he provides some performance data regarding top-down filtering. Pratt (1975) reports on a successful augmentation of a bottom-up chart-like parser with a top-down filter. Tomita (1985, 1986) introduces a very efficient, extended LR-parsing algorithm that can deal with full context-free languages. Based on empirical comparisons, Tomita shows his algorithm to be superior to Earley&apos;s algorithm and also to a modified version thereof (corresponding here to &apos;selective topdown&amp;quot;; cf. section 2.1.2). Thus, with respect to raw efficiency, it seems clear that T</context>
<context position="14058" citStr="Slocum (1981" startWordPosition="2257" endWordPosition="2258">orner symbol add an edge that subsumes the T edge. 2.2.3 Top-Down Filtering As often pointed out, bottom-up and left-corner strategies encounter problems with sets of rules like A —■ BC and A C (right common factors). For example, assuming standard grammar rules, when parsing the phrase &amp;quot;the birds fly&amp;quot; an unwanted sentence &amp;quot;birds fly&amp;quot; will be discovered. This problem can be met by adopting top-down filtering, a technique which can be seen as the dual of the selective top-down strategy. Descriptions of top-down filtering are given for example in Kay (1982) (&amp;quot;directed bottom-up parsing&amp;quot;) and in Slocum (1981:2). Also, the &amp;quot;oracle&amp;quot; used by Pratt (1975:424) is a top-down filter. Essentially top-down filtering is like running a topdown parser in parallel with a bottom-up parser. The (simulated) top-down parser rejects some of the edges that the bottom-up parser proposes, viz, those that the former would not discover. The additional question that the top-down filter asks is then: is there any place in a higher-level structure for the phrase about to be built by the bottom-up parser? On the chart, this corresponds to asking if any (active) edge ending in the starting vertex of the proposed edge needs </context>
<context position="26195" citStr="Slocum 1981" startWordPosition="4385" endWordPosition="4386">ivity filter here turned out to be much more time efficient, though. Selectivity testing is also basically a simple operation, seldom involving more than a few lookups (depending on the degree of lexical ambiguity). Paradoxically, the effect of top-down filtering was to degrade time performance as the grammars grew larger. To a large extent this is likely to have been caused by implementation idiosyncrasies: active edges incident to a vertex were searched linearly; when the number of edges increases, this gets very costly. After all, top-down filtering is generally considered beneficial (e.g. Slocum 1981:4). The maximally directed strategy — Kilbury&apos;s algorithm with selectivity and top-down filtering — remained the most efficient one throughout all the experiments, both with respect to edges produced and time consumed (but more so with respect to the former). Top-down filtering did not degrade time performance quite as much in this case, presumably because of the great number of active edges cut off by the selectivity filter. Finally, it should be mentioned that bottom-up parsing enjoys a special advantage not shown here, namely in being able to detect ungrammatical sentences much more effect</context>
</contexts>
<marker>Slocum, 1981</marker>
<rawString>Slocum, Jonathan (1981). A Practical Comparison of Parsing Strategies. Proc. 19th ACL, Stanford, California: 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Thompson</author>
</authors>
<title>Chart Parsing and Rule Schemata in GPSG.</title>
<date>1981</date>
<booktitle>Proc. 19th ACL,</booktitle>
<tech>Research Paper No. 165,</tech>
<pages>167--172</pages>
<institution>Department of Artificial Intelligence, University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="3530" citStr="Thompson (1981)" startWordPosition="535" endWordPosition="536">). Context-free phrase-structure grammar is of interest here in particular because it is utilized as the backbone of many declarative formalisms. The chart-parsing framework is of interest in this connection because, being a &amp;quot;higher-order algorithm&amp;quot; (Kay 1982:329), it lends itself easily to the processing of different grammatical formalisms. At the same time it is of course a natural test bed for experiments with various control strategies. Previously a number of comparisons of ruleinvocation strategies in this or in similar settings have been reported: 1This term seems to have been coined by Thompson (1981). Basically, it refers to the spectrum between top-down and bottom-up processing of the grammar rules. 2The other principal control-strategy dimension, the search stndegy (depth-first vs. breadth-first), is irrelevant for the efficiency in chart parsing since it only affects the order in which successive (partial) analyses are developed. 226 Kay (1982) is the principal source, providing a very general exposition of the control strategies and data structures involved in chart parsing. In considering the efficiency question, Kay favours a &amp;quot;directed&amp;quot; bottom-up strategy (cf. section 2.2.3). Thomps</context>
<context position="8070" citStr="Thompson (1981" startWordPosition="1230" endWordPosition="1231">basic familiarity with chart parsing. For an excellent introduction, see Thompson and Ritchie (1984). 4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to &amp;quot;items&amp;quot; in Aho and Ullman (1972:320). 6Top-down (context-free) chart parsing is sometimes called &amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way in which Earley&apos;s algorithm (Earley 1970) works. It should be pointed out that the parse-forest representation employed here does not suffer from the kind of defect claimed by Tomita (1985:762, 1986:74) to result from Earley&apos;s algorithm. 6This formulation is equivalent to the one in Thompson (1981:4). 7Note that in order to handle left-recursive rules without going into an infinite loop, this strategy needs a redundancy check which prevents more than one identical active edge from being added to the chart. 227 and Petrick 1965:291): by looking at the category/categories of the next word, it is possible to rule out some proposed edges that are known not to combine with the corresponding inactive edge(s). Given that top-down chart parsing starts with a scanning phase, the adoption of this filter is straightforward. The strategy makes use of a reachability relation 2 where ARB holds if th</context>
<context position="12793" citStr="Thompson (1981" startWordPosition="2050" endWordPosition="2052"> i.e. rules of the form A e), cannot be handled. This might look like a serious limitation since most current linguistic theories (e.g., LFG, GPSG) make explicit use of c-productions, typically for the handling of gaps. On the other hand, context-free grammars can be converted into grammars without eproductions (Aho and Ulhnan 1972:150). In practice however, e-productions can be handled in various ways which circumvent the problem. For example, Karttunen&apos;s D-PATR system 8The left corner of a rule is the leftmost symbol of its righthand side. 8This formulation is again equivalent to the one in Thompson (1981:4). Thompson however refers to it as &amp;quot;bottom-up&amp;quot;. 181n this case, left-recursive rules will not lead to infinite loops. The redundancy check is still needed to prevent superfluous analyses from being generated, though. 228 does not allow empty productions. Instead, it takes care of fillers and gaps through a &amp;quot;threading&amp;quot; technique (Karttunen 1986:77). Indeed, the system has been successfully used for writing LFG-style grammars (e.g., Dyvik 1986). Kilbury&apos;s left-corner strategy can be specified in the following manner. Strategy 4 (LCK) Whenever an inactive edge is added to the chart, if its cat</context>
<context position="20206" citStr="Thompson 1981" startWordPosition="3342" endWordPosition="3344">cope...&amp;quot;. In these experiments n = 1, , 7 was used. The dictionary was constructed from the category sequences given by Tomita together with the sentences (Tomita 1986 pp. 185-189). 3.2 Efficiency Measures A reasonable efficiency measure in chart parsing is the number of edges produced. The motivation for this is that the working of a chart parser is tightly centered around the production and manipulation of edges, and that much of its work can somehow be reduced to this. For example, a measure of the amount of work done at each vertex by the procedure which implements &amp;quot;the fundamental rule&amp;quot; (Thompson 1981:2) can be expressed as the product of the number of incoming active edges and the number of outgoing inactive edges. In addition, the number of chart edges produced is a measure which is independent of implementation and machine. On the other hand, the number of edges does not give any indication of the overhead costs involved in various strategies. Hence I also provide figures of the parsing times, albeit with a warning for taking them too seriously.13 The experiments were run on Xerox 1186 Lisp machines. The time measures were obtained using the Interlisp-D function TIMEALL. The time figure</context>
</contexts>
<marker>Thompson, 1981</marker>
<rawString>Thompson, Henry (1981). Chart Parsing and Rule Schemata in GPSG. Research Paper No. 165, Department of Artificial Intelligence, University of Edinburgh, Edinburgh, Scotland. Also in: Proc. 19th ACL, Stanford, California: 167-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Thompson</author>
<author>Graeme Ritchie</author>
</authors>
<title>Implementing Natural Language Parsers. In: Tim O&apos;Shea and Marc Eisenstadt, Artificial Intelligence: Tools, Techniques, and Applications. Harper Sc Row,</title>
<date>1984</date>
<pages>245--300</pages>
<location>New York, New York:</location>
<contexts>
<context position="7556" citStr="Thompson and Ritchie (1984)" startWordPosition="1151" endWordPosition="1154">bsidiary active edges also get produced. 2.1.2 Selective Top-Down Realistic natural-language grammars are likely to be highly branching. A weak point of the &amp;quot;normal&amp;quot; top-down strategy above will then be the excessive number of predictions typically made: in the beginning of a phrase new edges will be introduced for all constituents, and constituents within those constituents, that the phrase can possibly start with. One way of limiting the number of predictions is by making the strategy &apos;selective&amp;quot; (Griffiths 31 assume a basic familiarity with chart parsing. For an excellent introduction, see Thompson and Ritchie (1984). 4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to &amp;quot;items&amp;quot; in Aho and Ullman (1972:320). 6Top-down (context-free) chart parsing is sometimes called &amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way in which Earley&apos;s algorithm (Earley 1970) works. It should be pointed out that the parse-forest representation employed here does not suffer from the kind of defect claimed by Tomita (1985:762, 1986:74) to result from Earley&apos;s algorithm. 6This formulation is equivalent to the one in Thompson (1981:4). 7Note that in order to handle left-recursive rules without going into an infinite</context>
</contexts>
<marker>Thompson, Ritchie, 1984</marker>
<rawString>Thompson, Henry and Graeme Ritchie (1984). Implementing Natural Language Parsers. In: Tim O&apos;Shea and Marc Eisenstadt, Artificial Intelligence: Tools, Techniques, and Applications. Harper Sc Row, New York, New York: 245-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient Context-free Parsing Algorithm For Natural Languages.</title>
<date>1985</date>
<booktitle>Proc. 9th IJCAI,</booktitle>
<pages>756--764</pages>
<location>Los Angeles, California:</location>
<contexts>
<context position="5065" citStr="Tomita (1985" startWordPosition="762" endWordPosition="763"> top-down (Earley-style) strategy. Wang (1985) discusses Kilbury&apos;s and Earley&apos;s algorithms, favouring the latter because of the inefficient way in which bottom-up algorithms deal with rules with right common factors. Neither Wang nor Kilbury considers the natural approach to overcoming this problem, viz, top-down filtering (cf. section 2.2.3). As for empirical studies, Slocum (1981) is a rich source. Among many other things, he provides some performance data regarding top-down filtering. Pratt (1975) reports on a successful augmentation of a bottom-up chart-like parser with a top-down filter. Tomita (1985, 1986) introduces a very efficient, extended LR-parsing algorithm that can deal with full context-free languages. Based on empirical comparisons, Tomita shows his algorithm to be superior to Earley&apos;s algorithm and also to a modified version thereof (corresponding here to &apos;selective topdown&amp;quot;; cf. section 2.1.2). Thus, with respect to raw efficiency, it seems clear that Tomita&apos;s algorithm is superior to comparable chart-parsing algorithms. However, a chart-parsing framework does have its advantages, particularly in its flexibility and openendedness. The contribution this paper makes is: • to su</context>
<context position="7960" citStr="Tomita (1985" startWordPosition="1214" endWordPosition="1215"> One way of limiting the number of predictions is by making the strategy &apos;selective&amp;quot; (Griffiths 31 assume a basic familiarity with chart parsing. For an excellent introduction, see Thompson and Ritchie (1984). 4Edges correspond to &amp;quot;states&amp;quot; in Earley (1970) and to &amp;quot;items&amp;quot; in Aho and Ullman (1972:320). 6Top-down (context-free) chart parsing is sometimes called &amp;quot;Earley-style&amp;quot; chart parsing because it corresponds to the way in which Earley&apos;s algorithm (Earley 1970) works. It should be pointed out that the parse-forest representation employed here does not suffer from the kind of defect claimed by Tomita (1985:762, 1986:74) to result from Earley&apos;s algorithm. 6This formulation is equivalent to the one in Thompson (1981:4). 7Note that in order to handle left-recursive rules without going into an infinite loop, this strategy needs a redundancy check which prevents more than one identical active edge from being added to the chart. 227 and Petrick 1965:291): by looking at the category/categories of the next word, it is possible to rule out some proposed edges that are known not to combine with the corresponding inactive edge(s). Given that top-down chart parsing starts with a scanning phase, the adoptio</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, Masaru (1985). An Efficient Context-free Parsing Algorithm For Natural Languages. Proc. 9th IJCAI, Los Angeles, California: 756-764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language. A Fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, Massachusetts.</location>
<contexts>
<context position="18419" citStr="Tomita (1986)" startWordPosition="3026" endWordPosition="3027">ing the T edge if for some i = C or r(Ai)2C and for some j r(C) = pi or r(C)p1. 3 Empirical Results In order to assess the practical behaviour of the strategies discussed above, a test bench was developed where it was made possible in effect to switch between eight different parsers corresponding to the eight strategies above, and also between different grammars, dictionaries, and sentence sets. Several experiments were conducted along the way. The test grammars used were first partly based on a Swedish D-PATR grammar by Merkel (1986). Later on, I decided to use (some of) the data compiled by Tomita (1986) for the testings of his extended LR parser. This section presents the results of the latter experiments. 3.1 Grammars and Sentence Sets The three grammars and two sentence sets used in these experiments have been obtained from Masaru Tomita and can be found in his book (Tomita 1986). Grammars I and II are toy grammars consisting of 8 and 43 rules, respectively. Grammar III with 224 rules is constructed to fit sentence set I which is a collection of 40 sentences collected from authentic texts. (Grammar IV with 394 rules was not used here.) Because grammar III contains one empty production, not</context>
<context position="19760" citStr="Tomita 1986" startWordPosition="3267" endWordPosition="3268">d 21 sentences out of the sentence set. This reduced set will henceforth be referred to as sentence set 1.12 The sentences in this set vary in length between 1 and 27 words. Sentence set II was made systematically from the schema noun verb det noun (prep det noun)&apos;&apos;. &amp;quot;The sentences in the set are 1-3, 9, 13-15, 19-25, 29, and 35-40 (cf. Tomita 1986:152). An example of a sentence with this structure is &amp;quot;I saw the man in the park with a telescope...&amp;quot;. In these experiments n = 1, , 7 was used. The dictionary was constructed from the category sequences given by Tomita together with the sentences (Tomita 1986 pp. 185-189). 3.2 Efficiency Measures A reasonable efficiency measure in chart parsing is the number of edges produced. The motivation for this is that the working of a chart parser is tightly centered around the production and manipulation of edges, and that much of its work can somehow be reduced to this. For example, a measure of the amount of work done at each vertex by the procedure which implements &amp;quot;the fundamental rule&amp;quot; (Thompson 1981:2) can be expressed as the product of the number of incoming active edges and the number of outgoing inactive edges. In addition, the number of chart edg</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, Masaru (1986). Efficient Parsing for Natural Language. A Fast Algorithm for Practical Systems. Kluwer Academic Publishers, Norwell, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiguo Wang</author>
</authors>
<title>Computational Linguistics</title>
<date>1985</date>
<tech>Technical Notes No. 2. Technical Report 85/013,</tech>
<institution>Computer Science Department, Boston University,</institution>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="4499" citStr="Wang (1985)" startWordPosition="677" endWordPosition="678">the principal source, providing a very general exposition of the control strategies and data structures involved in chart parsing. In considering the efficiency question, Kay favours a &amp;quot;directed&amp;quot; bottom-up strategy (cf. section 2.2.3). Thompson (1981) is another fundamental source, though he discusses the effects of various ruleinvocation strategies mainly from the perspective of GPSG parsing which is not the main point here. Kilbury (1985) presents a left-corner strategy, arguing that with respect to natural-language grammars it will generally outperform the top-down (Earley-style) strategy. Wang (1985) discusses Kilbury&apos;s and Earley&apos;s algorithms, favouring the latter because of the inefficient way in which bottom-up algorithms deal with rules with right common factors. Neither Wang nor Kilbury considers the natural approach to overcoming this problem, viz, top-down filtering (cf. section 2.2.3). As for empirical studies, Slocum (1981) is a rich source. Among many other things, he provides some performance data regarding top-down filtering. Pratt (1975) reports on a successful augmentation of a bottom-up chart-like parser with a top-down filter. Tomita (1985, 1986) introduces a very efficien</context>
<context position="24697" citStr="Wang (1985" startWordPosition="4144" endWordPosition="4145">rategy. More apparent is its time advantage, due to the basic simplicity of the strategy. For example, it outperforms the top-down strategy in experiments 2 and 3. Results like those above are of course strongly grammar dependent. If, for example, the branching factor of the grammar increases, top-down overpredictions will soon dominate superfluous bottom-up substring generation. This was clearly seen in some of the early experiments not showed here. In cases like this, bottom-up parsing becomes advantageous and, in particular, Kilbury&apos;s strategy will outperform the two others. Thus, although Wang (1985:7) seems to be right in claiming that &amp;quot;...Earley&apos;s algorithm is better than Kilbury&apos;s in general.&amp;quot;, in practice this can often be different (as Wang himself recognizes). Incidentally, Wang&apos;s own example (:4), aimed at showing that Kilbury&apos;s algorithm handles right recursion worse than Earley&apos;s algorithm, illustrates this: Assume a grammar with rules S Ac, A --+ aA, A and a sentence &amp;quot;a aaab c&amp;quot; to be parsed. Here a bottom-up parser such as Kilbury&apos;s will obviously do some useless work in predicting several unwanted S edges. But even so the top-down overpredictions will actually dominate: the Ea</context>
</contexts>
<marker>Wang, 1985</marker>
<rawString>Wang, Weiguo (1985). Computational Linguistics Technical Notes No. 2. Technical Report 85/013, Computer Science Department, Boston University, Boston, Massachusetts.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>