<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000191">
<title confidence="0.985841">
Mapping dialectal variation by querying social media
</title>
<author confidence="0.997297">
Gabriel Doyle
</author>
<affiliation confidence="0.998974">
Department of Linguistics
University of California, San Diego
</affiliation>
<address confidence="0.472994">
La Jolla, CA, USA 92093-0108
</address>
<email confidence="0.998842">
gdoyle@ucsd.edu
</email>
<sectionHeader confidence="0.9939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999518">
We propose a Bayesian method of esti-
mating a conditional distribution of data
given metadata (e.g., the usage of a di-
alectal variant given a location) based
on queries from a big data/social me-
dia source, such as Twitter. This distri-
bution is structurally equivalent to those
built from traditional experimental meth-
ods, despite lacking negative examples.
Tests using Twitter to investigate the ge-
ographic distribution of dialectal forms
show that this method can provide distri-
butions that are tightly correlated with ex-
isting gold-standard studies at a fraction of
the time, cost, and effort.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945163265306">
Social media provides a linguist with a new data
source of unprecedented scale, opening novel av-
enues for research in empirically-driven areas,
such as corpus and sociolinguistics. Extracting the
right information from social media, though, is not
as straightforward as in traditional data sources, as
the size and format of big data makes it too un-
wieldy to observe as a whole. Researchers often
must interact with big data through queries, which
produce only positive results, those matching the
search term. At best, this can be augmented with
a set of “absences” covering results that do not
match the search term, but explicit negative data
(e.g., confirmation that a datapoint could never
match the search term) does not exist. In addition
to the lack of explicit negative data, query-derived
data has a conditional distribution that reverses the
dependent and independent variables compared to
traditional data sources, such as sociolinguistic in-
terviews.
This paper proposes a Bayesian method for
overcoming these two difficulties, allowing query-
derived data to be applied to traditional problems
without requiring explicit negative data or the abil-
ity to view the entire dataset at once. The test case
in this paper is dialect geography, where the pos-
itive data is the presence of a dialectal word or
phrase in a tweet, and the metadata is the location
of the person tweeting it. However, the method
is general and applies to any queryable big data
source that includes metadata about the user or set-
ting that generated the data.
The key to this method lies in using an indepen-
dent query to estimate the overall distribution of
the metadata. This estimated distribution corrects
for non-uniformity in the data source, enabling the
reversal of the conditionality on the query-derived
distribution to convert it to the distribution of in-
terest.
Section 2 explains the mathematical core of the
Bayesian analysis. Section 3 implements this anal-
ysis for Twitter and introduces an open-source
program for determining the geographic distri-
bution of tweets. Section 4 tests the method on
problems in linguistic geography and shows that
its results are well-correlated with those of tradi-
tional sociolinguistic research. Section 5 addresses
potential concerns about noise or biases in the
queries.
</bodyText>
<sectionHeader confidence="0.910699" genericHeader="introduction">
2 Reversing the conditionality of query
data
</sectionHeader>
<subsectionHeader confidence="0.99511">
2.1 Corpora and positive-only data
</subsectionHeader>
<bodyText confidence="0.99992">
In traditional linguistic studies, the experimenter
has control over the participants’ metadata, but
not over their data. For instance, a sociolinguist
may select speakers with known ages or locations,
but will not know their usages in advance. Cor-
pus queries reverse the direction of investigation;
the experimenter selects a linguistic form to search
for, but then lacks control over the metadata of the
participants who use the query. The direction of
conditionality must be reversed to get compara-
</bodyText>
<page confidence="0.857806">
98
</page>
<note confidence="0.9936115">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 98–106,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937854166667">
ble information from query-derived and traditional
data.
Queries also complicate the problem by pro-
viding only positive examples. This lack of ex-
plicit negative data is common in language ac-
quisition, as children encounter mostly grammat-
ical statements during learning, and receive few
explicitly ungrammatical examples, yet still de-
velop a consistent grammaticality classification
system as they mature. Similar positive-only prob-
lems abound in cognitive science and artificial in-
telligence, and a variety of proposals have been
offered to overcome it in different tasks. These
include biases like the Size Principle (Tenen-
baum and Griffiths, 2001), heuristics like gen-
erating pseudo-negatives from unobserved data
(Okanohara and Tsujii, 2007; Poon et al., 2009),
or innate prespecifications like Universal Gram-
mar in the Principles and Parameters framework.
For query-derived data, Bayesian reasoning can
address both problems by inverting the condi-
tionality of the distribution and implying negative
data. The key insight is that a lack of positive ex-
amples where positive examples are otherwise ex-
pected is implicit negative evidence. This method
allows a researcher to produce an estimated distri-
bution that approximates the true conditional dis-
tribution up to a normalizing factor. This condi-
tional distribution is that of data (e.g., a dialectal
form) conditioned on metadata (e.g., a location).
This distribution can be written as p(D|M),
where D and M are random variables represent-
ing the data and metadata. A query for a data value
d returns metadata values m distributed according
to p(M|D = d). All of the returned results will
have the searched-for data value, but the metadata
can take any value.
For most research, p(M|D = d) is not the dis-
tribution of interest, as it is conflated with the over-
all distribution of the metadata. For instance, if
the query results indicate that 60% of users of the
linguistic form d live in urban areas, this seems
to suggest that the linguistic form is more likely
in urban areas. But if 80% of people live in ur-
ban areas, the linguistic form is actually underrep-
resented in these areas, and positively associated
with rural areas. An example of the effect of such
misanalysis is shown in Sect. 4.2.
</bodyText>
<subsectionHeader confidence="0.999571">
2.2 Reversing the conditionality
</subsectionHeader>
<bodyText confidence="0.999896">
Bayesian reasoning allows a researcher to move
from the sampled p(M|D) distribution to the de-
sired p(D|M). We invoke Bayes’ Rule:
</bodyText>
<equation confidence="0.659528">
p(M)
</equation>
<bodyText confidence="0.9947716">
In some situations, these underlying distribu-
tions will be easily obtainable. For small corpora,
p(D) and p(M) can be calculated by enumeration.
For data with explicit negative examples available,
p(D) can be estimated as the ratio of positive ex-
amples to the sum of positive and negative exam-
ples.1 But for queries in general, neither of these
approximations is possible. Instead, we estimate
p(M) through the querying mechanism itself.
This is done by choosing a “baseline” query
term q whose distribution is approximately inde-
pendent of the metadata – that is, a query q such
that p(q|m) is approximately constant for all meta-
data values m E M. If p(q|m) is constant, then by
Bayes’ Rule:
</bodyText>
<equation confidence="0.998062">
p(m|q) = p(q|m)p(m) pz� p(m), bm E M
p(q)
</equation>
<bodyText confidence="0.997614428571428">
Thus we can treat results from a baseline query
as though they are draws directly from p(M), and
estimate the denominator from this distribution.
The remaining unknown distribution p(d) is con-
stant for a given data value d, so combining the
above equations yields the unnormalized probabil-
ity ˜p(d|M):
</bodyText>
<equation confidence="0.895218">
p(M|q). (1)
</equation>
<bodyText confidence="0.999798875">
This switch to the unnormalized distribution can
improve interpretability as well. If ˜p(d|m) = 1,
then p(m|d) = p(m|q), which means that the
metadata m is observed for the linguistic form d
just as often as it is for the baseline query. When
˜p(d|m) &gt; 1, the linguistic form is more common
for metadata m than average, and when ˜p(d|m) &lt;
1, the form is less common for that metadata.2
</bodyText>
<footnote confidence="0.994747125">
1This can be extended to multi-class outcomes; if D has
more than two outcomes, each possible outcome is an implicit
negative example for the other possible outcomes.
2If a normalized distribution is needed, p(d) may be es-
timable, depending on the data source. In the Twitter data pre-
sented here, tweets are sequentially numbered, so p(d) could
be estimated using these index numbers. This paper only uses
unnormalized distributions.
</footnote>
<equation confidence="0.999519">
p(D|M) = p(M|D)p(D)
p(d|M) a ˜p(d|M) = p(M|d)
</equation>
<page confidence="0.989467">
99
</page>
<subsectionHeader confidence="0.999506">
2.3 Coverage and confidence
</subsectionHeader>
<bodyText confidence="0.999993">
Due to the potentially non-uniform distribution
of metadata, the amount of error in the estimate
in Eq. 1 can vary with m. Intuitively, the confi-
dence in the conditional probability estimates de-
pends on the amount of data observed for each
metadata value. Because queries estimate p(MId)
by repeated draws from that distribution, the er-
ror in the estimate decreases as the number of
draws increases. The overall error in the estimate
of ˜p(d|m) decreases as the number of datapoints
observed at m increases. This suggests estimating
confidence as the square root of the count of ob-
servations of the metadata m, as the standard error
of the mean decreases in proportion to the square
root of the number of observations. More complex
Bayesian inference can be used improve error es-
timates in the future.
</bodyText>
<sectionHeader confidence="0.987835" genericHeader="method">
3 Sample Implementation: SeeTweet
</sectionHeader>
<bodyText confidence="0.9998368">
This section implements the method described in
the previous section on a case study of the ge-
ographic distributions of linguistic forms, calcu-
lated from recent tweets. It is implemented as
a suite of novel open-source Python/R programs
called SeeTweet, which queries Twitter, obtains
tweet locations, performs the mathematical anal-
ysis, and maps the results. The suite is avail-
able at http://github.com/gabedoyle/
seetweet.
</bodyText>
<subsectionHeader confidence="0.999663">
3.1 SeeTweet goals
</subsectionHeader>
<bodyText confidence="0.999979916666667">
Traditionally, sociolinguistic studies are highly
time-intensive, and broad coverage is difficult to
obtain at reasonable costs. Two data sources that
we compare SeeTweet to are the Atlas of North
American English (Labov et al., 2008, ANAE)
and the Harvard Dialect Survey (Vaux and Golder,
2003, HDS), both of which obtained high-quality
data, but over the course of years. Such studies
remain the gold-standard for most purposes, but
SeeTweet presents a rapid, cheap, and surprisingly
effective alternative for broad coverage on some
problems in dialect geography.
</bodyText>
<subsectionHeader confidence="0.999978">
3.2 Querying Twitter
</subsectionHeader>
<bodyText confidence="0.99438825">
SeeTweet queries Twitter through its API, us-
ing Mike Verdone’s Python Twitter Tools3. The
API returns the 1000 most recent query-matching
tweets or all query-matching tweets within the
</bodyText>
<footnote confidence="0.682919">
3http://mike.verdone.ca/twitter/
</footnote>
<bodyText confidence="0.999976433333333">
last week, whichever is smaller, and can be ge-
ographically limited to tweets within a certain
radius of a center point. In theory, the contigu-
ous United States are covered by a 2500km ra-
dius (Twitter’s maximum) around the geographic
center, approximately 39.8°N, 98.6°W, near the
Kansas-Nebraska border. In practice, though, such
a query only returns tweets from a non-circular re-
gion within the Great Plains.
Through trial-and-error, four search centers
were found that span the contiguous U.S. with
minimal overlap and nearly complete coverage,4
located near Austin, Kansas City, San Diego, and
San Francisco. All results presented here are based
on these four search centers. Tweets located out-
side the U.S. or with unmappable locations are dis-
carded.
The need for multiple queries and the API’s
tweet limit complicate the analysis. The four
searches must be balanced against each other to
avoid overrepresenting certain areas, especially in
constructing the baseline p(M). If any searches
reach the 1000-tweet limit, only the search with
the most recent 1000th tweet has all of its tweets
used. All tweets before that tweet are removed,
balancing the searches by having them all span the
same timeframe. Due to the seven-day limit for re-
cent tweets, many searches do not return 1000 hits;
if none of the searches max out, all returned tweets
are accepted.
</bodyText>
<subsectionHeader confidence="0.999746">
3.3 Establishing the baseline
</subsectionHeader>
<bodyText confidence="0.999889235294117">
For the baseline query (used to estimate p(M)),
SeeTweet needs a query with approximately uni-
form usage across the country. Function or stop
words are reasonable candidates for this task. We
use the word I here, which was chosen as it is
common in all American English dialects but not
other major languages of the U.S., and it has few
obvious alternative forms. Other stop words were
tested, but the specific baseline query had little im-
pact on the learned distribution; correlations be-
tween maps with I, of, the or a baselines were all
above .97 on both baseline distributions and esti-
mated conditional distributions.
Each tweet from the target query requires its
own baseline estimate, as the true distribution of
metadata varies over time. For instance, there will
be relatively more tweets on the East Coast in
</bodyText>
<footnote confidence="0.964851">
4Northern New England has limited coverage, and the
Mountain West returns little data outside the major cities.
</footnote>
<page confidence="0.986431">
100
</page>
<bodyText confidence="0.99998">
the early morning (when much of the West Coast
is still asleep). Thus, SeeTweet builds the base-
line distribution by querying the baseline term I,
and using the first 50 tweets preceding each tar-
get tweet. This query is performed for each search
center for each tweet, with the centers balanced as
discussed in the previous section.5
</bodyText>
<subsectionHeader confidence="0.99967">
3.4 Determining coordinates and mapping
</subsectionHeader>
<bodyText confidence="0.950401761904762">
A tweet’s geographic information can be specified
in many ways. These include coordinates specified
by a GPS system (“geotags”), user-specified coor-
dinates, or user specification of a home location
whose coordinates can be geocoded. Some tweets
may include more that one of these, and SeeTweet
uses this hierarchy: geotags are accepted first, fol-
lowed by user-specified coordinates, followed by
user-specified cities. This hierarchy moves from
sources with the least noise to the most.
Obtaining coordinates from user-specified loca-
tions is done in two steps. First, if the user’s loca-
tion follows a “city, state” format, it is searched
for in the US Board on Geographic Names’s
Geographic Names Information System6, which
matches city names to coordinates. Locations that
do not fit the “city, state” format are checked
against a manually compiled list of coordinates
for 100 major American cities. This second step
catches many cities that are sufficiently well-
known that a nickname is used for the city (e.g.,
Philly) and/or the state is omitted.
Tweets whose coordinates cannot be deter-
mined by these methods are discarded; this is ap-
proximately half of the returned tweets in the ex-
periments discussed here.
This process yields a database of tweet coor-
dinates for each query. To build the probability
distributions, SeeTweet uses a two-dimensional
Gaussian kernel density estimator. Gaussian distri-
butions account for local geographic dependency
and uncertainty in the exact location of a tweeter
as well as smoothing the distributions. The stan-
dard deviation (“bandwidth”) of the kernels is a
free parameter, and can be scaled to supply ap-
propriate coverage/granularity of the map. We use
5An alternative baseline, perhaps even more intuitive,
would be to use some number of sequential tweets preced-
ing the target tweet. However, the Twitter API query mecha-
nism subsamples from the overall set of tweets, so sequential
tweets may not follow the same distribution as the queries
and would provide an inappropriate baseline.
</bodyText>
<footnote confidence="0.925249">
6http://geonames.usgs.gov/domestic/
download_data.htm
</footnote>
<bodyText confidence="0.998605555555556">
3 degrees (approximately 200 miles) of band-
width for all maps in this paper, but found con-
sistently high correlation (at least .79 by Hosmer-
Lemeshow) to the ANAE data in Sect. 4.1 with
bandwidths between 0.5 and 10 degrees.
The KDE estimates probabilities on a grid over-
laid on the map; we make each grid box a square
one-tenth of a degree on each side and calculate
˜p(d|m) for each box m. SeeTweet maps plot the
value of ˜p(d|M) on a color gradient with approxi-
mately constant luminosity. Orange indicates high
probability of the search term, and blue low prob-
ability. Constant luminosity is used so that confi-
dence in the estimate can be represented by opac-
ity; regions with higher confidence in the esti-
mated probability appear more opaque.7 Unfortu-
nately, this means that the maps will not be infor-
mative if printed in black and white.
</bodyText>
<sectionHeader confidence="0.997769" genericHeader="method">
4 Experiments in dialect geography
</sectionHeader>
<bodyText confidence="0.99999325">
Our first goal is to test the SeeTweet results against
an existing gold standard in dialect geography;
for this, we compare SeeTweet distributions of
the needs done construction to those found by
long-term sociolinguistic studies and show that the
quick-and-dirty unsupervised SeeTweet distribu-
tions are accurate reflections of the slow-and-clean
results. Our second goal is show the importance of
using the correct conditional distribution, by com-
paring it to the unadjusted distribution. With these
points established, we then use SeeTweet to create
maps of previously uninvestigated problems.
</bodyText>
<subsectionHeader confidence="0.656331">
4.1 Method verification on need + past
participle
</subsectionHeader>
<bodyText confidence="0.999717642857143">
The Atlas of North American English (Labov et
al., 2008) is the most complete linguistic atlas of
American English dialect geography. It focuses on
phonological variation, but also includes a small
set of lexical/syntactic alternations. One is the
needs + past participle construction, as in The car
needs (to be) washed. This construction has a lim-
ited geographic distribution, and ANAE provides
the first nationwide survey of its usage.
We compare SeeTweet’s conditional probabili-
ties for this construction to the ANAE responses to
see how the relatively uncontrolled Twitter source
compares to the tightly controlled telephone sur-
vey data that ANAE reports. We create a SeeTweet
</bodyText>
<footnote confidence="0.9023935">
7Confidence is given by the square root of the smoothed
number of tweets in a grid box m, p(m|d) ∗ C(d).
</footnote>
<page confidence="0.991572">
101
</page>
<figure confidence="0.995255777777778">
50
45
30
25
−120 −100 −80
Longitude
(a) ANAE/Telsur survey responses for need+past partici-
ple.
(b) SeeTweet search for “needs done”.
</figure>
<figureCaption confidence="0.999789">
Figure 1: Comparing the SeeTweet distribution
</figureCaption>
<bodyText confidence="0.969674296296296">
and ANAE responses for needs done usage. Or-
ange indicates higher local usage, purple moder-
ate, and blue lower. Increased opacity indicates
more confidence (i.e., more tweets) in a region.
map and visually compare this to the ANAE map,
along with a Hosmer-Lemeshow-style analysis.
The SeeTweet map is not calibrated to the ANAE
map; they are each built independently.
The ANAE map (Fig. 1a) shows the responses
of 577 survey participants who were asked about
needs done. Three possible responses were consid-
ered: they used the construction themselves, they
did not use it but thought it was used in their area,
or they neither used it nor believed it to be used in
their area.
The SeeTweet map (Fig. 1b) is built from five
searches for the phrase “needs done”, yielding 480
positive tweets and 32275 baseline tweets.8 The
component distributions p(M|d) and p(M) are es-
timated by Gaussian kernels with bandwidth 3.
The log of ˜p(f|M), calculated as in Eq. 1, de-
termines the color of a region; orange indicates a
higher value, purple a middle (approx. 1) value,
and blue a low value. Confidence in the estimate
is reflected by opacity; higher opacity indicates
higher confidence in the estimate. Confidence val-
ues above 3 (corresponding to 9 tweets per bin) are
</bodyText>
<footnote confidence="0.73327975">
8The verb do was used as it was found to be the most com-
mon verb in corpus work on needs to be [verbed] construc-
tions (Doyle and Levy, 2008), appearing almost three times
as often as the second-most common verb (replace).
</footnote>
<bodyText confidence="0.99980712244898">
fully opaque. This description holds for all other
maps in this paper.
We start with a qualitative comparison of the
maps. Both maps show the construction to be most
prominent in the area between the Plains states and
central Pennsylvania (the North Midland dialect
region), with minimal use in New England and
Northern California and limited use elsewhere.
SeeTweet lacks data in the Mountain West and
Great Plains, and ANAE lacks data for Minnesota
and surrounding states.9 The most notable devia-
tion between the maps is that SeeTweet finds the
construction more common in the Southeast than
ANAE does.
Quantitative comparison is possible by compar-
ing SeeTweet’s estimates of the unnormalized con-
ditional probability of needs done in a location
with the ANAE informants’ judgments there. Two
such comparisons are shown in Fig. 2.
The first comparison (Fig. 2a) is a violin
plot with the ANAE divided into the three re-
sponse categories. The vertical axis represents
the SeeTweet estimates, and the width of a vi-
olin is proportional to the likelihood of that
ANAE response coming from a region of the
given SeeTweet estimate. The violins’ mass shifts
toward regions with lower SeeTweet estimates
(down in the graph) as the respondents report
decreasing use/familiarity with the construction
(moving left to right).
Users of the construction are most likely to
come from regions with above-average condi-
tional probability of needs done, as seen in the left-
most violin. Non-users, whether familiar with the
construction or not, are more likely to come from
regions with below-average conditional probabil-
ity. Non-users who are unfamiliar with it tend to
live in regions with the lowest conditional prob-
abilities of the three groups. This shows the ex-
pected correspondence trend between the ANAE
responses and the estimated prevalence of the con-
struction in an area; the mean SeeTweet estimates
for the three groups are 0.45, −0.34, and −0.61,
respectively.
The second comparison (Fig. 2b) is a Hosmer-
Lemeshow plot. The respondents are first divided
into deciles based on the SeeTweet estimate at
their location. Two mean values are calculated for
each decile: the mean SeeTweet log-probability
</bodyText>
<footnote confidence="0.941157">
9Murray et al. (1996)’s data suggest that these untested
areas would not use the construction; the SeeTweet data sug-
gests this as well.
</footnote>
<figure confidence="0.980136790123456">
Latitude
40
35
●
●
●
● ●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ● ●
●
●
●
●
● ●
●
● ● ●
● ● ● ●
● ●
● ●● ●
●
● ●
● ●
●
●
●
●
● ● ● ●
● ● ●● ● ● ●
● ●
● ● ● ●
● ● ●
● ●
● ●
● ● ●
● ●
●●
●
●
●
●
●
●
●
●
●
● ●
ANAE/Telsur Response Data
a.,. •.
● Used by
Respondent
Familiar to
Respondent
Unfamiliar to
Respondent
102
(a) Violin plot of SeeTweet estimated conditional proba-
bility against ANAE response type.
−2 −1 0
log proportion accepting needs+ppt [ANAE]
(b) Hosmer-Lemeshow plot of SeeTweet distribution
deciles against average probability of ANAE respondent
usage.
</figure>
<figureCaption confidence="0.76309">
Figure 2: Quantifying the relationship between the
SeeTweet distribution and ANAE reports for needs
done.
</figureCaption>
<bodyText confidence="0.99998105">
estimate (increasing with each decile) and the log-
proportion of respondents in that decile who use
the construction.10 If SeeTweet estimates of the
conditional distribution are an adequate reflection
of the ANAE survey data, we should see a tight
correlation between the SeeTweet and ANAE val-
ues in each decile. The correlation between the
two is R2 = 0.90. This is an improvement over the
inappropriate conditional distribution p(MId) that
is obtained by smoothing the tweet map without
dividing by the overall tweet distribution p(M).
Its Hosmer-Lemeshow correlation is R2 = 0.79
These experiments verify two important points:
the SeeTweet method can generate data that is
tightly correlated with gold-standard data from
controlled surveys, and conditionality inversion
establishes a more appropriate distribution to cor-
rect for different baseline frequencies in tweeting.
This second point will be examined further with
double modals in the next section.
</bodyText>
<subsectionHeader confidence="0.9196205">
4.2 Double modals and the importance of the
baseline
</subsectionHeader>
<bodyText confidence="0.998605695652174">
The double modal construction provides a second
test case. While ungrammatical in Standard Amer-
ican English, forms like I might could use your
help are grammatical and common in Southern
American dialects. This construction is interesting
both for its theoretical syntax implications on the
nature of modals as well as the relationship be-
tween its sociolinguistic distribution and its prag-
matics (Hasty, 2011).
The ANAE does not have data on double
modals’ distribution, but another large-scale soci-
olinguistic experiment does: the Harvard Dialect
Survey (Vaux and Golder, 2003). This online sur-
vey obtained 30788 responses to 122 dialect ques-
tions, including the use of double modals. Katz
(2013) used a nearest-neighbor model to create a
p(djM) distribution over the contiguous U.S. for
double modal usage, mapped in Fig. 3a.11 Lighter
colors indicate higher rates of double modal ac-
ceptance.
SeeTweet generates a similar map (Fig. 3b),
based on three searches with 928 positive and
66272 baseline tweets. As with the ANAE test, the
</bodyText>
<footnote confidence="0.856633142857143">
10We remove all respondents who do not use the construc-
tion but report it in their area. Such respondents are fairly
rare (slightly over 10% of the population), and removing this
response converts the data to a binary classification problem
appropriate to Hosmer-Lemeshow analysis.
11http://spark.rstudio.com/jkatz/Data/
comp-53.png
</footnote>
<figure confidence="0.997300813953489">
Used by Familiar to Unfamiliar to
Respondent Respondent Respondent
respondent opinion on needs+ppt [ANAE]
log unnormalized p(needs+ppt|location) [SeeTweet]
−1
−2
2
0
1
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●●
●
●
●
●
●
ANAE vs. SeeTweet, Binned Predictions
●
●
●
●
●
●
●
●
●
2
1
0
−1
−2
−3
log unnormalized p(needs+ppt|location) [SeeTweet]
</figure>
<page confidence="0.522071">
103
</page>
<figure confidence="0.985191">
(a) Katz’s nearest-neighbor estimates of the double
modal’s distribution in the Harvard Dialect Survey.
(b) SeeTweet distribution for might could.
(c) Inappropriate p(M|d) distribution directly estimated
from Twitter hits.
</figure>
<figureCaption confidence="0.99999">
Figure 3: Maps of the double modal’s distribution.
</figureCaption>
<bodyText confidence="0.999811233333333">
SeeTweet map is built independently of the HDS
data and is not calibrated to it.
The notable difference between the maps is
that SeeTweet does not localize double modals as
sharply to the Southeast, with pockets in cities
throughout the country. This may reflect the dif-
ference in the meaning of locations on Twitter and
in the HDS; Twitter locations will be a user’s cur-
rent home, whereas the HDS explicitly asks for a
respondent’s location during their formative years.
SeeTweet may partly capture the spread of dialec-
tal features due to migration.
Double modals also provide an illustration of
the importance of the Bayesian inversion in Eqn.
1, as shown in Fig. 3c. This map, based on
the inappropriate distribution p(M|d), which does
not account for the overall distribution p(M),
disagrees with general knowledge of the double
modal’s geography and the HDS map. Although
both maps find double modals to be prominent
around Atlanta, the inappropriate distribution find
New York City, Chicago, and Los Angeles to be
the next most prominent double modal regions,
with only moderate probability in the rest of the
Southeast. This is not incorrect, per se, as these
are the sources of many double modal tweets; but
these peaks are incidental, as major cities produce
more tweets than the rest of the country. This is
confirmed by their absence in the HDS map as
well as the appropriate SeeTweet map.
</bodyText>
<subsectionHeader confidence="0.997676">
4.3 Extending SeeTweet to new problems
</subsectionHeader>
<bodyText confidence="0.999938210526316">
Given SeeTweet’s success in mapping needs done
and double modals, it can also be used to test new
questions. An understudied issue in past work on
the need + past participle construction is its rela-
tionship with alternative forms need to be + past
participle and need + present participle. Murray
et al. (1996) suggest that their need + past par-
ticiple users reject both alternatives, although it
is worth noting that their informants are more ac-
cepting of the to be alternative, calling it merely
“too formal”, as opposed to an “odd” or “ungram-
matical” opinion about the present participle form.
Their analysis of the opinions on alternative forms
does not go beyond this anecdotal evidence.
SeeTweet provides the opportunity to examine
this issue, and finds that the to be form is per-
sistent across the country (Fig. 4c), both in areas
with and without the need + past participle form,
whereas the present participle alternant (Fig. 4b)
is strongest in areas where need + past participle is
not used. Although further analysis is necessary to
see if the same people use both the past participle
forms, the current data suggests that the bare past
participle and bare present participle forms are in
complementary distribution, while the to be form
is acceptable in most locations.
We also compare the alternative constructions
to the ANAE data. Using Hosmer-Lemeshow
analysis, we find negative correlations: R2 =
−.65 for needs doing and R2 = −.25 for needs
to be done. In addition, mean SeeTweet estimates
of needs doing usage were lower for regions where
respondents use needs done than for regions where
they do not: −.93 versus −.49.12 Thus, SeeTweet
provides evidence that needs done and needs do-
ing are in a geographically distinct distribution,
while needs done and needs to be done are at most
weakly distinct.
</bodyText>
<footnote confidence="0.5673125">
12SeeTweet estimates of needs to be done usage were com-
parable in both regions, −.018 against .019.
</footnote>
<page confidence="0.994094">
104
</page>
<figure confidence="0.994906">
(a) “Needs done” distribution
</figure>
<figureCaption confidence="0.9671595">
Figure 4: SeeTweet distributions for needs done,
needs to be done, and needs doing.
</figureCaption>
<sectionHeader confidence="0.650585" genericHeader="method">
5 The appropriateness of Twitter as a
data source
</sectionHeader>
<bodyText confidence="0.999994714285714">
A possible concern with this analysis is that Twit-
ter could be a biased and noisy dataset, inappropri-
ate for sociolinguistic investigation. Twitter skews
toward the young and slightly toward urbanites
(Duggan and Brenner, 2013). However, as young
urbanites tend to drive language change (Labov
et al., 2008), any such bias would make the re-
sults more useful for examining sociolinguistic
changes and emergent forms. The informality of
the medium also provides unedited writing data
that is more reflective of non-standard usage than
most corpora, and its large amounts of data in short
timescales offers new abilities to track emerging
linguistic change.
As for noise in the tweet data and locations, the
strong correlations between the gold-standard and
SeeTweet results show that, at least for these fea-
tures, the noise is mitigated by the size of dataset.
We examined the impact of noise on the needs
done dataset by manually inspecting the data for
false positives and re-mapping the clean data. Al-
though the false positive rate was 12%, the con-
ditional distribution learned with and without the
false positives removed remained tightly corre-
lated, at R2 = .94. The SeeTweet method ap-
pears to be robust to false positives, although nois-
ier queries may require manual inspection.
A final point to note is that while the datasets
used in constructing these maps are relatively
small, they are crucially derived from big data. Be-
cause the needs done and double modal construc-
tions are quite rare, there would be very few ex-
amples in a standard-sized corpus. Only because
there are so many tweets are we able to get the
hundreds of examples we used in this study.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976666666667">
We have shown that Bayesian inversion can be
used to build conditional probability distributions
over data given metadata from the results of
queries on social media, connecting query-derived
data to traditional data sources. Tests on Twitter
show that such calculations can provide dialect
geographies that are well correlated with exist-
ing gold-standard sources at a fraction of the time,
cost, and effort.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998694">
We wish to thank Roger Levy, Dan Michel, Emily
Morgan, Mark Mysl´ın, Bill Presant, Agatha Ven-
tura, and the reviewers for their advice, sugges-
tions, and testing. This work was supported in part
by NSF award 0830535.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998109578947368">
Gabriel Doyle and Roger Levy. 2008. Environment
prototypicality in syntactic alternation. In Proceed-
ings of the 34th Annual Meeting of the Berkeley Lin-
guistics Society.
Maeve Duggan and Joanna Brenner. 2013. The demo-
graphics of social media users – 2012. Pew Internet
and American Life Project.
J. Daniel Hasty. 2011. I might would not say that:
A sociolinguistic study of double modal acceptance.
In University of Pennsylvania Working Papers in
Linguistics, volume 17.
Joshua Katz. 2013. Beyond “soda, pop, or
coke”: Regional dialect variation in the continental
US. Retrieved from http://www4.ncsu.edu/
˜jakatz2/project-dialect.html.
William Labov, Sharon Ash, and Charles Boberg.
2008. The Atlas of North American English. Pho-
netics, Phonology, and Sound Change. de Gruyter
Mouton.
</reference>
<figure confidence="0.9778715">
(b) “Needs doing” distribution
(c) “Needs to be done” distribution
</figure>
<page confidence="0.987352">
105
</page>
<reference confidence="0.999351611111111">
Thomas Murray, Timothy Frazer, and Beth Lee Simon.
1996. Need + past participle in American English.
American Speech, 71:255–271.
Daisuke Okanohara and Jun’ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Joshua Tenenbaum and Thomas Griffiths. 2001. Gen-
eralization, similiarity, and Bayesian inference. Be-
havioral and Brain Sciences, 24:629–640.
Bert Vaux and Scott Golder. 2003. Harvard dialect
survey. Available at http://www4.uwm.edu/
FLL/linguistics/dialect/index.html.
</reference>
<page confidence="0.997317">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928477">
<title confidence="0.997255">Mapping dialectal variation by querying social media</title>
<author confidence="0.998606">Gabriel</author>
<affiliation confidence="0.9992405">Department of University of California, San</affiliation>
<address confidence="0.995727">La Jolla, CA, USA</address>
<email confidence="0.999864">gdoyle@ucsd.edu</email>
<abstract confidence="0.9955963125">We propose a Bayesian method of estimating a conditional distribution of data given metadata (e.g., the usage of a dialectal variant given a location) based on queries from a big data/social media source, such as Twitter. This distribution is structurally equivalent to those built from traditional experimental methods, despite lacking negative examples. Tests using Twitter to investigate the geographic distribution of dialectal forms show that this method can provide distributions that are tightly correlated with existing gold-standard studies at a fraction of the time, cost, and effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
<author>Roger Levy</author>
</authors>
<title>Environment prototypicality in syntactic alternation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Berkeley Linguistics Society.</booktitle>
<contexts>
<context position="19002" citStr="Doyle and Levy, 2008" startWordPosition="3051" endWordPosition="3054">ive tweets and 32275 baseline tweets.8 The component distributions p(M|d) and p(M) are estimated by Gaussian kernels with bandwidth 3. The log of ˜p(f|M), calculated as in Eq. 1, determines the color of a region; orange indicates a higher value, purple a middle (approx. 1) value, and blue a low value. Confidence in the estimate is reflected by opacity; higher opacity indicates higher confidence in the estimate. Confidence values above 3 (corresponding to 9 tweets per bin) are 8The verb do was used as it was found to be the most common verb in corpus work on needs to be [verbed] constructions (Doyle and Levy, 2008), appearing almost three times as often as the second-most common verb (replace). fully opaque. This description holds for all other maps in this paper. We start with a qualitative comparison of the maps. Both maps show the construction to be most prominent in the area between the Plains states and central Pennsylvania (the North Midland dialect region), with minimal use in New England and Northern California and limited use elsewhere. SeeTweet lacks data in the Mountain West and Great Plains, and ANAE lacks data for Minnesota and surrounding states.9 The most notable deviation between the map</context>
</contexts>
<marker>Doyle, Levy, 2008</marker>
<rawString>Gabriel Doyle and Roger Levy. 2008. Environment prototypicality in syntactic alternation. In Proceedings of the 34th Annual Meeting of the Berkeley Linguistics Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maeve Duggan</author>
<author>Joanna Brenner</author>
</authors>
<title>The demographics of social media users –</title>
<date>2013</date>
<booktitle>Pew Internet and American Life Project.</booktitle>
<contexts>
<context position="29012" citStr="Duggan and Brenner, 2013" startWordPosition="4787" endWordPosition="4790">and needs doing are in a geographically distinct distribution, while needs done and needs to be done are at most weakly distinct. 12SeeTweet estimates of needs to be done usage were comparable in both regions, −.018 against .019. 104 (a) “Needs done” distribution Figure 4: SeeTweet distributions for needs done, needs to be done, and needs doing. 5 The appropriateness of Twitter as a data source A possible concern with this analysis is that Twitter could be a biased and noisy dataset, inappropriate for sociolinguistic investigation. Twitter skews toward the young and slightly toward urbanites (Duggan and Brenner, 2013). However, as young urbanites tend to drive language change (Labov et al., 2008), any such bias would make the results more useful for examining sociolinguistic changes and emergent forms. The informality of the medium also provides unedited writing data that is more reflective of non-standard usage than most corpora, and its large amounts of data in short timescales offers new abilities to track emerging linguistic change. As for noise in the tweet data and locations, the strong correlations between the gold-standard and SeeTweet results show that, at least for these features, the noise is mi</context>
</contexts>
<marker>Duggan, Brenner, 2013</marker>
<rawString>Maeve Duggan and Joanna Brenner. 2013. The demographics of social media users – 2012. Pew Internet and American Life Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daniel Hasty</author>
</authors>
<title>I might would not say that: A sociolinguistic study of double modal acceptance.</title>
<date>2011</date>
<booktitle>In University of Pennsylvania Working Papers in Linguistics,</booktitle>
<volume>17</volume>
<contexts>
<context position="23513" citStr="Hasty, 2011" startWordPosition="3825" endWordPosition="3826">ibution to correct for different baseline frequencies in tweeting. This second point will be examined further with double modals in the next section. 4.2 Double modals and the importance of the baseline The double modal construction provides a second test case. While ungrammatical in Standard American English, forms like I might could use your help are grammatical and common in Southern American dialects. This construction is interesting both for its theoretical syntax implications on the nature of modals as well as the relationship between its sociolinguistic distribution and its pragmatics (Hasty, 2011). The ANAE does not have data on double modals’ distribution, but another large-scale sociolinguistic experiment does: the Harvard Dialect Survey (Vaux and Golder, 2003). This online survey obtained 30788 responses to 122 dialect questions, including the use of double modals. Katz (2013) used a nearest-neighbor model to create a p(djM) distribution over the contiguous U.S. for double modal usage, mapped in Fig. 3a.11 Lighter colors indicate higher rates of double modal acceptance. SeeTweet generates a similar map (Fig. 3b), based on three searches with 928 positive and 66272 baseline tweets. A</context>
</contexts>
<marker>Hasty, 2011</marker>
<rawString>J. Daniel Hasty. 2011. I might would not say that: A sociolinguistic study of double modal acceptance. In University of Pennsylvania Working Papers in Linguistics, volume 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Katz</author>
</authors>
<title>Beyond “soda, pop, or coke”: Regional dialect variation in the continental US.</title>
<date>2013</date>
<note>Retrieved from http://www4.ncsu.edu/ ˜jakatz2/project-dialect.html.</note>
<contexts>
<context position="23801" citStr="Katz (2013)" startWordPosition="3870" endWordPosition="3871">dard American English, forms like I might could use your help are grammatical and common in Southern American dialects. This construction is interesting both for its theoretical syntax implications on the nature of modals as well as the relationship between its sociolinguistic distribution and its pragmatics (Hasty, 2011). The ANAE does not have data on double modals’ distribution, but another large-scale sociolinguistic experiment does: the Harvard Dialect Survey (Vaux and Golder, 2003). This online survey obtained 30788 responses to 122 dialect questions, including the use of double modals. Katz (2013) used a nearest-neighbor model to create a p(djM) distribution over the contiguous U.S. for double modal usage, mapped in Fig. 3a.11 Lighter colors indicate higher rates of double modal acceptance. SeeTweet generates a similar map (Fig. 3b), based on three searches with 928 positive and 66272 baseline tweets. As with the ANAE test, the 10We remove all respondents who do not use the construction but report it in their area. Such respondents are fairly rare (slightly over 10% of the population), and removing this response converts the data to a binary classification problem appropriate to Hosmer</context>
</contexts>
<marker>Katz, 2013</marker>
<rawString>Joshua Katz. 2013. Beyond “soda, pop, or coke”: Regional dialect variation in the continental US. Retrieved from http://www4.ncsu.edu/ ˜jakatz2/project-dialect.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
<author>Sharon Ash</author>
<author>Charles Boberg</author>
</authors>
<date>2008</date>
<journal>The Atlas of North American English. Phonetics, Phonology, and Sound Change. de Gruyter Mouton.</journal>
<contexts>
<context position="9761" citStr="Labov et al., 2008" startWordPosition="1558" endWordPosition="1561">s section on a case study of the geographic distributions of linguistic forms, calculated from recent tweets. It is implemented as a suite of novel open-source Python/R programs called SeeTweet, which queries Twitter, obtains tweet locations, performs the mathematical analysis, and maps the results. The suite is available at http://github.com/gabedoyle/ seetweet. 3.1 SeeTweet goals Traditionally, sociolinguistic studies are highly time-intensive, and broad coverage is difficult to obtain at reasonable costs. Two data sources that we compare SeeTweet to are the Atlas of North American English (Labov et al., 2008, ANAE) and the Harvard Dialect Survey (Vaux and Golder, 2003, HDS), both of which obtained high-quality data, but over the course of years. Such studies remain the gold-standard for most purposes, but SeeTweet presents a rapid, cheap, and surprisingly effective alternative for broad coverage on some problems in dialect geography. 3.2 Querying Twitter SeeTweet queries Twitter through its API, using Mike Verdone’s Python Twitter Tools3. The API returns the 1000 most recent query-matching tweets or all query-matching tweets within the 3http://mike.verdone.ca/twitter/ last week, whichever is smal</context>
<context position="16689" citStr="Labov et al., 2008" startWordPosition="2662" endWordPosition="2665"> dialect geography; for this, we compare SeeTweet distributions of the needs done construction to those found by long-term sociolinguistic studies and show that the quick-and-dirty unsupervised SeeTweet distributions are accurate reflections of the slow-and-clean results. Our second goal is show the importance of using the correct conditional distribution, by comparing it to the unadjusted distribution. With these points established, we then use SeeTweet to create maps of previously uninvestigated problems. 4.1 Method verification on need + past participle The Atlas of North American English (Labov et al., 2008) is the most complete linguistic atlas of American English dialect geography. It focuses on phonological variation, but also includes a small set of lexical/syntactic alternations. One is the needs + past participle construction, as in The car needs (to be) washed. This construction has a limited geographic distribution, and ANAE provides the first nationwide survey of its usage. We compare SeeTweet’s conditional probabilities for this construction to the ANAE responses to see how the relatively uncontrolled Twitter source compares to the tightly controlled telephone survey data that ANAE repo</context>
<context position="29092" citStr="Labov et al., 2008" startWordPosition="4800" endWordPosition="4803">eds to be done are at most weakly distinct. 12SeeTweet estimates of needs to be done usage were comparable in both regions, −.018 against .019. 104 (a) “Needs done” distribution Figure 4: SeeTweet distributions for needs done, needs to be done, and needs doing. 5 The appropriateness of Twitter as a data source A possible concern with this analysis is that Twitter could be a biased and noisy dataset, inappropriate for sociolinguistic investigation. Twitter skews toward the young and slightly toward urbanites (Duggan and Brenner, 2013). However, as young urbanites tend to drive language change (Labov et al., 2008), any such bias would make the results more useful for examining sociolinguistic changes and emergent forms. The informality of the medium also provides unedited writing data that is more reflective of non-standard usage than most corpora, and its large amounts of data in short timescales offers new abilities to track emerging linguistic change. As for noise in the tweet data and locations, the strong correlations between the gold-standard and SeeTweet results show that, at least for these features, the noise is mitigated by the size of dataset. We examined the impact of noise on the needs don</context>
</contexts>
<marker>Labov, Ash, Boberg, 2008</marker>
<rawString>William Labov, Sharon Ash, and Charles Boberg. 2008. The Atlas of North American English. Phonetics, Phonology, and Sound Change. de Gruyter Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Murray</author>
<author>Timothy Frazer</author>
<author>Beth Lee Simon</author>
</authors>
<title>Need + past participle in American English.</title>
<date>1996</date>
<pages>71--255</pages>
<publisher>American Speech,</publisher>
<contexts>
<context position="21297" citStr="Murray et al. (1996)" startWordPosition="3419" endWordPosition="3422"> probability. Non-users who are unfamiliar with it tend to live in regions with the lowest conditional probabilities of the three groups. This shows the expected correspondence trend between the ANAE responses and the estimated prevalence of the construction in an area; the mean SeeTweet estimates for the three groups are 0.45, −0.34, and −0.61, respectively. The second comparison (Fig. 2b) is a HosmerLemeshow plot. The respondents are first divided into deciles based on the SeeTweet estimate at their location. Two mean values are calculated for each decile: the mean SeeTweet log-probability 9Murray et al. (1996)’s data suggest that these untested areas would not use the construction; the SeeTweet data suggests this as well. Latitude 40 35 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ANAE/Telsur Response Data a.,. •. ● Used by Respondent Familiar to Respondent Unfamiliar to Respondent 102 (a) Violin plot of SeeTweet estimated conditional probability against ANAE response type. −2 −1 0 log proportion accepting needs+ppt [ANAE] (b) Hosmer-Lemeshow pl</context>
<context position="27006" citStr="Murray et al. (1996)" startWordPosition="4455" endWordPosition="4458">s is not incorrect, per se, as these are the sources of many double modal tweets; but these peaks are incidental, as major cities produce more tweets than the rest of the country. This is confirmed by their absence in the HDS map as well as the appropriate SeeTweet map. 4.3 Extending SeeTweet to new problems Given SeeTweet’s success in mapping needs done and double modals, it can also be used to test new questions. An understudied issue in past work on the need + past participle construction is its relationship with alternative forms need to be + past participle and need + present participle. Murray et al. (1996) suggest that their need + past participle users reject both alternatives, although it is worth noting that their informants are more accepting of the to be alternative, calling it merely “too formal”, as opposed to an “odd” or “ungrammatical” opinion about the present participle form. Their analysis of the opinions on alternative forms does not go beyond this anecdotal evidence. SeeTweet provides the opportunity to examine this issue, and finds that the to be form is persistent across the country (Fig. 4c), both in areas with and without the need + past participle form, whereas the present pa</context>
</contexts>
<marker>Murray, Frazer, Simon, 1996</marker>
<rawString>Thomas Murray, Timothy Frazer, and Beth Lee Simon. 1996. Need + past participle in American English. American Speech, 71:255–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative samples.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="4628" citStr="Okanohara and Tsujii, 2007" startWordPosition="710" endWordPosition="713">ve examples. This lack of explicit negative data is common in language acquisition, as children encounter mostly grammatical statements during learning, and receive few explicitly ungrammatical examples, yet still develop a consistent grammaticality classification system as they mature. Similar positive-only problems abound in cognitive science and artificial intelligence, and a variety of proposals have been offered to overcome it in different tasks. These include biases like the Size Principle (Tenenbaum and Griffiths, 2001), heuristics like generating pseudo-negatives from unobserved data (Okanohara and Tsujii, 2007; Poon et al., 2009), or innate prespecifications like Universal Grammar in the Principles and Parameters framework. For query-derived data, Bayesian reasoning can address both problems by inverting the conditionality of the distribution and implying negative data. The key insight is that a lack of positive examples where positive examples are otherwise expected is implicit negative evidence. This method allows a researcher to produce an estimated distribution that approximates the true conditional distribution up to a normalizing factor. This conditional distribution is that of data (e.g., a </context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudo-negative samples. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4648" citStr="Poon et al., 2009" startWordPosition="714" endWordPosition="717">plicit negative data is common in language acquisition, as children encounter mostly grammatical statements during learning, and receive few explicitly ungrammatical examples, yet still develop a consistent grammaticality classification system as they mature. Similar positive-only problems abound in cognitive science and artificial intelligence, and a variety of proposals have been offered to overcome it in different tasks. These include biases like the Size Principle (Tenenbaum and Griffiths, 2001), heuristics like generating pseudo-negatives from unobserved data (Okanohara and Tsujii, 2007; Poon et al., 2009), or innate prespecifications like Universal Grammar in the Principles and Parameters framework. For query-derived data, Bayesian reasoning can address both problems by inverting the conditionality of the distribution and implying negative data. The key insight is that a lack of positive examples where positive examples are otherwise expected is implicit negative evidence. This method allows a researcher to produce an estimated distribution that approximates the true conditional distribution up to a normalizing factor. This conditional distribution is that of data (e.g., a dialectal form) cond</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Tenenbaum</author>
<author>Thomas Griffiths</author>
</authors>
<date>2001</date>
<booktitle>Generalization, similiarity, and Bayesian inference. Behavioral and Brain Sciences,</booktitle>
<pages>24--629</pages>
<contexts>
<context position="4534" citStr="Tenenbaum and Griffiths, 2001" startWordPosition="697" endWordPosition="701">m query-derived and traditional data. Queries also complicate the problem by providing only positive examples. This lack of explicit negative data is common in language acquisition, as children encounter mostly grammatical statements during learning, and receive few explicitly ungrammatical examples, yet still develop a consistent grammaticality classification system as they mature. Similar positive-only problems abound in cognitive science and artificial intelligence, and a variety of proposals have been offered to overcome it in different tasks. These include biases like the Size Principle (Tenenbaum and Griffiths, 2001), heuristics like generating pseudo-negatives from unobserved data (Okanohara and Tsujii, 2007; Poon et al., 2009), or innate prespecifications like Universal Grammar in the Principles and Parameters framework. For query-derived data, Bayesian reasoning can address both problems by inverting the conditionality of the distribution and implying negative data. The key insight is that a lack of positive examples where positive examples are otherwise expected is implicit negative evidence. This method allows a researcher to produce an estimated distribution that approximates the true conditional di</context>
</contexts>
<marker>Tenenbaum, Griffiths, 2001</marker>
<rawString>Joshua Tenenbaum and Thomas Griffiths. 2001. Generalization, similiarity, and Bayesian inference. Behavioral and Brain Sciences, 24:629–640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert Vaux</author>
<author>Scott Golder</author>
</authors>
<date>2003</date>
<note>Harvard dialect survey. Available at http://www4.uwm.edu/ FLL/linguistics/dialect/index.html.</note>
<contexts>
<context position="9822" citStr="Vaux and Golder, 2003" startWordPosition="1568" endWordPosition="1571">of linguistic forms, calculated from recent tweets. It is implemented as a suite of novel open-source Python/R programs called SeeTweet, which queries Twitter, obtains tweet locations, performs the mathematical analysis, and maps the results. The suite is available at http://github.com/gabedoyle/ seetweet. 3.1 SeeTweet goals Traditionally, sociolinguistic studies are highly time-intensive, and broad coverage is difficult to obtain at reasonable costs. Two data sources that we compare SeeTweet to are the Atlas of North American English (Labov et al., 2008, ANAE) and the Harvard Dialect Survey (Vaux and Golder, 2003, HDS), both of which obtained high-quality data, but over the course of years. Such studies remain the gold-standard for most purposes, but SeeTweet presents a rapid, cheap, and surprisingly effective alternative for broad coverage on some problems in dialect geography. 3.2 Querying Twitter SeeTweet queries Twitter through its API, using Mike Verdone’s Python Twitter Tools3. The API returns the 1000 most recent query-matching tweets or all query-matching tweets within the 3http://mike.verdone.ca/twitter/ last week, whichever is smaller, and can be geographically limited to tweets within a cer</context>
<context position="23682" citStr="Vaux and Golder, 2003" startWordPosition="3848" endWordPosition="3851">e modals and the importance of the baseline The double modal construction provides a second test case. While ungrammatical in Standard American English, forms like I might could use your help are grammatical and common in Southern American dialects. This construction is interesting both for its theoretical syntax implications on the nature of modals as well as the relationship between its sociolinguistic distribution and its pragmatics (Hasty, 2011). The ANAE does not have data on double modals’ distribution, but another large-scale sociolinguistic experiment does: the Harvard Dialect Survey (Vaux and Golder, 2003). This online survey obtained 30788 responses to 122 dialect questions, including the use of double modals. Katz (2013) used a nearest-neighbor model to create a p(djM) distribution over the contiguous U.S. for double modal usage, mapped in Fig. 3a.11 Lighter colors indicate higher rates of double modal acceptance. SeeTweet generates a similar map (Fig. 3b), based on three searches with 928 positive and 66272 baseline tweets. As with the ANAE test, the 10We remove all respondents who do not use the construction but report it in their area. Such respondents are fairly rare (slightly over 10% of</context>
</contexts>
<marker>Vaux, Golder, 2003</marker>
<rawString>Bert Vaux and Scott Golder. 2003. Harvard dialect survey. Available at http://www4.uwm.edu/ FLL/linguistics/dialect/index.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>