<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002264">
<title confidence="0.9955895">
A Semi-Supervised Key Phrase Extraction Approach: Learning from
Title Phrases through a Document Semantic Network
</title>
<author confidence="0.999536">
Decong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu3
</author>
<affiliation confidence="0.998683">
1Key Laboratory of Computational Linguistics, Peking University
2Department of Computing, The Hong Kong Polytechnic University
3 School of Computer Science and Technology, Nanjing Normal University
</affiliation>
<email confidence="0.992618">
{lidecong,lisujian, wwei }@pku.edu.cn cswjli@comp.polyu.edu.hk wgqu@njnu.edu.cn
</email>
<sectionHeader confidence="0.997305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999480363636364">
It is a fundamental and important task to ex-
tract key phrases from documents. Generally,
phrases in a document are not independent in
delivering the content of the document. In or-
der to capture and make better use of their re-
lationships in key phrase extraction, we sug-
gest exploring the Wikipedia knowledge to
model a document as a semantic network,
where both n-ary and binary relationships
among phrases are formulated. Based on a
commonly accepted assumption that the title
of a document is always elaborated to reflect
the content of a document and consequently
key phrases tend to have close semantics to the
title, we propose a novel semi-supervised key
phrase extraction approach in this paper by
computing the phrase importance in the se-
mantic network, through which the influence
of title phrases is propagated to the other
phrases iteratively. Experimental results dem-
onstrate the remarkable performance of this
approach.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931827586207">
Key phrases are defined as the phrases that ex-
press the main content of a document. Guided by
the given key phrases, people can easily under-
stand what a document describes, saving a great
amount of time reading the whole text. Conse-
quently, automatic key phrase extraction is in
high demand. Meanwhile, it is also fundamental
to many other natural language processing appli-
cations, such as information retrieval, text clus-
tering and so on.
Key phrase extraction can be normally cast as
a ranking problem solved by either supervised or
unsupervised methods. Supervised learning re-
quires a large amount of expensive training data,
whereas unsupervised learning totally ignores
human knowledge. To overcome the deficiencies
of these two kinds of methods, we propose a
novel semi-supervised key phrase extraction ap-
proach in this paper, which explores title phrases
as the source of knowledge.
It is well agreed that the title has a similar role
to the key phrases. They are both elaborated to
reflect the content of a document. Therefore,
phrases in the titles are often appropriate to be
key phrases. That is why position has been a
quite effective feature in the feature-based key
phrase extraction methods (Witten, 1999), i.e., if
a phrase is located in the title, it is ranked higher.
However, one can only include a couple of
most important phrases in the title prudently due
to the limitation of the title length, even though
many other key phrases are all pivotal to the un-
derstanding of the document. For example, when
we read the title “China Tightens Grip on the
Web”, we can only have a glimpse of what the
document says. On the other hand, the key
phrases, such as “China”, “Censorship”, “Web”,
“Domain name”, “Internet”, and “CNNIC”, etc.
can tell more details about the main topics of the
document. In this regard, title phrases are often
good key phrases but they are far from enough.
If we review the above example again, we will
find that the key phrase “Internet” can be in-
ferred from the title phrase “Web”. As a matter
of fact, key phrases often have close semantics to
title phrases. Then a question comes to our minds:
can we make use of these title phrases to infer
the other key phrases?
To provide a foundation of inference, a seman-
tic network that captures the relationships among
phrases is required. In the previous works (Tur-
dakov and Velikhov, 2008), semantic networks
are constructed based on the binary relations, and
the semantic relatedness between a pair of phras-
es is formulated by the weighted edges that con-
nects them. The deficiency of these approaches is
the incapability to capture the n-ary relations
among multiple phrases. For example, a group of
</bodyText>
<page confidence="0.976141">
296
</page>
<note confidence="0.5117905">
Proceedings of the ACL 2010 Conference Short Papers, pages 296–300,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997024206896552">
phrases may collectively describe an entity or an
event.
In this study, we propose to model a semantic
network as a hyper-graph, where vertices
represent phrases and weighted hyper-edges
measure the semantic relatedness of both binary
relations and n-ary relations among phrases. We
explore a universal knowledge base – Wikipedia
– to compute the semantic relatedness. Yet our
major contribution is to develop a novel semi-
supervised key phrase extraction approach by
computing the phrase importance in the semantic
network, through which the influence of title
phrases is propagated to the other phrases itera-
tively.
The goal of the semi-supervised learning is to
design a function that is sufficiently smooth with
respect to the intrinsic structure revealed by title
phrases and other phrases. Based on the assump-
tion that semantically related phrases are likely
to have similar scores, the function to be esti-
mated is required to assign title phrases a higher
score and meanwhile locally smooth on the con-
structed hyper-graph. Zhou et al.’s work (Zhou
2005) lays down a foundation for our semi-
supervised phrase ranking algorithm introduced
in Section 3. Experimental results presented in
Section 4 demonstrate the effectiveness of this
approach.
</bodyText>
<sectionHeader confidence="0.983359" genericHeader="method">
2 Wikipedia-based Semantic Network
Construction
</sectionHeader>
<bodyText confidence="0.99998575">
Wikipedia1 is a free online encyclopedia, which
has unarguably become the world’s largest col-
lection of encyclopedic knowledge. Articles are
the basic entries in the Wikipedia, with each ar-
ticle explaining one Wikipedia term. Articles
contain links pointing from one article to another.
Currently, there are over 3 million articles and 90
million links in English Wikipedia. In addition to
providing a large vocabulary, Wikipedia articles
also contain a rich body of lexical semantic in-
formation expressed via the extensive number of
links. During recent years, Wikipedia has been
used as a powerful tool to compute semantic re-
latedness between terms in a good few of works
(Turdakov 2008).
We consider a document composed of the
phrases that describe various aspects of entities
or events with different semantic relationships.
We then model a document as a semantic net-
work formulated by a weighted hyper-graph
</bodyText>
<footnote confidence="0.818367">
1 www.wikipedia.org
</footnote>
<bodyText confidence="0.999963735294118">
G=(V, E, W), where each vertex viEV (1&lt;_i&lt;_n)
represents a phrase, each hyper-edge ej�E
(1&lt;_j&lt;_m) is a subset of V, representing binary re-
lations or n-ary relations among phrases, and the
weight w(ej) measures the semantic relatedness
of ej.
By applying the WSD technique proposed by
(Turdakov and Velikhov, 2008), each phrase is
assigned with a single Wikipedia article that de-
scribes its meaning. Intuitively, if the fraction of
the links that the two articles have in common to
the total number of the links in both articles is
high, the two phrases corresponding to the two
articles are more semantically related. Also, an
article contains different types of links, which are
relevant to the computation of semantic related-
ness to different extent. Hence we adopt the
weighted Dice metric proposed by (Turdakov
2008) to compute the semantic relatedness of
each binary relation, resulting in the edge weight
w(eij), where eij is an edge connecting the phrases
vi and vj.
To define the n-ary relations in the semantic
network, a proper graph clustering technique is
needed. We adopt the weighted Girvan-Newman
algorithm (Newman 2004) to cluster phrases (in-
cluding title phrases) by computing their bet-
weenness centrality. The advantage of this algo-
rithm is that it need not specify a pre-defined
number of clusters. Then the phrases, within
each cluster, are connected by a n-ary relation. n-
ary relations among the phrases in the same clus-
ter are then measured based on binary relations.
The weight of a hyper-edge e is defined as:
</bodyText>
<equation confidence="0.997836">
w(e) ¦ w(eij)
e (1)
</equation>
<bodyText confidence="0.99994175">
where |e |is the number of the vertices in e, eij is
an edge with two vertices included in e and a ≥ 0
is a parameter balancing the relative importance
of n-ary hyper-edges compared with binary ones.
</bodyText>
<sectionHeader confidence="0.983799" genericHeader="method">
3 Semi-supervised Learning from Title
</sectionHeader>
<bodyText confidence="0.999974636363636">
Given the document semantic network
represented as a phrase hyper-graph, one way to
make better use of the semantic information is to
rank phrases with a semi-supervised learning
strategy, where the title phrases are regarded as
labeled samples, while the other phrases as unla-
beled ones. That is, the information we have at
the beginning about how to rank phrases is that
the title phrases are the most important phrases.
Initially, the title phrases are assigned with a pos-
itive score of 1 indicating its importance and oth-
</bodyText>
<page confidence="0.982828">
297
</page>
<bodyText confidence="0.999852625">
er phrases are assigned zero. Then the impor-
tance scores of the phrases are learned iteratively
from the title phrases through the hyper-graph.
The key idea behind hyper-graph based semi-
supervised ranking is that the vertices which
usually belong to the same hyper-edges should
be assigned with similar scores. Then, we have
the following two constraints:
</bodyText>
<listItem confidence="0.8252038">
1. The phrases which have many incident hy-
per-edges in common should be assigned similar
scores.
2. The given initial scores of the title phrases
should be changed as little as possible.
</listItem>
<bodyText confidence="0.996388315789474">
Given a weighted hyper-graph G, assume a
ranking function f over V, which assigns each
vertex v an importance score f(v). f can be
thought as a vector in Euclid space R|V|. For the
convenience of computation, we use an inci-
dence matrix H to represent the hypergraph, de-
fined as:
Based on the incidence matrix, we define the
degrees of the vertex v and the hyper-edge e as
and
Then, to formulate the above-mentioned con-
straints, let denote the initial score vector, then
the importance scores of the phrases are learned
iteratively by solving the following optimization
problem:
.
Using an approximation algorithm (e.g. Algo-
rithm 1), we can finally get a vector f
representing the approximate phrase scores.
</bodyText>
<construct confidence="0.73920175">
Algorithm 1: PhraseRank(V, T, a, b)
Input: Title phrase set = {v1,v2,...,vt},the set of other
phrases ={vt+1,vt+2,...,vn}, parameters a and 8, con-
vergence threshold �,
</construct>
<bodyText confidence="0.859491153846154">
Output: The approximate phrase scores f
Construct a document semantic network for all the
phrases {v1,v2,...,vn} using the method described in
section 2.
Let ;
Initialize the score vector y as , and
;
Finally we rank phrases in descending order of
the calculated importance scores and select those
highest ranked phrases as key phrases. Accord-
ing to the number of all the candidate phrases,
we choose an appropriate proportion, i.e. 10%, of
all the phrases as key phrases.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99545">
4.1 Experiment Set-up
</subsectionHeader>
<bodyText confidence="0.999974">
We first collect all the Wikipedia terms to com-
pose of a dictionary. The word sequences that
occur in the dictionary are identified as phrases.
Here we use a finite-state automaton to accom-
plish this task to avoid the imprecision of pre-
processing by POS tagging or chunking. Then,
we adopt the WSD technique proposed by (Tur-
dakov and Velikhov 2008) to find the corres-
ponding Wikipedia article for each phrase. As
mentioned in Section 2, a document semantic
network in the form of a hyper-graph is con-
structed, on which Algorithm 1 is applied to rank
the phrases.
To evaluate our proposed approach, we select
200 pieces of news from well-known English
media. 5 to 10 key phrases are manually labeled
in each news document and the average number
of the key phrases is 7.2 per document. Due to
the abbreviation and synonymy phenomena, we
construct a thesaurus and convert all manual and
automatic phrases into their canonical forms
when evaluated. The traditional Recall, Precision
and F1-measure metrics are adopted for evalua-
tion. This section conducts two sets of experi-
ment: (1) to examine the influence of two para-
meters: a and 8, on the key phrase extraction
performance; (2) to compare with other well
known state-of-art key phrase extraction ap-
proaches.
</bodyText>
<equation confidence="0.697281444444444">
Let , k = 0;
REPEAT
;
, ;
;
UNTIL
END
C f (u) _ f(v) 1 (6)
n(f)= 2E�E8(e)E{u,v}�w(e) d(u) d(v) )
</equation>
<bodyText confidence="0.998530875">
where a&gt; 0 is the parameter specifying the
tradeoff between the two competitive items. Let
Dv and De denote the diagonal matrices contain-
ing the vertex and the hyper-edge degrees re-
spectively, W denote the diagonal matrix con-
taining the hyper-edge weights, f* denote the so-
lution of (6). Zhou has given the solution (Zhou,
2005) as.
</bodyText>
<equation confidence="0.738325">
f&apos; = flof&apos; + (1- fl )y (7)
</equation>
<bodyText confidence="0.895005">
where O = Dv 12HWDe &apos;HT Dv 12 and fi 1 / (ft  1)
</bodyText>
<page confidence="0.777678">
2
298
</page>
<subsectionHeader confidence="0.9866">
4.2 Parameter tuning
</subsectionHeader>
<bodyText confidence="0.999904916666667">
The approach involves two parameters:  (0)
is a relation factor balancing the influence of n-
ary relations and binary relations;  (01) is a
learning factor tuning the influence from the title
phrases. It is hard to find a global optimized so-
lution for the combination of these two factors.
So we apply a gradient search strategy. At first,
the learning factor is set to =0.8. Different val-
ues of  ranging from 0 to 3 are examined. Then,
given that  is set to the value with the best per-
formance, we conduct experiments to find an
appropriate value for .
</bodyText>
<subsubsectionHeader confidence="0.737513">
4.2.1 : Relation Factor
</subsubsectionHeader>
<bodyText confidence="0.8732263">
First, we fix the learning factor  as 0.8 random-
ly and evaluate the performance by varying 
value from 0 to 3. When =0, it means that the
weight of n-ary relations is zero and only binary
relations are considered. As we can see from
Figure 1, the performance is improved in most
cases in terms of F1-measure and reaches a peak
at =1.8. This justifies the rational to incorpo-
rate n-ary relations with binary relations in the
document semantic network.
</bodyText>
<figureCaption confidence="0.998634">
Figure 1. F1-measures with  in [0 3]
</figureCaption>
<subsubsectionHeader confidence="0.570486">
4.2.2 : Learning factor
</subsubsectionHeader>
<bodyText confidence="0.9993948">
Next, we set the relation factor =1.8, we in-
spect the performance with the learning factor 
ranging from 0 to 1. =1 means that the ranking
scores learn from the semantic network without
any consideration of title phrases. As shown in
Figure 2, we find that the performance almost
keep a smooth fluctuation as  increases from 0
to 0.9, and then a diving when =1. This proves
that title phrases indeed provide valuable infor-
mation for learning.
</bodyText>
<figureCaption confidence="0.996925">
Figure 2. F1-measure with  in [0,1]
</figureCaption>
<subsectionHeader confidence="0.99595">
4.3 Comparison with Other Approaches
</subsectionHeader>
<bodyText confidence="0.99997088">
Our approach aims at inferring important key
phrases from title phrases through a semantic
network. Here we take a method of synonym
expansion as the baseline, called WordNet ex-
pansion here. The WordNet2 expansion approach
selects all the synonyms of the title phrases in the
document as key phrases. Afterwards, our ap-
proach is evaluated against two existing ap-
proaches, which rely on the conventional seman-
tic network and are able to capture binary rela-
tions only. One approach combines the title in-
formation into the Grineva’s community-based
method (Grineva et al., 2009), called title-
community approach. The title-community ap-
proach uses the Girvan-Newman algorithm to
cluster phrases into communities and selects
those phrases in the communities containing the
title phrases as key phrases. We do not limit the
number of key phrases selected. The other one is
based on topic-sensitive LexRank (Otterbacher et
al., 2005), called title-sensitive PageRank here.
The title-sensitive PageRank approach makes use
of title phrases to re-weight the transitions be-
tween vertices and picks up 10% top-ranked
phrases as key phrases.
</bodyText>
<table confidence="0.99983075">
Approach Precision Recall F1
Title-sensitive Pa- 34.8% 39.5% 37.0%
geRank (d=0.15)
Title-community 29.8% 56.9% 39.1%
Our approach a o 8 a
(a=1.8, =0.5) 39.4 /0 44.6 /0 41./o
WordNet expansion 7.9% 32.9% 12.5%
(baseline)
</table>
<tableCaption confidence="0.999994">
Table 1. Comparison with other approaches
</tableCaption>
<bodyText confidence="0.993553571428571">
Table 1 summarizes the performance on the
test data. The results presented in the table show
that our approach exhibits the best performance
among all the four approaches. It follows that the
key phrases inferred from a document semantic
network are not limited to the synonyms of title
phrases. As the title-sensitive PageRank ap-
</bodyText>
<footnote confidence="0.949937">
2 http://wordnet.princeton.edu
</footnote>
<page confidence="0.997287">
299
</page>
<bodyText confidence="0.999973909090909">
proach totally ignores the n-ary relations, its per-
formance is the worst. Based on binary relations,
the title-community approach clusters phrases
into communities and each community can be
considered as an n-ary relation. However, this
approach lacks of an importance propagation
process. Consequently, it has the highest recall
value but the lowest precision. In contrast, our
approach achieves the highest precision, due to
its ability to infer many correct key phrases using
importance propagation among n-ary relations.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999856">
This work is based on the belief that key phrases
tend to have close semantics to the title phrases.
In order to make better use of phrase relations in
key phrase extraction, we explore the Wikipedia
knowledge to model one document as a semantic
network in the form of hyper-graph, through
which the other phrases learned their importance
scores from the title phrases iteratively. Experi-
mental results demonstrate the effectiveness and
robustness of our approach.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999648166666667">
The work described in this paper was partially
supported by NSFC programs (No: 60773173,
60875042 and 90920011), and Hong Kong RGC
Projects (No: PolyU5217/07E). We thank the
anonymous reviewers for their insightful com-
ments.
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907833333333">
David Milne, Ian H. Witten. 2008. An Effective,
Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Wikipedia
and AI workshop at the AAAI-08 Conference,
Chicago, US.
Dengyong Zhou, Jiayuan Huang and Bernhard
Schölkopf. 2005. Beyond Pairwise Classifica-
tion and Clustering Using Hypergraphs. MPI
Technical Report, Tübingen, Germany.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
relatedness metric for wikipedia concepts
based on link analysis and its application to
word sense disambiguation. In Colloquium on
Databases and Information Systems (SYRCoDIS).
Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl
Gutwin , Craig G. Nevill-Manning. 1999. KEA:
practical automatic keyphrase extraction, In
Proceedings of the fourth ACM conference on Dig-
ital libraries, pp.254-255, California, USA.
Jahna Otterbacher, Gunes Erkan and Dragomir R.
Radev. 2005. Using Random Walks for Ques-
tion-focused Sentence Retrieval. In Proceedings
of HLT/EMNLP 2005, pp. 915-922, Vancouver,
Canada.
Maria Grineva, Maxim Grinev and Dmitry Lizorkin.
2009. Extracting key terms from noisy and
multitheme documents, In Proceedings of the
18th international conference on World wide web,
pp. 661-670, Madrid, Spain.
Michael Strube and Simone Paolo Ponzetto.
2006.WikiRelate! Computing Semantic Rela-
tedness using Wikipedia. In Proceedings of the
21st National Conference on Artificial Intelligence,
pp. 1419–1424, Boston, MA.
M. E. J. Newman. 2004. Analysis of Weighted Net-
works. Physical Review E 70, 056131.
</reference>
<page confidence="0.99798">
300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664819">
<title confidence="0.9967585">A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</title>
<author confidence="0.989791">Sujian Wenjie Wei Weiguang</author>
<affiliation confidence="0.995682666666667">Laboratory of Computational Linguistics, Peking University of Computing, The Hong Kong Polytechnic University 3School of Computer Science and Technology, Nanjing Normal University</affiliation>
<email confidence="0.722236">lidecong@pku.edu.cncswjli@comp.polyu.edu.hkwgqu@njnu.edu.cn</email>
<email confidence="0.722236">lisujian@pku.edu.cncswjli@comp.polyu.edu.hkwgqu@njnu.edu.cn</email>
<email confidence="0.722236">wwei@pku.edu.cncswjli@comp.polyu.edu.hkwgqu@njnu.edu.cn</email>
<abstract confidence="0.997792869565217">It is a fundamental and important task to extract key phrases from documents. Generally, phrases in a document are not independent in delivering the content of the document. In order to capture and make better use of their relationships in key phrase extraction, we suggest exploring the Wikipedia knowledge to model a document as a semantic network, both and binary relationships among phrases are formulated. Based on a commonly accepted assumption that the title of a document is always elaborated to reflect the content of a document and consequently key phrases tend to have close semantics to the title, we propose a novel semi-supervised key phrase extraction approach in this paper by computing the phrase importance in the semantic network, through which the influence of title phrases is propagated to the other phrases iteratively. Experimental results demonstrate the remarkable performance of this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links.</title>
<date>2008</date>
<booktitle>In Wikipedia and AI workshop at the AAAI-08 Conference,</booktitle>
<location>Chicago, US.</location>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne, Ian H. Witten. 2008. An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links. In Wikipedia and AI workshop at the AAAI-08 Conference, Chicago, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Jiayuan Huang</author>
<author>Bernhard Schölkopf</author>
</authors>
<title>Beyond Pairwise Classification and Clustering Using Hypergraphs.</title>
<date>2005</date>
<tech>MPI Technical Report,</tech>
<location>Tübingen, Germany.</location>
<marker>Zhou, Huang, Schölkopf, 2005</marker>
<rawString>Dengyong Zhou, Jiayuan Huang and Bernhard Schölkopf. 2005. Beyond Pairwise Classification and Clustering Using Hypergraphs. MPI Technical Report, Tübingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Turdakov</author>
<author>Pavel Velikhov</author>
</authors>
<title>Semantic relatedness metric for wikipedia concepts based on link analysis and its application to word sense disambiguation.</title>
<date>2008</date>
<booktitle>In Colloquium on Databases and Information Systems (SYRCoDIS).</booktitle>
<contexts>
<context position="3785" citStr="Turdakov and Velikhov, 2008" startWordPosition="610" endWordPosition="614"> can tell more details about the main topics of the document. In this regard, title phrases are often good key phrases but they are far from enough. If we review the above example again, we will find that the key phrase “Internet” can be inferred from the title phrase “Web”. As a matter of fact, key phrases often have close semantics to title phrases. Then a question comes to our minds: can we make use of these title phrases to infer the other key phrases? To provide a foundation of inference, a semantic network that captures the relationships among phrases is required. In the previous works (Turdakov and Velikhov, 2008), semantic networks are constructed based on the binary relations, and the semantic relatedness between a pair of phrases is formulated by the weighted edges that connects them. The deficiency of these approaches is the incapability to capture the n-ary relations among multiple phrases. For example, a group of 296 Proceedings of the ACL 2010 Conference Short Papers, pages 296–300, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics phrases may collectively describe an entity or an event. In this study, we propose to model a semantic network as a hyper-graph, wher</context>
<context position="6794" citStr="Turdakov and Velikhov, 2008" startWordPosition="1078" endWordPosition="1081">mantic relatedness between terms in a good few of works (Turdakov 2008). We consider a document composed of the phrases that describe various aspects of entities or events with different semantic relationships. We then model a document as a semantic network formulated by a weighted hyper-graph 1 www.wikipedia.org G=(V, E, W), where each vertex viEV (1&lt;_i&lt;_n) represents a phrase, each hyper-edge ej�E (1&lt;_j&lt;_m) is a subset of V, representing binary relations or n-ary relations among phrases, and the weight w(ej) measures the semantic relatedness of ej. By applying the WSD technique proposed by (Turdakov and Velikhov, 2008), each phrase is assigned with a single Wikipedia article that describes its meaning. Intuitively, if the fraction of the links that the two articles have in common to the total number of the links in both articles is high, the two phrases corresponding to the two articles are more semantically related. Also, an article contains different types of links, which are relevant to the computation of semantic relatedness to different extent. Hence we adopt the weighted Dice metric proposed by (Turdakov 2008) to compute the semantic relatedness of each binary relation, resulting in the edge weight w(</context>
<context position="11076" citStr="Turdakov and Velikhov 2008" startWordPosition="1795" endWordPosition="1799">escending order of the calculated importance scores and select those highest ranked phrases as key phrases. According to the number of all the candidate phrases, we choose an appropriate proportion, i.e. 10%, of all the phrases as key phrases. 4 Evaluation 4.1 Experiment Set-up We first collect all the Wikipedia terms to compose of a dictionary. The word sequences that occur in the dictionary are identified as phrases. Here we use a finite-state automaton to accomplish this task to avoid the imprecision of preprocessing by POS tagging or chunking. Then, we adopt the WSD technique proposed by (Turdakov and Velikhov 2008) to find the corresponding Wikipedia article for each phrase. As mentioned in Section 2, a document semantic network in the form of a hyper-graph is constructed, on which Algorithm 1 is applied to rank the phrases. To evaluate our proposed approach, we select 200 pieces of news from well-known English media. 5 to 10 key phrases are manually labeled in each news document and the average number of the key phrases is 7.2 per document. Due to the abbreviation and synonymy phenomena, we construct a thesaurus and convert all manual and automatic phrases into their canonical forms when evaluated. The</context>
</contexts>
<marker>Turdakov, Velikhov, 2008</marker>
<rawString>Denis Turdakov and Pavel Velikhov. 2008. Semantic relatedness metric for wikipedia concepts based on link analysis and its application to word sense disambiguation. In Colloquium on Databases and Information Systems (SYRCoDIS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Gordon W Paynter</author>
<author>Eibe Frank</author>
</authors>
<title>KEA: practical automatic keyphrase extraction,</title>
<date>1999</date>
<booktitle>In Proceedings of the fourth ACM conference on Digital libraries,</booktitle>
<pages>254--255</pages>
<location>California, USA.</location>
<marker>Witten, Paynter, Frank, 1999</marker>
<rawString>Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl Gutwin , Craig G. Nevill-Manning. 1999. KEA: practical automatic keyphrase extraction, In Proceedings of the fourth ACM conference on Digital libraries, pp.254-255, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Using Random Walks for Question-focused Sentence Retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>915--922</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="15076" citStr="Otterbacher et al., 2005" startWordPosition="2495" endWordPosition="2498"> Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. One approach combines the title information into the Grineva’s community-based method (Grineva et al., 2009), called titlecommunity approach. The title-community approach uses the Girvan-Newman algorithm to cluster phrases into communities and selects those phrases in the communities containing the title phrases as key phrases. We do not limit the number of key phrases selected. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here. The title-sensitive PageRank approach makes use of title phrases to re-weight the transitions between vertices and picks up 10% top-ranked phrases as key phrases. Approach Precision Recall F1 Title-sensitive Pa- 34.8% 39.5% 37.0% geRank (d=0.15) Title-community 29.8% 56.9% 39.1% Our approach a o 8 a (a=1.8, =0.5) 39.4 /0 44.6 /0 41./o WordNet expansion 7.9% 32.9% 12.5% (baseline) Table 1. Comparison with other approaches Table 1 summarizes the performance on the test data. The results presented in the table show that our approach exhibits the best perfo</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>Jahna Otterbacher, Gunes Erkan and Dragomir R. Radev. 2005. Using Random Walks for Question-focused Sentence Retrieval. In Proceedings of HLT/EMNLP 2005, pp. 915-922, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Grineva</author>
<author>Maxim Grinev</author>
<author>Dmitry Lizorkin</author>
</authors>
<title>Extracting key terms from noisy and multitheme documents,</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>661--670</pages>
<location>Madrid,</location>
<contexts>
<context position="14726" citStr="Grineva et al., 2009" startWordPosition="2442" endWordPosition="2445">in [0,1] 4.3 Comparison with Other Approaches Our approach aims at inferring important key phrases from title phrases through a semantic network. Here we take a method of synonym expansion as the baseline, called WordNet expansion here. The WordNet2 expansion approach selects all the synonyms of the title phrases in the document as key phrases. Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. One approach combines the title information into the Grineva’s community-based method (Grineva et al., 2009), called titlecommunity approach. The title-community approach uses the Girvan-Newman algorithm to cluster phrases into communities and selects those phrases in the communities containing the title phrases as key phrases. We do not limit the number of key phrases selected. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here. The title-sensitive PageRank approach makes use of title phrases to re-weight the transitions between vertices and picks up 10% top-ranked phrases as key phrases. Approach Precision Recall F1 Title-sensitive Pa</context>
</contexts>
<marker>Grineva, Grinev, Lizorkin, 2009</marker>
<rawString>Maria Grineva, Maxim Grinev and Dmitry Lizorkin. 2009. Extracting key terms from noisy and multitheme documents, In Proceedings of the 18th international conference on World wide web, pp. 661-670, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>1419--1424</pages>
<location>Boston, MA.</location>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006.WikiRelate! Computing Semantic Relatedness using Wikipedia. In Proceedings of the 21st National Conference on Artificial Intelligence, pp. 1419–1424, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Analysis of Weighted Networks.</title>
<date>2004</date>
<journal>Physical Review E</journal>
<volume>70</volume>
<pages>056131</pages>
<contexts>
<context position="7616" citStr="Newman 2004" startWordPosition="1214" endWordPosition="1215">cles is high, the two phrases corresponding to the two articles are more semantically related. Also, an article contains different types of links, which are relevant to the computation of semantic relatedness to different extent. Hence we adopt the weighted Dice metric proposed by (Turdakov 2008) to compute the semantic relatedness of each binary relation, resulting in the edge weight w(eij), where eij is an edge connecting the phrases vi and vj. To define the n-ary relations in the semantic network, a proper graph clustering technique is needed. We adopt the weighted Girvan-Newman algorithm (Newman 2004) to cluster phrases (including title phrases) by computing their betweenness centrality. The advantage of this algorithm is that it need not specify a pre-defined number of clusters. Then the phrases, within each cluster, are connected by a n-ary relation. nary relations among the phrases in the same cluster are then measured based on binary relations. The weight of a hyper-edge e is defined as: w(e) ¦ w(eij) e (1) where |e |is the number of the vertices in e, eij is an edge with two vertices included in e and a ≥ 0 is a parameter balancing the relative importance of n-ary hyper-edges compared</context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>M. E. J. Newman. 2004. Analysis of Weighted Networks. Physical Review E 70, 056131.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>