<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006498">
<title confidence="0.96938">
Tuning as Linear Regression
</title>
<author confidence="0.996388">
Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea
</author>
<affiliation confidence="0.997913">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.309038">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.979723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999569571428571">
We propose a tuning method for statistical ma-
chine translation, based on the pairwise rank-
ing approach. Hopkins and May (2011) pre-
sented a method that uses a binary classifier.
In this work, we use linear regression and
show that our approach is as effective as us-
ing a binary classifier and converges faster.
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930042553192">
Since its introduction, the minimum error rate train-
ing (MERT) (Och, 2003) method has been the most
popular method used for parameter tuning in ma-
chine translation. Although MERT has nice proper-
ties such as simplicity, effectiveness and speed, it is
known to not scale well for systems with large num-
bers of features. One alternative that has been used
for large numbers of features is the Margin Infused
Relaxed Algorithm (MIRA) (Chiang et al., 2008).
MIRA works well with a large number of features,
but the optimization problem is much more compli-
cated than MERT. MIRA also involves some modi-
fications to the decoder itself to produce hypotheses
with high scores against gold translations.
Hopkins and May (2011) introduced the method
of pairwise ranking optimization (PRO), which casts
the problem of tuning as a ranking problem be-
tween pairs of translation candidates. The problem
is solved by doing a binary classification between
“correctly ordered” and “incorrectly ordered” pairs.
Hopkins and May (2011) use the maximum entropy
classifier MegaM (Daum´e III, 2004) to do the binary
classification. Their method compares well to the
results of MERT, scales better for high dimensional
feature spaces, and is simpler than MIRA.
In this paper, we use the same idea for tuning, but,
instead of using a classifier, we use linear regression.
Linear regression is simpler than maximum entropy
based methods. The most complex computation that
it needs is a matrix inversion, whereas maximum en-
tropy based classifiers use iterative numerical opti-
mization methods.
We implemented a parameter tuning program
with linear regression and compared the results to
PRO’s results. The results of our experiments are
comparable to PRO, and in many cases (also on av-
erage) we get a better maximum BLEU score. We
also observed that on average, our method reaches
the maximum BLEU score in a smaller number of
iterations.
The contributions of this paper include: First, we
show that linear regression tuning is an effective
method for tuning, and it is comparable to tuning
with a binary maximum entropy classifier. Second,
we show linear regression is faster in terms of the
number of iterations it needs to reach the best re-
sults.
</bodyText>
<sectionHeader confidence="0.591178" genericHeader="method">
2 Tuning as Ranking
</sectionHeader>
<bodyText confidence="0.999952375">
The parameter tuning problem in machine transla-
tion is finding the feature weights of a linear trans-
lation model that maximize the scores of the candi-
date translations measured against reference transla-
tions. Hopkins and May (2011) introduce a tuning
method based on ranking the candidate translation
pairs, where the goal is to learn how to rank pairs of
candidate translations using a gold scoring function.
</bodyText>
<page confidence="0.906401">
543
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 543–547,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999952964285714">
PRO casts the tuning problem as the problem of
ranking pairs of sentences. This method iteratively
generates lists of “k-best” candidate translations for
each sentence, and tunes the weight vector for those
candidates. MERT finds the weight vector that max-
imizes the score for the highest scored candidate
translations. In contrast, PRO finds the weight vec-
tor which classifies pairs of candidate translations
into “correctly ordered” and “incorrectly ordered,”
based on the gold scoring function. While MERT
only considers the highest scored candidate to tune
the weights, PRO uses the entire k-best list to learn
the ranking between the pairs, which can help pre-
vent overfitting.
Let g(e) be a scoring function that maps each
translation candidate e to a number (score) using a
set of reference translations. The most commonly
used gold scoring function in machine translation
is the BLEU score, which is calculated for the en-
tire corpus, rather than for individual sentences. To
use BLEU as our gold scoring function, we need to
modify it to make it decomposable for single sen-
tences. One way to do this is to use a variation of
BLEU called BLEU+1 (Lin and Och, 2004), which
is a smoothed version of the BLEU score.
We assume that our machine translation system
scores translations by using a scoring function which
is a linear combination of the features:
</bodyText>
<equation confidence="0.993507">
h(e) = wTx(e) (1)
</equation>
<bodyText confidence="0.9999385">
where w is the weight vector and x is the feature vec-
tor. The goal of tuning as ranking is learning weights
such that for every two candidate translations e1 and
e2, the following inequality holds:
</bodyText>
<equation confidence="0.997054666666667">
g(e1) &gt; g(e2) ⇔ h(e1) &gt; h(e2) (2)
Using Equation 1, we can rewrite Equation 2:
g(e1) &gt; g(e2) ⇔ wT(x(e1) − x(e2)) &gt; 0 (3)
</equation>
<bodyText confidence="0.999969">
This problem can be viewed as a binary classifica-
tion problem for learning w, where each data point is
the difference vector between the feature vectors of
a pair of translation candidates, and the target of the
point is the sign of the difference between their gold
scores (BLEU+1). PRO uses the MegaM classifier
to solve this problem. MegaM is a binary maximum
entropy classifier which returns the weight vector
w as a linear classifier. Using this method, Hop-
kins and May (2011) tuned the weight vectors for
various translation systems. The results were close
to MERT’s and MIRA’s results in terms of BLEU
score, and the method was shown to scale well to
high dimensional feature spaces.
</bodyText>
<sectionHeader confidence="0.984768" genericHeader="method">
3 Linear Regression Tuning
</sectionHeader>
<bodyText confidence="0.999564857142857">
In this paper, we use the same idea as PRO for tun-
ing, but instead of using a maximum entropy clas-
sifier, we use a simple linear regression to estimate
the vector w in Equation 3. We use the least squares
method to estimate the linear regression. For a ma-
trix of data points X, and a target vector g, the
weight vector can be calculated as:
</bodyText>
<equation confidence="0.997913">
w = (XTX)−1XTg (4)
</equation>
<bodyText confidence="0.987318">
Adding L2 regularization with parameter A has the
following closed form solution:
</bodyText>
<equation confidence="0.998993">
w = (XTX + AI)−1XTg (5)
</equation>
<bodyText confidence="0.948708333333333">
Following the sampling method used in PRO, the
matrices X and vector g are prepared as follows:
For each sentence,
</bodyText>
<listItem confidence="0.998390611111111">
1. Generate a list containing the k best transla-
tions of the sentence, with each translation e
scored by the decoder using a function of the
form h(e) = wTx(e).
2. Use the uniform distribution to sample n ran-
dom pairs from the set of candidate transla-
tions.
3. Calculate the gold scores g for the candidates in
each pair using BLEU+1. Keep a pair of can-
didates as a potential pair if the difference be-
tween their g scores is bigger than a threshold
t.
4. From the potential pairs kept in the previous
step, keep the s pairs that have the highest dif-
ferences in g and discard the rest.
5. For each pair e1 and e2 kept in step 4, make two
data points (x(e1) − x(e2), g(e1) − g(e2)) and
(x(e2) − x(e1), g(e2) − g(e1)).
</listItem>
<page confidence="0.993941">
544
</page>
<bodyText confidence="0.995935363636364">
The rows of X consist of the inputs of the data points
created in step 5, i.e., the difference vectors x(e1) −
x(e2). Similarly, the corresponding rows in g are
the outputs of the data points, i.e., the gold score
differences g(e1) − g(e2).
One important difference between the linear re-
gression method and PRO is that rather than using
the signs of the gold score differences and doing a
binary classification, we use the differences of the
gold scores directly, which allows us to use the in-
formation about the magnitude of the differences.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.878523">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.857748323529412">
We used a Chinese-English parallel corpus with the
English side parsed for our experiments. The cor-
pus consists of 250K sentence pairs, which is 6.3M
words on the English side. The corpus derives from
newswire texts available from LDC.1 We used a 392-
sentence development set with four references for
parameter tuning, and a 428-sentence test set with
four references for testing. They are drawn from the
newswire portion of NIST evaluations (2004, 2005,
2006). The development set and the test set only
had sentences with less than 30 words for decoding
speed.
We extracted a general SCFG (GHKM) grammar
using standard methods (Galley et al., 2004; Wang
et al., 2010) from the parallel corpus with a mod-
ification to preclude any unary rules (Chung et al.,
2011). All rules over scope 3 are pruned (Hopkins
and Langmead, 2010). A set of nine standard fea-
tures was used for the experiments, which includes
globally normalized count of rules, lexical weight-
ing (Koehn et al., 2003), and length penalty. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
We implemented linear regression tuning using
1We randomly sampled our data from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M sen-
tences, which is 39.3M words.)
</bodyText>
<table confidence="0.9772225">
Average of max BLEU Max BLEU
dev test dev test
Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6
PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2
</table>
<tableCaption confidence="0.9994">
Table 1: Average of maximum BLEU scores of the ex-
</tableCaption>
<bodyText confidence="0.927600071428571">
periments and the maximum BLEU score from the ex-
periments. Numbers in the parentheses indicate standard
of deviations of maximum BLEU scores.
the method explained in Section 3. Following Hop-
kins and May (2011), we used the following param-
eters for the sampling task: For each sentence, the
decoder generates the 1500 best candidate transla-
tions (k = 1500), and the sampler samples 5000
pairs (n = 5000). Each pair is kept as a potential
data point if their BLEU+1 score difference is big-
ger than 0.05 (t = 0.05). Finally, for each sentence,
the sampler keeps the 50 pairs with the highest dif-
ference in BLEU+1(s = 50) and generates two data
points for each pair.
</bodyText>
<sectionHeader confidence="0.655237" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.995793666666667">
We ran eight experiments with random initial weight
vectors and ran each experiment for 25 iterations.
Similar to what PRO does, in each iteration, we lin-
early interpolate the weight vector learned by the re-
gression (w) with the weight vector of the previous
iteration (wt−1) using a factor of 0.1:
</bodyText>
<equation confidence="0.978546">
wt = 0.1 · w + 0.9 · wt−1 (6)
</equation>
<bodyText confidence="0.999882235294118">
For the sake of comparison, we also implemented
PRO with exactly the same parameters, and ran it
with the same initial weight vectors.
For each initial weight vector, we selected the iter-
ation at which the BLEU score on the development
set is highest, and then decoded using this weight
vector on the test set. The results of our experi-
ments are presented in Table 1. In the first column,
we show the average over the eight initial weight
vectors of the BLEU score achieved, while in the
second column we show the results from the ini-
tial weight vector with the highest BLEU score on
the development set. Thus, while the second col-
umn corresponds to a tuning process where the sin-
gle best result is retained, the first column shows the
expected behavior of the procedure on a single ini-
tial weight vector. The linear regression method has
</bodyText>
<page confidence="0.994412">
545
</page>
<figure confidence="0.9960735">
0 5 10 15 20 25
Iteration
</figure>
<figureCaption confidence="0.999995">
Figure 1: Average of eight runs of regression and PRO.
</figureCaption>
<bodyText confidence="0.99994404">
higher BLEU scores on both development and test
data for both the average over initial weights and the
maximum over initial weights.
Figure 1 shows the average of the BLEU scores
on the development set of eight runs of the experi-
ments. We observe that on average, the linear regres-
sion experiments reach the maximum BLEU score
in a smaller number of iterations. On average, linear
regression reached the maximum BLEU score after
14 iterations and PRO reached the maximum BLEU
score after 20 iterations. One iteration took several
minutes for both of the algorithms. The largest por-
tion of this time is spent on decoding the develop-
ment set and reading in the k-best list. The sampling
phase, which includes performing linear regression
or running MegaM, takes a negligible amount of
time compared to the rest of the operations.
We experimented with adding L2 regularization
to linear regression. As expected, the experiments
with regularization produced lower variance among
the different experiments in terms of the BLEU
score, and the resulting set of the parameters had a
smaller norm. However, because of the small num-
ber of features used in our experiments, regulariza-
tion was not necessary to control overfitting.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999532258064516">
We applied the idea of tuning as ranking and modi-
fied it to use linear regression instead of binary clas-
sification. The results of our experiments show that
tuning as linear regression is as effective as PRO,
and on average it reaches a better BLEU score in a
fewer number of iterations.
In comparison with MERT, PRO and linear re-
gression are different in the sense that the latter two
approaches take into account rankings of the k-best
list, whereas MERT is only concerned with separat-
ing the top 1-best sentence from the rest of the k-
best list. PRO and linear regression are similar in
the sense that both are concerned with ranking the
k-best list. Their difference lies in the fact that PRO
only uses the information on the relative rankings
and uses binary classification to rank the points; on
the contrary, linear regression directly uses the infor-
mation on the magnitude of the differences. This dif-
ference between PRO and linear regression explains
why linear regression converges faster and also may
explain the fact that linear regression achieves a
somewhat higher BLEU score. In this sense, lin-
ear regression is also similar to MIRA since MIRA’s
loss function also uses the information on the magni-
tude of score difference. However, the optimization
problem for linear regression is simpler, does not re-
quire any changes to the decoder, and therefore the
familiar MERT framework can be kept.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work was
supported by NSF grant IIS-0910611.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998072666666667">
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon. As-
sociation for Computational Linguistics.
Hal Daum´e III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression. August.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273–280, Boston.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
</reference>
<figure confidence="0.949353083333333">
28
26
24
22
20
18
16
14
12
reg-avg
pro-avg
BLEU
</figure>
<page confidence="0.987145">
546
</page>
<reference confidence="0.999676538461538">
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646–655, Cambridge, MA,
October. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352–1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of Coling 2004,
pages 501–507, Geneva, Switzerland, Aug 23–Aug
27. COLING.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36:247–277, June.
</reference>
<page confidence="0.997663">
547
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959069">
<title confidence="0.998945">Tuning as Linear Regression</title>
<author confidence="0.973464">Chung</author>
<affiliation confidence="0.999807">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.998361">We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP08).</booktitle>
<contexts>
<context position="941" citStr="Chiang et al., 2008" startWordPosition="150" endWordPosition="153">a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster. 1 Introduction Since its introduction, the minimum error rate training (MERT) (Och, 2003) method has been the most popular method used for parameter tuning in machine translation. Although MERT has nice properties such as simplicity, effectiveness and speed, it is known to not scale well for systems with large numbers of features. One alternative that has been used for large numbers of features is the Margin Infused Relaxed Algorithm (MIRA) (Chiang et al., 2008). MIRA works well with a large number of features, but the optimization problem is much more complicated than MERT. MIRA also involves some modifications to the decoder itself to produce hypotheses with high scores against gold translations. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. The problem is solved by doing a binary classification between “correctly ordered” and “incorrectly ordered” pairs. Hopkins and May (2011) use the maximum entropy classifier Meg</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Conference on Empirical Methods in Natural Language Processing (EMNLP08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="8829" citStr="Chiang, 2007" startWordPosition="1494" endWordPosition="1495">extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.) Average of max BLEU Max BLEU dev test dev test Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6 PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2 Table 1: Average of maximum BLEU score</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Licheng Fang</author>
<author>Daniel Gildea</author>
</authors>
<title>Issues concerning decoding with synchronous context-free grammar.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Conference Short Papers,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="8413" citStr="Chung et al., 2011" startWordPosition="1425" endWordPosition="1428"> 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392- sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LD</context>
</contexts>
<marker>Chung, Fang, Gildea, 2011</marker>
<rawString>Tagyoung Chung, Licheng Fang, and Daniel Gildea. 2011. Issues concerning decoding with synchronous context-free grammar. In Proceedings of the ACL 2011 Conference Short Papers, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<date>2004</date>
<booktitle>Notes on CG and LM-BFGS optimization of logistic regression.</booktitle>
<marker>Daum´e, 2004</marker>
<rawString>Hal Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-04),</booktitle>
<pages>273--280</pages>
<location>Boston.</location>
<contexts>
<context position="8299" citStr="Galley et al., 2004" startWordPosition="1404" endWordPosition="1407">llel corpus with the English side parsed for our experiments. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392- sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled o</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the 2004 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-04), pages 273–280, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>646--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="8477" citStr="Hopkins and Langmead, 2010" startWordPosition="1436" endWordPosition="1439"> newswire texts available from LDC.1 We used a 392- sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC200</context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1205" citStr="Hopkins and May (2011)" startWordPosition="193" endWordPosition="196">e most popular method used for parameter tuning in machine translation. Although MERT has nice properties such as simplicity, effectiveness and speed, it is known to not scale well for systems with large numbers of features. One alternative that has been used for large numbers of features is the Margin Infused Relaxed Algorithm (MIRA) (Chiang et al., 2008). MIRA works well with a large number of features, but the optimization problem is much more complicated than MERT. MIRA also involves some modifications to the decoder itself to produce hypotheses with high scores against gold translations. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. The problem is solved by doing a binary classification between “correctly ordered” and “incorrectly ordered” pairs. Hopkins and May (2011) use the maximum entropy classifier MegaM (Daum´e III, 2004) to do the binary classification. Their method compares well to the results of MERT, scales better for high dimensional feature spaces, and is simpler than MIRA. In this paper, we use the same idea for tuning, but, instead of using a classifie</context>
<context position="2960" citStr="Hopkins and May (2011)" startWordPosition="477" endWordPosition="480">s the maximum BLEU score in a smaller number of iterations. The contributions of this paper include: First, we show that linear regression tuning is an effective method for tuning, and it is comparable to tuning with a binary maximum entropy classifier. Second, we show linear regression is faster in terms of the number of iterations it needs to reach the best results. 2 Tuning as Ranking The parameter tuning problem in machine translation is finding the feature weights of a linear translation model that maximize the scores of the candidate translations measured against reference translations. Hopkins and May (2011) introduce a tuning method based on ranking the candidate translation pairs, where the goal is to learn how to rank pairs of candidate translations using a gold scoring function. 543 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 543–547, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics PRO casts the tuning problem as the problem of ranking pairs of sentences. This method iteratively generates lists of “k-best” candidate translations for each sentence, and tunes the weight vec</context>
<context position="5545" citStr="Hopkins and May (2011)" startWordPosition="908" endWordPosition="912">g inequality holds: g(e1) &gt; g(e2) ⇔ h(e1) &gt; h(e2) (2) Using Equation 1, we can rewrite Equation 2: g(e1) &gt; g(e2) ⇔ wT(x(e1) − x(e2)) &gt; 0 (3) This problem can be viewed as a binary classification problem for learning w, where each data point is the difference vector between the feature vectors of a pair of translation candidates, and the target of the point is the sign of the difference between their gold scores (BLEU+1). PRO uses the MegaM classifier to solve this problem. MegaM is a binary maximum entropy classifier which returns the weight vector w as a linear classifier. Using this method, Hopkins and May (2011) tuned the weight vectors for various translation systems. The results were close to MERT’s and MIRA’s results in terms of BLEU score, and the method was shown to scale well to high dimensional feature spaces. 3 Linear Regression Tuning In this paper, we use the same idea as PRO for tuning, but instead of using a maximum entropy classifier, we use a simple linear regression to estimate the vector w in Equation 3. We use the least squares method to estimate the linear regression. For a matrix of data points X, and a target vector g, the weight vector can be calculated as: w = (XTX)−1XTg (4) Add</context>
<context position="9649" citStr="Hopkins and May (2011)" startWordPosition="1612" endWordPosition="1616">005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.) Average of max BLEU Max BLEU dev test dev test Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6 PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2 Table 1: Average of maximum BLEU scores of the experiments and the maximum BLEU score from the experiments. Numbers in the parentheses indicate standard of deviations of maximum BLEU scores. the method explained in Section 3. Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k = 1500), and the sampler samples 5000 pairs (n = 5000). Each pair is kept as a potential data point if their BLEU+1 score difference is bigger than 0.05 (t = 0.05). Finally, for each sentence, the sampler keeps the 50 pairs with the highest difference in BLEU+1(s = 50) and generates two data points for each pair. 4.2 Results We ran eight experiments with random initial weight vectors and ran each experiment for 25 iterations. Similar to what PRO does, in ea</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="8630" citStr="Koehn et al., 2003" startWordPosition="1462" endWordPosition="1465">erences for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sen</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>501--507</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="4537" citStr="Lin and Och, 2004" startWordPosition="729" endWordPosition="732"> to tune the weights, PRO uses the entire k-best list to learn the ranking between the pairs, which can help prevent overfitting. Let g(e) be a scoring function that maps each translation candidate e to a number (score) using a set of reference translations. The most commonly used gold scoring function in machine translation is the BLEU score, which is calculated for the entire corpus, rather than for individual sentences. To use BLEU as our gold scoring function, we need to modify it to make it decomposable for single sentences. One way to do this is to use a variation of BLEU called BLEU+1 (Lin and Och, 2004), which is a smoothed version of the BLEU score. We assume that our machine translation system scores translations by using a scoring function which is a linear combination of the features: h(e) = wTx(e) (1) where w is the weight vector and x is the feature vector. The goal of tuning as ranking is learning weights such that for every two candidate translations e1 and e2, the following inequality holds: g(e1) &gt; g(e2) ⇔ h(e1) &gt; h(e2) (2) Using Equation 1, we can rewrite Equation 2: g(e1) &gt; g(e2) ⇔ wT(x(e1) − x(e2)) &gt; 0 (3) This problem can be viewed as a binary classification problem for learnin</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of Coling 2004, pages 501–507, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, re-labeling, and realigning for syntax-based machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<contexts>
<context position="8319" citStr="Wang et al., 2010" startWordPosition="1408" endWordPosition="1411">English side parsed for our experiments. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. The corpus derives from newswire texts available from LDC.1 We used a 392- sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. They are drawn from the newswire portion of NIST evaluations (2004, 2005, 2006). The development set and the test set only had sentences with less than 30 words for decoding speed. We extracted a general SCFG (GHKM) grammar using standard methods (Galley et al., 2004; Wang et al., 2010) from the parallel corpus with a modification to preclude any unary rules (Chung et al., 2011). All rules over scope 3 are pruned (Hopkins and Langmead, 2010). A set of nine standard features was used for the experiments, which includes globally normalized count of rules, lexical weighting (Koehn et al., 2003), and length penalty. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). We implemented linear regression tuning using 1We randomly sampled our data from various</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36:247–277, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>