<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.9027945">
Exploiting Syntactic and Shallow Semantic Kernels
for Question/Answer Classification
</title>
<author confidence="0.997067">
Alessandro Moschitti
</author>
<affiliation confidence="0.999246">
University of Trento
</affiliation>
<address confidence="0.5238225">
38050 Povo di Trento
Italy
</address>
<email confidence="0.994465">
moschitti@dit.unitn.it
</email>
<author confidence="0.983782">
Silvia Quarteroni
</author>
<affiliation confidence="0.992051">
The University of York
</affiliation>
<address confidence="0.915394">
York YO10 5DD
United Kingdom
</address>
<email confidence="0.996318">
silvia@cs.york.ac.uk
</email>
<author confidence="0.983738">
Roberto Basili
</author>
<affiliation confidence="0.980745">
“Tor Vergata” University
</affiliation>
<address confidence="0.9627115">
Via del Politecnico 1
00133 Rome, Italy
</address>
<email confidence="0.996417">
basili@info.uniroma2.it
</email>
<author confidence="0.965318">
Suresh Manandhar
</author>
<affiliation confidence="0.982962">
The University of York
</affiliation>
<address confidence="0.91327">
York YO10 5DD
United Kingdom
</address>
<email confidence="0.997688">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799">
We study the impact of syntactic and shallow
semantic information in automatic classifi-
cation of questions and answers and answer
re-ranking. We define (a) new tree struc-
tures based on shallow semantics encoded
in Predicate Argument Structures (PASs)
and (b) new kernel functions to exploit the
representational power of such structures
with Support Vector Machines. Our ex-
periments suggest that syntactic information
helps tasks such as question/answer classifi-
cation and that shallow semantics gives re-
markable contribution when a reliable set of
PASs can be extracted, e.g. from answers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996083">
Question answering (QA) is as a form of informa-
tion retrieval where one or more answers are re-
turned to a question in natural language in the form
of sentences or phrases. The typical QA system ar-
chitecture consists of three phases: question pro-
cessing, document retrieval and answer extraction
(Kwok et al., 2001).
Question processing is often centered on question
classification, which selects one of k expected an-
swer classes. Most accurate models apply super-
vised machine learning techniques, e.g. SNoW (Li
and Roth, 2005), where questions are encoded us-
ing various lexical, syntactic and semantic features.
The retrieval and answer extraction phases consist in
retrieving relevant documents (Collins-Thompson et
al., 2004) and selecting candidate answer passages
from them. A further answer re-ranking phase is op-
tionally applied. Here, too, the syntactic structure
of a sentence appears to provide more useful infor-
mation than a bag of words (Chen et al., 2006), al-
though the correct way to exploit it is still an open
problem.
An effective way to integrate syntactic structures
in machine learning algorithms is the use of tree ker-
nel (TK) functions (Collins and Duffy, 2002), which
have been successfully applied to question classifi-
cation (Zhang and Lee, 2003; Moschitti, 2006) and
other tasks, e.g. relation extraction (Zelenko et al.,
2003; Moschitti, 2006). In more complex tasks such
as computing the relatedness between questions and
answers in answer re-ranking, to our knowledge no
study uses kernel functions to encode syntactic in-
formation. Moreover, the study of shallow semantic
information such as predicate argument structures
annotated in the PropBank (PB) project (Kingsbury
and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a
promising research direction. We argue that seman-
tic structures can be used to characterize the relation
between a question and a candidate answer.
In this paper, we extensively study new structural
representations, encoding parse trees, bag-of-words,
POS tags and predicate argument structures (PASs)
for question classification and answer re-ranking.
We define new tree representations for both simple
and nested PASs, i.e. PASs whose arguments are
other predicates (Section 2). Moreover, we define
new kernel functions to exploit PASs, which we au-
tomatically derive with our SRL system (Moschitti
et al., 2005) (Section 3).
Our experiments using SVMs and the above ker-
</bodyText>
<page confidence="0.955251">
776
</page>
<note confidence="0.925895">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999739428571428">
nels and data (Section 4) shows the following: (a)
our approach reaches state-of-the-art accuracy on
question classification. (b) PB predicative structures
are not effective for question classification but show
promising results for answer classification on a cor-
pus of answers to TREC-QA 2001 description ques-
tions. We created such dataset by using YourQA
(Quarteroni and Manandhar, 2006), our basic Web-
based QA system1. (c) The answer classifier in-
creases the ranking accuracy of our QA system by
about 25%.
Our results show that PAS and syntactic parsing
are promising methods to address tasks affected by
data sparseness like question/answer categorization.
</bodyText>
<sectionHeader confidence="0.963088" genericHeader="method">
2 Encoding Shallow Semantic Structures
</sectionHeader>
<bodyText confidence="0.933305733333334">
Traditionally, information retrieval techniques are
based on the bag-of-words (BOW) approach aug-
mented by language modeling (Allan et al., 2002).
When the task requires the use of more complex se-
mantics, the above approaches are often inadequate
to perform fine-level textual analysis.
An improvement on BOW is given by the use of
syntactic parse trees, e.g. for question classification
(Zhang and Lee, 2003), but these, too are inadequate
when dealing with definitional answers expressed by
long and articulated sentences or even paragraphs.
On the contrary, shallow semantic representations,
bearing a more “compact” information, could pre-
vent the sparseness of deep structural approaches
and the weakness of BOW models.
Initiatives such as PropBank (PB) (Kingsbury
and Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems (Carreras and M`arquez, 2005). Attempting
an application of SRL to QA hence seems natural,
as pinpointing the answer to a question relies on a
deep understanding of the semantics of both.
Let us consider the PB annotation: [ARG1
Antigens] were [AM−TMP originally] [rel
defined] [ARG2 as non-self molecules].
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g. [ARG0
Researchers] [rel describe] [ARG1 antigens]
[ARG2 as foreign molecules] [ARGM−LOC in
</bodyText>
<footnote confidence="0.901652">
1Demoat:http://cs.york.ac.uk/aig/aqua.
</footnote>
<figureCaption confidence="0.881206666666667">
Figure 1: Compact predicate argument structures of
two different sentences.
the body].
</figureCaption>
<bodyText confidence="0.986857382352941">
For this purpose, we can represent the above anno-
tated sentences using the tree structures described in
Figure 1. In this compact representation, hereafter
Predicate-Argument Structures (PAS), arguments
are replaced with their most important word – often
referred to as the semantic head. This reduces
data sparseness with respect to a typical BOW
representation.
However, sentences rarely contain a single pred-
icate; it happens more generally that propositions
contain one or more subordinate clauses. For
instance let us consider a slight modification of the
first sentence: “Antigens were originally defined
as non-self molecules which bound specifically to
antibodies2.” Here, the main predicate is “defined”,
followed by a subordinate predicate “bound”. Our
SRL system outputs the following two annotations:
(1) [ARG1 Antigens] were [ARGM−TMP
originally] [rel defined] [ARG2 as non-self
molecules which bound specifically to
antibodies].
(2)Antigens were originally defined as
[ARG1 non-self molecules] [R−A1 which] [rel
bound] [ARGM−MNR specifically] [ARG2 to
antibodies].
giving the PASs in Figure 2.(a) resp. 2.(b).
As visible in Figure 2.(a), when an argument node
corresponds to an entire subordinate clause, we label
its leaf with PAS, e.g. the leaf of ARG2. Such PAS
node is actually the root of the subordinate clause
in Figure 2.(b). Taken as standalone, such PASs do
not express the whole meaning of the sentence; it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
</bodyText>
<footnote confidence="0.981235">
2This is an actual answer to ”What are antibodies?” from
our question answering system, YourQA.
</footnote>
<figure confidence="0.991175421875">
PAS
ARG2
molecules
ARGM-TMP
originally
rel
define
ARG1
antigens
PAS
ARG0
researchers
ARG2
molecules
ARGM-LOC
body
rel
describe
ARG1
antigens
777
PAS
PAS
PAS
(a)
(b)
(c)
Figure 2: Two PASs composing a PASN
AM-TMP
originally
ARG1
molecules
AM-ADV
specifically
ARG2
antibodies
AM-TMP
originally
AM-ADV
specifically
rel
define
ARG1
antigens
ARG2
PAS
rel
bound
R-ARG1
which
rel
bound
rel
define
ARG1
molecules
ARG1
antigens
R-ARG1
which
ARG2
PAS
ARG2
antibodies
</figure>
<figureCaption confidence="0.998163">
Figure 2.(c). We refer to nested PASs as PASNs.
</figureCaption>
<bodyText confidence="0.991839476190476">
It is worth to note that semantically equivalent
sentences syntactically expressed in different ways
share the same PB arguments and the same PASs,
whereas semantically different sentences result in
different PASs. For example, the sentence: “Anti-
gens were originally defined as antibodies which
bound specifically to non-self molecules”, uses the
same words as (2) but has different meaning. Its PB
annotation:
(3)Antigens were originally defined
as [ARG1 antibodies] [R−A1 which] [rel
bound] [ARGM−MNR specifically] [ARG2 to
non-self molecules],
clearly differs from (2), as ARG2 is now non-
self molecules; consequently, the PASs are also
different.
Once we have assumed that parse trees and PASs
can improve on the simple BOW representation, we
face the problem of representing tree structures in
learning machines. Section 3 introduces a viable ap-
proach based on tree kernels.
</bodyText>
<sectionHeader confidence="0.958381" genericHeader="method">
3 Syntactic and Semantic Kernels for Text
</sectionHeader>
<bodyText confidence="0.999982913043478">
As mentioned above, encoding syntactic/semantic
information represented by means of tree structures
in the learning algorithm is problematic. A first so-
lution is to use all its possible substructures as fea-
tures. Given the combinatorial explosion of consid-
ering subparts, the resulting feature space is usually
very large. A tree kernel (TK) function which com-
putes the number of common subtrees between two
syntactic parse trees has been given in (Collins and
Duffy, 2002). Unfortunately, such subtrees are sub-
ject to the constraint that their nodes are taken with
all or none of the children they have in the original
tree. This makes the TK function not well suited for
the PAS trees defined above. For instance, although
the two PASs of Figure 1 share most of the subtrees
rooted in the PAS node, Collins and Duffy’s kernel
would compute no match.
In the next section we describe a new kernel de-
rived from the above tree kernel, able to evaluate the
meaningful substructures for PAS trees. Moreover,
as a single PAS may not be sufficient for text rep-
resentation, we propose a new kernel that combines
the contributions of different PASs.
</bodyText>
<subsectionHeader confidence="0.998007">
3.1 Tree kernels
</subsectionHeader>
<bodyText confidence="0.9988425">
Given two trees T1 and T2, let {f1, f2, ..} = F be
the set of substructures (fragments) and Ii(n) be
equal to 1 if fi is rooted at node n, 0 otherwise.
Collins and Duffy’s kernel is defined as
</bodyText>
<equation confidence="0.979207">
TK(T1,T2) = En1ENT1 En2ENT2 A(n1,n2), (1)
</equation>
<bodyText confidence="0.9508665">
where NT1 and NT2 are the sets of nodes
in T1 and T2, respectively and A(n1, n2) =
E|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number
of common fragments rooted in nodes n1 and n2. A
can be computed as follows:
</bodyText>
<listItem confidence="0.931694111111111">
(1) if the productions (i.e. the nodes with their
direct children) at n1 and n2 are different then
A(n1, n2) = 0;
(2) if the productions at n1 and n2 are the same, and
n1 and n2 only have leaf children (i.e. they are pre-
terminal symbols) then A(n1, n2) = 1;
(3) if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then A(n1, n2) =
Hnc(11)(1+A(cjn1, c2
</listItem>
<bodyText confidence="0.987649909090909">
jn)), where nc(n1) is the num-
j=
ber of children of n1 and cjn is the j-th child of n.
Such tree kernel can be normalized and a λ factor
can be added to reduce the weight of large structures
(refer to (Collins and Duffy, 2002) for a complete
description). The critical aspect of steps (1), (2) and
(3) is that the productions of two evaluated nodes
have to be identical to allow the match of further de-
scendants. This means that common substructures
cannot be composed by a node with only some of its
</bodyText>
<page confidence="0.970375">
778
</page>
<figure confidence="0.999613621621621">
SLOT
rel
define
SLOT
ARG1
antigens
PAS
SLOT
ARG2
PAS
SLOT
ARGM-TMP
originally
SLOT
rel
define
SLOT
ARG1
antigens
PAS
SLOT
null
SLOT
null
SLOT
rel
define
SLOT
null
PAS
SLOT
ARG2
PAS
SLOT
null
* * * * *
(a) (b) (c)
</figure>
<figureCaption confidence="0.999933">
Figure 3: A PAS with some of its fragments.
</figureCaption>
<bodyText confidence="0.99989725">
children as an effective PAS representation would
require. We solve this problem by designing the
Shallow Semantic Tree Kernel (SSTK) which allows
to match portions of a PAS.
</bodyText>
<subsectionHeader confidence="0.998226">
3.2 The Shallow Semantic Tree Kernel (SSTK)
</subsectionHeader>
<bodyText confidence="0.999299961538462">
The SSTK is based on two ideas: first, we change
the PAS, as shown in Figure 3.(a) by adding SLOT
nodes. These accommodate argument labels in a
specific order, i.e. we provide a fixed number of
slots, possibly filled with null arguments, that en-
code all possible predicate arguments. For simplic-
ity, the figure shows a structure of just 4 arguments,
but more can be added to accommodate the max-
imum number of arguments a predicate can have.
Leaf nodes are filled with the wildcard character *
but they may alternatively accommodate additional
information.
The slot nodes are used in such a way that the
adopted TK function can generate fragments con-
taining one or more children like for example those
shown in frames (b) and (c) of Figure 3. As pre-
viously pointed out, if the arguments were directly
attached to the root node, the kernel function would
only generate the structure with all children (or the
structure with no children, i.e. empty).
Second, as the original tree kernel would generate
many matches with slots filled with the null label,
we have set a new step 0:
(0) if n1 (or n2) is a pre-terminal node and its child
label is null, Δ(n1, n2) = 0;
and subtract one unit to Δ(n1, n2), in step 3:
</bodyText>
<equation confidence="0.997375">
(3) Δ(n1,n2) = Qnc(n1)
j=1 (1 + Δ(cjn1,cjn2)) − 1,
</equation>
<bodyText confidence="0.9962529">
The above changes generate a new Δ which,
when substituted (in place of the original Δ) in Eq.
1, gives the new Shallow Semantic Tree Kernel. To
show that SSTK is effective in counting the number
of relations shared by two PASs, we propose the fol-
lowing:
Proposition 1 The new Δ function applied to the
modified PAS counts the number of all possible k-
ary relations derivable from a set of k arguments,
i.e. Pkk � relations of arity from 1 to k (the pred-
</bodyText>
<equation confidence="0.978386">
i=1 i
</equation>
<bodyText confidence="0.99821475">
icate being considered as a special argument).
Proof We observe that a kernel applied to a tree and
itself computes all its substructures, thus if we eval-
uate SSTK between a PAS and itself we must obtain
the number of generated k-ary relations. We prove
by induction the above claim.
For the base case (k = 0): we use a PAS with no
arguments, i.e. all its slots are filled with null la-
bels. Let r be the PAS root; since r is not a pre-
terminal, step 3 is selected and Δ is recursively ap-
plied to all r’s children, i.e. the slot nodes. For the
latter, step 0 assigns Δ(cjr, cjr) = 0. As a result,
</bodyText>
<equation confidence="0.977406210526316">
Δ(r, r) =Qnc(r)
j=1 (1 + 0) − 1 = 0 and the base case
holds.
For the general case, r is the root of a PAS with k+1
arguments. Δ(r, r) = Qnc(r)
j=1 (1 + Δ(cjr, cjr)) − 1
=Qk j=1(1+Δ(cjr,cjr))×(1+Δ(ck+1
r ,ck+1
r ))−1. For
k arguments, we assume by induction that Qkj=1(1+
Δ(cjr, cjr)) − 1 = Pk �k �, i.e. the number of k-ary
i=1 i
relations. Moreover, (1 + Δ(ck+1
r , ck+1
r )) = 2, thus
Δ(r, r) = Pk �k � × 2 = 2k × 2 = 2k+1 = Pk+1
�, i.e. all the relations until arity k + 1
i=1 i i=1
�k+1 ✷
</equation>
<bodyText confidence="0.986532875">
i
TK functions can be applied to sentence parse
trees, therefore their usefulness for text processing
applications, e.g. question classification, is evident.
On the contrary, the SSTK applied to one PAS ex-
tracted from a text fragment may not be meaningful
since its representation needs to take into account all
the PASs that it contains. We address such problem
</bodyText>
<page confidence="0.996595">
779
</page>
<bodyText confidence="0.997872">
by defining a kernel on multiple PASs.
Let Pt and Pty be the sets of PASs extracted from
the text fragment t and t&apos;. We define:
</bodyText>
<equation confidence="0.96152">
�Kall(Pt, Pt�) = E SSTK(p, p% (2)
pEPt p&apos;EPt/
</equation>
<bodyText confidence="0.99995225">
While during the experiments (Sect. 4) the Kall
kernel is used to handle predicate argument struc-
tures, TK (Eq. 1) is used to process parse trees and
the linear kernel to handle POS and BOW features.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999938173913043">
The purpose of our experiments is to study the im-
pact of the new representations introduced earlier for
QA tasks. In particular, we focus on question clas-
sification and answer re-ranking for Web-based QA
systems.
In the question classification task, we extend pre-
vious studies, e.g. (Zhang and Lee, 2003; Moschitti,
2006), by testing a set of previously designed ker-
nels and their combination with our new Shallow Se-
mantic Tree Kernel. In the answer re-ranking task,
we approach the problem of detecting description
answers, among the most complex in the literature
(Cui et al., 2005; Kazawa et al., 2001).
The representations that we adopt are: bag-of-
words (BOW), bag-of-POS tags (POS), parse tree
(PT), predicate argument structure (PAS) and nested
PAS (PASN). BOW and POS are processed by
means of a linear kernel, PT is processed with TK,
PAS and PASN are processed by SSTK. We imple-
mented the proposed kernels in the SVM-light-TK
software available at ai-nlp.info.uniroma2.it/
moschitti/ which encodes tree kernel functions in
SVM-light (Joachims, 1999).
</bodyText>
<subsectionHeader confidence="0.997536">
4.1 Question classification
</subsectionHeader>
<bodyText confidence="0.994749045454545">
As a first experiment, we focus on question classi-
fication, for which benchmarks and baseline results
are available (Zhang and Lee, 2003; Li and Roth,
2005). We design a question multi-classifier by
combining n binary SVMs3 according to the ONE-
vs-ALL scheme, where the final output class is the
one associated with the most probable prediction.
The PASs were automatically derived by our SRL
3We adopted the default regularization parameter (i.e., the
average of 1/||x||) and tried a few cost-factor values to adjust
the rate between Precision and Recall on the development set.
system which achieves a 76% F1-measure (Mos-
chitti et al., 2005).
As benchmark data, we use the question train-
ing and test set available at: l2r.cs.uiuc.edu/
∼cogcomp/Data/QA/QC/, where the test set are the
500 TREC 2001 test questions (Voorhees, 2001).
We refer to this split as UIUC. The performance of
the multi-classifier and the individual binary classi-
fiers is measured with accuracy resp. F1-measure.
To collect statistically significant information, we
run 10-fold cross validation on the 6,000 questions.
</bodyText>
<table confidence="0.999406666666667">
Features Accuracy (UIUC) Accuracy (c.v.)
PT 90.4 84.8±1.2
BOW 90.6 84.7±1.2
PAS 34.2 43.0±1.9
POS 26.4 32.4±2.1
PT+BOW 91.8 86.1±1.1
PT+BOW+POS 91.8 84.7±1.5
PAS+BOW 90.0 82.1±1.3
PAS+BOW+POS 88.8 81.0±1.5
</table>
<tableCaption confidence="0.9476855">
Table 1: Accuracy of the question classifier with dif-
ferent feature combinations
</tableCaption>
<bodyText confidence="0.979356772727273">
Question classification results Table 1 shows the
accuracy of different question representations on the
UIUC split (Column 1) and the average accuracy ±
the corresponding confidence limit (at 90% signifi-
cance) on the cross validation splits (Column 2).(i)
The TK on PT and the linear kernel on BOW pro-
duce a very high result, i.e. about 90.5%. This is
higher than the best outcome derived in (Zhang and
Lee, 2003), i.e. 90%, obtained with a kernel combin-
ing BOW and PT on the same data. Combined with
PT, BOW reaches 91.8%, very close to the 92.5%
accuracy reached in (Li and Roth, 2005) using com-
plex semantic information from external resources.
(ii) The PAS feature provides no improvement. This
is mainly because at least half of the training and
test questions only contain the predicate “to be”, for
which a PAS cannot be derived by a PB-based shal-
low semantic parser.
(iii) The 10-fold cross-validation experiments con-
firm the trends observed in the UIUC split. The
best model (according to statistical significance) is
PT+BOW, achieving an 86.1% average accuracy4.
</bodyText>
<footnote confidence="0.9998015">
4This value is lower than the UIUC split one as the UIUC
test set is not consistent with the training set (it contains the
</footnote>
<page confidence="0.981964">
780
</page>
<subsectionHeader confidence="0.447453">
4.2 Answer classification
</subsectionHeader>
<bodyText confidence="0.977722677419355">
Question classification does not allow to fully ex-
ploit the PAS potential since questions tend to be
short and with few verbal predicates (i.e. the only
ones that our SRL system can extract). A differ-
ent scenario is answer classification, i.e. deciding
if a passage/sentence correctly answers a question.
Here, the semantics to be generated by the classi-
fier are not constrained to a small taxonomy and an-
swer length may make the PT-based representation
too sparse.
We learn answer classification with abinary SVM
which determines if an answer is correct for the tar-
get question: here, the classification instances are
(question, answer) pairs. Each pair component can
be encoded with PT, BOW, PAS and PASN repre-
sentations (processed by previous kernels).
As test data, we collected the 138 TREC 2001 test
questions labeled as “description” and for each, we
obtained a list of answer paragraphs extracted from
Web documents using YourQA. Each paragraph sen-
tence was manually evaluated based on whether it
contained an answer to the corresponding question.
Moreover, to simplify the classification problem, we
isolated for each paragraph the sentence which ob-
tained the maximal judgment (in case more than one
sentence in the paragraph had the same judgment,
we chose the first one). We collected a corpus con-
taining 1309 sentences, 416 of which – labeled “+1”
– answered the question either concisely or with
noise; the rest – labeled “-1”– were either irrele-
vant to the question or contained hints relating to the
</bodyText>
<figureCaption confidence="0.91919896">
question but could not be judged as valid answers5.
Answer classification results To test the impact
of our models on answer classification, we ran 5-fold
cross-validation, with the constraint that two pairs
(q, a1) and (q, a2) associated with the same ques-
tion q could not be split between training and test-
ing. Hence, each reported value is the average over 5
different outcomes. The standard deviations ranged
Figure 4: Impact of the BOW and PT features on
answer classification
Figure 5: Impact of the PAS and PASN features
combined with the BOW and PT features on answer
classification
TREC 2001 questions) and includes a larger percentage of eas-
ily classified question types, e.g. the numeric (22.6%) and de-
scription classes (27.6%) whose percentage in training is 16.4%
resp. 16.2%.
5For instance, given the question “What are invertebrates?”,
the sentence “At least 99% of all animal species are inverte-
brates, comprising ...” was labeled “-1” , while “Invertebrates
are animals without backbones.” was labeled “+1”.
Figure 6: Comparison between PAS and PASN
when used as standalone features for the answer on
answer classification
781
</figureCaption>
<bodyText confidence="0.999730561403509">
approximately between 2.5 and 5. The experiments
were organized as follows:
First, we examined the contributions of BOW and
PT representations as they proved very important for
question classification. Figure 4 reports the plot of
the F1-measure of answer classifiers trained with all
combinations of the above models according to dif-
ferent values of the cost-factor parameter, adjusting
the rate between Precision and Recall. We see here
that the most accurate classifiers are the ones using
both the answer’s BOW and PT feature and either
the question’s PT or BOW feature (i.e. Q(BOW) +
A(PT,BOW) resp. Q(PT) + A(PT,BOW) combina-
tions). When PT is used for the answer the sim-
ple BOW model is outperformed by 2 to 3 points.
Hence, we infer that both the answer’s PT and BOW
features are very useful in the classification task.
However, PT does not seem to provide additional
information to BOW when used for question repre-
sentation. This can be explained by considering that
answer classification (restricted to description ques-
tions) does not require question type classification
since its main purpose is to detect question/answer
relations. In this scenario, the question’s syntactic
structure does not seem to provide much more infor-
mation than BOW.
Secondly, we evaluated the impact of the newly
defined PAS and PASN features combined with the
best performing previous model, i.e. Q(BOW) +
A(PT,BOW). Figure 5 illustrates the F1-measure
plots again according to the cost-factor param-
eter. We observe here that model Q(BOW)
+ A(PT,BOW,PAS) greatly outperforms model
Q(BOW) + A(PT,BOW), proving that the PAS fea-
ture is very useful for answer classification, i.e.
the improvement is about 2 to 3 points while the
difference with the BOW model, i.e. Q(BOW)
+ A(BOW), exceeds 3 points. The Q(BOW) +
A(PT,BOW,PASN) model is not more effective than
Q(BOW) + A(PT,BOW,PAS). This suggests either
that PAS is more effective than PASN or that when
the PT information is added, the PASN contribution
fades out.
To further investigate the previous issue, we fi-
nally compared the contribution of the PAS and
PASN when combined with the question’s BOW
feature alone, i.e. no PT is used. The results, re-
ported in Figure 6, show that this time PASN per-
forms better than PAS. This suggests that the depen-
dencies between the nested PASs are in some way
captured by the PT information. Indeed, it should
be noted that we join predicates only in case one is
subordinate to the other, thus considering only a re-
stricted set of all possible predicate dependencies.
However, the improvement over PAS confirms that
PASN is the right direction to encode shallow se-
mantics from different sentence predicates.
</bodyText>
<table confidence="0.999936285714286">
Baseline P R F1-measure
Gg@5 39.22±3.59 33.15±4.22 35.92±3.95
QA@5 39.72±3.44 34.22±3.63 36.76±3.56
Gg@all 31.58±0.58 100 48.02±0.67
QA@all 31.58±0.58 100 48.02±0.67
Gg QA Re-ranker
MRR 48.97±3.77 56.21±3.18 81.12±2.12
</table>
<tableCaption confidence="0.9412545">
Table 2: Baseline classifiers accuracy and MRR of
YourQA (QA), Google (Gg) and the best re-ranker
</tableCaption>
<subsectionHeader confidence="0.992905">
4.3 Answer re-ranking
</subsectionHeader>
<bodyText confidence="0.99575">
The output of the answer classifier can be used to
re-rank the list of candidate answers of a QA sys-
tem. Starting from the top answer, each instance can
be classified based on its correctness with respect
to the question. If it is classified as correct its rank
is unchanged; otherwise it is pushed down, until a
lower ranked incorrect answer is found.
We used the answer classifier with the highest F1-
measure on the development set according to differ-
ent cost-factor values6. We applied such model to
the Google ranks and to the ranks of our Web-based
QA system, i.e. YourQA. The latter uses Web docu-
ments corresponding to the top 20 Google results for
the question. Then, each sentence in each document
is compared to the question via a blend of similar-
ity metrics used in the answer extraction phase to
select the most relevant sentence. A passage of up
to 750 bytes is then created around the sentence and
returned as an answer.
Table 2 illustrates the results of the answer classi-
fiers derived by exploiting Google (Gg) and YourQA
(QA) ranks: the top N ranked results are considered
as correct definitions and the remaining ones as in-
6However, by observing the curves in Fig. 5, the selected
parameters appear as pessimistic estimates for the best model
improvement: the one for BOW is the absolute maximum, but
an average one is selected for the best model.
</bodyText>
<page confidence="0.989559">
782
</page>
<bodyText confidence="0.999968423076923">
correct for different values of N. We show N = 5
and the maximum N (all), i.e. all the available an-
swers. Each measure is the average of the Precision,
Recall and F1-measure from cross validation. The
F1-measure of Google and YourQA are greatly out-
performed by our answer classifier.
The last row of Table 2 reports the MRR7
achieved by Google, YourQA (QA) and YourQA af-
ter re-ranking (Re-ranker). We note that Google is
outperformed by YourQA since its ranks are based
on whole documents, not on single passages. Thus
Google may rank a document containing several
sparsely distributed question words higher than doc-
uments with several words concentrated in one pas-
sage, which are more interesting. When the answer
classifier is applied to improve the YourQA ranking,
the MRR reaches 81.1%, rising by about 25%.
Finally, it is worth to note that the answer clas-
sifier based on Q(BOW)+A(BOW,PT,PAS) model
(parameterized as described) gave a 4% higher MRR
than the one based on the simple BOW features. As
an example, for question “What is foreclosure?”, the
sentence “Foreclosure means that the lender takes
possession of your home and sells it in order to get
its money back.” was correctly classified by the best
model, while BOW failed.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99992375">
In this paper, we have introduced new structures to
represent textual information in three question an-
swering tasks: question classification, answer classi-
fication and answer re-ranking. We have defined tree
structures (PAS and PASN) to represent predicate-
argument relations, which we automatically extract
using our SRL system. We have also introduced two
functions, SSTK and Kall, to exploit their repre-
sentative power.
Our experiments with SVMs and the above models
suggest that syntactic information helps tasks such
as question classification whereas semantic informa-
tion contained in PAS and PASN gives promising re-
sults in answer classification.
In the future, we aim to study ways to capture re-
lations between predicates so that more general se-
</bodyText>
<page confidence="0.635962">
7 The Mean Reciprocal Rank is defined as: MRR =
</page>
<bodyText confidence="0.9303535">
1 n 1 where n is the number of questions and rank
n L-�i=1 ranki , i
is the rank of the first correct answer to question i.
mantics can be encoded by PASN. Forms of general-
ization for predicates and arguments within PASNs
like LSA clusters, WordNet synsets and FrameNet
(roles and frames) information also appear as a
promising research area.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996194">
We thank the anonymous reviewers for their helpful sugges-
tions. Alessandro Moschitti would like to thank the AMI2 lab
at the University of Trento and the EU project LUNA “spoken
Language UNderstanding in multilinguAl communication sys-
tems” contract no 33549 for supporting part of his research.
</bodyText>
<sectionHeader confidence="0.999664" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891076923077">
J. Allan, J. Aslam, N. Belkin, and C. Buckley. 2002. Chal-
lenges in IR and language modeling. In Report of a Work-
shop at the University ofAmherst.
X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-
2005 shared task: SRL. In CoNLL-2005.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers
from definitional QA using language models. In ACL’06.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In ACL’02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke.
2004. The effect of document retrieval quality on factoid QA
performance. In SIGIR’04. ACM.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern mod-
els for definitional QA. In SIGIR’05. ACM.
T. Joachims. 1999. Making large-scale SVM learning practical.
In Advances in Kernel Methods - Support Vector Learning.
H. Kazawa, H. Isozaki, and E. Maeda. 2001. NTT question
answering system in TREC 2001. In TREC’01.
P. Kingsbury and M. Palmer. 2002. From Treebank to Prop-
Bank. In LREC’02.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling
question answering to the web. In WWW’01.
X. Li and D. Roth. 2005. Learning question classifiers: the role
of semantic information. Journ. Nat. Lang. Eng.
A. Moschitti, B. Coppola, A. Giuglea, and R. Basili. 2005.
Hierarchical semantic role labeling. In CoNLL 2005 shared
task.
A. Moschitti. 2006. Efficient convolution kernels for depen-
dency and constituent syntactic trees. In ECML’06.
S. Quarteroni and S. Manandhar. 2006. User modelling for
Adaptive Question Answering and Information Retrieval. In
FLAIRS’06.
E. M. Voorhees. 2001. Overview of the TREC 2001 QA track.
In TREC’01.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journ. ofMach. Learn. Res.
D. Zhang and W. Lee. 2003. Question classification using sup-
port vector machines. In SIGIR’03. ACM.
</reference>
<page confidence="0.998955">
783
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491849">
<title confidence="0.9998155">Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification</title>
<author confidence="0.999966">Alessandro Moschitti</author>
<affiliation confidence="0.999975">University of Trento</affiliation>
<address confidence="0.973986">38050 Povo di Trento Italy</address>
<email confidence="0.997181">moschitti@dit.unitn.it</email>
<author confidence="0.996183">Silvia Quarteroni</author>
<affiliation confidence="0.999421">The University of York</affiliation>
<address confidence="0.9234075">York YO10 5DD United Kingdom</address>
<email confidence="0.995599">silvia@cs.york.ac.uk</email>
<author confidence="0.999861">Roberto Basili</author>
<affiliation confidence="0.999478">Tor Vergata” University</affiliation>
<address confidence="0.9833115">Via del Politecnico 1 00133 Rome, Italy</address>
<email confidence="0.993621">basili@info.uniroma2.it</email>
<author confidence="0.744734">Suresh Manandhar</author>
<affiliation confidence="0.995067">The University of York</affiliation>
<address confidence="0.9243965">York YO10 5DD United Kingdom</address>
<email confidence="0.998142">suresh@cs.york.ac.uk</email>
<abstract confidence="0.99962">We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking. We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Aslam</author>
<author>N Belkin</author>
<author>C Buckley</author>
</authors>
<date>2002</date>
<booktitle>Challenges in IR and language modeling. In Report of a Workshop at the University ofAmherst.</booktitle>
<contexts>
<context position="4531" citStr="Allan et al., 2002" startWordPosition="667" endWordPosition="670">ts for answer classification on a corpus of answers to TREC-QA 2001 description questions. We created such dataset by using YourQA (Quarteroni and Manandhar, 2006), our basic Webbased QA system1. (c) The answer classifier increases the ranking accuracy of our QA system by about 25%. Our results show that PAS and syntactic parsing are promising methods to address tasks affected by data sparseness like question/answer categorization. 2 Encoding Shallow Semantic Structures Traditionally, information retrieval techniques are based on the bag-of-words (BOW) approach augmented by language modeling (Allan et al., 2002). When the task requires the use of more complex semantics, the above approaches are often inadequate to perform fine-level textual analysis. An improvement on BOW is given by the use of syntactic parse trees, e.g. for question classification (Zhang and Lee, 2003), but these, too are inadequate when dealing with definitional answers expressed by long and articulated sentences or even paragraphs. On the contrary, shallow semantic representations, bearing a more “compact” information, could prevent the sparseness of deep structural approaches and the weakness of BOW models. Initiatives such as P</context>
</contexts>
<marker>Allan, Aslam, Belkin, Buckley, 2002</marker>
<rawString>J. Allan, J. Aslam, N. Belkin, and C. Buckley. 2002. Challenges in IR and language modeling. In Report of a Workshop at the University ofAmherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL2005 shared task: SRL.</title>
<date>2005</date>
<booktitle>In CoNLL-2005.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL2005 shared task: SRL. In CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>M Zhou</author>
<author>S Wang</author>
</authors>
<title>Reranking answers from definitional QA using language models.</title>
<date>2006</date>
<booktitle>In ACL’06.</booktitle>
<contexts>
<context position="2044" citStr="Chen et al., 2006" startWordPosition="298" endWordPosition="301">ered on question classification, which selects one of k expected answer classes. Most accurate models apply supervised machine learning techniques, e.g. SNoW (Li and Roth, 2005), where questions are encoded using various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover</context>
</contexts>
<marker>Chen, Zhou, Wang, 2006</marker>
<rawString>Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers from definitional QA using language models. In ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL’02.</booktitle>
<contexts>
<context position="2259" citStr="Collins and Duffy, 2002" startWordPosition="335" endWordPosition="338">ing various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We ar</context>
<context position="9461" citStr="Collins and Duffy, 2002" startWordPosition="1412" endWordPosition="1415">esenting tree structures in learning machines. Section 3 introduces a viable approach based on tree kernels. 3 Syntactic and Semantic Kernels for Text As mentioned above, encoding syntactic/semantic information represented by means of tree structures in the learning algorithm is problematic. A first solution is to use all its possible substructures as features. Given the combinatorial explosion of considering subparts, the resulting feature space is usually very large. A tree kernel (TK) function which computes the number of common subtrees between two syntactic parse trees has been given in (Collins and Duffy, 2002). Unfortunately, such subtrees are subject to the constraint that their nodes are taken with all or none of the children they have in the original tree. This makes the TK function not well suited for the PAS trees defined above. For instance, although the two PASs of Figure 1 share most of the subtrees rooted in the PAS node, Collins and Duffy’s kernel would compute no match. In the next section we describe a new kernel derived from the above tree kernel, able to evaluate the meaningful substructures for PAS trees. Moreover, as a single PAS may not be sufficient for text representation, we pro</context>
<context position="11206" citStr="Collins and Duffy, 2002" startWordPosition="1744" endWordPosition="1747">uted as follows: (1) if the productions (i.e. the nodes with their direct children) at n1 and n2 are different then A(n1, n2) = 0; (2) if the productions at n1 and n2 are the same, and n1 and n2 only have leaf children (i.e. they are preterminal symbols) then A(n1, n2) = 1; (3) if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then A(n1, n2) = Hnc(11)(1+A(cjn1, c2 jn)), where nc(n1) is the numj= ber of children of n1 and cjn is the j-th child of n. Such tree kernel can be normalized and a λ factor can be added to reduce the weight of large structures (refer to (Collins and Duffy, 2002) for a complete description). The critical aspect of steps (1), (2) and (3) is that the productions of two evaluated nodes have to be identical to allow the match of further descendants. This means that common substructures cannot be composed by a node with only some of its 778 SLOT rel define SLOT ARG1 antigens PAS SLOT ARG2 PAS SLOT ARGM-TMP originally SLOT rel define SLOT ARG1 antigens PAS SLOT null SLOT null SLOT rel define SLOT null PAS SLOT ARG2 PAS SLOT null * * * * * (a) (b) (c) Figure 3: A PAS with some of its fragments. children as an effective PAS representation would require. We so</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>The effect of document retrieval quality on factoid QA performance.</title>
<date>2004</date>
<booktitle>In SIGIR’04.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="1804" citStr="Collins-Thompson et al., 2004" startWordPosition="258" endWordPosition="261">ned to a question in natural language in the form of sentences or phrases. The typical QA system architecture consists of three phases: question processing, document retrieval and answer extraction (Kwok et al., 2001). Question processing is often centered on question classification, which selects one of k expected answer classes. Most accurate models apply supervised machine learning techniques, e.g. SNoW (Li and Roth, 2005), where questions are encoded using various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extractio</context>
</contexts>
<marker>Collins-Thompson, Callan, Terra, Clarke, 2004</marker>
<rawString>K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke. 2004. The effect of document retrieval quality on factoid QA performance. In SIGIR’04. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Kan</author>
<author>T Chua</author>
</authors>
<title>Generic soft pattern models for definitional QA.</title>
<date>2005</date>
<booktitle>In SIGIR’05. ACM. T. Joachims.</booktitle>
<contexts>
<context position="16106" citStr="Cui et al., 2005" startWordPosition="2658" endWordPosition="2661"> and BOW features. 4 Experiments The purpose of our experiments is to study the impact of the new representations introduced earlier for QA tasks. In particular, we focus on question classification and answer re-ranking for Web-based QA systems. In the question classification task, we extend previous studies, e.g. (Zhang and Lee, 2003; Moschitti, 2006), by testing a set of previously designed kernels and their combination with our new Shallow Semantic Tree Kernel. In the answer re-ranking task, we approach the problem of detecting description answers, among the most complex in the literature (Cui et al., 2005; Kazawa et al., 2001). The representations that we adopt are: bag-ofwords (BOW), bag-of-POS tags (POS), parse tree (PT), predicate argument structure (PAS) and nested PAS (PASN). BOW and POS are processed by means of a linear kernel, PT is processed with TK, PAS and PASN are processed by SSTK. We implemented the proposed kernels in the SVM-light-TK software available at ai-nlp.info.uniroma2.it/ moschitti/ which encodes tree kernel functions in SVM-light (Joachims, 1999). 4.1 Question classification As a first experiment, we focus on question classification, for which benchmarks and baseline r</context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern models for definitional QA. In SIGIR’05. ACM. T. Joachims. 1999. Making large-scale SVM learning practical.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kazawa</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>NTT question answering system in TREC</title>
<date>2001</date>
<booktitle>In Advances in Kernel Methods - Support</booktitle>
<contexts>
<context position="16128" citStr="Kazawa et al., 2001" startWordPosition="2662" endWordPosition="2665"> 4 Experiments The purpose of our experiments is to study the impact of the new representations introduced earlier for QA tasks. In particular, we focus on question classification and answer re-ranking for Web-based QA systems. In the question classification task, we extend previous studies, e.g. (Zhang and Lee, 2003; Moschitti, 2006), by testing a set of previously designed kernels and their combination with our new Shallow Semantic Tree Kernel. In the answer re-ranking task, we approach the problem of detecting description answers, among the most complex in the literature (Cui et al., 2005; Kazawa et al., 2001). The representations that we adopt are: bag-ofwords (BOW), bag-of-POS tags (POS), parse tree (PT), predicate argument structure (PAS) and nested PAS (PASN). BOW and POS are processed by means of a linear kernel, PT is processed with TK, PAS and PASN are processed by SSTK. We implemented the proposed kernels in the SVM-light-TK software available at ai-nlp.info.uniroma2.it/ moschitti/ which encodes tree kernel functions in SVM-light (Joachims, 1999). 4.1 Question classification As a first experiment, we focus on question classification, for which benchmarks and baseline results are available (</context>
</contexts>
<marker>Kazawa, Isozaki, Maeda, 2001</marker>
<rawString>In Advances in Kernel Methods - Support Vector Learning. H. Kazawa, H. Isozaki, and E. Maeda. 2001. NTT question answering system in TREC 2001. In TREC’01. P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In LREC’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C T Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the web. In WWW’01.</title>
<date>2001</date>
<contexts>
<context position="1391" citStr="Kwok et al., 2001" startWordPosition="199" endWordPosition="202">ntational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers. 1 Introduction Question answering (QA) is as a form of information retrieval where one or more answers are returned to a question in natural language in the form of sentences or phrases. The typical QA system architecture consists of three phases: question processing, document retrieval and answer extraction (Kwok et al., 2001). Question processing is often centered on question classification, which selects one of k expected answer classes. Most accurate models apply supervised machine learning techniques, e.g. SNoW (Li and Roth, 2005), where questions are encoded using various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more usefu</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling question answering to the web. In WWW’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers: the role of semantic information.</title>
<date>2005</date>
<journal>Journ. Nat. Lang. Eng.</journal>
<contexts>
<context position="1603" citStr="Li and Roth, 2005" startWordPosition="231" endWordPosition="234">ontribution when a reliable set of PASs can be extracted, e.g. from answers. 1 Introduction Question answering (QA) is as a form of information retrieval where one or more answers are returned to a question in natural language in the form of sentences or phrases. The typical QA system architecture consists of three phases: question processing, document retrieval and answer extraction (Kwok et al., 2001). Question processing is often centered on question classification, which selects one of k expected answer classes. Most accurate models apply supervised machine learning techniques, e.g. SNoW (Li and Roth, 2005), where questions are encoded using various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use</context>
<context position="16767" citStr="Li and Roth, 2005" startWordPosition="2760" endWordPosition="2763"> that we adopt are: bag-ofwords (BOW), bag-of-POS tags (POS), parse tree (PT), predicate argument structure (PAS) and nested PAS (PASN). BOW and POS are processed by means of a linear kernel, PT is processed with TK, PAS and PASN are processed by SSTK. We implemented the proposed kernels in the SVM-light-TK software available at ai-nlp.info.uniroma2.it/ moschitti/ which encodes tree kernel functions in SVM-light (Joachims, 1999). 4.1 Question classification As a first experiment, we focus on question classification, for which benchmarks and baseline results are available (Zhang and Lee, 2003; Li and Roth, 2005). We design a question multi-classifier by combining n binary SVMs3 according to the ONEvs-ALL scheme, where the final output class is the one associated with the most probable prediction. The PASs were automatically derived by our SRL 3We adopted the default regularization parameter (i.e., the average of 1/||x||) and tried a few cost-factor values to adjust the rate between Precision and Recall on the development set. system which achieves a 76% F1-measure (Moschitti et al., 2005). As benchmark data, we use the question training and test set available at: l2r.cs.uiuc.edu/ ∼cogcomp/Data/QA/QC/</context>
<context position="18577" citStr="Li and Roth, 2005" startWordPosition="3049" endWordPosition="3052">different feature combinations Question classification results Table 1 shows the accuracy of different question representations on the UIUC split (Column 1) and the average accuracy ± the corresponding confidence limit (at 90% significance) on the cross validation splits (Column 2).(i) The TK on PT and the linear kernel on BOW produce a very high result, i.e. about 90.5%. This is higher than the best outcome derived in (Zhang and Lee, 2003), i.e. 90%, obtained with a kernel combining BOW and PT on the same data. Combined with PT, BOW reaches 91.8%, very close to the 92.5% accuracy reached in (Li and Roth, 2005) using complex semantic information from external resources. (ii) The PAS feature provides no improvement. This is mainly because at least half of the training and test questions only contain the predicate “to be”, for which a PAS cannot be derived by a PB-based shallow semantic parser. (iii) The 10-fold cross-validation experiments confirm the trends observed in the UIUC split. The best model (according to statistical significance) is PT+BOW, achieving an 86.1% average accuracy4. 4This value is lower than the UIUC split one as the UIUC test set is not consistent with the training set (it cont</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>X. Li and D. Roth. 2005. Learning question classifiers: the role of semantic information. Journ. Nat. Lang. Eng.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>B Coppola</author>
<author>A Giuglea</author>
<author>R Basili</author>
</authors>
<title>Hierarchical semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoNLL</booktitle>
<note>shared task.</note>
<contexts>
<context position="3437" citStr="Moschitti et al., 2005" startWordPosition="507" endWordPosition="510">ace) is a promising research direction. We argue that semantic structures can be used to characterize the relation between a question and a candidate answer. In this paper, we extensively study new structural representations, encoding parse trees, bag-of-words, POS tags and predicate argument structures (PASs) for question classification and answer re-ranking. We define new tree representations for both simple and nested PASs, i.e. PASs whose arguments are other predicates (Section 2). Moreover, we define new kernel functions to exploit PASs, which we automatically derive with our SRL system (Moschitti et al., 2005) (Section 3). Our experiments using SVMs and the above ker776 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics nels and data (Section 4) shows the following: (a) our approach reaches state-of-the-art accuracy on question classification. (b) PB predicative structures are not effective for question classification but show promising results for answer classification on a corpus of answers to TREC-QA 2001 description questions. We created such dataset by using Y</context>
<context position="17253" citStr="Moschitti et al., 2005" startWordPosition="2837" endWordPosition="2841">nt, we focus on question classification, for which benchmarks and baseline results are available (Zhang and Lee, 2003; Li and Roth, 2005). We design a question multi-classifier by combining n binary SVMs3 according to the ONEvs-ALL scheme, where the final output class is the one associated with the most probable prediction. The PASs were automatically derived by our SRL 3We adopted the default regularization parameter (i.e., the average of 1/||x||) and tried a few cost-factor values to adjust the rate between Precision and Recall on the development set. system which achieves a 76% F1-measure (Moschitti et al., 2005). As benchmark data, we use the question training and test set available at: l2r.cs.uiuc.edu/ ∼cogcomp/Data/QA/QC/, where the test set are the 500 TREC 2001 test questions (Voorhees, 2001). We refer to this split as UIUC. The performance of the multi-classifier and the individual binary classifiers is measured with accuracy resp. F1-measure. To collect statistically significant information, we run 10-fold cross validation on the 6,000 questions. Features Accuracy (UIUC) Accuracy (c.v.) PT 90.4 84.8±1.2 BOW 90.6 84.7±1.2 PAS 34.2 43.0±1.9 POS 26.4 32.4±2.1 PT+BOW 91.8 86.1±1.1 PT+BOW+POS 91.8 8</context>
</contexts>
<marker>Moschitti, Coppola, Giuglea, Basili, 2005</marker>
<rawString>A. Moschitti, B. Coppola, A. Giuglea, and R. Basili. 2005. Hierarchical semantic role labeling. In CoNLL 2005 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML’06.</booktitle>
<contexts>
<context position="2363" citStr="Moschitti, 2006" startWordPosition="352" endWordPosition="353">ieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We argue that semantic structures can be used to characterize the relation between a question and a candidate</context>
<context position="15844" citStr="Moschitti, 2006" startWordPosition="2616" endWordPosition="2617">text fragment t and t&apos;. We define: �Kall(Pt, Pt�) = E SSTK(p, p% (2) pEPt p&apos;EPt/ While during the experiments (Sect. 4) the Kall kernel is used to handle predicate argument structures, TK (Eq. 1) is used to process parse trees and the linear kernel to handle POS and BOW features. 4 Experiments The purpose of our experiments is to study the impact of the new representations introduced earlier for QA tasks. In particular, we focus on question classification and answer re-ranking for Web-based QA systems. In the question classification task, we extend previous studies, e.g. (Zhang and Lee, 2003; Moschitti, 2006), by testing a set of previously designed kernels and their combination with our new Shallow Semantic Tree Kernel. In the answer re-ranking task, we approach the problem of detecting description answers, among the most complex in the literature (Cui et al., 2005; Kazawa et al., 2001). The representations that we adopt are: bag-ofwords (BOW), bag-of-POS tags (POS), parse tree (PT), predicate argument structure (PAS) and nested PAS (PASN). BOW and POS are processed by means of a linear kernel, PT is processed with TK, PAS and PASN are processed by SSTK. We implemented the proposed kernels in the</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Quarteroni</author>
<author>S Manandhar</author>
</authors>
<title>User modelling for Adaptive Question Answering and Information Retrieval.</title>
<date>2006</date>
<booktitle>In FLAIRS’06.</booktitle>
<contexts>
<context position="4075" citStr="Quarteroni and Manandhar, 2006" startWordPosition="599" endWordPosition="602">on 3). Our experiments using SVMs and the above ker776 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics nels and data (Section 4) shows the following: (a) our approach reaches state-of-the-art accuracy on question classification. (b) PB predicative structures are not effective for question classification but show promising results for answer classification on a corpus of answers to TREC-QA 2001 description questions. We created such dataset by using YourQA (Quarteroni and Manandhar, 2006), our basic Webbased QA system1. (c) The answer classifier increases the ranking accuracy of our QA system by about 25%. Our results show that PAS and syntactic parsing are promising methods to address tasks affected by data sparseness like question/answer categorization. 2 Encoding Shallow Semantic Structures Traditionally, information retrieval techniques are based on the bag-of-words (BOW) approach augmented by language modeling (Allan et al., 2002). When the task requires the use of more complex semantics, the above approaches are often inadequate to perform fine-level textual analysis. An</context>
</contexts>
<marker>Quarteroni, Manandhar, 2006</marker>
<rawString>S. Quarteroni and S. Manandhar. 2006. User modelling for Adaptive Question Answering and Information Retrieval. In FLAIRS’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>QA track.</title>
<date>2001</date>
<journal>Overview of the TREC</journal>
<booktitle>In TREC’01.</booktitle>
<contexts>
<context position="17441" citStr="Voorhees, 2001" startWordPosition="2869" endWordPosition="2870">ry SVMs3 according to the ONEvs-ALL scheme, where the final output class is the one associated with the most probable prediction. The PASs were automatically derived by our SRL 3We adopted the default regularization parameter (i.e., the average of 1/||x||) and tried a few cost-factor values to adjust the rate between Precision and Recall on the development set. system which achieves a 76% F1-measure (Moschitti et al., 2005). As benchmark data, we use the question training and test set available at: l2r.cs.uiuc.edu/ ∼cogcomp/Data/QA/QC/, where the test set are the 500 TREC 2001 test questions (Voorhees, 2001). We refer to this split as UIUC. The performance of the multi-classifier and the individual binary classifiers is measured with accuracy resp. F1-measure. To collect statistically significant information, we run 10-fold cross validation on the 6,000 questions. Features Accuracy (UIUC) Accuracy (c.v.) PT 90.4 84.8±1.2 BOW 90.6 84.7±1.2 PAS 34.2 43.0±1.9 POS 26.4 32.4±2.1 PT+BOW 91.8 86.1±1.1 PT+BOW+POS 91.8 84.7±1.5 PAS+BOW 90.0 82.1±1.3 PAS+BOW+POS 88.8 81.0±1.5 Table 1: Accuracy of the question classifier with different feature combinations Question classification results Table 1 shows the a</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>E. M. Voorhees. 2001. Overview of the TREC 2001 QA track. In TREC’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journ. ofMach. Learn. Res. D. Zhang</journal>
<booktitle>In SIGIR’03.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="2427" citStr="Zelenko et al., 2003" startWordPosition="360" endWordPosition="363">d selecting candidate answer passages from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We argue that semantic structures can be used to characterize the relation between a question and a candidate answer. In this paper, we extensively study new structural repr</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. Journ. ofMach. Learn. Res. D. Zhang and W. Lee. 2003. Question classification using support vector machines. In SIGIR’03. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>