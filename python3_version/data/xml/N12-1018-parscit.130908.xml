<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002041">
<title confidence="0.9572625">
Apples to Oranges: Evaluating Image Annotations from Natural Language
Processing Systems
</title>
<note confidence="0.441988333333333">
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
</note>
<email confidence="0.998943">
{rebecca,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.996662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999875333333333">
We examine evaluation methods for systems
that automatically annotate images using co-
occurring text. We compare previous datasets
for this task using a series of baseline mea-
sures inspired by those used in information re-
trieval, computer vision, and extractive sum-
marization. Some of our baselines match or
exceed the best published scores for those
datasets. These results illuminate incorrect as-
sumptions and improper practices regarding
preprocessing, evaluation metrics, and the col-
lection of gold image annotations. We con-
clude with a list of recommended practices for
future research combining language and vi-
sion processing techniques.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954894736842">
Automatic image annotation is an important area
with many applications such as tagging, generat-
ing captions, and indexing and retrieval on the web.
Given an input image, the goal is to generate rel-
evant descriptive keywords that describe the visual
content of the image. The Computer Vision (CV)
literature contains countless approaches to this task,
using a wide range of learning techniques and visual
features to identify aspects such as objects, people,
scenes, and events.
Text processing is computationally less expensive
than image processing and easily provides informa-
tion that is difficult to learn visually. For this reason,
most commerical image search websites identify the
semantic content of images using co-occurring text
exclusively. But co-occurring text is also a noisy
source for candidate annotations, since not all of the
text is visually relevant. Techniques from Natural
Language Processing help align descriptive words
and images. Some examples of previous research
use named-entity recognition to identify people in
images (Deschacht and Moens, 2007); term associa-
tion to estimate the “visualness” of candidate anno-
tations (Boiy et al., 2008; Leong et al., 2010); and
topic models to annotate images given both visual
and textual features (Feng and Lapata, 2010b).
Image annotation using NLP is still an emerging
area with many different tasks, datasets, and eval-
uation methods, making it impossible to compare
many recent systems to each other. Although there is
some effort being made towards establishing shared
tasks1, it is not yet clear which kinds of tasks and
datasets will provide interesting research questions
and practical applications in the long term. Until
then, establishing general “best practices” for NLP
image annotation will help advance and legitimitize
this work. In this paper, we propose some good prac-
tices and demonstrate why they are important.
</bodyText>
<sectionHeader confidence="0.996752" genericHeader="introduction">
2 Image Annotation Evaluation in CV and
NLP
</sectionHeader>
<bodyText confidence="0.999860166666667">
In this section, we first review related work in im-
age annotation evaluation in computer vision, spe-
cific challenges, and proposed solutions. We then
relate these challenges to the NLP image annotation
task and some of the specific problems we propose
to address.
</bodyText>
<footnote confidence="0.985702">
1http://imageclef.org/
</footnote>
<page confidence="0.914499">
172
</page>
<note confidence="0.564481">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 172–181,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.784132" genericHeader="method">
2.1 Related Work in Computer Vision
</sectionHeader>
<bodyText confidence="0.99970365">
The work of M¨uller et al. (2002) is one of the first
to address the issue of evaluation for image annota-
tion systems. While using the exact same annotation
system, dataset, and evaluation metric, they dramati-
cally improve the apparent performance of their sys-
tem by using dataset pruning heuristics.
Others have criticized commonly-used CV
datasets for being too “easy” — images with the
same keywords are extremely similar in low-level
features such as orientation, lighting, and color;
while differences between images with different
keywords are very clear (Westerveld and de Vries,
2003; Ponce et al., 2006; Herv´e and Boujemaa,
2007; Tang and Lewis, 2007). These features are
unwittingly exploited by certain algorithms and
obscure the benefits of using more complex tech-
niques (Ponce et al., 2006). The problem is further
exacerbated by evaluation metrics which essentially
prefer precision over recall and are biased towards
certain keywords. Annotations in test data might
not include all of the “correct” keywords, and
evaluation metrics need to account for the fact that
frequent keywords in the corpus are safer guesses
than keywords that appear less frequently (Monay
and Gatica-Perez, 2003).
New baseline techniques, evaluation metrics, and
datasets for image annotation have been developed
in response to these problems. Makadia et al. (2008;
2010) define a basic set of low-level features, and
propose new baselines for more complex systems to
evaluate against. Barnard et al. (2003) present a nor-
malized loss function to address the preference to-
wards precision in evaluation metrics. New datasets
are larger and provide more diverse images, and it is
now easy to obtain multiple human-annotations per
image thanks to distributed services such as Ama-
zon’s Mechanical Turk, and the ESP game (von
Ahn and Dabbish, 2004). Hanbury (2008) provides
an overview of popular CV annotation datasets and
methods used for building them.
</bodyText>
<subsectionHeader confidence="0.9961755">
2.2 Image Annotation using Natural Language
Processing
</subsectionHeader>
<bodyText confidence="0.9999482">
Many of the problems from CV image annotation
are also applicable to NLP image annotation, and
bringing NLP to the task brings new challenges as
well. One of these challenges is whether to allow
infrequent words to be pruned. In CV annotation it
is typical to remove infrequent terms from both the
keyword vocabulary and the evaluation data because
CV algorithms typically need a large number of ex-
amples to train on. However, using NLP systems
and baselines one can correctly annotate using key-
words that did not appear in the training set. Remov-
ing “unlearnable” keywords from evaluation data, as
done in (Boiy et al., 2008; Feng and Lapata, 2010b),
artificially inflates performance against simple base-
lines such as term frequency.
Nearly all NLP annotation datasets use naturally-
occurring sources of images and text. A particu-
larly popular source is news images alongside cap-
tions or articles, which are collected online from
sources such as Yahoo! News (Berg et al., 2004; De-
schacht and Moens, 2007). There are also domain-
specific databases with images and descriptions such
as the art, antiques, and flowers corpora used in Boiy
et al. (2008). Wikipedia has also been used as a
source of images and associated text (Tsikrika et al.,
2011). These sources typically offer well-written
and cleanly formatted text but introduce the problem
of converting text into annotations, and the annota-
tions may not meet the requirements of the new task
(as shown in Section 3.1). Obtaining data via image
search engines is a common practice in CV (Fei-Fei
et al., 2004; Berg and Forsyth, 2006) and can also
be used to provide more challenging and diverse in-
stances of images and co-occurring text. The addi-
tional challenge for NLP is that text content on many
websites is written to improve their rank in search
engines, using techniques such as listing dozens of
popular keywords. Co-occurring text for retrieved
images on popular queries may not be representative
of the task to be performed.
</bodyText>
<sectionHeader confidence="0.998001" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.999852375">
In this paper, we examine two established image an-
notation datasets: the BBC News Dataset of Feng
and Lapata (2008) (henceforth referred to as BBC),
and the general web dataset of Leong et al. (2010)
(henceforth referred to as UNT). These datasets
were both built to evaluate image annotation systems
that use longer co-occurring text such as a news ar-
ticle or a webpage, but they use data from differ-
</bodyText>
<page confidence="0.997246">
173
</page>
<table confidence="0.858817384615385">
Dataset BBC UNT
data instances article, image, and caption from a image and text from a webpage
news story
source of data scraped from BBC News website Google Image Search results
candidate keywords or descriptive unigram words from n &lt; 7-grams extracted from co-
collocations for anno- training data occurring text; collocations must ap-
tation pear as article names on Wikipedia
gold annotations descriptive words from held-out im- multiple human-authored annota-
age captions tions for each image
evaluation metric precision and recall against gold an- metrics adapted from evaluation of
notations lexical substitutions (SemEval)
number of train in- 3121 instances of related news arti- none (train using cross-validation)
stances cle, image, and caption
number of test in- 240 instances of news article and re- 300 instances of webpage with text
stances lated image and image
preprocessing proce- lemmatize tokens, remove from stem all tokens
dure dataset all words that are not descrip-
tive or that appear fewer than five
times in training articles
average number of 169 word tokens per article, 4.5 per 278 word tokens per webpage
text tokens after caption
preprocessing
average document title 4 word tokens 6 word tokens
length
total vocabulary after 10479 word types 8409 word types
preprocessing
</table>
<tableCaption confidence="0.999716">
Table 1: Comparison of the BBC and UNT image annotation datasets.
</tableCaption>
<bodyText confidence="0.996963833333333">
ent domains, different sources of gold image anno-
tations, different preprocessing procedures, and dif-
ferent evaluation measures.
Table 1 provides an overview of the datasets;
while this section covers the source of the datsets
and their gold annotations in more detail.
</bodyText>
<subsectionHeader confidence="0.994705">
3.1 BBC
</subsectionHeader>
<bodyText confidence="0.998035">
The BBC Dataset (Feng and Lapata, 2008)2 contains
news articles, image captions, and images taken
from the BBC News website. Training instances
consist of a news article, image, and image caption
from the same news story. Test instances are just the
image and the article, and hold-out the caption as a
source of gold image annotations.
Using news image captions as annotations has
</bodyText>
<footnote confidence="0.9656945">
2Downloaded from http://homepages.inuf.ed.
ac.uk/s0677528/data.html
</footnote>
<bodyText confidence="0.9999344">
the disadvantage that captions often describe back-
ground information or relate the photo to the story,
rather than listing important entities in the image.
It also fails to capture variation in how humans de-
scribe images, since it is limited to one caption per
image.3 However, captions are a cheap source of
data; BBC has ten times as many images as UNT.
To address the problem of converting natural lan-
guage into annotations, a large amount of prepro-
cessing is performed. The established preprocessing
procedure for this dataset is to lemmatize and POS-
tag using TreeTagger (Schmid, 1994) then remove
all but the “descriptive” words (defined as nouns, ad-
jectives, and certain classes of verbs). This leaves
a total text vocabulary of about 32K words, which
</bodyText>
<footnote confidence="0.821731666666667">
3The Pascal Sentences dataset (vision.cs.uiuc.edu/
pascal-sentences) provides multiple captions per image,
but they are not naturally-occurring.
</footnote>
<page confidence="0.996201">
174
</page>
<bodyText confidence="0.9998935">
is further reduced by removing words that appear
fewer than five times in the training set articles. Ta-
ble 1 shows the number of word tokens and types
after performing these steps.4
</bodyText>
<subsectionHeader confidence="0.998203">
3.2 UNT
</subsectionHeader>
<bodyText confidence="0.999686736842105">
The UNT Dataset (Leong et al., 2010)5 consists
of images and co-occurring text from webpages.
The webpages are found by querying Google Image
Search with frequent English words, and randomly
selecting from the results.
Each image in UNT is annotated by five people
via Mechanical Turk. In order to make human and
system results comparable, human annotators are re-
quired to only select words and collocations that are
directly extracted from the text, and the gold anno-
tations are the count of how many times each key-
word or collocation is selected. The human annota-
tors write keywords into a text box; while the col-
locations are presented as a list of candidates and
annotators mark which ones are relevant. Human
annotators tend to select subsets of collocations in
addition to the entire collocation. For example, the
gold annotation for one image has “university of
texas”, “university of texas at dallas”, “the univer-
sity of texas”, and “the university of texas at dal-
las”, each selected by at least four of the five an-
notators. Additionally, annotators can select mul-
tiple forms of the same word (such as “tank” and
“tanks”). Gold annotations are stemmed after they
are collected, and keywords with the same stem have
their counts merged. For this reason, many key-
words have a higher count than the number of an-
notators.
4 We are unable to reproduce work from Feng &amp; Lapata
(2008; 2010a; 2010b) and Feng (2011). Specifically, our vocab-
ulary counts after preprocessing (as in Table 1) are much higher
than reported counts, although the number of tokens per arti-
cle/caption they report is higher than ours. We have contacted
the authors, who confirmed that they took additional steps to re-
duce the size of the vocabulary, but were unable to tell us exactly
what those steps are. Therefore, all system and baseline scores
presented on their dataset are of our own implementation, and
do not match those reported in previous publications.
</bodyText>
<footnote confidence="0.947138">
5Downloaded from http://lit.csci.unt.edu/
index.php?P=research/downloads
</footnote>
<sectionHeader confidence="0.975469" genericHeader="method">
4 Baselines
</sectionHeader>
<bodyText confidence="0.999979">
We run several baselines on the datasets. Term fre-
quency, tf*idf, and corpus frequency are features
that are often used in annotation systems, so it is im-
portant to test them on their own. Document Title
and tf*idf are both baselines that were used in the
original papers where these datasets came from.
Sentence extraction is a new baseline that we pro-
pose specifically for the BBC dataset, in order see if
we can exploit certain properties of the gold annota-
tions, which are also derived from sentences.
</bodyText>
<subsectionHeader confidence="0.99919">
4.1 Term Frequency
</subsectionHeader>
<bodyText confidence="0.999976090909091">
Term frequency has been shown to be a power-
ful feature in summarization (Nenkova and Vander-
wende, 2005). Words that appear frequently are
considered more meaningful than infrequent words.
Term frequency is the number of times a term (ex-
cluding function words) appears in a document, di-
vided by the total number of terms in that document.
On the UNT dataset we use the stopword list in-
cluded with the MALLET6 toolkit, while the BBC
dataset doesn’t matter because the function words
have already been removed.
</bodyText>
<subsectionHeader confidence="0.997946">
4.2 tf*idf
</subsectionHeader>
<bodyText confidence="0.999827">
While term frequency baseline requires the use of an
ad hoc function word list, tf*idf adjusts the weights
of different words depending on how important they
are in the corpus. It is a standard baseline used for
information retrieval tasks, based on the intuition
that a word that appears in a smaller number of doc-
uments is more likely to be meaningful than a word
that appears in many documents.
tf*idf is the product of term frequency and inverse
document frequency – idf(ti) = log n where N is
the number of documents in the corpus, and ni is
the number of documents that contain the term ti.
For the BBC Dataset, we base the idf weights on
the document frequency of the training articles. For
UNT, we use the reported tf*idf score which uses the
British National Corpus to calculate the idf scores.7
</bodyText>
<footnote confidence="0.9437075">
6mallet.cs.umass.edu
7We also ran tf*idf where for each document we recalcu-
late idf using the other 299, but it didn’t make any meaningful
difference.
</footnote>
<page confidence="0.992016">
175
</page>
<subsectionHeader confidence="0.998463">
4.3 Corpus Frequency
</subsectionHeader>
<bodyText confidence="0.9999774">
Image annotations in both NLP and CV tend to be
distributed with a relatively small number of fre-
quently occuring keywords, and a long tail of key-
words that only appear a few times. For UNT, we
use the total keyword frequency of all the gold an-
notations, except for the one document that we are
currently scoring. For BBC, we only measure the
frequency of keywords in the training set captions,
since we are specifically interested in the frequency
of terms in captions.
</bodyText>
<subsectionHeader confidence="0.998887">
4.4 Document Title
</subsectionHeader>
<bodyText confidence="0.9998445">
For BBC, the news article headline, and for UNT,
the title of the webpage.
</bodyText>
<subsectionHeader confidence="0.994053">
4.5 Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.999973875">
Our baseline extracts the most central sentence from
the co-occurring text and uses descriptive words
from that sentence as the image annotation. Un-
like sentence extraction techniques from Feng and
Lapata (2010a), we determine which sentence to ex-
tract using the term frequency distribution directly.
We extract the sentence with the minimum KL-
divergence to the entire document.8
</bodyText>
<sectionHeader confidence="0.996078" genericHeader="method">
5 BBC Dataset Experiments
</sectionHeader>
<subsectionHeader confidence="0.980764">
5.1 System Comparison
</subsectionHeader>
<bodyText confidence="0.999970857142857">
In addition to the baselines, we compare against the
Mix LDA system from Feng and Lapata (2010b). In
Mix LDA, each instance is represented as a bag of
textual features (unigrams) and visual features (SIFT
features quantized to discrete “image words” using
k-means). A Latent Dirichlet Allocation topic model
is trained on articles, images, and captions from the
training set. Keywords are generated for an unseen
image and article pair by estimating the distribution
of topics that generates the test instance, then multi-
plying them with the word distributions in each topic
to find the probability of textual keywords for the
image. Text LDA is is the same model but only us-
ing words and not image features.
</bodyText>
<footnote confidence="0.943568">
8One could also think of this as a version of the KLSum
summarization system (Haghighi and Vanderwende, 2009) that
stops after one sentence.
</footnote>
<subsectionHeader confidence="0.986772">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999988636363636">
The evaluation metric and the source of gold anno-
tations is described in Table 1. For the baselines 4.1,
4.2, 4.3 and the Mix LDA system, the generated an-
notation for each test image is its ten most likely
keywords. We also run all baselines and the Mix
LDA system on an unpruned version of the dataset,
where infrequent terms are not removed from train-
ing data, test data, or the gold annotations. The pur-
pose of this evaluation is to see if candidate key-
words deemd “unlearnable” by the Mix LDA system
can be learned by the baselines.
</bodyText>
<subsectionHeader confidence="0.516859">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999989393939394">
The evaluation results for the BBC Dataset are
shown in Table 2. Clearly, term frequency is a
stronger baseline than tf*idf by a large margin. The
reason for this is simple: since nearly all of BBC’s
function words are removed during preprocessing,
the only words downweighted by the idf score are
common – but meaningful – words such as police or
government. This is worth pointing out because, in
many cases, the choice of using a term frequency or
tf*idf baseline is made based on what was used in
previous work. As we show here and in Section 6.3,
the choice of frequency baseline should be based on
the data and processing techniques being used.
We use the corpus frequency baseline to illus-
trate the difference between standard and include-
infrequent evaluations. Since including infrequent
words doesn’t change which are most frequent in
the dataset, precision for corpus frequency doesn’t
change. But since infrequent words are now in-
cluded in the evaluation data, we see a 0.5% drop in
recall (since corpus frequency won’t capture infre-
quent words). Compared to the other baselines, this
is not a large difference. Other baselines see a larger
drop in recall because they have both more gold key-
words to estimate and more candidate keywords to
consider. tf*idf is the most affected by this, because
idf overly favors very infrequent keywords, despite
their low term frequency. In comparison, the term
frequency baseline is not as negatively affected and
even improves in precision because there are some
cases where a word is very important to an article
in the test set but just didn’t appear very often in the
training set (see Table 3 for examples). But the base-
</bodyText>
<page confidence="0.986253">
176
</page>
<table confidence="0.999671555555556">
Standard F1 Include-infrequent
Precision Recall Precision Recall F1
Term Frequency 13.13 27.84 17.84 13.62 25.71 17.81
tf * idf 9.21 19.97 12.61 7.25 13.52 9.44
Doc Title 17.23 13.70 15.26 15.91 11.86 13.59
Corpus Frequency 3.17 6.52 4.26 3.17 6.02 4.15
Sentence Extraction 16.67 15.61 16.13 18.62 16.83 17.68
Mix LDA 7.30 16.16 10.06 7.50 13.98 9.76
Text LDA 8.38 17.46 11.32 7.79 14.52 10.14
</table>
<tableCaption confidence="0.997769">
Table 2: Image annotation results for previous systems and our proposed baselines on the BBC Dataset.
Table 3: Examples of gold annotations from the test sec-
</tableCaption>
<bodyText confidence="0.9972445">
tion of the BBC Dataset. The bolded words are the ones
that appear five or more times in the training set; the un-
bolded words appear fewer than five times and would be
removed from both the candidate and gold keywords in
the standard BBC evaluation.
lines with the best precision are the Doc Title and
Sentence Extraction baselines, which do not need to
generate ten keywords for every image.
While sentence extraction has a lower recall than
term frequency, it is the only baseline or system
that has improved recall when including infrequent
words. This is unexpected because our baseline se-
lects a sentence based on the term frequency of the
document, and the recall for term frequency fell.
One possible explanation is that extraction implic-
itly uses correlations between keywords. Probabili-
ties of objects appearing together in an image are not
independent; and the accuracy of annotations can be
improved by generating annotation keywords as a
set (Moran and Lavrenko, 2011). Recent works in
image captioning also use these correlations: explic-
itly, using graphical models (Kulkarni et al., 2011;
Yang et al., 2011); and implicitly, using language
models (Feng and Lapata, 2010a). In comparison,
sentence extraction is very implicit.
Unsurprisingly, the Text LDA and Mix LDA sys-
tems do worse on the include-infrequent evaluation
than they do on the standard, because words that
do not appear in the training set will not have high
probability in the trained topic models. We were un-
able to reproduce the reported scores for Mix LDA
from Feng and Lapata (2010b) where Mix LDA’s
scores were double the scores of Text LDA (see
Footnote 4). We were also unable to reproduce re-
ported scores for tf*idf and Doc Title (Feng and Lap-
ata, 2008). However, we have three reasons why we
believe our results are correct. First, BBC has more
keywords, and fewer images, than typically seen in
CV datasets. The BBC dataset is simply not suited
for learning from visual data. Second, a single SIFT
descriptor describes which way edges are oriented
at a certain point in an image (Lowe, 1999). While
certain types of edges may correlate to visual objects
also described in the text, we do not expect SIFT fea-
tures to be as informative as textual features for this
task. Third, we refer to the best system scores re-
ported by Leong et al. (2010), who evaluate their text
mining system (see section 6.1) on the standard BBC
dataset.9 While their f1 score is slightly worse than
our term frequency baseline, they do 4.86% better
than tf*idf. But, using the baselines reported in Feng
and Lapata (2008), their improvement over tf*idf is
12.06%. Next, we compare their system against fre-
quency baselines using the 10 keyword generation
task on the UNT dataset (the oot normal scores in
table 5). Their best system performs 4.45% better
</bodyText>
<footnote confidence="0.784178666666667">
9Combined model; precision: 13.38, recall: 25.17, f1:
17.47. Crucially, they do not reimplement previous systems or
baselines, but use scores reported from Feng and Lapata (2008).
</footnote>
<figure confidence="0.984552">
Cadbury increase
contamination
testing level
malaria parasite
spread mosquito
</figure>
<page confidence="0.985841">
177
</page>
<bodyText confidence="0.9997348">
than term frequency, and 0.55% worse than tf*idf.10
Although it is difficult to compare different datasets
and evaluation metrics, our baselines for BBC seem
more reasonable than the reported baselines, given
their relative performance to Leong et al’s system.
</bodyText>
<sectionHeader confidence="0.985701" genericHeader="method">
6 UNT Dataset Experiments
</sectionHeader>
<subsectionHeader confidence="0.997229">
6.1 System Comparison
</subsectionHeader>
<bodyText confidence="0.999954944444445">
We evaluate against the text mining system from
(Leong et al., 2010). Their system generates image
keywords by extracting text from the co-occurring
text of an image. It uses three features for select-
ing keywords. Flickr Picturability queries the Flickr
API with words from the text in order to find re-
lated image tags. Retrieved tags that appear as sur-
face forms in the text are rewarded proportional to
their frequency in the text. Wikipedia Salience as-
signs scores to words based on a graph-based mea-
sure of importance that considers each term’s docu-
ment frequency in Wikipedia. Pachinko Allocation
Model is a topic model that captures correlations be-
tween topics (Li and McCallum, 2006). PAM infers
subtopics and supertopics for the text, then retrieves
top words from the top topics as annotations. There
is also a combined model of these features using an
SVM with 10-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.995395">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998208833333333">
Evaluation on UNT uses a framework originally de-
veloped for the SemEval lexical substitution task
(McCarthy and Navigli, 2007). This framework
accounts for disagreement between annotators by
weighting each generated keyword by the number of
human annotators who also selected that keyword.
The scoring framework consists of four evaluation
measures: best normal, best mode, oot (out-of-ten)
normal, and oot mode.11
The two best evaluations find the accuracy of a
single “best” keyword generated by the system12.
10And as we stated earlier, the relative performance of term
frequency vs tf*idf is different from dataset to dataset.
11Both the original framework and its adaptation by Leong
et al. (2010) give precision and recall for each of the evaluation
measures. However, precision and recall are identical for all
baselines and systems, and only slightly different on the upper
bound (human) scores. To preserve space, we only present the
metric and scores for precision.
12In contrast to the original SemEval task, where systems can
Best normal measures the accuracy for each system
annotation aj as the number of times aj appears in
the Rj, the multi-set union of human tags, and aver-
ages over all the test images.
</bodyText>
<equation confidence="0.971359333333333">
aj ∈Rj |
�ij ∈I |Rj|
III
</equation>
<bodyText confidence="0.953">
In oot normal, up to ten unordered guesses can be
made without penalty.
</bodyText>
<equation confidence="0.952224">
EajEAj |aj∈Rj|
Eij∈I |Rj|
III
</equation>
<bodyText confidence="0.99992425">
where Aj is the set of ten system annotations for
image ij.
The best mode and oot mode metrics are the same
as the normal metrics except they only evaluate sys-
tem annotations for images where Rj contains a sin-
gle most frequent tag. We use the scoring software
provided by SemEval13 with the gold annotation file
provided in the UNT Dataset.
</bodyText>
<subsectionHeader confidence="0.948354">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.995383045454545">
The results of the lexical substitution evaluation on
the UNT Dataset are shown in Table 5. The results
from the normal show support for our earlier idea
that the relative performance of term frequency vs
tf*idf depends on the dataset. Although the term fre-
quency baseline uses a stopword list, there are other
words that appear frequently enough to suggest they
are not meaningful to the document – such as copy-
right disclaimers.
Recall that the mode evaluation is only measured
on data instances where the gold annotations have
a single most frequent keyword. While running
the evaluation script on the gold annotation file that
came with the UNT dataset, we discover that Se-
mEval only identifies 28 of the 300 instances as hav-
ing a single mode annotation, and that for 21 of
those 28 instances, the mode keyword is “cartoon”.
Those 21/28 images correspond to the 75% best
mode score obtained by Corpus Frequency baseline.
Given the small number of instances that actually
make from zero to many “best” guesses, penalized by the total
number of guesses made.
</bodyText>
<footnote confidence="0.8797335">
13http://nlp.cs.swarthmore.edu/semeval/
tasks/task10/data.shtml
</footnote>
<equation confidence="0.966649722222222">
Bestnormal =
ootnormal =
178
cartoon(6), market(5), market share(5),
declin(3), imag(3), share(3), pictur(1),
illustr(1), cartoonstock(1), origin(1),
artist(1), meet(1), jfa0417(1), meeting-
copyright(1)
cartoon(6), bill gate(5), gate(4), monop-
oli(4), pearli gate(4), bill(3), imag(3),
caricatur(2), pictur(2), illustr(1), copy-
right(1), artist(1), own(1), pearli(1)
lift index(5), gener(3), index(3), con-
dit(2), comput(2), comput gener(2),
unstabl(2), zone(2), area(1), field(1),
between(1), stabl(1), encyclopedia(1),
thunderstorm(1), lift(1), free encyclope-
dia(1), wikipedia(1)
</equation>
<tableCaption confidence="0.998881">
Table 4: Examples of gold annotations from the UNT Dataset.
</tableCaption>
<table confidence="0.9999784">
Best Mode Out-of-ten (oot)
Normal Normal Mode
Term Frequency 5.67 14.29 33.40 89.29
tf * idf 5.94 14.29 38.40 78.57
Doc Title 6.40 7.14 35.19 92.86
Corpus Frequency 2.54 75.00 8.22 82.14
Flickr Picturability 6.32 78.57 35.61 92.86
Wikipedia Salience 6.40 7.14 35.19 92.86
Topic Model (PAM) 5.99 42.86 37.13 85.71
Combined (SVM) 6.87 67.49 37.85 100.00
</table>
<tableCaption confidence="0.999655">
Table 5: Image annotation results for our proposed baselines, the text mining systems from (Leong et al., 2010)
</tableCaption>
<bodyText confidence="0.99991547826087">
count towards these metrics, we conclude that mode
evaluation is not a meaningful way to compare im-
age annotation systems on the UNT dataset.
That said, the number of cartoons in the dataset
does seem to be strikingly high. Looking at the
source of the images, we find that 45 of the 300
images were collected from a single online cartoon
library. Predictably, we find that the co-occurring
text to these images contains a long list of keywords,
and little other text that is relevant to the image. We
looked at a small sample of the rest of the dataset
and found that many of the other text documents in
UNT also have keyword lists.
Including this types of text in a general web cor-
pus is not necessarily a problem, but it’s difficult to
measure the benefits of using complex techniques
like topic modeling and graph similarily to find and
extract annotations when in so many cases the anno-
tations have already been found and extracted. This
is shown in the normal evaluation results, where the
combined system is only slightly better at selecting
the single best keyword, and no better than tf*idf for
the out-of-ten measure.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9990398">
The intent of this paper is not to teach researchers
how to inflate their own results, but to encourage bet-
ter practices. With that purpose in mind, we make
the following suggestions regarding future work in
this area:
</bodyText>
<page confidence="0.99611">
179
</page>
<bodyText confidence="0.999146565217392">
Get to know your data. The ability to quickly
and cheaply collect very large – but very noisy – col-
lections of data from the internet is a great advance
for both NLP and CV research. However, there still
needs to be an appopriate match betwen the task be-
ing performed, the system being proposed, and the
dataset being used; and large noisy datasets can hide
unintended features or incorrect assumptions about
the data.
Use relevant gold annotations. Do not convert
other sources of data into annotations. When collect-
ing human annotations, avoid postprocessing steps
such as merging or deleting keywords that change
the annotators’ original intent. Keep an open di-
alogue with annotators about issues that they find
confusing, since that is a sign of an ill-formed task.
Preprocessing should be simple and reprodu-
cable. The use of different preprocessing proce-
dures affects the apparent performance of systems
and sometimes has unintended consequences.
Use strong baselines and compare to other work
only when appropriate. Systems developed for dif-
ferent tasks or datasets can make for misleading
comparisons if they don’t use all features available.
Strong baselines explicitly exploit low-level features
that are implicitly exploited by proposed systems, as
well as low-level features of the dataset.
Don’t remove keywords from gold annotations.
Just because keywords are impossible for one sys-
tem to learn, does not mean they are impossible for
all systems to learn. Removing evaluation data arti-
ficially inflates system scores and limits comparison
to related work.
If a proposed system is to learn associations be-
tween visual and textual features, then it is neces-
sary to use larger datasets. In general, global an-
notations, such as scenes, is easiest; identifying spe-
cific objects is more difficult; and identification of
events, activities, and other abstract qualities has a
very low success rate (Fluhr et al., 2006). Alter-
nately, use simpler image features that are known
to have a high sucess rate. For example, Deschacht
and Moens (2007) used a face detector to determine
the number of faces in an image, and then used NLP
to determine the names of those people from associ-
ated text.
</bodyText>
<sectionHeader confidence="0.983471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.953699372549019">
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
pictures. The Journal of Machine Learning Research,
3:1107–1135.
Tamara L. Berg and David A. Forsyth. 2006. Animals on
the web. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ’06, pages 1463–1470,
Washington, DC, USA. IEEE Computer Society.
T.L. Berg, A.C. Berg, J. Edwards, M. Maire, R. White,
Yee-Whye Teh, E. Learned-Miller, and D.A. Forsyth.
2004. Names and faces in the news. In Computer Vi-
sion and Pattern Recognition, 2004. CVPR 2004. Pro-
ceedings of the 2004 IEEE Computer Society Confer-
ence on, volume 2, pages II–848 – II–854 Vol.2, june-2
july.
E. Boiy, K. Deschacht, and M.-F. Moens. 2008. Learn-
ing visual entities and their visual attributes from text
corpora. In Database and Expert Systems Applica-
tion, 2008. DEXA ’08. 19th International Workshop
on, pages 48 –53, sept.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In ACL, vol-
ume 45, page 1000.
Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-
ing generative visual models from few training ex-
amples: An incremental bayesian approach tested on
101 object categories. In Proceedings of the 2004
Conference on Computer Vision and Pattern Recog-
nition Workshop (CVPRW’04) Volume 12 - Volume
12, CVPRW ’04, pages 178–, Washington, DC, USA.
IEEE Computer Society.
Yansong Feng and Mirella Lapata. 2008. Automatic im-
age annotation using auxiliary text information. Pro-
ceedings of ACL-08: HLT, pages 272–280.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831–839.
Yansong Feng. 2011. Automatic caption generation for
news images. Ph.D. thesis, University of Edinburgh.
Christian Fluhr, Pierre-Alain Mollic, and Patrick Hde.
2006. Usage-oriented multimedia information re-
trieval technological evaluation. In James Ze Wang,
Nozha Boujemaa, and Yixin Chen, editors, Multime-
dia Information Retrieval, pages 301–306. ACM.
</reference>
<page confidence="0.973518">
180
</page>
<reference confidence="0.999770210526316">
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362–370, Boulder, Colorado, June. Association
for Computational Linguistics.
Allan Hanbury. 2008. A survey of methods for image
annotation. J. Vis. Lang. Comput., 19:617–627, Octo-
ber.
Nicolas Herv´e and Nozha Boujemaa. 2007. Image an-
notation: which approach for realistic databases? In
Proceedings of the 6th ACM international conference
on Image and video retrieval, CIVR ’07, pages 170–
177, New York, NY, USA. ACM.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR, pages 1601–
1608.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING, pages 647–655.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: Dag-structured mixture models of topic correla-
tions. In Proceedings of the 23rd international confer-
ence on Machine learning, ICML ’06, pages 577–584,
New York, NY, USA. ACM.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The Pro-
ceedings of the Seventh IEEE International Confer-
ence on, volume 2, pages 1150 –1157 vol.2.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar.
2008. A new baseline for image annotation. In Pro-
ceedings of the 10th European Conference on Com-
puter Vision: Part III, ECCV ’08, pages 316–329,
Berlin, Heidelberg. Springer-Verlag.
A. Makadia, V. Pavlovic, and S. Kumar. 2010. Baselines
for image annotation. International Journal of Com-
puter Vision, 90(1):88–105.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 48–53.
Florent Monay and Daniel Gatica-Perez. 2003. On im-
age auto-annotation with latent space models. In Pro-
ceedings of the eleventh ACM international conference
on Multimedia, Multimedia ’03, pages 275–278, New
York, NY, USA. ACM.
S. Moran and V. Lavrenko. 2011. Optimal tag sets for
automatic image.
Henning M¨uller, St´ephane Marchand-Maillet, and
Thierry Pun. 2002. The truth about corel - evaluation
in image retrieval. In Proceedings of the International
Conference on Image and Video Retrieval, CIVR ’02,
pages 38–49, London, UK, UK. Springer-Verlag.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Technical report,
Microsoft Research.
J. Ponce, T. Berg, M. Everingham, D. Forsyth, M. Hebert,
S. Lazebnik, M. Marszalek, C. Schmid, B. Russell,
A. Torralba, C. Williams, J. Zhang, and A. Zisser-
man. 2006. Dataset issues in object recognition.
In Jean Ponce, Martial Hebert, Cordelia Schmid, and
Andrew Zisserman, editors, Toward Category-Level
Object Recognition, volume 4170 of Lecture Notes
in Computer Science, pages 29–48. Springer Berlin /
Heidelberg. 10.1007/11957959 2.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of international
conference on new methods in language processing,
volume 12, pages 44–49. Manchester, UK.
Jiayu Tang and Paul Lewis. 2007. A study of quality
issues for image auto-annotation with the corel data-
set. IEEE Transactions on Circuits and Systems for
Video Technology, Vol. 1(NO. 3):384–389, March.
T. Tsikrika, A. Popescu, and J. Kludas. 2011. Overview
of the wikipedia image retrieval task at imageclef
2011. In CLEF (Notebook Papers/LABs/Workshops):
CLEF.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, CHI ’04, pages 319–326, New York, NY,
USA. ACM.
Thijs Westerveld and Arjen P. de Vries. 2003. Ex-
perimental evaluation of a generative probabilistic im-
age retrieval model on ’easy’ data. In In Proceedings
of the SIGIR Multimedia Information Retrieval Work-
shop 2003, Aug.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing (EMNLP), Edinburgh,
Scotland.
</reference>
<page confidence="0.998376">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512103">
<title confidence="0.9897175">Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems</title>
<author confidence="0.933678">Mason</author>
<affiliation confidence="0.7565745">Brown Laboratory for Linguistic Information Processing Brown University, Providence, RI</affiliation>
<abstract confidence="0.9994226875">We examine evaluation methods for systems that automatically annotate images using cooccurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>D Forsyth</author>
<author>N De Freitas</author>
<author>D M Blei</author>
<author>M I Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, Forsyth, De Freitas, Blei, Jordan, 2003</marker>
<rawString>K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M. Blei, and M.I. Jordan. 2003. Matching words and pictures. The Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara L Berg</author>
<author>David A Forsyth</author>
</authors>
<title>Animals on the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ’06,</booktitle>
<pages>1463--1470</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6994" citStr="Berg and Forsyth, 2006" startWordPosition="1076" endWordPosition="1079">g et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). These sources typically offer well-written and cleanly formatted text but introduce the problem of converting text into annotations, and the annotations may not meet the requirements of the new task (as shown in Section 3.1). Obtaining data via image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listing dozens of popular keywords. Co-occurring text for retrieved images on popular queries may not be representative of the task to be performed. 3 Datasets In this paper, we examine two established image annotation datasets: the BBC News Dataset of Feng and Lapata (2008) (henceforth referred to as BBC), and the general web dataset of Leong e</context>
</contexts>
<marker>Berg, Forsyth, 2006</marker>
<rawString>Tamara L. Berg and David A. Forsyth. 2006. Animals on the web. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ’06, pages 1463–1470, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Berg</author>
<author>A C Berg</author>
<author>J Edwards</author>
<author>M Maire</author>
<author>R White</author>
<author>Yee-Whye Teh</author>
<author>E Learned-Miller</author>
<author>D A Forsyth</author>
</authors>
<title>Names and faces in the news.</title>
<date>2004</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>pages</pages>
<contexts>
<context position="6385" citStr="Berg et al., 2004" startWordPosition="974" endWordPosition="977">hms typically need a large number of examples to train on. However, using NLP systems and baselines one can correctly annotate using keywords that did not appear in the training set. Removing “unlearnable” keywords from evaluation data, as done in (Boiy et al., 2008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). These sources typically offer well-written and cleanly formatted text but introduce the problem of converting text into annotations, and the annotations may not meet the requirements of the new task (as shown in Section 3.1). Obtaining data via image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsy</context>
</contexts>
<marker>Berg, Berg, Edwards, Maire, White, Teh, Learned-Miller, Forsyth, 2004</marker>
<rawString>T.L. Berg, A.C. Berg, J. Edwards, M. Maire, R. White, Yee-Whye Teh, E. Learned-Miller, and D.A. Forsyth. 2004. Names and faces in the news. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–848 – II–854 Vol.2, june-2 july.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boiy</author>
<author>K Deschacht</author>
<author>M-F Moens</author>
</authors>
<title>Learning visual entities and their visual attributes from text corpora.</title>
<date>2008</date>
<booktitle>In Database and Expert Systems Application,</booktitle>
<pages>48--53</pages>
<contexts>
<context position="2085" citStr="Boiy et al., 2008" startWordPosition="300" endWordPosition="303">and easily provides information that is difficult to learn visually. For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1, it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation </context>
<context position="6034" citStr="Boiy et al., 2008" startWordPosition="919" endWordPosition="922">the problems from CV image annotation are also applicable to NLP image annotation, and bringing NLP to the task brings new challenges as well. One of these challenges is whether to allow infrequent words to be pruned. In CV annotation it is typical to remove infrequent terms from both the keyword vocabulary and the evaluation data because CV algorithms typically need a large number of examples to train on. However, using NLP systems and baselines one can correctly annotate using keywords that did not appear in the training set. Removing “unlearnable” keywords from evaluation data, as done in (Boiy et al., 2008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsi</context>
</contexts>
<marker>Boiy, Deschacht, Moens, 2008</marker>
<rawString>E. Boiy, K. Deschacht, and M.-F. Moens. 2008. Learning visual entities and their visual attributes from text corpora. In Database and Expert Systems Application, 2008. DEXA ’08. 19th International Workshop on, pages 48 –53, sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text analysis for automatic image annotation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>1000</pages>
<contexts>
<context position="1994" citStr="Deschacht and Moens, 2007" startWordPosition="285" endWordPosition="288">people, scenes, and events. Text processing is computationally less expensive than image processing and easily provides information that is difficult to learn visually. For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1, it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in</context>
<context position="6413" citStr="Deschacht and Moens, 2007" startWordPosition="978" endWordPosition="982">a large number of examples to train on. However, using NLP systems and baselines one can correctly annotate using keywords that did not appear in the training set. Removing “unlearnable” keywords from evaluation data, as done in (Boiy et al., 2008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). These sources typically offer well-written and cleanly formatted text but introduce the problem of converting text into annotations, and the annotations may not meet the requirements of the new task (as shown in Section 3.1). Obtaining data via image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be us</context>
</contexts>
<marker>Deschacht, Moens, 2007</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2007. Text analysis for automatic image annotation. In ACL, volume 45, page 1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Fei-Fei</author>
<author>Rob Fergus</author>
<author>Pietro Perona</author>
</authors>
<title>Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW’04) Volume 12 - Volume 12, CVPRW ’04,</booktitle>
<pages>178</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6969" citStr="Fei-Fei et al., 2004" startWordPosition="1072" endWordPosition="1075">ch as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). These sources typically offer well-written and cleanly formatted text but introduce the problem of converting text into annotations, and the annotations may not meet the requirements of the new task (as shown in Section 3.1). Obtaining data via image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listing dozens of popular keywords. Co-occurring text for retrieved images on popular queries may not be representative of the task to be performed. 3 Datasets In this paper, we examine two established image annotation datasets: the BBC News Dataset of Feng and Lapata (2008) (henceforth referred to as BBC), and the gener</context>
</contexts>
<marker>Fei-Fei, Fergus, Perona, 2004</marker>
<rawString>Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW’04) Volume 12 - Volume 12, CVPRW ’04, pages 178–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>272--280</pages>
<contexts>
<context position="7522" citStr="Feng and Lapata (2008)" startWordPosition="1165" endWordPosition="1168">image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listing dozens of popular keywords. Co-occurring text for retrieved images on popular queries may not be representative of the task to be performed. 3 Datasets In this paper, we examine two established image annotation datasets: the BBC News Dataset of Feng and Lapata (2008) (henceforth referred to as BBC), and the general web dataset of Leong et al. (2010) (henceforth referred to as UNT). These datasets were both built to evaluate image annotation systems that use longer co-occurring text such as a news article or a webpage, but they use data from differ173 Dataset BBC UNT data instances article, image, and caption from a image and text from a webpage news story source of data scraped from BBC News website Google Image Search results candidate keywords or descriptive unigram words from n &lt; 7-grams extracted from cocollocations for anno- training data occurring t</context>
<context position="9493" citStr="Feng and Lapata, 2008" startWordPosition="1480" endWordPosition="1483">er of 169 word tokens per article, 4.5 per 278 word tokens per webpage text tokens after caption preprocessing average document title 4 word tokens 6 word tokens length total vocabulary after 10479 word types 8409 word types preprocessing Table 1: Comparison of the BBC and UNT image annotation datasets. ent domains, different sources of gold image annotations, different preprocessing procedures, and different evaluation measures. Table 1 provides an overview of the datasets; while this section covers the source of the datsets and their gold annotations in more detail. 3.1 BBC The BBC Dataset (Feng and Lapata, 2008)2 contains news articles, image captions, and images taken from the BBC News website. Training instances consist of a news article, image, and image caption from the same news story. Test instances are just the image and the article, and hold-out the caption as a source of gold image annotations. Using news image captions as annotations has 2Downloaded from http://homepages.inuf.ed. ac.uk/s0677528/data.html the disadvantage that captions often describe background information or relate the photo to the story, rather than listing important entities in the image. It also fails to capture variatio</context>
<context position="21450" citStr="Feng and Lapata, 2008" startWordPosition="3482" endWordPosition="3486">11); and implicitly, using language models (Feng and Lapata, 2010a). In comparison, sentence extraction is very implicit. Unsurprisingly, the Text LDA and Mix LDA systems do worse on the include-infrequent evaluation than they do on the standard, because words that do not appear in the training set will not have high probability in the trained topic models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA’s scores were double the scores of Text LDA (see Footnote 4). We were also unable to reproduce reported scores for tf*idf and Doc Title (Feng and Lapata, 2008). However, we have three reasons why we believe our results are correct. First, BBC has more keywords, and fewer images, than typically seen in CV datasets. The BBC dataset is simply not suited for learning from visual data. Second, a single SIFT descriptor describes which way edges are oriented at a certain point in an image (Lowe, 1999). While certain types of edges may correlate to visual objects also described in the text, we do not expect SIFT features to be as informative as textual features for this task. Third, we refer to the best system scores reported by Leong et al. (2010), who eva</context>
<context position="22706" citStr="Feng and Lapata (2008)" startWordPosition="3694" endWordPosition="3697">e section 6.1) on the standard BBC dataset.9 While their f1 score is slightly worse than our term frequency baseline, they do 4.86% better than tf*idf. But, using the baselines reported in Feng and Lapata (2008), their improvement over tf*idf is 12.06%. Next, we compare their system against frequency baselines using the 10 keyword generation task on the UNT dataset (the oot normal scores in table 5). Their best system performs 4.45% better 9Combined model; precision: 13.38, recall: 25.17, f1: 17.47. Crucially, they do not reimplement previous systems or baselines, but use scores reported from Feng and Lapata (2008). Cadbury increase contamination testing level malaria parasite spread mosquito 177 than term frequency, and 0.55% worse than tf*idf.10 Although it is difficult to compare different datasets and evaluation metrics, our baselines for BBC seem more reasonable than the reported baselines, given their relative performance to Leong et al’s system. 6 UNT Dataset Experiments 6.1 System Comparison We evaluate against the text mining system from (Leong et al., 2010). Their system generates image keywords by extracting text from the co-occurring text of an image. It uses three features for selecting key</context>
<context position="12385" citStr="Feng &amp; Lapata (2008" startWordPosition="1954" endWordPosition="1957">n addition to the entire collocation. For example, the gold annotation for one image has “university of texas”, “university of texas at dallas”, “the university of texas”, and “the university of texas at dallas”, each selected by at least four of the five annotators. Additionally, annotators can select multiple forms of the same word (such as “tank” and “tanks”). Gold annotations are stemmed after they are collected, and keywords with the same stem have their counts merged. For this reason, many keywords have a higher count than the number of annotators. 4 We are unable to reproduce work from Feng &amp; Lapata (2008; 2010a; 2010b) and Feng (2011). Specifically, our vocabulary counts after preprocessing (as in Table 1) are much higher than reported counts, although the number of tokens per article/caption they report is higher than ours. We have contacted the authors, who confirmed that they took additional steps to reduce the size of the vocabulary, but were unable to tell us exactly what those steps are. Therefore, all system and baseline scores presented on their dataset are of our own implementation, and do not match those reported in previous publications. 5Downloaded from http://lit.csci.unt.edu/ in</context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Yansong Feng and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. Proceedings of ACL-08: HLT, pages 272–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1239--1249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2205" citStr="Feng and Lapata, 2010" startWordPosition="320" endWordPosition="323">ebsites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1, it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance and legitimitize this work. In this paper, we propose some good practices and demonstrate why they are</context>
<context position="6057" citStr="Feng and Lapata, 2010" startWordPosition="923" endWordPosition="926">V image annotation are also applicable to NLP image annotation, and bringing NLP to the task brings new challenges as well. One of these challenges is whether to allow infrequent words to be pruned. In CV annotation it is typical to remove infrequent terms from both the keyword vocabulary and the evaluation data because CV algorithms typically need a large number of examples to train on. However, using NLP systems and baselines one can correctly annotate using keywords that did not appear in the training set. Removing “unlearnable” keywords from evaluation data, as done in (Boiy et al., 2008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). Th</context>
<context position="15850" citStr="Feng and Lapata (2010" startWordPosition="2538" endWordPosition="2541">es. For UNT, we use the total keyword frequency of all the gold annotations, except for the one document that we are currently scoring. For BBC, we only measure the frequency of keywords in the training set captions, since we are specifically interested in the frequency of terms in captions. 4.4 Document Title For BBC, the news article headline, and for UNT, the title of the webpage. 4.5 Sentence Extraction Our baseline extracts the most central sentence from the co-occurring text and uses descriptive words from that sentence as the image annotation. Unlike sentence extraction techniques from Feng and Lapata (2010a), we determine which sentence to extract using the term frequency distribution directly. We extract the sentence with the minimum KLdivergence to the entire document.8 5 BBC Dataset Experiments 5.1 System Comparison In addition to the baselines, we compare against the Mix LDA system from Feng and Lapata (2010b). In Mix LDA, each instance is represented as a bag of textual features (unigrams) and visual features (SIFT features quantized to discrete “image words” using k-means). A Latent Dirichlet Allocation topic model is trained on articles, images, and captions from the training set. Keywor</context>
<context position="20893" citStr="Feng and Lapata, 2010" startWordPosition="3385" endWordPosition="3388">xpected because our baseline selects a sentence based on the term frequency of the document, and the recall for term frequency fell. One possible explanation is that extraction implicitly uses correlations between keywords. Probabilities of objects appearing together in an image are not independent; and the accuracy of annotations can be improved by generating annotation keywords as a set (Moran and Lavrenko, 2011). Recent works in image captioning also use these correlations: explicitly, using graphical models (Kulkarni et al., 2011; Yang et al., 2011); and implicitly, using language models (Feng and Lapata, 2010a). In comparison, sentence extraction is very implicit. Unsurprisingly, the Text LDA and Mix LDA systems do worse on the include-infrequent evaluation than they do on the standard, because words that do not appear in the training set will not have high probability in the trained topic models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA’s scores were double the scores of Text LDA (see Footnote 4). We were also unable to reproduce reported scores for tf*idf and Doc Title (Feng and Lapata, 2008). However, we have three reasons why we bel</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1239–1249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>831--839</pages>
<contexts>
<context position="2205" citStr="Feng and Lapata, 2010" startWordPosition="320" endWordPosition="323">ebsites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1, it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance and legitimitize this work. In this paper, we propose some good practices and demonstrate why they are</context>
<context position="6057" citStr="Feng and Lapata, 2010" startWordPosition="923" endWordPosition="926">V image annotation are also applicable to NLP image annotation, and bringing NLP to the task brings new challenges as well. One of these challenges is whether to allow infrequent words to be pruned. In CV annotation it is typical to remove infrequent terms from both the keyword vocabulary and the evaluation data because CV algorithms typically need a large number of examples to train on. However, using NLP systems and baselines one can correctly annotate using keywords that did not appear in the training set. Removing “unlearnable” keywords from evaluation data, as done in (Boiy et al., 2008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). Th</context>
<context position="15850" citStr="Feng and Lapata (2010" startWordPosition="2538" endWordPosition="2541">es. For UNT, we use the total keyword frequency of all the gold annotations, except for the one document that we are currently scoring. For BBC, we only measure the frequency of keywords in the training set captions, since we are specifically interested in the frequency of terms in captions. 4.4 Document Title For BBC, the news article headline, and for UNT, the title of the webpage. 4.5 Sentence Extraction Our baseline extracts the most central sentence from the co-occurring text and uses descriptive words from that sentence as the image annotation. Unlike sentence extraction techniques from Feng and Lapata (2010a), we determine which sentence to extract using the term frequency distribution directly. We extract the sentence with the minimum KLdivergence to the entire document.8 5 BBC Dataset Experiments 5.1 System Comparison In addition to the baselines, we compare against the Mix LDA system from Feng and Lapata (2010b). In Mix LDA, each instance is represented as a bag of textual features (unigrams) and visual features (SIFT features quantized to discrete “image words” using k-means). A Latent Dirichlet Allocation topic model is trained on articles, images, and captions from the training set. Keywor</context>
<context position="20893" citStr="Feng and Lapata, 2010" startWordPosition="3385" endWordPosition="3388">xpected because our baseline selects a sentence based on the term frequency of the document, and the recall for term frequency fell. One possible explanation is that extraction implicitly uses correlations between keywords. Probabilities of objects appearing together in an image are not independent; and the accuracy of annotations can be improved by generating annotation keywords as a set (Moran and Lavrenko, 2011). Recent works in image captioning also use these correlations: explicitly, using graphical models (Kulkarni et al., 2011; Yang et al., 2011); and implicitly, using language models (Feng and Lapata, 2010a). In comparison, sentence extraction is very implicit. Unsurprisingly, the Text LDA and Mix LDA systems do worse on the include-infrequent evaluation than they do on the standard, because words that do not appear in the training set will not have high probability in the trained topic models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA’s scores were double the scores of Text LDA (see Footnote 4). We were also unable to reproduce reported scores for tf*idf and Doc Title (Feng and Lapata, 2008). However, we have three reasons why we bel</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In HLTNAACL, pages 831–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
</authors>
<title>Automatic caption generation for news images.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="12416" citStr="Feng (2011)" startWordPosition="1961" endWordPosition="1962">or example, the gold annotation for one image has “university of texas”, “university of texas at dallas”, “the university of texas”, and “the university of texas at dallas”, each selected by at least four of the five annotators. Additionally, annotators can select multiple forms of the same word (such as “tank” and “tanks”). Gold annotations are stemmed after they are collected, and keywords with the same stem have their counts merged. For this reason, many keywords have a higher count than the number of annotators. 4 We are unable to reproduce work from Feng &amp; Lapata (2008; 2010a; 2010b) and Feng (2011). Specifically, our vocabulary counts after preprocessing (as in Table 1) are much higher than reported counts, although the number of tokens per article/caption they report is higher than ours. We have contacted the authors, who confirmed that they took additional steps to reduce the size of the vocabulary, but were unable to tell us exactly what those steps are. Therefore, all system and baseline scores presented on their dataset are of our own implementation, and do not match those reported in previous publications. 5Downloaded from http://lit.csci.unt.edu/ index.php?P=research/downloads 4 </context>
</contexts>
<marker>Feng, 2011</marker>
<rawString>Yansong Feng. 2011. Automatic caption generation for news images. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Fluhr</author>
<author>Pierre-Alain Mollic</author>
<author>Patrick Hde</author>
</authors>
<title>Usage-oriented multimedia information retrieval technological evaluation.</title>
<date>2006</date>
<booktitle>Multimedia Information Retrieval,</booktitle>
<pages>301--306</pages>
<editor>In James Ze Wang, Nozha Boujemaa, and Yixin Chen, editors,</editor>
<publisher>ACM.</publisher>
<marker>Fluhr, Mollic, Hde, 2006</marker>
<rawString>Christian Fluhr, Pierre-Alain Mollic, and Patrick Hde. 2006. Usage-oriented multimedia information retrieval technological evaluation. In James Ze Wang, Nozha Boujemaa, and Yixin Chen, editors, Multimedia Information Retrieval, pages 301–306. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="16886" citStr="Haghighi and Vanderwende, 2009" startWordPosition="2707" endWordPosition="2710">al features (SIFT features quantized to discrete “image words” using k-means). A Latent Dirichlet Allocation topic model is trained on articles, images, and captions from the training set. Keywords are generated for an unseen image and article pair by estimating the distribution of topics that generates the test instance, then multiplying them with the word distributions in each topic to find the probability of textual keywords for the image. Text LDA is is the same model but only using words and not image features. 8One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. 5.2 Evaluation The evaluation metric and the source of gold annotations is described in Table 1. For the baselines 4.1, 4.2, 4.3 and the Mix LDA system, the generated annotation for each test image is its ten most likely keywords. We also run all baselines and the Mix LDA system on an unpruned version of the dataset, where infrequent terms are not removed from training data, test data, or the gold annotations. The purpose of this evaluation is to see if candidate keywords deemd “unlearnable” by the Mix LDA system can be learned by the baselines. 5.3 Results The </context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Hanbury</author>
</authors>
<title>A survey of methods for image annotation.</title>
<date>2008</date>
<journal>J. Vis. Lang. Comput.,</journal>
<pages>19--617</pages>
<contexts>
<context position="5262" citStr="Hanbury (2008)" startWordPosition="793" endWordPosition="794">ation metrics, and datasets for image annotation have been developed in response to these problems. Makadia et al. (2008; 2010) define a basic set of low-level features, and propose new baselines for more complex systems to evaluate against. Barnard et al. (2003) present a normalized loss function to address the preference towards precision in evaluation metrics. New datasets are larger and provide more diverse images, and it is now easy to obtain multiple human-annotations per image thanks to distributed services such as Amazon’s Mechanical Turk, and the ESP game (von Ahn and Dabbish, 2004). Hanbury (2008) provides an overview of popular CV annotation datasets and methods used for building them. 2.2 Image Annotation using Natural Language Processing Many of the problems from CV image annotation are also applicable to NLP image annotation, and bringing NLP to the task brings new challenges as well. One of these challenges is whether to allow infrequent words to be pruned. In CV annotation it is typical to remove infrequent terms from both the keyword vocabulary and the evaluation data because CV algorithms typically need a large number of examples to train on. However, using NLP systems and base</context>
</contexts>
<marker>Hanbury, 2008</marker>
<rawString>Allan Hanbury. 2008. A survey of methods for image annotation. J. Vis. Lang. Comput., 19:617–627, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Herv´e</author>
<author>Nozha Boujemaa</author>
</authors>
<title>Image annotation: which approach for realistic databases?</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th ACM international conference on Image and video retrieval, CIVR ’07,</booktitle>
<pages>170--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Herv´e, Boujemaa, 2007</marker>
<rawString>Nicolas Herv´e and Nozha Boujemaa. 2007. Image annotation: which approach for realistic databases? In Proceedings of the 6th ACM international conference on Image and video retrieval, CIVR ’07, pages 170– 177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR,</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="20811" citStr="Kulkarni et al., 2011" startWordPosition="3372" endWordPosition="3375">ne or system that has improved recall when including infrequent words. This is unexpected because our baseline selects a sentence based on the term frequency of the document, and the recall for term frequency fell. One possible explanation is that extraction implicitly uses correlations between keywords. Probabilities of objects appearing together in an image are not independent; and the accuracy of annotations can be improved by generating annotation keywords as a set (Moran and Lavrenko, 2011). Recent works in image captioning also use these correlations: explicitly, using graphical models (Kulkarni et al., 2011; Yang et al., 2011); and implicitly, using language models (Feng and Lapata, 2010a). In comparison, sentence extraction is very implicit. Unsurprisingly, the Text LDA and Mix LDA systems do worse on the include-infrequent evaluation than they do on the standard, because words that do not appear in the training set will not have high probability in the trained topic models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA’s scores were double the scores of Text LDA (see Footnote 4). We were also unable to reproduce reported scores for tf*id</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In CVPR, pages 1601– 1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
<author>Samer Hassan</author>
</authors>
<title>Text mining for automatic image tagging.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>647--655</pages>
<contexts>
<context position="2106" citStr="Leong et al., 2010" startWordPosition="304" endWordPosition="307"> information that is difficult to learn visually. For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1, it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance and</context>
<context position="7606" citStr="Leong et al. (2010)" startWordPosition="1180" endWordPosition="1183">, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listing dozens of popular keywords. Co-occurring text for retrieved images on popular queries may not be representative of the task to be performed. 3 Datasets In this paper, we examine two established image annotation datasets: the BBC News Dataset of Feng and Lapata (2008) (henceforth referred to as BBC), and the general web dataset of Leong et al. (2010) (henceforth referred to as UNT). These datasets were both built to evaluate image annotation systems that use longer co-occurring text such as a news article or a webpage, but they use data from differ173 Dataset BBC UNT data instances article, image, and caption from a image and text from a webpage news story source of data scraped from BBC News website Google Image Search results candidate keywords or descriptive unigram words from n &lt; 7-grams extracted from cocollocations for anno- training data occurring text; collocations must aptation pear as article names on Wikipedia gold annotations </context>
<context position="11037" citStr="Leong et al., 2010" startWordPosition="1724" endWordPosition="1727">for this dataset is to lemmatize and POStag using TreeTagger (Schmid, 1994) then remove all but the “descriptive” words (defined as nouns, adjectives, and certain classes of verbs). This leaves a total text vocabulary of about 32K words, which 3The Pascal Sentences dataset (vision.cs.uiuc.edu/ pascal-sentences) provides multiple captions per image, but they are not naturally-occurring. 174 is further reduced by removing words that appear fewer than five times in the training set articles. Table 1 shows the number of word tokens and types after performing these steps.4 3.2 UNT The UNT Dataset (Leong et al., 2010)5 consists of images and co-occurring text from webpages. The webpages are found by querying Google Image Search with frequent English words, and randomly selecting from the results. Each image in UNT is annotated by five people via Mechanical Turk. In order to make human and system results comparable, human annotators are required to only select words and collocations that are directly extracted from the text, and the gold annotations are the count of how many times each keyword or collocation is selected. The human annotators write keywords into a text box; while the collocations are present</context>
<context position="22041" citStr="Leong et al. (2010)" startWordPosition="3588" endWordPosition="3591">itle (Feng and Lapata, 2008). However, we have three reasons why we believe our results are correct. First, BBC has more keywords, and fewer images, than typically seen in CV datasets. The BBC dataset is simply not suited for learning from visual data. Second, a single SIFT descriptor describes which way edges are oriented at a certain point in an image (Lowe, 1999). While certain types of edges may correlate to visual objects also described in the text, we do not expect SIFT features to be as informative as textual features for this task. Third, we refer to the best system scores reported by Leong et al. (2010), who evaluate their text mining system (see section 6.1) on the standard BBC dataset.9 While their f1 score is slightly worse than our term frequency baseline, they do 4.86% better than tf*idf. But, using the baselines reported in Feng and Lapata (2008), their improvement over tf*idf is 12.06%. Next, we compare their system against frequency baselines using the 10 keyword generation task on the UNT dataset (the oot normal scores in table 5). Their best system performs 4.45% better 9Combined model; precision: 13.38, recall: 25.17, f1: 17.47. Crucially, they do not reimplement previous systems </context>
<context position="24713" citStr="Leong et al. (2010)" startWordPosition="4007" endWordPosition="4010">stitution task (McCarthy and Navigli, 2007). This framework accounts for disagreement between annotators by weighting each generated keyword by the number of human annotators who also selected that keyword. The scoring framework consists of four evaluation measures: best normal, best mode, oot (out-of-ten) normal, and oot mode.11 The two best evaluations find the accuracy of a single “best” keyword generated by the system12. 10And as we stated earlier, the relative performance of term frequency vs tf*idf is different from dataset to dataset. 11Both the original framework and its adaptation by Leong et al. (2010) give precision and recall for each of the evaluation measures. However, precision and recall are identical for all baselines and systems, and only slightly different on the upper bound (human) scores. To preserve space, we only present the metric and scores for precision. 12In contrast to the original SemEval task, where systems can Best normal measures the accuracy for each system annotation aj as the number of times aj appears in the Rj, the multi-set union of human tags, and averages over all the test images. aj ∈Rj | �ij ∈I |Rj| III In oot normal, up to ten unordered guesses can be made w</context>
<context position="27938" citStr="Leong et al., 2010" startWordPosition="4511" endWordPosition="4514">een(1), stabl(1), encyclopedia(1), thunderstorm(1), lift(1), free encyclopedia(1), wikipedia(1) Table 4: Examples of gold annotations from the UNT Dataset. Best Mode Out-of-ten (oot) Normal Normal Mode Term Frequency 5.67 14.29 33.40 89.29 tf * idf 5.94 14.29 38.40 78.57 Doc Title 6.40 7.14 35.19 92.86 Corpus Frequency 2.54 75.00 8.22 82.14 Flickr Picturability 6.32 78.57 35.61 92.86 Wikipedia Salience 6.40 7.14 35.19 92.86 Topic Model (PAM) 5.99 42.86 37.13 85.71 Combined (SVM) 6.87 67.49 37.85 100.00 Table 5: Image annotation results for our proposed baselines, the text mining systems from (Leong et al., 2010) count towards these metrics, we conclude that mode evaluation is not a meaningful way to compare image annotation systems on the UNT dataset. That said, the number of cartoons in the dataset does seem to be strikingly high. Looking at the source of the images, we find that 45 of the 300 images were collected from a single online cartoon library. Predictably, we find that the co-occurring text to these images contains a long list of keywords, and little other text that is relevant to the image. We looked at a small sample of the rest of the dataset and found that many of the other text documen</context>
</contexts>
<marker>Leong, Mihalcea, Hassan, 2010</marker>
<rawString>Chee Wee Leong, Rada Mihalcea, and Samer Hassan. 2010. Text mining for automatic image tagging. In COLING, pages 647–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning, ICML ’06,</booktitle>
<pages>577--584</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23789" citStr="Li and McCallum, 2006" startWordPosition="3865" endWordPosition="3868">. Their system generates image keywords by extracting text from the co-occurring text of an image. It uses three features for selecting keywords. Flickr Picturability queries the Flickr API with words from the text in order to find related image tags. Retrieved tags that appear as surface forms in the text are rewarded proportional to their frequency in the text. Wikipedia Salience assigns scores to words based on a graph-based measure of importance that considers each term’s document frequency in Wikipedia. Pachinko Allocation Model is a topic model that captures correlations between topics (Li and McCallum, 2006). PAM infers subtopics and supertopics for the text, then retrieves top words from the top topics as annotations. There is also a combined model of these features using an SVM with 10-fold cross-validation. 6.2 Evaluation Evaluation on UNT uses a framework originally developed for the SemEval lexical substitution task (McCarthy and Navigli, 2007). This framework accounts for disagreement between annotators by weighting each generated keyword by the number of human annotators who also selected that keyword. The scoring framework consists of four evaluation measures: best normal, best mode, oot </context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd international conference on Machine learning, ICML ’06, pages 577–584, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Lowe</author>
</authors>
<title>Object recognition from local scaleinvariant features. In Computer Vision,</title>
<date>1999</date>
<booktitle>The Proceedings of the Seventh IEEE International Conference on,</booktitle>
<volume>2</volume>
<pages>1150--1157</pages>
<contexts>
<context position="21790" citStr="Lowe, 1999" startWordPosition="3543" endWordPosition="3544"> models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA’s scores were double the scores of Text LDA (see Footnote 4). We were also unable to reproduce reported scores for tf*idf and Doc Title (Feng and Lapata, 2008). However, we have three reasons why we believe our results are correct. First, BBC has more keywords, and fewer images, than typically seen in CV datasets. The BBC dataset is simply not suited for learning from visual data. Second, a single SIFT descriptor describes which way edges are oriented at a certain point in an image (Lowe, 1999). While certain types of edges may correlate to visual objects also described in the text, we do not expect SIFT features to be as informative as textual features for this task. Third, we refer to the best system scores reported by Leong et al. (2010), who evaluate their text mining system (see section 6.1) on the standard BBC dataset.9 While their f1 score is slightly worse than our term frequency baseline, they do 4.86% better than tf*idf. But, using the baselines reported in Feng and Lapata (2008), their improvement over tf*idf is 12.06%. Next, we compare their system against frequency base</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>D.G. Lowe. 1999. Object recognition from local scaleinvariant features. In Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pages 1150 –1157 vol.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ameesh Makadia</author>
<author>Vladimir Pavlovic</author>
<author>Sanjiv Kumar</author>
</authors>
<title>A new baseline for image annotation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 10th European Conference on Computer Vision: Part III, ECCV ’08,</booktitle>
<pages>316--329</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4768" citStr="Makadia et al. (2008" startWordPosition="711" endWordPosition="714">e the benefits of using more complex techniques (Ponce et al., 2006). The problem is further exacerbated by evaluation metrics which essentially prefer precision over recall and are biased towards certain keywords. Annotations in test data might not include all of the “correct” keywords, and evaluation metrics need to account for the fact that frequent keywords in the corpus are safer guesses than keywords that appear less frequently (Monay and Gatica-Perez, 2003). New baseline techniques, evaluation metrics, and datasets for image annotation have been developed in response to these problems. Makadia et al. (2008; 2010) define a basic set of low-level features, and propose new baselines for more complex systems to evaluate against. Barnard et al. (2003) present a normalized loss function to address the preference towards precision in evaluation metrics. New datasets are larger and provide more diverse images, and it is now easy to obtain multiple human-annotations per image thanks to distributed services such as Amazon’s Mechanical Turk, and the ESP game (von Ahn and Dabbish, 2004). Hanbury (2008) provides an overview of popular CV annotation datasets and methods used for building them. 2.2 Image Anno</context>
</contexts>
<marker>Makadia, Pavlovic, Kumar, 2008</marker>
<rawString>Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar. 2008. A new baseline for image annotation. In Proceedings of the 10th European Conference on Computer Vision: Part III, ECCV ’08, pages 316–329, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Makadia</author>
<author>V Pavlovic</author>
<author>S Kumar</author>
</authors>
<title>Baselines for image annotation.</title>
<date>2010</date>
<journal>International Journal of Computer Vision,</journal>
<volume>90</volume>
<issue>1</issue>
<marker>Makadia, Pavlovic, Kumar, 2010</marker>
<rawString>A. Makadia, V. Pavlovic, and S. Kumar. 2010. Baselines for image annotation. International Journal of Computer Vision, 90(1):88–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<contexts>
<context position="24137" citStr="McCarthy and Navigli, 2007" startWordPosition="3919" endWordPosition="3922">frequency in the text. Wikipedia Salience assigns scores to words based on a graph-based measure of importance that considers each term’s document frequency in Wikipedia. Pachinko Allocation Model is a topic model that captures correlations between topics (Li and McCallum, 2006). PAM infers subtopics and supertopics for the text, then retrieves top words from the top topics as annotations. There is also a combined model of these features using an SVM with 10-fold cross-validation. 6.2 Evaluation Evaluation on UNT uses a framework originally developed for the SemEval lexical substitution task (McCarthy and Navigli, 2007). This framework accounts for disagreement between annotators by weighting each generated keyword by the number of human annotators who also selected that keyword. The scoring framework consists of four evaluation measures: best normal, best mode, oot (out-of-ten) normal, and oot mode.11 The two best evaluations find the accuracy of a single “best” keyword generated by the system12. 10And as we stated earlier, the relative performance of term frequency vs tf*idf is different from dataset to dataset. 11Both the original framework and its adaptation by Leong et al. (2010) give precision and reca</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Monay</author>
<author>Daniel Gatica-Perez</author>
</authors>
<title>On image auto-annotation with latent space models.</title>
<date>2003</date>
<booktitle>In Proceedings of the eleventh ACM international conference on Multimedia, Multimedia ’03,</booktitle>
<pages>275--278</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4616" citStr="Monay and Gatica-Perez, 2003" startWordPosition="689" endWordPosition="692">d de Vries, 2003; Ponce et al., 2006; Herv´e and Boujemaa, 2007; Tang and Lewis, 2007). These features are unwittingly exploited by certain algorithms and obscure the benefits of using more complex techniques (Ponce et al., 2006). The problem is further exacerbated by evaluation metrics which essentially prefer precision over recall and are biased towards certain keywords. Annotations in test data might not include all of the “correct” keywords, and evaluation metrics need to account for the fact that frequent keywords in the corpus are safer guesses than keywords that appear less frequently (Monay and Gatica-Perez, 2003). New baseline techniques, evaluation metrics, and datasets for image annotation have been developed in response to these problems. Makadia et al. (2008; 2010) define a basic set of low-level features, and propose new baselines for more complex systems to evaluate against. Barnard et al. (2003) present a normalized loss function to address the preference towards precision in evaluation metrics. New datasets are larger and provide more diverse images, and it is now easy to obtain multiple human-annotations per image thanks to distributed services such as Amazon’s Mechanical Turk, and the ESP ga</context>
</contexts>
<marker>Monay, Gatica-Perez, 2003</marker>
<rawString>Florent Monay and Daniel Gatica-Perez. 2003. On image auto-annotation with latent space models. In Proceedings of the eleventh ACM international conference on Multimedia, Multimedia ’03, pages 275–278, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moran</author>
<author>V Lavrenko</author>
</authors>
<title>Optimal tag sets for automatic image.</title>
<date>2011</date>
<contexts>
<context position="20690" citStr="Moran and Lavrenko, 2011" startWordPosition="3354" endWordPosition="3357">enerate ten keywords for every image. While sentence extraction has a lower recall than term frequency, it is the only baseline or system that has improved recall when including infrequent words. This is unexpected because our baseline selects a sentence based on the term frequency of the document, and the recall for term frequency fell. One possible explanation is that extraction implicitly uses correlations between keywords. Probabilities of objects appearing together in an image are not independent; and the accuracy of annotations can be improved by generating annotation keywords as a set (Moran and Lavrenko, 2011). Recent works in image captioning also use these correlations: explicitly, using graphical models (Kulkarni et al., 2011; Yang et al., 2011); and implicitly, using language models (Feng and Lapata, 2010a). In comparison, sentence extraction is very implicit. Unsurprisingly, the Text LDA and Mix LDA systems do worse on the include-infrequent evaluation than they do on the standard, because words that do not appear in the training set will not have high probability in the trained topic models. We were unable to reproduce the reported scores for Mix LDA from Feng and Lapata (2010b) where Mix LDA</context>
</contexts>
<marker>Moran, Lavrenko, 2011</marker>
<rawString>S. Moran and V. Lavrenko. 2011. Optimal tag sets for automatic image.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning M¨uller</author>
<author>St´ephane Marchand-Maillet</author>
<author>Thierry Pun</author>
</authors>
<title>The truth about corel - evaluation in image retrieval.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Image and Video Retrieval, CIVR ’02,</booktitle>
<pages>38--49</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<marker>M¨uller, Marchand-Maillet, Pun, 2002</marker>
<rawString>Henning M¨uller, St´ephane Marchand-Maillet, and Thierry Pun. 2002. The truth about corel - evaluation in image retrieval. In Proceedings of the International Conference on Image and Video Retrieval, CIVR ’02, pages 38–49, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="13654" citStr="Nenkova and Vanderwende, 2005" startWordPosition="2159" endWordPosition="2163">s We run several baselines on the datasets. Term frequency, tf*idf, and corpus frequency are features that are often used in annotation systems, so it is important to test them on their own. Document Title and tf*idf are both baselines that were used in the original papers where these datasets came from. Sentence extraction is a new baseline that we propose specifically for the BBC dataset, in order see if we can exploit certain properties of the gold annotations, which are also derived from sentences. 4.1 Term Frequency Term frequency has been shown to be a powerful feature in summarization (Nenkova and Vanderwende, 2005). Words that appear frequently are considered more meaningful than infrequent words. Term frequency is the number of times a term (excluding function words) appears in a document, divided by the total number of terms in that document. On the UNT dataset we use the stopword list included with the MALLET6 toolkit, while the BBC dataset doesn’t matter because the function words have already been removed. 4.2 tf*idf While term frequency baseline requires the use of an ad hoc function word list, tf*idf adjusts the weights of different words depending on how important they are in the corpus. It is a</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Ponce</author>
<author>T Berg</author>
<author>M Everingham</author>
<author>D Forsyth</author>
<author>M Hebert</author>
<author>S Lazebnik</author>
<author>M Marszalek</author>
<author>C Schmid</author>
<author>B Russell</author>
<author>A Torralba</author>
<author>C Williams</author>
<author>J Zhang</author>
<author>A Zisserman</author>
</authors>
<title>Dataset issues in object recognition.</title>
<date>2006</date>
<booktitle>Toward Category-Level Object Recognition,</booktitle>
<volume>4170</volume>
<pages>29--48</pages>
<editor>In Jean Ponce, Martial Hebert, Cordelia Schmid, and Andrew Zisserman, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="4023" citStr="Ponce et al., 2006" startWordPosition="599" endWordPosition="602">Vision The work of M¨uller et al. (2002) is one of the first to address the issue of evaluation for image annotation systems. While using the exact same annotation system, dataset, and evaluation metric, they dramatically improve the apparent performance of their system by using dataset pruning heuristics. Others have criticized commonly-used CV datasets for being too “easy” — images with the same keywords are extremely similar in low-level features such as orientation, lighting, and color; while differences between images with different keywords are very clear (Westerveld and de Vries, 2003; Ponce et al., 2006; Herv´e and Boujemaa, 2007; Tang and Lewis, 2007). These features are unwittingly exploited by certain algorithms and obscure the benefits of using more complex techniques (Ponce et al., 2006). The problem is further exacerbated by evaluation metrics which essentially prefer precision over recall and are biased towards certain keywords. Annotations in test data might not include all of the “correct” keywords, and evaluation metrics need to account for the fact that frequent keywords in the corpus are safer guesses than keywords that appear less frequently (Monay and Gatica-Perez, 2003). New b</context>
</contexts>
<marker>Ponce, Berg, Everingham, Forsyth, Hebert, Lazebnik, Marszalek, Schmid, Russell, Torralba, Williams, Zhang, Zisserman, 2006</marker>
<rawString>J. Ponce, T. Berg, M. Everingham, D. Forsyth, M. Hebert, S. Lazebnik, M. Marszalek, C. Schmid, B. Russell, A. Torralba, C. Williams, J. Zhang, and A. Zisserman. 2006. Dataset issues in object recognition. In Jean Ponce, Martial Hebert, Cordelia Schmid, and Andrew Zisserman, editors, Toward Category-Level Object Recognition, volume 4170 of Lecture Notes in Computer Science, pages 29–48. Springer Berlin / Heidelberg. 10.1007/11957959 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of international conference on new methods in language processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="10493" citStr="Schmid, 1994" startWordPosition="1640" endWordPosition="1641">c.uk/s0677528/data.html the disadvantage that captions often describe background information or relate the photo to the story, rather than listing important entities in the image. It also fails to capture variation in how humans describe images, since it is limited to one caption per image.3 However, captions are a cheap source of data; BBC has ten times as many images as UNT. To address the problem of converting natural language into annotations, a large amount of preprocessing is performed. The established preprocessing procedure for this dataset is to lemmatize and POStag using TreeTagger (Schmid, 1994) then remove all but the “descriptive” words (defined as nouns, adjectives, and certain classes of verbs). This leaves a total text vocabulary of about 32K words, which 3The Pascal Sentences dataset (vision.cs.uiuc.edu/ pascal-sentences) provides multiple captions per image, but they are not naturally-occurring. 174 is further reduced by removing words that appear fewer than five times in the training set articles. Table 1 shows the number of word tokens and types after performing these steps.4 3.2 UNT The UNT Dataset (Leong et al., 2010)5 consists of images and co-occurring text from webpages</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international conference on new methods in language processing, volume 12, pages 44–49. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiayu Tang</author>
<author>Paul Lewis</author>
</authors>
<title>A study of quality issues for image auto-annotation with the corel dataset.</title>
<date>2007</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>1</volume>
<pages>3--384</pages>
<contexts>
<context position="4073" citStr="Tang and Lewis, 2007" startWordPosition="607" endWordPosition="610"> of the first to address the issue of evaluation for image annotation systems. While using the exact same annotation system, dataset, and evaluation metric, they dramatically improve the apparent performance of their system by using dataset pruning heuristics. Others have criticized commonly-used CV datasets for being too “easy” — images with the same keywords are extremely similar in low-level features such as orientation, lighting, and color; while differences between images with different keywords are very clear (Westerveld and de Vries, 2003; Ponce et al., 2006; Herv´e and Boujemaa, 2007; Tang and Lewis, 2007). These features are unwittingly exploited by certain algorithms and obscure the benefits of using more complex techniques (Ponce et al., 2006). The problem is further exacerbated by evaluation metrics which essentially prefer precision over recall and are biased towards certain keywords. Annotations in test data might not include all of the “correct” keywords, and evaluation metrics need to account for the fact that frequent keywords in the corpus are safer guesses than keywords that appear less frequently (Monay and Gatica-Perez, 2003). New baseline techniques, evaluation metrics, and datase</context>
</contexts>
<marker>Tang, Lewis, 2007</marker>
<rawString>Jiayu Tang and Paul Lewis. 2007. A study of quality issues for image auto-annotation with the corel dataset. IEEE Transactions on Circuits and Systems for Video Technology, Vol. 1(NO. 3):384–389, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tsikrika</author>
<author>A Popescu</author>
<author>J Kludas</author>
</authors>
<title>Overview of the wikipedia image retrieval task at imageclef 2011.</title>
<date>2011</date>
<booktitle>In CLEF (Notebook Papers/LABs/Workshops): CLEF.</booktitle>
<contexts>
<context position="6653" citStr="Tsikrika et al., 2011" startWordPosition="1020" endWordPosition="1023">008; Feng and Lapata, 2010b), artificially inflates performance against simple baselines such as term frequency. Nearly all NLP annotation datasets use naturallyoccurring sources of images and text. A particularly popular source is news images alongside captions or articles, which are collected online from sources such as Yahoo! News (Berg et al., 2004; Deschacht and Moens, 2007). There are also domainspecific databases with images and descriptions such as the art, antiques, and flowers corpora used in Boiy et al. (2008). Wikipedia has also been used as a source of images and associated text (Tsikrika et al., 2011). These sources typically offer well-written and cleanly formatted text but introduce the problem of converting text into annotations, and the annotations may not meet the requirements of the new task (as shown in Section 3.1). Obtaining data via image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listin</context>
</contexts>
<marker>Tsikrika, Popescu, Kludas, 2011</marker>
<rawString>T. Tsikrika, A. Popescu, and J. Kludas. 2011. Overview of the wikipedia image retrieval task at imageclef 2011. In CLEF (Notebook Papers/LABs/Workshops): CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’04,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’04, pages 319–326, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thijs Westerveld</author>
<author>Arjen P de Vries</author>
</authors>
<title>Experimental evaluation of a generative probabilistic image retrieval model on ’easy’ data. In</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGIR Multimedia Information Retrieval Workshop</booktitle>
<marker>Westerveld, de Vries, 2003</marker>
<rawString>Thijs Westerveld and Arjen P. de Vries. 2003. Experimental evaluation of a generative probabilistic image retrieval model on ’easy’ data. In In Proceedings of the SIGIR Multimedia Information Retrieval Workshop 2003, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, Scotland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>