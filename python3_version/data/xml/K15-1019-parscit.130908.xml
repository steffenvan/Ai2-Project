<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.784089">
Linking Entities Across Images and Text
</title>
<author confidence="0.678412">
Rebecka Weegar Kalle ˚Astr¨om Pierre Nugues
</author>
<affiliation confidence="0.755177">
DSV Dept. of Mathematics Dept. of Computer Science
Stockholm University Lund University Lund University
</affiliation>
<email confidence="0.975767">
rebeckaw@dsv.su.se,kalle@maths.lth.se,Pierre.Nugues@cs.lth.se
</email>
<sectionHeader confidence="0.997103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993407407408">
This paper describes a set of methods to
link entities across images and text. As
a corpus, we used a data set of images,
where each image is commented by a short
caption and where the regions in the im-
ages are manually segmented and labeled
with a category. We extracted the entity
mentions from the captions and we com-
puted a semantic similarity between the
mentions and the region labels. We also
measured the statistical associations be-
tween these mentions and the labels and
we combined them with the semantic sim-
ilarity to produce mappings in the form
of pairs consisting of a region label and
a caption entity. In a second step, we
used the syntactic relationships between
the mentions and the spatial relationships
between the regions to rerank the lists
of candidate mappings. To evaluate our
methods, we annotated a test set of 200
images, where we manually linked the im-
age regions to their corresponding men-
tions in the captions. Eventually, we could
match objects in pictures to their correct
mentions for nearly 89 percent of the seg-
ments, when such a matching exists.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999055">
Linking an object in an image to a mention of that
object in an accompanying text is a challenging
task, which we can imagine useful in a number
of settings. It could, for instance, improve im-
age retrieval by complementing the geometric re-
lationships extracted from the images with textual
descriptions from the text. A successful mapping
would also make it possible to translate knowledge
and information across image and text.
In this paper, we describe methods to link men-
tions of entities in captions to labeled image seg-
ments and we investigate how the syntactic struc-
ture of a caption can be used to better understand
the contents of an image. We do not address the
closely related task of object recognition in the im-
ages. This latter task can be seen as a complement
to entity linking across text and images. See Rus-
sakovsky et al. (2015) for a description of progress
and results to date in object detection and classifi-
cation in images.
</bodyText>
<sectionHeader confidence="0.971578" genericHeader="introduction">
2 An Example
</sectionHeader>
<bodyText confidence="0.907249545454546">
Figure 1 shows an example of an image from the
Segmented and Annotated IAPR TC-12 data set
(Escalantea et al., 2010). It has four regions la-
beled cloud, grass, hill, and river, and the caption:
a flat landscape with a dry meadow in
the foreground, a lagoon behind it and
many clouds in the sky
containing mentions of five entities that we iden-
tify with the words meadow, landscape, lagoon,
cloud, and sky. A correct association of the men-
tions in the caption to the image regions would
</bodyText>
<figureCaption confidence="0.63142525">
Figure 1: Image from the Segmented and Anno-
tated IAPR TC-12 data set with the caption: a flat
landscape with a dry meadow in the foreground, a
lagoon behind it and many clouds in the sky
</figureCaption>
<page confidence="0.962288">
185
</page>
<note confidence="0.980203">
Proceedings of the 19th Conference on Computational Language Learning, pages 185–193,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999654083333333">
map clouds to the region labeled cloud, meadow
to grass, and lagoon to river.
This image, together with its caption, illustrates
a couple of issues: The objects or regions labelled
or visible in an image are not always mentioned
in the caption, and for most of the images in the
data set, more entities are mentioned in the cap-
tions than there are regions in the images. In addi-
tion, for a same entity, the words used to mention
it are usually different from the words used as la-
bels (the categories), as in the case of grass and
meadow.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.999980594594594">
Related work includes the automatic generation
of image captions that describes relevant objects
in an image and their relationships. Kulkarni et
al. (2011) assign each detected image object a vi-
sual attribute and a spatial relationship to the other
objects in the image. The spatial relationships
are translated into selected prepositions in the re-
sulting captions. Elliott and Keller (2013) used
manually segmented and labeled images and intro-
duced visual dependency representations (VDRs)
that describe spatial relationships between the im-
age objects. The captions are generated using tem-
plates. Both Kulkarni et al. (2011) and Elliott and
Keller (2013) used the BLEU-score and human
evaluators to assess grammatically the generated
captions and on how well they describe the image.
Although much work has been done to link
complete images to a whole text, there are only a
few papers on the association of elements inside a
text and an image. Naim et al. (2014) analyzed
parallel sets of videos and written texts, where
the videos show laboratory experiments. Written
instructions are used to describe how to conduct
these experiments. The paper describes models for
matching objects detected in the video with men-
tions of those objects in the instructions. The au-
thors mainly focus on objects that get touched by a
hand in the video. For manually annotated videos,
Naim et al. (2014) could match objects to nouns
nearly 50% of the time.
Karpathy et al. (2014) proposed a system for re-
trieving related images and sentences. They used
neural networks and they show that the results are
improved if image objects and sentence fragments
are included in the model. Sentence fragments
are extracted from dependency graphs, where each
edge in the graphs corresponds to a fragment.
</bodyText>
<sectionHeader confidence="0.993157" genericHeader="method">
4 Entity Pairs
</sectionHeader>
<subsectionHeader confidence="0.998673">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.9999674375">
We used the Segmented and Annotated IAPR TC-
12 Benchmark data set (Escalantea et al., 2010)
that consists of about 20,000 photographs with
a wide variety of themes. Each image has a
short caption that describes its content, most often
consisting of one to three sentences separated by
semicolons. The images are manually segmented
into regions with, on average, about 5 segments in
each image.
Each region is labelled with one out of 275
predefined image labels. The labels are arranged
in a hierarchy, where all the nodes are available
as labels and where object is the top node.
The labels humans, animals, man-made,
landscape/nature, food, and other form
the next level.
</bodyText>
<subsectionHeader confidence="0.979949">
4.2 Entities and Mentions
</subsectionHeader>
<bodyText confidence="0.999957620689655">
An image caption describes a set of entities, the
caption entities CE, where each entity CEi is
referred to by a set of mentions M. To detect
them, we applied the Stanford CoreNLP pipeline
(Toutanova et al., 2003) that consists of a part-
of-speech tagger, lemmatizer, named entity recog-
nizer (Finkel et al., 2005), dependency parser, and
coreference solver. We considered each noun in
a caption as an entity candidate. If an entity CEi
had only one mention Mj, we identified it by the
head noun of its mention. We represented the en-
tities mentioned more than once by the head noun
of their most representative mention. We applied
the entity extraction to all the captions in the data
set, and we found 3,742 different nouns or noun
compounds to represent the entities.
In addition to the caption entities, each image
has a set of labeled segments (or regions) corre-
sponding to the image entities, IE. The Carte-
sian product of these two sets results in pairs P
generating all the possible mappings of caption
entities to image labels. We considered a pair
(IEi, CEj) a correct mapping, if the image la-
bel IEi and the caption entity CEj referred to the
same entity. We represented a pair by the region
label and the identifier of the caption entity, i.e.
the head noun of the entity mention. In Fig. 1, the
correct pairs are (grass, meadow), (river, lagoon),
and (cloud, clouds).
</bodyText>
<page confidence="0.997723">
186
</page>
<subsectionHeader confidence="0.999487">
4.3 Building a Test Set
</subsectionHeader>
<bodyText confidence="0.9995665625">
As the Segmented and Annotated IAPR TC-12
data set does not provide information on links be-
tween the image regions and the mentions, we
annotated a set of 200 randomly selected images
from the data set to evaluate the automatic linking
accuracy. We assigned the image regions to enti-
ties in the captions and we excluded these images
from the training set. The annotation does not al-
ways produce a 1:1 mapping of caption entities to
regions. In many cases, objects are grouped or di-
vided into parts differently in the captions and in
the segmentation. We created a set of guidelines
to handle these mappings in a consistent way. Ta-
ble 1 shows the sizes of the different image sets
and the fraction of image regions that have a cor-
responding entity mention in the caption.
</bodyText>
<table confidence="0.37681775">
Set Files Regions Mappings %
Data set 19,176 – – –
Train. set 18,976 – – –
Test set 200 928 730 78.7
</table>
<tableCaption confidence="0.996377">
Table 1: The sizes of the different image sets.
</tableCaption>
<sectionHeader confidence="0.905623" genericHeader="method">
5 Ranking Entity Pairs
</sectionHeader>
<bodyText confidence="0.999966428571429">
To identify the links between the regions of an im-
age and the entity identifiers in its caption, we
first generated all the possible pairs. We then
ranked these pairs using a semantic distance de-
rived from WordNet (Miller, 1995), statistical as-
sociation metrics, and finally, a combination of
both techniques.
</bodyText>
<subsectionHeader confidence="0.995269">
5.1 Semantic Distance
</subsectionHeader>
<bodyText confidence="0.999968852941176">
The image labels are generic English words that
are semantically similar to those used in the cap-
tions. In Fig. 1, cloud and clouds are used both
as label and in the caption, but the region labeled
grass is described as a meadow and the region la-
beled river, as a lagoon. We used the WordNet
Similarity for Java library, (WS4J), (Shima, 2014)
to compute the semantic similarity of the region
labels and the entity identifiers. WS4J comes with
a number of metrics that approximate similarity as
distances between WordNet synsets: PATH, WUP
(Wu and Palmer, 1994), RES, (Resnik, 1995), JCN
(Jiang and Conrath, 1997), HSO (Hirst and St-
Onge, 1998), LIN (Lin, 1998), LCH (Leacock
and Chodorow, 1998), and LESK (Banerjee and
Banerjee, 2002).
We manually lemmatized and simplified the im-
age labels and the entity mentions so that they are
compatible with WordNet entries. It resulted in a
smaller set of labels: 250 instead of the 275 orig-
inal labels. We also simplified the named entities
from the captions. When a person or location was
not present in WordNet, we used its named entity
type as identifier. In some cases, it was not possi-
ble to find an entity identifier in WordNet, mostly
due to misspellings in the caption, like buldings,
or buidling, or because of POS-tagging errors. We
chose to identify these entities with the word en-
tity. The normalization reduced the 3,742 entity
identifiers to 2,216 unique ones.
Finally, we computed a 250 x 2216 matrix con-
taining the similarity scores for each (image label,
entity identifier) pair for each of the WS4J seman-
tic similarity metrics.
</bodyText>
<subsectionHeader confidence="0.998552">
5.2 Statistical Associations
</subsectionHeader>
<bodyText confidence="0.998193">
We used three functions to reflect the statistical
association between an image label and an entity
identifier:
</bodyText>
<listItem confidence="0.999173454545455">
• Co-occurrence counts, i.e. the frequencies of
the region labels and entity identifiers that oc-
cur together in the pictures of the training set;
• Pointwise mutual information (PMI) (Fano,
1961) that compares the joint probability of
the occurrence of a (image label, entity iden-
tifier) pair to the independent probability of
the region label and the caption entity occur-
ring by themselves; and finally
• The simplified Student’s t-score as described
in Church and Mercer (1993).
</listItem>
<bodyText confidence="0.999473">
As with the semantic similarity scores, we used
matrices to hold the scores for all the (image la-
bel, entity identifier) pairs for the three association
metrics.
</bodyText>
<subsectionHeader confidence="0.998229">
5.3 The Mapping Algorithm
</subsectionHeader>
<bodyText confidence="0.999984875">
To associate the region labels of an image to the
entities in its caption, we mapped the label LZ to
the caption entity E3 that had the highest score
with respect to LZ. We did this for the three associ-
ation scores and the eight semantic metrics. Note
that a region label is not systematically paired with
the same caption entity, since each caption con-
tains different sets of entities.
</bodyText>
<page confidence="0.989902">
187
</page>
<bodyText confidence="0.999956">
Background and foreground are two of the most
frequent words in the captions and they were fre-
quently assigned to image regions. Since they
rarely represent entities, but merely tell where the
entities are located, we included them in a list of
stop words, as well as middle, left, right, and front
that we removed from the identifiers.
We applied the linking algorithm to the anno-
tated set. We formed the Cartesian product of the
image labels and the entity identifiers and, for each
image region, we ranked the caption entities using
the individual scoring functions. This results in an
ordered list of entity candidates for each region.
Table 2 shows the average ranks of the correct can-
didate for each of the scoring functions and the to-
tal number of correct candidates at different ranks.
car, 2007). The possible values are adjacent or
disjoint for the topological category, beside or hor-
izontally aligned for the horizontal one, and finally
above, below, or vertically aligned for the vertical
one.
</bodyText>
<subsectionHeader confidence="0.999709">
6.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999949125">
The syntactic features are all based on the struc-
ture of the sentences’ dependency graphs. We fol-
lowed the graph from the caption-entity in the pair
to extract its closest ancestors and descendants.
We only considered children to the right of the
candidate. We also included all the prepositions
between the entity and these ancestor and descen-
dant.
</bodyText>
<sectionHeader confidence="0.992394" genericHeader="method">
6 Reranking
</sectionHeader>
<bodyText confidence="0.999931346153846">
The algorithm in Sect. 5.3 determines the relation-
ship holding between a pair of entities, where one
element in the pair comes from the image and the
other from the caption. The entities on each side
are considered in isolation. We extended their de-
scription with relationships inside the image and
the caption. Weegar et al. (2014) showed that pairs
of entities in a text that were linked by the preposi-
tions on, at, with, or in, often corresponded to pairs
of segments that were close to each other. We fur-
ther investigated the idea that spatial relationships
in the image relate to syntactical relationships in
the captions and we implemented it in the form of
a reranker.
For each label-identifier pair, we included the
relationship between the image segment in the pair
and the closest segment in the image. As in Wee-
gar et al. (2014), we defined the closeness as the
Euclidean distance between the gravity centers of
the bounding boxes of the segments. We also
added the relationship between the caption entity
in the label-identifier pair and the entity mentions
which were the closest in the caption. We parsed
the captions and we measured the distance as the
number of edges between the two entities in the
dependency graph.
</bodyText>
<subsectionHeader confidence="0.998702">
6.1 Spatial Features
</subsectionHeader>
<bodyText confidence="0.9998602">
The Segmented and Annotated IAPR TC-12 data
set comes with annotations for three different
types of spatial relationships holding between the
segment pairs in each image: Topological, hori-
zontal, and vertical (Hern´andez-Gracidas and Su-
</bodyText>
<figureCaption confidence="0.9738265">
Figure 2: Dependency graph of the sentence a flat
landscape with a dry meadow in the foreground
</figureCaption>
<bodyText confidence="0.9718161875">
Figure 2 shows the dependency graph of the
sentence a flat landscape with a dry meadow in
the foreground. The descendants of the landscape
entity are meadow and foreground linked respec-
tively by the prepositions with and in. Its an-
cestor is the root node and the distance between
landscape and meadow is 2. The syntactic fea-
tures we extract for the entities in this sentence ar-
ranged in the order ancestor, distance to ancestor,
preposition, descendant, distance to descendant,
and preposition are for landscape, (root, 1, null,
meadow, 2, with) and (root, 1, null, foreground,
2, in), for meadow, (landscape, 2, with, null, –,
null), and for foreground, (landscape, 2, in, null,
–, null). We discard foreground as it is part of the
stop words.
</bodyText>
<subsectionHeader confidence="0.999601">
6.3 Pairing Features
</subsectionHeader>
<bodyText confidence="0.999552">
The single features consist of the label, entity iden-
tifier, and score of the pair. To take interaction
into account, we also paired features characteriz-
ing properties across image and text. The list of
these features is (Table 3):
</bodyText>
<listItem confidence="0.940235">
1. The label of the image region and the identi-
fier of the caption entity. In Fig 2, we create
grass meadow from (grass, meadow).
2. The label of the closest image segment to the
ancestor of the caption entity. The closest
</listItem>
<page confidence="0.967905">
188
</page>
<table confidence="0.999752083333333">
Scoring function Average rank Rank = 1 Rank &lt; 2 Rank &lt; 3 Rank &lt; 4
co-occurrence 1.58 338 525 609 667
PMI 1.61 340 527 624 673
t-scores 1.59 337 540 623 669
PATH 1.19 559 604 643 668
HSO 1.18 574 637 666 691
JCN 1.22 535 580 626 653
LCH 1.19 559 604 643 668
LESK 1.19 560 609 646 670
LIN 1.19 542 581 623 652
RES 1.17 559 611 638 665
WUP 1.21 546 599 640 663
</table>
<tableCaption confidence="0.940852">
Table 2: Average rank of the correct candidate obtained by each scoring function on the 200 annotated
images of the test set, and number of correct candidates that are ranked first, first or second, etc. The
ceiling is 730
</tableCaption>
<table confidence="0.5843165">
Label: Simplified segment label Entity: Identifier for the caption en-
tity
Score: Score given by the current
scoring function
AncDist: Distance between the an-
cestor and the caption entity, and dis-
tance between segments
TopoRel AncPreps: Topological
</table>
<bodyText confidence="0.623594714285714">
relationship between the segments and
the prepositions linking the caption en-
tity with its ancestor
YRel DescPreps: Vertical relation-
ship between segments and the prepo-
sitions linking the caption entity with
its descendant
Anc ClosestSeg: Closest segment
label with the ancestor of the caption
entity
DescDist: Distance between the de-
scendant and the caption entity, and
distance between the segments
XRel DescPreps: Horizontal re-
lationship between segments and the
prepositions linking the caption entity
with its descendant
YRel AncPreps: Vertical relation-
ship between segments and the prepo-
sitions linking the caption entity with
its ancestor
</bodyText>
<table confidence="0.853330333333333">
Label Entity: Label and entity
features combined
Desc ClosestSeg: Closest seg-
ment label with the descendant of the
caption entity
TopoRel DescPreps: Topological
</table>
<tableCaption confidence="0.51114125">
relationship between segments and the
prepositions linking the caption entity
with its descendant
XRel AncPreps: Horizontal rela-
tionship between segments and the
prepositions linking the caption entity
with its ancestor
SegmentDist: Distance (in pix-
els) between the gravity center of the
bounding boxes framing the two clos-
est segments
Table 3: The reranking features using the current segment and its closest segment in the image
</tableCaption>
<bodyText confidence="0.985095457142857">
segment of the grass segment is river and the
ancestor of meadow is landscape. This gives
the paired feature meadow landscape.
The labels of the segments closest to the cur-
rent segment and the descendant of meadow
are also paired.
3. The distance between the segment pairs in
the image divided into seven intervals with
the distance between the caption entities. We
measured the distance in pixels since all the
images have the same pixel dimensions.
4. The spatial relationships of the closest seg-
ments with the prepositions found between
their corresponding caption entities. The seg-
ments grass and river in the image are ad-
jacent and horizontally aligned and grass
is located below the segment labeled river.
Each of the spatial features is paired with the
prepositions for both the ancestor and the de-
scendant.
We trained the reranking models from the pairs
of labeled segments and caption entities, where
the correct mappings formed the positive exam-
ples and the rest, the negative ones. In Fig. 1,
the mapping (grass, meadow) is marked as correct
for the region labeled grass, while the mappings
(grass, lagoon) and (grass, cloud) are marked as
incorrect. We used the manually annotated images
(200 images, Table 1) as training data, a leave-one-
out cross-validation, and L2-regularized logistic
regression from LIBLINEAR (Fan et al., 2008).
We applied a cutoff of 3 for the list of candidates
in the reranking and we multiplied the original
score of the label-identifier pairs with the rerank-
ing probability.
</bodyText>
<subsectionHeader confidence="0.952624">
6.4 Reranking Example
</subsectionHeader>
<tableCaption confidence="0.7869535">
Table 4, upper part, shows the two top candidates
obtained from the co-occurrence scores for the
</tableCaption>
<page confidence="0.98043">
189
</page>
<bodyText confidence="0.435800222222222">
Label Entity 1 Score Entity 2 Score
cloud sky 2207 cloud 1096
grass sky 1489 meadow 887
hill sky 861 cloud 327
river sky 655 cloud 250
cloud cloud 769 sky 422
grass meadow 699 landscape 176
hill landscape 113 cloud 28
river cloud 37 meadow 10
</bodyText>
<figure confidence="0.972807153846154">
Feature
Label
Entity
Label Entity
Score
Anc ClosestSeg
Desc ClosestSeg
AncDist
DescDist
TopoRel DescPrep
TopoRel AncPrep
XRel DescPrep
XRel AncPrep
(grass, meadow) (grass, sky)
grass grass
meadow sky
grass meadow grass sky
881 1,477
landscape river cloud river
lagoon river null river
2 a 2 a
1 a 100 a
adj null adj null
adj with adj in
horiz null horiz null
horiz with horiz in
</figure>
<tableCaption confidence="0.843449">
Table 4: An example of an assignment before (up-
</tableCaption>
<bodyText confidence="0.9994388">
per part) and after (lower part) reranking. The cap-
tion entities are ranked according to the number of
co-occurrences with the label. We obtain the new
score for a label-identifier pair by multiplying the
original score by the output of the reranker for this
pair
four regions in Fig. 1. The column Entity 1 shows
that the scoring function maps the caption entity
sky to all of the regions. We created a reranker’s
feature vector for each of the 8 label-identifier
pairs. Table 5 shows two of them corresponding
to the pairs (grass, sky) and (grass, meadow). The
pair (grass, meadow) is a correct mapping, but it
has a lower co-occurrence score than the incorrect
pair (grass, sky).
In the cross-validation evaluation, we applied
the classifier to these vectors and we obtained the
reranking scores of 0.0244 for (grass, sky) and
0.79 for (grass, meadow) resulting in the respec-
tive final scores of 36 and 699. Table 4, lower
part, shows the new rankings, where the high-
est scores correspond to the associations: (cloud,
cloud), (grass, meadow), (hill, landscape), and
(river, cloud), which are all correct except the last
one.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="evaluation">
7 Results
</sectionHeader>
<subsectionHeader confidence="0.996789">
7.1 Individual Scoring Functions
</subsectionHeader>
<bodyText confidence="0.999956">
We evaluated the three scoring functions: Co-
occurrence, mutual information, and t-score, and
the semantic similarity functions. Each labeled
segment in the annotated set was assigned the
caption-entity that gave the highest scoring label-
identifier pair.
To confront the lack of annotated data we also
investigated a self-training method. We used the
statistical associations we derived from the train-
ing set and we applied the mapping procedure in
Sect. 5.3 to this set. We repeated this procedure
</bodyText>
<table confidence="0.81169625">
YRel DescPrep
YRel AncPrep
SegmentDist 24 24
Classification correct incorrect
</table>
<tableCaption confidence="0.7736355">
Table 5: Feature vectors for the pairs (grass,
meadow) and (grass, sky). The ancestor distance
</tableCaption>
<bodyText confidence="0.971170702702703">
2 a means that there are two edges in the depen-
dency graph between the words meadow and land-
scape, and a represents the smallest of the distance
intervals, meaning that the two segments grass and
river are less than 50 pixels apart
with the three statistical scoring functions. We
counted all the mappings we obtained between the
region labels and the caption identifiers and we
used these counts to create three new scoring func-
tions denoted with a E sign.
Table 6 shows the performance comparison be-
tween the different functions. The second column
shows how many correct mappings were found
by each function. The fourth column shows the
improved score when the stop words were re-
moved. The removal of the stop words as en-
tity candidates improved the co-occurrence and t-
score scoring functions considerably, but provided
only marginal improvement for the scoring func-
tions based on semantic similarity and pointwise
mutual information. The percentage of correct
mappings is based on the 730 regions that have a
matching caption entity in the annotated test set.
The semantic similarity functions – PATH,
HSO, JCN, LCH, LESK, LIN, RES and WUP –
outperform the statistical one and the self-trained
versions of the statistical scoring functions yield
better results than the original ones.
We applied an ensemble voting procedure with
the individual scoring functions, where each func-
tion was given a number of votes to place on its
preferred label-identifier pair. We counted the
votes and the entity that received the majority of
the votes was selected as the mapping for the
current label. Table 7 shows the results, where
below null below null
below with below in
</bodyText>
<page confidence="0.804749">
190
</page>
<table confidence="0.9998980625">
With stop words Without stop words
Function # correct % # correct %
co-oc. 208 28.5 338 46.3
PMI 339 46.4 340 46.6
t-score 241 33.0 337 46.1
� co-oc. 226 30.0 387 53.0
� PMI 457 62.6 458 62.7
� t-score 247 33.8 397 54.4
PATH 552 75.6 559 76.6
HSO 562 77.0 574 78.6
JCN 527 72.2 535 73.3
LCH 552 75.6 559 76.6
LESK 549 75.2 560 76.7
LIN 532 72.9 542 74.2
RES 539 73.8 559 76.6
WUP 540 74.0 546 74.8
</table>
<tableCaption confidence="0.996645">
Table 6: Comparison of the individual scoring
</tableCaption>
<bodyText confidence="0.866998833333333">
functions. This test is performed on the annotated
set of 200 images, with 730 possible correct map-
pings
we reached a maximum 79.45% correct mappings
when all the functions were used together with one
vote each.
</bodyText>
<table confidence="0.956425705882353">
Scoring function Number of votes
co-oc. 1 0 1
PMI 1 0 1
t-score 1 0 1
� co-oc. 1 0 1
� PMI 1 0 1
� t-score 1 0 1
PATH 0 1 1
HSO 0 1 1
JCN 0 1 1
LCH 0 1 1
LESK 0 1 1
LIN 0 1 1
RES 0 1 1
WUP 0 1 1
number correct 382 569 580
percent correct 52 78 79
</table>
<tableCaption confidence="0.9915765">
Table 7: Results of ensemble voting on the anno-
tated set
</tableCaption>
<subsectionHeader confidence="0.997273">
7.2 Reranking
</subsectionHeader>
<bodyText confidence="0.9997205">
We reranked all the scoring functions using the
methods described in Sect. 6. We used the three
label-identifier pairs with the highest score for
each segment and function to build the model and
we also reranked the top three label-identifier pairs
for each of the assignments. Table 8 shows the re-
sults we obtained with the reranker compared to
the original scoring functions. The reranking pro-
</bodyText>
<figureCaption confidence="0.869769">
Figure 3: A comparison of the number of correctly
</figureCaption>
<bodyText confidence="0.951703111111111">
assigned labels when using the different scoring
functions. The leftmost bars show the results of
the original functions, the middle bars show the
performance when the stop words are removed,
and the rightmost ones show the performance of
the reranked functions
cedure improves the performance of all the scoring
functions, especially the statistical ones, where the
maximal improvement reaches 58%.
</bodyText>
<table confidence="0.9987426">
Function correct correct rerank. % Improv.
co-oc. 338 515 52.4
PMI 340 515 51.5
t-score 337 532 57.9
� co-oc. 387 506 30.7
� PMI 458 552 20.5
� t-score 397 521 31.2
PATH 559 587 5.0
HSO 574 587 2.3
JCN 535 558 4.3
LCH 559 586 4.8
LESK 560 563 0.5
LIN 542 558 3.0
RES 559 585 4.7
WUP 546 565 3.5
</table>
<tableCaption confidence="0.997299">
Table 8: The performance of the reranked scoring
</tableCaption>
<bodyText confidence="0.9421205">
functions compared to the original scoring func-
tions
Figure 3 shows the comparison between the
original scoring functions, the scoring functions
without stop words, and the reranked versions.
There is a total of 928 segments, where 730 have
a matching entity in the caption.
We applied an ensemble voting with the
reranked functions (Table 9). Reranking yields a
significant improvement for the statistical scoring
</bodyText>
<figure confidence="0.9939385">
Number of segments 600
400
200
0
</figure>
<bodyText confidence="0.9971068">
functions. When they get one vote each in the en-
semble voting, the results increase from 52% cor-
rect mappings to 75%. When used in an ensemble
with the semantic similarity scoring functions, the
results improve further.
</bodyText>
<figure confidence="0.995327888888889">
Number of votes
Original Reranked
Scoring function
co-oc.
PMI
t-score
0 0
2 3
0 0
</figure>
<table confidence="0.991140647058824">
Scoring function Number of votes
Reranked co-oc. 1 0 1
Reranked PMI 1 0 1
Reranked t-score 1 0 1
Reranked P co-oc. 1 0 1
Reranked P PMI 1 0 1
Reranked P t-score 1 0 1
Reranked PATH 0 1 1
Reranked HSO 0 1 1
Reranked JCN 0 1 1
Reranked LCH 0 1 1
Reranked LESK 0 1 1
Reranked LIN 0 1 1
Reranked RES 0 1 1
Reranked WUP 0 1 1
number correct 546 594 633
percent correct 75 81 87
</table>
<tableCaption confidence="0.9705095">
Table 9: Results of ensemble voting with reranked
assignments segments
</tableCaption>
<bodyText confidence="0.999710111111111">
We also evaluated ensemble voting with differ-
ent numbers of votes for the different functions.
We tested all the permutations of integer weights
in the interval {0,3} on the development set. Ta-
ble 10 shows the best result for both the original
assignments and the reranked assignments on the
test set. The reranked assignments gave the best
results, 88.76% correct mappings, and this is also
the best result we have been able to reach.
</bodyText>
<sectionHeader confidence="0.994841" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999974">
The extraction of relations across text and image
is a new area for research. We showed in this pa-
per that we could use semantic and statistical func-
tions to link the entities in an image to mentions of
the same entities in captions describing this image.
We also showed that using the syntactic structure
of the caption and the spatial structure of the image
improves linking accuracy. Eventually, we man-
aged to map correctly nearly 89% of the image
segments in our data set, counting only segments
that have a matching entity in the caption.
The semantic similarity functions form the most
accurate mapping tool, when using functions in
isolation. The statistical functions improve sig-
</bodyText>
<figure confidence="0.977749846153846">
P co-oc.
P PMI
P t-score
1 1
2 3
0 0
0 0
1 0
2 0
0 0
0 0
number correct
percent correct
</figure>
<tableCaption confidence="0.991889">
Table 10: Results of weighted ensemble voting.
</tableCaption>
<bodyText confidence="0.999896631578947">
nificantly their results when they are used in an
ensemble. This shows that it is preferable to use
multiple scoring functions, as their different prop-
erties contribute to the final score.
Including the syntactic structures of the cap-
tions and pairing them with the spatial structures
of the images is also useful when mapping entities
to segments. By training a model on such features
and using this model to rerank the assignments,
the ordering of entities in the assignments is im-
proved with a better precision for all the scoring
functions.
Although we used images manually annotated
with segments and labels, we believe the meth-
ods we described here can be applied on automati-
cally segmented and labeled images. Using image
recognition would then certainly introduce incor-
rectly classified image regions and thus probably
decrease the linking scores.
</bodyText>
<sectionHeader confidence="0.99814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997175666666667">
This research was supported by Vetenskapsr˚adet
under grant 621-2010-4800, and the Det digitalis-
erade samh¨allet and eSSENCE programs.
</bodyText>
<figure confidence="0.999022384615384">
0 1
2 1
0 1
PATH
HSO
JCN
LCH
LESK
LIN
RES
WUP
298 316
83.71 88.76
</figure>
<page confidence="0.986031">
192
</page>
<sectionHeader confidence="0.997582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934843137255">
Satanjeev Banerjee and Satanjeev Banerjee. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using Wordnet. In Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, pages 136–145.
Kenneth Church and Robert Mercer. 1993. Introduc-
tion to the special issue on computational linguis-
tics using large corpora. Computational Linguistics,
19(1):1–24.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1292–1302, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Hugo Jair Escalantea, Carlos A. Hern´andeza, Je-
sus A. Gonzaleza, A. L´opez-L´opeza, Manuel Mon-
tesa, Eduardo F. Moralesa, L. Enrique Sucara,
Luis Villase˜nora, and Michael Grubinger. 2010.
The segmented and annotated IAPR TC-12 bench-
mark. Computer Vision and Image Understanding,
114:419–428.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Robert Fano. 1961. Transmission of Information: A
Statistical Theory of Communications. MIT Press,
Cambridge, MA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 363–370, Ann Arbor.
Carlos Arturo Hern´andez-Gracidas and Luis Enrique
Sucar. 2007. Markov random fields and spatial in-
formation to improve automatic image annotation.
In Domingo Mery and Luis Rueda, editors, PSIVT,
volume 4872 of Lecture Notes in Computer Science,
pages 879–892. Springer.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Christiane
Fellbaum, editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. CoRR, cmp-lg/9709008.
Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014.
Deep fragment embeddings for bidirectional image
sentence mapping. CoRR, abs/1406.5679.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Baby talk: Understanding and generat-
ing image descriptions. In Proceedings of the 24th
CVPR.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, WordNet: An Electronic Lexical Database. MIT
press, Cambridge, MA.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296–304. Morgan Kaufmann.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41, November.
Iftekhar Naim, Young Chol Song, Qiguang Liu, Henry
Kautz, Jiebo Luo, and Daniel Gildea. 2014. Unsu-
pervised alignment of natural language instructions
with video segments. In Proceedings of the National
Conference on Artificial Intelligence (AAAI-14).
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448–453.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet
Large Scale Visual Recognition Challenge. Interna-
tional Journal of Computer Vision (IJCV).
Hideki Shima. 2014. WordNet Similarity for Java,
February.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the HLT-
NAACL, pages 252–259, Edmonton.
Rebecka Weegar, Linus Hammarlund, Agnes Tegen,
Magnus Oskarsson, Kalle ˚Astr¨om, and Pierre
Nugues. 2014. Visual entity linking: A preliminary
study. In Proceedings of the AAAI 2014 Workshop
on Cognitive Computing for Augmented Human In-
telligence, pages 46–49, Qu´ebec, July 27.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, ACL ’94, pages 133–138,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.999249">
193
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925050">
<title confidence="0.999391">Linking Entities Across Images and Text</title>
<author confidence="0.9621">Weegar Kalle Pierre Nugues</author>
<affiliation confidence="0.99485">DSV Dept. of Mathematics Dept. of Computer Science Stockholm University Lund University Lund</affiliation>
<abstract confidence="0.998820142857143">This paper describes a set of methods to link entities across images and text. As a corpus, we used a data set of images, where each image is commented by a short caption and where the regions in the images are manually segmented and labeled with a category. We extracted the entity mentions from the captions and we computed a semantic similarity between the mentions and the region labels. We also measured the statistical associations between these mentions and the labels and we combined them with the semantic similarity to produce mappings in the form of pairs consisting of a region label and a caption entity. In a second step, we used the syntactic relationships between the mentions and the spatial relationships between the regions to rerank the lists of candidate mappings. To evaluate our methods, we annotated a test set of 200 images, where we manually linked the image regions to their corresponding mentions in the captions. Eventually, we could match objects in pictures to their correct mentions for nearly 89 percent of the segments, when such a matching exists.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Satanjeev Banerjee</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using Wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="9636" citStr="Banerjee and Banerjee, 2002" startWordPosition="1640" endWordPosition="1643">n Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identifier in WordNet, mostly due to misspellings in the caption, like buldings, or buidling, or because of POS-tagging errors. We chose to identify these entities with the word en</context>
</contexts>
<marker>Banerjee, Banerjee, 2002</marker>
<rawString>Satanjeev Banerjee and Satanjeev Banerjee. 2002. An adapted Lesk algorithm for word sense disambiguation using Wordnet. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Robert Mercer</author>
</authors>
<title>Introduction to the special issue on computational linguistics using large corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="11110" citStr="Church and Mercer (1993)" startWordPosition="1883" endWordPosition="1886">cs. 5.2 Statistical Associations We used three functions to reflect the statistical association between an image label and an entity identifier: • Co-occurrence counts, i.e. the frequencies of the region labels and entity identifiers that occur together in the pictures of the training set; • Pointwise mutual information (PMI) (Fano, 1961) that compares the joint probability of the occurrence of a (image label, entity identifier) pair to the independent probability of the region label and the caption entity occurring by themselves; and finally • The simplified Student’s t-score as described in Church and Mercer (1993). As with the semantic similarity scores, we used matrices to hold the scores for all the (image label, entity identifier) pairs for the three association metrics. 5.3 The Mapping Algorithm To associate the region labels of an image to the entities in its caption, we mapped the label LZ to the caption entity E3 that had the highest score with respect to LZ. We did this for the three association scores and the eight semantic metrics. Note that a region label is not systematically paired with the same caption entity, since each caption contains different sets of entities. 187 Background and fore</context>
</contexts>
<marker>Church, Mercer, 1993</marker>
<rawString>Kenneth Church and Robert Mercer. 1993. Introduction to the special issue on computational linguistics using large corpora. Computational Linguistics, 19(1):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image description using visual dependency representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1292--1302</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="4109" citStr="Elliott and Keller (2013)" startWordPosition="692" endWordPosition="695">he captions than there are regions in the images. In addition, for a same entity, the words used to mention it are usually different from the words used as labels (the categories), as in the case of grass and meadow. 3 Previous Work Related work includes the automatic generation of image captions that describes relevant objects in an image and their relationships. Kulkarni et al. (2011) assign each detected image object a visual attribute and a spatial relationship to the other objects in the image. The spatial relationships are translated into selected prepositions in the resulting captions. Elliott and Keller (2013) used manually segmented and labeled images and introduced visual dependency representations (VDRs) that describe spatial relationships between the image objects. The captions are generated using templates. Both Kulkarni et al. (2011) and Elliott and Keller (2013) used the BLEU-score and human evaluators to assess grammatically the generated captions and on how well they describe the image. Although much work has been done to link complete images to a whole text, there are only a few papers on the association of elements inside a text and an image. Naim et al. (2014) analyzed parallel sets of </context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Jair Escalantea</author>
<author>Carlos A Hern´andeza</author>
<author>Jesus A Gonzaleza</author>
<author>A L´opez-L´opeza</author>
<author>Manuel Montesa</author>
<author>Eduardo F Moralesa</author>
<author>L Enrique Sucara</author>
<author>Luis Villase˜nora</author>
<author>Michael Grubinger</author>
</authors>
<date>2010</date>
<booktitle>The segmented and annotated IAPR TC-12 benchmark. Computer Vision and Image Understanding,</booktitle>
<pages>114--419</pages>
<marker>Escalantea, Hern´andeza, Gonzaleza, L´opez-L´opeza, Montesa, Moralesa, Sucara, Villase˜nora, Grubinger, 2010</marker>
<rawString>Hugo Jair Escalantea, Carlos A. Hern´andeza, Jesus A. Gonzaleza, A. L´opez-L´opeza, Manuel Montesa, Eduardo F. Moralesa, L. Enrique Sucara, Luis Villase˜nora, and Michael Grubinger. 2010. The segmented and annotated IAPR TC-12 benchmark. Computer Vision and Image Understanding, 114:419–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="19306" citStr="Fan et al., 2008" startWordPosition="3255" endWordPosition="3258">ial features is paired with the prepositions for both the ancestor and the descendant. We trained the reranking models from the pairs of labeled segments and caption entities, where the correct mappings formed the positive examples and the rest, the negative ones. In Fig. 1, the mapping (grass, meadow) is marked as correct for the region labeled grass, while the mappings (grass, lagoon) and (grass, cloud) are marked as incorrect. We used the manually annotated images (200 images, Table 1) as training data, a leave-oneout cross-validation, and L2-regularized logistic regression from LIBLINEAR (Fan et al., 2008). We applied a cutoff of 3 for the list of candidates in the reranking and we multiplied the original score of the label-identifier pairs with the reranking probability. 6.4 Reranking Example Table 4, upper part, shows the two top candidates obtained from the co-occurrence scores for the 189 Label Entity 1 Score Entity 2 Score cloud sky 2207 cloud 1096 grass sky 1489 meadow 887 hill sky 861 cloud 327 river sky 655 cloud 250 cloud cloud 769 sky 422 grass meadow 699 landscape 176 hill landscape 113 cloud 28 river cloud 37 meadow 10 Feature Label Entity Label Entity Score Anc ClosestSeg Desc Clos</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Communications.</title>
<date>1961</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10826" citStr="Fano, 1961" startWordPosition="1839" endWordPosition="1840">ties with the word entity. The normalization reduced the 3,742 entity identifiers to 2,216 unique ones. Finally, we computed a 250 x 2216 matrix containing the similarity scores for each (image label, entity identifier) pair for each of the WS4J semantic similarity metrics. 5.2 Statistical Associations We used three functions to reflect the statistical association between an image label and an entity identifier: • Co-occurrence counts, i.e. the frequencies of the region labels and entity identifiers that occur together in the pictures of the training set; • Pointwise mutual information (PMI) (Fano, 1961) that compares the joint probability of the occurrence of a (image label, entity identifier) pair to the independent probability of the region label and the caption entity occurring by themselves; and finally • The simplified Student’s t-score as described in Church and Mercer (1993). As with the semantic similarity scores, we used matrices to hold the scores for all the (image label, entity identifier) pairs for the three association metrics. 5.3 The Mapping Algorithm To associate the region labels of an image to the entities in its caption, we mapped the label LZ to the caption entity E3 tha</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Robert Fano. 1961. Transmission of Information: A Statistical Theory of Communications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>363--370</pages>
<location>Ann Arbor.</location>
<contexts>
<context position="6535" citStr="Finkel et al., 2005" startWordPosition="1095" endWordPosition="1098">. Each region is labelled with one out of 275 predefined image labels. The labels are arranged in a hierarchy, where all the nodes are available as labels and where object is the top node. The labels humans, animals, man-made, landscape/nature, food, and other form the next level. 4.2 Entities and Mentions An image caption describes a set of entities, the caption entities CE, where each entity CEi is referred to by a set of mentions M. To detect them, we applied the Stanford CoreNLP pipeline (Toutanova et al., 2003) that consists of a partof-speech tagger, lemmatizer, named entity recognizer (Finkel et al., 2005), dependency parser, and coreference solver. We considered each noun in a caption as an entity candidate. If an entity CEi had only one mention Mj, we identified it by the head noun of its mention. We represented the entities mentioned more than once by the head noun of their most representative mention. We applied the entity extraction to all the captions in the data set, and we found 3,742 different nouns or noun compounds to represent the entities. In addition to the caption entities, each image has a set of labeled segments (or regions) corresponding to the image entities, IE. The Cartesia</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the ACL, pages 363–370, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Arturo Hern´andez-Gracidas</author>
<author>Luis Enrique Sucar</author>
</authors>
<title>Markov random fields and spatial information to improve automatic image annotation.</title>
<date>2007</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4872</volume>
<pages>879--892</pages>
<editor>In Domingo Mery and Luis Rueda, editors, PSIVT,</editor>
<publisher>Springer.</publisher>
<marker>Hern´andez-Gracidas, Sucar, 2007</marker>
<rawString>Carlos Arturo Hern´andez-Gracidas and Luis Enrique Sucar. 2007. Markov random fields and spatial information to improve automatic image annotation. In Domingo Mery and Luis Rueda, editors, PSIVT, volume 4872 of Lecture Notes in Computer Science, pages 879–892. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<location>CoRR, cmp-lg/9709008.</location>
<contexts>
<context position="9515" citStr="Jiang and Conrath, 1997" startWordPosition="1620" endWordPosition="1623">ic Distance The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identifier in WordNet, mostly due to misspellings in the ca</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. CoRR, cmp-lg/9709008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Armand Joulin</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep fragment embeddings for bidirectional image sentence mapping.</title>
<date>2014</date>
<location>CoRR, abs/1406.5679.</location>
<contexts>
<context position="5181" citStr="Karpathy et al. (2014)" startWordPosition="869" endWordPosition="872"> to a whole text, there are only a few papers on the association of elements inside a text and an image. Naim et al. (2014) analyzed parallel sets of videos and written texts, where the videos show laboratory experiments. Written instructions are used to describe how to conduct these experiments. The paper describes models for matching objects detected in the video with mentions of those objects in the instructions. The authors mainly focus on objects that get touched by a hand in the video. For manually annotated videos, Naim et al. (2014) could match objects to nouns nearly 50% of the time. Karpathy et al. (2014) proposed a system for retrieving related images and sentences. They used neural networks and they show that the results are improved if image objects and sentence fragments are included in the model. Sentence fragments are extracted from dependency graphs, where each edge in the graphs corresponds to a fragment. 4 Entity Pairs 4.1 Data Set We used the Segmented and Annotated IAPR TC12 Benchmark data set (Escalantea et al., 2010) that consists of about 20,000 photographs with a wide variety of themes. Each image has a short caption that describes its content, most often consisting of one to th</context>
</contexts>
<marker>Karpathy, Joulin, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014. Deep fragment embeddings for bidirectional image sentence mapping. CoRR, abs/1406.5679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 24th CVPR.</booktitle>
<contexts>
<context position="3873" citStr="Kulkarni et al. (2011)" startWordPosition="655" endWordPosition="658">, together with its caption, illustrates a couple of issues: The objects or regions labelled or visible in an image are not always mentioned in the caption, and for most of the images in the data set, more entities are mentioned in the captions than there are regions in the images. In addition, for a same entity, the words used to mention it are usually different from the words used as labels (the categories), as in the case of grass and meadow. 3 Previous Work Related work includes the automatic generation of image captions that describes relevant objects in an image and their relationships. Kulkarni et al. (2011) assign each detected image object a visual attribute and a spatial relationship to the other objects in the image. The spatial relationships are translated into selected prepositions in the resulting captions. Elliott and Keller (2013) used manually segmented and labeled images and introduced visual dependency representations (VDRs) that describe spatial relationships between the image objects. The captions are generated using templates. Both Kulkarni et al. (2011) and Elliott and Keller (2013) used the BLEU-score and human evaluators to assess grammatically the generated captions and on how </context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Baby talk: Understanding and generating image descriptions. In Proceedings of the 24th CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</booktitle>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9596" citStr="Leacock and Chodorow, 1998" startWordPosition="1634" endWordPosition="1637">imilar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identifier in WordNet, mostly due to misspellings in the caption, like buldings, or buidling, or because of POS-tagging errors. We chose to </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9562" citStr="Lin, 1998" startWordPosition="1631" endWordPosition="1632">re semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identifier in WordNet, mostly due to misspellings in the caption, like buldings, or buidling, or because o</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296–304. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="8799" citStr="Miller, 1995" startWordPosition="1503" endWordPosition="1504">ted a set of guidelines to handle these mappings in a consistent way. Table 1 shows the sizes of the different image sets and the fraction of image regions that have a corresponding entity mention in the caption. Set Files Regions Mappings % Data set 19,176 – – – Train. set 18,976 – – – Test set 200 928 730 78.7 Table 1: The sizes of the different image sets. 5 Ranking Entity Pairs To identify the links between the regions of an image and the entity identifiers in its caption, we first generated all the possible pairs. We then ranked these pairs using a semantic distance derived from WordNet (Miller, 1995), statistical association metrics, and finally, a combination of both techniques. 5.1 Semantic Distance The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as di</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iftekhar Naim</author>
<author>Young Chol Song</author>
<author>Qiguang Liu</author>
<author>Henry Kautz</author>
<author>Jiebo Luo</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised alignment of natural language instructions with video segments.</title>
<date>2014</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI-14).</booktitle>
<contexts>
<context position="4682" citStr="Naim et al. (2014)" startWordPosition="786" endWordPosition="789">resulting captions. Elliott and Keller (2013) used manually segmented and labeled images and introduced visual dependency representations (VDRs) that describe spatial relationships between the image objects. The captions are generated using templates. Both Kulkarni et al. (2011) and Elliott and Keller (2013) used the BLEU-score and human evaluators to assess grammatically the generated captions and on how well they describe the image. Although much work has been done to link complete images to a whole text, there are only a few papers on the association of elements inside a text and an image. Naim et al. (2014) analyzed parallel sets of videos and written texts, where the videos show laboratory experiments. Written instructions are used to describe how to conduct these experiments. The paper describes models for matching objects detected in the video with mentions of those objects in the instructions. The authors mainly focus on objects that get touched by a hand in the video. For manually annotated videos, Naim et al. (2014) could match objects to nouns nearly 50% of the time. Karpathy et al. (2014) proposed a system for retrieving related images and sentences. They used neural networks and they sh</context>
</contexts>
<marker>Naim, Song, Liu, Kautz, Luo, Gildea, 2014</marker>
<rawString>Iftekhar Naim, Young Chol Song, Qiguang Liu, Henry Kautz, Jiebo Luo, and Daniel Gildea. 2014. Unsupervised alignment of natural language instructions with video segments. In Proceedings of the National Conference on Artificial Intelligence (AAAI-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="9484" citStr="Resnik, 1995" startWordPosition="1617" endWordPosition="1618">chniques. 5.1 Semantic Distance The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identifier in WordNet, mostl</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Russakovsky</author>
<author>Jia Deng</author>
<author>Hao Su</author>
<author>Jonathan Krause</author>
<author>Sanjeev Satheesh</author>
<author>Sean Ma</author>
<author>Zhiheng Huang</author>
<author>Andrej Karpathy</author>
<author>Aditya Khosla</author>
<author>Michael Bernstein</author>
<author>Alexander C Berg</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet Large Scale Visual Recognition Challenge.</title>
<date>2015</date>
<journal>International Journal of Computer Vision (IJCV).</journal>
<contexts>
<context position="2203" citStr="Russakovsky et al. (2015)" startWordPosition="362" endWordPosition="366">g the geometric relationships extracted from the images with textual descriptions from the text. A successful mapping would also make it possible to translate knowledge and information across image and text. In this paper, we describe methods to link mentions of entities in captions to labeled image segments and we investigate how the syntactic structure of a caption can be used to better understand the contents of an image. We do not address the closely related task of object recognition in the images. This latter task can be seen as a complement to entity linking across text and images. See Russakovsky et al. (2015) for a description of progress and results to date in object detection and classification in images. 2 An Example Figure 1 shows an example of an image from the Segmented and Annotated IAPR TC-12 data set (Escalantea et al., 2010). It has four regions labeled cloud, grass, hill, and river, and the caption: a flat landscape with a dry meadow in the foreground, a lagoon behind it and many clouds in the sky containing mentions of five entities that we identify with the words meadow, landscape, lagoon, cloud, and sky. A correct association of the mentions in the caption to the image regions would </context>
</contexts>
<marker>Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, Fei-Fei, 2015</marker>
<rawString>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Shima</author>
</authors>
<date>2014</date>
<institution>WordNet Similarity for Java,</institution>
<contexts>
<context position="9245" citStr="Shima, 2014" startWordPosition="1580" endWordPosition="1581">entity identifiers in its caption, we first generated all the possible pairs. We then ranked these pairs using a semantic distance derived from WordNet (Miller, 1995), statistical association metrics, and finally, a combination of both techniques. 5.1 Semantic Distance The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels</context>
</contexts>
<marker>Shima, 2014</marker>
<rawString>Hideki Shima. 2014. WordNet Similarity for Java, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the HLTNAACL,</booktitle>
<pages>252--259</pages>
<location>Edmonton.</location>
<contexts>
<context position="6436" citStr="Toutanova et al., 2003" startWordPosition="1079" endWordPosition="1082">olons. The images are manually segmented into regions with, on average, about 5 segments in each image. Each region is labelled with one out of 275 predefined image labels. The labels are arranged in a hierarchy, where all the nodes are available as labels and where object is the top node. The labels humans, animals, man-made, landscape/nature, food, and other form the next level. 4.2 Entities and Mentions An image caption describes a set of entities, the caption entities CE, where each entity CEi is referred to by a set of mentions M. To detect them, we applied the Stanford CoreNLP pipeline (Toutanova et al., 2003) that consists of a partof-speech tagger, lemmatizer, named entity recognizer (Finkel et al., 2005), dependency parser, and coreference solver. We considered each noun in a caption as an entity candidate. If an entity CEi had only one mention Mj, we identified it by the head noun of its mention. We represented the entities mentioned more than once by the head noun of their most representative mention. We applied the entity extraction to all the captions in the data set, and we found 3,742 different nouns or noun compounds to represent the entities. In addition to the caption entities, each ima</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the HLTNAACL, pages 252–259, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecka Weegar</author>
<author>Linus Hammarlund</author>
<author>Agnes Tegen</author>
<author>Magnus Oskarsson</author>
<author>Kalle ˚Astr¨om</author>
<author>Pierre Nugues</author>
</authors>
<title>Visual entity linking: A preliminary study.</title>
<date>2014</date>
<booktitle>In Proceedings of the AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence,</booktitle>
<pages>46--49</pages>
<location>Qu´ebec,</location>
<marker>Weegar, Hammarlund, Tegen, Oskarsson, ˚Astr¨om, Nugues, 2014</marker>
<rawString>Rebecka Weegar, Linus Hammarlund, Agnes Tegen, Magnus Oskarsson, Kalle ˚Astr¨om, and Pierre Nugues. 2014. Visual entity linking: A preliminary study. In Proceedings of the AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence, pages 46–49, Qu´ebec, July 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9463" citStr="Wu and Palmer, 1994" startWordPosition="1612" endWordPosition="1615">ly, a combination of both techniques. 5.1 Semantic Distance The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). We manually lemmatized and simplified the image labels and the entity mentions so that they are compatible with WordNet entries. It resulted in a smaller set of labels: 250 instead of the 275 original labels. We also simplified the named entities from the captions. When a person or location was not present in WordNet, we used its named entity type as identifier. In some cases, it was not possible to find an entity identif</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>