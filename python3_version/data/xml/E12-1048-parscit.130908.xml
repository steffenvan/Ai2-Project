<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028716">
<title confidence="0.8859755">
Evaluating language understanding accuracy with respect to objective
outcomes in a dialogue system
</title>
<author confidence="0.811195">
Myroslava O. Dzikovska and Peter Bell and Amy Isard and Johanna D. Moore
</author>
<affiliation confidence="0.9908">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh, United Kingdom
</affiliation>
<email confidence="0.997764">
{m.dzikovska,peter.bell,amy.isard,j.moore}@ed.ac.uk
</email>
<sectionHeader confidence="0.994772" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857107142857">
It is not always clear how the differences
in intrinsic evaluation metrics for a parser
or classifier will affect the performance of
the system that uses it. We investigate the
relationship between the intrinsic evalua-
tion scores of an interpretation component
in a tutorial dialogue system and the learn-
ing outcomes in an experiment with human
users. Following the PARADISE method-
ology, we use multiple linear regression to
build predictive models of learning gain,
an important objective outcome metric in
tutorial dialogue. We show that standard
intrinsic metrics such as F-score alone do
not predict the outcomes well. However,
we can build predictive performance func-
tions that account for up to 50% of the vari-
ance in learning gain by combining fea-
tures based on standard evaluation scores
and on the confusion matrix entries. We
argue that building such predictive mod-
els can help us better evaluate performance
of NLP components that cannot be distin-
guished based on F-score alone, and illus-
trate our approach by comparing the cur-
rent interpretation component in the system
to a new classifier trained on the evaluation
data.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999023959183674">
Much of the work in natural language processing
relies on intrinsic evaluation: computing standard
evaluation metrics such as precision, recall and F-
score on the same data set to compare the perfor-
mance of different approaches to the same NLP
problem. However, once a component, such as
a parser, is included in a larger system, it is not
always clear that improvements in intrinsic eval-
uation scores will translate into improved over-
all system performance. Therefore, extrinsic or
task-based evaluation can be used to complement
intrinsic evaluations. For example, NLP com-
ponents such as parsers and co-reference resolu-
tion algorithms could be compared in terms of
how much they contribute to the performance of
a textual entailment (RTE) system (Sammons et
al., 2010; Yuret et al., 2010); parser performance
could be evaluated by how well it contributes to
an information retrieval task (Miyao et al., 2008).
However, task-based evaluation can be difficult
and expensive for interactive applications. Specif-
ically, task-based evaluation for dialogue systems
typically involves collecting data from a number
of people interacting with the system, which is
time-consuming and labor-intensive. Thus, it is
desirable to develop an off-line evaluation pro-
cedure that relates intrinsic evaluation metrics to
predicted interaction outcomes, reducing the need
to conduct experiments with human participants.
This problem can be addressed via the use of
the PARADISE evaluation methodology for spo-
ken dialogue systems (Walker et al., 2000). In a
PARADISE study, after an initial data collection
with users, a performance function is created to
predict an outcome metric (e.g., user satisfaction)
which can normally only be measured through
user surveys. Typically, a multiple linear regres-
sion is used to fit a predictive model of the desired
metric based on the values of interaction param-
eters that can be derived from system logs with-
out additional user studies (e.g., dialogue length,
word error rate, number of misunderstandings).
PARADISE models have been used extensively
in task-oriented spoken dialogue systems to estab-
lish which components of the system most need
improvement, with user satisfaction as the out-
come metric (M¨oller et al., 2007; M¨oller et al.,
2008; Walker et al., 2000; Larsen, 2003). In tu-
torial dialogue, PARADISE studies investigated
</bodyText>
<page confidence="0.982491">
471
</page>
<note confidence="0.9767455">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471–481,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998680766666667">
which manually annotated features predict learn-
ing outcomes, to justify new features needed in
the system (Forbes-Riley et al., 2007; Rotaru and
Litman, 2006; Forbes-Riley and Litman, 2006).
We adapt the PARADISE methodology to eval-
uating individual NLP components, linking com-
monly used intrinsic evaluation scores with ex-
trinsic outcome metrics. We describe an evalua-
tion of an interpretation component of a tutorial
dialogue system, with student learning gain as the
target outcome measure. We first describe the
evaluation setup, which uses standard classifica-
tion accuracy metrics for system evaluation (Sec-
tion 2). We discuss the results of the intrinsic sys-
tem evaluation in Section 3. We then show that
standard evaluation metrics do not serve as good
predictors of system performance for the system
we evaluated. However, adding confusion matrix
features improves the predictive model (Section
4). We argue that in practical applications such
predictive metrics should be used alongside stan-
dard metrics for component evaluations, to bet-
ter predict how different components will perform
in the context of a specific task. We demonstrate
how this technique can help differentiate the out-
put quality between a majority class baseline, the
system’s output, and the output of a new classifier
we trained on our data (Section 5). Finally, we
discuss some limitations and possible extensions
to this approach (Section 6).
</bodyText>
<sectionHeader confidence="0.978257" genericHeader="introduction">
2 Evaluation Procedure
</sectionHeader>
<subsectionHeader confidence="0.985862">
2.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.999967375">
We collected transcripts of students interacting
with BEETLE II (Dzikovska et al., 2010b), a tu-
torial dialogue system for teaching conceptual
knowledge in the basic electricity and electron-
ics domain. The system is a learning environment
with a self-contained curriculum targeted at stu-
dents with no knowledge of high school physics.
When interacting with the system, students spend
3-5 hours going through pre-prepared reading ma-
terial, building and observing circuits in a simula-
tor, and talking with a dialogue-based computer
tutor via a text-based chat interface.
During the interaction, students can be asked
two types of questions. Factual questions require
them to name a set of objects or a simple prop-
erty, e.g., “Which components in circuit 1 are in
a closed path?” or “Are bulbs A and B wired
in series or in parallel”. Explanation and defi-
nition questions require longer answers that con-
sist of 1-2 sentences, e.g., “Why was bulb A on
when switch Z was open?” (expected answer “Be-
cause it was still in a closed path with the bat-
tery”) or “What is voltage?” (expected answer
“Voltage is the difference in states between two
terminals”). We focus on the performance of the
system on these long-answer questions, since re-
acting to them appropriately requires processing
more complex input than factual questions.
We collected a corpus of 35 dialogues from
paid undergraduate volunteers interacting with the
system as part of a formative system evaluation.
Each student completed a multiple-choice test as-
sessing their knowledge of the material before and
after the session. In addition, system logs con-
tained information about how each student’s utter-
ance was interpreted. The resulting data set con-
tains 3426 student answers grouped into 35 sub-
sets, paired with test results. The answers were
then manually annotated to create a gold standard
evaluation corpus.
</bodyText>
<subsectionHeader confidence="0.994424">
2.2 BEETLE II Interpretation Output
</subsectionHeader>
<bodyText confidence="0.999275666666667">
The interpretation component of BEETLE II uses
a syntactic parser and a set of hand-authored rules
to extract the domain-specific semantic represen-
tations of student utterances from the text. The
student answer is first classified with respect to its
domain-specific speech act, as follows:
</bodyText>
<listItem confidence="0.999207235294118">
• Answer: a contentful expression to which
the system responds with a tutoring action,
either accepting it as correct or remediating
the problems as discussed in (Dzikovska et
al., 2010a).
• Help request: any expression indicating that
the student does not know the answer and
without domain content.
• Social: any expression such as “sorry” which
appears to relate to social interaction and has
no recognizable domain content.
• Uninterpretable: the system could not arrive
at any interpretation of the utterance. It will
respond by identifying the likely source of
error, if possible (e.g., a word it does not un-
derstand) and asking the student to rephrase
their utterance (Dzikovska et al., 2009).
</listItem>
<page confidence="0.998187">
472
</page>
<bodyText confidence="0.996778714285714">
If the student utterance was determined to be an
answer, it is further diagnosed for correctness as
discussed in (Dzikovska et al., 2010b), using a do-
main reasoner together with semantic representa-
tions of expected correct answers supplied by hu-
man tutors. The resulting diagnosis contains the
following information:
</bodyText>
<listItem confidence="0.987401210526316">
• Consistency: whether the student statement
correctly describes the facts mentioned in
the question and the simulation environment:
e.g., student saying “Switch X is closed” is
labeled inconsistent if the question stipulated
that this switch is open.
• Diagnosis: an analysis of how well the stu-
dent’s explanation matches the expected an-
swer. It consists of 4 parts
– Matched: parts of the student utterance
that matched the expected answer
– Contradictory: parts of the student ut-
terance that contradict the expected an-
swer
– Extra: parts of the student utterance that
do not appear in the expected answer
– Not-mentioned: parts of the expected
answer missing from the student utter-
ance.
</listItem>
<bodyText confidence="0.9999495">
The speech act and the diagnosis are passed to
the tutorial planner which makes decisions about
feedback. They constitute the output of the inter-
pretation component, and its quality is likely to
affect the learning outcomes, therefore we need
an effective way to evaluate it. In future work,
performance of individual pipeline components
could also be evaluated in a similar fashion.
</bodyText>
<subsectionHeader confidence="0.999454">
2.3 Data Annotation
</subsectionHeader>
<bodyText confidence="0.999989607142857">
The general idea of breaking down the student an-
swer into correct, incorrect and missing parts is
common in tutorial dialogue systems (Nielsen et
al., 2008; Dzikovska et al., 2010b; Jordan et al.,
2006). However, representation details are highly
system specific, and difficult and time-consuming
to annotate. Therefore we implemented a simpli-
fied annotation scheme which classifies whole an-
swers as correct, partially correct but incomplete,
or contradictory, without explicitly identifying the
correct and incorrect parts. This makes it easier to
create the gold standard and still retains useful in-
formation, because tutoring systems often choose
the tutoring strategy based on the general answer
class (correct, incomplete, or contradictory). In
addition, this allows us to cast the problem in
terms of classifier evaluation, and to use standard
classifier evaluation metrics. If more detailed an-
notations were available, this approach could eas-
ily be extended, as discussed in Section 6.
We employed a hierarchical annotation scheme
shown in Figure 1, which is a simplification of
the DeMAND coding scheme (Campbell et al.,
2009). Student utterances were first annotated
as either related to domain content, or not con-
taining any domain content, but expressing the
student’s metacognitive state or attitudes. Utter-
ances expressing domain content were then coded
with respect to their correctness, as being fully
correct, partially correct but incomplete, contain-
ing some errors (rather than just omissions) or
irrelevant1. The “irrelevant” category was used
for utterances which were correct in general but
which did not directly answer the question. Inter-
annotator agreement for this annotation scheme
on the corpus was r. = 0.69.
The speech acts and diagnoses logged by the
system can be automatically mapped into our an-
notation labels. Help requests and social acts
are assigned the “non-content” label; answers
are assigned a label based on which diagnosis
fields were filled: “contradictory” for those an-
swers labeled as either inconsistent, or contain-
ing something in the contradictory field; “incom-
plete” if there is something not mentioned, but
something matched as well, and “irrelevant” if
nothing matched (i.e., the entire expected answer
is in not-mentioned). Finally, uninterpretable ut-
terances are treated as unclassified, analogous to a
situation where a statistical classifier does not out-
put a label for an input because the classification
probability is below its confidence threshold.
This mapping was then compared against the
manually annotated labels to compute the intrin-
sic evaluation scores for the BEETLE II interpreter
described in Section 3.
</bodyText>
<sectionHeader confidence="0.998926" genericHeader="method">
3 Intrinsic Evaluation Results
</sectionHeader>
<bodyText confidence="0.973879166666667">
The interpretation component of BEETLE II was
developed based on the transcripts of 8 sessions
1Several different subcategories of non-content utter-
ances, and of contradictory utterances, were recorded. How-
ever, they resulting classes were too small and so were col-
lapsed into a single category for purposes of this study.
</bodyText>
<page confidence="0.987941">
473
</page>
<bodyText confidence="0.815660545454545">
Category Subcategory Description
Non-content Metacognitive and social expressions without domain content, e.g., “I
don’t know”, “I need help”, “you are stupid”
Content correct The utterance includes domain content.
pc incomplete The student answer is fully correct
contradictory The student said something correct, but incomplete, with some parts of
irrelevant the expected answer missing
The student’s answer contains something incorrect or contradicting the
expected answer, rather than just an omission
The student’s statement is correct in general, but it does not answer the
question.
</bodyText>
<figureCaption confidence="0.997776">
Figure 1: Annotation scheme used in creating the gold standard
</figureCaption>
<table confidence="0.998043333333333">
Label Count Frequency
correct 1438 0.43
pc incomplete 796 0.24
contradictory 808 0.24
irrelevant 105 0.03
non content 232 0.07
</table>
<tableCaption confidence="0.98778">
Table 1: Distribution of annotated labels in the evalu-
ation corpus
</tableCaption>
<bodyText confidence="0.999967051282051">
of students interacting with earlier versions of the
system. These sessions were completed prior to
the beginning of the experiment during which our
evaluation corpus was collected, and are not in-
cluded in the corpus. Thus, the corpus constitutes
unseen testing data for the BEETLE II interpreter.
Table 1 shows the distribution of codes in
the annotated data. The distribution is unbal-
anced, and therefore in our evaluation results we
use two different ways to average over per-class
evaluation scores. Macro-average combines per-
class scores disregarding the class sizes; micro-
average weighs the per-class scores by class size.
The overall classification accuracy (defined as the
number of correctly classified instances out of all
instances) is mathematically equivalent to micro-
averaged recall; however, macro-averaging better
reflects performance on small classes, and is com-
monly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)).
The detailed evaluation results are presented
in Table 2. We will focus on two metrics: the
overall classification accuracy (listed as “micro-
averaged recall” as discussed above), and the
macro-averaged F score.
The majority class baseline is to assign “cor-
rect” to every instance. Its overall accuracy is
43%, the same as BEETLE II. However, this is
obviously not a good choice for a tutoring sys-
tem, since students who make mistakes will never
get tutoring feedback. This is reflected in a much
lower value of the F score (0.12 macroaverage F
score for baseline vs. 0.44 for BEETLE II). Note
also that there is a large difference in the micro-
and macro- averaged scores. It is not immediately
clear which of these metrics is the most important,
and how they relate to actual system performance.
We discuss machine learning models to help an-
swer this question in the next section.
</bodyText>
<sectionHeader confidence="0.893301" genericHeader="method">
4 Linking Evaluation Measures to
Outcome Measures
</sectionHeader>
<bodyText confidence="0.999963952380952">
Although the intrinsic evaluation shows that the
BEETLE II interpreter performs better than the
baseline on the F score, ultimately system devel-
opers are not interested in improving interpreta-
tion for its own sake: they want to know whether
the time spent on improvements, and the compli-
cations in system design which may accompany
them, are worth the effort. Specifically, do such
changes translate into improvement in overall sys-
tem performance?
To answer this question without running expen-
sive user studies we can build a model which pre-
dicts likely outcomes based on the data observed
so far, and then use the model’s predictions as an
additional evaluation metric. We chose a multiple
linear regression model for this task, linking the
classification scores with learning gain as mea-
sured during the data collection. This approach
follows the general PARADISE approach (Walker
et al., 2000), but while PARADISE is typically
used to determine which system components need
</bodyText>
<page confidence="0.991578">
474
</page>
<table confidence="0.999797">
Label baseline BEETLE II
prec. recall F1 prec. recall F1
correct 0.43 1.00 0.60 0.93 0.52 0.67
pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47
contradictory 0.00 0.00 0.00 0.57 0.22 0.31
irrelevant 0.00 0.00 0.00 0.17 0.15 0.16
non-content 0.00 0.00 0.00 0.91 0.41 0.57
macroaverage 0.09 0.20 0.12 0.60 0.37 0.44
microaverage 0.18 0.43 0.25 0.70 0.43 0.51
</table>
<tableCaption confidence="0.998142">
Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline
</tableCaption>
<bodyText confidence="0.981827925925926">
the most improvement, we focus on finding a bet-
ter performance metric for a single component
(interpretation), using standard evaluation scores
as features.
Recall from Section 2.1 that each participant
in our data collection was given a pre-test and
a post-test, measuring their knowledge of course
material. The test score was equal to the propor-
tion of correctly answered questions. The normal-
ized learning gain, post−pre is a metric typically
1−pre
used to assess system quality in intelligent tutor-
ing, and this is the metric we are trying to model.
Thus, the training data for our model consists of
35 instances, each corresponding to a single dia-
logue and the learning gain associated with it. We
can compute intrinsic evaluation scores for each
dialogue, in order to build a model that predicts
that student’s learning gain based on these scores.
If the model’s predictions are sufficiently reliable,
we can also use them for predicting the learning
gain that a student could achieve when interacting
with a new version of the interpretation compo-
nent for the system, not yet tested with users. We
can then use the predicted score to compare dif-
ferent implementations and choose the one with
the highest predicted learning gain.
</bodyText>
<subsectionHeader confidence="0.890301">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.99996815">
Table 4 lists the feature sets we used. We tried two
basic types of features. First, we used the eval-
uation scores reported in the previous section as
features. Second, we hypothesized that some er-
rors that the system makes are likely to be worse
than others from a tutoring perspective. For ex-
ample, if the student gives a contradictory answer,
accepting it as correct may lead to student miscon-
ceptions; on the other hand, calling an irrelevant
answer “partially correct but incomplete” may be
less of a problem. Therefore, we computed sepa-
rate confusion matrices for each student. We nor-
malized each confusion matrix cell by the total
number of incorrect classifications for that stu-
dent. We then added features based on confusion
frequencies to our feature set.2
Ideally, we should add 20 different features to
our model, corresponding to every possible con-
fusion. However, we are facing a sparse data
problem, illustrated by the overall confusion ma-
trix for the corpus in Table 3. For example,
we only observed 25 instances where a contra-
dictory utterance was miscategorized as correct
(compared to 200 “contradictory–pc incomplete”
confusions), and so for many students this mis-
classification was never observed, and predictions
based on this feature are not likely to be reliable.
Therefore, we limited our features to those mis-
classifications that occurred at least twice for each
student (i.e., at least 70 times in the entire cor-
pus). The list of resulting features is shown in the
“conf” row of Table 4. Since only a small num-
ber of features was included, this limits the appli-
cability of the model we derived from this data
set to the systems which make similar types of
confusions. However, it is still interesting to in-
vestigate whether confusion probabilities provide
additional information compared to standard eval-
uation metrics. We discuss how better coverage
could be obtained in Section 6.
</bodyText>
<subsectionHeader confidence="0.941132">
4.2 Regression Models
</subsectionHeader>
<bodyText confidence="0.9923475">
Table 5 shows the regression models we obtained
using different feature sets. All models were ob-
tained using stepwise linear regression, using the
Akaike information criterion (AIC) for variable
</bodyText>
<footnote confidence="0.82162675">
2We also experimented with using % unclassified as an
additional feature, since % of rejections is known to be a
problem for spoken dialogue systems. However, it did not
improve the models, and we do not report it here for brevity.
</footnote>
<page confidence="0.98441">
475
</page>
<table confidence="0.999582142857143">
Actual
Predicted contradictory correct irrelevant non-content pc incomplete
contradictory 175 86 3 0 43
correct 25 752 1 4 26
irrelevant 31 12 16 4 29
non-content 1 3 2 95 3
pc incomplete 200 317 40 28 419
</table>
<tableCaption confidence="0.999967">
Table 3: Confusion matrix for BEETLE II. System predicted values are in rows; actual values in columns.
</tableCaption>
<bodyText confidence="0.998875324324324">
selection implemented in the R stepwise regres-
sion library. As measures of model quality, we re-
port R2, the percentage of variance accounted for
by the models (a typical measure of fit in regres-
sion modeling), and mean squared error (MSE).
These were estimated using leave-one-out cross-
validation, since our data set is small.
We used feature ablation to evaluate the contri-
bution of different features. First, we investigated
models using precision, recall or F-score alone.
As can be seen from the table, precision is not pre-
dictive of learning gain, while F-score and recall
perform similarly to one another, with R2 = 0.12.
In comparison, the model using only confusion
frequencies has substantially higher estimated R2
and a lower MSE.3 In addition, out of the 3 con-
fusion features, only one is selected as predictive.
This supports our hypothesis that different types
of errors may have different importance within a
practical system.
The confusion frequency feature chosen by
the stepwise model (“predicted-pc incomplete-
actual-contradictory”) has a reasonable theoret-
ical justification. Previous research shows that
students who give more correct or partially cor-
rect answers, either in human-human or human-
computer dialogue, exhibit higher learning gains,
and this has been established for different sys-
tems and tutoring domains (Litman et al., 2009).
Consequently, % of contradictory answers is neg-
atively predictive of learning gain. It is reasonable
to suppose, as predicted by our model, that sys-
tems that do not identify such answers well, and
therefore do not remediate them correctly, will do
worse in terms of learning outcomes.
Based on this initial finding, we investigated
the models that combined either F scores or the
</bodyText>
<footnote confidence="0.9820545">
3The decrease in MSE is not statistically significant, pos-
sibly because of the small data set. However, since we ob-
serve the same pattern of results across our models, it is still
useful to examine.
</footnote>
<bodyText confidence="0.999748692307692">
full set of intrinsic evaluation scores with confu-
sion frequencies. Note that if the full set of met-
rics (precision, recall, F score) is used, the model
derives a more complex formula which covers
about 33% of the variance. Our best models,
however, combine the averaged scores with con-
fusion frequencies, resulting in a higher R2 and
a lower MSE (22% relative decrease between the
“scores.f” and “conf+scores.f” models in the ta-
ble). This shows that these features have comple-
mentary information, and that combining them in
an application-specific way may help to predict
how the components will behave in practice.
</bodyText>
<sectionHeader confidence="0.777206" genericHeader="method">
5 Using prediction models in evaluation
</sectionHeader>
<bodyText confidence="0.999872208333333">
The models from Table 5 can be used to compare
different possible implementations of the inter-
pretation component, under the assumption that
the component with a higher predicted learning
gain score is more appropriate to use in an ITS.
To show how our predictive models can be used
in making implementation decisions, we compare
three possible choices for an interpretation com-
ponent: the original BEETLE II interpreter, the
baseline classifier described earlier, and a new de-
cision tree classifier trained on our data.
We built a decision tree classifier using the
Weka implementation of C4.5 pruned decision
trees, with default parameters. As features, we
used lexical similarity scores computed by the
Text::Similarity package4. We computed
8 features: the similarity between student answer
and either the expected answer text or the question
text, using 4 different scores: raw number of over-
lapping words, F1 score, lesk score and cosine
score. Its intrinsic evaluation scores are shown in
Table 6, estimated using 10-fold cross-validation.
We can compare BEETLE II and baseline clas-
sifier using the “scores.all” model. The predicted
</bodyText>
<footnote confidence="0.970342">
4http://search.cpan.org/dist/Text-Similarity/
</footnote>
<page confidence="0.995884">
476
</page>
<table confidence="0.999542692307692">
Name Variables
scores.fm fmeasure.microaverage, fmeasure.macroaverage, fmeasure.correct,
fmeasure.contradictory, fmeasure.pc incomplete,fmeasure.non-content,
fmeasure.irrelevant
scores.precision precision.microaverage, precision.macroaverage, precision.correct,
precision.contradictory, precision.pc incomplete,precision.non-content,
precision.irrelevant
scores.recall recall.microaverage, recall.macroaverage, recall.correct, recall.contradictory,
recall.pc incomplete,recall.non-content, recall.irrelevant
scores.all scores.fm + scores.precision + scores.recall
conf Freq.predicted.contradictory.actual.correct,
Freq.predicted.pc incomplete.actual.correct,
Freq.predicted.pc incomplete.actual.contradictory
</table>
<tableCaption confidence="0.983112">
Table 4: Feature sets for regression models
</tableCaption>
<table confidence="0.999960482758621">
Variables Cross- Cross- Formula
validation validation
R2 MSE
scores.f 0.12 0.0232 0.32
(0.02) (0.0302) + 0.56 * fmeasure.microaverage
scores.precision 0.00 0.0242 0.61
(0.00) (0.0370)
scores.recall 0.12 0.0232 0.37
(0.02) (0.0310) + 0.56 * recall.microaverage
conf 0.25 0.0197 0.74
(0.03) (0.0262) − 0.56 *
Freq.predicted.pc incomplete.actual.contradictory
scores.all 0.33 0.0218 0.63
(0.03) (0.0264) + 4.20 * fmeasure.microaverage
− 1.30 * precision.microaverage
− 2.79 * recall.microaverage
− 0.07 * recall.non − content
conf+scores.f 0.36 0.0179 0.52
(0.03) (0.0281) − 0.66 *
Freq.predicted.pc incomplete.actual.contradictory
+ 0.42 * fmeasure.correct
− 0.07 * fmeasure.non − content
full 0.49 0.0189 0.88
(conf+scores.all) (0.02) (0.0248) − 0.68 *
Freq.predicted.pc incomplete.actual.contradictory
− 0.06 * precision.non domain
+ 0.28 * recall.correct
− 0.79 * precision.microaverage
+ 0.65 * fmeasure.microaverage
</table>
<tableCaption confidence="0.9975905">
Table 5: Regression models for learning gain. R2 and MSE estimated with leave-one-out cross-validation.
Standard deviation in parentheses.
</tableCaption>
<page confidence="0.998082">
477
</page>
<bodyText confidence="0.999888333333333">
score for BEETLE II is 0.66. The predicted
score for the baseline is 0.28. We cannot use
the models based on confusion scores (“conf”,
“conf+scores.f” or “full”) for evaluating the base-
line, because the confusions it makes are always
to predict that the answer is correct when the
actual label is “incomplete” or “contradictory”.
Such situations were too rare in our training data,
and therefore were not included in the models (as
discussed in Section 4.1). Additional data will
need to be collected before this model can rea-
sonably predict baseline behavior.
Compared to our new classifier, BEETLE II has
lower overall accuracy (0.43 vs. 0.53), but per-
forms micro- and macro- averaged scores. BEE-
TLE II precision is higher than that of the classi-
fier. This is not unexpected given how the system
was designed: since misunderstandings caused
dialogue breakdown in pilot tests, the interpreter
was built to prefer rejecting utterances as uninter-
pretable rather than assigning them to an incorrect
class, leading to high precision but lower recall.
However, we can use all our predictive models
to evaluate the classifier. We checked the the con-
fusion matrix (not shown here due to space lim-
itations), and saw that the classifier made some
of the same types of confusions that BEETLE II
interpreter made. On the “scores.all” model, the
predicted learning gain score for the classifier is
0.63, also very close to BEETLE II. But with the
“conf+scores.all” model, the predicted score is
0.89, compared to 0.59 for BEETLE II, indicating
that we should prefer the newly built classifier.
Looking at individual class performance, the
classifier performs better than the BEETLE II in-
terpreter on identifying “correct” and “contradic-
tory” answers, but does not do as well for par-
tially correct but incomplete, and for irrelevant an-
swers. Using our predictive performance metric
highlights the differences between the classifiers
and effectively helps determine which confusion
types are the most important.
One limitation of this prediction, however, is
that the original system’s output is considerably
more complex: the BEETLE II interpreter explic-
itly identifies correct, incorrect and missing parts
of the student answer which are then used by the
system to formulate adaptive feedback. This is
an important feature of the system because it al-
lows for implementation of strategies such as ac-
knowledging and restating correct parts of the an-
</bodyText>
<table confidence="0.996333625">
Label prec. recall F1
correct 0.66 0.76 0.71
pc incomplete 0.38 0.34 0.36
contradictory 0.40 0.35 0.37
irrelevant 0.07 0.04 0.05
non-content 0.62 0.76 0.68
macroaverage 0.43 0.45 0.43
microaverage 0.51 0.53 0.52
</table>
<tableCaption confidence="0.9937195">
Table 6: Intrinsic evaluation scores for our newly built
classifier.
</tableCaption>
<bodyText confidence="0.9997773">
swer. However, we could still use a classifier to
“double-check” the interpreter’s output. If the
predictions made by the original interpreter and
the classifier differ, and in particular when the
classifier assigns the “contradictory” label to an
answer, BEETLE II may choose to use a generic
strategy for contradictory utterances, e.g. telling
the student that their answer is incorrect without
specifying the exact problem, or asking them to
re-read portions of the material.
</bodyText>
<sectionHeader confidence="0.991816" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999968925925926">
In this paper, we proposed an approach for cost-
sensitive evaluation of language interpretation
within practical applications. Our approach is
based on the PARADISE methodology for dia-
logue system evaluation (Walker et al., 2000).
We followed the typical pattern of a PARADISE
study, but instead of relying on a variety of fea-
tures that characterize the interaction, we used
scores that reflect only the performance of the
interpretation component. For BEETLE II we
could build regression models that account for
nearly 50% variance in the desired outcomes, on
par with models reported in earlier PARADISE
studies (M¨oller et al., 2007; M¨oller et al., 2008;
Walker et al., 2000; Larsen, 2003). More impor-
tantly, we demonstrated that combining averaged
scores with features based on confusion frequen-
cies improves prediction quality and allows us to
see differences between systems which are not ob-
vious from the scores alone.
Previous work on task-based evaluation of NLP
components used RTE or information extraction
as target tasks (Sammons et al., 2010; Yuret et al.,
2010; Miyao et al., 2008), based on standard cor-
pora. We specifically targeted applications which
involve human-computer interaction, where run-
ning task-based evaluations is particularly expen-
</bodyText>
<page confidence="0.996568">
478
</page>
<bodyText confidence="0.999515215189874">
sive, and building a predictive model of system
performance can simplify system development.
Our evaluation data limited the set of features
that we could use in our models. For most con-
fusion features, there were not enough instances
in the data to build a model that would reliably
predict learning gain for those cases. One way
to solve this problem would be to conduct a user
study in which the system simulates random er-
rors appearing some of the time. This could pro-
vide the data needed for more accurate models.
The general pattern we observed in our data
is that a model based on F-scores alone predicts
only a small proportion of the variance. If a full
set of metrics (including F-score, precision and
recall) is used, linear regression derives a more
complex equation, with different weights for pre-
cision and recall. Instead of the linear model, we
may consider using a model based on Fβ score,
Fβ _ (1 + Q2) PR
β&apos;P �R, and fitting it to the data to
derive the Q weight rather than using the standard
F1 score. We plan to investigate this in the future.
Our method would apply to a wide range of
systems. It can be used straightforwardly with
many current spoken dialogue systems which rely
on classifiers to support language understanding
in domains such as call routing and technical sup-
port (Gupta et al., 2006; Acomb et al., 2007).
We applied it to a system that outputs more com-
plex logical forms, but we showed that we could
simplify its output to a set of labels which still
allowed us to make informed decisions. Simi-
lar simplifications could be derived for other sys-
tems based on domain-specific dialogue acts typ-
ically used in dialogue management. For slot-
based systems, it may be useful to consider con-
cept accuracy for recognizing individual slot val-
ues. Finally, for tutoring systems it is possible
to annotate the answers on a more fine-grained
level. Nielsen et al. (2008) proposed an annota-
tion scheme based on the output of a dependency
parser, and trained a classifier to identify individ-
ual dependencies as “expressed”, “contradicted”
or “unaddressed”. Their system could be evalu-
ated using the same approach.
The specific formulas we derived are not likely
to be highly generalizable. It is a well-known
limitation of PARADISE evaluations that models
built based on one system often do not perform
well when applied to different systems (M¨oller et
al., 2008). But using them to compare implemen-
tation variants during the system development,
without re-running user evaluations, can provide
important information, as we illustrated with an
example of evaluating a new classifier we built for
our interpretation task. Moreover, the confusion
frequency feature that our models picked is con-
sistent with earlier results from a different tutor-
ing domain (see Section 4.2). Thus, these models
could provide a starting point when making sys-
tem development choices, which can then be con-
firmed by user evaluations in new domains.
The models we built do not fully account for
the variance in the training data. This is expected,
since interpretation performance is not the only
factor influencing the objective outcome: other
factors, such choosing the the appropriate tutor-
ing strategy, are also important. Similar models
could be built for other system components to ac-
count for their contribution to the variance. Fi-
nally, we could consider using different learning
algorithms. M¨oller et al. (2008) examined deci-
sion trees and neural networks in addition to mul-
tiple linear regression for predicting user satisfac-
tion in spoken dialogue. They found that neural
networks had the best prediction performance for
their task. We plan to explore other learning algo-
rithms for this task as part of our future work.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997275">
In this paper, we described an evaluation of an
interpretation component of a tutorial dialogue
system using predictive models that link intrin-
sic evaluation scores with learning outcomes. We
showed that adding features based on confusion
frequencies for individual classes significantly
improves the prediction. This approach can be
used to compare different implementations of lan-
guage interpretation components, and to decide
which option to use, based on the predicted im-
provement in a task-specific target outcome met-
ric trained on previous evaluation data.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999568714285714">
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation; and Christo-
pher Brew for helpful comments and discussion.
This work has been supported in part by the US
ONR award N000141010085.
</bodyText>
<page confidence="0.998889">
479
</page>
<sectionHeader confidence="0.982159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844955752212">
Kate Acomb, Jonathan Bloom, Krishna Dayanidhi,
Phillip Hunter, Peter Krogh, Esther Levin, and
Roberto Pieraccini. 2007. Technical support dia-
log systems: Issues, problems, and solutions. In
Proceedings of the Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Tech-
nologies, pages 25–31, Rochester, NY, April.
Gwendolyn C. Campbell, Natalie B. Steinhauser,
Myroslava O. Dzikovska, Johanna D. Moore,
Charles B. Callaway, and Elaine Farrow. 2009. The
DeMAND coding scheme: A “common language”
for representing and analyzing student discourse. In
Proceedings of 14th International Conference on
Artificial Intelligence in Education (AIED), poster
session, Brighton, UK, July.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn E. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages
38–45, London, UK, September.
Myroslava Dzikovska, Diana Bental, Johanna D.
Moore, Natalie B. Steinhauser, Gwendolyn E.
Campbell, Elaine Farrow, and Charles B. Callaway.
2010a. Intelligent tutoring with natural language
support in the Beetle II system. In Sustaining TEL:
From Innovation to Learning and Practice - 5th Eu-
ropean Conference on Technology Enhanced Learn-
ing, (EC-TEL 2010), Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a sys-
tem for tutoring and computational linguistics ex-
perimentation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2010) demo session, Uppsala, Swe-
den, July.
Kate Forbes-Riley and Diane J. Litman. 2006. Mod-
elling user satisfaction and student learning in a
spoken dialogue tutoring system with generic, tu-
toring, and user affect parameters. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL
’06), pages 264–271, Stroudsburg, PA, USA.
Kate Forbes-Riley, Diane Litman, Amruta Purandare,
Mihai Rotaru, and Joel Tetreault. 2007. Compar-
ing linguistic features for modeling learning in com-
puter tutoring. In Proceedings of the 2007 confer-
ence on Artificial Intelligence in Education: Build-
ing Technology Rich Learning Contexts That Work,
pages 270–277, Amsterdam, The Netherlands. IOS
Press.
Narendra K. Gupta, G¨okhan T¨ur, Dilek Hakkani-T¨ur,
Srinivas Bangalore, Giuseppe Riccardi, and Mazin
Gilbert. 2006. The AT&amp;T spoken language un-
derstanding system. IEEE Transactions on Audio,
Speech &amp; Language Processing, 14(1):213–222.
Pamela W. Jordan, Maxim Makatchev, and Umarani
Pappuswamy. 2006. Understanding complex nat-
ural language explanations in tutorial applications.
In Proceedings of the Third Workshop on Scalable
Natural Language Understanding, ScaNaLU ’06,
pages 17–24.
Lars Bo Larsen. 2003. Issues in the evaluation of spo-
ken dialogue systems using objective and subjective
measures. In Proceedings of the 2003 IEEE Work-
shop on Automatic Speech Recognition and Under-
standing, pages 209–214.
David D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the workshop on Speech and Nat-
ural Language, HLT ’91, pages 312–318, Strouds-
burg, PA, USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural lan-
guage processing to analyze tutorial dialogue cor-
pora across domains and modalities. In Proceed-
ings of 14th International Conference on Artificial
Intelligence in Education (AIED), Brighton, UK,
July.
Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08: HLT, pages 46–
54, Columbus, Ohio, June.
Sebastian M¨oller, Paula Smeele, Heleen Boland, and
Jan Krebber. 2007. Evaluating spoken dialogue
systems according to de-facto standards: A case
study. Computer Speech &amp; Language, 21(1):26 –
53.
Sebastian M¨oller, Klaus-Peter Engelbrecht, and
Robert Schleicher. 2008. Predicting the quality and
usability of spoken dialogue services. Speech Com-
munication, pages 730–744.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Mihai Rotaru and Diane J. Litman. 2006. Exploit-
ing discourse structure for spoken dialogue perfor-
mance analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’06, pages 85–93, Strouds-
burg, PA, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2010. “Ask not what textual entailment can
do for you...”. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1199–1208, Uppsala, Sweden, July.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
</reference>
<page confidence="0.975685">
480
</page>
<reference confidence="0.997933">
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
51–56, Uppsala, Sweden, July.
</reference>
<page confidence="0.998705">
481
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619562">
<title confidence="0.9739785">Evaluating language understanding accuracy with respect to outcomes in a dialogue system</title>
<author confidence="0.778162">O Dzikovska Bell Isard D</author>
<affiliation confidence="0.858806">Institute for Language, Cognition and School of Informatics, University of Edinburgh, United</affiliation>
<abstract confidence="0.997652206896552">It is not always clear how the differences in intrinsic evaluation metrics for a parser or classifier will affect the performance of the system that uses it. We investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users. Following the PARADISE methodology, we use multiple linear regression to build predictive models of learning gain, an important objective outcome metric in tutorial dialogue. We show that standard intrinsic metrics such as F-score alone do not predict the outcomes well. However, we can build predictive performance functions that account for up to 50% of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries. We argue that building such predictive models can help us better evaluate performance of NLP components that cannot be distinguished based on F-score alone, and illustrate our approach by comparing the current interpretation component in the system to a new classifier trained on the evaluation data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kate Acomb</author>
<author>Jonathan Bloom</author>
<author>Krishna Dayanidhi</author>
<author>Phillip Hunter</author>
<author>Peter Krogh</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>Technical support dialog systems: Issues, problems, and solutions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,</booktitle>
<pages>25--31</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="32398" citStr="Acomb et al., 2007" startWordPosition="5027" endWordPosition="5030">, linear regression derives a more complex equation, with different weights for precision and recall. Instead of the linear model, we may consider using a model based on Fβ score, Fβ _ (1 + Q2) PR β&apos;P �R, and fitting it to the data to derive the Q weight rather than using the standard F1 score. We plan to investigate this in the future. Our method would apply to a wide range of systems. It can be used straightforwardly with many current spoken dialogue systems which rely on classifiers to support language understanding in domains such as call routing and technical support (Gupta et al., 2006; Acomb et al., 2007). We applied it to a system that outputs more complex logical forms, but we showed that we could simplify its output to a set of labels which still allowed us to make informed decisions. Similar simplifications could be derived for other systems based on domain-specific dialogue acts typically used in dialogue management. For slotbased systems, it may be useful to consider concept accuracy for recognizing individual slot values. Finally, for tutoring systems it is possible to annotate the answers on a more fine-grained level. Nielsen et al. (2008) proposed an annotation scheme based on the out</context>
</contexts>
<marker>Acomb, Bloom, Dayanidhi, Hunter, Krogh, Levin, Pieraccini, 2007</marker>
<rawString>Kate Acomb, Jonathan Bloom, Krishna Dayanidhi, Phillip Hunter, Peter Krogh, Esther Levin, and Roberto Pieraccini. 2007. Technical support dialog systems: Issues, problems, and solutions. In Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies, pages 25–31, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gwendolyn C Campbell</author>
<author>Natalie B Steinhauser</author>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
<author>Charles B Callaway</author>
<author>Elaine Farrow</author>
</authors>
<title>The DeMAND coding scheme: A “common language” for representing and analyzing student discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), poster session,</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="10984" citStr="Campbell et al., 2009" startWordPosition="1705" endWordPosition="1708">rrect parts. This makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory). In addition, this allows us to cast the problem in terms of classifier evaluation, and to use standard classifier evaluation metrics. If more detailed annotations were available, this approach could easily be extended, as discussed in Section 6. We employed a hierarchical annotation scheme shown in Figure 1, which is a simplification of the DeMAND coding scheme (Campbell et al., 2009). Student utterances were first annotated as either related to domain content, or not containing any domain content, but expressing the student’s metacognitive state or attitudes. Utterances expressing domain content were then coded with respect to their correctness, as being fully correct, partially correct but incomplete, containing some errors (rather than just omissions) or irrelevant1. The “irrelevant” category was used for utterances which were correct in general but which did not directly answer the question. Interannotator agreement for this annotation scheme on the corpus was r. = 0.6</context>
</contexts>
<marker>Campbell, Steinhauser, Dzikovska, Moore, Callaway, Farrow, 2009</marker>
<rawString>Gwendolyn C. Campbell, Natalie B. Steinhauser, Myroslava O. Dzikovska, Johanna D. Moore, Charles B. Callaway, and Elaine Farrow. 2009. The DeMAND coding scheme: A “common language” for representing and analyzing student discourse. In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), poster session, Brighton, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Charles B Callaway</author>
<author>Elaine Farrow</author>
<author>Johanna D Moore</author>
<author>Natalie B Steinhauser</author>
<author>Gwendolyn E Campbell</author>
</authors>
<title>Dealing with interpretation errors in tutorial dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>38--45</pages>
<location>London, UK,</location>
<contexts>
<context position="8433" citStr="Dzikovska et al., 2009" startWordPosition="1307" endWordPosition="1310">ng action, either accepting it as correct or remediating the problems as discussed in (Dzikovska et al., 2010a). • Help request: any expression indicating that the student does not know the answer and without domain content. • Social: any expression such as “sorry” which appears to relate to social interaction and has no recognizable domain content. • Uninterpretable: the system could not arrive at any interpretation of the utterance. It will respond by identifying the likely source of error, if possible (e.g., a word it does not understand) and asking the student to rephrase their utterance (Dzikovska et al., 2009). 472 If the student utterance was determined to be an answer, it is further diagnosed for correctness as discussed in (Dzikovska et al., 2010b), using a domain reasoner together with semantic representations of expected correct answers supplied by human tutors. The resulting diagnosis contains the following information: • Consistency: whether the student statement correctly describes the facts mentioned in the question and the simulation environment: e.g., student saying “Switch X is closed” is labeled inconsistent if the question stipulated that this switch is open. • Diagnosis: an analysis </context>
</contexts>
<marker>Dzikovska, Callaway, Farrow, Moore, Steinhauser, Campbell, 2009</marker>
<rawString>Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore, Natalie B. Steinhauser, and Gwendolyn E. Campbell. 2009. Dealing with interpretation errors in tutorial dialogue. In Proceedings of the SIGDIAL 2009 Conference, pages 38–45, London, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava Dzikovska</author>
<author>Diana Bental</author>
<author>Johanna D Moore</author>
<author>Natalie B Steinhauser</author>
<author>Gwendolyn E Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Intelligent tutoring with natural language support in the Beetle II system.</title>
<date>2010</date>
<booktitle>In Sustaining TEL: From Innovation to Learning and Practice - 5th European Conference on Technology Enhanced Learning, (EC-TEL 2010),</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5622" citStr="Dzikovska et al., 2010" startWordPosition="856" endWordPosition="859"> applications such predictive metrics should be used alongside standard metrics for component evaluations, to better predict how different components will perform in the context of a specific task. We demonstrate how this technique can help differentiate the output quality between a majority class baseline, the system’s output, and the output of a new classifier we trained on our data (Section 5). Finally, we discuss some limitations and possible extensions to this approach (Section 6). 2 Evaluation Procedure 2.1 Data Collection We collected transcripts of students interacting with BEETLE II (Dzikovska et al., 2010b), a tutorial dialogue system for teaching conceptual knowledge in the basic electricity and electronics domain. The system is a learning environment with a self-contained curriculum targeted at students with no knowledge of high school physics. When interacting with the system, students spend 3-5 hours going through pre-prepared reading material, building and observing circuits in a simulator, and talking with a dialogue-based computer tutor via a text-based chat interface. During the interaction, students can be asked two types of questions. Factual questions require them to name a set of o</context>
<context position="7919" citStr="Dzikovska et al., 2010" startWordPosition="1224" endWordPosition="1227">ired with test results. The answers were then manually annotated to create a gold standard evaluation corpus. 2.2 BEETLE II Interpretation Output The interpretation component of BEETLE II uses a syntactic parser and a set of hand-authored rules to extract the domain-specific semantic representations of student utterances from the text. The student answer is first classified with respect to its domain-specific speech act, as follows: • Answer: a contentful expression to which the system responds with a tutoring action, either accepting it as correct or remediating the problems as discussed in (Dzikovska et al., 2010a). • Help request: any expression indicating that the student does not know the answer and without domain content. • Social: any expression such as “sorry” which appears to relate to social interaction and has no recognizable domain content. • Uninterpretable: the system could not arrive at any interpretation of the utterance. It will respond by identifying the likely source of error, if possible (e.g., a word it does not understand) and asking the student to rephrase their utterance (Dzikovska et al., 2009). 472 If the student utterance was determined to be an answer, it is further diagnosed</context>
<context position="10029" citStr="Dzikovska et al., 2010" startWordPosition="1563" endWordPosition="1566">er missing from the student utterance. The speech act and the diagnosis are passed to the tutorial planner which makes decisions about feedback. They constitute the output of the interpretation component, and its quality is likely to affect the learning outcomes, therefore we need an effective way to evaluate it. In future work, performance of individual pipeline components could also be evaluated in a similar fashion. 2.3 Data Annotation The general idea of breaking down the student answer into correct, incorrect and missing parts is common in tutorial dialogue systems (Nielsen et al., 2008; Dzikovska et al., 2010b; Jordan et al., 2006). However, representation details are highly system specific, and difficult and time-consuming to annotate. Therefore we implemented a simplified annotation scheme which classifies whole answers as correct, partially correct but incomplete, or contradictory, without explicitly identifying the correct and incorrect parts. This makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory). In addition, this allows us to c</context>
</contexts>
<marker>Dzikovska, Bental, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava Dzikovska, Diana Bental, Johanna D. Moore, Natalie B. Steinhauser, Gwendolyn E. Campbell, Elaine Farrow, and Charles B. Callaway. 2010a. Intelligent tutoring with natural language support in the Beetle II system. In Sustaining TEL: From Innovation to Learning and Practice - 5th European Conference on Technology Enhanced Learning, (EC-TEL 2010), Barcelona, Spain, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
<author>Natalie Steinhauser</author>
<author>Gwendolyn Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Beetle II: a system for tutoring and computational linguistics experimentation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-2010) demo session,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5622" citStr="Dzikovska et al., 2010" startWordPosition="856" endWordPosition="859"> applications such predictive metrics should be used alongside standard metrics for component evaluations, to better predict how different components will perform in the context of a specific task. We demonstrate how this technique can help differentiate the output quality between a majority class baseline, the system’s output, and the output of a new classifier we trained on our data (Section 5). Finally, we discuss some limitations and possible extensions to this approach (Section 6). 2 Evaluation Procedure 2.1 Data Collection We collected transcripts of students interacting with BEETLE II (Dzikovska et al., 2010b), a tutorial dialogue system for teaching conceptual knowledge in the basic electricity and electronics domain. The system is a learning environment with a self-contained curriculum targeted at students with no knowledge of high school physics. When interacting with the system, students spend 3-5 hours going through pre-prepared reading material, building and observing circuits in a simulator, and talking with a dialogue-based computer tutor via a text-based chat interface. During the interaction, students can be asked two types of questions. Factual questions require them to name a set of o</context>
<context position="7919" citStr="Dzikovska et al., 2010" startWordPosition="1224" endWordPosition="1227">ired with test results. The answers were then manually annotated to create a gold standard evaluation corpus. 2.2 BEETLE II Interpretation Output The interpretation component of BEETLE II uses a syntactic parser and a set of hand-authored rules to extract the domain-specific semantic representations of student utterances from the text. The student answer is first classified with respect to its domain-specific speech act, as follows: • Answer: a contentful expression to which the system responds with a tutoring action, either accepting it as correct or remediating the problems as discussed in (Dzikovska et al., 2010a). • Help request: any expression indicating that the student does not know the answer and without domain content. • Social: any expression such as “sorry” which appears to relate to social interaction and has no recognizable domain content. • Uninterpretable: the system could not arrive at any interpretation of the utterance. It will respond by identifying the likely source of error, if possible (e.g., a word it does not understand) and asking the student to rephrase their utterance (Dzikovska et al., 2009). 472 If the student utterance was determined to be an answer, it is further diagnosed</context>
<context position="10029" citStr="Dzikovska et al., 2010" startWordPosition="1563" endWordPosition="1566">er missing from the student utterance. The speech act and the diagnosis are passed to the tutorial planner which makes decisions about feedback. They constitute the output of the interpretation component, and its quality is likely to affect the learning outcomes, therefore we need an effective way to evaluate it. In future work, performance of individual pipeline components could also be evaluated in a similar fashion. 2.3 Data Annotation The general idea of breaking down the student answer into correct, incorrect and missing parts is common in tutorial dialogue systems (Nielsen et al., 2008; Dzikovska et al., 2010b; Jordan et al., 2006). However, representation details are highly system specific, and difficult and time-consuming to annotate. Therefore we implemented a simplified annotation scheme which classifies whole answers as correct, partially correct but incomplete, or contradictory, without explicitly identifying the correct and incorrect parts. This makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory). In addition, this allows us to c</context>
</contexts>
<marker>Dzikovska, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava O. Dzikovska, Johanna D. Moore, Natalie Steinhauser, Gwendolyn Campbell, Elaine Farrow, and Charles B. Callaway. 2010b. Beetle II: a system for tutoring and computational linguistics experimentation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-2010) demo session, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane J Litman</author>
</authors>
<title>Modelling user satisfaction and student learning in a spoken dialogue tutoring system with generic, tutoring, and user affect parameters.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06),</booktitle>
<pages>264--271</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4256" citStr="Forbes-Riley and Litman, 2006" startWordPosition="645" endWordPosition="648"> components of the system most need improvement, with user satisfaction as the outcome metric (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). In tutorial dialogue, PARADISE studies investigated 471 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471–481, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-Riley et al., 2007; Rotaru and Litman, 2006; Forbes-Riley and Litman, 2006). We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics. We describe an evaluation of an interpretation component of a tutorial dialogue system, with student learning gain as the target outcome measure. We first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (Section 2). We discuss the results of the intrinsic system evaluation in Section 3. We then show that standard evaluation metrics do not serve as good predictors of system performa</context>
</contexts>
<marker>Forbes-Riley, Litman, 2006</marker>
<rawString>Kate Forbes-Riley and Diane J. Litman. 2006. Modelling user satisfaction and student learning in a spoken dialogue tutoring system with generic, tutoring, and user affect parameters. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06), pages 264–271, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
<author>Amruta Purandare</author>
<author>Mihai Rotaru</author>
<author>Joel Tetreault</author>
</authors>
<title>Comparing linguistic features for modeling learning in computer tutoring.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 conference on Artificial Intelligence in Education: Building Technology Rich Learning Contexts That Work,</booktitle>
<pages>270--277</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="4199" citStr="Forbes-Riley et al., 2007" startWordPosition="637" endWordPosition="640">-oriented spoken dialogue systems to establish which components of the system most need improvement, with user satisfaction as the outcome metric (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). In tutorial dialogue, PARADISE studies investigated 471 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471–481, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-Riley et al., 2007; Rotaru and Litman, 2006; Forbes-Riley and Litman, 2006). We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics. We describe an evaluation of an interpretation component of a tutorial dialogue system, with student learning gain as the target outcome measure. We first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (Section 2). We discuss the results of the intrinsic system evaluation in Section 3. We then show that standard evaluation m</context>
</contexts>
<marker>Forbes-Riley, Litman, Purandare, Rotaru, Tetreault, 2007</marker>
<rawString>Kate Forbes-Riley, Diane Litman, Amruta Purandare, Mihai Rotaru, and Joel Tetreault. 2007. Comparing linguistic features for modeling learning in computer tutoring. In Proceedings of the 2007 conference on Artificial Intelligence in Education: Building Technology Rich Learning Contexts That Work, pages 270–277, Amsterdam, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narendra K Gupta</author>
<author>G¨okhan T¨ur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Srinivas Bangalore</author>
<author>Giuseppe Riccardi</author>
<author>Mazin Gilbert</author>
</authors>
<title>The AT&amp;T spoken language understanding system.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Gupta, T¨ur, Hakkani-T¨ur, Bangalore, Riccardi, Gilbert, 2006</marker>
<rawString>Narendra K. Gupta, G¨okhan T¨ur, Dilek Hakkani-T¨ur, Srinivas Bangalore, Giuseppe Riccardi, and Mazin Gilbert. 2006. The AT&amp;T spoken language understanding system. IEEE Transactions on Audio, Speech &amp; Language Processing, 14(1):213–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Maxim Makatchev</author>
<author>Umarani Pappuswamy</author>
</authors>
<title>Understanding complex natural language explanations in tutorial applications.</title>
<date>2006</date>
<booktitle>In Proceedings of the Third Workshop on Scalable Natural Language Understanding, ScaNaLU ’06,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="10052" citStr="Jordan et al., 2006" startWordPosition="1567" endWordPosition="1570">nt utterance. The speech act and the diagnosis are passed to the tutorial planner which makes decisions about feedback. They constitute the output of the interpretation component, and its quality is likely to affect the learning outcomes, therefore we need an effective way to evaluate it. In future work, performance of individual pipeline components could also be evaluated in a similar fashion. 2.3 Data Annotation The general idea of breaking down the student answer into correct, incorrect and missing parts is common in tutorial dialogue systems (Nielsen et al., 2008; Dzikovska et al., 2010b; Jordan et al., 2006). However, representation details are highly system specific, and difficult and time-consuming to annotate. Therefore we implemented a simplified annotation scheme which classifies whole answers as correct, partially correct but incomplete, or contradictory, without explicitly identifying the correct and incorrect parts. This makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory). In addition, this allows us to cast the problem in term</context>
</contexts>
<marker>Jordan, Makatchev, Pappuswamy, 2006</marker>
<rawString>Pamela W. Jordan, Maxim Makatchev, and Umarani Pappuswamy. 2006. Understanding complex natural language explanations in tutorial applications. In Proceedings of the Third Workshop on Scalable Natural Language Understanding, ScaNaLU ’06, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Bo Larsen</author>
</authors>
<title>Issues in the evaluation of spoken dialogue systems using objective and subjective measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>209--214</pages>
<contexts>
<context position="3799" citStr="Larsen, 2003" startWordPosition="582" endWordPosition="583">ich can normally only be measured through user surveys. Typically, a multiple linear regression is used to fit a predictive model of the desired metric based on the values of interaction parameters that can be derived from system logs without additional user studies (e.g., dialogue length, word error rate, number of misunderstandings). PARADISE models have been used extensively in task-oriented spoken dialogue systems to establish which components of the system most need improvement, with user satisfaction as the outcome metric (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). In tutorial dialogue, PARADISE studies investigated 471 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471–481, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-Riley et al., 2007; Rotaru and Litman, 2006; Forbes-Riley and Litman, 2006). We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic o</context>
<context position="30477" citStr="Larsen, 2003" startWordPosition="4704" endWordPosition="4705"> interpretation within practical applications. Our approach is based on the PARADISE methodology for dialogue system evaluation (Walker et al., 2000). We followed the typical pattern of a PARADISE study, but instead of relying on a variety of features that characterize the interaction, we used scores that reflect only the performance of the interpretation component. For BEETLE II we could build regression models that account for nearly 50% variance in the desired outcomes, on par with models reported in earlier PARADISE studies (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). More importantly, we demonstrated that combining averaged scores with features based on confusion frequencies improves prediction quality and allows us to see differences between systems which are not obvious from the scores alone. Previous work on task-based evaluation of NLP components used RTE or information extraction as target tasks (Sammons et al., 2010; Yuret et al., 2010; Miyao et al., 2008), based on standard corpora. We specifically targeted applications which involve human-computer interaction, where running task-based evaluations is particularly expen478 sive, and building a pred</context>
</contexts>
<marker>Larsen, 2003</marker>
<rawString>Lars Bo Larsen. 2003. Issues in the evaluation of spoken dialogue systems using objective and subjective measures. In Proceedings of the 2003 IEEE Workshop on Automatic Speech Recognition and Understanding, pages 209–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Evaluating text categorization.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>312--318</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14684" citStr="Lewis, 1991" startWordPosition="2263" endWordPosition="2264">he annotated data. The distribution is unbalanced, and therefore in our evaluation results we use two different ways to average over per-class evaluation scores. Macro-average combines perclass scores disregarding the class sizes; microaverage weighs the per-class scores by class size. The overall classification accuracy (defined as the number of correctly classified instances out of all instances) is mathematically equivalent to microaveraged recall; however, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., (Lewis, 1991)). The detailed evaluation results are presented in Table 2. We will focus on two metrics: the overall classification accuracy (listed as “microaveraged recall” as discussed above), and the macro-averaged F score. The majority class baseline is to assign “correct” to every instance. Its overall accuracy is 43%, the same as BEETLE II. However, this is obviously not a good choice for a tutoring system, since students who make mistakes will never get tutoring feedback. This is reflected in a much lower value of the F score (0.12 macroaverage F score for baseline vs. 0.44 for BEETLE II). Note also</context>
</contexts>
<marker>Lewis, 1991</marker>
<rawString>David D. Lewis. 1991. Evaluating text categorization. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 312–318, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Johanna Moore</author>
<author>Myroslava Dzikovska</author>
<author>Elaine Farrow</author>
</authors>
<title>Using natural language processing to analyze tutorial dialogue corpora across domains and modalities.</title>
<date>2009</date>
<booktitle>In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED),</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="22317" citStr="Litman et al., 2009" startWordPosition="3513" endWordPosition="3516">E.3 In addition, out of the 3 confusion features, only one is selected as predictive. This supports our hypothesis that different types of errors may have different importance within a practical system. The confusion frequency feature chosen by the stepwise model (“predicted-pc incompleteactual-contradictory”) has a reasonable theoretical justification. Previous research shows that students who give more correct or partially correct answers, either in human-human or humancomputer dialogue, exhibit higher learning gains, and this has been established for different systems and tutoring domains (Litman et al., 2009). Consequently, % of contradictory answers is negatively predictive of learning gain. It is reasonable to suppose, as predicted by our model, that systems that do not identify such answers well, and therefore do not remediate them correctly, will do worse in terms of learning outcomes. Based on this initial finding, we investigated the models that combined either F scores or the 3The decrease in MSE is not statistically significant, possibly because of the small data set. However, since we observe the same pattern of results across our models, it is still useful to examine. full set of intrins</context>
</contexts>
<marker>Litman, Moore, Dzikovska, Farrow, 2009</marker>
<rawString>Diane Litman, Johanna Moore, Myroslava Dzikovska, and Elaine Farrow. 2009. Using natural language processing to analyze tutorial dialogue corpora across domains and modalities. In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), Brighton, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Sætre</author>
<author>Kenji Sagae</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>46--54</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2404" citStr="Miyao et al., 2008" startWordPosition="370" endWordPosition="373">arser, is included in a larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved overall system performance. Therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations. For example, NLP components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of a textual entailment (RTE) system (Sammons et al., 2010; Yuret et al., 2010); parser performance could be evaluated by how well it contributes to an information retrieval task (Miyao et al., 2008). However, task-based evaluation can be difficult and expensive for interactive applications. Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive. Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants. This problem can be addressed via the use of the PARADISE evaluation methodology for spoken dialogue system</context>
<context position="30881" citStr="Miyao et al., 2008" startWordPosition="4766" endWordPosition="4769">regression models that account for nearly 50% variance in the desired outcomes, on par with models reported in earlier PARADISE studies (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). More importantly, we demonstrated that combining averaged scores with features based on confusion frequencies improves prediction quality and allows us to see differences between systems which are not obvious from the scores alone. Previous work on task-based evaluation of NLP components used RTE or information extraction as target tasks (Sammons et al., 2010; Yuret et al., 2010; Miyao et al., 2008), based on standard corpora. We specifically targeted applications which involve human-computer interaction, where running task-based evaluations is particularly expen478 sive, and building a predictive model of system performance can simplify system development. Our evaluation data limited the set of features that we could use in our models. For most confusion features, there were not enough instances in the data to build a model that would reliably predict learning gain for those cases. One way to solve this problem would be to conduct a user study in which the system simulates random errors</context>
</contexts>
<marker>Miyao, Sætre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of ACL-08: HLT, pages 46– 54, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian M¨oller</author>
<author>Paula Smeele</author>
<author>Heleen Boland</author>
<author>Jan Krebber</author>
</authors>
<title>Evaluating spoken dialogue systems according to de-facto standards: A case study.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>1</issue>
<marker>M¨oller, Smeele, Boland, Krebber, 2007</marker>
<rawString>Sebastian M¨oller, Paula Smeele, Heleen Boland, and Jan Krebber. 2007. Evaluating spoken dialogue systems according to de-facto standards: A case study. Computer Speech &amp; Language, 21(1):26 – 53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian M¨oller</author>
<author>Klaus-Peter Engelbrecht</author>
<author>Robert Schleicher</author>
</authors>
<title>Predicting the quality and usability of spoken dialogue services. Speech Communication,</title>
<date>2008</date>
<pages>730--744</pages>
<marker>M¨oller, Engelbrecht, Schleicher, 2008</marker>
<rawString>Sebastian M¨oller, Klaus-Peter Engelbrecht, and Robert Schleicher. 2008. Predicting the quality and usability of spoken dialogue services. Speech Communication, pages 730–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Learning to assess low-level conceptual understanding.</title>
<date>2008</date>
<booktitle>In Proceedings 21st International FLAIRS Conference, Coconut</booktitle>
<location>Grove, Florida,</location>
<contexts>
<context position="10005" citStr="Nielsen et al., 2008" startWordPosition="1559" endWordPosition="1562">s of the expected answer missing from the student utterance. The speech act and the diagnosis are passed to the tutorial planner which makes decisions about feedback. They constitute the output of the interpretation component, and its quality is likely to affect the learning outcomes, therefore we need an effective way to evaluate it. In future work, performance of individual pipeline components could also be evaluated in a similar fashion. 2.3 Data Annotation The general idea of breaking down the student answer into correct, incorrect and missing parts is common in tutorial dialogue systems (Nielsen et al., 2008; Dzikovska et al., 2010b; Jordan et al., 2006). However, representation details are highly system specific, and difficult and time-consuming to annotate. Therefore we implemented a simplified annotation scheme which classifies whole answers as correct, partially correct but incomplete, or contradictory, without explicitly identifying the correct and incorrect parts. This makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory). In addit</context>
<context position="32951" citStr="Nielsen et al. (2008)" startWordPosition="5122" endWordPosition="5125">uting and technical support (Gupta et al., 2006; Acomb et al., 2007). We applied it to a system that outputs more complex logical forms, but we showed that we could simplify its output to a set of labels which still allowed us to make informed decisions. Similar simplifications could be derived for other systems based on domain-specific dialogue acts typically used in dialogue management. For slotbased systems, it may be useful to consider concept accuracy for recognizing individual slot values. Finally, for tutoring systems it is possible to annotate the answers on a more fine-grained level. Nielsen et al. (2008) proposed an annotation scheme based on the output of a dependency parser, and trained a classifier to identify individual dependencies as “expressed”, “contradicted” or “unaddressed”. Their system could be evaluated using the same approach. The specific formulas we derived are not likely to be highly generalizable. It is a well-known limitation of PARADISE evaluations that models built based on one system often do not perform well when applied to different systems (M¨oller et al., 2008). But using them to compare implementation variants during the system development, without re-running user e</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2008</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2008. Learning to assess low-level conceptual understanding. In Proceedings 21st International FLAIRS Conference, Coconut Grove, Florida, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Rotaru</author>
<author>Diane J Litman</author>
</authors>
<title>Exploiting discourse structure for spoken dialogue performance analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>85--93</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4224" citStr="Rotaru and Litman, 2006" startWordPosition="641" endWordPosition="644">ystems to establish which components of the system most need improvement, with user satisfaction as the outcome metric (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). In tutorial dialogue, PARADISE studies investigated 471 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471–481, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-Riley et al., 2007; Rotaru and Litman, 2006; Forbes-Riley and Litman, 2006). We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics. We describe an evaluation of an interpretation component of a tutorial dialogue system, with student learning gain as the target outcome measure. We first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (Section 2). We discuss the results of the intrinsic system evaluation in Section 3. We then show that standard evaluation metrics do not serve as go</context>
</contexts>
<marker>Rotaru, Litman, 2006</marker>
<rawString>Mihai Rotaru and Diane J. Litman. 2006. Exploiting discourse structure for spoken dialogue performance analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 85–93, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sammons</author>
<author>V G Vinod Vydiswaran</author>
<author>Dan Roth</author>
</authors>
<title>Ask not what textual entailment can do for you...”.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1199--1208</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2263" citStr="Sammons et al., 2010" startWordPosition="347" endWordPosition="350">Fscore on the same data set to compare the performance of different approaches to the same NLP problem. However, once a component, such as a parser, is included in a larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved overall system performance. Therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations. For example, NLP components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of a textual entailment (RTE) system (Sammons et al., 2010; Yuret et al., 2010); parser performance could be evaluated by how well it contributes to an information retrieval task (Miyao et al., 2008). However, task-based evaluation can be difficult and expensive for interactive applications. Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive. Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct exp</context>
<context position="30840" citStr="Sammons et al., 2010" startWordPosition="4758" endWordPosition="4761">n component. For BEETLE II we could build regression models that account for nearly 50% variance in the desired outcomes, on par with models reported in earlier PARADISE studies (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). More importantly, we demonstrated that combining averaged scores with features based on confusion frequencies improves prediction quality and allows us to see differences between systems which are not obvious from the scores alone. Previous work on task-based evaluation of NLP components used RTE or information extraction as target tasks (Sammons et al., 2010; Yuret et al., 2010; Miyao et al., 2008), based on standard corpora. We specifically targeted applications which involve human-computer interaction, where running task-based evaluations is particularly expen478 sive, and building a predictive model of system performance can simplify system development. Our evaluation data limited the set of features that we could use in our models. For most confusion features, there were not enough instances in the data to build a model that would reliably predict learning gain for those cases. One way to solve this problem would be to conduct a user study in</context>
</contexts>
<marker>Sammons, Vydiswaran, Roth, 2010</marker>
<rawString>Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth. 2010. “Ask not what textual entailment can do for you...”. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199–1208, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards Developing General Models of Usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="3027" citStr="Walker et al., 2000" startWordPosition="457" endWordPosition="460">owever, task-based evaluation can be difficult and expensive for interactive applications. Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive. Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants. This problem can be addressed via the use of the PARADISE evaluation methodology for spoken dialogue systems (Walker et al., 2000). In a PARADISE study, after an initial data collection with users, a performance function is created to predict an outcome metric (e.g., user satisfaction) which can normally only be measured through user surveys. Typically, a multiple linear regression is used to fit a predictive model of the desired metric based on the values of interaction parameters that can be derived from system logs without additional user studies (e.g., dialogue length, word error rate, number of misunderstandings). PARADISE models have been used extensively in task-oriented spoken dialogue systems to establish which </context>
<context position="16513" citStr="Walker et al., 2000" startWordPosition="2563" endWordPosition="2566"> the complications in system design which may accompany them, are worth the effort. Specifically, do such changes translate into improvement in overall system performance? To answer this question without running expensive user studies we can build a model which predicts likely outcomes based on the data observed so far, and then use the model’s predictions as an additional evaluation metric. We chose a multiple linear regression model for this task, linking the classification scores with learning gain as measured during the data collection. This approach follows the general PARADISE approach (Walker et al., 2000), but while PARADISE is typically used to determine which system components need 474 Label baseline BEETLE II prec. recall F1 prec. recall F1 correct 0.43 1.00 0.60 0.93 0.52 0.67 pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47 contradictory 0.00 0.00 0.00 0.57 0.22 0.31 irrelevant 0.00 0.00 0.00 0.17 0.15 0.16 non-content 0.00 0.00 0.00 0.91 0.41 0.57 macroaverage 0.09 0.20 0.12 0.60 0.37 0.44 microaverage 0.18 0.43 0.25 0.70 0.43 0.51 Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baseline the most improvement, we focus on finding a better performance metric for a s</context>
<context position="30013" citStr="Walker et al., 2000" startWordPosition="4626" endWordPosition="4629">y the original interpreter and the classifier differ, and in particular when the classifier assigns the “contradictory” label to an answer, BEETLE II may choose to use a generic strategy for contradictory utterances, e.g. telling the student that their answer is incorrect without specifying the exact problem, or asking them to re-read portions of the material. 6 Discussion and Future Work In this paper, we proposed an approach for costsensitive evaluation of language interpretation within practical applications. Our approach is based on the PARADISE methodology for dialogue system evaluation (Walker et al., 2000). We followed the typical pattern of a PARADISE study, but instead of relying on a variety of features that characterize the interaction, we used scores that reflect only the performance of the interpretation component. For BEETLE II we could build regression models that account for nearly 50% variance in the desired outcomes, on par with models reported in earlier PARADISE studies (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). More importantly, we demonstrated that combining averaged scores with features based on confusion frequencies improves prediction qual</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Aydin Han</author>
<author>Zehra Turgut</author>
</authors>
<title>SemEval-2010 task 12: Parser evaluation using textual entailments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>51--56</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2284" citStr="Yuret et al., 2010" startWordPosition="351" endWordPosition="354">a set to compare the performance of different approaches to the same NLP problem. However, once a component, such as a parser, is included in a larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved overall system performance. Therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations. For example, NLP components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of a textual entailment (RTE) system (Sammons et al., 2010; Yuret et al., 2010); parser performance could be evaluated by how well it contributes to an information retrieval task (Miyao et al., 2008). However, task-based evaluation can be difficult and expensive for interactive applications. Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive. Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human p</context>
<context position="30860" citStr="Yuret et al., 2010" startWordPosition="4762" endWordPosition="4765">E II we could build regression models that account for nearly 50% variance in the desired outcomes, on par with models reported in earlier PARADISE studies (M¨oller et al., 2007; M¨oller et al., 2008; Walker et al., 2000; Larsen, 2003). More importantly, we demonstrated that combining averaged scores with features based on confusion frequencies improves prediction quality and allows us to see differences between systems which are not obvious from the scores alone. Previous work on task-based evaluation of NLP components used RTE or information extraction as target tasks (Sammons et al., 2010; Yuret et al., 2010; Miyao et al., 2008), based on standard corpora. We specifically targeted applications which involve human-computer interaction, where running task-based evaluations is particularly expen478 sive, and building a predictive model of system performance can simplify system development. Our evaluation data limited the set of features that we could use in our models. For most confusion features, there were not enough instances in the data to build a model that would reliably predict learning gain for those cases. One way to solve this problem would be to conduct a user study in which the system si</context>
</contexts>
<marker>Yuret, Han, Turgut, 2010</marker>
<rawString>Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using textual entailments. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 51–56, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>