<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.987962">
Semantic Frame Identification with Distributed Word Representations
</title>
<author confidence="0.994358">
Karl Moritz Hermann$* Dipanjan Das† Jason Weston† Kuzman Ganchev†
</author>
<affiliation confidence="0.9354055">
$Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom
† Google Inc., 76 9th Avenue, New York, NY 10011, United States
</affiliation>
<email confidence="0.982115">
karl.moritz.hermann@cs.ox.ac.uk
{dipanjand,kuzman}@google.com jaseweston@gmail.com
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999295">
We present a novel technique for semantic
frame identification using distributed rep-
resentations of predicates and their syntac-
tic context; this technique leverages auto-
matic syntactic parses and a generic set
of word embeddings. Given labeled data
annotated with frame-semantic parses, we
learn a model that projects the set of word
representations for the syntactic context
around a predicate to a low dimensional
representation. The latter is used for se-
mantic frame identification; with a stan-
dard argument identification method in-
spired by prior work, we achieve state-of-
the-art results on FrameNet-style frame-
semantic analysis. Additionally, we report
strong results on PropBank-style semantic
role labeling in comparison to prior work.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999082357142857">
Distributed representations of words have proved
useful for a number of tasks. By providing richer
representations of meaning than what can be en-
compassed in a discrete representation, such ap-
proaches have successfully been applied to tasks
such as sentiment analysis (Socher et al., 2011),
topic classification (Klementiev et al., 2012) or
word-word similarity (Mitchell and Lapata, 2008).
We present a new technique for semantic frame
identification that leverages distributed word rep-
resentations. According to the theory of frame se-
mantics (Fillmore, 1982), a semantic frame rep-
resents an event or scenario, and possesses frame
elements (or semantic roles) that participate in the
</bodyText>
<footnote confidence="0.791676">
∗The majority of this research was carried out during an
internship at Google.
</footnote>
<bodyText confidence="0.99861684">
event. Most work on frame-semantic parsing has
usually divided the task into two major subtasks:
frame identification, namely the disambiguation of
a given predicate to a frame, and argument iden-
tification (or semantic role labeling), the analysis
of words and phrases in the sentential context that
satisfy the frame’s semantic roles (Das et al., 2010;
Das et al., 2014).1 Here, we focus on the first sub-
task of frame identification for given predicates;
we use our novel method (§3) in conjunction with
a standard argument identification model (§4) to
perform full frame-semantic parsing.
We present experiments on two tasks. First, we
show that for frame identification on the FrameNet
corpus (Baker et al., 1998; Fillmore et al., 2003),
we outperform the prior state of the art (Das et al.,
2014). Moreover, for full frame-semantic parsing,
with the presented frame identification technique
followed by our argument identification method,
we report the best results on this task to date. Sec-
ond, we present results on PropBank-style seman-
tic role labeling (Palmer et al., 2005; Meyers et al.,
2004; M`arquez et al., 2008), that approach strong
baselines, and are on par with prior state of the art
(Punyakanok et al., 2008).
</bodyText>
<sectionHeader confidence="0.996311" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999038">
Early work in frame-semantic analysis was pio-
neered by Gildea and Jurafsky (2002). Subsequent
work in this area focused on either the FrameNet
or PropBank frameworks, and research on the lat-
ter has been more popular. Since the CoNLL
2004-2005 shared tasks (Carreras and M`arquez,
</bodyText>
<footnote confidence="0.9921868">
1There are exceptions, wherein the task has been modeled
using a pipeline of three classifiers that perform frame iden-
tification, a binary stage that classifies candidate arguments,
and argument identification on the filtered candidates (Baker
et al., 2007; Johansson and Nugues, 2007).
</footnote>
<page confidence="0.895443">
1448
</page>
<note confidence="0.9199105">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448–1458,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997978666666667">
Figure 1: Example sentences with frame-semantic analyses.
FrameNet annotation conventions are used in (a) while (b)
denotes PropBank conventions.
</figureCaption>
<bodyText confidence="0.9981405">
2004; Carreras and M`arquez, 2005) on PropBank
semantic role labeling (SRL), it has been treated
as an important NLP problem. However, research
has mostly focused on argument analysis, skipping
the frame disambiguation step, and its interaction
with argument identification.
</bodyText>
<subsectionHeader confidence="0.847766">
2.1 Frame-Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.993619095890411">
Closely related to SRL, frame-semantic parsing
consists of the resolution of predicate sense into
a frame, and the analysis of the frame’s argu-
ments. Work in this area exclusively uses the
FrameNet full text annotations. Johansson and
Nugues (2007) presented the best performing sys-
tem at SemEval 2007 (Baker et al., 2007), and Das
et al. (2010) improved performance, and later set
the current state of the art on this task (Das et al.,
2014). We briefly discuss FrameNet, and subse-
quently PropBank annotation conventions here.
FrameNet The FrameNet project (Baker et al.,
1998) is a lexical database that contains informa-
tion about words and phrases (represented as lem-
mas conjoined with a coarse part-of-speech tag)
termed as lexical units, with a set of semantic
frames that they could evoke. For each frame,
there is a list of associated frame elements (or
roles, henceforth), that are also distinguished as
core or non-core.2 Sentences are annotated us-
ing this universal frame inventory. For exam-
ple, consider the pair of sentences in Figure 1(a).
COMMERCE BUY is a frame that can be evoked by
morphological variants of the two example lexical
units buy.V and sell.V. Buyer, Seller and Goods are
some example roles for this frame.
2Additional information such as finer distinction of the
coreness properties of roles, the relationship between frames,
and that of roles are also present, but we do not leverage that
information in this work.
PropBank The PropBank project (Palmer et al.,
2005) is another popular resource related to se-
mantic role labeling. The PropBank corpus has
verbs annotated with sense frames and their ar-
guments. Like FrameNet, it also has a lexi-
cal database that stores type information about
verbs, in the form of sense frames and the possi-
ble semantic roles each frame could take. There
are modifier roles that are shared across verb
frames, somewhat similar to the non-core roles
in FrameNet. Figure 1(b) shows annotations for
two verbs “bought” and “sold”, with their lemmas
(akin to the lexical units in FrameNet) and their
verb frames buy.01 and sell.01. Generic core role
labels (of which there are seven, namely A0-A5 and
AA) for the verb frames are marked in the figure.3
A key difference between the two annotation sys-
tems is that PropBank uses a local frame inven-
tory, where frames are predicate-specific. More-
over, role labels, although few in number, take spe-
cific meaning for each verb frame. Figure 1 high-
lights this difference: while both sell.V and buy.V
are members of the same frame in FrameNet, they
evoke different frames in PropBank. In spite of
this difference, nearly identical statistical models
could be employed for both frameworks.
Modeling In this paper, we model the frame-
semantic parsing problem in two stages: frame
identification and argument identification. As
mentioned in §1, these correspond to a frame dis-
ambiguation stage,4 and a stage that finds the var-
ious arguments that fulfill the frame’s semantic
roles within the sentence, respectively. This re-
sembles the framework of Das et al. (2014), who
solely focus on FrameNet corpora, unlike this pa-
per. The novelty of this paper lies in the frame
identification stage (§3). Note that this two-stage
approach is unusual for the PropBank corpora
when compared to prior work, where the vast ma-
jority of published papers have not focused on the
verb frame disambiguation problem at all, only fo-
cusing on the role labeling stage (see the overview
paper of M`arquez et al. (2008) for example).
</bodyText>
<footnote confidence="0.9976835">
3NomBank (Meyers et al., 2004) is a similar resource for
nominal predicates, but we do not consider it in our experi-
ments.
4For example in PropBank, the lexical unit buy.V has
three verb frames and in sentential context, we want to disam-
biguate its frame. (Although PropBank never formally uses
the term lexical unit, we adopt its usage from the frame se-
mantics literature.)
</footnote>
<figure confidence="0.994465461538462">
COMMERCE_BUY buy.01
buy.V buy.V
COMMERCE_BUY sell.01
sell.V sell.V
(a) (b)
John bought a car .
Buyer Goods
John bought a car .
A0 A1
Mary sold a car .
Seller Goods
Mary sold a car .
A0 A1
</figure>
<page confidence="0.982623">
1449
</page>
<subsectionHeader confidence="0.99252">
2.2 Distributed Frame Identification
</subsectionHeader>
<bodyText confidence="0.999996617647059">
We present a model that takes word embeddings
as input and learns to identify semantic frames.
A word embedding is a distributed representa-
tion of meaning where each word is represented
as a vector in Rn. Such representations allow a
model to share meaning between similar words,
and have been used to capture semantic, syntac-
tic and morphological content (Collobert and We-
ston, 2008; Turian et al., 2010, inter alia). We use
word embeddings to represent the syntactic con-
text of a particular predicate instance as a vector.
For example, consider the sentence “He runs the
company.” The predicate runs has two syntac-
tic dependents – a subject and direct object (but
no prepositional phrases or clausal complements).
We could represent the syntactic context of runs as
a vector with blocks for all the possible dependents
warranted by a syntactic parser; for example, we
could assume that positions 0 ... n in the vector
correspond to the subject dependent, n+1... 2n
correspond to the clausal complement dependent,
and so forth. Thus, the context is a vector in Rnk
with the embedding of He at the subject position,
the embedding of company in direct object posi-
tion and zeros everywhere else. Given input vec-
tors of this form for our training data, we learn a
matrix that maps this high dimensional and sparse
representation into a lower dimensional space. Si-
multaneously, the model learns an embedding for
all the possible labels (i.e. the frames in a given
lexicon). At inference time, the predicate-context
is mapped to the low dimensional space, and we
choose the nearest frame label as our classifica-
tion. We next describe this model in detail.
</bodyText>
<sectionHeader confidence="0.985159" genericHeader="method">
3 Frame Identification with Embeddings
</sectionHeader>
<bodyText confidence="0.999992413793104">
We continue using the example sentence from
§2.2: “He runs the company.” where we want to
disambiguate the frame of runs in context. First,
we extract the words in the syntactic context of
runs; next, we concatenate their word embeddings
as described in §2.2 to create an initial vector space
representation. Subsequently, we learn a map-
ping from this initial representation into a low-
dimensional space; we also learn an embedding
for each possible frame label in the same low-
dimensional space. The goal of learning is to
make sure that the correct frame label is as close as
possible to the mapped context, while competing
frame labels are farther away.
Formally, let x represent the actual sentence
with a marked predicate, along with the associated
syntactic parse tree; let our initial representation
of the predicate context be g(x). Suppose that the
word embeddings we start with are of dimension
n. Then g is a function from a parsed sentence
x to Rnk, where k is the number of possible syn-
tactic context types. For example g selects some
important positions relative to the predicate, and
reserves a block in its output space for the embed-
ding of words found at that position. Suppose g
considers clausal complements and direct objects.
Then g : X —* R2n and for the example sentence
it has zeros in positions 0 ... n and the embedding
of the word company in positions n+1... 2n.
</bodyText>
<equation confidence="0.996177">
g(x) = [0, ... , 0, embedding of company].
</equation>
<bodyText confidence="0.999857866666667">
Section 3.1 describes the context positions we use
in our experiments. Let the low dimensional space
we map to be R&apos; and the learned mapping be M :
Rnk —* R&apos;. The mapping M is a linear trans-
formation, and we learn it using the WSABIE algo-
rithm (Weston et al., 2011). WSABIE also learns an
embedding for each frame label (y, henceforth).
In our setting, this means that each frame corre-
sponds to a point in R&apos;. If we have F possi-
ble frames we can store those parameters in an
F x m matrix, one m-dimensional point for each
frame, which we will refer to as the linear map-
ping Y . Let the lexical unit (the lemma conjoined
with a coarse POS tag) for the marked predicate
be E. We denote the frames that associate with
E in the frame lexicon5 and our training corpus
as Ft. WSABIE performs gradient-based updates
on an objective that tries to minimize the distance
between M(g(x)) and the embedding of the cor-
rect label Y (y), while maintaining a large distance
between M(g(x)) and the other possible labels
Y (¯y) in the confusion set Ft. At disambiguation
time, we use a simple dot product similarity as our
distance metric, meaning that the model chooses
a label by computing the argmaxys(x, y) where
s(x, y) = M(g(x)) · Y (y), where the argmax iter-
ates over the possible frames y E Ft if E was seen
in the lexicon or the training data, or y E F, if it
was unseen.6 Model learning is performed using
the margin ranking loss function as described in
</bodyText>
<footnote confidence="0.996112">
5The frame lexicon stores the frames, corresponding se-
mantic roles and the lexical units associated with the frame.
6This disambiguation scheme is similar to the one adopted
by Das et al. (2014), but they use unlemmatized words to
define their confusion set.
</footnote>
<page confidence="0.984317">
1450
</page>
<figureCaption confidence="0.910643">
Figure 2: Context representation extraction for the
</figureCaption>
<bodyText confidence="0.969082428571429">
embedding model. Given a dependency parse (1)
the model extracts all words matching a set of paths
from the frame evoking predicate and its direct de-
pendents (2). The model computes a composed rep-
resentation of the predicate instance by using dis-
tributed vector representations for words (3) – the
(red) vertical embedding vectors for each word are
concatenated into a long vector. Finally, we learn a
linear transformation function parametrized by the
context blocks (4).
Weston et al. (2011), and in more detail in section
3.2.
Since WSABIE learns a single mapping from g(x)
to Rm, parameters are shared between different
words and different frames. So for example “He
runs the company” could help the model disam-
biguate “He owns the company.” Moreover, since
g(x) relies on word embeddings rather than word
identities, information is shared between words.
For example “He runs the company” could help
us to learn about “She runs a corporation”.
</bodyText>
<subsectionHeader confidence="0.999206">
3.1 Context Representation Extraction
</subsectionHeader>
<bodyText confidence="0.999947894736842">
In principle g(x) could be any feature function, but
we performed an initial investigation of two partic-
ular variants. In both variants, our representation
is a block vector where each block corresponds to
a syntactic position relative to the predicate, and
each block’s values correspond to the embedding
of the word at that position.
Direct Dependents The first context function we
considered corresponds to the examples in §3. To
elaborate, the positions of interest are the labels of
the direct dependents of the predicate, so k is the
number of labels that the dependency parser can
produce. For example, if the label on the edge be-
tween runs and He is nsubj, we would put the em-
bedding of He in the block corresponding to nsubj.
If a label occurs multiple times, then the embed-
dings of the words below this label are averaged.
Unfortunately, using only the direct dependents
can miss a lot of useful information. For exam-
ple, topicalization can place discriminating infor-
mation farther from the predicate. Consider “He
runs the company.” vs. “It was the company that
he runs.” In the second sentence, the discrim-
inating word, company dominates the predicate
runs. Similarly, predicates in embedded clauses
may have a distant agent which cannot be captured
using direct dependents. Consider “The athlete
ran the marathon.” vs. “The athlete prepared him-
self for three months to run the marathon.” In the
second example, for the predicate run, the agent
The athlete is not a direct dependent, but is con-
nected via a longer dependency path.
Dependency Paths To capture more relevant
context, we developed a second context function
as follows. We scanned the training data for a
given task (either the PropBank or the FrameNet
domains) for the dependency paths that connected
the gold predicates to the gold semantic argu-
ments. This set of dependency paths were deemed
as possible positions in the initial vector space rep-
resentation. In addition, akin to the first context
function, we also added all dependency labels to
the context set. Thus for this context function, the
block cardinality k was the sum of the number of
scanned gold dependency path types and the num-
ber of dependency labels. Given a predicate in its
sentential context, we therefore extract only those
context words that appear in positions warranted
by the above set. See Figure 2 for an illustration
of this process.
We performed initial experiments using con-
text extracted from 1) direct dependents, 2) de-
pendency paths, and 3) both. For all our experi-
ments, setting 3) which concatenates the direct de-
pendents and dependency path always dominated
the other two, so we only report results for this
setting.
</bodyText>
<subsectionHeader confidence="0.999636">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.9999659">
We model our objective function following We-
ston et al. (2011), using a weighted approximate-
rank pairwise loss, learned with stochastic gradi-
ent descent. The mapping from g(x) to the low
dimensional space Rm is a linear transformation,
so the model parameters to be learnt are the matrix
M E Rnk×m as well as the embedding of each
possible frame label, represented as another ma-
trix Y E RF ×m where there are F frames in total.
The training objective function minimizes:
</bodyText>
<equation confidence="0.993815">
E EL(ranky(x)) max(0,γ+s(x, y)−s(x, ¯y)).
x y¯
</equation>
<page confidence="0.843288">
1451
</page>
<bodyText confidence="0.9991968">
where x, y are the training inputs and their cor-
responding correct frames, and y¯ are negative
frames, γ is the margin. Here, ranky(x) is the
rank of the positive frame y relative to all the neg-
ative frames:
</bodyText>
<equation confidence="0.9381105">
�ranky(x) = I(s(x, y) G γ + s(x, ¯y)),
y¯
</equation>
<bodyText confidence="0.994334206896552">
and L(η) converts the rank to a weight. Choos-
ing L(η) = Cη for any positive constant C opti-
mizes the mean rank, whereas a weighting such as
L(η) = �ηi=1 1/i (adopted here) optimizes the
top of the ranked list, as described in (Usunier
et al., 2009). To train with such an objective,
stochastic gradient is employed. For speed the
computation of ranky(x) is then replaced with a
sampled approximation: sample N items y¯ until
a violation is found, i.e. max(0, γ + s(x, ¯y) −
s(x, y))) &gt; 0 and then approximate the rank with
(F − 1)/N, see Weston et al. (2011) for more
details on this procedure. For the choices of the
stochastic gradient learning rate, margin (γ) and
dimensionality (m), please refer to §5.4-§5.5.
Note that an alternative approach could learn
only the matrix M, and then use a k-nearest neigh-
bor classifier in Rm, as in Weinberger and Saul
(2009). The advantage of learning an embedding
for the frame labels is that at inference time we
need to consider only the set of labels for classi-
fication rather than all training examples. Addi-
tionally, since we use a frame lexicon that gives
us the possible frames for a given predicate, we
usually only consider a handful of candidate la-
bels. If we used all training examples for a given
predicate for finding a nearest-neighbor match at
inference time, we would have to consider many
more candidates, making the process very slow.
</bodyText>
<sectionHeader confidence="0.995227" genericHeader="method">
4 Argument Identification
</sectionHeader>
<bodyText confidence="0.999991090909091">
Here, we briefly describe the argument identifi-
cation model used in our frame-semantic parsing
experiments, post frame identification. Given x,
the sentence with a marked predicate, the argu-
ment identification model assumes that the pred-
icate frame y has been disambiguated. From a
frame lexicon, we look up the set of semantic roles
Ry that associate with y. This set also contains the
null role ro. From x, a rule-based candidate argu-
ment extraction algorithm extracts a set of spans
A that could potentially serve as the overt7 argu-
</bodyText>
<footnote confidence="0.700092">
7By overtness, we mean the non-null instantiation of a
semantic role in a frame-semantic parse.
</footnote>
<listItem confidence="0.998309523809524">
• starting word of a • POS of the starting word of a
• ending word of a • POS of the ending word of a
• head word of a • POS of the head word of a
• bag of words in a • bag of POS tags in a
• a bias feature • voice of the predicate use
• word cluster of a’s head
• word cluster of a’s head conjoined with word cluster
of the predicate*
• dependency path between a’s head and the predicate
• the set of dependency labels of the predicate’s children
• dependency path conjoined with the POS tag of a’s
head
• dependency path conjoined with the word cluster of
a’s head
• position of a with respect to the predicate (before, after,
overlap or identical)
• whether the subject of the predicate is missing (miss-
ingsubj)
• missingsubj, conjoined with the dependency path
• missingsubj, conjoined with the dependency path from
the verb dominating the predicate to a’s head
</listItem>
<tableCaption confidence="0.9666076">
Table 1: Argument identification features. The span in con-
sideration is termed a. Every feature in this list has two ver-
sions, one conjoined with the given role r and the other con-
joined with both r and the frame y. The feature with a * su-
perscript is only conjoined with the role to reduce its sparsity.
</tableCaption>
<bodyText confidence="0.68867025">
ments Ay for y (see §5.4-§5.5 for the details of the
candidate argument extraction algorithms).
Learning Given training data of the form
(Wi), y(i), M(i)))Ni=1, where,
</bodyText>
<equation confidence="0.992436">
M = {(r,a} : r ∈ Ry, a ∈ A ∪ Ay}, (1)
</equation>
<bodyText confidence="0.9999494">
a set of tuples that associates each role r in Ry
with a span a according to the gold data. Note that
this mapping associates spans with the null role ro
as well. We optimize the following log-likelihood
to train our model:
</bodyText>
<equation confidence="0.6580305">
� − C�θ�2
log pθ �(r, a)j|x, y, Ry 2
</equation>
<bodyText confidence="0.999509461538462">
where pθ is a log-linear model normalized over the
set Ry, with features described in Table 1. We
set C = 1.0 and use L-BFGS (Liu and Nocedal,
1989) for training.
Inference Although our learning mechanism
uses a local log-linear model, we perform infer-
ence globally on a per-frame basis by applying
hard structural constraints. Following Das et al.
(2014) and Punyakanok et al. (2008) we use the
log-probability of the local classifiers as a score in
an integer linear program (ILP) to assign roles sub-
ject to hard constraints described in §5.4 and §5.5.
We use an off-the-shelf ILP solver for inference.
</bodyText>
<table confidence="0.462559">
max N |M(i)|
θ i=1 �
j=1
</table>
<page confidence="0.964911">
1452
</page>
<sectionHeader confidence="0.998795" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999978857142857">
In this section, we present our experiments and
the results achieved. We evaluate our novel frame
identification approach in isolation and also con-
joined with argument identification resulting in
full frame-semantic structures; before presenting
our model’s performance we first focus on the
datasets, baselines and the experimental setup.
</bodyText>
<subsectionHeader confidence="0.952024">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999933766666667">
We evaluate our models on both FrameNet- and
PropBank-style structures. For FrameNet, we use
the full-text annotations in the FrameNet 1.5 re-
lease8 which was used by Das et al. (2014, §3.2).
We used the same test set as Das et al. contain-
ing 23 documents with 4,458 predicates. Of the
remaining 55 documents, 16 documents were ran-
domly chosen for development.9
For experiments with PropBank, we used the
Ontonotes corpus (Hovy et al., 2006), version 4.0,
and only made use of the Wall Street Journal doc-
uments; we used sections 2-21 for training, sec-
tion 24 for development and section 23 for testing.
This resembles the setup used by Punyakanok et
al. (2008). All the verb frame files in Ontonotes
were used for creating our frame lexicon.
At test time, this model chooses the best frame as
argmaxyψ · f(y, x, `) where argmax iterates over
the possible frames y E F` if ` was seen in the
lexicon or the training data, or y E F, if it was un-
seen, like the disambiguation scheme of §3. We
train this model by maximizing L2 regularized
log-likelihood, using L-BFGS; the regularization
constant was set to 0.1 in all experiments.
For comparison with our model from §3, which
we call WSABIE EMBEDDING, we implemented two
baselines with the log-linear model. Both the
baselines use features very similar to the input rep-
resentations described in §3.1. The first one com-
putes the direct dependents and dependency paths
</bodyText>
<footnote confidence="0.9955075">
8Seehttps://framenet.icsi.berkeley.edu.
9These documents are listed in appendix A.
</footnote>
<bodyText confidence="0.999669928571429">
as described in §3.1 but conjoins them with the
word identity rather than a word embedding. Ad-
ditionally, this model uses the un-conjoined words
as backoff features. This would be a standard NLP
approach for the frame identification problem, but
is surprisingly competitive with the state of the art.
We call this baseline LOG-LINEAR WORDS. The sec-
ond baseline, tries to decouple the WSABIE training
from the embedding input, and trains a log linear
model using the embeddings. So the second base-
line has the same input representation as WSABIE
EMBEDDING but uses a log-linear model instead of
WSABIE. We call this model LOG-LINEAR EMBED-
DING.
</bodyText>
<subsectionHeader confidence="0.996516">
5.3 Common Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999994935483871">
We process our PropBank and FrameNet training,
development and test corpora with a shift-reduce
dependency parser that uses the Stanford conven-
tions (de Marneffe and Manning, 2013) and uses
an arc-eager transition system with beam size of 8;
the parser and its features are described by Zhang
and Nivre (2011). Before parsing the data, it is
tagged with a POS tagger trained with a condi-
tional random field (Lafferty et al., 2001) with the
following emission features: word, the word clus-
ter, word suffixes of length 1, 2 and 3, capitaliza-
tion, whether it has a hyphen, digit and punctua-
tion. Beyond the bias transition feature, we have
two cluster features for the left and right words in
the transition. We use Brown clusters learned us-
ing the algorithm of Uszkoreit and Brants (2008)
on a large English newswire corpus for cluster fea-
tures. We use the same word clusters for the argu-
ment identification features in Table 1.
We learn the initial embedding representations
for our frame identification model (§3) using a
deep neural language model similar to the one pro-
posed by Bengio et al. (2003). We use 3 hidden
layers each with 1024 neurons and learn a 128-
dimensional embedding from a large corpus con-
taining over 100 billion tokens. In order to speed
up learning, we use an unnormalized output layer
and a hinge-loss objective. The objective tries to
ensure that the correct word scores higher than a
random incorrect word, and we train with mini-
batch stochastic gradient descent.
</bodyText>
<subsectionHeader confidence="0.95494">
5.4 Experimental Setup for FrameNet
</subsectionHeader>
<bodyText confidence="0.999755333333333">
Hyperparameters For our frame identification
model with embeddings, we search for the WSA-
BIE hyperparameters using the development data.
</bodyText>
<subsectionHeader confidence="0.985359">
5.2 Frame Identification Baselines
</subsectionHeader>
<bodyText confidence="0.99993425">
For comparison, we implemented a set of baseline
models, with varying feature configurations. The
baselines use a log-linear model that models the
following probability at training time:
</bodyText>
<equation confidence="0.978285333333333">
eψ·f(y,x,`)
p(y|x,`) = r. ψ· f( `) (2)
yEFe e
</equation>
<page confidence="0.961875">
1453
</page>
<table confidence="0.993903916666667">
SEMAFOR LEXICON FULL LEXICON
Development Data Model All Ambiguous Rare All Ambiguous Rare
LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07
LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70
WSABIE EMBEDDING (§3) 96.90 92.73 96.44 96.99 93.12 96.39
SEMAFOR LEXICON FULL LEXICON
Model All Ambiguous Rare Unseen All Ambiguous Rare
Das et al. (2014) supervised 82.97 69.27 80.97 23.08
Das et al. (2014) best 83.60 69.19 82.31 42.67
Test Data LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10
LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03
WSABIE EMBEDDING (§3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38
</table>
<tableCaption confidence="0.95577">
Table 2: Frame identification results for FrameNet. See §5.6.
</tableCaption>
<table confidence="0.9971935">
SEMAFOR LEXICON FULL LEXICON
Model Precision Recall Fl Precision Recall Fl
LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19
Development Data WSABIE EMBEDDING (§3) 89.89 76.40 82.59 89.94 76.27 82.54
Das et al. supervised 67.81 60.68 64.05
Das et al. best 68.33 61.14 64.54
Test Data LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08
WSABIE EMBEDDING (§3) 72.79 64.95 68.64 74.44 66.17 70.06
</table>
<tableCaption confidence="0.989145">
Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We
skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
</tableCaption>
<bodyText confidence="0.990376034482759">
We search for the stochastic gradient learning
rate in {0.0001, 0.001, 0.01}, the margin γ ∈
{0.001,0.01,0.1, 1} and the dimensionality of the
final vector space m ∈ {256, 512}, to maximize
the frame identification accuracy of ambiguous
lexical units; by ambiguous, we imply lexical units
that appear in the training data or the lexicon with
more than one semantic frame. The underlined
values are the chosen hyperparameters used to an-
alyze the test data.
Argument Candidates The candidate argument
extraction method used for the FrameNet data, (as
mentioned in §4) was adapted from the algorithm
of Xue and Palmer (2004) applied to dependency
trees. Since the original algorithm was designed
for verbs, we added a few extra rules to handle
non-verbal predicates: we added 1) the predicate
itself as a candidate argument, 2) the span ranging
from the sentence position to the right of the pred-
icate to the rightmost index of the subtree headed
by the predicate’s head; this helped capture cases
like “afew months” (where few is the predicate and
months is the argument), and 3) the span ranging
from the leftmost index of the subtree headed by
the predicate’s head to the position immediately
before the predicate, for cases like “your gift to
Goodwill” (where to is the predicate and your gift
is the argument).10
10Note that Das et al. (2014) describe the state of the art
in FrameNet-based analysis, but their argument identifica-
tion strategy considered all possible dependency subtrees in
Frame Lexicon In our experimental setup, we
scanned the XML files in the “frames” directory
of the FrameNet 1.5 release, which lists all the
frames, the corresponding roles and the associ-
ated lexical units, and created a frame lexicon to
be used in our frame and argument identification
models. We noted that this renders every lexical
unit as seen; in other words, at frame disambigua-
tion time on our test set, for all instances, we only
had to score the frames in Ft for a predicate with
lexical unit E (see §3 and §5.2). We call this setup
FULL LEXICON. While comparing with prior state
of the art on the same corpus, we noted that Das et
al. (2014) found several unseen predicates at test
time.11 For fair comparison, we took the lexical
units for the predicates that Das et al. considered
as seen, and constructed a lexicon with only those;
training instances, if any, for the unseen predicates
under Das et al.’s setup were thrown out as well.
We call this setup SEMAFOR LEXICON.12 We also
experimented on the set of unseen instances used
by Das et al.
ILP constraints For FrameNet, we used three
ILP constraints during argument identification
(§4). 1) each span could have only one role, 2)
each core role could be present only once, and 3)
all overt arguments had to be non-overlapping.
</bodyText>
<footnote confidence="0.963706">
a parse, resulting in a much larger search space.
11Instead of using the frame files, Das et al. built a frame
lexicon from FrameNet’s exemplars and the training corpus.
12We got Das et al.’s seen predicates from the authors.
</footnote>
<page confidence="0.953039">
1454
</page>
<table confidence="0.999490473684211">
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.21 90.54 93.33
LOG-LINEAR EMBEDDING 93.81 89.86 93.73
WSABIE EMBEDDING (§3) 94.79 91.52 92.55
Dev data T 1 Test data
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.74 92.07 91.32
LOG-LINEAR EMBEDDING 94.04 90.95 90.97
WSABIE EMBEDDING (§3) 94.56 91.82 90.62
Model P R Fl
LOG-LINEAR WORDS 77.29 71.50 74.28
WSABIE EMBEDDING (§3) 77.13 71.32 74.11
Dev data T 1 Test data
Model P R Fl
LOG-LINEAR WORDS 79.47 75.11 77.23
WSABIE EMBEDDING (§3) 79.36 75.04 77.14
Punyakanok et al. Collins 75.92 71.45 73.62
Punyakanok et al. Charniak 77.09 75.51 76.29
Punyakanok et al. Combined 80.53 76.94 78.69
</table>
<tableCaption confidence="0.745311333333333">
Table 4: Frame identification accuracy results for PropBank.
The model and the column names have the same semantics
as Table 2.
</tableCaption>
<table confidence="0.999924857142857">
Model P R Fl
LOG-LINEAR WORDS 80.02 75.58 77.74
WSABIE EMBEDDING (§3) 80.06 75.74 77.84
Dev data T 1 Test data
Model P R Fl
LOG-LINEAR WORDS 81.55 77.83 79.65
WSABIE EMBEDDING (§3) 81.32 77.97 79.61
</table>
<tableCaption confidence="0.839741333333333">
Table 5: Full frame-structure prediction results for Propbank.
This is a metric that takes into account frames and arguments
together. See §5.7 for more details.
</tableCaption>
<subsectionHeader confidence="0.92979">
5.5 Experimental Setup for PropBank
</subsectionHeader>
<bodyText confidence="0.9996155">
Hyperparameters As in §5.4, we made a hyper-
parameter sweep in the same space. The chosen
learning rate was 0.01, while the other values were
γ = 0.01 and m = 512. Ambiguous lexical units
were used for this selection process.
Argument Candidates For PropBank we use
the algorithm of Xue and Palmer (2004) applied
to dependency trees.
Frame Lexicon For the PropBank experiments
we scanned the frame files for propositions in
Ontonotes 4.0, and stored possible core roles for
each verb frame. The lexical units were simply
the verb associating with the verb frames. There
were no unseen verbs at test time.
ILP constraints We used the constraints of Pun-
yakanok et al. (2008).
</bodyText>
<subsectionHeader confidence="0.998134">
5.6 FrameNet Results
</subsectionHeader>
<bodyText confidence="0.9984396">
Table 2 presents accuracy results on frame iden-
tification.13 We present results on all predicates,
ambiguous predicates seen in the lexicon or the
training data, and rare ambiguous predicates that
appear ≤ 11 times in the training data. The WS-
ABIE EMBEDDING model from §3 performs signif-
icantly better than the LOG-LINEAR WORDS base-
line, while LOG-LINEAR EMBEDDING underperforms
in every metric. For the SEMAFOR LEXICON setup,
we also compare with the state of the art from Das
</bodyText>
<tableCaption confidence="0.837911333333333">
13We do not report partial frame accuracy that has been
reported by prior work.
Table 6: Argument only evaluation (semantic role labeling
metrics) using the CoNLL 2005 shared task evaluation script
(Carreras and M`arquez, 2005). Results from Punyakanok et
al. (2008) are taken from Table 11 of that paper.
</tableCaption>
<bodyText confidence="0.9998745">
et al. (2014), who used a semi-supervised learn-
ing method to improve upon a supervised latent-
variable log-linear model. For unseen predicates
from the Das et al. system, we perform better as
well. Finally, for the FULL LEXICON setting, the ab-
solute accuracy numbers are even better for our
best model. Table 3 presents results on the full
frame-semantic parsing task (measured by a reim-
plementation of the SemEval 2007 shared task
evaluation script) when our argument identifica-
tion model (§4) is used after frame identification.
We notice similar trends as in Table 2, and our re-
sults outperform the previously published best re-
sults, setting a new state of the art.
</bodyText>
<subsectionHeader confidence="0.997971">
5.7 PropBank Results
</subsectionHeader>
<bodyText confidence="0.999129409090909">
Table 4 shows frame identification results on the
PropBank data. On the development set, our best
model performs with the highest accuracy on all
and ambiguous predicates, but performs worse on
rare ambiguous predicates. On the test set, the
LOG-LINEAR WORDS baseline performs best by a
very narrow margin. See §6 for a discussion.
Table 5 presents results where we measure pre-
cision, recall and Fi for frames and arguments to-
gether; this strict metric penalizes arguments for
mismatched frames, like in Table 3. We see the
same trend as in Table 4. Finally, Table 6 presents
SRL results that measures argument performance
only, irrespective of the frame; we use the eval-
uation script from CoNLL 2005 (Carreras and
M`arquez, 2005). We note that with a better frame
identification model, our performance on SRL im-
proves in general. Here, too, the embedding model
barely misses the performance of the best baseline,
but we are at par and sometimes better than the sin-
gle parser setting of a state-of-the-art SRL system
(Punyakanok et al., 2008).14
</bodyText>
<footnote confidence="0.888297">
14The last row of Table 6 refers to a system which used the
</footnote>
<page confidence="0.993378">
1455
</page>
<sectionHeader confidence="0.999568" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998407213333333">
For FrameNet, the WSABIE EMBEDDING model we
propose strongly outperforms the baselines on all
metrics, and sets a new state of the art. We be-
lieve that the WSABIE EMBEDDING model performs
better than the LOG-LINEAR EMBEDDING baseline
(that uses the same input representation) because
the former setting allows examples with differ-
ent labels and confusion sets to share informa-
tion; this is due to the fact that all labels live in
the same label space, and a single projection ma-
trix is shared across the examples to map the input
features to this space. Consequently, the WSABIE
EMBEDDING model can share more information be-
tween different examples in the training data than
the LOG-LINEAR EMBEDDING model. Since the LOG-
LINEAR WORDS model always performs better than
the LOG-LINEAR EMBEDDING model, we conclude
that the primary benefit does not come from the
input embedding representation.15
On the PropBank data, we see that the LOG-
LINEAR WORDS baseline has roughly the same per-
formance as our model on most metrics: slightly
better on the test data and slightly worse on the
development data. This can be partially explained
with the significantly larger training set size for
PropBank, making features based on words more
useful. Another important distinction between
PropBank and FrameNet is that the latter shares
frames between multiple lexical units. The ef-
fect of this is clearly observable from the “Rare”
column in Table 4. WSABIE EMBEDDING performs
poorly in this setting while LOG-LINEAR EMBEDDING
performs well. Part of the explanation has to do
with the specifics of WSABIE training. Recall that
the WSABIE EMBEDDING model needs to estimate
the label location in R&apos; for each frame. In other
words, it must estimate 512 parameters based on
at most 10 training examples. However, since the
input representation is shared across all frames,
every other training example from all the lexical
units affects the optimal estimate, since they all
modify the joint parameter matrix M. By contrast,
in the log-linear models each label has its own
set of parameters, and they interact only via the
normalization constant. The LOG-LINEAR WORDS
model does not have this entanglement, but cannot
share information between words. For PropBank,
combination of two syntactic parsers as input.
15One could imagine training a WSABIE model with word
features, but we did not perform this experiment.
these drawbacks and benefits balance out and we
see similar performance for LOG-LINEAR WORDS
and LOG-LINEAR EMBEDDING. For FrameNet, esti-
mating the label embedding is not as much of a
problem because even if a lexical unit is rare, the
potential frames can be frequent. For example, we
might have seen the SENDING frame many times,
even though telex.V is a rare lexical unit.
In comparison to prior work on FrameNet, even
our baseline models outperform the previous state
of the art. A particularly interesting comparison is
between our LOG-LINEAR WORDS baseline and the
supervised model of Das et al. (2014). They also
use a log-linear model, but they incorporate a la-
tent variable that uses WordNet (Fellbaum, 1998)
to get lexical-semantic relationships and smooths
over frames for ambiguous lexical units. It is
possible that this reduces the model’s power and
causes it to over-generalize. Another difference is
that when training the log-linear model, they nor-
malize over all frames, while we normalize over
the allowed frames for the current lexical unit.
This would tend to encourage their model to ex-
pend more of its modeling power to rule out pos-
sibilities that will be pruned out at test time.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969055555556">
We have presented a simple model that outper-
forms the prior state of the art on FrameNet-
style frame-semantic parsing, and performs at par
with one of the previous-best single-parser sys-
tems on PropBank SRL. Unlike Das et al. (2014),
our model does not rely on heuristics to con-
struct a similarity graph and leverage WordNet;
hence, in principle it is generalizable to varying
domains, and to other languages. Finally, we pre-
sented results on PropBank-style semantic role la-
beling with a system that included the task of au-
tomatic verb frame identification, in tune with the
FrameNet literature; we believe that such a sys-
tem produces more interpretable output, both from
the perspective of human understanding as well as
downstream applications, than pipelines that are
oblivious to the verb frame, only focusing on ar-
gument analysis.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999662">
We thank Emily Pitler for comments on an early
draft, and the anonymous reviewers for their valu-
able feedback.
</bodyText>
<page confidence="0.976654">
1456
</page>
<bodyText confidence="0.88045325">
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
</bodyText>
<sectionHeader confidence="0.893401" genericHeader="method">
References
</sectionHeader>
<bodyText confidence="0.784963">
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL.
</bodyText>
<reference confidence="0.999947734042553">
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9–56.
M.-C. de Marneffe and C. D. Manning, 2013. Stanford
typed dependencies manual.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database.
C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3):235–250.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111–137. Hanshin
Publishing Co., Seoul, South Korea.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90 In Pro-
ceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503 – 528.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145–159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
HLT.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257–287.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL,
Stroudsburg, PA, USA.
N. Usunier, D. Buffoni, and P. Gallinari. 2009. Rank-
ing with ordered weighted pairwise classification. In
ICML.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. In Proceedings of
ACL-HLT.
K. Q. Weinberger and L. K. Saul. 2009. Distance met-
ric learning for large margin nearest neighbor clas-
sification. Journal of Machine Learning Research,
10:207–244.
J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie:
Scaling up to large vocabulary image annotation. In
Proceedings of IJCAI.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP
2004.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT.
</reference>
<page confidence="0.958533">
1457
</page>
<figure confidence="0.311378176470588">
Number Filename
dev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xml
dev-2 NTI SouthAfrica Introduction.xml
dev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml
dev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xml
dev-5 PropBank TicketSplitting.xml
dev-6 Miscellaneous Hijack.xml
dev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xml
dev-8 NTI WMDNews 042106.xml
dev-9 C-4 C-4Text.xml
dev-10 ANC EntrepreneurAsMadonna.xml
dev-11 NTI LibyaCountry1.xml
dev-12 NTI NorthKorea NuclearOverview.xml
dev-13 LUCorpus-v0.3 20000424 nyt-NEW.xml
dev-14 NTI WMDNews 062606.xml
dev-15 ANC 110CYL070.xml
dev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml
</figure>
<tableCaption confidence="0.9569">
Table 7: List of files used as development set for the FrameNet 1.5 corpus.
</tableCaption>
<sectionHeader confidence="0.865598" genericHeader="method">
A Development Data
</sectionHeader>
<bodyText confidence="0.9916765">
Table 7 features a list of the 16 randomly selected
documents from the FrameNet 1.5 corpus, which
we used for development. The resultant develop-
ment set consists of roughly 4,500 predicates. We
use the same test set as in Das et al. (2014), con-
taining 23 documents and 4,458 predicates.
</bodyText>
<page confidence="0.992847">
1458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894211">
<title confidence="0.999838">Semantic Frame Identification with Distributed Word Representations</title>
<author confidence="0.999371">Moritz Dipanjan</author>
<note confidence="0.9220145">of Computer Science, University of Oxford, Oxford OX1 3QD, United Inc., 76 9th Avenue, New York, NY 10011, United</note>
<email confidence="0.989841">jaseweston@gmail.com</email>
<abstract confidence="0.999613368421053">We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>M Ellsworth</author>
<author>K Erk</author>
</authors>
<title>SemEval2007 Task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="3674" citStr="Baker et al., 2007" startWordPosition="551" endWordPosition="554"> and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). 1448 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448–1458, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation s</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval2007 Task 19: Frame semantic structure extraction. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="25761" citStr="Bengio et al. (2003)" startWordPosition="4318" endWordPosition="4321">ord cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification features in Table 1. We learn the initial embedding representations for our frame identification model (§3) using a deep neural language model similar to the one proposed by Bengio et al. (2003). We use 3 hidden layers each with 1024 neurons and learn a 128- dimensional embedding from a large corpus containing over 100 billion tokens. In order to speed up learning, we use an unnormalized output layer and a hinge-loss objective. The objective tries to ensure that the correct word scores higher than a random incorrect word, and we train with minibatch stochastic gradient descent. 5.4 Experimental Setup for FrameNet Hyperparameters For our frame identification model with embeddings, we search for the WSABIE hyperparameters using the development data. 5.2 Frame Identification Baselines F</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>X. Carreras and L. M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-2005 shared task: semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="8848" citStr="Collobert and Weston, 2008" startWordPosition="1392" endWordPosition="1396">literature.) COMMERCE_BUY buy.01 buy.V buy.V COMMERCE_BUY sell.01 sell.V sell.V (a) (b) John bought a car . Buyer Goods John bought a car . A0 A1 Mary sold a car . Seller Goods Mary sold a car . A0 A1 1449 2.2 Distributed Frame Identification We present a model that takes word embeddings as input and learns to identify semantic frames. A word embedding is a distributed representation of meaning where each word is represented as a vector in Rn. Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We use word embeddings to represent the syntactic context of a particular predicate instance as a vector. For example, consider the sentence “He runs the company.” The predicate runs has two syntactic dependents – a subject and direct object (but no prepositional phrases or clausal complements). We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 ... n in the vector correspond to the subject dependent, n+1... 2n correspond to th</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N Schneider</author>
<author>D Chen</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2250" citStr="Das et al., 2010" startWordPosition="323" endWordPosition="326"> According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report</context>
<context position="4701" citStr="Das et al. (2010)" startWordPosition="699" endWordPosition="702">005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with argument identification. 2.1 Frame-Semantic Parsing Closely related to SRL, frame-semantic parsing consists of the resolution of predicate sense into a frame, and the analysis of the frame’s arguments. Work in this area exclusively uses the FrameNet full text annotations. Johansson and Nugues (2007) presented the best performing system at SemEval 2007 (Baker et al., 2007), and Das et al. (2010) improved performance, and later set the current state of the art on this task (Das et al., 2014). We briefly discuss FrameNet, and subsequently PropBank annotation conventions here. FrameNet The FrameNet project (Baker et al., 1998) is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke. For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core.2 Sentences are </context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010. Probabilistic frame-semantic parsing. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>D Chen</author>
<author>A F T Martins</author>
<author>N Schneider</author>
<author>N A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="2269" citStr="Das et al., 2014" startWordPosition="327" endWordPosition="330">theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results o</context>
<context position="4798" citStr="Das et al., 2014" startWordPosition="717" endWordPosition="720">owever, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with argument identification. 2.1 Frame-Semantic Parsing Closely related to SRL, frame-semantic parsing consists of the resolution of predicate sense into a frame, and the analysis of the frame’s arguments. Work in this area exclusively uses the FrameNet full text annotations. Johansson and Nugues (2007) presented the best performing system at SemEval 2007 (Baker et al., 2007), and Das et al. (2010) improved performance, and later set the current state of the art on this task (Das et al., 2014). We briefly discuss FrameNet, and subsequently PropBank annotation conventions here. FrameNet The FrameNet project (Baker et al., 1998) is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke. For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core.2 Sentences are annotated using this universal frame inventory. For example, consider the pair of sentences in Fi</context>
<context position="7416" citStr="Das et al. (2014)" startWordPosition="1146" endWordPosition="1149"> highlights this difference: while both sell.V and buy.V are members of the same frame in FrameNet, they evoke different frames in PropBank. In spite of this difference, nearly identical statistical models could be employed for both frameworks. Modeling In this paper, we model the framesemantic parsing problem in two stages: frame identification and argument identification. As mentioned in §1, these correspond to a frame disambiguation stage,4 and a stage that finds the various arguments that fulfill the frame’s semantic roles within the sentence, respectively. This resembles the framework of Das et al. (2014), who solely focus on FrameNet corpora, unlike this paper. The novelty of this paper lies in the frame identification stage (§3). Note that this two-stage approach is unusual for the PropBank corpora when compared to prior work, where the vast majority of published papers have not focused on the verb frame disambiguation problem at all, only focusing on the role labeling stage (see the overview paper of M`arquez et al. (2008) for example). 3NomBank (Meyers et al., 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments. 4For example in PropBank, the lex</context>
<context position="13228" citStr="Das et al. (2014)" startWordPosition="2165" endWordPosition="2168">on set Ft. At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmaxys(x, y) where s(x, y) = M(g(x)) · Y (y), where the argmax iterates over the possible frames y E Ft if E was seen in the lexicon or the training data, or y E F, if it was unseen.6 Model learning is performed using the margin ranking loss function as described in 5The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame. 6This disambiguation scheme is similar to the one adopted by Das et al. (2014), but they use unlemmatized words to define their confusion set. 1450 Figure 2: Context representation extraction for the embedding model. Given a dependency parse (1) the model extracts all words matching a set of paths from the frame evoking predicate and its direct dependents (2). The model computes a composed representation of the predicate instance by using distributed vector representations for words (3) – the (red) vertical embedding vectors for each word are concatenated into a long vector. Finally, we learn a linear transformation function parametrized by the context blocks (4). Westo</context>
<context position="21849" citStr="Das et al. (2014)" startWordPosition="3663" endWordPosition="3666"> a set of tuples that associates each role r in Ry with a span a according to the gold data. Note that this mapping associates spans with the null role ro as well. We optimize the following log-likelihood to train our model: � − C�θ�2 log pθ �(r, a)j|x, y, Ry 2 where pθ is a log-linear model normalized over the set Ry, with features described in Table 1. We set C = 1.0 and use L-BFGS (Liu and Nocedal, 1989) for training. Inference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints. Following Das et al. (2014) and Punyakanok et al. (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in §5.4 and §5.5. We use an off-the-shelf ILP solver for inference. max N |M(i)| θ i=1 � j=1 1452 5 Experiments In this section, we present our experiments and the results achieved. We evaluate our novel frame identification approach in isolation and also conjoined with argument identification resulting in full frame-semantic structures; before presenting our model’s performance we first focus on the datasets, base</context>
<context position="26953" citStr="Das et al. (2014)" startWordPosition="4504" endWordPosition="4507">dentification Baselines For comparison, we implemented a set of baseline models, with varying feature configurations. The baselines use a log-linear model that models the following probability at training time: eψ·f(y,x,`) p(y|x,`) = r. ψ· f( `) (2) yEFe e 1453 SEMAFOR LEXICON FULL LEXICON Development Data Model All Ambiguous Rare All Ambiguous Rare LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07 LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70 WSABIE EMBEDDING (§3) 96.90 92.73 96.44 96.99 93.12 96.39 SEMAFOR LEXICON FULL LEXICON Model All Ambiguous Rare Unseen All Ambiguous Rare Das et al. (2014) supervised 82.97 69.27 80.97 23.08 Das et al. (2014) best 83.60 69.19 82.31 42.67 Test Data LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10 LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03 WSABIE EMBEDDING (§3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38 Table 2: Frame identification results for FrameNet. See §5.6. SEMAFOR LEXICON FULL LEXICON Model Precision Recall Fl Precision Recall Fl LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19 Development Data WSABIE EMBEDDING (§3) 89.89 76.40 82.59 89.94 76.27 82.54 Das et al. supervised 67.81 60.68 64.05 Das et al. best </context>
<context position="29251" citStr="Das et al. (2014)" startWordPosition="4875" endWordPosition="4878">les to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate’s head; this helped capture cases like “afew months” (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate’s head to the position immediately before the predicate, for cases like “your gift to Goodwill” (where to is the predicate and your gift is the argument).10 10Note that Das et al. (2014) describe the state of the art in FrameNet-based analysis, but their argument identification strategy considered all possible dependency subtrees in Frame Lexicon In our experimental setup, we scanned the XML files in the “frames” directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models. We noted that this renders every lexical unit as seen; in other words, at frame disambiguation time on our test set, for all instances, we only had to score</context>
<context position="38379" citStr="Das et al. (2014)" startWordPosition="6392" endWordPosition="6395">eriment. these drawbacks and benefits balance out and we see similar performance for LOG-LINEAR WORDS and LOG-LINEAR EMBEDDING. For FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent. For example, we might have seen the SENDING frame many times, even though telex.V is a rare lexical unit. In comparison to prior work on FrameNet, even our baseline models outperform the previous state of the art. A particularly interesting comparison is between our LOG-LINEAR WORDS baseline and the supervised model of Das et al. (2014). They also use a log-linear model, but they incorporate a latent variable that uses WordNet (Fellbaum, 1998) to get lexical-semantic relationships and smooths over frames for ambiguous lexical units. It is possible that this reduces the model’s power and causes it to over-generalize. Another difference is that when training the log-linear model, they normalize over all frames, while we normalize over the allowed frames for the current lexical unit. This would tend to encourage their model to expend more of its modeling power to rule out possibilities that will be pruned out at test time. 7 Co</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>D. Das, D. Chen, A. F. T. Martins, N. Schneider, and N. A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>C D Manning</author>
</authors>
<date>2013</date>
<note>Stanford typed dependencies manual.</note>
<marker>de Marneffe, Manning, 2013</marker>
<rawString>M.-C. de Marneffe and C. D. Manning, 2013. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: an electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="2637" citStr="Fillmore et al., 2003" startWordPosition="386" endWordPosition="389">, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work </context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3):235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Frame Semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm,</booktitle>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.,</publisher>
<location>Seoul, South</location>
<contexts>
<context position="1694" citStr="Fillmore, 1982" startWordPosition="237" endWordPosition="238">e labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the </context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>C. J. Fillmore. 1982. Frame Semantics. In Linguistics in the Morning Calm, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3219" citStr="Gildea and Jurafsky (2002)" startWordPosition="481" endWordPosition="484"> (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). 1448 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448–1458, Bal</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: The 90 In</title>
<date>2006</date>
<booktitle>Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="22932" citStr="Hovy et al., 2006" startWordPosition="3841" endWordPosition="3844">ntification resulting in full frame-semantic structures; before presenting our model’s performance we first focus on the datasets, baselines and the experimental setup. 5.1 Data We evaluate our models on both FrameNet- and PropBank-style structures. For FrameNet, we use the full-text annotations in the FrameNet 1.5 release8 which was used by Das et al. (2014, §3.2). We used the same test set as Das et al. containing 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 documents were randomly chosen for development.9 For experiments with PropBank, we used the Ontonotes corpus (Hovy et al., 2006), version 4.0, and only made use of the Wall Street Journal documents; we used sections 2-21 for training, section 24 for development and section 23 for testing. This resembles the setup used by Punyakanok et al. (2008). All the verb frame files in Ontonotes were used for creating our frame lexicon. At test time, this model chooses the best frame as argmaxyψ · f(y, x, `) where argmax iterates over the possible frames y E F` if ` was seen in the lexicon or the training data, or y E F, if it was unseen, like the disambiguation scheme of §3. We train this model by maximizing L2 regularized log-li</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: The 90 In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>LTH: semantic structure extraction using nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="3703" citStr="Johansson and Nugues, 2007" startWordPosition="555" endWordPosition="558"> prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). 1448 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448–1458, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. LTH: semantic structure extraction using nonprojective dependency trees. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>I Titov</author>
<author>B Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1471" citStr="Klementiev et al., 2012" startWordPosition="203" endWordPosition="206">dentification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a fr</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>A. Klementiev, I. Titov, and B. Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>45</volume>
<issue>3</issue>
<pages>528</pages>
<contexts>
<context position="21642" citStr="Liu and Nocedal, 1989" startWordPosition="3632" endWordPosition="3635">rsity. ments Ay for y (see §5.4-§5.5 for the details of the candidate argument extraction algorithms). Learning Given training data of the form (Wi), y(i), M(i)))Ni=1, where, M = {(r,a} : r ∈ Ry, a ∈ A ∪ Ay}, (1) a set of tuples that associates each role r in Ry with a span a according to the gold data. Note that this mapping associates spans with the null role ro as well. We optimize the following log-likelihood to train our model: � − C�θ�2 log pθ �(r, a)j|x, y, Ry 2 where pθ is a log-linear model normalized over the set Ry, with features described in Table 1. We set C = 1.0 and use L-BFGS (Liu and Nocedal, 1989) for training. Inference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints. Following Das et al. (2014) and Punyakanok et al. (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in §5.4 and §5.5. We use an off-the-shelf ILP solver for inference. max N |M(i)| θ i=1 � j=1 1452 5 Experiments In this section, we present our experiments and the results achieved. We evaluate our novel frame i</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503 – 528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>K C Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic role labeling: an introduction to the special issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. M`arquez, X. Carreras, K. C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Computational Linguistics, 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL/HLT Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="2999" citStr="Meyers et al., 2004" startWordPosition="444" endWordPosition="447">3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments,</context>
<context position="7890" citStr="Meyers et al., 2004" startWordPosition="1227" endWordPosition="1230">the various arguments that fulfill the frame’s semantic roles within the sentence, respectively. This resembles the framework of Das et al. (2014), who solely focus on FrameNet corpora, unlike this paper. The novelty of this paper lies in the frame identification stage (§3). Note that this two-stage approach is unusual for the PropBank corpora when compared to prior work, where the vast majority of published papers have not focused on the verb frame disambiguation problem at all, only focusing on the role labeling stage (see the overview paper of M`arquez et al. (2008) for example). 3NomBank (Meyers et al., 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments. 4For example in PropBank, the lexical unit buy.V has three verb frames and in sentential context, we want to disambiguate its frame. (Although PropBank never formally uses the term lexical unit, we adopt its usage from the frame semantics literature.) COMMERCE_BUY buy.01 buy.V buy.V COMMERCE_BUY sell.01 sell.V sell.V (a) (b) John bought a car . Buyer Goods John bought a car . A0 A1 Mary sold a car . Seller Goods Mary sold a car . A0 A1 1449 2.2 Distributed Frame Identification We present a model that t</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: An interim report. In Proceedings of NAACL/HLT Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACLHLT.</booktitle>
<contexts>
<context position="1523" citStr="Mitchell and Lapata, 2008" startWordPosition="210" endWordPosition="213">tion method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role l</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2978" citStr="Palmer et al., 2005" startWordPosition="440" endWordPosition="443">e our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies</context>
<context position="5853" citStr="Palmer et al., 2005" startWordPosition="887" endWordPosition="890">h), that are also distinguished as core or non-core.2 Sentences are annotated using this universal frame inventory. For example, consider the pair of sentences in Figure 1(a). COMMERCE BUY is a frame that can be evoked by morphological variants of the two example lexical units buy.V and sell.V. Buyer, Seller and Goods are some example roles for this frame. 2Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work. PropBank The PropBank project (Palmer et al., 2005) is another popular resource related to semantic role labeling. The PropBank corpus has verbs annotated with sense frames and their arguments. Like FrameNet, it also has a lexical database that stores type information about verbs, in the form of sense frames and the possible semantic roles each frame could take. There are modifier roles that are shared across verb frames, somewhat similar to the non-core roles in FrameNet. Figure 1(b) shows annotations for two verbs “bought” and “sold”, with their lemmas (akin to the lexical units in FrameNet) and their verb frames buy.01 and sell.01. Generic </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="3125" citStr="Punyakanok et al., 2008" startWordPosition="466" endWordPosition="469">xperiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). 1448 Proceedings of </context>
<context position="21878" citStr="Punyakanok et al. (2008)" startWordPosition="3668" endWordPosition="3671">associates each role r in Ry with a span a according to the gold data. Note that this mapping associates spans with the null role ro as well. We optimize the following log-likelihood to train our model: � − C�θ�2 log pθ �(r, a)j|x, y, Ry 2 where pθ is a log-linear model normalized over the set Ry, with features described in Table 1. We set C = 1.0 and use L-BFGS (Liu and Nocedal, 1989) for training. Inference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints. Following Das et al. (2014) and Punyakanok et al. (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in §5.4 and §5.5. We use an off-the-shelf ILP solver for inference. max N |M(i)| θ i=1 � j=1 1452 5 Experiments In this section, we present our experiments and the results achieved. We evaluate our novel frame identification approach in isolation and also conjoined with argument identification resulting in full frame-semantic structures; before presenting our model’s performance we first focus on the datasets, baselines and the experimental se</context>
<context position="23151" citStr="Punyakanok et al. (2008)" startWordPosition="3880" endWordPosition="3883">eNet- and PropBank-style structures. For FrameNet, we use the full-text annotations in the FrameNet 1.5 release8 which was used by Das et al. (2014, §3.2). We used the same test set as Das et al. containing 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 documents were randomly chosen for development.9 For experiments with PropBank, we used the Ontonotes corpus (Hovy et al., 2006), version 4.0, and only made use of the Wall Street Journal documents; we used sections 2-21 for training, section 24 for development and section 23 for testing. This resembles the setup used by Punyakanok et al. (2008). All the verb frame files in Ontonotes were used for creating our frame lexicon. At test time, this model chooses the best frame as argmaxyψ · f(y, x, `) where argmax iterates over the possible frames y E F` if ` was seen in the lexicon or the training data, or y E F, if it was unseen, like the disambiguation scheme of §3. We train this model by maximizing L2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments. For comparison with our model from §3, which we call WSABIE EMBEDDING, we implemented two baselines with the log-linear model. Both </context>
<context position="32756" citStr="Punyakanok et al. (2008)" startWordPosition="5468" endWordPosition="5472">meter sweep in the same space. The chosen learning rate was 0.01, while the other values were γ = 0.01 and m = 512. Ambiguous lexical units were used for this selection process. Argument Candidates For PropBank we use the algorithm of Xue and Palmer (2004) applied to dependency trees. Frame Lexicon For the PropBank experiments we scanned the frame files for propositions in Ontonotes 4.0, and stored possible core roles for each verb frame. The lexical units were simply the verb associating with the verb frames. There were no unseen verbs at test time. ILP constraints We used the constraints of Punyakanok et al. (2008). 5.6 FrameNet Results Table 2 presents accuracy results on frame identification.13 We present results on all predicates, ambiguous predicates seen in the lexicon or the training data, and rare ambiguous predicates that appear ≤ 11 times in the training data. The WSABIE EMBEDDING model from §3 performs significantly better than the LOG-LINEAR WORDS baseline, while LOG-LINEAR EMBEDDING underperforms in every metric. For the SEMAFOR LEXICON setup, we also compare with the state of the art from Das 13We do not report partial frame accuracy that has been reported by prior work. Table 6: Argument o</context>
<context position="35294" citStr="Punyakanok et al., 2008" startWordPosition="5888" endWordPosition="5891">ments together; this strict metric penalizes arguments for mismatched frames, like in Table 3. We see the same trend as in Table 4. Finally, Table 6 presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from CoNLL 2005 (Carreras and M`arquez, 2005). We note that with a better frame identification model, our performance on SRL improves in general. Here, too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the single parser setting of a state-of-the-art SRL system (Punyakanok et al., 2008).14 14The last row of Table 6 refers to a system which used the 1455 6 Discussion For FrameNet, the WSABIE EMBEDDING model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art. We believe that the WSABIE EMBEDDING model performs better than the LOG-LINEAR EMBEDDING baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to ma</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1423" citStr="Socher et al., 2011" startWordPosition="197" endWordPosition="200">ion. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8869" citStr="Turian et al., 2010" startWordPosition="1397" endWordPosition="1400">y.01 buy.V buy.V COMMERCE_BUY sell.01 sell.V sell.V (a) (b) John bought a car . Buyer Goods John bought a car . A0 A1 Mary sold a car . Seller Goods Mary sold a car . A0 A1 1449 2.2 Distributed Frame Identification We present a model that takes word embeddings as input and learns to identify semantic frames. A word embedding is a distributed representation of meaning where each word is represented as a vector in Rn. Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We use word embeddings to represent the syntactic context of a particular predicate instance as a vector. For example, consider the sentence “He runs the company.” The predicate runs has two syntactic dependents – a subject and direct object (but no prepositional phrases or clausal complements). We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 ... n in the vector correspond to the subject dependent, n+1... 2n correspond to the clausal complement </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Usunier</author>
<author>D Buffoni</author>
<author>P Gallinari</author>
</authors>
<title>Ranking with ordered weighted pairwise classification.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18053" citStr="Usunier et al., 2009" startWordPosition="2978" endWordPosition="2981">here are F frames in total. The training objective function minimizes: E EL(ranky(x)) max(0,γ+s(x, y)−s(x, ¯y)). x y¯ 1451 where x, y are the training inputs and their corresponding correct frames, and y¯ are negative frames, γ is the margin. Here, ranky(x) is the rank of the positive frame y relative to all the negative frames: �ranky(x) = I(s(x, y) G γ + s(x, ¯y)), y¯ and L(η) converts the rank to a weight. Choosing L(η) = Cη for any positive constant C optimizes the mean rank, whereas a weighting such as L(η) = �ηi=1 1/i (adopted here) optimizes the top of the ranked list, as described in (Usunier et al., 2009). To train with such an objective, stochastic gradient is employed. For speed the computation of ranky(x) is then replaced with a sampled approximation: sample N items y¯ until a violation is found, i.e. max(0, γ + s(x, ¯y) − s(x, y))) &gt; 0 and then approximate the rank with (F − 1)/N, see Weston et al. (2011) for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §5.4-§5.5. Note that an alternative approach could learn only the matrix M, and then use a k-nearest neighbor classifier in Rm, as in Weinberger</context>
</contexts>
<marker>Usunier, Buffoni, Gallinari, 2009</marker>
<rawString>N. Usunier, D. Buffoni, and P. Gallinari. 2009. Ranking with ordered weighted pairwise classification. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Uszkoreit</author>
<author>T Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="25447" citStr="Uszkoreit and Brants (2008)" startWordPosition="4264" endWordPosition="4267">fe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification features in Table 1. We learn the initial embedding representations for our frame identification model (§3) using a deep neural language model similar to the one proposed by Bengio et al. (2003). We use 3 hidden layers each with 1024 neurons and learn a 128- dimensional embedding from a large corpus containing over 100 billion tokens. In order to speed up learning, we use an unnormalized output layer and a hinge-loss objective. The objective tries to ensure that the correct w</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>J. Uszkoreit and T. Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Q Weinberger</author>
<author>L K Saul</author>
</authors>
<title>Distance metric learning for large margin nearest neighbor classification.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--207</pages>
<contexts>
<context position="18669" citStr="Weinberger and Saul (2009)" startWordPosition="3086" endWordPosition="3089">al., 2009). To train with such an objective, stochastic gradient is employed. For speed the computation of ranky(x) is then replaced with a sampled approximation: sample N items y¯ until a violation is found, i.e. max(0, γ + s(x, ¯y) − s(x, y))) &gt; 0 and then approximate the rank with (F − 1)/N, see Weston et al. (2011) for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §5.4-§5.5. Note that an alternative approach could learn only the matrix M, and then use a k-nearest neighbor classifier in Rm, as in Weinberger and Saul (2009). The advantage of learning an embedding for the frame labels is that at inference time we need to consider only the set of labels for classification rather than all training examples. Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels. If we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow. 4 Argument Identification Here, we briefly describe the argument ident</context>
</contexts>
<marker>Weinberger, Saul, 2009</marker>
<rawString>K. Q. Weinberger and L. K. Saul. 2009. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>S Bengio</author>
<author>N Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="11851" citStr="Weston et al., 2011" startWordPosition="1913" endWordPosition="1916">o the predicate, and reserves a block in its output space for the embedding of words found at that position. Suppose g considers clausal complements and direct objects. Then g : X —* R2n and for the example sentence it has zeros in positions 0 ... n and the embedding of the word company in positions n+1... 2n. g(x) = [0, ... , 0, embedding of company]. Section 3.1 describes the context positions we use in our experiments. Let the low dimensional space we map to be R&apos; and the learned mapping be M : Rnk —* R&apos;. The mapping M is a linear transformation, and we learn it using the WSABIE algorithm (Weston et al., 2011). WSABIE also learns an embedding for each frame label (y, henceforth). In our setting, this means that each frame corresponds to a point in R&apos;. If we have F possible frames we can store those parameters in an F x m matrix, one m-dimensional point for each frame, which we will refer to as the linear mapping Y . Let the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be E. We denote the frames that associate with E in the frame lexicon5 and our training corpus as Ft. WSABIE performs gradient-based updates on an objective that tries to minimize the distance betw</context>
<context position="13843" citStr="Weston et al. (2011)" startWordPosition="2263" endWordPosition="2266">2014), but they use unlemmatized words to define their confusion set. 1450 Figure 2: Context representation extraction for the embedding model. Given a dependency parse (1) the model extracts all words matching a set of paths from the frame evoking predicate and its direct dependents (2). The model computes a composed representation of the predicate instance by using distributed vector representations for words (3) – the (red) vertical embedding vectors for each word are concatenated into a long vector. Finally, we learn a linear transformation function parametrized by the context blocks (4). Weston et al. (2011), and in more detail in section 3.2. Since WSABIE learns a single mapping from g(x) to Rm, parameters are shared between different words and different frames. So for example “He runs the company” could help the model disambiguate “He owns the company.” Moreover, since g(x) relies on word embeddings rather than word identities, information is shared between words. For example “He runs the company” could help us to learn about “She runs a corporation”. 3.1 Context Representation Extraction In principle g(x) could be any feature function, but we performed an initial investigation of two particula</context>
<context position="17094" citStr="Weston et al. (2011)" startWordPosition="2801" endWordPosition="2805"> dependency path types and the number of dependency labels. Given a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set. See Figure 2 for an illustration of this process. We performed initial experiments using context extracted from 1) direct dependents, 2) dependency paths, and 3) both. For all our experiments, setting 3) which concatenates the direct dependents and dependency path always dominated the other two, so we only report results for this setting. 3.2 Learning We model our objective function following Weston et al. (2011), using a weighted approximaterank pairwise loss, learned with stochastic gradient descent. The mapping from g(x) to the low dimensional space Rm is a linear transformation, so the model parameters to be learnt are the matrix M E Rnk×m as well as the embedding of each possible frame label, represented as another matrix Y E RF ×m where there are F frames in total. The training objective function minimizes: E EL(ranky(x)) max(0,γ+s(x, y)−s(x, ¯y)). x y¯ 1451 where x, y are the training inputs and their corresponding correct frames, and y¯ are negative frames, γ is the margin. Here, ranky(x) is t</context>
<context position="18363" citStr="Weston et al. (2011)" startWordPosition="3035" endWordPosition="3038">he negative frames: �ranky(x) = I(s(x, y) G γ + s(x, ¯y)), y¯ and L(η) converts the rank to a weight. Choosing L(η) = Cη for any positive constant C optimizes the mean rank, whereas a weighting such as L(η) = �ηi=1 1/i (adopted here) optimizes the top of the ranked list, as described in (Usunier et al., 2009). To train with such an objective, stochastic gradient is employed. For speed the computation of ranky(x) is then replaced with a sampled approximation: sample N items y¯ until a violation is found, i.e. max(0, γ + s(x, ¯y) − s(x, y))) &gt; 0 and then approximate the rank with (F − 1)/N, see Weston et al. (2011) for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §5.4-§5.5. Note that an alternative approach could learn only the matrix M, and then use a k-nearest neighbor classifier in Rm, as in Weinberger and Saul (2009). The advantage of learning an embedding for the frame labels is that at inference time we need to consider only the set of labels for classification rather than all training examples. Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usuall</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="28528" citStr="Xue and Palmer (2004)" startWordPosition="4751" endWordPosition="4754">We search for the stochastic gradient learning rate in {0.0001, 0.001, 0.01}, the margin γ ∈ {0.001,0.01,0.1, 1} and the dimensionality of the final vector space m ∈ {256, 512}, to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame. The underlined values are the chosen hyperparameters used to analyze the test data. Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in §4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate’s head; this helped capture cases like “afew months” (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate’s head to the position immediately before the predicate, for ca</context>
<context position="32388" citStr="Xue and Palmer (2004)" startWordPosition="5408" endWordPosition="5411">74 77.84 Dev data T 1 Test data Model P R Fl LOG-LINEAR WORDS 81.55 77.83 79.65 WSABIE EMBEDDING (§3) 81.32 77.97 79.61 Table 5: Full frame-structure prediction results for Propbank. This is a metric that takes into account frames and arguments together. See §5.7 for more details. 5.5 Experimental Setup for PropBank Hyperparameters As in §5.4, we made a hyperparameter sweep in the same space. The chosen learning rate was 0.01, while the other values were γ = 0.01 and m = 512. Ambiguous lexical units were used for this selection process. Argument Candidates For PropBank we use the algorithm of Xue and Palmer (2004) applied to dependency trees. Frame Lexicon For the PropBank experiments we scanned the frame files for propositions in Ontonotes 4.0, and stored possible core roles for each verb frame. The lexical units were simply the verb associating with the verb frames. There were no unseen verbs at test time. ILP constraints We used the constraints of Punyakanok et al. (2008). 5.6 FrameNet Results Table 2 presents accuracy results on frame identification.13 We present results on all predicates, ambiguous predicates seen in the lexicon or the training data, and rare ambiguous predicates that appear ≤ 11 </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="24970" citStr="Zhang and Nivre (2011)" startWordPosition="4180" endWordPosition="4183">, tries to decouple the WSABIE training from the embedding input, and trains a log linear model using the embeddings. So the second baseline has the same input representation as WSABIE EMBEDDING but uses a log-linear model instead of WSABIE. We call this model LOG-LINEAR EMBEDDING. 5.3 Common Experimental Setup We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conventions (de Marneffe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification fea</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Y. Zhang and J. Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of ACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>