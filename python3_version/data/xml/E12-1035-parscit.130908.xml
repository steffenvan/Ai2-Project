<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001104">
<title confidence="0.994679">
The Problem with Kappa
</title>
<author confidence="0.99271">
David M W Powers
</author>
<affiliation confidence="0.966731">
Centre for Knowledge &amp; Interaction Technology, CSEM
Flinders University
</affiliation>
<email confidence="0.996641">
David.Powers@flinders.edu.au
</email>
<sectionHeader confidence="0.996969" genericHeader="abstract">
Introduction
</sectionHeader>
<bodyText confidence="0.999517702702703">
Research in Computational Linguistics usually
requires some form of quantitative evaluation. A
number of traditional measures borrowed from
Information Retrieval (Manning &amp; Schütze,
1999) are in common use but there has been
considerable critical evaluation of these measures
themselves over the last decade or so (Entwisle
&amp; Powers, 1998, Flach, 2003, Ben-David. 2008).
Receiver Operating Analysis (ROC) has been
advocated as an alternative by many, and in
particular has been used by Fürnkranz and Flach
(2005), Ben-David (2008) and Powers (2008) to
better understand both learning algorithms
relationship and the between the various
measures, and the inherent biases that make
many of them suspect. One of the key advantages
of ROC is that it provides a clear indication of
chance level performance as well as a less well
known indication of the relative cost weighting
of positive and negative cases for each possible
system or parameterization represented.
ROC Area Under the Curve (Fig. 1) has been
also used as a performance measure but averages
over the false positive rate (Fallout) and is thus a
function of cost that is dependent on the
classifier rather than the application. For this
reason it has come into considerable criticism
and a number of variants and alternatives have
been proposed (e.g. AUK, Kaymak et. Al, 2010
and H-measure, Hand, 2009). An AUC curve
that is at least as good as a second curve at all
points, is said to dominate it and indicates that
the first classifier is equal or better than the
second for all plotted values of the parameters,
and all cost ratios. However AUC being greater
for one classifier than another does not have such
a property – indeed deconvexities within or
</bodyText>
<sectionHeader confidence="0.635839" genericHeader="acknowledgments">
Abstract
</sectionHeader>
<bodyText confidence="0.98215105882353">
It is becoming clear that traditional
evaluation measures used in
Computational Linguistics (including
Error Rates, Accuracy, Recall, Precision
and F-measure) are of limited value for
unbiased evaluation of systems, and are
not meaningful for comparison of
algorithms unless both the dataset and
algorithm parameters are strictly
controlled for skew (Prevalence and
Bias). The use of techniques originally
designed for other purposes, in particular
Receiver Operating Characteristics Area
Under Curve, plus variants of Kappa,
have been proposed to fill the void.
This paper aims to clear up some of the
confusion relating to evaluation, by
demonstrating that the usefulness of each
evaluation method is highly dependent on
the assumptions made about the
distributions of the dataset and the
underlying populations. The behaviour of
a number of evaluation measures is
compared under common assumptions.
Deploying a system in a context which
has the opposite skew from its validation
set can be expected to approximately
negate Fleiss Kappa and halve Cohen
Kappa but leave Powers Kappa
unchanged. For most performance
evaluation purposes, the latter is thus
most appropriate, whilst for comparison
of behaviour, Matthews Correlation is
recommended.
</bodyText>
<page confidence="0.984892">
345
</page>
<note confidence="0.97659">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 345–355,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999745244444445">
intersections of ROC curves are both prima facie
evidence that fusion of the parameterized
classifiers will be useful (cf. Provost and Facett,
2001; Flach and Wu, 2005).
AUK stands for Area under Kappa, and
represents a step in the advocacy of Kappa (Ben-
David, 2008ab) as an alternative to the traditional
measures and ROC AUC. Powers (2003,2007)
has also proposed a Kappa-like measure
(Informedness) and analysed it in terms of ROC,
and there are many more, Warrens (2010) analyzing
the relationships between some of the others.
Systems like RapidMiner (2011) and Weka
(Witten and Frank, 2005) provide almost all of
the measures we have considered, and many
more besides. This encourages the use of
multiple measures, and indeed it is now
becoming routine to display tables of multiple
results for each system, and this is in particular
true for the frameworks of some of the
challenges and competitions brought to the
communities (e.g. 2nd i2b2 Challenge in NLP for
Clinical Data, 2011; 2nd Pascal Challenge on
HTC, 2011)).
This use of multiple statistics is no doubt in
response to the criticism levelled at the
evaluation mechanisms used in earlier
generations of competitions and the above
mentioned critiques, but the proliferation of
alternate measures in some ways merely
compounds the problem. Researchers have the
temptation of choosing those that favour their
system as they face the dilemma of what to do
about competing (and often disagreeing)
evaluation measures that they do not completely
understand. These systems and competitions also
exhibit another issue, the tendency to macro-
averages over multiple classes, even of measures
that are not denominated in class (e.g. that are
proportions of predicted labels rather than real
classes, as with Precision).
This paper is directed at better understanding
some of these new and old measures as well as
providing recommendations as to which measures
are appropriate in which circumstances.
</bodyText>
<subsectionHeader confidence="0.742961">
What’s in a Kappa?
</subsectionHeader>
<bodyText confidence="0.999783545454546">
In this paper we focus on the Kappa family of
measures, as well as some closely related
statistics named for other letters of the Greek
alphabet, and some measures that we will show
behave as Kappa measures although they were
not originally defined as such. These include
Informedness, Gini Coefficient and single point
ROC AUC, which are in fact all equivalent to
DeltaP’ in the dichotomous case, which we deal
with first, and to the other Kappas when the
marginal prevalences (or biases) match.
</bodyText>
<subsectionHeader confidence="0.999788">
1.1 Two classes and non-negative Kappa.
</subsectionHeader>
<bodyText confidence="0.999935">
Kappa was originally proposed (Cohen, 1960) to
compare human ratings in a binary, or
dichotomous, classification task. Cohen (1960)
recognized that Rand Accuracy did not take
chance into account and therefore proposed to
subtract off the chance level of Accuracy and
then renormalize to the form of a probability:
</bodyText>
<equation confidence="0.964882">
K(Acc) = [Acc – E(Acc)] / [1 – E(Acc)] (1)
</equation>
<bodyText confidence="0.999873833333333">
This leaves the question of how to estimate the
expected Accuracy, E(Acc). Cohen (1960) made
the assumption that raters would have different
distributions that could be estimated as
the products of the corresponding marginal
coefficients of the contingency table:
</bodyText>
<table confidence="0.9967675">
+ve Class −ve Class
+ve Prediction A=TP B=FP PP
−ve Prediction C=FN D=TN PN
Notation RP RN N
</table>
<tableCaption confidence="0.999407">
Table 1. Statistical and IR Contingency Notation
</tableCaption>
<bodyText confidence="0.980985214285714">
In order to discuss this further it is important
to discuss our notational conventions, and it is
noted that in statistics, the letters A-D (upper
case or lower case) are conventionally used to
label the cells, and their sums may be used to
label the marginal cells. However in the
literature on ROC analysis, which we follow
here, it is usual to talk about true and false
positives (that is positive predictions that are
correct or incorrect), and conversely true and
false negatives. Often upper case is used to
indicate counts in the contingency table, which
sum to the number of instances, N. In this case
lower case letters are used to indicate
probabilities, which means that the
corresponding upper case values in the
contingency table are all divided by N, and n=1.
Statistics relative to (the total numbers of
items in) the real classes are called Rates and
have the number (or proportion) of Real
Positives (RP) or Real Negatives (RN) in the
denominator. In this notation, we have Recall =
TPR = TP/RP.
Conversely statistics relative to the (number
of) predictions are called Accuracies, so relative
to the predictions that label instances positively,
Predicted Positives (PP), we have Precision =
TPA = TP/PP.
</bodyText>
<page confidence="0.999101">
346
</page>
<figureCaption confidence="0.8139025">
Figure 1. Illustration of ROC Analysis. The
solid diagonal represents chance performance
</figureCaption>
<bodyText confidence="0.992986855263158">
for different rates of guessing positive or
negative labels. The dotted line represent the
convex hull enclosing the results of different
systems, thresholds or parameters tested. The
(0,0) and (1,1) points represent guessing always
negative and always positive and are always
nominal systems in a ROC curve. The points
along any straight line segment of a convex hull
are achievable by probabilistic interpolation of
the systems at each end, the gradient represents
the cost ratio and all points along the segment,
including the endpoints have the same effective
cost benefit. AUC is the area under the curve
joining the systems with straight edges and
AUCH is the area under the convex hull where
points within it are ignored. The height above
the chance line of any point represents DeltaP’,
the Gini Coefficient and also the Dichotomous
Informedness of the corresponding system, and
also corresponds to twice the area of the triangle
between it and the chance line, and thus 2AUC-1
where AUC is calculated on this single point
curve (not shown) joining it to (0,0) and (1,1).
The (1,0) point represents perfect performance
with 100% True Positive Rate and 0% False
Negative Rate.
The accuracy of all our predictions, positive or
negative, is given by Rand Accuracy =
(TF+TN)/N = tf+tn, and this is what is meant in
general by the unadorned term Accuracy, or the
abbreviation Acc.
Rand Accuracy is the weighted average of
Precision and Inverse Precision (probability that
negative predictions are correctly labeled), where
the weighting is made according to the number
of predictions made for the corresponding labels.
Rand Accuracy is also the weighted average of
Recall and Inverse Recall (probability that
negative instances are correctly predicted),
where the weighting is made according to the
number of instances in the corresponding
classes.
The marginal probabilities rp and pp are also
known as Prevalence (the class prevalence of
positive instances) and Bias (the label bias to
positive predictions), and the corresponding
probabilities of negative classes and labels are
the Inverse Prevalence and Inverse Bias
respectively. In the ROC literature, the ratios of
negative to positive classes is often referred to as
the class ratio or skew. We can similarly also
refer to a label ratio, prediction ratio or
prediction skew. Note that optimal performance
can only be achieved if class skew = label skew.
The Expected True Positives and Expected
True Negatives for Cohen Kappa, as well as Chi-
squared significance, are estimated as the
product of Bias and Prevalence, and the product
of Inverse Bias and Inverse Prevalence, resp.,
where traditional uses of Kappa for agreement of
human raters, the contingency table represents
one rater as providing the classification to be
predicted by the other rater. Cohen assumes that
their distribution of ratings are independent, as
reflected both by the margins and the
contingencies: ETP = RP*PP; ETN = RN*NN.
This gives us E(Acc) = (ETP+ETN)/N=etp+etn.
By contrast the two rater two class form of
Fleiss (1981) Kappa, also known as Scott Pi,
assumes that both raters are labeling
independently using the same distribution, and
that the margins reflect this potential variation.
The expected number of positives is thus
effectively estimated as the average of the two
raters’ counts, so that EP = (RP+PP)/2, and EN =
(RN+PN)/2, ETP = EP2 and ETN = EN2.
</bodyText>
<subsectionHeader confidence="0.987771">
1.2 Inverting Kappa
</subsectionHeader>
<bodyText confidence="0.9999857">
The definition of Kappa in Eqn (1) can be seen
to be applicable to arbitrary definitions of
Expected Accuracy, and in order to discover how
other measures relate to the family of Kappa
measures it is useful to invert Kappa to discover
the implicit definition of Expected Accuracy that
allows a measure to be interpreted as a form of
Kappa. We simply make E(Acc) the subject by
multiplying out Eqn (1) to a common
denominator and associating factors of E(Acc):
</bodyText>
<page confidence="0.904721">
347
</page>
<equation confidence="0.9999725">
K(Acc) = [Acc – E(Acc)] / [1 – E(Acc)] (1)
E(Acc) = [Acc – K(Acc)] / [1 – K(Acc)] (2)
</equation>
<bodyText confidence="0.997879333333333">
Note that for a given value of Acc the function
connecting E(Acc) and K(Acc) is its own
inverse:
</bodyText>
<equation confidence="0.999991">
E(Acc) = fAcc(K(Acc)) (3)
K(Acc) = fAcc(E(Acc)) (4)
</equation>
<bodyText confidence="0.9999174">
For the future we will tend to drop the Acc
argument or subscript when it is clear, and we
will also subscript E and K with the name or
initial of the corresponding definition of
Expectation and thus Kappa (viz. Fleiss and
Cohen so far).
Note that given Acc and E(Acc) are in the
range of 0..1 as probabilities, Kappa is also
restricted to this range, and takes the form of a
probability.
</bodyText>
<subsectionHeader confidence="0.996856">
1.3 Multiclass multirater Kappa
</subsectionHeader>
<bodyText confidence="0.999836922222223">
Fleiss (1981) and others sought to generalize the
Cohen (1960) definition of Kappa to handle both
multiple class (not just positive/negative) and
multiple raters (not just two – one of which we
have called real and the other prediction). Fleiss
in fact generalized Scott’s (1955) Pi in both
senses, not Cohen Kappa. The Fleiss Kappa is
not formulated as we have done here for
exposition, but in terms of pairings (agreements)
amongst the raters, who are each assumed to
have rated the same number of items, N, but not
necessarily all. Krippendorf’s (1970, 1978)
effectively generalizes further by dealing with
arbitrary numbers of raters assessing different
numbers of items.
Light (1971) and Hubert (1977) successfully
generalized Cohen Kappa. Another approach to
estimating E(Acc) was taken by Bennett et al
(1955) which basically assumed all classes were
equilikely (effectively what use of Accuracy, F-
Measure etc. do, although they don’t subtract off
the chance component).
The Bennett Kappa was generalized by
Randolph (2005), but as our starting point is that
we need to take the actual margins into account,
we do not pursue these further. However,
Warrens (2010a) shows that, under certain
conditions, Fleiss Kappa is a lower bound of
both the Hubert generalization of Cohen Kappa
and the Randolph generalization of Bennet
Kappa, which is itself correspondingly an upper
bound of both the Hubert and the Light
generalizations of Cohen Kappa. Unfortunately
the conditions are that there is some agreement
between the class and label skews (viz. the
prevalence and bias of each class/label). Our
focus in this paper is the behaviour of the various
Kappa measures as we move from strongly
matched to strongly mismatched biases.
Cohen (1968) also introduced a weighted
variant of Kappa. We have also discussed cost
weighting in the context of ROC, and Hand
(2009) seeks to improve on ROC AUC by
introducing a beta distribution as an estimated
cost profile, but we will not discuss them further
here as we are more interested in the
effectiveness of the classifer overall rather than
matching a particular cost profile, and are
skeptical about any generic cost distribution. In
particular the beta distribution gives priority to
central tendency rather than boundary conditions,
but boundary conditions are frequently
encountered in optimization. Similarly Kaymak
et al.’s (2010) proposal to replace AUC by AUK
corresponds to a Cohen Kappa reweighting of
ROC that eliminates many of its useful
properties, without any expectation that the
measure, as an integration across a surrogate cost
distribution, has any validity for system
selection. Introducing alternative weights is also
allowed in the definition of F-Measure, although
in practice this is almost invariably employed as
the equally weighted harmonic mean of Recall
and Precision. Introducing additional weight or
distribution parameters, just multiplies the
confusion as to which measure to believe.
Powers (2003) derived a further multiclass
Kappa-like measure from first principles,
dubbing it Informedness, based on an analogy of
Bookmaker associating costs/payoffs based on
the odds. This is then proven to measure the
proportion of time (or probability) a decision is
informed versus random, based on the same
assumptions re expectation as Cohen Kappa, and
we will thus call it Powers Kappa, and derive an
formulation of the corresponding expectation.
Powers (2007) further identifies that the
dichotomous form of Powers Kappa is equivalent
to the Gini cooefficient as a deskewed version of
the weighted Relative Accuracy proposed by
Flach (2003) based on his analysis and
deskewing of common evaluation measures in
the ROC paradigm. Powers (2007) also identifies
that Dichotomous Informedness is equivalent to
an empirically derived psychological measure
called DeltaP’ (Perruchet et al. 2004). DeltaP’
(and its dual DeltaP) were derived based on
analysis of human word association data – the
combination of this empirical observation with
the place of DeltaP’ as the dichotomous case of
</bodyText>
<page confidence="0.991663">
348
</page>
<bodyText confidence="0.98024">
Powers’ ‘Informedness’ suggests that human
association is in some sense optimal. Powers
(2007) also introduces a dual of Informedness
that he names Markedness, and shows that the
geometric mean of Informedness and
Markedness is Matthews Correlation, the
nominal analog of Pearson Correlation.
Powers’ Informedness is in fact a variant of
Kappa with some similarities to Cohen Kappa,
but also some advantages over both Cohen and
Fleiss Kappa due to its asymmetric relation with
Recall, in the dichotomous form of Powers (2007),
Informedness = Recall + InverseRecall – 1
= (Recall – Bias) / (1 – Prevalence).
If we think of Kappa as assessing the
relationship between two raters, Powers’ statistic
is not evenhanded and the Informedness and
Markedness duals measure the two directions of
prediction, normalizing Recall and Precision. In
fact, the relationship with Correlation allows
these to be interpreted as regression coefficients
for the prediction function and its inverse.
</bodyText>
<subsectionHeader confidence="0.987103">
1.4 Kappa vs Correlation
</subsectionHeader>
<bodyText confidence="0.996185170212766">
It is often asked why we don’t just use
Correlation to measure. In fact, Castellan (1996)
uses Tetrachoric Correlation, another
generalization of Pearson Correlation that
assumes that the two class variables are given by
underlying normal distributions. Uebersax
(1987), Hutchison (1993) and Bonnet and Price
(2005) each compare Kappa and Correlation and
conclude that there does not seem to be any
situation where Kappa would be preferable to
Correlation. However all the Kappa and
Correlation variants considered were symmetric,
and it is thus interesting to consider the separate
regression coefficients underlying it that
represent the Powers Kappa duals of
Informedness and Markedness, which have the
advantage of separating out the influences of
Prevalence and Bias (which then allows macro-
averaging, which is not admissable for any
symmetric form of Correlation or Kappa, as we
will discuss shortly). Powers (2007) regards
Matthews Correlation as an appropriate measure
for symmetric situations (like rater agreement)
and generalizes the relationships between
Correlation and Significance to the Markedness
and Informedness Measures. The differences
between Informedness and Markedness, which
relate to mismatches in Prevalence and Bias,
mean that the pair of numbers provides further
information about the nature of the relationship
between the two classifications or raters, whilst
the ability to take the geometric mean (of macro-
averaged) Informedness and Markedness means
that a single Correlation can be provided when
appropriate.
Our aim now is therefore to characterize
Informedness (and hence as its dual Markedness)
as a Kappa measure in relation to the families of
Kappa measures represented by Cohen and Fleiss
Kappa in the dichotomous case. Note that
Warrens (2011) shows that a linearly weighted
versions of Cohen’s (1968) Kappa is in fact a
weighted average of dichotomous Kappas.
Similarly Powers (2003) shows that his Kappa
(Informedness) has this property. Thus it is
appropriate to consider the dichotomous case,
and from this we can generalize as required.
</bodyText>
<subsectionHeader confidence="0.953321">
1.5 Kappa vs Determinant
</subsectionHeader>
<bodyText confidence="0.999681476190476">
Warrens (2010c) discusses another commonly
used measure, the Odds Ratio ad/bc (in
Epidemiology rather than Computer Science or
Computational Linguistics). Closely related to
this is the Determinant of the Contingency
Matrix dtp = ad-bc = etp-etn (in the Chi-Sqr,
Cohen and Powers sense based on independent
marginal probabilities). Both show whether the
odds favour positives over negatives more for the
first rater (real) than the second (predicted) – for
the ratio it is if it is greater than one, for the
difference it is if it is greater than 0. Note that
taking logs of all coefficients would maintain the
same relationship and that the difference of the
logs corresponds to the log of the ratio, mapping
into the information domain.
Warrens (2010c) further shows (in cost-
weighted form) that Cohen Kappa is given by the
following (in the notation of this paper, but
preferring the notations Prevalence and Inverse
Prevalence to rp and rn for clarity):
</bodyText>
<equation confidence="0.982523">
KC = dtp/[(Prev*IBias+Bias*IPrev)/2]. (5)
</equation>
<bodyText confidence="0.9928445">
Based on the previous characterization of
Fleiss Kappa, we can further characterize it by
</bodyText>
<equation confidence="0.988174">
KF = dtp/[(Prev+Bias)*(IBias+IPrev)/4]. (6)
</equation>
<bodyText confidence="0.9731845">
Powers (2007) also showed corresponding
formulations for Bookmaker Informedness (B, or
Powers Kappa = KP), Markedness and Matthews
Correlation:
</bodyText>
<equation confidence="0.999376">
B = dtp/[(Prev*IPrev)].
M = dtp/[(Bias*IBias)].
C = dtp/[√(Prev*IPrev*Bias*IBias)]. (9)
</equation>
<bodyText confidence="0.999003666666667">
These elegant dichotomous forms are
straightforward, with the independence
assumptions on Bias and Prevalence clear in
</bodyText>
<page confidence="0.996012">
349
</page>
<bodyText confidence="0.976421363636364">
Cohen Kappa, the arithmetic means of Bias and
Prevalence clear in Fleiss Kappa, and the
geometric means of Bias and Prevalence in the
Matthews Correlation. Further the independence
of Bias is apparent for Powers Kappa in the
Informedness form, and independence of
Prevalence is clear in the Markedness direction.
Note that the names Powers uses suggest that
we are measuring something about the
information conveyed by the prediction about the
class in the case of Informedness, and the
information conveyed to the predictor by the
class state in the case of Markedness. To the
extent that Prevalence and Bias can be controlled
independently, Informedness and Markedness are
independent and Correlation represents the joint
probability of information being passed in both
directions! Powers (2007) further proposes using
log formulations of these measures to take them
into the information domain, as well as relating
them to mutual information, G-squared and chi-
squared significance.
</bodyText>
<subsectionHeader confidence="0.986796">
1.6 Kappa vs Concordance
</subsectionHeader>
<bodyText confidence="0.999994793103448">
The pairwise approach used by Fleiss Kappa and
its relatives does not assume raters use a
common distribution, but does assume they are
using the same set, and number of categories.
When undertaking comparison of unconstrained
ratings or unsupervised learning, this constraint
is removed and we need to use a measure of
concordance to compare clusterings against each
other or against a Gold Standard. Some of the
concordance measures use operators in
probability space and relate closely to the
techniques here, whilst others operate in
information space. See Pfitzner et al. (2009) for
reviews of clustering comparison/concordance.
A complete coverage of evaluation would also
cover significance and the multiple testing
problem, but we will confine our focus in this
paper to the issue of choice of Kappa or
Correlation statistic, as well as addressing some
issues relating to the use of macro-averaging. In
this paper we are regarding the choice of Bias as
under the control of the experimenter, as we have
a focus on learned or hand crafted computational
linguistics systems. In fact, when we are using
bootstrapping techniques or dealing with
multiple real samples or different subjects or
ecosystems, Prevalence may also vary. Thus the
simple marginal assumptions of Cohen or
Powers statistics are the appropriate ones.
</bodyText>
<subsectionHeader confidence="0.994719">
1.7 Averaging
</subsectionHeader>
<bodyText confidence="0.998650461538462">
We now consider the issue of dealing with
multiple measures and results of multiple
classifiers by averaging. We first consider
averages of some of the individual measures we
have seen. The averages need not be arithmetic
means, or may represent means over the
Prevalences and Biases.
We will be punctuating our theoretical
discussions and explanations with empirical
demonstrations where we use 1:1 and 4:1
prevalence versus matching and mismatching
bias to generate the chance level contingency
based on marginal independence. We then mix
in a proportion of informed decisions, with the
remaining decisions made by chance.
Table 2 compares Accuracy and F-Measure
for an informed decision percentage of 0, 100, 15
and -15. Note that Powers Kappa or
‘Informedness’ purports to recover this
proportion or probability.
F-Measure is one of the most common
measures in Computational Linguistics and
Information Retrieval, being a Harmonic Mean
of Recall and Precision, which in the common
unweighted form also is interpretable with
respect to a mean of Prevalence and Bias:
</bodyText>
<equation confidence="0.99394">
F = tp / [(Prev+Bias)/2] (10)
</equation>
<bodyText confidence="0.998567">
Note that like Recall and Precision, F-Measure
ignores totally cell D corresponding to tn. This
is an issue when Prevalence and Bias are uneven
or mismatched. In Information Retrieval, it is
often justified on the basis that the number of
irrelevant documents is large and not precisely
known, but in fact this is due to lack of
knowledge of the number of relevant documents,
which affects Recall. In fact if tn is large with
respect to both rp and pp, and thus with respect
to components tp, fp and fn, then both tn/pn and
tn/rn approach 0 as tn increases without bound.
As discussed earlier, Rand Accuracy is a
prevalence (real class) weighted average of
Precision and Inverse Precision, as well as a bias
(prediction label) weighted average of Recall and
Inverse Precision. It reflects the D (tn) cell unlike
F, and while it does not remove the effect of
chance it does not have the positive bias of F.
</bodyText>
<equation confidence="0.960801">
Acc = tp + fp (11)
</equation>
<bodyText confidence="0.9860888">
We also point out that the differences between
the various Kappas shown in Determinant
normalized form in Eqns (5-9) vary only in the
way prevalences and biases are averaged
together in the normalizing denominator.
</bodyText>
<page confidence="0.992355">
350
</page>
<table confidence="0.999323777777778">
Informed 1:1/1:1 4:1/4:1 4:1/1:4
0% Acc 50% 68% 32%
F 50% 80% 32%
100% Acc 100% 100% 100%
F 100% 100% 100%
15% Acc 57.5% 72.8% 42.2%
F 57.5% 83% 46.97%
-15% Acc 42.5% 57.8% 27.2%
F 42.5% 72% 27.2%
</table>
<tableCaption confidence="0.998054">
Table 2. Accuracy and F-Measure for different
</tableCaption>
<bodyText confidence="0.99764393877551">
mixes of prevalence and bias skew (odds ratio
shown) as well as different proportions of correct
(informed) answers versus guessing – negative
proportions imply that the informed decisions are
deliberately made incorrectly (oracle tells me
what to do and I do the opposite).
From Table 2 we note that the first set of
statistics notes the chance level varies from the
50% expected for Bias=Prevalence=50%. This is
in fact the E(Acc) used in calculating Cohen
Kappa. Where Prevalences and Biases are equal
and balanced, all common statistics agree –
Recall = Precision = Accuracy = F, and they are
interpretable with respect to this 50% chance
level. All the Kappas will also agree, as the
different averages of the identical prevalences
and biases all come down to 50% as well. So
subtracting 50% from 57.5% and normalizing
(dividing) by the average effective prevalence of
50%, we return 15% informed decisions in all
cases (as seen in detail in Table 3).
However, F-measure gives an inflated estimate
when it focus on the more prevalent positive
class, with corresponding bias in the chance
component.
Worse still is the strength of the Acc and F
scores under conditions of matched bias and
prevalence when the deviation from chance is -
15% - that is making the wrong decision 15% of
the time and guessing the rest of the time. In
academic terms, if we bump these rates up to
±25% F-factor gives a High Distinction for
guessing 75% of the time and putting the right
answer for the other 25%, a Distinction for 100%
guessing, and a Credit for guessing 75% of the
time and putting a wrong answer for the other
25%! In fact, the Powers Kappa corresponds to
the methodology of multiple choice marking,
where for questions with k+1 choices, a right
answer gets 1 mark, and a wrong answer gets -1/k
so that guessing achieves an expected mark of 0.
Cohen Kappa achieves a very similar result for
unbiased guessing strategies.
We now turn to macro-averaging across
multiple classifiers or raters. The Area Under the
Curve measures are all of this form, whether we
are talking about ROC, Kappa, Recall-Precision
curves or whatever. The controversy over these
averages, and macro-averaging in general, relates
to one of two issues: 1. The averages are not in
general over the appropriate units or
denominators of the individual statistics; or 2.
The averages are over a classifier determined
cost function rather than an externally or
standardly defined cost function. AUK and H-
Measure seek to address these issues as discussed
earlier. In fact they both boil down to averaging
with an inappropriate distribution of weights.
Commonly macro-averaging averages across
classes as average statistics derived for each class
weighted by the cardinality of the class (viz.
prevalence). In our review above, we cited four
examples, but we will refer only to WEKA
(Witten et al., 2005) here as a commonly used
system and associated text book that employs
and advocates macro-averaging. WEKA
averages over tpr, fpr, Recall (yes redundantly),
Precision, F-Factor and ROC AUC. Only the
average over tpr=Recall is actually meaningful,
because only it has the number of members of
the class, or its prevalence, as its denominator.
Precision needs to be macro-averaged over the
number of predictions for each class, in which
case it is equivalent to micro-averaging.
Other micro-averaged statistics are also
shown, including Kappa (with the expectation
determined from ZeroR – predicting the majority
class, leading to a Cohen-like Kappa).
AUC will be pointwise for classifiers that
don’t provide any probabilistic information
associated with label prediction, and thus don’t
allow varying a threshold for additional points on
the ROC or other threshold curves. In the case
where multiple threshold points are available,
ROC AUC cannot be interpreted as having any
relevance to any particular classifier, but is an
average over a range of classifiers. Even then it
is not so meaningful as AUCH, which should be
used as classifiers on the convex hull are usually
available. The AUCH measure will then
dominate any individual classifiers, as if the
convex hull is not the same as the single
classifier it must include points that are above the
classifier curve and thus its enclosed area totally
includes the area that is enclosed by the
individual classifier.
Macroaveraging of the curve based on each
class in turn as the Positive Class, and weighted
</bodyText>
<page confidence="0.99693">
351
</page>
<bodyText confidence="0.999966736842105">
by the size of the positive class, is not
meaningful as effectively shown by Powers
(2003) for the special case of the single point
curve given its equivalence to Powers Kappa.
In fact Markedness does admit averaging over
classes, whilst Informedness requires averaging
over predicted labels, as does Precision. The
other Kappa and Correlations are more complex
(note the demoninators in Eqns 5-9) and how
they might be meaningfully macro-averaged is
an open question. However, microaveraging can
always be done quickly and easily by simply
summing all the contingency tables (the true
contingency tables are tables of counts, not
probabilities, as shown in Table 1).
Macroaveraging should never be done except
for the special cases of Recall and Markedness
when it is equivalent to micro-average, which is
only slightly more expensive/complicated to do.
</bodyText>
<subsectionHeader confidence="0.879868">
Comparison of Kappas
</subsectionHeader>
<bodyText confidence="0.99997916">
We now turn to explore the different definitions
of Kappas, using the same approach employed
with Accuracy and F-Factor in Table 1: We will
consider 0%, 100%, 15% and -15% informed
decisions, with random decisions modelled on
the basis of independent Bias and Prevalence.
This clearly biases against the Fleiss family of
Kappas, which is entirely appropriate. As
pointed out by Entwisle &amp; Powers (1998) the
practice of deliberately skewing bias to achieve
better statistics is to be deprecated – they used
the real-life example of a CL researcher choosing
to say water was always a noun because it was a
noun more often than not. With Cohen or Powers’
measures, any actual power of the system to
determine PoS, however weak, would be
reflected in an improvement in the scores versus
any random choice, whatever the distribution.
Recall that choosing one answer all the time
corresponds to the extreme points of the chance
line in the ROC curve.
Studies like Fitzgibbon et al (2007) and
Leibbrandt and Powers (2012) show divergences
amongst the conventional and debiased measures,
but it is tricky to prove which is better.
</bodyText>
<subsectionHeader confidence="0.781429">
Kappa in the Limit
</subsectionHeader>
<bodyText confidence="0.991163956521739">
It is however straightforward to derive limits for
the various Kappas and Expectations under
extreme and central conditions of bias and
prevalence, including both match and mismatch.
The 36 theoretical results match the mixture
model results in Table 3, however, due to space
constraints, formal treatment will be limited to
two of the more complex cases that both relate to
Fleiss Kappa with its mismatch to the marginal
independence assumptions we prefer. These will
provide informedness of probability B plus a
remaining proportion 1-B of random responses
exhibiting extreme bias versus both neutral and
contrary prevalence. Note that we consider only
|B|&lt;1 as all Kappas give Acc=1 and thus K=1 for
B=1, and only Powers Kappa is designed to work
for B&lt;1, giving K= -1 for B= -1.
Recall that the general calculation of Expected
Accuracy is
E(Acc) = etp+etn (11)
For Fleiss Kappa we must calculate the
expected values of the correct contingencies as
discussed previously with expected probabilities
</bodyText>
<equation confidence="0.998222">
ep = (rp+pp)/2 &amp; en = (rn+pn)/2 (12)
etp = ep2 &amp; etn = en2 (13)
</equation>
<bodyText confidence="0.984057">
We first consider cases where prevalence is
extreme and the chance component exhibits
inverse bias. We thus consider limits as
rp0, rn1, pp1-B, pnB. This gives us
(assuming |B|&lt;1)
</bodyText>
<equation confidence="0.998495333333333">
EF(Acc) = (1/4+B2/4+B/2)2+(1/4+B2/4-B/2)2
= (1+B2)/2 (14)
KF(Acc) = (1-B)2/[B2-2] (15)
</equation>
<bodyText confidence="0.832408">
We second consider cases where the
prevalence is balanced and chance extreme, with
rp0.5, rn0.5, pp1-B, pnB, giving
</bodyText>
<equation confidence="0.9591885">
EF(Acc) = 1/2 + (B-1/2)2/2
= 5/8 + B(B-1)/2 (16)
KF(Acc)=[(B-1/2)-(B-1/2)2/2]/[1/2-(B-1/2)2/2] (17)
=[B-5/8+B(B-1)/2]/[1-(5/8+B(B-1)/2)
</equation>
<subsectionHeader confidence="0.425702">
Conclusions
</subsectionHeader>
<bodyText confidence="0.999971894736842">
The asymmetric Powers Informedness gives
the clearest measure of the predictive value of a
system, while the Matthews Correlation (as
geometric mean with the Powers Markedness
dual) is appropriate for comparing equally valid
classifications or ratings into an agreed number
of classes. Concordance measures should be used
if number of classes is not agreed or specified.
For mismatch cases (15) Fleiss is always
negative for |B|&lt;1) and thus fails to adequately
reward good performance under these marginal
conditions. For the chance case (17), the first
form we provide shows that the deviation from
matching Prevalence is a driver in a Kappa-like
function. Cohen on the other hand (Table 3)
tends to apply multiply the weight given to error
in even mild prevalence-bias mismatch
conditions. None of the symmetric Kappas
designed for raters are suitable for classifiers.
</bodyText>
<page confidence="0.991419">
352
</page>
<table confidence="0.999952521126761">
1:1 1:1 4:1 4:1 4:1 1:4 1:1 1:1 4:1 4:1 4:1 1:4 1:1 1:1 4:1 4:1 4:1 1:4
Informedness 0% 0% 0% 0% 0% 0% 0% 0% 0%
50% 80% 80% 50% 80% 80% 50% 20% 20%
50% 20% 20% 50% 20% 20% 50% 80% 80%
50% 80% 20% 50% 80% 20% 50% 20% 80%
50% 20% 80% 50% 20% 80% 50% 80% 20%
100% 25% 25% 100% 25% 25% 100% 400% 400%
100% 25% 400% 100% 25% 400% 100% 400% 25%
100% 100% 6% 100% 100% 6% 100% 100% 1600%
50% 68% 32% 50% 68% 32% 50% 68% 32%
50% 68% 32% 50% 68% 32% 50% 68% 32%
50% 68% 50% 50% 68% 50% 50% 68% 50%
0% 0% 0% 0% 0% 0% 0% 0% 0%
0% 0% 0% 0% 0% 0% 0% 0% 0%
0% 0% -36% 0% 0% -36% 0% 0% -36%
Prevalence
Iprevalence
Bias
Ibias
SkewR
SkewP
OddsRatio
ePowers
eCohen
eFleiss
kPowers
kCohen
kFleiss
Informedness
100% 100% 100% 100% 100% 100% 100% 100% 100%
Prevalence 50% 80% 80% 50% 80% 80% 50% 20% 20%
Iprevalence 50% 20% 20% 50% 20% 20% 50% 80% 80%
Bias 50% 80% 80% 50% 80% 80% 50% 20% 20%
Ibias 50% 20% 20% 50% 20% 20% 50% 80% 80%
SkewR 100% 25% 25% 100% 25% 25% 100% 400% 400%
SkewP 100% 25% 25% 100% 25% 25% 100% 400% 400%
OddsRatio 100% 100% 100% 100% 100% 100% 100% 100% 100%
ePowers 50% 68% 68% 50% 68% 68% 50% 68% 68%
aCohen 50% 68% 68% 50% 68% 68% 50% 68% 68%
aFleiss 50% 68% 68% 50% 68% 68% 50% 68% 68%
kPowers 100% 100% 100% 100% 100% 100% 100% 100% 100%
kCohen 100% 100% 100% 100% 100% 100% 100% 100% 100%
kFleiss 100% 100% 100% 100% 100% 100% 100% 100% 100%
Informedness 15% 15% 15% 99% 99% 99% 99% 99% 99%
Prevalence 50% 80% 80% 50% 80% 80% 50% 20% 20%
Iprevalence 50% 20% 20% 50% 20% 20% 50% 80% 80%
Bias 50% 80% 29% 50% 80% 79% 50% 20% 79%
Ibias 50% 20% 71% 50% 20% 21% 50% 80% 21%
SkewR 100% 25% 25% 100% 25% 25% 100% 400% 400%
SkewP 100% 25% 245% 100% 25% 26% 100% 400% 26%
OddsRatio 100% 100% 6% 100% 100% 6% 100% 100% 1600%
ePowers 50% 68% 32% 50% 68% 32% 50% 68% 32%
eCohen 50% 68% 37% 50% 68% 68% 50% 68% 32%
eFleiss 50% 68% 50% 50% 68% 68% 50% 68% 50%
kPowers 15% 15% 15% 99% 99% 99% 1% 1% 1%
kCohen 15% 15% 8% 99% 99% 98% 1% 1% 0%
kFleiss 15% 15% -17% 99% 99% 98% 1% 1% -35%
Informedness -15% -15% -15% -99% -99% -99% -99% -99% -99%
Prevalence 50% 80% 20% 50% 80% 80% 50% 20% 20%
Iprevalence 50% 20% 80% 50% 20% 20% 50% 80% 80%
Bias 50% 71% 80% 50% 21% 20% 50% 21% 80%
Ibias 50% 29% 20% 50% 79% 80% 50% 79% 20%
SkewR 100% 25% 400% 100% 25% 25% 100% 400% 400%
SkewP 100% 41% 25% 100% 385% 400% 100% 385% 25%
OddsRatio 100% 65% 1038% 100% 25% 25% 100% 104% 1542%
ePowers 50% 63% 37% 50% 50% 50% 50% 68% 32%
eCohen 50% 63% 32% 50% 32% 32% 50% 68% 32%
eFleiss 50% 63% 50% 50% 50% 50% 50% 68% 50%
kPowers -15% -15% -15% -99% -99% -99% -1% -1% -1%
kCohen -15% -13% -7% -99% -47% -47% -1% -1% 0%
kFleiss -15% -14% -46% -99% -99% -99% -1% -1% -37%
</table>
<tableCaption confidence="0.9933625">
Table 3. Empirical Results for Accuracy and Kappa for Fleiss/Scott, Cohen and Powers. Shaded
cells indicate misleading results, which occur for both Cohen and Fleiss Kappas.
</tableCaption>
<page confidence="0.997882">
353
</page>
<sectionHeader confidence="0.723378" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.622523333333333">
2nd i2b2 Workshop on Challenges in Natural
Language Processing for Clinical Data (2008).
http://gnode1.mib.man.ac.uk/awards.html
</footnote>
<note confidence="0.80733">
(accessed 4 November 2011)
</note>
<footnote confidence="0.485341333333333">
2nd Pascal Challenge on Hierarchical Text
Classification http://lshtc.iit.demokritos.gr/node/48
(accessed 4 November 2011)
</footnote>
<reference confidence="0.999373538461538">
N. Ailon. and M. Mohri (2010) Preference-based
learning to rank. Machine Learning 80:189-211.
A. Ben-David. (2008a). About the relationship
between ROC curves and Cohen’s kappa.
Engineering Applications of AI, 21:874–882, 2008.
A. Ben-David (2008b). Comparison of classification
accuracy using Cohen’s Weighted Kappa, Expert
Systems with Applications 34 (2008) 825–832
Y. Benjamini and Y. Hochberg (1995). &amp;quot;Controlling
the false discovery rate: a practical and powerful
approach to multiple testing&amp;quot;. Journal of the Royal
Statistical Society. Series B (Methodological) 57
(1), 289–300.
D. G. Bonett &amp; R.M. Price, (2005). Inferential
Methods for the Tetrachoric Correlation
Coefficient, Journal of Educational and Behavioral
Statistics 30:2, 213-225
J. Carletta (1996). Assessing agreement on
classification tasks: the kappa statistic.
Computational Linguistics 22(2):249-254
N. J. Castellan, (1966). On the estimation of the
tetrachoric correlation coefficient. Psychometrika,
31(1), 67-73.
J. Cohen (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 1960:37-46.
J. Cohen (1968). Weighted kappa: Nominal scale
agreement with provision for scaled disagreement
or partial credit. Psychological Bulletin 70:213-20.
B. Di Eugenio and M. Glass (2004), The Kappa
Statistic: A Second Look., Computational
Linguistics 30:1 95-101.
J. Entwisle and D. M. W. Powers (1998). &amp;quot;The
Present Use of Statistics in the Evaluation of NLP
Parsers&amp;quot;, pp215-224, NeMLaP3/CoNLL98 Joint
Conference, Sydney, January 1998
Sean Fitzgibbon, David M. W. Powers, Kenneth
Pope, and C. Richard Clark (2007). Removal of
EEG noise and artefact using blind source
separation. Journal of Clinical Neurophysiology
24(3):232-243, June 2007
P. A. Flach (2003). The Geometry of ROC Space:
Understanding Machine Learning Metrics through
ROC Isometrics, Proceedings of the Twentieth
International Conference on Machine Learning
(ICML-2003), Washington DC, 2003, pp. 226-233.
J. L. Fleiss (1981). Statistical methods for rates and
proportions (2nd ed.). New York: Wiley.
A. Fraser &amp; D. Marcu (2007). Measuring Word
Alignment Quality for Statistical Machine
Translation, Computational Linguistics 33(3):293-
303.
J. Fizrnkranz &amp; P. A. Flach (2005). ROC ’n’ Rule
Learning – Towards a Better Understanding of
Covering Algorithms, Machine Learning 58(1):39-
77.
D. J. Hand (2009). Measuring classifier performance:
a coherent alternative to the area under the ROC
curve. Machine Learning 77:103-123.
T. P. Hutchinson (1993). Focus on Psychometrics.
Kappa muddles together two sources of
disagreement: tetrachoric correlation is preferable.
Research in Nursing &amp; Health 16(4):313-6, 1993
Aug.
U. Kaymak, A. Ben-David and R. Potharst (2010),
AUK: a sinple alternative to the AUC, Technical
Report, Erasmus Research Institute of
Management, Erasmus School of Economics,
Rotterdam NL.
K. Krippendorff (1970). Estimating the reliability,
systematic error, and random error of interval data.
Educational and Psychological Measurement, 30
(1),61-70.
K. Krippendorff (1978). Reliability of binary attribute
data. Biometrics, 34 (1), 142-144.
J. Lafferty, A. McCallum. &amp; F. Pereira. (2001).
Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data.
Proceedings of the 18th International Conference
on Machine Learning (ICML-2001), San
Francisco, CA: Morgan Kaufmann, pp. 282-289.
R. Leibbrandt &amp; D. M. W. Powers, Robust Induction
of Parts-of-Speech in Child-Directed Language by
Co-Clustering of Words and Contexts. (2012).
EACL Joint Workshop of ROBUS (Robust
Unsupervised and Semi-supervised Methods in
NLP) and UNSUP (Unsupervised Learning in NLP).
P. J. G. Lisboa, A. Vellido &amp; H. Wong (2000). Bias
reduction in skewed binary classfication with
Bayesian neural networks. Neural Networks
13:407-410.
</reference>
<page confidence="0.989354">
354
</page>
<reference confidence="0.999866915789474">
R. Lowry (1999). Concepts and Applications of
Inferential Statistics. (Published on the web as
http:// faculty.vassar.edu/lowry/webtext.html.)
C. D. Manning, and H. Schütze (1999). Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, MA.
J. H McDonald, (2007). The Handbook of Biological
Statistics. (Course handbook web published as
http: //udel.edu/~mcdonald/statpermissions.html)
J.C. Nunnally and Bernstein, I.H. (1994).
Psychometric Theory (Third ed.). McGraw-Hill.
K. Pearson and D. Heron (1912). On Theories of
Association. J. Royal Stat. Soc. LXXV:579-652
P. Perruchet and R. Peereman (2004). The
exploitation of distributional information in
syllable processing, J. Neurolinguistics 17:97−119.
D. Pfitzner, R. E. Leibbrandt and D. M. W. Powers
(2009). Characterization and evaluation of
similarity measures for pairs of clusterings,
Knowledge and Information Systems, 19:3, 361-394
D. M. W. Powers (2003), Recall and Precision versus
the Bookmaker, Proceedings of the International
Conference on Cognitive Science (ICSC-2003),
Sydney Australia, 2003, pp. 529-534. (See http://
david.wardpowers.info/BM/index.htm.)
D. M. W. Powers (2008), Evaluation Evaluation, The
18th European Conference on Artificial
Intelligence (ECAI’08)
D. M W Powers, (2007/2011) Evaluation: From
Precision, Recall and F-Factor to ROC,
Informedness, Markedness &amp; Correlation,
School of Informatics and Engineering, Flinders
University, Adelaide, Australia, TR SIE-07-001,
Journal of Machine Learning Technologies 2:1 37-63.
https://dl-web.dropbox.com/get/Public/201101-
Evaluation_JMLT_Postprint-Colour.pdf?w=abcda988
D. M. W. Powers, 2012. The Problem of Area Under
the Curve. International Conference on Information
Science and Technology, ICIST2012, in press.
D. M. W. Powers and A. Atyabi, 2012. The Problem
of Cross-Validation: Averaging and Bias,
Repetition and Significance, SCET2012, in press.
F. Provost and T. Fawcett. Robust classification for
imprecise environments. Machine Learning,
44:203–231, 2001.
RapidMiner (2011). http://rapid-i.com (accessed 4
November 2011).
L. H. Reeker, (2000), Theoretic Constructs and
Measurement of Performance and Intelligence in
Intelligent Systems, PerMIS 2000. (See
http://www.isd.mel.nist.gov/research_areas/
research_engineering/PerMIS_Workshop/ accessed
22 December 2007.)
W. A. Scott (1955). Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quarterly, 19, 321-325.
D. R. Shanks (1995). Is human learning rational?
Quarterly Journal of Experimental Psychology,
48A, 257-279.
T. Sellke, Bayarri, M.J. and Berger, J. (2001), Calibration
of P-values for testing precise null hypotheses,
American Statistician 55, 62-71. (See http://
www.stat.duke.edu/%7Eberger/papers.html#p-value
accessed 22 December 2007.)
P. J. Smith, Rae, DS, Manderscheid, RW and
Silbergeld, S. (1981). Approximating the moments
and distribution of the likelihood ratio statistic for
multinomial goodness of fit. Journal of the
American Statistical Association 76:375,737-740.
R. R. Sokal, Rohlf FJ (1995) Biometry: The principles
and practice of statistics in biological research, 3rd
ed New York: WH Freeman and Company.
J. Uebersax (1987). Diversity of decision-making
models and the measurement of interrater
agreement. Psychological Bulletin 101, 140−146.
J. Uebersax (2009) http://ourworld.compuserve.com/
homepages/jsuebersax/agree.htm accessed 24
February 2011.
M. J. Warrens (2010a), Inequalities between multi-
rater kappas. Advances in Data Analysis and
Classification 4:271-286.
M. J. Warrens (2010b). A formal proof of a paradox
associated with Cohen’s kappa. Journal of
Classificaiton 27:322-332.
M. J. Warrens (2010c). A Kraemer-type rescaling that
transforms the Odds Ratio into the Weighted
Kappa Coefficient. Psychometrika 75:2 328-330.
M. J. Warrens (2011). Cohen’s linearly wieghted
Kappa is a weighted average of 2x2 Kappas.
Psychometrika 76:3, 471-486.
D. A. Williams (1976). Improved Likelihood Ratio
Tests for Complete Contingency Tables,
Biometrika 63:33-37.
I. H. Witten &amp; E. Frank, (2005). Data mining (2nd
ed.). London: Academic Press.
</reference>
<page confidence="0.999007">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.683127">
<title confidence="0.999785">The Problem with Kappa</title>
<author confidence="0.996252">David M W Powers</author>
<affiliation confidence="0.996907">Centre for Knowledge &amp; Interaction Technology, Flinders University</affiliation>
<intro confidence="0.689655">David.Powers@flinders.edu.au</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Preference-based learning to rank.</title>
<date>2010</date>
<journal>Machine Learning</journal>
<pages>80--189</pages>
<marker>Mohri, 2010</marker>
<rawString>N. Ailon. and M. Mohri (2010) Preference-based learning to rank. Machine Learning 80:189-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ben-David</author>
</authors>
<title>About the relationship between ROC curves and Cohen’s kappa.</title>
<date>2008</date>
<journal>Engineering Applications of AI,</journal>
<volume>21</volume>
<contexts>
<context position="684" citStr="Ben-David (2008)" startWordPosition="95" endWordPosition="96">action Technology, CSEM Flinders University David.Powers@flinders.edu.au Introduction Research in Computational Linguistics usually requires some form of quantitative evaluation. A number of traditional measures borrowed from Information Retrieval (Manning &amp; Schütze, 1999) are in common use but there has been considerable critical evaluation of these measures themselves over the last decade or so (Entwisle &amp; Powers, 1998, Flach, 2003, Ben-David. 2008). Receiver Operating Analysis (ROC) has been advocated as an alternative by many, and in particular has been used by Fürnkranz and Flach (2005), Ben-David (2008) and Powers (2008) to better understand both learning algorithms relationship and the between the various measures, and the inherent biases that make many of them suspect. One of the key advantages of ROC is that it provides a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented. ROC Area Under the Curve (Fig. 1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is</context>
</contexts>
<marker>Ben-David, 2008</marker>
<rawString>A. Ben-David. (2008a). About the relationship between ROC curves and Cohen’s kappa. Engineering Applications of AI, 21:874–882, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ben-David</author>
</authors>
<title>(2008b). Comparison of classification accuracy using Cohen’s Weighted Kappa,</title>
<date>2008</date>
<journal>Expert Systems with Applications</journal>
<volume>34</volume>
<pages>825--832</pages>
<contexts>
<context position="684" citStr="Ben-David (2008)" startWordPosition="95" endWordPosition="96">action Technology, CSEM Flinders University David.Powers@flinders.edu.au Introduction Research in Computational Linguistics usually requires some form of quantitative evaluation. A number of traditional measures borrowed from Information Retrieval (Manning &amp; Schütze, 1999) are in common use but there has been considerable critical evaluation of these measures themselves over the last decade or so (Entwisle &amp; Powers, 1998, Flach, 2003, Ben-David. 2008). Receiver Operating Analysis (ROC) has been advocated as an alternative by many, and in particular has been used by Fürnkranz and Flach (2005), Ben-David (2008) and Powers (2008) to better understand both learning algorithms relationship and the between the various measures, and the inherent biases that make many of them suspect. One of the key advantages of ROC is that it provides a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented. ROC Area Under the Curve (Fig. 1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is</context>
</contexts>
<marker>Ben-David, 2008</marker>
<rawString>A. Ben-David (2008b). Comparison of classification accuracy using Cohen’s Weighted Kappa, Expert Systems with Applications 34 (2008) 825–832</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benjamini</author>
<author>Y Hochberg</author>
</authors>
<title>Controlling the false discovery rate: a practical and powerful approach to multiple testing&amp;quot;.</title>
<date>1995</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological)</journal>
<volume>57</volume>
<issue>1</issue>
<pages>289--300</pages>
<marker>Benjamini, Hochberg, 1995</marker>
<rawString>Y. Benjamini and Y. Hochberg (1995). &amp;quot;Controlling the false discovery rate: a practical and powerful approach to multiple testing&amp;quot;. Journal of the Royal Statistical Society. Series B (Methodological) 57 (1), 289–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bonett</author>
<author>R M Price</author>
</authors>
<title>Inferential Methods for the Tetrachoric Correlation Coefficient,</title>
<date>2005</date>
<journal>Journal of Educational and Behavioral Statistics</journal>
<volume>30</volume>
<pages>213--225</pages>
<marker>Bonett, Price, 2005</marker>
<rawString>D. G. Bonett &amp; R.M. Price, (2005). Inferential Methods for the Tetrachoric Correlation Coefficient, Journal of Educational and Behavioral Statistics 30:2, 213-225</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<pages>22--2</pages>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta (1996). Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics 22(2):249-254</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Castellan</author>
</authors>
<title>On the estimation of the tetrachoric correlation coefficient.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>67--73</pages>
<marker>Castellan, 1966</marker>
<rawString>N. J. Castellan, (1966). On the estimation of the tetrachoric correlation coefficient. Psychometrika, 31(1), 67-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>1960--37</pages>
<contexts>
<context position="5893" citStr="Cohen, 1960" startWordPosition="923" endWordPosition="924">rcumstances. What’s in a Kappa? In this paper we focus on the Kappa family of measures, as well as some closely related statistics named for other letters of the Greek alphabet, and some measures that we will show behave as Kappa measures although they were not originally defined as such. These include Informedness, Gini Coefficient and single point ROC AUC, which are in fact all equivalent to DeltaP’ in the dichotomous case, which we deal with first, and to the other Kappas when the marginal prevalences (or biases) match. 1.1 Two classes and non-negative Kappa. Kappa was originally proposed (Cohen, 1960) to compare human ratings in a binary, or dichotomous, classification task. Cohen (1960) recognized that Rand Accuracy did not take chance into account and therefore proposed to subtract off the chance level of Accuracy and then renormalize to the form of a probability: K(Acc) = [Acc – E(Acc)] / [1 – E(Acc)] (1) This leaves the question of how to estimate the expected Accuracy, E(Acc). Cohen (1960) made the assumption that raters would have different distributions that could be estimated as the products of the corresponding marginal coefficients of the contingency table: +ve Class −ve Class +v</context>
<context position="12535" citStr="Cohen (1960)" startWordPosition="2024" endWordPosition="2025">n value of Acc the function connecting E(Acc) and K(Acc) is its own inverse: E(Acc) = fAcc(K(Acc)) (3) K(Acc) = fAcc(E(Acc)) (4) For the future we will tend to drop the Acc argument or subscript when it is clear, and we will also subscript E and K with the name or initial of the corresponding definition of Expectation and thus Kappa (viz. Fleiss and Cohen so far). Note that given Acc and E(Acc) are in the range of 0..1 as probabilities, Kappa is also restricted to this range, and takes the form of a probability. 1.3 Multiclass multirater Kappa Fleiss (1981) and others sought to generalize the Cohen (1960) definition of Kappa to handle both multiple class (not just positive/negative) and multiple raters (not just two – one of which we have called real and the other prediction). Fleiss in fact generalized Scott’s (1955) Pi in both senses, not Cohen Kappa. The Fleiss Kappa is not formulated as we have done here for exposition, but in terms of pairings (agreements) amongst the raters, who are each assumed to have rated the same number of items, N, but not necessarily all. Krippendorf’s (1970, 1978) effectively generalizes further by dealing with arbitrary numbers of raters assessing different numb</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 1960:37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological Bulletin</journal>
<pages>70--213</pages>
<contexts>
<context position="14218" citStr="Cohen (1968)" startWordPosition="2293" endWordPosition="2294">hese further. However, Warrens (2010a) shows that, under certain conditions, Fleiss Kappa is a lower bound of both the Hubert generalization of Cohen Kappa and the Randolph generalization of Bennet Kappa, which is itself correspondingly an upper bound of both the Hubert and the Light generalizations of Cohen Kappa. Unfortunately the conditions are that there is some agreement between the class and label skews (viz. the prevalence and bias of each class/label). Our focus in this paper is the behaviour of the various Kappa measures as we move from strongly matched to strongly mismatched biases. Cohen (1968) also introduced a weighted variant of Kappa. We have also discussed cost weighting in the context of ROC, and Hand (2009) seeks to improve on ROC AUC by introducing a beta distribution as an estimated cost profile, but we will not discuss them further here as we are more interested in the effectiveness of the classifer overall rather than matching a particular cost profile, and are skeptical about any generic cost distribution. In particular the beta distribution gives priority to central tendency rather than boundary conditions, but boundary conditions are frequently encountered in optimizat</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>J. Cohen (1968). Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin 70:213-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>M Glass</author>
</authors>
<title>The Kappa Statistic: A Second Look.,</title>
<date>2004</date>
<journal>Computational Linguistics</journal>
<volume>30</volume>
<pages>95--101</pages>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>B. Di Eugenio and M. Glass (2004), The Kappa Statistic: A Second Look., Computational Linguistics 30:1 95-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Entwisle</author>
<author>D M W Powers</author>
</authors>
<title>The Present Use of Statistics</title>
<date>1998</date>
<booktitle>in the Evaluation of NLP Parsers&amp;quot;, pp215-224, NeMLaP3/CoNLL98 Joint Conference,</booktitle>
<location>Sydney,</location>
<contexts>
<context position="31684" citStr="Entwisle &amp; Powers (1998)" startWordPosition="5056" endWordPosition="5059"> in Table 1). Macroaveraging should never be done except for the special cases of Recall and Markedness when it is equivalent to micro-average, which is only slightly more expensive/complicated to do. Comparison of Kappas We now turn to explore the different definitions of Kappas, using the same approach employed with Accuracy and F-Factor in Table 1: We will consider 0%, 100%, 15% and -15% informed decisions, with random decisions modelled on the basis of independent Bias and Prevalence. This clearly biases against the Fleiss family of Kappas, which is entirely appropriate. As pointed out by Entwisle &amp; Powers (1998) the practice of deliberately skewing bias to achieve better statistics is to be deprecated – they used the real-life example of a CL researcher choosing to say water was always a noun because it was a noun more often than not. With Cohen or Powers’ measures, any actual power of the system to determine PoS, however weak, would be reflected in an improvement in the scores versus any random choice, whatever the distribution. Recall that choosing one answer all the time corresponds to the extreme points of the chance line in the ROC curve. Studies like Fitzgibbon et al (2007) and Leibbrandt and P</context>
</contexts>
<marker>Entwisle, Powers, 1998</marker>
<rawString>J. Entwisle and D. M. W. Powers (1998). &amp;quot;The Present Use of Statistics in the Evaluation of NLP Parsers&amp;quot;, pp215-224, NeMLaP3/CoNLL98 Joint Conference, Sydney, January 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Fitzgibbon</author>
<author>David M W Powers</author>
<author>Kenneth Pope</author>
<author>C Richard Clark</author>
</authors>
<title>Removal of EEG noise and artefact using blind source separation.</title>
<date>2007</date>
<journal>Journal of Clinical Neurophysiology</journal>
<pages>24--3</pages>
<contexts>
<context position="32263" citStr="Fitzgibbon et al (2007)" startWordPosition="5156" endWordPosition="5159">. As pointed out by Entwisle &amp; Powers (1998) the practice of deliberately skewing bias to achieve better statistics is to be deprecated – they used the real-life example of a CL researcher choosing to say water was always a noun because it was a noun more often than not. With Cohen or Powers’ measures, any actual power of the system to determine PoS, however weak, would be reflected in an improvement in the scores versus any random choice, whatever the distribution. Recall that choosing one answer all the time corresponds to the extreme points of the chance line in the ROC curve. Studies like Fitzgibbon et al (2007) and Leibbrandt and Powers (2012) show divergences amongst the conventional and debiased measures, but it is tricky to prove which is better. Kappa in the Limit It is however straightforward to derive limits for the various Kappas and Expectations under extreme and central conditions of bias and prevalence, including both match and mismatch. The 36 theoretical results match the mixture model results in Table 3, however, due to space constraints, formal treatment will be limited to two of the more complex cases that both relate to Fleiss Kappa with its mismatch to the marginal independence assu</context>
</contexts>
<marker>Fitzgibbon, Powers, Pope, Clark, 2007</marker>
<rawString>Sean Fitzgibbon, David M. W. Powers, Kenneth Pope, and C. Richard Clark (2007). Removal of EEG noise and artefact using blind source separation. Journal of Clinical Neurophysiology 24(3):232-243, June 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Flach</author>
</authors>
<title>The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics,</title>
<date>2003</date>
<booktitle>Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003),</booktitle>
<pages>226--233</pages>
<location>Washington DC,</location>
<contexts>
<context position="16089" citStr="Flach (2003)" startWordPosition="2578" endWordPosition="2579">ass Kappa-like measure from first principles, dubbing it Informedness, based on an analogy of Bookmaker associating costs/payoffs based on the odds. This is then proven to measure the proportion of time (or probability) a decision is informed versus random, based on the same assumptions re expectation as Cohen Kappa, and we will thus call it Powers Kappa, and derive an formulation of the corresponding expectation. Powers (2007) further identifies that the dichotomous form of Powers Kappa is equivalent to the Gini cooefficient as a deskewed version of the weighted Relative Accuracy proposed by Flach (2003) based on his analysis and deskewing of common evaluation measures in the ROC paradigm. Powers (2007) also identifies that Dichotomous Informedness is equivalent to an empirically derived psychological measure called DeltaP’ (Perruchet et al. 2004). DeltaP’ (and its dual DeltaP) were derived based on analysis of human word association data – the combination of this empirical observation with the place of DeltaP’ as the dichotomous case of 348 Powers’ ‘Informedness’ suggests that human association is in some sense optimal. Powers (2007) also introduces a dual of Informedness that he names Marke</context>
</contexts>
<marker>Flach, 2003</marker>
<rawString>P. A. Flach (2003). The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics, Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC, 2003, pp. 226-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L</author>
</authors>
<title>Fleiss</title>
<date>1981</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<marker>L, 1981</marker>
<rawString>J. L. Fleiss (1981). Statistical methods for rates and proportions (2nd ed.). New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Measuring Word Alignment Quality for Statistical Machine Translation,</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<pages>33--3</pages>
<marker>Fraser, Marcu, 2007</marker>
<rawString>A. Fraser &amp; D. Marcu (2007). Measuring Word Alignment Quality for Statistical Machine Translation, Computational Linguistics 33(3):293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fizrnkranz</author>
<author>P A Flach</author>
</authors>
<title>ROC ’n’ Rule Learning – Towards a Better Understanding of Covering Algorithms,</title>
<date>2005</date>
<journal>Machine Learning</journal>
<pages>58--1</pages>
<marker>Fizrnkranz, Flach, 2005</marker>
<rawString>J. Fizrnkranz &amp; P. A. Flach (2005). ROC ’n’ Rule Learning – Towards a Better Understanding of Covering Algorithms, Machine Learning 58(1):39-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Hand</author>
</authors>
<title>Measuring classifier performance: a coherent alternative to the area under the ROC curve.</title>
<date>2009</date>
<journal>Machine Learning</journal>
<pages>77--103</pages>
<contexts>
<context position="1516" citStr="Hand, 2009" startWordPosition="233" endWordPosition="234">des a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented. ROC Area Under the Curve (Fig. 1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is dependent on the classifier rather than the application. For this reason it has come into considerable criticism and a number of variants and alternatives have been proposed (e.g. AUK, Kaymak et. Al, 2010 and H-measure, Hand, 2009). An AUC curve that is at least as good as a second curve at all points, is said to dominate it and indicates that the first classifier is equal or better than the second for all plotted values of the parameters, and all cost ratios. However AUC being greater for one classifier than another does not have such a property – indeed deconvexities within or Abstract It is becoming clear that traditional evaluation measures used in Computational Linguistics (including Error Rates, Accuracy, Recall, Precision and F-measure) are of limited value for unbiased evaluation of systems, and are not meaningf</context>
<context position="14340" citStr="Hand (2009)" startWordPosition="2314" endWordPosition="2315">rt generalization of Cohen Kappa and the Randolph generalization of Bennet Kappa, which is itself correspondingly an upper bound of both the Hubert and the Light generalizations of Cohen Kappa. Unfortunately the conditions are that there is some agreement between the class and label skews (viz. the prevalence and bias of each class/label). Our focus in this paper is the behaviour of the various Kappa measures as we move from strongly matched to strongly mismatched biases. Cohen (1968) also introduced a weighted variant of Kappa. We have also discussed cost weighting in the context of ROC, and Hand (2009) seeks to improve on ROC AUC by introducing a beta distribution as an estimated cost profile, but we will not discuss them further here as we are more interested in the effectiveness of the classifer overall rather than matching a particular cost profile, and are skeptical about any generic cost distribution. In particular the beta distribution gives priority to central tendency rather than boundary conditions, but boundary conditions are frequently encountered in optimization. Similarly Kaymak et al.’s (2010) proposal to replace AUC by AUK corresponds to a Cohen Kappa reweighting of ROC that </context>
</contexts>
<marker>Hand, 2009</marker>
<rawString>D. J. Hand (2009). Measuring classifier performance: a coherent alternative to the area under the ROC curve. Machine Learning 77:103-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T P Hutchinson</author>
</authors>
<title>Focus on Psychometrics. Kappa muddles together two sources of disagreement: tetrachoric correlation is preferable.</title>
<date>1993</date>
<journal>Research in Nursing &amp; Health</journal>
<pages>16--4</pages>
<marker>Hutchinson, 1993</marker>
<rawString>T. P. Hutchinson (1993). Focus on Psychometrics. Kappa muddles together two sources of disagreement: tetrachoric correlation is preferable. Research in Nursing &amp; Health 16(4):313-6, 1993 Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Kaymak</author>
<author>A Ben-David</author>
<author>R Potharst</author>
</authors>
<title>AUK: a sinple alternative to the AUC,</title>
<date>2010</date>
<tech>Technical Report,</tech>
<institution>Erasmus Research Institute of Management, Erasmus School of Economics,</institution>
<location>Rotterdam NL.</location>
<marker>Kaymak, Ben-David, Potharst, 2010</marker>
<rawString>U. Kaymak, A. Ben-David and R. Potharst (2010), AUK: a sinple alternative to the AUC, Technical Report, Erasmus Research Institute of Management, Erasmus School of Economics, Rotterdam NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Estimating the reliability, systematic error, and random error of interval data.</title>
<date>1970</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>30</volume>
<pages>1--61</pages>
<marker>Krippendorff, 1970</marker>
<rawString>K. Krippendorff (1970). Estimating the reliability, systematic error, and random error of interval data. Educational and Psychological Measurement, 30 (1),61-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Reliability of binary attribute data.</title>
<date>1978</date>
<journal>Biometrics,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>142--144</pages>
<marker>Krippendorff, 1978</marker>
<rawString>K. Krippendorff (1978). Reliability of binary attribute data. Biometrics, 34 (1), 142-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the 18th International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA:</location>
<marker>Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum. &amp; F. Pereira. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the 18th International Conference on Machine Learning (ICML-2001), San Francisco, CA: Morgan Kaufmann, pp. 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Leibbrandt</author>
<author>D M W Powers</author>
</authors>
<title>Robust Induction of Parts-of-Speech in Child-Directed Language by Co-Clustering of Words and Contexts.</title>
<date>2012</date>
<booktitle>EACL Joint Workshop of ROBUS (Robust Unsupervised and Semi-supervised Methods in NLP) and UNSUP (Unsupervised Learning in NLP).</booktitle>
<contexts>
<context position="32296" citStr="Leibbrandt and Powers (2012)" startWordPosition="5161" endWordPosition="5164"> &amp; Powers (1998) the practice of deliberately skewing bias to achieve better statistics is to be deprecated – they used the real-life example of a CL researcher choosing to say water was always a noun because it was a noun more often than not. With Cohen or Powers’ measures, any actual power of the system to determine PoS, however weak, would be reflected in an improvement in the scores versus any random choice, whatever the distribution. Recall that choosing one answer all the time corresponds to the extreme points of the chance line in the ROC curve. Studies like Fitzgibbon et al (2007) and Leibbrandt and Powers (2012) show divergences amongst the conventional and debiased measures, but it is tricky to prove which is better. Kappa in the Limit It is however straightforward to derive limits for the various Kappas and Expectations under extreme and central conditions of bias and prevalence, including both match and mismatch. The 36 theoretical results match the mixture model results in Table 3, however, due to space constraints, formal treatment will be limited to two of the more complex cases that both relate to Fleiss Kappa with its mismatch to the marginal independence assumptions we prefer. These will pro</context>
</contexts>
<marker>Leibbrandt, Powers, 2012</marker>
<rawString>R. Leibbrandt &amp; D. M. W. Powers, Robust Induction of Parts-of-Speech in Child-Directed Language by Co-Clustering of Words and Contexts. (2012). EACL Joint Workshop of ROBUS (Robust Unsupervised and Semi-supervised Methods in NLP) and UNSUP (Unsupervised Learning in NLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J G Lisboa</author>
<author>A Vellido</author>
<author>H Wong</author>
</authors>
<title>Bias reduction in skewed binary classfication with Bayesian neural networks.</title>
<date>2000</date>
<journal>Neural Networks</journal>
<pages>13--407</pages>
<marker>Lisboa, Vellido, Wong, 2000</marker>
<rawString>P. J. G. Lisboa, A. Vellido &amp; H. Wong (2000). Bias reduction in skewed binary classfication with Bayesian neural networks. Neural Networks 13:407-410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lowry</author>
</authors>
<title>Concepts and Applications of Inferential Statistics.</title>
<date>1999</date>
<note>(Published on the web as http:// faculty.vassar.edu/lowry/webtext.html.)</note>
<marker>Lowry, 1999</marker>
<rawString>R. Lowry (1999). Concepts and Applications of Inferential Statistics. (Published on the web as http:// faculty.vassar.edu/lowry/webtext.html.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Schütze, 1999</marker>
<rawString>C. D. Manning, and H. Schütze (1999). Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H McDonald</author>
</authors>
<title>The Handbook of Biological Statistics. (Course handbook web published as http: //udel.edu/~mcdonald/statpermissions.html)</title>
<date>2007</date>
<marker>McDonald, 2007</marker>
<rawString>J. H McDonald, (2007). The Handbook of Biological Statistics. (Course handbook web published as http: //udel.edu/~mcdonald/statpermissions.html)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Nunnally</author>
<author>I H Bernstein</author>
</authors>
<title>Psychometric Theory (Third</title>
<date>1994</date>
<editor>ed.).</editor>
<publisher>McGraw-Hill.</publisher>
<marker>Nunnally, Bernstein, 1994</marker>
<rawString>J.C. Nunnally and Bernstein, I.H. (1994). Psychometric Theory (Third ed.). McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Pearson</author>
<author>D Heron</author>
</authors>
<date>1912</date>
<journal>On Theories of Association. J. Royal Stat. Soc.</journal>
<pages>579--652</pages>
<marker>Pearson, Heron, 1912</marker>
<rawString>K. Pearson and D. Heron (1912). On Theories of Association. J. Royal Stat. Soc. LXXV:579-652</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Perruchet</author>
<author>R Peereman</author>
</authors>
<title>The exploitation of distributional information in syllable processing,</title>
<date>2004</date>
<journal>J. Neurolinguistics</journal>
<pages>17--97</pages>
<marker>Perruchet, Peereman, 2004</marker>
<rawString>P. Perruchet and R. Peereman (2004). The exploitation of distributional information in syllable processing, J. Neurolinguistics 17:97−119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pfitzner</author>
<author>R E Leibbrandt</author>
<author>D M W Powers</author>
</authors>
<title>Characterization and evaluation of similarity measures for pairs of clusterings,</title>
<date>2009</date>
<journal>Knowledge and Information Systems,</journal>
<volume>19</volume>
<pages>361--394</pages>
<contexts>
<context position="22728" citStr="Pfitzner et al. (2009)" startWordPosition="3586" endWordPosition="3589">significance. 1.6 Kappa vs Concordance The pairwise approach used by Fleiss Kappa and its relatives does not assume raters use a common distribution, but does assume they are using the same set, and number of categories. When undertaking comparison of unconstrained ratings or unsupervised learning, this constraint is removed and we need to use a measure of concordance to compare clusterings against each other or against a Gold Standard. Some of the concordance measures use operators in probability space and relate closely to the techniques here, whilst others operate in information space. See Pfitzner et al. (2009) for reviews of clustering comparison/concordance. A complete coverage of evaluation would also cover significance and the multiple testing problem, but we will confine our focus in this paper to the issue of choice of Kappa or Correlation statistic, as well as addressing some issues relating to the use of macro-averaging. In this paper we are regarding the choice of Bias as under the control of the experimenter, as we have a focus on learned or hand crafted computational linguistics systems. In fact, when we are using bootstrapping techniques or dealing with multiple real samples or different</context>
</contexts>
<marker>Pfitzner, Leibbrandt, Powers, 2009</marker>
<rawString>D. Pfitzner, R. E. Leibbrandt and D. M. W. Powers (2009). Characterization and evaluation of similarity measures for pairs of clusterings, Knowledge and Information Systems, 19:3, 361-394</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
</authors>
<title>Recall and Precision versus the Bookmaker,</title>
<date>2003</date>
<booktitle>Proceedings of the International Conference on Cognitive Science (ICSC-2003),</booktitle>
<pages>529--534</pages>
<location>Sydney Australia,</location>
<note>(See http:// david.wardpowers.info/BM/index.htm.)</note>
<contexts>
<context position="3681" citStr="Powers (2003" startWordPosition="570" endWordPosition="571">arison of behaviour, Matthews Correlation is recommended. 345 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 345–355, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics intersections of ROC curves are both prima facie evidence that fusion of the parameterized classifiers will be useful (cf. Provost and Facett, 2001; Flach and Wu, 2005). AUK stands for Area under Kappa, and represents a step in the advocacy of Kappa (BenDavid, 2008ab) as an alternative to the traditional measures and ROC AUC. Powers (2003,2007) has also proposed a Kappa-like measure (Informedness) and analysed it in terms of ROC, and there are many more, Warrens (2010) analyzing the relationships between some of the others. Systems like RapidMiner (2011) and Weka (Witten and Frank, 2005) provide almost all of the measures we have considered, and many more besides. This encourages the use of multiple measures, and indeed it is now becoming routine to display tables of multiple results for each system, and this is in particular true for the frameworks of some of the challenges and competitions brought to the communities (e.g. 2n</context>
<context position="15451" citStr="Powers (2003)" startWordPosition="2480" endWordPosition="2481">k et al.’s (2010) proposal to replace AUC by AUK corresponds to a Cohen Kappa reweighting of ROC that eliminates many of its useful properties, without any expectation that the measure, as an integration across a surrogate cost distribution, has any validity for system selection. Introducing alternative weights is also allowed in the definition of F-Measure, although in practice this is almost invariably employed as the equally weighted harmonic mean of Recall and Precision. Introducing additional weight or distribution parameters, just multiplies the confusion as to which measure to believe. Powers (2003) derived a further multiclass Kappa-like measure from first principles, dubbing it Informedness, based on an analogy of Bookmaker associating costs/payoffs based on the odds. This is then proven to measure the proportion of time (or probability) a decision is informed versus random, based on the same assumptions re expectation as Cohen Kappa, and we will thus call it Powers Kappa, and derive an formulation of the corresponding expectation. Powers (2007) further identifies that the dichotomous form of Powers Kappa is equivalent to the Gini cooefficient as a deskewed version of the weighted Rela</context>
<context position="19464" citStr="Powers (2003)" startWordPosition="3087" endWordPosition="3088">e nature of the relationship between the two classifications or raters, whilst the ability to take the geometric mean (of macroaveraged) Informedness and Markedness means that a single Correlation can be provided when appropriate. Our aim now is therefore to characterize Informedness (and hence as its dual Markedness) as a Kappa measure in relation to the families of Kappa measures represented by Cohen and Fleiss Kappa in the dichotomous case. Note that Warrens (2011) shows that a linearly weighted versions of Cohen’s (1968) Kappa is in fact a weighted average of dichotomous Kappas. Similarly Powers (2003) shows that his Kappa (Informedness) has this property. Thus it is appropriate to consider the dichotomous case, and from this we can generalize as required. 1.5 Kappa vs Determinant Warrens (2010c) discusses another commonly used measure, the Odds Ratio ad/bc (in Epidemiology rather than Computer Science or Computational Linguistics). Closely related to this is the Determinant of the Contingency Matrix dtp = ad-bc = etp-etn (in the Chi-Sqr, Cohen and Powers sense based on independent marginal probabilities). Both show whether the odds favour positives over negatives more for the first rater (</context>
<context position="30496" citStr="Powers (2003)" startWordPosition="4873" endWordPosition="4874">nge of classifiers. Even then it is not so meaningful as AUCH, which should be used as classifiers on the convex hull are usually available. The AUCH measure will then dominate any individual classifiers, as if the convex hull is not the same as the single classifier it must include points that are above the classifier curve and thus its enclosed area totally includes the area that is enclosed by the individual classifier. Macroaveraging of the curve based on each class in turn as the Positive Class, and weighted 351 by the size of the positive class, is not meaningful as effectively shown by Powers (2003) for the special case of the single point curve given its equivalence to Powers Kappa. In fact Markedness does admit averaging over classes, whilst Informedness requires averaging over predicted labels, as does Precision. The other Kappa and Correlations are more complex (note the demoninators in Eqns 5-9) and how they might be meaningfully macro-averaged is an open question. However, microaveraging can always be done quickly and easily by simply summing all the contingency tables (the true contingency tables are tables of counts, not probabilities, as shown in Table 1). Macroaveraging should </context>
</contexts>
<marker>Powers, 2003</marker>
<rawString>D. M. W. Powers (2003), Recall and Precision versus the Bookmaker, Proceedings of the International Conference on Cognitive Science (ICSC-2003), Sydney Australia, 2003, pp. 529-534. (See http:// david.wardpowers.info/BM/index.htm.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
</authors>
<date>2008</date>
<booktitle>Evaluation Evaluation, The 18th European Conference on Artificial Intelligence (ECAI’08)</booktitle>
<contexts>
<context position="702" citStr="Powers (2008)" startWordPosition="98" endWordPosition="99">EM Flinders University David.Powers@flinders.edu.au Introduction Research in Computational Linguistics usually requires some form of quantitative evaluation. A number of traditional measures borrowed from Information Retrieval (Manning &amp; Schütze, 1999) are in common use but there has been considerable critical evaluation of these measures themselves over the last decade or so (Entwisle &amp; Powers, 1998, Flach, 2003, Ben-David. 2008). Receiver Operating Analysis (ROC) has been advocated as an alternative by many, and in particular has been used by Fürnkranz and Flach (2005), Ben-David (2008) and Powers (2008) to better understand both learning algorithms relationship and the between the various measures, and the inherent biases that make many of them suspect. One of the key advantages of ROC is that it provides a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented. ROC Area Under the Curve (Fig. 1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is dependent on the </context>
</contexts>
<marker>Powers, 2008</marker>
<rawString>D. M. W. Powers (2008), Evaluation Evaluation, The 18th European Conference on Artificial Intelligence (ECAI’08)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
</authors>
<title>Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness &amp; Correlation,</title>
<date>2007</date>
<journal>Journal of Machine Learning Technologies</journal>
<volume>2</volume>
<pages>07--001</pages>
<institution>School of Informatics and Engineering, Flinders University,</institution>
<location>Adelaide, Australia, TR</location>
<contexts>
<context position="15908" citStr="Powers (2007)" startWordPosition="2550" endWordPosition="2551"> Recall and Precision. Introducing additional weight or distribution parameters, just multiplies the confusion as to which measure to believe. Powers (2003) derived a further multiclass Kappa-like measure from first principles, dubbing it Informedness, based on an analogy of Bookmaker associating costs/payoffs based on the odds. This is then proven to measure the proportion of time (or probability) a decision is informed versus random, based on the same assumptions re expectation as Cohen Kappa, and we will thus call it Powers Kappa, and derive an formulation of the corresponding expectation. Powers (2007) further identifies that the dichotomous form of Powers Kappa is equivalent to the Gini cooefficient as a deskewed version of the weighted Relative Accuracy proposed by Flach (2003) based on his analysis and deskewing of common evaluation measures in the ROC paradigm. Powers (2007) also identifies that Dichotomous Informedness is equivalent to an empirically derived psychological measure called DeltaP’ (Perruchet et al. 2004). DeltaP’ (and its dual DeltaP) were derived based on analysis of human word association data – the combination of this empirical observation with the place of DeltaP’ as </context>
<context position="18460" citStr="Powers (2007)" startWordPosition="2939" endWordPosition="2940"> (2005) each compare Kappa and Correlation and conclude that there does not seem to be any situation where Kappa would be preferable to Correlation. However all the Kappa and Correlation variants considered were symmetric, and it is thus interesting to consider the separate regression coefficients underlying it that represent the Powers Kappa duals of Informedness and Markedness, which have the advantage of separating out the influences of Prevalence and Bias (which then allows macroaveraging, which is not admissable for any symmetric form of Correlation or Kappa, as we will discuss shortly). Powers (2007) regards Matthews Correlation as an appropriate measure for symmetric situations (like rater agreement) and generalizes the relationships between Correlation and Significance to the Markedness and Informedness Measures. The differences between Informedness and Markedness, which relate to mismatches in Prevalence and Bias, mean that the pair of numbers provides further information about the nature of the relationship between the two classifications or raters, whilst the ability to take the geometric mean (of macroaveraged) Informedness and Markedness means that a single Correlation can be provi</context>
<context position="20793" citStr="Powers (2007)" startWordPosition="3297" endWordPosition="3298"> greater than 0. Note that taking logs of all coefficients would maintain the same relationship and that the difference of the logs corresponds to the log of the ratio, mapping into the information domain. Warrens (2010c) further shows (in costweighted form) that Cohen Kappa is given by the following (in the notation of this paper, but preferring the notations Prevalence and Inverse Prevalence to rp and rn for clarity): KC = dtp/[(Prev*IBias+Bias*IPrev)/2]. (5) Based on the previous characterization of Fleiss Kappa, we can further characterize it by KF = dtp/[(Prev+Bias)*(IBias+IPrev)/4]. (6) Powers (2007) also showed corresponding formulations for Bookmaker Informedness (B, or Powers Kappa = KP), Markedness and Matthews Correlation: B = dtp/[(Prev*IPrev)]. M = dtp/[(Bias*IBias)]. C = dtp/[√(Prev*IPrev*Bias*IBias)]. (9) These elegant dichotomous forms are straightforward, with the independence assumptions on Bias and Prevalence clear in 349 Cohen Kappa, the arithmetic means of Bias and Prevalence clear in Fleiss Kappa, and the geometric means of Bias and Prevalence in the Matthews Correlation. Further the independence of Bias is apparent for Powers Kappa in the Informedness form, and independen</context>
</contexts>
<marker>Powers, 2007</marker>
<rawString>D. M W Powers, (2007/2011) Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness &amp; Correlation, School of Informatics and Engineering, Flinders University, Adelaide, Australia, TR SIE-07-001, Journal of Machine Learning Technologies 2:1 37-63. https://dl-web.dropbox.com/get/Public/201101-Evaluation_JMLT_Postprint-Colour.pdf?w=abcda988</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
</authors>
<date>2012</date>
<booktitle>The Problem of Area Under the Curve. International Conference on Information Science and Technology, ICIST2012,</booktitle>
<note>in press.</note>
<contexts>
<context position="32296" citStr="Powers (2012)" startWordPosition="5163" endWordPosition="5164">) the practice of deliberately skewing bias to achieve better statistics is to be deprecated – they used the real-life example of a CL researcher choosing to say water was always a noun because it was a noun more often than not. With Cohen or Powers’ measures, any actual power of the system to determine PoS, however weak, would be reflected in an improvement in the scores versus any random choice, whatever the distribution. Recall that choosing one answer all the time corresponds to the extreme points of the chance line in the ROC curve. Studies like Fitzgibbon et al (2007) and Leibbrandt and Powers (2012) show divergences amongst the conventional and debiased measures, but it is tricky to prove which is better. Kappa in the Limit It is however straightforward to derive limits for the various Kappas and Expectations under extreme and central conditions of bias and prevalence, including both match and mismatch. The 36 theoretical results match the mixture model results in Table 3, however, due to space constraints, formal treatment will be limited to two of the more complex cases that both relate to Fleiss Kappa with its mismatch to the marginal independence assumptions we prefer. These will pro</context>
</contexts>
<marker>Powers, 2012</marker>
<rawString>D. M. W. Powers, 2012. The Problem of Area Under the Curve. International Conference on Information Science and Technology, ICIST2012, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
<author>A Atyabi</author>
</authors>
<title>The Problem of Cross-Validation: Averaging and Bias, Repetition and Significance,</title>
<date>2012</date>
<note>SCET2012, in press.</note>
<marker>Powers, Atyabi, 2012</marker>
<rawString>D. M. W. Powers and A. Atyabi, 2012. The Problem of Cross-Validation: Averaging and Bias, Repetition and Significance, SCET2012, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
<author>T Fawcett</author>
</authors>
<title>Robust classification for imprecise environments.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>44--203</pages>
<marker>Provost, Fawcett, 2001</marker>
<rawString>F. Provost and T. Fawcett. Robust classification for imprecise environments. Machine Learning, 44:203–231, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RapidMiner</author>
</authors>
<date>2011</date>
<volume>4</volume>
<note>http://rapid-i.com (accessed</note>
<contexts>
<context position="3901" citStr="RapidMiner (2011)" startWordPosition="603" endWordPosition="604">7 2012. c�2012 Association for Computational Linguistics intersections of ROC curves are both prima facie evidence that fusion of the parameterized classifiers will be useful (cf. Provost and Facett, 2001; Flach and Wu, 2005). AUK stands for Area under Kappa, and represents a step in the advocacy of Kappa (BenDavid, 2008ab) as an alternative to the traditional measures and ROC AUC. Powers (2003,2007) has also proposed a Kappa-like measure (Informedness) and analysed it in terms of ROC, and there are many more, Warrens (2010) analyzing the relationships between some of the others. Systems like RapidMiner (2011) and Weka (Witten and Frank, 2005) provide almost all of the measures we have considered, and many more besides. This encourages the use of multiple measures, and indeed it is now becoming routine to display tables of multiple results for each system, and this is in particular true for the frameworks of some of the challenges and competitions brought to the communities (e.g. 2nd i2b2 Challenge in NLP for Clinical Data, 2011; 2nd Pascal Challenge on HTC, 2011)). This use of multiple statistics is no doubt in response to the criticism levelled at the evaluation mechanisms used in earlier generat</context>
</contexts>
<marker>RapidMiner, 2011</marker>
<rawString>RapidMiner (2011). http://rapid-i.com (accessed 4 November 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Reeker</author>
</authors>
<title>Theoretic Constructs and Measurement of Performance and Intelligence in Intelligent Systems,</title>
<date>2000</date>
<location>PerMIS</location>
<note>See http://www.isd.mel.nist.gov/research_areas/ research_engineering/PerMIS_Workshop/ accessed 22</note>
<marker>Reeker, 2000</marker>
<rawString>L. H. Reeker, (2000), Theoretic Constructs and Measurement of Performance and Intelligence in Intelligent Systems, PerMIS 2000. (See http://www.isd.mel.nist.gov/research_areas/ research_engineering/PerMIS_Workshop/ accessed 22 December 2007.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding.</title>
<date>1955</date>
<journal>Public Opinion Quarterly,</journal>
<volume>19</volume>
<pages>321--325</pages>
<marker>Scott, 1955</marker>
<rawString>W. A. Scott (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 19, 321-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Shanks</author>
</authors>
<title>Is human learning rational?</title>
<date>1995</date>
<journal>Quarterly Journal of Experimental Psychology,</journal>
<volume>48</volume>
<pages>257--279</pages>
<marker>Shanks, 1995</marker>
<rawString>D. R. Shanks (1995). Is human learning rational? Quarterly Journal of Experimental Psychology, 48A, 257-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sellke</author>
<author>M J Bayarri</author>
<author>J Berger</author>
</authors>
<title>Calibration of P-values for testing precise null hypotheses,</title>
<date>2001</date>
<journal>American Statistician</journal>
<volume>55</volume>
<pages>62--71</pages>
<note>See http:// www.stat.duke.edu/%7Eberger/papers.html#p-value accessed 22</note>
<marker>Sellke, Bayarri, Berger, 2001</marker>
<rawString>T. Sellke, Bayarri, M.J. and Berger, J. (2001), Calibration of P-values for testing precise null hypotheses, American Statistician 55, 62-71. (See http:// www.stat.duke.edu/%7Eberger/papers.html#p-value accessed 22 December 2007.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Smith</author>
<author>DS Rae</author>
<author>RW Manderscheid</author>
<author>S Silbergeld</author>
</authors>
<title>Approximating the moments and distribution of the likelihood ratio statistic for multinomial goodness of fit.</title>
<date>1981</date>
<journal>Journal of the American Statistical Association</journal>
<pages>76--375</pages>
<marker>Smith, Rae, Manderscheid, Silbergeld, 1981</marker>
<rawString>P. J. Smith, Rae, DS, Manderscheid, RW and Silbergeld, S. (1981). Approximating the moments and distribution of the likelihood ratio statistic for multinomial goodness of fit. Journal of the American Statistical Association 76:375,737-740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Sokal</author>
<author>Rohlf FJ</author>
</authors>
<title>Biometry: The principles and practice of statistics</title>
<date>1995</date>
<publisher>WH Freeman and Company.</publisher>
<location>New York:</location>
<note>in biological research, 3rd ed</note>
<marker>Sokal, FJ, 1995</marker>
<rawString>R. R. Sokal, Rohlf FJ (1995) Biometry: The principles and practice of statistics in biological research, 3rd ed New York: WH Freeman and Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Uebersax</author>
</authors>
<title>Diversity of decision-making models and the measurement of interrater agreement.</title>
<date>1987</date>
<journal>Psychological Bulletin</journal>
<volume>101</volume>
<pages>140--146</pages>
<contexts>
<context position="17808" citStr="Uebersax (1987)" startWordPosition="2839" endWordPosition="2840"> between two raters, Powers’ statistic is not evenhanded and the Informedness and Markedness duals measure the two directions of prediction, normalizing Recall and Precision. In fact, the relationship with Correlation allows these to be interpreted as regression coefficients for the prediction function and its inverse. 1.4 Kappa vs Correlation It is often asked why we don’t just use Correlation to measure. In fact, Castellan (1996) uses Tetrachoric Correlation, another generalization of Pearson Correlation that assumes that the two class variables are given by underlying normal distributions. Uebersax (1987), Hutchison (1993) and Bonnet and Price (2005) each compare Kappa and Correlation and conclude that there does not seem to be any situation where Kappa would be preferable to Correlation. However all the Kappa and Correlation variants considered were symmetric, and it is thus interesting to consider the separate regression coefficients underlying it that represent the Powers Kappa duals of Informedness and Markedness, which have the advantage of separating out the influences of Prevalence and Bias (which then allows macroaveraging, which is not admissable for any symmetric form of Correlation </context>
</contexts>
<marker>Uebersax, 1987</marker>
<rawString>J. Uebersax (1987). Diversity of decision-making models and the measurement of interrater agreement. Psychological Bulletin 101, 140−146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Uebersax</author>
</authors>
<date>2009</date>
<note>http://ourworld.compuserve.com/ homepages/jsuebersax/agree.htm accessed 24</note>
<marker>Uebersax, 2009</marker>
<rawString>J. Uebersax (2009) http://ourworld.compuserve.com/ homepages/jsuebersax/agree.htm accessed 24 February 2011.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J</author>
</authors>
<title>Warrens (2010a), Inequalities between multirater kappas.</title>
<booktitle>Advances in Data Analysis and Classification</booktitle>
<pages>4--271</pages>
<marker>J, </marker>
<rawString>M. J. Warrens (2010a), Inequalities between multirater kappas. Advances in Data Analysis and Classification 4:271-286.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J</author>
</authors>
<title>Warrens (2010b). A formal proof of a paradox associated with Cohen’s kappa.</title>
<journal>Journal of Classificaiton</journal>
<pages>27--322</pages>
<marker>J, </marker>
<rawString>M. J. Warrens (2010b). A formal proof of a paradox associated with Cohen’s kappa. Journal of Classificaiton 27:322-332.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J</author>
</authors>
<title>Warrens (2010c). A Kraemer-type rescaling that transforms the Odds Ratio into the Weighted Kappa Coefficient.</title>
<journal>Psychometrika</journal>
<volume>75</volume>
<pages>328--330</pages>
<marker>J, </marker>
<rawString>M. J. Warrens (2010c). A Kraemer-type rescaling that transforms the Odds Ratio into the Weighted Kappa Coefficient. Psychometrika 75:2 328-330.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J</author>
</authors>
<title>Warrens (2011). Cohen’s linearly wieghted Kappa is a weighted average of 2x2 Kappas.</title>
<journal>Psychometrika</journal>
<volume>76</volume>
<pages>471--486</pages>
<marker>J, </marker>
<rawString>M. J. Warrens (2011). Cohen’s linearly wieghted Kappa is a weighted average of 2x2 Kappas. Psychometrika 76:3, 471-486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Williams</author>
</authors>
<title>Improved Likelihood Ratio Tests for Complete Contingency Tables,</title>
<date>1976</date>
<journal>Biometrika</journal>
<pages>63--33</pages>
<marker>Williams, 1976</marker>
<rawString>D. A. Williams (1976). Improved Likelihood Ratio Tests for Complete Contingency Tables, Biometrika 63:33-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data mining (2nd ed.).</booktitle>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="3935" citStr="Witten and Frank, 2005" startWordPosition="607" endWordPosition="610">or Computational Linguistics intersections of ROC curves are both prima facie evidence that fusion of the parameterized classifiers will be useful (cf. Provost and Facett, 2001; Flach and Wu, 2005). AUK stands for Area under Kappa, and represents a step in the advocacy of Kappa (BenDavid, 2008ab) as an alternative to the traditional measures and ROC AUC. Powers (2003,2007) has also proposed a Kappa-like measure (Informedness) and analysed it in terms of ROC, and there are many more, Warrens (2010) analyzing the relationships between some of the others. Systems like RapidMiner (2011) and Weka (Witten and Frank, 2005) provide almost all of the measures we have considered, and many more besides. This encourages the use of multiple measures, and indeed it is now becoming routine to display tables of multiple results for each system, and this is in particular true for the frameworks of some of the challenges and competitions brought to the communities (e.g. 2nd i2b2 Challenge in NLP for Clinical Data, 2011; 2nd Pascal Challenge on HTC, 2011)). This use of multiple statistics is no doubt in response to the criticism levelled at the evaluation mechanisms used in earlier generations of competitions and the above</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten &amp; E. Frank, (2005). Data mining (2nd ed.). London: Academic Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>