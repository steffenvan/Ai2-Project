<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000867">
<title confidence="0.995066">
Setting Up User Action Probabilities in User Simulations for Dialog
System Development
</title>
<author confidence="0.998268">
Hua Ai
</author>
<affiliation confidence="0.8393585">
University of Pittsburgh
Pittsburgh PA, 15260, USA
</affiliation>
<email confidence="0.998621">
hua@cs.pitt.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897875">
User simulations are shown to be useful in
spoken dialog system development. Since
most current user simulations deploy prob-
ability models to mimic human user be-
haviors, how to set up user action proba-
bilities in these models is a key problem
to solve. One generally used approach is
to estimate these probabilities from human
user data. However, when building a new
dialog system, usually no data or only a
small amount of data is available. In this
study, we compare estimating user proba-
bilities from a small user data set versus
handcrafting the probabilities. We discuss
the pros and cons of both solutions for dif-
ferent dialog system development tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984">
User simulations are widely used in spoken di-
alog system development. Recent studies use
user simulations to generate training corpora to
learn dialog strategies automatically ((Williams
and Young, 2007), (Lemon and Liu, 2007)), or to
evaluate dialog system performance (L´opez-C´ozar
et al., 2003). Most studies show that using user
simulations significantly improves dialog system
performance as well as speeds up system devel-
opment. Since user simulation is such a useful
tool, dialog system researchers have studied how
to build user simulations from a variety of perspec-
tives. Some studies look into the impact of training
data on user simulations. For example, (Georgila
et al., 2008) observe differences between simu-
lated users trained from human users of different
age groups. Other studies explore different simu-
lation models, i.e. the mechanism of deciding the
next user actions given the current dialog context.
(Schatzmann et al., 2006) give a thorough review
of different types of simulation models. Since
</bodyText>
<author confidence="0.829137">
Diane Litman
</author>
<affiliation confidence="0.8819845">
University of Pittsburgh
Pittsburgh PA, 15260, USA
</affiliation>
<email confidence="0.987579">
litman@cs.pitt.edu
</email>
<bodyText confidence="0.999984842105263">
most of these current user simulation techniques
use probabilistic models to generate user actions,
how to set up the probabilities in the simulations
is another important problem to solve.
One general approach to set up user action prob-
abilities is to learn the probabilities from a col-
lected human user dialog corpus ((Schatzmann et
al., 2007b), (Georgila et al., 2008)). While this
approach takes advantage of observed user behav-
iors in predicting future user behaviors, it suffers
from the problem of learning probabilities from
one group of users while potentially using them
with another group of users. The accuracy of the
learned probabilities becomes more questionable
when the collected human corpus is small. How-
ever, this is a common problem in building new
dialog systems, when often no data1 or only a
small amount of data is available. An alterna-
tive approach is to handcraft user action proba-
bilities ((Schatzmann et al., 2007a), (Janarthanam
and Lemon, 2008)). This approach is less data-
intensive, but requires nontrivial work by domain
experts. What is more, as the number of proba-
bilities increases, it is hard even for the experts to
set the probabilities. Since both handcrafting and
training user action probabilities have their own
pros and cons, it is an interesting research ques-
tion to investigate which approach is better for a
certain task given the amount of data that is avail-
able.
In this study, we investigate a manual and a
trained approach in setting up user action proba-
bilities, applied to building the same probabilis-
tic simulation model. For the manual user simula-
tions, we look into two sets of handcrafted proba-
bilities which use the same expert knowledge but
differ in individual probability values. This aims
to take into account small variations that can possi-
</bodyText>
<footnote confidence="0.976944">
1When no human user data is collected with the dialog
system, Wizard-of-Oz experiments can be conducted to col-
lect training data for building user simulations.
</footnote>
<page confidence="0.873527">
888
</page>
<note confidence="0.999625">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 888–896,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999795882352941">
bly be introduced by different domain experts. For
the trained user simulations, we examine two sets
of probabilities trained from user corpora of dif-
ferent sizes, since the amount of training data will
impact the quality of the trained probability mod-
els. We compare the trained and the handcrafted
simulations on three tasks. We observe that in our
task settings, the two manual simulations do not
differ significantly on any tasks. In addition, there
is no significant difference among the trained and
the manual simulations in generating corpus level
dialog behaviors as well as in generating training
corpora for learning dialog strategies. When com-
paring on a dialog system evaluation task, the sim-
ulation trained from more data significantly out-
performs the two manual simulations, which again
outperforms the simulation trained from less data.
Based on our observations, we answer the orig-
inal question of how to design user action proba-
bilities for simulations that are similar to ours in
terms of the complexity of the simulations2. We
suggest that handcrafted user simulations can per-
form reasonably well in building a new dialog sys-
tem, especially when we are not sure that there is
enough data for training simulation models. How-
ever, once we have a dialog system, it is use-
ful to collect human user data in order to train a
new user simulation model since the trained sim-
ulations perform better than the handcrafted user
simulations on more tasks. Since how to decide
whether enough data is available for simulation
training is another research question to answer, we
will further discuss the impact of our results later
in Section 6.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99597275">
Most current simulation models are probabilistic
models in which the models simulate user actions
based on dialog context features (Schatzmann et
al., 2006). We represent these models as:
</bodyText>
<equation confidence="0.968108">
P(user action|feature1,...,featuren) (1)
</equation>
<bodyText confidence="0.9310696">
The number of probabilities involved in this
model is:
(# of feature values). (2)
Some studies handcraft these probabilities. For
example, (Schatzmann et al., 2007a) condition the
</bodyText>
<footnote confidence="0.97661">
2The number of user action probabilities and the simu-
lated user behaviors will impact the design choice.
</footnote>
<bodyText confidence="0.999168941176471">
user actions on user’s goals and the agenda to
reach those goals. They manually author the prob-
abilities in the user’s agenda update model and the
goal update model, and then calculate the user ac-
tion probabilities based on the two models. (Ja-
narthanam and Lemon, 2008) handcraft 15 proba-
bilities in simulated users’ initial profiles and then
author rules to update these probabilities during
the dialogs.
Other studies use a human user corpus as the
training corpus to learn user action probabilities
in user simulations. Since the human user cor-
pus often does not include all possible actions that
users may take during interactions with the dialog
system, different strategies are used to account for
user actions that do not appear in the training cor-
pus but may be present when testing the user sim-
ulations. For example, (Schatzmann et al., 2007b)
introduce a summary space approach to map the
actual dialog context space into a more tractable
summary space. Then, they use forward and back-
ward learning algorithms to learn the probabili-
ties from a corpus generated by 40 human users
(160 dialogs). (Rieser and Lemon, 2006) use a
two step approach in computing the probabilities
from a corpus consisting of dialogs from 24 hu-
man users (70 dialogs). They first cluster dialog
contexts based on selected features and then build
conditional probability models for each cluster.
In our study, we build a conditional probability
model which will be described in detail in Sec-
tion 3.2.1. There are 40 probabilities to set up in
this model3. We will explain different approaches
to assign these probabilities later in Section 3.2.2.
</bodyText>
<sectionHeader confidence="0.97194" genericHeader="method">
3 System and User Simulations
</sectionHeader>
<bodyText confidence="0.998825666666667">
In this section, we describe the dialog system, the
human user corpus we collected with the system,
and the user simulation we used.
</bodyText>
<subsectionHeader confidence="0.999662">
3.1 System and Corpus
</subsectionHeader>
<bodyText confidence="0.999659714285714">
The ITSPOKE system (Litman and Silliman,
2004) is an Intelligent Tutoring System which
teaches Newtonian physics. It is a speech-
enhanced version of the Why2-Atlas tutoring sys-
tem (Vanlehn et al., 2002). During the interac-
tion with students, the system initiates a spoken
tutoring dialog to correct misconceptions and to
</bodyText>
<footnote confidence="0.9702775">
3There are 2 possible actions in our model, 20 possible
values for the first feature qCluster and 2 possible values for
the second feature prevCorrectness as described later in Sec-
tion 3.2.1. Using Equation 2, 40=(2-1)*20*2.
</footnote>
<equation confidence="0.631042">
(# of possible actions-1) *
n
11
k=1
</equation>
<page confidence="0.994311">
889
</page>
<table confidence="0.5482958">
SYSTEM1: Do you recall what Newton’s
third law says? [3rdLaw]
Student1: Force equals mass times
acceleration. [ic, c%=0, ncert]
SYSTEM2: Newton’s third law says ...
If you hit the wall harder, is the
force of your fist acting on the
wall greater or less? [3rdLaw]
Student2: Greater. [c, c%=50%,cert]
Dialog goes on
</table>
<tableCaption confidence="0.999724">
Table 1: Sample coded dialog excerpt.
</tableCaption>
<bodyText confidence="0.9999018">
elicit further explanation. A pretest is given before
the interaction and a posttest is given afterwards.
We calculate a Normalized Learning Gain for each
student to evaluate the performance of the system
in terms of the student’s knowledge gain:
</bodyText>
<equation confidence="0.8414185">
NLG = posttest score - pretest score (3)
1-pretest score
</equation>
<bodyText confidence="0.999976928571429">
The current tutoring dialog strategy was hand-
crafted in a finite state paradigm by domain ex-
perts, and the tutor’s response is based only on the
correctness of the student’s answer4. However, tu-
toring research (Craig et al., 2004) suggests that
other underlying information in student utterances
(e.g., student certainty) is also useful in improving
learning. Therefore, we are working on learning
a dialog strategy to also take into account student
certainty.
In our prior work, a corpus of 100 dialogs (1388
student turns) was collected between 20 human
subjects (5 dialogs per subject) and the ITSPOKE
system. Correctness (correct(c), incorrect(ic)) is
automatically judged by the system and is kept in
the system’s logs. We also computed the student’s
correctness rate (c%) and labeled it after every
student turn. Each student utterance was manu-
ally annotated for certainty (certain(cert), notcer-
tain(ncert)) in a previous study based on both lex-
ical and prosodic information5. In addition, we
manually clustered tutor questions into 20 clusters
based on the knowledge that is required to answer
that question, e.g. questions on Newton’s Third
Law are put into a cluster labeled as (3rdLaw).
There are other clusters such as gravity, acceler-
ation, etc. An example of a coded dialog between
the system and a student is given in Table 1.
</bodyText>
<footnote confidence="0.743902">
4Despite the limitation of the current system, students
learn significantly after interacting with the system.
5Kappa of 0.68 is gained in the agreement study.
</footnote>
<subsectionHeader confidence="0.963184">
3.2 User Simulation Model and Model
Probabilities Set-up
3.2.1 User Simulation Model
</subsectionHeader>
<bodyText confidence="0.996736311111111">
We build a Knowledge Consistency Model6 (KC
Model) to simulate consistent student behaviors
while interacting with a tutoring system. Ac-
cording to learning literature (Cen et al., 2006),
once a student acquires certain knowledge, his/her
performance on similar problems that require
the same knowledge (i.e. questions from the
same cluster we introduced in Section 3.1) will
become stable. Therefore, in the KC Model,
we condition the student action stuAction based
on the cluster of tutor question (qCluster) and
the student’s correctness when last encountering
a question from that cluster (prevCorrectness):
P(stuAction|qCluster, prevCorrectness). For
example, in Table 1, when deciding the student’s
answer after the second tutor question, the simu-
lation looks back into the dialog and finds out that
the last time (in Student1) the student answered
a question from the same cluster 3rdLaw incor-
rectly. Therefore, this time the simulation gives
a correct student answer based on the probability
P(cl3rdLaw, ic).
Since different groups of students often have
different learning abilities, we examine such dif-
ferences among our users by grouping the users
based on Normalized Learning Gains (NLG),
which is an important feature to describe user be-
haviors in tutoring systems. By dividing our hu-
man users into high/low learners based on the me-
dian of NLG, we find a significant difference in the
NLG of the two groups based on 2-tailed t-tests
(p &lt; 0.05). Therefore, we construct a simula-
tion to represent low learners and another simula-
tion to represent high learners to better character-
ize the differences in high/low learners’ behaviors.
Similar approaches are adopted in other studies in
building user simulations for dialog systems (e.g.,
(Georgila et al., 2008) simulate old versus young
users separately).
Our simulation models work on the word level 7
because generating student dialog acts alone does
not provide sufficient information for our tutoring
system to decide the next system action. Since it
is hard to generate a natural language utterance for
each tutor’s question, we use the student answers
</bodyText>
<footnote confidence="0.987333666666667">
6This is the best model we built in our previous studies
(Ai and Litman, 2007).
7See (Ai and Litman, 2006) for more details.
</footnote>
<page confidence="0.996221">
890
</page>
<bodyText confidence="0.999501">
in the human user corpus as the candidate answers
for the simulated students.
</bodyText>
<subsectionHeader confidence="0.819166">
3.2.2 Model Probabilities Set-up
</subsectionHeader>
<bodyText confidence="0.999373913043478">
Now we discuss how to set up user action prob-
abilities in the KC Model. We compare learning
probabilities from human user data to handcrafting
probabilities based on expert knowledge. Since we
represent high/low learners using different mod-
els, we build simulation models with separate user
action probabilities to represent the two groups of
learners.
When learning the probabilities in the Trained
KC Models, we calculate user action probabilities
for high/low learners in our human corpus sepa-
rately. We use add-one smoothing to account for
user actions that do not appear in the human user
corpus. For the first time the student answers a
question in a certain cluster, we back-off the user
action probability to P(stuAction I average cor-
rectness rate of this question in human user cor-
pus). We first train a KC model using the data
from all 20 human users to build the TrainedMore
(Tmore) Model. Then, in order to investigate the
impact of the amount of training data on the qual-
ity of trained simulations, we randomly pick 5 out
of the 10 high learners and 5 out of the 10 low
learners to get an even smaller human user corpus.
We train the TrainedLess (Tless) Model from this
small corpus.
When handcrafting the probabilities in the Man-
ual KC Models8, the clusters of questions are
first grouped into three difficulty groups (Easy,
Medium, Hard). Based on expert knowledge,
we assume on average 70% of students can cor-
rectly answer the tutor questions from the Easy
group, while for the Medium group only 60%
and for the hard group 50%. Then, we assign
a correctness rate higher than the average for
the high learners and a corresponding correctness
rate lower than the average for the low learners.
For the first Manual KC model (M1), within the
same difficulty group, the same two probabilities
P1(stuAction|qClusteri, prevCorrectness = c) and
P2(stuAction|qClusteri, prevCorrectness = ic) are
assigned to each clusteri as the averages for the
corresponding high/low learners. Since a different
human expert will possibly provide a slightly dif-
ferent set of probabilities even based on the same
mechanism, we also design another set of prob-
</bodyText>
<footnote confidence="0.681054">
8The first author of the paper acts as the domain expert.
</footnote>
<bodyText confidence="0.999862625">
abilities to account for such variations. For the
second Manual KC model (M2), we allow dif-
ferences among the clusters within the same dif-
ficulty group. For the clusters in each difficulty
group, we randomly assign a probability that dif-
fers no more than 5% from the average. For exam-
ple, for the easy clusters, we assign average proba-
bilities of high/low learners between [65%, 75%].
Although human experts may differ to some ex-
tent in assigning individual probability values, we
hypothesize that in general a certain amount of ex-
pertise is required in assigning these probabilities.
To investigate this, we build a baseline simula-
tion with no expert knowledge, which is a Ran-
dom Model (Ran) that randomly assigns values
for these user action probabilities.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="method">
4 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.999985315789474">
In this section, we introduce the evaluation mea-
sures for comparing the simulated corpora gen-
erated by different simulation models to the hu-
man user corpus. In Section 4.1, we use a set of
widely used domain independent features to com-
pare the simulated and the human user corpora
on corpus-level dialog behaviors. These compar-
isons give us a direct impression of how similar
the simulated dialogs are to human user dialogs.
Then, we compare the simulations in task-oriented
contexts. Since simulated user corpora are often
used as training corpora for using MDPs to learn
new dialog strategies, in Section 4.2 we estimate
how different the learned dialog strategies would
be when trained from different simulated corpora.
Another way to use user simulation is to test dialog
systems. Therefore, in Section 4.3, we compare
the user actions predicted by the various simula-
tion models with actual human user actions.
</bodyText>
<subsectionHeader confidence="0.9696225">
4.1 Measures on Corpus Level Dialog
Behaviors
</subsectionHeader>
<bodyText confidence="0.999927090909091">
We compare the dialog corpora generated by user
simulations to our human user corpus using a com-
prehensive set of corpus level measures proposed
by (Schatzmann et al., 2005). Here, we use a sub-
set of the measures which describe high-level dia-
log features that are applicable to our data. The
measures we use include the number of student
turns (Sturn), the number of tutor turns (Tturn), the
number of words per student turn (Swordrate), the
number of words per tutor turn (Twordrate), the ra-
tio of system/user words per dialog (WordRatio),
</bodyText>
<page confidence="0.992523">
891
</page>
<bodyText confidence="0.969264">
and the percentage of correct answers (cRate).
</bodyText>
<subsectionHeader confidence="0.938259">
4.2 Measures on Dialog Strategy Learning
</subsectionHeader>
<bodyText confidence="0.999455642857143">
In this section, we introduce two measures to com-
pare the simulations based on their performance
on a dialog strategy learning task. In recent stud-
ies (e.g., (Janarthanam and Lemon, 2008)), user
simulations are built to generate a large corpus
to build MDPs in using Reinforcement Learning
(RL) to learn new dialog strategies. When building
an MDP from a training corpus9, we compute the
transition probabilities P(st+1|st, a) (the proba-
bility of getting from state st to the next state st+1
after taking action a), and the reward of this transi-
tion R(st, a, st+1). Then, the expected cumulative
value (V-value) of a state s can be calculated using
this recursive function:
</bodyText>
<equation confidence="0.990815">
V (s) � � P(st+1st, a)[R(st, a, st+1) + &apos;yV (st+1)]
st+1
(4)
</equation>
<bodyText confidence="0.966623857142857">
-y is a discount factor which ranges between 0 and
1.
For our evaluation, we first compare the tran-
sition probabilities calculated from all simulated
corpora. The transition probabilities are only de-
termined by the states and user actions presented
by the training corpus, regardless of the rest of the
MDP configuration. Since the MDP configuration
has a big impact on the learned strategies, we want
to first factor this impact out and estimate the dif-
ferences in learned strategies that are brought in
by the training corpora alone. As a second evalua-
tion measure, we apply reinforcement learning to
the MDP representing each simulated corpus sep-
arately to learn dialog strategies. We compare the
Expected Cumulative Rewards (ECRs)(Williams
and Young, 2007) of these dialog strategies, which
show the expectation of the rewards we can obtain
by applying the learned strategies.
The MDP learning task in our study is to max-
imize student certainty during tutoring dialogs.
The dialog states are characterized using the cor-
rectness of the current student answer and the stu-
dent correctness rate so far. We represent the cor-
rectness rate as a binary feature: lc if it is below
the training corpus average and hc if it is above the
average. The end of dialog reward is assigned to
be +100 if the dialog has a percent certainty higher
</bodyText>
<footnote confidence="0.997129">
9In this paper, we use off-line model-based RL (Paek,
2006) rather than learning an optimal strategy online during
system-user interactions.
</footnote>
<bodyText confidence="0.999622818181819">
than the median from the training corpus and -100
otherwise. The action choice of the tutoring sys-
tem is to give a strong (s) or weak (w) feedback.
A strong feedback clearly indicates the correctness
of the current student answer while the weak feed-
back does not. For example, the second system
turn in Table 1 contains a weak feedback. If the
system says “Your answer is incorrect” at the be-
ginning of this turn, that would be a strong feed-
back. In order to simulate student certainty, we
simply output the student certainty originally asso-
ciated in each student utterance. Thus, the output
of the KC Models here is a student utterance along
with the student certainty (cert, ncert). In a pre-
vious study (Ai et al., 2007), we investigated the
impact of different MDP configurations by com-
paring the ECRs of the learned dialog strategies.
Here, we use one of the best-performing MDP
configurations, but vary the simulated corpora that
we train the dialog strategies on. Our goal is to see
which user simulation performs better in generat-
ing a training corpus for dialog strategy learning.
</bodyText>
<subsectionHeader confidence="0.997824">
4.3 Measures on Dialog System Evaluation
</subsectionHeader>
<bodyText confidence="0.999982407407408">
In this section, we introduce two ways to com-
pare human user actions with the actions predicted
by the simulations. The aim of this comparison
is to assess how accurately the simulations can
replicate human user behaviors when encounter-
ing the same dialog situation. A simulated user
that can accurately predict human user behaviors
is needed to replace human users when evaluating
dialog systems.
We randomly divide the human user dialog cor-
pus into four parts: each part contains a balanced
amount of high/low learner data. Then we perform
four fold cross validation by always using 3 parts
of the data as our training corpus for user simula-
tions, and the remaining one part of the data as
testing data to compare with simulated user ac-
tions. We always compare high human learners
only with simulation models that represent high
learners and low human learners only with simu-
lation models that represent low learners. Compar-
isons are done on a turn by turn basis. Every time
the human user takes an action in the dialogs in the
testing data, the user simulations are used to pre-
dict an action based on related dialog information
from the human user dialog. For a KC Model, the
related dialog information includes qCluster and
prevCorrectness . We first compare the simulation
</bodyText>
<page confidence="0.995854">
892
</page>
<bodyText confidence="0.964169230769231">
predicted user actions directly with human user ac-
tions. We define simulation accuracy as:
simulated user action contributes to the compar-
ison between different simulations. In contrast,
the measures introduced in Section 4.1 and Sec-
tion 4.2 have less differentiating power since they
compare at the corpus level.
Accuracy Correctly predicted human user actions
y = (5)
Total number of human user actions
However, since our simulation model is a prob-
abilistic model, the model will take an action
stochastically after the same tutor turn. In other
words, we need to take into account the probabil-
ity for the simulation to predict the right human
user action. If the simulation outputs the right ac-
tion with a small probability, it is less likely that
this simulation can correctly predict human user
behaviors when generating a large dialog corpus.
We consider a simulated action associated with a
higher probability to be ranked higher than an ac-
tion with a lower probability. Then, we use the re-
ciprocal ranking from information retrieval tasks
(Radev et al., 2002) to assess the simulation per-
formance10. Mean Reciprocal Ranking is defined
as:
</bodyText>
<equation confidence="0.986596">
1(6)
ranki
</equation>
<bodyText confidence="0.999273695652174">
In Equation 6, A stands for the total number of
human user actions, ranki stands for the ranking
of the simulated action which matches the i-th hu-
man user action.
Table 2 shows an example of comparing simu-
lated user actions with human user actions in the
sample dialog in Table 1. In the first turn Stu-
dent1, a simulation model has a 60% chance to
output an incorrect answer and a 40% chance to
output a correct answer while it actually outputs
an incorrect answer. In this case, we consider the
simulation ranks the actions in the order of: ic, c.
Since the human user gives an incorrect answer at
this time, the simulated action matches with this
human user action and the reciprocal ranking is
1. However, in the turn Student2, the simulation’s
output does not match the human user action. This
time, the correct simulated user action is ranked
second. Therefore, the reciprocal ranking of this
simulation action is 1/2.
We hypothesize that the measures introduced
in this section have larger power in differentiat-
ing different simulated user behaviors since every
</bodyText>
<footnote confidence="0.9817675">
10(Georgila et al., 2008) use Precision and Recall to cap-
ture similar information as our accuracy, and Expected Pre-
cision and Expected Recall to capture similar information as
our reciprocal ranking.
</footnote>
<sectionHeader confidence="0.998555" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999985875">
We let all user simulations interact with our dia-
log system, where each simulates 250 low learners
and 250 high learners. In this section, we report
the results of applying the evaluation measures we
discuss in Section 4 on comparing simulated and
human user corpora. When we talk about signifi-
cant results in the statistics tests below, we always
mean that the p-value of the test is &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.986465">
5.1 Comparing on Corpus Level Dialog
Behavior
</subsectionHeader>
<bodyText confidence="0.9997405625">
Figure 1 shows the results of comparisons using
domain independent high-level dialog features of
our corpora. The x-axis shows the evaluation mea-
sures; the y-axis shows the mean for each corpus
normalized to the mean of the human user cor-
pus. Error bars show the standard deviations of
the mean values. As we can see from the figure,
the Random Model performs differently from the
human and all the other simulated models. There
is no difference in dialog behaviors among the hu-
man corpus, the trained and the manual simulated
corpora.
In sum, both the Trained KC Models and
the Manual KC Models can generate human-like
high-level dialog behaviors while the Random
Model cannot.
</bodyText>
<subsectionHeader confidence="0.995901">
5.2 Comparing on Dialog Strategy Learning
Task
</subsectionHeader>
<bodyText confidence="0.999559428571429">
Next, we compare the difference in dialog strategy
learning when training on the simulated corpora
using similar approaches in (Tetreault and Litman,
2008). Table 3 shows the transition probabilities
starting from the state (c, lc). For example, the
first cell shows in the Tmore corpus, the probabil-
ity of starting from state (c, lc), getting a strong
feedback, and transitioning into the same state is
24.82%. We calculate the same table for the other
three states (c, hc), (ic, lc), and (ic, hc). Using
paired-sample t-tests with bonferroni corrections,
the only significant differences are observed be-
tween the random simulated corpus and each of
the other simulated corpora.
</bodyText>
<figure confidence="0.9931392">
1
MRR = A
A
E
���
</figure>
<page confidence="0.996043">
893
</page>
<table confidence="0.99669925">
i-th Turn human Simulation Model Simulation Output CorrectlyPredictedActions ReciprocalRanking
Student1 ic 60% ic, 40% c ic 1 1
Student2 c 70% ic, 30% c ic 0 1/2
Average / / / (1+0)/2 (1+1/2)/2
</table>
<tableCaption confidence="0.996094">
Table 2: An Example of Comparing Simulated Actions with Human User Actions.
</tableCaption>
<figureCaption confidence="0.870153">
Figure 1: Comparison of human and simulated dialogs by high-level dialog features.
</figureCaption>
<table confidence="0.999688555555556">
Tmore Tless M1 M2 Ran
s→c lc 24.82 31.42 25.64 22.70 13.25
w→c lc 17.64 12.35 16.62 18.85 9.74
s→ic lc 2.11 7.07 1.70 1.63 19.31
w→ic lc 1.80 2.17 2.05 3.25 21.06
s→c hc 29.95 26.46 22.23 31.04 10.54
w→c hc 13.93 9.50 22.73 15.10 11.29
s→ic hc 5.52 2.51 4.29 0.54 7.13
w→ic hc 4.24 9.08 4.74 6.89 7.68
</table>
<tableCaption confidence="0.970595666666667">
Table 3: Comparisons of MDP transition proba-
bilities at state (c, lc) (Numbers in this table are
percentages).
</tableCaption>
<table confidence="0.999007666666667">
Tmore Tless M1 M2 Ran
ECR 15.10 11.72 15.24 15.51 7.03
CI ±2.21 ±1.95 ±2.07 ±3.46 ±2.11
</table>
<tableCaption confidence="0.803979">
Table 4: Comparisons of ECR of learned dialog
strategies.
</tableCaption>
<bodyText confidence="0.999956315789473">
We also use a MDP toolkit to learn dialog strate-
gies from all the simulated corpora and then com-
pute the Expected Cumulative Reward (ECR) for
the learned strategies. In Table 4, the upper part
of each cell shows the ECR of the learned dialog
strategy; the lower part of the cell shows the 95%
Confidence Interval (CI) of the ECR. We can see
from the overlap of the confidence intervals that
the only significant difference is observed between
the dialog strategy trained from the random simu-
lated corpus and the strategies trained from each
of the other simulated corpora. Also, it is inter-
esting to see that the CI of the two manual simu-
lations overlap more with the CI of Tmore model
than with the CI of the Tless model.
In sum, the manual user simulations work as
well as the trained user simulation when being
used to generate a training corpus to apply MDPs
to learn new dialog strategies.
</bodyText>
<table confidence="0.9991704">
Tmore Tless M1 M2 Ran
Accu- 0.78 0.60 0.70 0.72 0.41
racy (±0.01) (±0.02) (±0.02) (±0.02) (±0.02)
MRR 0.72 0.52 0.63 0.64 0.32
(±0.02) (±0.02) (±0.02) (±0.01) (±0.02)
</table>
<tableCaption confidence="0.9852055">
Table 5: Comparisons of correctly predicted hu-
man user actions.
</tableCaption>
<subsectionHeader confidence="0.8557855">
5.3 Comparisons in Dialog System
Evaluation
</subsectionHeader>
<bodyText confidence="0.999982703703704">
Finally, we compare how accurately the user sim-
ulations can predict human user actions given the
same dialog context. Table 5 shows the averages
and CIs (in parenthesis) from the four fold cross
validations. The second row shows the results
based on direct comparisons with human user ac-
tions, and the third row shows the mean recipro-
cal ranking of simulated actions. We observe that
in terms of both the accuracy and the reciprocal
ranking, the performance ranking from the high-
est to the lowest (with significant difference be-
tween adjacent ranks) is: the Tmore Model, both
of the manual models (no significant differences
between these two models), the Tless Model, and
the Ran Model. Therefore, we suggest that the
handcrafted user simulation is not sufficient to be
used in evaluating dialog systems because it does
not generate user actions that are as similar to hu-
man user actions. However, the handcrafted user
simulation is still better than a user simulation
trained with not enough training data. This re-
sult also indicates that this evaluation measure has
more differentiating power than the previous mea-
sures since it captures significant differences that
are not shown by the previous measures.
In sum, the Tmore simulation performs the best
in predicting human user actions.
</bodyText>
<page confidence="0.998003">
894
</page>
<sectionHeader confidence="0.991406" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989588888889">
Setting up user action probabilities in user sim-
ulation is a non-trivial task, especially when no
training data or only a small amount of data is
available. In this study, we compare several ap-
proaches in setting up user action probabilities
for the same simulation model: training from all
available human user data, training from half of
the available data, two handcrafting approaches
which use the same expert knowledge but differ
slightly in individual probability assignments, and
a baseline approach which randomly assigns all
user action probabilities. We compare the built
simulations from different aspects. We find that
the two trained simulations and the two hand-
crafted simulations outperform the random simu-
lation in all tasks. No significant difference is ob-
served among the trained and the handcrafted sim-
ulations when comparing their generated corpora
on corpus-level dialog features as well as when
serving as the training corpora for learning dialog
strategies. However, the simulation trained from
all available human user data can predict human
user actions more accurately than the handcrafted
simulations, which again perform better than the
model trained from half of the human user corpus.
Nevertheless, no significant difference is observed
between the two handcrafted simulations.
Our study takes a first step in comparing the
choices of handcrafting versus training user simu-
lations when only limited or even no training data
is available, e.g., when constructing a new dialog
system. As shown for our task setting, both types
of user simulations can be used in generating train-
ing data for learning new dialog strategies. How-
ever, we observe (as in a prior study by (Schatz-
mann et al., 2007b)) that the simulation trained
from more user data has a better chance to outper-
form the simulation trained from less training data.
We also observe that a handcrafted user simulation
with expert knowledge can reach the performance
of the better trained simulation. However, a cer-
tain level of expert knowledge is needed in hand-
crafting user simulations since a random simula-
tion does not perform well in any tasks. Therefore,
our results suggest that if an expert is available for
designing a user simulation when not enough user
data is collected, it may be better to handcraft the
user simulation than training the simulation from
the small amount of human user data. However,
it is another open research question to answer how
much data is enough for training a user simulation,
which depends on many factors such as the com-
plexity of the user simulation model. When using
simulations to test a dialog system, our results sug-
gest that once we have enough human user data, it
is better to use the data to train a new simulation
to replace the handcrafted simulation.
In the future, we will conduct follow up stud-
ies to confirm our current findings since there are
several factors that can impact our results. First
of all, our current system mainly distinguishes the
student answers as correct and incorrect. We are
currently looking into dividing the incorrect stu-
dent answers into more categories (such as par-
tially correct answers, vague answers, or over-
specific answers) which will increase the number
of simulated user actions. Also, although the size
of the human corpus which we build the trained
user simulations from is comparable to other stud-
ies (e.g., (Rieser and Lemon, 2006), (Schatzmann
et al., 2007b)), using a larger human corpus may
improve the performance of the trained simula-
tions. We are in the process of collecting another
corpus which will consist of 60 human users (300
dialogs). We plan to re-train a simulation when
this new corpus is available. Also, we would be
able to train more complex models (e.g., a simula-
tion model which takes into account a longer dia-
log history) with the extra data. Finally, although
we add some noise into the current manual simula-
tion designed by our domain expert to account for
variations of expert knowledge, we would like to
recruit another human expert to construct a new
manual simulation to compare with the existing
simulations. It would also be interesting to repli-
cate our experiments on other dialog systems to
see whether our observations will generalize. Our
long term goal is to provide guidance of how to ef-
fectively build user simulations for different dialog
system development tasks given limited resources.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999216">
The first author is supported by Mellon Fellow-
ship from the University of Pittsburgh. This work
is supported partially by NSF 0325054. We thank
K. Forbes-Riley, P. Jordan and the anonymous re-
viewers for their insightful suggestions.
</bodyText>
<sectionHeader confidence="0.999073" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.956496">
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
</reference>
<page confidence="0.98816">
895
</page>
<reference confidence="0.999883779069767">
Dialogue Corpora. In Proc. of the AAAI Workshop
on Statistical and Empirical Approaches for Spoken
Dialogue Systems.
H. Ai and D. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of
Interspeech 2007.
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. of NAACL-HLT 2007.
H. Cen, K. Koedinger and B. Junker. 2006. Learn-
ing Factors Analysis-A General Method for Cogni-
tive Model Evaluation and Improvement. In Proc. of
8th International Conference on ITS.
S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004.
Affect and learning: an exploratory look into the
role of affect in learning with AutoTutor. Journal
of Educational Media 29(3), 241250.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc. of Interspeech
2005.
K. Georgila, M. Wolters, and J. Moore. 2008. Simu-
lating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems. In
Proc. of 46th ACL.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. of the
12th SEMdial Workshop on on the Semantics and
Pragmatics of Dialogues.
O. Lemon and X. Liu. 2007. Dialogue Policy Learn-
ing for combinations of Noise and User Simulation:
transfer results. In Proc. of 8th SIGdial.
D. Litman and S. Silliman. 2004. ITSPOKE: An Intel-
ligent Tutoring Spoken Dialogue System. In Com-
panion Proc. of the Human Language Technology:
NAACL.
R. L´opez-C´ozar, A. De la Torre, J. C. Segura and A.
J. Rubio. 2003. Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication (40): 387-407.
T. Paek. 2006. Reinforcement learning for spo-
ken dialogue systems: Comparing strengths and
weaknesses for practical deployment. In Proc.
of Interspeech-06 Workshop on ”Dialogue on Dia-
logues - Multidisciplinary Evaluation of Advanced
Speech-based Interacive Systems”.
D. Radev, H. Qi, H. Wu, and W. Fan. 2002. Evaluating
web-based question answering systems. In Proc. of
LREC 2002.
V. Rieser and O. Lemon. 2006. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. of Interspeech 2006.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of 6th
SIGDial.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S.
Young. 2006. A Survey of Statistical User Simula-
tion Techniques for Reinforcement-Learning of Di-
alogue Management Strategies. Knowledge Engi-
neering Review 21(2): 97-126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc. of HLT/NAACL 2007.
J. Schatzmann, B. Thomson and S. Young. 2007b. Sta-
tistical User Simulation with a Hidden Agenda. In
Proc. of 8th SIGdial.
J. Tetreault and D. Litman. 2008. A Reinforcement
Learning Approach to Evaluating State Representa-
tions in Spoken Dialogue Systems. Speech Commu-
nication (Special Issue on Evaluating new methods
and models for advanced speech-based interactive
systems), 50(8-9): 683-696.
K. VanLehn, P. Jordan, C. Ros´e, D. Bhembe, M.
B¨ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R.
Srivastava, and R. Wilson. 2002. The architecture
of Why2-Atlas: A coach for qualitative physics es-
say writing. In Proc. Intelligent Tutoring Systems
Conference..
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
</reference>
<page confidence="0.99901">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970804">
<title confidence="0.9990485">Setting Up User Action Probabilities in User Simulations for Dialog System Development</title>
<author confidence="0.999696">Hua Ai</author>
<affiliation confidence="0.999998">University of Pittsburgh</affiliation>
<address confidence="0.998788">Pittsburgh PA, 15260, USA</address>
<email confidence="0.999673">hua@cs.pitt.edu</email>
<abstract confidence="0.998474117647059">User simulations are shown to be useful in spoken dialog system development. Since most current user simulations deploy probability models to mimic human user behaviors, how to set up user action probabilities in these models is a key problem to solve. One generally used approach is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D Litman</author>
</authors>
<title>Comparing Real-Real, Simulated-Simulated, and Simulated-Real Spoken Dialogue Corpora.</title>
<date>2006</date>
<booktitle>In Proc. of the AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="13113" citStr="Ai and Litman, 2006" startWordPosition="2102" endWordPosition="2105">rences in high/low learners’ behaviors. Similar approaches are adopted in other studies in building user simulations for dialog systems (e.g., (Georgila et al., 2008) simulate old versus young users separately). Our simulation models work on the word level 7 because generating student dialog acts alone does not provide sufficient information for our tutoring system to decide the next system action. Since it is hard to generate a natural language utterance for each tutor’s question, we use the student answers 6This is the best model we built in our previous studies (Ai and Litman, 2007). 7See (Ai and Litman, 2006) for more details. 890 in the human user corpus as the candidate answers for the simulated students. 3.2.2 Model Probabilities Set-up Now we discuss how to set up user action probabilities in the KC Model. We compare learning probabilities from human user data to handcrafting probabilities based on expert knowledge. Since we represent high/low learners using different models, we build simulation models with separate user action probabilities to represent the two groups of learners. When learning the probabilities in the Trained KC Models, we calculate user action probabilities for high/low lea</context>
</contexts>
<marker>Ai, Litman, 2006</marker>
<rawString>H. Ai and D. Litman. 2006. Comparing Real-Real, Simulated-Simulated, and Simulated-Real Spoken Dialogue Corpora. In Proc. of the AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D Litman</author>
</authors>
<title>Knowledge Consistent User Simulations for Dialog Systems.</title>
<date>2007</date>
<booktitle>In Proc. of Interspeech</booktitle>
<contexts>
<context position="13085" citStr="Ai and Litman, 2007" startWordPosition="2097" endWordPosition="2100">etter characterize the differences in high/low learners’ behaviors. Similar approaches are adopted in other studies in building user simulations for dialog systems (e.g., (Georgila et al., 2008) simulate old versus young users separately). Our simulation models work on the word level 7 because generating student dialog acts alone does not provide sufficient information for our tutoring system to decide the next system action. Since it is hard to generate a natural language utterance for each tutor’s question, we use the student answers 6This is the best model we built in our previous studies (Ai and Litman, 2007). 7See (Ai and Litman, 2006) for more details. 890 in the human user corpus as the candidate answers for the simulated students. 3.2.2 Model Probabilities Set-up Now we discuss how to set up user action probabilities in the KC Model. We compare learning probabilities from human user data to handcrafting probabilities based on expert knowledge. Since we represent high/low learners using different models, we build simulation models with separate user action probabilities to represent the two groups of learners. When learning the probabilities in the Trained KC Models, we calculate user action pr</context>
</contexts>
<marker>Ai, Litman, 2007</marker>
<rawString>H. Ai and D. Litman. 2007. Knowledge Consistent User Simulations for Dialog Systems. In Proc. of Interspeech 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>J Tetreault</author>
<author>D Litman</author>
</authors>
<title>Comparing User Simulation Models for Dialog Strategy Learning.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT</booktitle>
<contexts>
<context position="20759" citStr="Ai et al., 2007" startWordPosition="3381" endWordPosition="3384">o give a strong (s) or weak (w) feedback. A strong feedback clearly indicates the correctness of the current student answer while the weak feedback does not. For example, the second system turn in Table 1 contains a weak feedback. If the system says “Your answer is incorrect” at the beginning of this turn, that would be a strong feedback. In order to simulate student certainty, we simply output the student certainty originally associated in each student utterance. Thus, the output of the KC Models here is a student utterance along with the student certainty (cert, ncert). In a previous study (Ai et al., 2007), we investigated the impact of different MDP configurations by comparing the ECRs of the learned dialog strategies. Here, we use one of the best-performing MDP configurations, but vary the simulated corpora that we train the dialog strategies on. Our goal is to see which user simulation performs better in generating a training corpus for dialog strategy learning. 4.3 Measures on Dialog System Evaluation In this section, we introduce two ways to compare human user actions with the actions predicted by the simulations. The aim of this comparison is to assess how accurately the simulations can r</context>
</contexts>
<marker>Ai, Tetreault, Litman, 2007</marker>
<rawString>H. Ai, J. Tetreault, and D. Litman. 2007. Comparing User Simulation Models for Dialog Strategy Learning. In Proc. of NAACL-HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cen</author>
<author>K Koedinger</author>
<author>B Junker</author>
</authors>
<title>Learning Factors Analysis-A General Method for Cognitive Model Evaluation and Improvement.</title>
<date>2006</date>
<booktitle>In Proc. of 8th International Conference on ITS.</booktitle>
<contexts>
<context position="11076" citStr="Cen et al., 2006" startWordPosition="1780" endWordPosition="1783">re put into a cluster labeled as (3rdLaw). There are other clusters such as gravity, acceleration, etc. An example of a coded dialog between the system and a student is given in Table 1. 4Despite the limitation of the current system, students learn significantly after interacting with the system. 5Kappa of 0.68 is gained in the agreement study. 3.2 User Simulation Model and Model Probabilities Set-up 3.2.1 User Simulation Model We build a Knowledge Consistency Model6 (KC Model) to simulate consistent student behaviors while interacting with a tutoring system. According to learning literature (Cen et al., 2006), once a student acquires certain knowledge, his/her performance on similar problems that require the same knowledge (i.e. questions from the same cluster we introduced in Section 3.1) will become stable. Therefore, in the KC Model, we condition the student action stuAction based on the cluster of tutor question (qCluster) and the student’s correctness when last encountering a question from that cluster (prevCorrectness): P(stuAction|qCluster, prevCorrectness). For example, in Table 1, when deciding the student’s answer after the second tutor question, the simulation looks back into the dialog</context>
</contexts>
<marker>Cen, Koedinger, Junker, 2006</marker>
<rawString>H. Cen, K. Koedinger and B. Junker. 2006. Learning Factors Analysis-A General Method for Cognitive Model Evaluation and Improvement. In Proc. of 8th International Conference on ITS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Craig</author>
<author>A Graesser</author>
<author>J Sullins</author>
<author>B Gholson</author>
</authors>
<title>Affect and learning: an exploratory look into the role of affect in learning with AutoTutor.</title>
<date>2004</date>
<journal>Journal of Educational Media</journal>
<volume>29</volume>
<issue>3</issue>
<pages>241250</pages>
<contexts>
<context position="9535" citStr="Craig et al., 2004" startWordPosition="1540" endWordPosition="1543">ent2: Greater. [c, c%=50%,cert] Dialog goes on Table 1: Sample coded dialog excerpt. elicit further explanation. A pretest is given before the interaction and a posttest is given afterwards. We calculate a Normalized Learning Gain for each student to evaluate the performance of the system in terms of the student’s knowledge gain: NLG = posttest score - pretest score (3) 1-pretest score The current tutoring dialog strategy was handcrafted in a finite state paradigm by domain experts, and the tutor’s response is based only on the correctness of the student’s answer4. However, tutoring research (Craig et al., 2004) suggests that other underlying information in student utterances (e.g., student certainty) is also useful in improving learning. Therefore, we are working on learning a dialog strategy to also take into account student certainty. In our prior work, a corpus of 100 dialogs (1388 student turns) was collected between 20 human subjects (5 dialogs per subject) and the ITSPOKE system. Correctness (correct(c), incorrect(ic)) is automatically judged by the system and is kept in the system’s logs. We also computed the student’s correctness rate (c%) and labeled it after every student turn. Each studen</context>
</contexts>
<marker>Craig, Graesser, Sullins, Gholson, 2004</marker>
<rawString>S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004. Affect and learning: an exploratory look into the role of affect in learning with AutoTutor. Journal of Educational Media 29(3), 241250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>Learning User Simulations for Information State Update Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc. of Interspeech</booktitle>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2005. Learning User Simulations for Information State Update Dialogue Systems. In Proc. of Interspeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>M Wolters</author>
<author>J Moore</author>
</authors>
<title>Simulating the Behaviour of Older versus Younger Users when Interacting with Spoken Dialogue Systems.</title>
<date>2008</date>
<booktitle>In Proc. of 46th ACL.</booktitle>
<contexts>
<context position="1534" citStr="Georgila et al., 2008" startWordPosition="237" endWordPosition="240">. Recent studies use user simulations to generate training corpora to learn dialog strategies automatically ((Williams and Young, 2007), (Lemon and Liu, 2007)), or to evaluate dialog system performance (L´opez-C´ozar et al., 2003). Most studies show that using user simulations significantly improves dialog system performance as well as speeds up system development. Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives. Some studies look into the impact of training data on user simulations. For example, (Georgila et al., 2008) observe differences between simulated users trained from human users of different age groups. Other studies explore different simulation models, i.e. the mechanism of deciding the next user actions given the current dialog context. (Schatzmann et al., 2006) give a thorough review of different types of simulation models. Since Diane Litman University of Pittsburgh Pittsburgh PA, 15260, USA litman@cs.pitt.edu most of these current user simulation techniques use probabilistic models to generate user actions, how to set up the probabilities in the simulations is another important problem to solve</context>
<context position="12659" citStr="Georgila et al., 2008" startWordPosition="2027" endWordPosition="2030">g the users based on Normalized Learning Gains (NLG), which is an important feature to describe user behaviors in tutoring systems. By dividing our human users into high/low learners based on the median of NLG, we find a significant difference in the NLG of the two groups based on 2-tailed t-tests (p &lt; 0.05). Therefore, we construct a simulation to represent low learners and another simulation to represent high learners to better characterize the differences in high/low learners’ behaviors. Similar approaches are adopted in other studies in building user simulations for dialog systems (e.g., (Georgila et al., 2008) simulate old versus young users separately). Our simulation models work on the word level 7 because generating student dialog acts alone does not provide sufficient information for our tutoring system to decide the next system action. Since it is hard to generate a natural language utterance for each tutor’s question, we use the student answers 6This is the best model we built in our previous studies (Ai and Litman, 2007). 7See (Ai and Litman, 2006) for more details. 890 in the human user corpus as the candidate answers for the simulated students. 3.2.2 Model Probabilities Set-up Now we discu</context>
<context position="24700" citStr="Georgila et al., 2008" startWordPosition="4041" endWordPosition="4044">is case, we consider the simulation ranks the actions in the order of: ic, c. Since the human user gives an incorrect answer at this time, the simulated action matches with this human user action and the reciprocal ranking is 1. However, in the turn Student2, the simulation’s output does not match the human user action. This time, the correct simulated user action is ranked second. Therefore, the reciprocal ranking of this simulation action is 1/2. We hypothesize that the measures introduced in this section have larger power in differentiating different simulated user behaviors since every 10(Georgila et al., 2008) use Precision and Recall to capture similar information as our accuracy, and Expected Precision and Expected Recall to capture similar information as our reciprocal ranking. 5 Results We let all user simulations interact with our dialog system, where each simulates 250 low learners and 250 high learners. In this section, we report the results of applying the evaluation measures we discuss in Section 4 on comparing simulated and human user corpora. When we talk about significant results in the statistics tests below, we always mean that the p-value of the test is &lt; 0.05. 5.1 Comparing on Corpu</context>
</contexts>
<marker>Georgila, Wolters, Moore, 2008</marker>
<rawString>K. Georgila, M. Wolters, and J. Moore. 2008. Simulating the Behaviour of Older versus Younger Users when Interacting with Spoken Dialogue Systems. In Proc. of 46th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. of the 12th SEMdial Workshop on on the Semantics and Pragmatics of Dialogues.</booktitle>
<contexts>
<context position="2921" citStr="Janarthanam and Lemon, 2008" startWordPosition="454" endWordPosition="457"> (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a), (Janarthanam and Lemon, 2008)). This approach is less dataintensive, but requires nontrivial work by domain experts. What is more, as the number of probabilities increases, it is hard even for the experts to set the probabilities. Since both handcrafting and training user action probabilities have their own pros and cons, it is an interesting research question to investigate which approach is better for a certain task given the amount of data that is available. In this study, we investigate a manual and a trained approach in setting up user action probabilities, applied to building the same probabilistic simulation model.</context>
<context position="6515" citStr="Janarthanam and Lemon, 2008" startWordPosition="1042" endWordPosition="1046">). We represent these models as: P(user action|feature1,...,featuren) (1) The number of probabilities involved in this model is: (# of feature values). (2) Some studies handcraft these probabilities. For example, (Schatzmann et al., 2007a) condition the 2The number of user action probabilities and the simulated user behaviors will impact the design choice. user actions on user’s goals and the agenda to reach those goals. They manually author the probabilities in the user’s agenda update model and the goal update model, and then calculate the user action probabilities based on the two models. (Janarthanam and Lemon, 2008) handcraft 15 probabilities in simulated users’ initial profiles and then author rules to update these probabilities during the dialogs. Other studies use a human user corpus as the training corpus to learn user action probabilities in user simulations. Since the human user corpus often does not include all possible actions that users may take during interactions with the dialog system, different strategies are used to account for user actions that do not appear in the training corpus but may be present when testing the user simulations. For example, (Schatzmann et al., 2007b) introduce a summ</context>
<context position="18016" citStr="Janarthanam and Lemon, 2008" startWordPosition="2914" endWordPosition="2917">bset of the measures which describe high-level dialog features that are applicable to our data. The measures we use include the number of student turns (Sturn), the number of tutor turns (Tturn), the number of words per student turn (Swordrate), the number of words per tutor turn (Twordrate), the ratio of system/user words per dialog (WordRatio), 891 and the percentage of correct answers (cRate). 4.2 Measures on Dialog Strategy Learning In this section, we introduce two measures to compare the simulations based on their performance on a dialog strategy learning task. In recent studies (e.g., (Janarthanam and Lemon, 2008)), user simulations are built to generate a large corpus to build MDPs in using Reinforcement Learning (RL) to learn new dialog strategies. When building an MDP from a training corpus9, we compute the transition probabilities P(st+1|st, a) (the probability of getting from state st to the next state st+1 after taking action a), and the reward of this transition R(st, a, st+1). Then, the expected cumulative value (V-value) of a state s can be calculated using this recursive function: V (s) � � P(st+1st, a)[R(st, a, st+1) + &apos;yV (st+1)] st+1 (4) -y is a discount factor which ranges between 0 and 1</context>
</contexts>
<marker>Janarthanam, Lemon, 2008</marker>
<rawString>S. Janarthanam and O. Lemon. 2008. User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems. In Proc. of the 12th SEMdial Workshop on on the Semantics and Pragmatics of Dialogues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>X Liu</author>
</authors>
<title>Dialogue Policy Learning for combinations of Noise and User Simulation: transfer results.</title>
<date>2007</date>
<booktitle>In Proc. of 8th SIGdial.</booktitle>
<contexts>
<context position="1070" citStr="Lemon and Liu, 2007" startWordPosition="165" endWordPosition="168"> is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks. 1 Introduction User simulations are widely used in spoken dialog system development. Recent studies use user simulations to generate training corpora to learn dialog strategies automatically ((Williams and Young, 2007), (Lemon and Liu, 2007)), or to evaluate dialog system performance (L´opez-C´ozar et al., 2003). Most studies show that using user simulations significantly improves dialog system performance as well as speeds up system development. Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives. Some studies look into the impact of training data on user simulations. For example, (Georgila et al., 2008) observe differences between simulated users trained from human users of different age groups. Other studies explore different simulatio</context>
</contexts>
<marker>Lemon, Liu, 2007</marker>
<rawString>O. Lemon and X. Liu. 2007. Dialogue Policy Learning for combinations of Noise and User Simulation: transfer results. In Proc. of 8th SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>S Silliman</author>
</authors>
<title>ITSPOKE: An Intelligent Tutoring Spoken Dialogue System. In</title>
<date>2004</date>
<booktitle>Companion Proc. of the Human Language Technology: NAACL.</booktitle>
<contexts>
<context position="8108" citStr="Litman and Silliman, 2004" startWordPosition="1306" endWordPosition="1309">ers (70 dialogs). They first cluster dialog contexts based on selected features and then build conditional probability models for each cluster. In our study, we build a conditional probability model which will be described in detail in Section 3.2.1. There are 40 probabilities to set up in this model3. We will explain different approaches to assign these probabilities later in Section 3.2.2. 3 System and User Simulations In this section, we describe the dialog system, the human user corpus we collected with the system, and the user simulation we used. 3.1 System and Corpus The ITSPOKE system (Litman and Silliman, 2004) is an Intelligent Tutoring System which teaches Newtonian physics. It is a speechenhanced version of the Why2-Atlas tutoring system (Vanlehn et al., 2002). During the interaction with students, the system initiates a spoken tutoring dialog to correct misconceptions and to 3There are 2 possible actions in our model, 20 possible values for the first feature qCluster and 2 possible values for the second feature prevCorrectness as described later in Section 3.2.1. Using Equation 2, 40=(2-1)*20*2. (# of possible actions-1) * n 11 k=1 889 SYSTEM1: Do you recall what Newton’s third law says? [3rdLaw</context>
</contexts>
<marker>Litman, Silliman, 2004</marker>
<rawString>D. Litman and S. Silliman. 2004. ITSPOKE: An Intelligent Tutoring Spoken Dialogue System. In Companion Proc. of the Human Language Technology: NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L´opez-C´ozar</author>
<author>A De la Torre</author>
<author>J C Segura</author>
<author>A J Rubio</author>
</authors>
<title>Assessment of dialogue systems by means of a new simulation technique.</title>
<date>2003</date>
<journal>Speech Communication</journal>
<volume>40</volume>
<pages>387--407</pages>
<marker>L´opez-C´ozar, Torre, Segura, Rubio, 2003</marker>
<rawString>R. L´opez-C´ozar, A. De la Torre, J. C. Segura and A. J. Rubio. 2003. Assessment of dialogue systems by means of a new simulation technique. Speech Communication (40): 387-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech-06 Workshop on ”Dialogue on Dialogues - Multidisciplinary Evaluation of Advanced Speech-based Interacive Systems”.</booktitle>
<contexts>
<context position="19955" citStr="Paek, 2006" startWordPosition="3244" endWordPosition="3245">ategies, which show the expectation of the rewards we can obtain by applying the learned strategies. The MDP learning task in our study is to maximize student certainty during tutoring dialogs. The dialog states are characterized using the correctness of the current student answer and the student correctness rate so far. We represent the correctness rate as a binary feature: lc if it is below the training corpus average and hc if it is above the average. The end of dialog reward is assigned to be +100 if the dialog has a percent certainty higher 9In this paper, we use off-line model-based RL (Paek, 2006) rather than learning an optimal strategy online during system-user interactions. than the median from the training corpus and -100 otherwise. The action choice of the tutoring system is to give a strong (s) or weak (w) feedback. A strong feedback clearly indicates the correctness of the current student answer while the weak feedback does not. For example, the second system turn in Table 1 contains a weak feedback. If the system says “Your answer is incorrect” at the beginning of this turn, that would be a strong feedback. In order to simulate student certainty, we simply output the student ce</context>
</contexts>
<marker>Paek, 2006</marker>
<rawString>T. Paek. 2006. Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment. In Proc. of Interspeech-06 Workshop on ”Dialogue on Dialogues - Multidisciplinary Evaluation of Advanced Speech-based Interacive Systems”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Qi</author>
<author>H Wu</author>
<author>W Fan</author>
</authors>
<title>Evaluating web-based question answering systems.</title>
<date>2002</date>
<booktitle>In Proc. of LREC</booktitle>
<contexts>
<context position="23516" citStr="Radev et al., 2002" startWordPosition="3839" endWordPosition="3842">obabilistic model, the model will take an action stochastically after the same tutor turn. In other words, we need to take into account the probability for the simulation to predict the right human user action. If the simulation outputs the right action with a small probability, it is less likely that this simulation can correctly predict human user behaviors when generating a large dialog corpus. We consider a simulated action associated with a higher probability to be ranked higher than an action with a lower probability. Then, we use the reciprocal ranking from information retrieval tasks (Radev et al., 2002) to assess the simulation performance10. Mean Reciprocal Ranking is defined as: 1(6) ranki In Equation 6, A stands for the total number of human user actions, ranki stands for the ranking of the simulated action which matches the i-th human user action. Table 2 shows an example of comparing simulated user actions with human user actions in the sample dialog in Table 1. In the first turn Student1, a simulation model has a 60% chance to output an incorrect answer and a 40% chance to output a correct answer while it actually outputs an incorrect answer. In this case, we consider the simulation ra</context>
</contexts>
<marker>Radev, Qi, Wu, Fan, 2002</marker>
<rawString>D. Radev, H. Qi, H. Wu, and W. Fan. 2002. Evaluating web-based question answering systems. In Proc. of LREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Cluster-based User Simulations for Learning Dialogue Strategies.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech</booktitle>
<contexts>
<context position="7374" citStr="Rieser and Lemon, 2006" startWordPosition="1185" endWordPosition="1188">user simulations. Since the human user corpus often does not include all possible actions that users may take during interactions with the dialog system, different strategies are used to account for user actions that do not appear in the training corpus but may be present when testing the user simulations. For example, (Schatzmann et al., 2007b) introduce a summary space approach to map the actual dialog context space into a more tractable summary space. Then, they use forward and backward learning algorithms to learn the probabilities from a corpus generated by 40 human users (160 dialogs). (Rieser and Lemon, 2006) use a two step approach in computing the probabilities from a corpus consisting of dialogs from 24 human users (70 dialogs). They first cluster dialog contexts based on selected features and then build conditional probability models for each cluster. In our study, we build a conditional probability model which will be described in detail in Section 3.2.1. There are 40 probabilities to set up in this model3. We will explain different approaches to assign these probabilities later in Section 3.2.2. 3 System and User Simulations In this section, we describe the dialog system, the human user corp</context>
<context position="33569" citStr="Rieser and Lemon, 2006" startWordPosition="5525" endWordPosition="5528">. In the future, we will conduct follow up studies to confirm our current findings since there are several factors that can impact our results. First of all, our current system mainly distinguishes the student answers as correct and incorrect. We are currently looking into dividing the incorrect student answers into more categories (such as partially correct answers, vague answers, or overspecific answers) which will increase the number of simulated user actions. Also, although the size of the human corpus which we build the trained user simulations from is comparable to other studies (e.g., (Rieser and Lemon, 2006), (Schatzmann et al., 2007b)), using a larger human corpus may improve the performance of the trained simulations. We are in the process of collecting another corpus which will consist of 60 human users (300 dialogs). We plan to re-train a simulation when this new corpus is available. Also, we would be able to train more complex models (e.g., a simulation model which takes into account a longer dialog history) with the extra data. Finally, although we add some noise into the current manual simulation designed by our domain expert to account for variations of expert knowledge, we would like to </context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>V. Rieser and O. Lemon. 2006. Cluster-based User Simulations for Learning Dialogue Strategies. In Proc. of Interspeech 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Georgila</author>
<author>S Young</author>
</authors>
<title>Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc. of 6th SIGDial.</booktitle>
<contexts>
<context position="17369" citStr="Schatzmann et al., 2005" startWordPosition="2806" endWordPosition="2809">ora are often used as training corpora for using MDPs to learn new dialog strategies, in Section 4.2 we estimate how different the learned dialog strategies would be when trained from different simulated corpora. Another way to use user simulation is to test dialog systems. Therefore, in Section 4.3, we compare the user actions predicted by the various simulation models with actual human user actions. 4.1 Measures on Corpus Level Dialog Behaviors We compare the dialog corpora generated by user simulations to our human user corpus using a comprehensive set of corpus level measures proposed by (Schatzmann et al., 2005). Here, we use a subset of the measures which describe high-level dialog features that are applicable to our data. The measures we use include the number of student turns (Sturn), the number of tutor turns (Tturn), the number of words per student turn (Swordrate), the number of words per tutor turn (Twordrate), the ratio of system/user words per dialog (WordRatio), 891 and the percentage of correct answers (cRate). 4.2 Measures on Dialog Strategy Learning In this section, we introduce two measures to compare the simulations based on their performance on a dialog strategy learning task. In rece</context>
</contexts>
<marker>Schatzmann, Georgila, Young, 2005</marker>
<rawString>J. Schatzmann, K. Georgila, and S. Young. 2005. Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems. In Proc. of 6th SIGDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M Stuttle</author>
<author>S Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies.</title>
<date>2006</date>
<journal>Knowledge Engineering Review</journal>
<volume>21</volume>
<issue>2</issue>
<pages>97--126</pages>
<contexts>
<context position="1792" citStr="Schatzmann et al., 2006" startWordPosition="276" endWordPosition="279">sing user simulations significantly improves dialog system performance as well as speeds up system development. Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives. Some studies look into the impact of training data on user simulations. For example, (Georgila et al., 2008) observe differences between simulated users trained from human users of different age groups. Other studies explore different simulation models, i.e. the mechanism of deciding the next user actions given the current dialog context. (Schatzmann et al., 2006) give a thorough review of different types of simulation models. Since Diane Litman University of Pittsburgh Pittsburgh PA, 15260, USA litman@cs.pitt.edu most of these current user simulation techniques use probabilistic models to generate user actions, how to set up the probabilities in the simulations is another important problem to solve. One general approach to set up user action probabilities is to learn the probabilities from a collected human user dialog corpus ((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predic</context>
<context position="5888" citStr="Schatzmann et al., 2006" startWordPosition="944" endWordPosition="947">nough data for training simulation models. However, once we have a dialog system, it is useful to collect human user data in order to train a new user simulation model since the trained simulations perform better than the handcrafted user simulations on more tasks. Since how to decide whether enough data is available for simulation training is another research question to answer, we will further discuss the impact of our results later in Section 6. 2 Related Work Most current simulation models are probabilistic models in which the models simulate user actions based on dialog context features (Schatzmann et al., 2006). We represent these models as: P(user action|feature1,...,featuren) (1) The number of probabilities involved in this model is: (# of feature values). (2) Some studies handcraft these probabilities. For example, (Schatzmann et al., 2007a) condition the 2The number of user action probabilities and the simulated user behaviors will impact the design choice. user actions on user’s goals and the agenda to reach those goals. They manually author the probabilities in the user’s agenda update model and the goal update model, and then calculate the user action probabilities based on the two models. (J</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review 21(2): 97-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proc. of HLT/NAACL</booktitle>
<contexts>
<context position="2290" citStr="Schatzmann et al., 2007" startWordPosition="353" endWordPosition="356">tion models, i.e. the mechanism of deciding the next user actions given the current dialog context. (Schatzmann et al., 2006) give a thorough review of different types of simulation models. Since Diane Litman University of Pittsburgh Pittsburgh PA, 15260, USA litman@cs.pitt.edu most of these current user simulation techniques use probabilistic models to generate user actions, how to set up the probabilities in the simulations is another important problem to solve. One general approach to set up user action probabilities is to learn the probabilities from a collected human user dialog corpus ((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a)</context>
<context position="6124" citStr="Schatzmann et al., 2007" startWordPosition="977" endWordPosition="980">r simulations on more tasks. Since how to decide whether enough data is available for simulation training is another research question to answer, we will further discuss the impact of our results later in Section 6. 2 Related Work Most current simulation models are probabilistic models in which the models simulate user actions based on dialog context features (Schatzmann et al., 2006). We represent these models as: P(user action|feature1,...,featuren) (1) The number of probabilities involved in this model is: (# of feature values). (2) Some studies handcraft these probabilities. For example, (Schatzmann et al., 2007a) condition the 2The number of user action probabilities and the simulated user behaviors will impact the design choice. user actions on user’s goals and the agenda to reach those goals. They manually author the probabilities in the user’s agenda update model and the goal update model, and then calculate the user action probabilities based on the two models. (Janarthanam and Lemon, 2008) handcraft 15 probabilities in simulated users’ initial profiles and then author rules to update these probabilities during the dialogs. Other studies use a human user corpus as the training corpus to learn us</context>
<context position="31877" citStr="Schatzmann et al., 2007" startWordPosition="5239" endWordPosition="5243">han the handcrafted simulations, which again perform better than the model trained from half of the human user corpus. Nevertheless, no significant difference is observed between the two handcrafted simulations. Our study takes a first step in comparing the choices of handcrafting versus training user simulations when only limited or even no training data is available, e.g., when constructing a new dialog system. As shown for our task setting, both types of user simulations can be used in generating training data for learning new dialog strategies. However, we observe (as in a prior study by (Schatzmann et al., 2007b)) that the simulation trained from more user data has a better chance to outperform the simulation trained from less training data. We also observe that a handcrafted user simulation with expert knowledge can reach the performance of the better trained simulation. However, a certain level of expert knowledge is needed in handcrafting user simulations since a random simulation does not perform well in any tasks. Therefore, our results suggest that if an expert is available for designing a user simulation when not enough user data is collected, it may be better to handcraft the user simulation</context>
<context position="33595" citStr="Schatzmann et al., 2007" startWordPosition="5529" endWordPosition="5532">onduct follow up studies to confirm our current findings since there are several factors that can impact our results. First of all, our current system mainly distinguishes the student answers as correct and incorrect. We are currently looking into dividing the incorrect student answers into more categories (such as partially correct answers, vague answers, or overspecific answers) which will increase the number of simulated user actions. Also, although the size of the human corpus which we build the trained user simulations from is comparable to other studies (e.g., (Rieser and Lemon, 2006), (Schatzmann et al., 2007b)), using a larger human corpus may improve the performance of the trained simulations. We are in the process of collecting another corpus which will consist of 60 human users (300 dialogs). We plan to re-train a simulation when this new corpus is available. Also, we would be able to train more complex models (e.g., a simulation model which takes into account a longer dialog history) with the extra data. Finally, although we add some noise into the current manual simulation designed by our domain expert to account for variations of expert knowledge, we would like to recruit another human expe</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. 2007a. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc. of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Statistical User Simulation with a Hidden Agenda.</title>
<date>2007</date>
<booktitle>In Proc. of 8th SIGdial.</booktitle>
<contexts>
<context position="2290" citStr="Schatzmann et al., 2007" startWordPosition="353" endWordPosition="356">tion models, i.e. the mechanism of deciding the next user actions given the current dialog context. (Schatzmann et al., 2006) give a thorough review of different types of simulation models. Since Diane Litman University of Pittsburgh Pittsburgh PA, 15260, USA litman@cs.pitt.edu most of these current user simulation techniques use probabilistic models to generate user actions, how to set up the probabilities in the simulations is another important problem to solve. One general approach to set up user action probabilities is to learn the probabilities from a collected human user dialog corpus ((Schatzmann et al., 2007b), (Georgila et al., 2008)). While this approach takes advantage of observed user behaviors in predicting future user behaviors, it suffers from the problem of learning probabilities from one group of users while potentially using them with another group of users. The accuracy of the learned probabilities becomes more questionable when the collected human corpus is small. However, this is a common problem in building new dialog systems, when often no data1 or only a small amount of data is available. An alternative approach is to handcraft user action probabilities ((Schatzmann et al., 2007a)</context>
<context position="6124" citStr="Schatzmann et al., 2007" startWordPosition="977" endWordPosition="980">r simulations on more tasks. Since how to decide whether enough data is available for simulation training is another research question to answer, we will further discuss the impact of our results later in Section 6. 2 Related Work Most current simulation models are probabilistic models in which the models simulate user actions based on dialog context features (Schatzmann et al., 2006). We represent these models as: P(user action|feature1,...,featuren) (1) The number of probabilities involved in this model is: (# of feature values). (2) Some studies handcraft these probabilities. For example, (Schatzmann et al., 2007a) condition the 2The number of user action probabilities and the simulated user behaviors will impact the design choice. user actions on user’s goals and the agenda to reach those goals. They manually author the probabilities in the user’s agenda update model and the goal update model, and then calculate the user action probabilities based on the two models. (Janarthanam and Lemon, 2008) handcraft 15 probabilities in simulated users’ initial profiles and then author rules to update these probabilities during the dialogs. Other studies use a human user corpus as the training corpus to learn us</context>
<context position="31877" citStr="Schatzmann et al., 2007" startWordPosition="5239" endWordPosition="5243">han the handcrafted simulations, which again perform better than the model trained from half of the human user corpus. Nevertheless, no significant difference is observed between the two handcrafted simulations. Our study takes a first step in comparing the choices of handcrafting versus training user simulations when only limited or even no training data is available, e.g., when constructing a new dialog system. As shown for our task setting, both types of user simulations can be used in generating training data for learning new dialog strategies. However, we observe (as in a prior study by (Schatzmann et al., 2007b)) that the simulation trained from more user data has a better chance to outperform the simulation trained from less training data. We also observe that a handcrafted user simulation with expert knowledge can reach the performance of the better trained simulation. However, a certain level of expert knowledge is needed in handcrafting user simulations since a random simulation does not perform well in any tasks. Therefore, our results suggest that if an expert is available for designing a user simulation when not enough user data is collected, it may be better to handcraft the user simulation</context>
<context position="33595" citStr="Schatzmann et al., 2007" startWordPosition="5529" endWordPosition="5532">onduct follow up studies to confirm our current findings since there are several factors that can impact our results. First of all, our current system mainly distinguishes the student answers as correct and incorrect. We are currently looking into dividing the incorrect student answers into more categories (such as partially correct answers, vague answers, or overspecific answers) which will increase the number of simulated user actions. Also, although the size of the human corpus which we build the trained user simulations from is comparable to other studies (e.g., (Rieser and Lemon, 2006), (Schatzmann et al., 2007b)), using a larger human corpus may improve the performance of the trained simulations. We are in the process of collecting another corpus which will consist of 60 human users (300 dialogs). We plan to re-train a simulation when this new corpus is available. Also, we would be able to train more complex models (e.g., a simulation model which takes into account a longer dialog history) with the extra data. Finally, although we add some noise into the current manual simulation designed by our domain expert to account for variations of expert knowledge, we would like to recruit another human expe</context>
</contexts>
<marker>Schatzmann, Thomson, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson and S. Young. 2007b. Statistical User Simulation with a Hidden Agenda. In Proc. of 8th SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>D Litman</author>
</authors>
<title>A Reinforcement Learning Approach to Evaluating State Representations in Spoken Dialogue Systems. Speech Communication (Special Issue on Evaluating new methods and models for advanced speech-based interactive systems),</title>
<date>2008</date>
<pages>50--8</pages>
<contexts>
<context position="26205" citStr="Tetreault and Litman, 2008" startWordPosition="4290" endWordPosition="4293">show the standard deviations of the mean values. As we can see from the figure, the Random Model performs differently from the human and all the other simulated models. There is no difference in dialog behaviors among the human corpus, the trained and the manual simulated corpora. In sum, both the Trained KC Models and the Manual KC Models can generate human-like high-level dialog behaviors while the Random Model cannot. 5.2 Comparing on Dialog Strategy Learning Task Next, we compare the difference in dialog strategy learning when training on the simulated corpora using similar approaches in (Tetreault and Litman, 2008). Table 3 shows the transition probabilities starting from the state (c, lc). For example, the first cell shows in the Tmore corpus, the probability of starting from state (c, lc), getting a strong feedback, and transitioning into the same state is 24.82%. We calculate the same table for the other three states (c, hc), (ic, lc), and (ic, hc). Using paired-sample t-tests with bonferroni corrections, the only significant differences are observed between the random simulated corpus and each of the other simulated corpora. 1 MRR = A A E ��� 893 i-th Turn human Simulation Model Simulation Output Co</context>
</contexts>
<marker>Tetreault, Litman, 2008</marker>
<rawString>J. Tetreault and D. Litman. 2008. A Reinforcement Learning Approach to Evaluating State Representations in Spoken Dialogue Systems. Speech Communication (Special Issue on Evaluating new methods and models for advanced speech-based interactive systems), 50(8-9): 683-696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>P Jordan</author>
<author>C Ros´e</author>
<author>D Bhembe</author>
<author>M B¨ottner</author>
<author>A Gaydos</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>M Ringenberg</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>R Wilson</author>
</authors>
<title>The architecture of Why2-Atlas: A coach for qualitative physics essay writing.</title>
<date>2002</date>
<booktitle>In Proc. Intelligent Tutoring Systems Conference..</booktitle>
<marker>VanLehn, Jordan, Ros´e, Bhembe, B¨ottner, Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque, Siler, Srivastava, Wilson, 2002</marker>
<rawString>K. VanLehn, P. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. Intelligent Tutoring Systems Conference..</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2007</date>
<journal>Computer Speech and Language</journal>
<volume>21</volume>
<issue>2</issue>
<pages>231--422</pages>
<contexts>
<context position="1047" citStr="Williams and Young, 2007" startWordPosition="161" endWordPosition="164"> One generally used approach is to estimate these probabilities from human user data. However, when building a new dialog system, usually no data or only a small amount of data is available. In this study, we compare estimating user probabilities from a small user data set versus handcrafting the probabilities. We discuss the pros and cons of both solutions for different dialog system development tasks. 1 Introduction User simulations are widely used in spoken dialog system development. Recent studies use user simulations to generate training corpora to learn dialog strategies automatically ((Williams and Young, 2007), (Lemon and Liu, 2007)), or to evaluate dialog system performance (L´opez-C´ozar et al., 2003). Most studies show that using user simulations significantly improves dialog system performance as well as speeds up system development. Since user simulation is such a useful tool, dialog system researchers have studied how to build user simulations from a variety of perspectives. Some studies look into the impact of training data on user simulations. For example, (Georgila et al., 2008) observe differences between simulated users trained from human users of different age groups. Other studies expl</context>
<context position="19324" citStr="Williams and Young, 2007" startWordPosition="3130" endWordPosition="3133"> all simulated corpora. The transition probabilities are only determined by the states and user actions presented by the training corpus, regardless of the rest of the MDP configuration. Since the MDP configuration has a big impact on the learned strategies, we want to first factor this impact out and estimate the differences in learned strategies that are brought in by the training corpora alone. As a second evaluation measure, we apply reinforcement learning to the MDP representing each simulated corpus separately to learn dialog strategies. We compare the Expected Cumulative Rewards (ECRs)(Williams and Young, 2007) of these dialog strategies, which show the expectation of the rewards we can obtain by applying the learned strategies. The MDP learning task in our study is to maximize student certainty during tutoring dialogs. The dialog states are characterized using the correctness of the current student answer and the student correctness rate so far. We represent the correctness rate as a binary feature: lc if it is below the training corpus average and hc if it is above the average. The end of dialog reward is assigned to be +100 if the dialog has a percent certainty higher 9In this paper, we use off-l</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language 21(2): 231-422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>