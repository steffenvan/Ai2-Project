<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001996">
<title confidence="0.9972885">
Mapping Lexical Entries in a Verbs Database
to WordNet Senses
</title>
<author confidence="0.995131">
Rebecca Green and Lisa Pearl and Bonnie J. Dorr and Philip Resnik
</author>
<affiliation confidence="0.998179">
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.90758">
College Park, MD 20742 USA
</address>
<email confidence="0.998978">
rgreen,llsp,bonnie,resnik @umiacs.umd.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997700142857143">
This paper describes automatic tech-
niques for mapping 9611 entries in a
database of English verbs to Word-
Net senses. The verbs were initially
grouped into 491 classes based on
syntactic features. Mapping these
verbs into WordNet senses provides a
resource that supports disambiguation
in multilingual applications such as
machine translation and cross-language
information retrieval. Our techniques
make use of (1) a training set of 1791
disambiguated entries, representing
1442 verb entries from 167 classes;
</bodyText>
<listItem confidence="0.896057076923077">
(2) word sense probabilities, from
frequency counts in a tagged corpus;
(3) semantic similarity of WordNet
senses for verbs within the same class;
(4) probabilistic correlations between
WordNet data and attributes of the
verb classes. The best results achieved
72% precision and 58% recall, versus a
lower bound of 62% precision and 38%
recall for assigning the most frequently
occurring WordNet sense, and an upper
bound of 87% precision and 75% recall
for human judgment.
</listItem>
<sectionHeader confidence="0.997225" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985541666667">
Our goal is to map entries in a lexical database
of 4076 English verbs automatically to Word-
Net senses (Miller and Fellbaum, 1991), (Fell-
baum, 1998) to support such applications as ma-
chine translation and cross-language information
retrieval. For example, the verb drop is multi-
ply ambiguous, with many potential translations
in Spanish: bajar, caerse, dejar caer, derribar,
disminuir,echar, hundir, soltar, etc. The database
specifies a set of interpretations for drop, depend-
ing on its context in the source-language (SL). In-
clusion of WordNet senses in the database enables
the selection of an appropriate verb in the target
language (TL). Final selection is based on a fre-
quency count of WordNet senses across all classes
to which the verb belongs—e.g., disminuir is se-
lected when the WordNet sense corresponds to the
meaning of drop in Prices dropped.
Our task differs from standard word sense dis-
ambiguation (WSD) in several ways. First, the
words to be disambiguated are entries in a lexical
database, not tokens in a text corpus. Second, we
take an “all-words” rather than a “lexical-sample”
approach (Kilgarriff and Rosenzweig, 2000): All
words in the lexical database “text” are disam-
biguated, not just a small number for which de-
tailed knowledge is available. Third, we replace
the contextual data typically used for WSD with
information about verb senses encoded in terms
of thematic grids and lexical-semantic representa-
tions from (Olsen et al., 1997). Fourth, whereas a
single word sense for each token in a text corpus
is often assumed, the absence of sentential context
leads to a situation where several WordNet senses
may be equally appropriate for a database entry.
Indeed, as distinctions between WordNet senses
can be fine-grained (Palmer, 2000), it may be un-
clear, even in context, which sense is meant.
The verb database contains mostly syntactic in-
formation about its entries, much of which ap-
plies at the class level within the database. Word-
Net, on the other hand, is a significant source for
information about semantic relationships, much
of which applies at the “synset” level (“synsets”
are WordNet’s groupings of synonymous word
senses). Mapping entries in the database to their
corresponding WordNet senses greatly extends
the semantic potential of the database.
</bodyText>
<sectionHeader confidence="0.986317" genericHeader="method">
2 Lexical Resources
</sectionHeader>
<bodyText confidence="0.999716382352941">
We use an existing classification of 4076 English
verbs, based initially on English Verbs Classes
and Alternations (Levin, 1993) and extended
through the splitting of some classes into sub-
classes and the addition of new classes. The re-
sulting 491 classes (e.g., “Roll Verbs, Group I”,
which includes drift, drop, glide, roll, swing) are
referred to here as Levin+ classes. As verbs may
be assigned to multiple Levin+ classes, the actual
number of entries in the database is larger, 9611.
Following the model of (Dorr and Olsen, 1997),
each Levin+ class is associated with a thematic
grid (henceforth abbreviated -grid), which sum-
marizes a verb’s syntactic behavior by specify-
ing its predicate argument structure. For exam-
ple, the Levin+ class “Roll Verbs, Group I” is as-
sociated with the -grid [th goal], in which a
theme and a goal are used (e.g., The ball dropped
to the ground).1 Each -grid specification corre-
sponds to a Grid class. There are 48 Grid classes,
with a one-to-many relationship between Grid and
Levin+ classes.
WordNet, the lexical resource to which we are
mapping entries from the lexical database, groups
synonymous word senses into “synsets” and struc-
tures the synsets into part-of-speech hierarchies.
Our mapping operation uses several other data el-
ements pertaining to WordNet: semantic relation-
ships between synsets, frequency data, and syn-
tactic information.
Seven semantic relationship types exist be-
tween synsets, including, for example, antonymy,
hyperonymy, and entailment. Synsets are often
related to a half dozen or more other synsets; they
</bodyText>
<footnote confidence="0.98791575">
1There is also a Levin+ class “Roll Verbs, Group II”
which is associated with the -grid [th particle(down)], in
which a theme and a particle ‘down’ are used (e.g., The ball
dropped down).
</footnote>
<bodyText confidence="0.999566775">
may be related to multiple synsets through a single
relationship or may be related to a single synset
through multiple relationship types.
Our frequency data for WordNet senses is de-
rived from SEMCOR—a semantic concordance in-
corporating tagging of the Brown corpus with
WordNet senses.2
Syntactic patterns (“frames”) are associated
with each synset, e.g., Somebody s something;
Something s; Somebody s somebody into
V-ing something. There are 35 such verb frames
in WordNet and a synset may have only one or as
many as a half dozen or so frames assigned to it.
Our mapping of verbs in Levin+ classes to
WordNet senses relies in part on the relation be-
tween thematic roles in Levin+ and verb frames in
WordNet. Both reflect how many and what kinds
of arguments a verb may take. However, con-
structing a direct mapping between -grids and
WordNet frames is not possible, as the underly-
ing classifications differ in significant ways. The
correlations between the two sets of data are better
viewed probabilistically.
Table 1 illustrates the relation between Levin+
classes and WordNet for the verb drop. In our
multilingual applications (e.g., lexical selection in
machine translation), the Grid information pro-
vides a context-based means of associating a verb
with a Levin+ class according to its usage in the
SL sentence. The WordNet sense possibilities are
thus pared down during SL analysis, but not suffi-
ciently for the final selection of a TL verb. For ex-
ample, Levin+ class 9.4 has three possible Word-
Net senses for drop. However, the WordNet sense
8 is not associated with any of the other classes;
thus, it is considered to have a higher “information
content” than the others. The upshot is that the
lexical-selection routine prefers dejar caer over
other translations such as derribar and bajar.3
The other classes are similarly associated with ap-
</bodyText>
<footnote confidence="0.995583545454545">
2For further information see the WordNet manuals, sec-
tion 7, SEMCOR at http://www.cogsci.princeton.edu.
3This lexical-selection approach is an adaptation of the
notion of reduction in entropy, measured by information
gain (Mitchell, 1997). Using information content to quan-
tify the “value” of a node in the WordNet hierarchy has
also been used for measuring semantic similarity in a tax-
onomy (Resnik, 1999b). More recently, context-based mod-
els of disambiguation have been shown to represent signif-
icant improvements over the baseline (Bangalore and Ram-
bow, 2000), (Ratnaparkhi, 2000).
</footnote>
<table confidence="0.9417434375">
Levin+ Grid/Example WN Sense Spanish Verb(s)
9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar
Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse
Put 8. drop set down, put down 8. dejar caer, echar, soltar
45.6 [th] 1. move, displace 1. derribar, echar
Calibratable Prices dropped 3. decline, go down, wane 3. disminuir
Change of
State
47.7 [th src goal] 2. descend, fall, go down 2. bajar, caerse
Meander The river dropped from 4. sink, drop, drop down 4. hundir, caer
the lake to the sea
51.3.1 [th goal] 2. descend, fall, go down 2. bajar, caerse
Roll I The ball dropped to the
ground
51.3.1 [th particle(down)] 2. descend, fall, go down 2. bajar, caerse
Roll II The ball dropped down
</table>
<tableCaption confidence="0.999497">
Table 1: Relation Between Levin+ and WN Senses for ‘drop’
</tableCaption>
<bodyText confidence="0.96863775">
,
propriate TL verbs during lexical selection: dis-
minuir (class 45.6), hundir (class 47.7), and bajar
(class 51.3.1).4
</bodyText>
<sectionHeader confidence="0.9621" genericHeader="method">
3 Training Data
</sectionHeader>
<bodyText confidence="0.997077285714286">
We began with the lexical database of (Dorr and
Jones, 1996), which contains a significant number
of WordNet-tagged verb entries. Some of the as-
signments were in doubt, since class splitting had
occurred subsequent to those assignments, with
all old WordNet senses carried over to new sub-
classes. New classes had also been added since
the manual tagging. It was determined that the
tagging for only 1791 entries—including 1442
verbs in 167 classes—could be considered stable;
for these entries, 2756 assignments of WordNet
senses had been made. Data for these entries,
taken from both WordNet and the verb lexicon,
constitute the training data for this study.
The following probabilities were generated
from the training data:
,
where is a relation (of relationship type ,
e.g., synonymy) between two synsets, and ,
where is mapped to by a verb in Grid class G
and is mapped to by a verb in Grid class G .
</bodyText>
<footnote confidence="0.940263666666667">
4The full set of Spanish translations is selected from
WordNet associations developed in the EuroWordNet effort
(Dorr et al., 1997).
</footnote>
<bodyText confidence="0.993750323943662">
This is the probability that if one synset is related
to another through a particular relationship type,
then a verb mapped to the first synset will belong
to the same Grid class as a verb mapped to the
second synset. Computed values generally range
between .3 and .35.
where is as above, except that s is mapped to
by a verb in Levin+ class L+ and s is mapped
to by a verb in Levin+ class L+ . This is the
probability that if one synset is related to another
through a particular relationship type, then a
verb mapped to the first synset will belong to
the same Levin+ class as a verb mapped to the
second synset. Computed values generally range
between .25 and .3.
,
where is the occurrence of the entire -grid
for verb entry and cf is the occurrence of the
entire frame sequence for a WordNet sense to
which verb entry is mapped. This is the prob-
ability that a verb in a Levin+ class is mapped to
a WordNet verb sense with some specific combi-
nation of frames. Values average only .11, but in
some cases the probability is 1.0.
,
where is the occurrence of the single -grid
component for verb entry and cf is the occur-
rence of the single frame for a WordNet sense to
which verb entry is mapped. This is the proba-
bility that a verb in a Levin+ class with a partic-
ular -grid component (possibly among others) is
mapped to a WordNet verb sense assigned a spe-
cific frame (possibly among others). Values aver-
age .20, but in some cases the probability is 1.0.
where
☆ ★,
is an occurrence of tag (for a particular synset)
in SEMCOR and is an occurrence of any of a set
of tags for verb in SEMCOR, with being one
of the senses possible for verb . This probability
is the prior probability of specific WordNet verb
senses. Values average .11, but in some cases the
probability is 1.0.
In addition to the foregoing data elements,
based on the training set, we also made use of
a semantic similarity measure, which reflects the
confidence with which a verb, given the total set
of verbs assigned to its Levin+ class, is mapped
to a specific WordNet sense. This represents an
implementation of a class disambiguation algo-
rithm (Resnik, 1999a), modified to run against the
WordNet verb hierarchy.5
We also made a powerful “same-synset as-
sumption”: If (1) two verbs are assigned to the
same Levin+ class, (2) one of the verbs has
been mapped to a specific WordNet sense , and
(3) the other verb has a WordNet sense syn-
onymous with , then should be mapped to .
Since WordNet groups synonymous word senses
into “synsets,” and would correspond to
the same synset. Since Levin+ verbs are mapped
to WordNet senses via their corresponding synset
identifiers, when the set of conditions enumer-
ated above are met, the two verb entries would be
mapped to the same WordNet synset.
As an example, the two verbs tag and mark
have been assigned to the same Levin+ class. In
WordNet, each occurs in five synsets, only one
in which they both occur. If tag has a WordNet
synset assigned to it for the Levin+ class it shares
with mark, and it is the synset that covers senses
</bodyText>
<footnote confidence="0.921876142857143">
5The assumption underlying this measure is that the ap-
propriate word senses for a group of semantically related
words should themselves be semantically related. Given
WordNet’s hierarchical structure, the semantic similarity be-
tween two WordNet senses corresponds to the degree of in-
formativeness of the most specific concept that subsumes
them both.
</footnote>
<bodyText confidence="0.989382666666667">
of both tag and mark, we can safely assume that
that synset is also appropriate for mark, since in
that context, the two verb senses are synonymous.
</bodyText>
<sectionHeader confidence="0.998171" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.989619175824176">
Subsequent to the culling of the training set, sev-
eral processes were undertaken that resulted in
full mapping of entries in the lexical database to
WordNet senses. Much, but not all, of this map-
ping was accomplished manually.
Each entry whose WordNet senses were as-
signed manually was considered by at least two
coders, one coder who was involved in the entire
manual assignment process and the other drawn
from a handful of coders working independently
on different subsets of the verb lexicon. In the
manual tagging, if a WordNet sense was consid-
ered appropriate for a lexical entry by any one of
the coders, it was assigned. Overall, 13452 Word-
Net sense assignments were made. Of these, 51%
were agreed upon by multiple coders. The kappa
coefficient ( ) of intercoder agreement was .47
for a first round of manual tagging and (only) .24
for a second round of more problematic cases.6
While the full tagging of the lexical database
may make the automatic tagging task appear su-
perfluous, the low rate of agreement between
coders and the automatic nature of some of the
tagging suggest there is still room for adjust-
ment of WordNet sense assignments in the verb
database. On the one hand, even the higher of
the kappa coefficients mentioned above is signifi-
cantly lower than the standard suggested for good
reliability ( ) or even the level where ten-
tative conclusions may be drawn (
) (Carletta, 1996), (Krippendorff, 1980). On
the other hand, if the automatic assignments agree
with human coding at levels comparable to the de-
gree of agreement among humans, it may be used
to identify current assignments that need review
6The kappa statistic measures the degree to which pair-
wise agreement of coders on a classification task surpasses
what would be expected by chance; the standard definition
of this coefficient is: ,
where is the actual percentage of agreement and
is the expected percentage of agreement, averaged over all
pairs of assignments. Several adjustments in the computation
of the kappa coefficient were made necessary by the possible
assignment of multiple senses for each verb in a Levin+ class,
since without prior knowledge of how many senses are to be
assigned, there is no basis on which to compute .
and to suggest new assignments for consideration.
In addition, consistency checking is done more
easily by machine than by hand. For example, the
same-synset assumption is more easily enforced
automatically than manually. When this assump-
tion is implemented for the 2756 senses in the
training set, another 967 sense assignments are
generated, only 131 of which were actually as-
signed manually. Similarly, when this premise is
enforced on the entirety of the lexical database
of 13452 assignments, another 5059 sense assign-
ments are generated. If the same-synset assump-
tion is valid and if the senses assigned in the
database are accurate, then the human tagging has
a recall of no more than 73%.
Because a word sense was assigned even if only
one coder judged it to apply, human coding has
been treated as having a precision of 100%. How-
ever, some of the solo judgments are likely to have
been in error. To determine what proportion of
such judgments were in reality precision failures,
a random sample of 50 WordNet senses selected
by only one of the two original coders was in-
vestigated further by a team of three judges. In
this round, judges rated WordNet senses assigned
to verb entries as falling into one of three cate-
gories: definitely correct, definitely incorrect, and
arguable whether correct. As it turned out, if any
one of the judges rated a sense definitely correct,
another judge independently judged it definitely
correct; this accounts for 31 instances. In 13 in-
stances the assignments were judged definitely in-
correct by at least two of the judges. No con-
sensus was reached on the remaining 6 instances.
Extrapolating from this sample to the full set of
solo judgments in the database leads to an estimate
that approximately 1725 (26% of 6636 solo judg-
ments) of those senses are incorrect. This suggests
that the precision of the human coding is approx-
imately 87%.
The upper bound for this task, as set by human
performance, is thus 73% recall and 87% preci-
sion. The lower bound, based on assigning the
WordNet sense with the greatest prior probability,
is 38% recall and 62% precision.
</bodyText>
<sectionHeader confidence="0.97127" genericHeader="method">
5 Mapping Strategies
</sectionHeader>
<bodyText confidence="0.996408272727273">
Recent work (Van Halteren et al., 1998) has
demonstrated improvement in part-of-speech tag-
ging when the outputs of multiple taggers are
combined. When the errors of multiple classi-
fiers are not significantly correlated, the result of
combining votes from a set of individual classi-
fiers often outperforms the best result from any
single classifier. Using a voting strategy seems es-
pecially appropriate here: The measures outlined
in Section 3 average only 41% recall on the train-
ing set, but the senses picked out by their highest
values vary significantly.
The investigations undertaken used both sim-
ple and aggregate voters, combined using var-
ious voting strategies. The simple voters were
the 7 measures previously introduced.7 In addi-
tion, three aggregate voters were generated: (1)
the product of the simple measures (smoothed so
that zero values wouldn’t offset all other mea-
sures); (2) the weighted sum of the simple mea-
sures, with weights representing the percentage of
the training set assignments correctly identified by
the highest score of the simple probabilities; and
(3) the maximum score of the simple measures.
Using these data, two different types of vot-
ing schemes were investigated. The schemes dif-
fer most significantly on the circumstances un-
der which a voter casts its vote for a WordNet
sense, the size of the vote cast by each voter, and
the circumstances under which a WordNet sense
was selected. We will refer to these two schemes
as Majority Voting Scheme and Threshold Voting
Scheme.
</bodyText>
<subsectionHeader confidence="0.995843">
5.1 Majority Voting Scheme
</subsectionHeader>
<bodyText confidence="0.999946222222222">
Although we do not know in advance how many
WordNet senses should be assigned to an entry in
the lexical database, we assume that, in general,
there is at least one. In line with this intuition, one
strategy we investigated was to have both simple
and aggregate measures cast a vote for whichever
sense(s) of a verb in a Levin+ class received the
highest (non-zero) value for that measure. Ten
variations are given here:
</bodyText>
<note confidence="0.663441">
PriorProb: Prior Probability of WordNet
senses
</note>
<footnote confidence="0.996651333333333">
7Only 6 measures (including the semantic similarity mea-
sure) were set out in the earlier section; the measures total 7
because Indv frame probability is used in two different ways.
</footnote>
<note confidence="0.5857245">
SemSim: Semantic Similarity
SimpleProd: Product of all simple measures
</note>
<bodyText confidence="0.998738121951219">
SimpleWtdSum: Weighted sum of all sim-
ple measures
MajSimpleSgl: Majority vote of all (7) sim-
ple voters
MajSimplePair: Majority vote of all (21)
pairs of simple voters8
MajAggr: Majority vote of SimpleProd and
SimpleWtdSum
Maj3Best: Majority vote of SemSim, Sim-
pleProd, and SimpleWtdSum
MajSgl+Aggr: Majority vote of MajSim-
pleSgl and MajAggr
MajPair+Aggr: Majority vote of MajSim-
plePair and MajAggr
Table 2 gives recall and precision measures for
all variations of this voting scheme, both with
and without enforcement of the same-synset as-
sumption. If we use the harmonic mean of recall
and precision as a criterion for comparing results,
the best voting scheme is MajAggr, with 58% re-
call and 72% precision without enforcement of the
same-synset assumption. Note that if the same-
synset assumption is correct, the drop in precision
that accompanies its enforcement mostly reflects
inconsistencies in human judgments in the train-
ing set; the true precision value for MajAggr after
enforcing the same-synset assumption is probably
close to 67%.
Of the simple voters, only PriorProb and Sem-
Sim are individually strong enough to warrant dis-
cussion. Although PriorProb was used to estab-
lish our lower bound, SemSim proves to be the
stronger voter, bested only by MajAggr (the ma-
jority vote of SimpleProd and SimpleWtdSum) in
voting that enforces the same-synset assumption.
Both PriorProb and SemSim provide better results
than the majority vote of all 7 simple voters (Ma-
jSimpleSgl) and the majority vote of all 21 pairs
of simple voters (MajSimplePair). Moreover, the
inclusion of MajSimpleSgl and MajSimplePair in
a majority vote with MajAggr (in MajSgl+Aggr
</bodyText>
<footnote confidence="0.759406">
8A pair cast a vote for a sense if, among all the senses of a
verb, a specific sense had the highest value for both measures.
</footnote>
<table confidence="0.999974583333333">
Variation W/O SS W/ SS
R P R P
PriorProb 38% 62% 45% 46%
SemSim 56% 71% 60% 55%
SimpleProd 51% 74% 57% 55%
SimpleWtdSum 53% 77% 58% 56%
MajSimpleSgl 23% 71% 30% 48%
MajSimplePair 38% 60% 45% 43%
MajAggr 58% 72% 63% 53%
Maj3Best 52% 78% 57% 57%
MajSgl+Aggr 44% 74% 50% 54%
MajPair+Aggr 49% 77% 55% 57%
</table>
<tableCaption confidence="0.822874666666667">
Table 2: Recall (R) and Precision (P) for Majority
Voting Scheme, Before (W/O) and After (W/) En-
forcement of the Same-Synset (SS) Assumption
</tableCaption>
<table confidence="0.999937">
Variation R P
AutoMap+ 61% 54%
AutoMap- 61% 54%
Triples 63% 52%
Combo 53% 44%
Combo&amp;Auto 59% 45%
</table>
<tableCaption confidence="0.877614">
Table 3: Recall (R) and Precision (P) for Thresh-
old Voting Scheme
</tableCaption>
<bodyText confidence="0.997027705882353">
and MapPair+Aggr, respectively) turn in poorer
results than MajAggr alone.
The poor performance of MajSimpleSgl and
MajSimplePair do not point, however, to a gen-
eral failure of the principle that multiple voters
are better than individual voters. SimpleProd, the
product of all simple measures, and SimpleWtd-
Sum, the weighted sum of all simple measures,
provide reasonably strong results, and a majority
vote of the both of them (MajAggr) gives the best
results of all. When they are joined by SemSim in
Maj3Best, they continue to provide good results.
The bottom line is that SemSim makes the most
significant contribution of any single simple voter,
while the product and weighted sums of all simple
voters, in concert with each other, provide the best
results of all with this voting scheme.
</bodyText>
<subsectionHeader confidence="0.997884">
5.2 Threshold Voting Scheme
</subsectionHeader>
<bodyText confidence="0.999824851851852">
The second voting strategy first identified, for
each simple and aggregate measure, the threshold
value at which the product of recall and precision
scores in the training set has the highest value if
that threshold is used to select WordNet senses.
During the voting, if a WordNet sense has a higher
score for a measure than its threshold, the measure
votes for the sense; otherwise, it votes against it.
The weight of the measure’s vote is the precision-
recall product at the threshold. This voting strat-
egy has the advantage of taking into account each
individual attribute’s strength of prediction.
Five variations on this basic voting scheme
were investigated. In each, senses were selected
if their vote total exceeded a variation-specific
threshold. Table 3 summarizes recall and pre-
cision for these variations at their optimal vote
thresholds.
In the AutoMap+ variation, Grid and Levin+
probabilities abstain from voting when their val-
ues are zero (a common occurrence, because
of data sparsity in the training set); the same-
synset assumption is automatically implemented.
AutoMap- differs in that it disregards the Grid
and Levin+ probabilities completely. The Triples
variation places the simple and composite mea-
sures into three groups, the three with the high-
est weights, the three with the lowest weights,
and the middle or remaining three. Voting first
occurs within the group, and the group’s vote is
brought forward with a weight equaling the sum
of the group members’ weights. This variation
also adds to the vote total if the sense was as-
signed in the training data. The Combo variation
is like Triples, but rather than using the weights
and thresholds calculated for the single measures
from the training data, this variation calculates
weights and thresholds for combinations of two,
three, four, five, six, and, seven measures. Finally,
the Combo&amp;Auto variation adds the same-synset
assumption to the previous variation.
Although not evident in Table 3 because of
rounding, AutoMap- has slightly higher values for
both recall and precision than does AutoMap+,
giving it the highest recall-precision product of the
threshold voting schemes. This suggests that the
Grid and Levin+ probabilities could profitably be
dropped from further use.
Of the more exotic voting variations, Triples
voting achieved results nearly as good as the Au-
toMap voting schemes, but the Combo schemes
fell short, indicating that weights and thresholds
are better based on single measures than combi-
nations of measures.
</bodyText>
<sectionHeader confidence="0.99613" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999984333333333">
The voting schemes still leave room for improve-
ment, as the best results (58% recall and 72% pre-
cision, or, optimistically, 63% recall and 67% pre-
cision) fall shy of the upper bound of 73% re-
call and 87% precision for human coding.9 At
the same time, these results are far better than the
lower bound of 38% recall and 62% precision for
the most frequent WordNet sense.
As has been true in many other evaluation stud-
ies, the best results come from combining classi-
fiers (MajAggr): not only does this variation use
a majority voting scheme, but more importantly,
the two voters take into account all of the sim-
ple voters, in different ways. The next-best re-
sults come from Maj3Best, in which the three best
single measures vote. We should note, however,
that the single best measure, the semantic similar-
ity measure from SemSim, lags only slightly be-
hind the two best voting schemes.
This research demonstrates that credible word
sense disambiguation results can be achieved
without recourse to contextual data. Lexical re-
sources enriched with, for example, syntactic in-
formation, in which some portion of the resource
is hand-mapped to another lexical resource may
be rich enough to support such a task. The de-
gree of success achieved here also owes much to
the confluence of WordNet’s hierarchical struc-
ture and SEMCOR tagging, as used in the compu-
tation of the semantic similarity measure, on the
one hand, and the classified structure of the verb
lexicon, which provided the underlying groupings
used in that measure, on the other hand. Even
where one measure yields good results, several
data sources needed to be combined to enable its
success.
</bodyText>
<sectionHeader confidence="0.998322" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8672845">
The authors are supported, in part, by
PFF/PECASE Award IRI-9629108, DOD
</bodyText>
<footnote confidence="0.7484895">
9The criteria for the majority voting schemes preclude
their assigning more than 2 senses to any single database en-
try. Controlled relaxation of these criteria may achieve some-
what better results.
</footnote>
<reference confidence="0.983177">
Contract MDA904-96-C-1250, DARPA/ITO
Contracts N66001-97-C-8540 and N66001-
00-28910, and a National Science Foundation
Graduate Research Fellowship.
</reference>
<sectionHeader confidence="0.799177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998795207317073">
Srinivas Bangalore and Owen Rambow. 2000.
Corpus-Based Lexical Choice in Natural Language
Generation. In Proceedings of the ACL, Hong
Kong.
Olivier Bodenreider and Carol A. Bean. 2001. Re-
lationships among Knowledge Structures: Vocabu-
lary Integration within a Subject Domain. In C.A.
Bean and R. Green, editors, Relationships in the
Organization of Knowledge, pages 81–98. Kluwer,
Dordrecht.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Lingustics, 22(2):249–254, June.
Bonnie J. Dorr and Douglas Jones. 1996. Robust Lex-
ical Acquisition: Word Sense Disambiguation to In-
crease Recall and Precision. Technical report, Uni-
versity of Maryland, College Park, MD.
Bonnie J. Dorr and Mari Broman Olsen. 1997. De-
riving Verbal and Compositional Lexical Aspect
for NLP Applications. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics (ACL-97), pages 151–158,
Madrid, Spain, July 7-12.
Bonnie J. Dorr, M. Antonia Marti, and Irene Castell´on.
1997. Spanish EuroWordNet and LCS-Based In-
terlingual MT. In Proceedings of the Workshop on
Interlinguas in MT, MT Summit, New Mexico State
University Technical Report MCCS-97-314, pages
19–32, San Diego, CA, October.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Eduard Hovy. In press. Comparing Sets of Semantic
Relations in Ontologies. In R. Green, C.A. Bean,
and S. Myaeng, editors, The Semantics of Rela-
tionships: An Interdisciplinary Perspective. Book
manuscript submitted for review.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and Results for English SENSEVAL. Computers
and the Humanities, 34:15–48.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage, Beverly Hills.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
George A. Miller and Christiane Fellbaum. 1991. Se-
mantic Networks of English. In Beth Levin and
Steven Pinker, editors, Lexical and Conceptual Se-
mantics, pages 197–229. Elsevier Science Publish-
ers, B.V., Amsterdam, The Netherlands.
Tom Mitchell. 1997. Machine Learning. McGraw
Hill.
Mari Broman Olsen, Bonnie J. Dorr, and David J.
Clark. 1997. Using WordNet to Posit Hierarchical
Structure in Levin’s Verb Classes. In Proceedings
of the Workshop on Interlinguas in MT, MT Sum-
mit, New Mexico State University Technical Report
MCCS-97-314, pages 99–110, San Diego, CA, Oc-
tober.
Martha Palmer. 2000. Consistent Criteria for
Sense Distinctions. Computers and the Humanities,
34:217–222.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the ANLP-NAACL, Seattle, WA.
Philip Resnik. 1999a. Disambiguating noun group-
ings with respect to wordnet senses. In S. Arm-
strong, K. Church, P. Isabelle, E. Tzoukermann
S. Manzi, and D. Yarowsky, editors, Natural Lan-
guage Processing Using Very Large Corpora, pages
77–98. Kluwer Academic, Dordrecht.
Philip Resnik. 1999b. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
In Journal ofArtificial Intelligence Research, num-
ber 11, pages 95–130.
Hans Van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data-driven wordclass tag-
ging by system combination. In Proceedings of the
36th Annual Meeting of the Associationfor Compu-
tational Linguistics and the 17th International Con-
ference on Computational Linguistics, pages 491–
497.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953257">
<title confidence="0.9904905">Mapping Lexical Entries in a Verbs Database to WordNet Senses</title>
<author confidence="0.997756">Green Pearl J Dorr Resnik</author>
<affiliation confidence="0.999263333333333">Institute for Advanced Computer Studies Department of Computer Science University of Maryland</affiliation>
<address confidence="0.999942">College Park, MD 20742 USA</address>
<email confidence="0.998621">rgreen,llsp,bonnie,resnik@umiacs.umd.edu</email>
<abstract confidence="0.999131428571429">This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to Word- Net senses. The verbs were initially grouped into 491 classes based on syntactic features. Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval. Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Contract MDA904-96-C-1250</author>
</authors>
<title>DARPA/ITO Contracts N66001-97-C-8540 and N66001-00-28910, and a National Science Foundation Graduate Research Fellowship.</title>
<marker>MDA904-96-C-1250, </marker>
<rawString>Contract MDA904-96-C-1250, DARPA/ITO Contracts N66001-97-C-8540 and N66001-00-28910, and a National Science Foundation Graduate Research Fellowship.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Corpus-Based Lexical Choice in Natural Language Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL, Hong Kong.</booktitle>
<contexts>
<context position="7789" citStr="Bangalore and Rambow, 2000" startWordPosition="1231" endWordPosition="1235">r and bajar.3 The other classes are similarly associated with ap2For further information see the WordNet manuals, section 7, SEMCOR at http://www.cogsci.princeton.edu. 3This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information gain (Mitchell, 1997). Using information content to quantify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a taxonomy (Resnik, 1999b). More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). Levin+ Grid/Example WN Sense Spanish Verb(s) 9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse Put 8. drop set down, put down 8. dejar caer, echar, soltar 45.6 [th] 1. move, displace 1. derribar, echar Calibratable Prices dropped 3. decline, go down, wane 3. disminuir Change of State 47.7 [th src goal] 2. descend, fall, go down 2. bajar, caerse Meander The river dropped from 4. sink, drop, drop down 4. hundir, caer the lake to the sea 51.3.1 [th goal] 2. descend, fall, go down 2. b</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Corpus-Based Lexical Choice in Natural Language Generation. In Proceedings of the ACL, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
<author>Carol A Bean</author>
</authors>
<title>Relationships among Knowledge Structures: Vocabulary Integration within a Subject Domain.</title>
<date>2001</date>
<booktitle>Relationships in the Organization of Knowledge,</booktitle>
<pages>81--98</pages>
<editor>In C.A. Bean and R. Green, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Bodenreider, Bean, 2001</marker>
<rawString>Olivier Bodenreider and Carol A. Bean. 2001. Relationships among Knowledge Structures: Vocabulary Integration within a Subject Domain. In C.A. Bean and R. Green, editors, Relationships in the Organization of Knowledge, pages 81–98. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: The Kappa Statistic.</title>
<date>1996</date>
<journal>Computational Lingustics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="14736" citStr="Carletta, 1996" startWordPosition="2458" endWordPosition="2459"> for a first round of manual tagging and (only) .24 for a second round of more problematic cases.6 While the full tagging of the lexical database may make the automatic tagging task appear superfluous, the low rate of agreement between coders and the automatic nature of some of the tagging suggest there is still room for adjustment of WordNet sense assignments in the verb database. On the one hand, even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability ( ) or even the level where tentative conclusions may be drawn ( ) (Carletta, 1996), (Krippendorff, 1980). On the other hand, if the automatic assignments agree with human coding at levels comparable to the degree of agreement among humans, it may be used to identify current assignments that need review 6The kappa statistic measures the degree to which pairwise agreement of coders on a classification task surpasses what would be expected by chance; the standard definition of this coefficient is: , where is the actual percentage of agreement and is the expected percentage of agreement, averaged over all pairs of assignments. Several adjustments in the computation of the kappa</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Lingustics, 22(2):249–254, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Douglas Jones</author>
</authors>
<title>Robust Lexical Acquisition: Word Sense Disambiguation to Increase Recall and Precision.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="8793" citStr="Dorr and Jones, 1996" startWordPosition="1406" endWordPosition="1409">Change of State 47.7 [th src goal] 2. descend, fall, go down 2. bajar, caerse Meander The river dropped from 4. sink, drop, drop down 4. hundir, caer the lake to the sea 51.3.1 [th goal] 2. descend, fall, go down 2. bajar, caerse Roll I The ball dropped to the ground 51.3.1 [th particle(down)] 2. descend, fall, go down 2. bajar, caerse Roll II The ball dropped down Table 1: Relation Between Levin+ and WN Senses for ‘drop’ , propriate TL verbs during lexical selection: disminuir (class 45.6), hundir (class 47.7), and bajar (class 51.3.1).4 3 Training Data We began with the lexical database of (Dorr and Jones, 1996), which contains a significant number of WordNet-tagged verb entries. Some of the assignments were in doubt, since class splitting had occurred subsequent to those assignments, with all old WordNet senses carried over to new subclasses. New classes had also been added since the manual tagging. It was determined that the tagging for only 1791 entries—including 1442 verbs in 167 classes—could be considered stable; for these entries, 2756 assignments of WordNet senses had been made. Data for these entries, taken from both WordNet and the verb lexicon, constitute the training data for this study. </context>
</contexts>
<marker>Dorr, Jones, 1996</marker>
<rawString>Bonnie J. Dorr and Douglas Jones. 1996. Robust Lexical Acquisition: Word Sense Disambiguation to Increase Recall and Precision. Technical report, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Mari Broman Olsen</author>
</authors>
<title>Deriving Verbal and Compositional Lexical Aspect for NLP Applications.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL-97),</booktitle>
<pages>151--158</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="4146" citStr="Dorr and Olsen, 1997" startWordPosition="647" endWordPosition="650">dNet senses greatly extends the semantic potential of the database. 2 Lexical Resources We use an existing classification of 4076 English verbs, based initially on English Verbs Classes and Alternations (Levin, 1993) and extended through the splitting of some classes into subclasses and the addition of new classes. The resulting 491 classes (e.g., “Roll Verbs, Group I”, which includes drift, drop, glide, roll, swing) are referred to here as Levin+ classes. As verbs may be assigned to multiple Levin+ classes, the actual number of entries in the database is larger, 9611. Following the model of (Dorr and Olsen, 1997), each Levin+ class is associated with a thematic grid (henceforth abbreviated -grid), which summarizes a verb’s syntactic behavior by specifying its predicate argument structure. For example, the Levin+ class “Roll Verbs, Group I” is associated with the -grid [th goal], in which a theme and a goal are used (e.g., The ball dropped to the ground).1 Each -grid specification corresponds to a Grid class. There are 48 Grid classes, with a one-to-many relationship between Grid and Levin+ classes. WordNet, the lexical resource to which we are mapping entries from the lexical database, groups synonymo</context>
</contexts>
<marker>Dorr, Olsen, 1997</marker>
<rawString>Bonnie J. Dorr and Mari Broman Olsen. 1997. Deriving Verbal and Compositional Lexical Aspect for NLP Applications. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL-97), pages 151–158, Madrid, Spain, July 7-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>M Antonia Marti</author>
<author>Irene Castell´on</author>
</authors>
<title>Spanish EuroWordNet and LCS-Based Interlingual MT.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Interlinguas in MT,</booktitle>
<tech>Technical Report MCCS-97-314,</tech>
<pages>19--32</pages>
<institution>MT Summit, New Mexico State University</institution>
<location>San Diego, CA,</location>
<marker>Dorr, Marti, Castell´on, 1997</marker>
<rawString>Bonnie J. Dorr, M. Antonia Marti, and Irene Castell´on. 1997. Spanish EuroWordNet and LCS-Based Interlingual MT. In Proceedings of the Workshop on Interlinguas in MT, MT Summit, New Mexico State University Technical Report MCCS-97-314, pages 19–32, San Diego, CA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1447" citStr="Fellbaum, 1998" startWordPosition="217" endWordPosition="219">lities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment. 1 Introduction Our goal is to map entries in a lexical database of 4076 English verbs automatically to WordNet senses (Miller and Fellbaum, 1991), (Fellbaum, 1998) to support such applications as machine translation and cross-language information retrieval. For example, the verb drop is multiply ambiguous, with many potential translations in Spanish: bajar, caerse, dejar caer, derribar, disminuir,echar, hundir, soltar, etc. The database specifies a set of interpretations for drop, depending on its context in the source-language (SL). Inclusion of WordNet senses in the database enables the selection of an appropriate verb in the target language (TL). Final selection is based on a frequency count of WordNet senses across all classes to which the verb belo</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>In press. Comparing Sets of Semantic Relations in Ontologies. In</title>
<editor>R. Green, C.A. Bean, and S. Myaeng, editors,</editor>
<marker>Hovy, </marker>
<rawString>Eduard Hovy. In press. Comparing Sets of Semantic Relations in Ontologies. In R. Green, C.A. Bean, and S. Myaeng, editors, The Semantics of Relationships: An Interdisciplinary Perspective. Book manuscript submitted for review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Framework and Results for English SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--15</pages>
<contexts>
<context position="2441" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="372" endWordPosition="375">language (SL). Inclusion of WordNet senses in the database enables the selection of an appropriate verb in the target language (TL). Final selection is based on a frequency count of WordNet senses across all classes to which the verb belongs—e.g., disminuir is selected when the WordNet sense corresponds to the meaning of drop in Prices dropped. Our task differs from standard word sense disambiguation (WSD) in several ways. First, the words to be disambiguated are entries in a lexical database, not tokens in a text corpus. Second, we take an “all-words” rather than a “lexical-sample” approach (Kilgarriff and Rosenzweig, 2000): All words in the lexical database “text” are disambiguated, not just a small number for which detailed knowledge is available. Third, we replace the contextual data typically used for WSD with information about verb senses encoded in terms of thematic grids and lexical-semantic representations from (Olsen et al., 1997). Fourth, whereas a single word sense for each token in a text corpus is often assumed, the absence of sentential context leads to a situation where several WordNet senses may be equally appropriate for a database entry. Indeed, as distinctions between WordNet senses can be fin</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig. 2000. Framework and Results for English SENSEVAL. Computers and the Humanities, 34:15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>1980</date>
<location>Beverly Hills.</location>
<contexts>
<context position="14758" citStr="Krippendorff, 1980" startWordPosition="2460" endWordPosition="2461"> of manual tagging and (only) .24 for a second round of more problematic cases.6 While the full tagging of the lexical database may make the automatic tagging task appear superfluous, the low rate of agreement between coders and the automatic nature of some of the tagging suggest there is still room for adjustment of WordNet sense assignments in the verb database. On the one hand, even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability ( ) or even the level where tentative conclusions may be drawn ( ) (Carletta, 1996), (Krippendorff, 1980). On the other hand, if the automatic assignments agree with human coding at levels comparable to the degree of agreement among humans, it may be used to identify current assignments that need review 6The kappa statistic measures the degree to which pairwise agreement of coders on a classification task surpasses what would be expected by chance; the standard definition of this coefficient is: , where is the actual percentage of agreement and is the expected percentage of agreement, averaged over all pairs of assignments. Several adjustments in the computation of the kappa coefficient were made</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage, Beverly Hills.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="3741" citStr="Levin, 1993" startWordPosition="580" endWordPosition="581">verb database contains mostly syntactic information about its entries, much of which applies at the class level within the database. WordNet, on the other hand, is a significant source for information about semantic relationships, much of which applies at the “synset” level (“synsets” are WordNet’s groupings of synonymous word senses). Mapping entries in the database to their corresponding WordNet senses greatly extends the semantic potential of the database. 2 Lexical Resources We use an existing classification of 4076 English verbs, based initially on English Verbs Classes and Alternations (Levin, 1993) and extended through the splitting of some classes into subclasses and the addition of new classes. The resulting 491 classes (e.g., “Roll Verbs, Group I”, which includes drift, drop, glide, roll, swing) are referred to here as Levin+ classes. As verbs may be assigned to multiple Levin+ classes, the actual number of entries in the database is larger, 9611. Following the model of (Dorr and Olsen, 1997), each Levin+ class is associated with a thematic grid (henceforth abbreviated -grid), which summarizes a verb’s syntactic behavior by specifying its predicate argument structure. For example, th</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Semantic Networks of English.</title>
<date>1991</date>
<booktitle>Lexical and Conceptual Semantics,</booktitle>
<pages>197--229</pages>
<editor>In Beth Levin and Steven Pinker, editors,</editor>
<publisher>Elsevier Science Publishers, B.V.,</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="1429" citStr="Miller and Fellbaum, 1991" startWordPosition="213" endWordPosition="216">asses; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment. 1 Introduction Our goal is to map entries in a lexical database of 4076 English verbs automatically to WordNet senses (Miller and Fellbaum, 1991), (Fellbaum, 1998) to support such applications as machine translation and cross-language information retrieval. For example, the verb drop is multiply ambiguous, with many potential translations in Spanish: bajar, caerse, dejar caer, derribar, disminuir,echar, hundir, soltar, etc. The database specifies a set of interpretations for drop, depending on its context in the source-language (SL). Inclusion of WordNet senses in the database enables the selection of an appropriate verb in the target language (TL). Final selection is based on a frequency count of WordNet senses across all classes to w</context>
</contexts>
<marker>Miller, Fellbaum, 1991</marker>
<rawString>George A. Miller and Christiane Fellbaum. 1991. Semantic Networks of English. In Beth Levin and Steven Pinker, editors, Lexical and Conceptual Semantics, pages 197–229. Elsevier Science Publishers, B.V., Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="7464" citStr="Mitchell, 1997" startWordPosition="1182" endWordPosition="1183"> 9.4 has three possible WordNet senses for drop. However, the WordNet sense 8 is not associated with any of the other classes; thus, it is considered to have a higher “information content” than the others. The upshot is that the lexical-selection routine prefers dejar caer over other translations such as derribar and bajar.3 The other classes are similarly associated with ap2For further information see the WordNet manuals, section 7, SEMCOR at http://www.cogsci.princeton.edu. 3This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information gain (Mitchell, 1997). Using information content to quantify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a taxonomy (Resnik, 1999b). More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). Levin+ Grid/Example WN Sense Spanish Verb(s) 9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse Put 8. drop set down, put down 8. dejar caer, echar, soltar 45.6 [</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Broman Olsen</author>
<author>Bonnie J Dorr</author>
<author>David J Clark</author>
</authors>
<title>Using WordNet to Posit Hierarchical Structure in Levin’s Verb Classes.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Interlinguas in MT,</booktitle>
<tech>Technical Report MCCS-97-314,</tech>
<pages>99--110</pages>
<institution>MT Summit, New Mexico State University</institution>
<location>San Diego, CA,</location>
<contexts>
<context position="2763" citStr="Olsen et al., 1997" startWordPosition="424" endWordPosition="427">es dropped. Our task differs from standard word sense disambiguation (WSD) in several ways. First, the words to be disambiguated are entries in a lexical database, not tokens in a text corpus. Second, we take an “all-words” rather than a “lexical-sample” approach (Kilgarriff and Rosenzweig, 2000): All words in the lexical database “text” are disambiguated, not just a small number for which detailed knowledge is available. Third, we replace the contextual data typically used for WSD with information about verb senses encoded in terms of thematic grids and lexical-semantic representations from (Olsen et al., 1997). Fourth, whereas a single word sense for each token in a text corpus is often assumed, the absence of sentential context leads to a situation where several WordNet senses may be equally appropriate for a database entry. Indeed, as distinctions between WordNet senses can be fine-grained (Palmer, 2000), it may be unclear, even in context, which sense is meant. The verb database contains mostly syntactic information about its entries, much of which applies at the class level within the database. WordNet, on the other hand, is a significant source for information about semantic relationships, muc</context>
</contexts>
<marker>Olsen, Dorr, Clark, 1997</marker>
<rawString>Mari Broman Olsen, Bonnie J. Dorr, and David J. Clark. 1997. Using WordNet to Posit Hierarchical Structure in Levin’s Verb Classes. In Proceedings of the Workshop on Interlinguas in MT, MT Summit, New Mexico State University Technical Report MCCS-97-314, pages 99–110, San Diego, CA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
</authors>
<title>Consistent Criteria for Sense Distinctions. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--217</pages>
<contexts>
<context position="3065" citStr="Palmer, 2000" startWordPosition="474" endWordPosition="475"> in the lexical database “text” are disambiguated, not just a small number for which detailed knowledge is available. Third, we replace the contextual data typically used for WSD with information about verb senses encoded in terms of thematic grids and lexical-semantic representations from (Olsen et al., 1997). Fourth, whereas a single word sense for each token in a text corpus is often assumed, the absence of sentential context leads to a situation where several WordNet senses may be equally appropriate for a database entry. Indeed, as distinctions between WordNet senses can be fine-grained (Palmer, 2000), it may be unclear, even in context, which sense is meant. The verb database contains mostly syntactic information about its entries, much of which applies at the class level within the database. WordNet, on the other hand, is a significant source for information about semantic relationships, much of which applies at the “synset” level (“synsets” are WordNet’s groupings of synonymous word senses). Mapping entries in the database to their corresponding WordNet senses greatly extends the semantic potential of the database. 2 Lexical Resources We use an existing classification of 4076 English ve</context>
</contexts>
<marker>Palmer, 2000</marker>
<rawString>Martha Palmer. 2000. Consistent Criteria for Sense Distinctions. Computers and the Humanities, 34:217–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="7810" citStr="Ratnaparkhi, 2000" startWordPosition="1236" endWordPosition="1237">s are similarly associated with ap2For further information see the WordNet manuals, section 7, SEMCOR at http://www.cogsci.princeton.edu. 3This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information gain (Mitchell, 1997). Using information content to quantify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a taxonomy (Resnik, 1999b). More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). Levin+ Grid/Example WN Sense Spanish Verb(s) 9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse Put 8. drop set down, put down 8. dejar caer, echar, soltar 45.6 [th] 1. move, displace 1. derribar, echar Calibratable Prices dropped 3. decline, go down, wane 3. disminuir Change of State 47.7 [th src goal] 2. descend, fall, go down 2. bajar, caerse Meander The river dropped from 4. sink, drop, drop down 4. hundir, caer the lake to the sea 51.3.1 [th goal] 2. descend, fall, go down 2. bajar, caerse Roll I T</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of the ANLP-NAACL, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Disambiguating noun groupings with respect to wordnet senses.</title>
<date>1999</date>
<booktitle>Natural Language Processing Using Very Large Corpora,</booktitle>
<pages>77--98</pages>
<editor>In S. Armstrong, K. Church, P. Isabelle, E. Tzoukermann S. Manzi, and D. Yarowsky, editors,</editor>
<publisher>Kluwer Academic,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="7631" citStr="Resnik, 1999" startWordPosition="1211" endWordPosition="1212">formation content” than the others. The upshot is that the lexical-selection routine prefers dejar caer over other translations such as derribar and bajar.3 The other classes are similarly associated with ap2For further information see the WordNet manuals, section 7, SEMCOR at http://www.cogsci.princeton.edu. 3This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information gain (Mitchell, 1997). Using information content to quantify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a taxonomy (Resnik, 1999b). More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). Levin+ Grid/Example WN Sense Spanish Verb(s) 9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse Put 8. drop set down, put down 8. dejar caer, echar, soltar 45.6 [th] 1. move, displace 1. derribar, echar Calibratable Prices dropped 3. decline, go down, wane 3. disminuir Change of State 47.7 [th src goal] 2. descend, fall, go dow</context>
<context position="11902" citStr="Resnik, 1999" startWordPosition="1968" endWordPosition="1969">n SEMCOR and is an occurrence of any of a set of tags for verb in SEMCOR, with being one of the senses possible for verb . This probability is the prior probability of specific WordNet verb senses. Values average .11, but in some cases the probability is 1.0. In addition to the foregoing data elements, based on the training set, we also made use of a semantic similarity measure, which reflects the confidence with which a verb, given the total set of verbs assigned to its Levin+ class, is mapped to a specific WordNet sense. This represents an implementation of a class disambiguation algorithm (Resnik, 1999a), modified to run against the WordNet verb hierarchy.5 We also made a powerful “same-synset assumption”: If (1) two verbs are assigned to the same Levin+ class, (2) one of the verbs has been mapped to a specific WordNet sense , and (3) the other verb has a WordNet sense synonymous with , then should be mapped to . Since WordNet groups synonymous word senses into “synsets,” and would correspond to the same synset. Since Levin+ verbs are mapped to WordNet senses via their corresponding synset identifiers, when the set of conditions enumerated above are met, the two verb entries would be mapped</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999a. Disambiguating noun groupings with respect to wordnet senses. In S. Armstrong, K. Church, P. Isabelle, E. Tzoukermann S. Manzi, and D. Yarowsky, editors, Natural Language Processing Using Very Large Corpora, pages 77–98. Kluwer Academic, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>In Journal ofArtificial Intelligence Research, number</journal>
<volume>11</volume>
<pages>95--130</pages>
<contexts>
<context position="7631" citStr="Resnik, 1999" startWordPosition="1211" endWordPosition="1212">formation content” than the others. The upshot is that the lexical-selection routine prefers dejar caer over other translations such as derribar and bajar.3 The other classes are similarly associated with ap2For further information see the WordNet manuals, section 7, SEMCOR at http://www.cogsci.princeton.edu. 3This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information gain (Mitchell, 1997). Using information content to quantify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a taxonomy (Resnik, 1999b). More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). Levin+ Grid/Example WN Sense Spanish Verb(s) 9.4 [ag th mod-loc src goal] 1. move, displace 1. derribar, echar Directional I dropped the stone 2. descend, fall, go down 2. bajar, caerse Put 8. drop set down, put down 8. dejar caer, echar, soltar 45.6 [th] 1. move, displace 1. derribar, echar Calibratable Prices dropped 3. decline, go down, wane 3. disminuir Change of State 47.7 [th src goal] 2. descend, fall, go dow</context>
<context position="11902" citStr="Resnik, 1999" startWordPosition="1968" endWordPosition="1969">n SEMCOR and is an occurrence of any of a set of tags for verb in SEMCOR, with being one of the senses possible for verb . This probability is the prior probability of specific WordNet verb senses. Values average .11, but in some cases the probability is 1.0. In addition to the foregoing data elements, based on the training set, we also made use of a semantic similarity measure, which reflects the confidence with which a verb, given the total set of verbs assigned to its Levin+ class, is mapped to a specific WordNet sense. This represents an implementation of a class disambiguation algorithm (Resnik, 1999a), modified to run against the WordNet verb hierarchy.5 We also made a powerful “same-synset assumption”: If (1) two verbs are assigned to the same Levin+ class, (2) one of the verbs has been mapped to a specific WordNet sense , and (3) the other verb has a WordNet sense synonymous with , then should be mapped to . Since WordNet groups synonymous word senses into “synsets,” and would correspond to the same synset. Since Levin+ verbs are mapped to WordNet senses via their corresponding synset identifiers, when the set of conditions enumerated above are met, the two verb entries would be mapped</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999b. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. In Journal ofArtificial Intelligence Research, number 11, pages 95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving data-driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Associationfor Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>491--497</pages>
<marker>Van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Hans Van Halteren, Jakub Zavrel, and Walter Daelemans. 1998. Improving data-driven wordclass tagging by system combination. In Proceedings of the 36th Annual Meeting of the Associationfor Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 491– 497.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>