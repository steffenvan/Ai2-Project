<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.9983065">
Exploiting Web-Derived Selectional Preference to Improve Statistical
Dependency Parsing
</title>
<author confidence="0.998109">
Guangyou Zhou, Jun Zhao; Kang Liu, and Li Cai
</author>
<affiliation confidence="0.9970405">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
</affiliation>
<address confidence="0.967136">
95 Zhongguancun East Road, Beijing 100190, China
</address>
<email confidence="0.9996">
{gyzhou,jzhao,kliu,lcai}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999570894736842">
In this paper, we present a novel approach
which incorporates the web-derived selec-
tional preferences to improve statistical de-
pendency parsing. Conventional selectional
preference learning methods have usually fo-
cused on word-to-class relations, e.g., a verb
selects as its subject a given nominal class.
This paper extends previous work to word-
to-word selectional preferences by using web-
scale data. Experiments show that web-scale
data improves statistical dependency pars-
ing, particularly for long dependency relation-
ships. There is no data like more data, perfor-
mance improves log-linearly with the number
of parameters (unique N-grams). More impor-
tantly, when operating on new domains, we
show that using web-derived selectional pref-
erences is essential for achieving robust per-
formance.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997888909090909">
Dependency parsing is the task of building depen-
dency links between words in a sentence, which has
recently gained a wide interest in the natural lan-
guage processing community. With the availabil-
ity of large-scale annotated corpora such as Penn
Treebank (Marcus et al., 1993), it is easy to train
a high-performance dependency parser using super-
vised learning methods.
However, current state-of-the-art statistical de-
pendency parsers (McDonald et al., 2005; McDon-
ald and Pereira, 2006; Hall et al., 2006) tend to have
</bodyText>
<note confidence="0.871799">
Correspondence author: jzhao@nlpr.ia.ac.cn
</note>
<bodyText confidence="0.988247323529412">
lower accuracies for longer dependencies (McDon-
ald and Nivre, 2007). The length of a dependency
from word wi to word wj is simply equal to |i − i|.
Longer dependencies typically represent the mod-
ifier of the root or the main verb, internal depen-
dencies of longer NPs or PP-attachment in a sen-
tence. Figure 1 shows the F1 score&apos; relative to the
dependency length on the development set by using
the graph-based dependency parsers (McDonald et
al., 2005; McDonald and Pereira, 2006). We note
that the parsers provide very good results for adja-
cent dependencies (96.89% for dependency length
=1), while the dependency length increases, the ac-
curacies degrade sharply. These longer dependen-
cies are therefore a major opportunity to improve the
overall performance of dependency parsing. Usu-
ally, these longer dependencies can be parsed de-
pendent on the specific words involved due to the
limited range of features (e.g., a verb and its mod-
ifiers). Lexical statistics are therefore needed for
resolving ambiguous relationships, yet the lexical-
ized statistics are sparse and difficult to estimate di-
rectly. To solve this problem, some information with
different granularity has been investigated. Koo et
al. (2008) proposed a semi-supervised dependency
parsing by introducing lexical intermediaries at a
coarser level than words themselves via a cluster
method. This approach, however, ignores the se-
lectional preference for word-to-word interactions,
such as head-modifier relationship. Extra resources
&apos;Precision represents the percentage of predicted arcs of
length d that are correct, and recall measures the percentage
of gold-standard arcs of length d that are correctly predicted.
Fl = 2 x precision x recall/(precision + recall)
</bodyText>
<page confidence="0.943947">
1556
</page>
<note confidence="0.987305">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1556–1565,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.406243">
Dependency Length
</figure>
<figureCaption confidence="0.999566">
Figure 1: F score relative to dependency length.
</figureCaption>
<bodyText confidence="0.998824612903226">
beyond the annotated corpora are needed to capture
the bi-lexical relationship at the word-to-word level.
Our purpose in this paper is to exploit web-
derived selectional preferences to improve the su-
pervised statistical dependency parsing. All of our
lexical statistics are derived from two kinds of web-
scale corpus: one is the web, which is the largest
data set that is available for NLP (Keller and Lap-
ata, 2003). Another is a web-scale N-gram corpus,
which is a N-gram corpus with N-grams of length 1-
5 (Brants and Franz, 2006), we call it Google V1 in
this paper. The idea is very simple: web-scale data
have large coverage for word pair acquisition. By
leveraging some assistant data, the dependency pars-
ing model can directly utilize the additional informa-
tion to capture the word-to-word level relationships.
We address two natural and related questions which
some previous studies leave open:
Question I: Is there a benefit in incorporating
web-derived selectional preference features for sta-
tistical dependency parsing, especially for longer de-
pendencies?
Question II: How well do web-derived selec-
tional preferences perform on new domains?
For Question I, we systematically assess the value
of using web-scale data in state-of-the-art super-
vised dependency parsers. We compare dependency
parsers that include or exclude selectional prefer-
ence features obtained from web-scale corpus. To
the best of our knowledge, none of the existing stud-
ies directly address long dependencies of depen-
dency parsing by using web-scale data.
Most statistical parsers are highly domain depen-
dent. For example, the parsers trained on WSJ text
perform poorly on Brown corpus. Some studies have
investigated domain adaptation for parsers (Mc-
Closky et al., 2006; Daume III, 2007; McClosky et
al., 2010). These approaches assume that the parsers
know which domain it is used, and that it has ac-
cess to representative data in that domain. How-
ever, in practice, these assumptions are unrealistic
in many real applications, such as when processing
the heterogeneous genre of web texts. In this paper
we incorporate the web-derived selectional prefer-
ence features to design our parsers for robust open-
domain testing.
We conduct the experiments on the English Penn
Treebank (PTB) (Marcus et al., 1993). The results
show that web-derived selectional preference can
improve the statistical dependency parsing, partic-
ularly for long dependency relationships. More im-
portantly, when operating on new domains, the web-
derived selectional preference features show great
potential for achieving robust performance (Section
4.3).
The remainder of this paper is divided as follows.
Section 2 gives a brief introduction of dependency
parsing. Section 3 describes the web-derived selec-
tional preference features. Experimental evaluation
and results are reported in Section 4. Finally, we dis-
cuss related work and draw conclusion in Section 5
and Section 6, respectively.
</bodyText>
<sectionHeader confidence="0.985881" genericHeader="method">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.998677636363636">
In dependency parsing, we attempt to build head-
modifier (or head-dependent) relations between
words in a sentence. The discriminative parser we
used in this paper is based on the part-factored
model and features of the MSTParser (McDonald et
al., 2005; McDonald and Pereira, 2006; Carreras,
2007). The parsing model can be defined as a con-
ditional distribution p(y|x; w) over each projective
parse tree y for a particular sentence x, parameter-
ized by a vector w. The probability of a parse tree
is
</bodyText>
<equation confidence="0.993161666666667">
p(y|x; w) = 1exp{∑w · 4)(x, P) } (1)
Z(x; w)
P��
</equation>
<bodyText confidence="0.9942805">
where Z(x; w) is the partition function and 4) are
part-factored feature functions that include head-
</bodyText>
<figure confidence="0.995166909090909">
1
MST1
MST2
1 5 10 15 20 25 30
0.95
0.9
0.85
0.8
0.75
0.7
F1 Score (%)
</figure>
<page confidence="0.994968">
1557
</page>
<bodyText confidence="0.998937333333333">
modifier parts, sibling parts and grandchild parts.
Given the training set {(xi, yi)}Ni=1, parameter es-
timation for log-linear models generally resolve
around optimization of a regularized conditional
log-likelihood objective w* = argminK,L(w)
where
</bodyText>
<equation confidence="0.9974385">
logp(yiJxi; w) + 2JJwJJ2 (2)
1
</equation>
<bodyText confidence="0.99595475">
The parameter C &gt; 0 is a constant dictating the
level of regularization in the model. Since objec-
tive function L(w) is smooth and convex, which is
convenient for standard gradient-based optimization
techniques. In this paper we use the dual exponenti-
ated gradient (EG)2 descent, which is a particularly
effective optimization algorithm for log-linear mod-
els (Collins et al., 2008).
</bodyText>
<sectionHeader confidence="0.90715" genericHeader="method">
3 Web-Derived Selectional Preference
Features
</sectionHeader>
<bodyText confidence="0.999708428571428">
In this paper, we employ two different feature sets:
a baseline feature seta which draw upon “normal”
information source, such as word forms and part-of-
speech (POS) without including the web-derived se-
lectional preference4 features, a feature set conjoins
the baseline features and the web-derived selectional
preference features.
</bodyText>
<subsectionHeader confidence="0.998157">
3.1 Web-scale resources
</subsectionHeader>
<bodyText confidence="0.99996375">
All of our selectional preference features described
in this paper rely on probabilities derived from unla-
beled data. To use the largest amount of data possi-
ble, we exploit web-scale resources. one is web, N-
gram counts are approximated by Google hits. An-
other we use is Google V1(Brants and Franz, 2006).
This N-gram corpus records how often each unique
sequence of words occurs. N-grams appearing 40
</bodyText>
<footnote confidence="0.9933176">
2http://groups.csail.mit.edu/nlp/egstra/
3This kind of feature sets are similar to other feature sets in
the literature (McDonald et al., 2005; Carreras, 2007), so we
will not attempt to give a exhaustive description.
4Selectional preference tells us which arguments are plau-
</footnote>
<bodyText confidence="0.985470333333333">
sible for a particular predicate, one way to determine the se-
lectional preference is from co-occurrences of predicates and
arguments in text (Bergsma et al., 2008). In this paper, the
selectional preferences have the same meaning with N-grams,
which model the word-to-word relationships, rather than only
considering the predicates and arguments relationships.
</bodyText>
<figureCaption confidence="0.7397195">
Figure 2: An example of a labeled dependency tree. The
tree contains a special token “$” which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
</figureCaption>
<bodyText confidence="0.99996975">
times or more (1 in 25 billion) are kept, and appear
in the n-gram tables. All n-grams with lower counts
are discarded. Co-occurrence probabilities can be
calculated directly from the N-gram counts.
</bodyText>
<subsectionHeader confidence="0.990974">
3.2 Web-derived N-gram features
3.2.1 PMI
</subsectionHeader>
<bodyText confidence="0.999979">
Previous work on noun compounds bracketing
has used adjacency model (Resnik, 1993) and de-
pendency model (Lauer, 1995) to compute associa-
tion statistics between pairs of words. In this pa-
per we generalize the adjacency and dependency
models by including the pointwise mutual informa-
tion (Church and Hanks, 1900) between all pairs of
words in the dependency tree:
</bodyText>
<equation confidence="0.987877">
PMI(x, y) = log p(“x y”) (3)
p(“x”)p(“y”)
</equation>
<bodyText confidence="0.999878090909091">
where p(“x y”) is the co-occurrence probabilities.
When use the Google V1 corpus, this probabilities
can be calculated directly from the N-gram counts,
while using the Google hits, we send the queries to
the search engine Google5 and all the search queries
are performed as exact matches by using quotation
marks.6
The value of these features is the PMI, if it is de-
fined. If the PMI is undefined, following the work
of (Pitler et al., 2010), we include one of two binary
features:
</bodyText>
<equation confidence="0.921689">
p(“x y”) = 0 or p(“x”) V p(“y”) = 0
</equation>
<bodyText confidence="0.926567">
Besides, we also consider the trigram features be-
</bodyText>
<footnote confidence="0.574824142857143">
5http://www.google.com/
6Google only allows automated querying through the
Google Web API, this involves obtaining a license key, which
then restricts the number of queries to a daily quota of 1000.
However, we obtained a quota of 20,000 queries per day by
sending a request to api-support@google.com for research pur-
poses.
</footnote>
<figure confidence="0.423452142857143">
obi
det
det
root
subi
mod
obi
</figure>
<equation confidence="0.9230939">
L(w) = −C ∑N
i=1
1558
PMI(“hit with”)
xi-word=“hit”, xj-word=“with”, PMI(“hit with”)
xi-word=“hit”, xj-word=“with”, xj-pos=“IN”, PMI(“hit with”)
xi-word=“hit”, xi-pos=“VBD”, xj-word=“with”, PMI(“hit with”)
xi-word=“hit”, b-pos=“ball”, xj-word=“with”, PMI(“hit with”)
xi-word=“hit”, xj-word=“with”, PMI(“hit with”), dir=R, dist=3
� � �
</equation>
<tableCaption confidence="0.982505">
Table 1: An example of the N-gram PMI features and the conjoin features with the baseline.
</tableCaption>
<bodyText confidence="0.865238">
tween the three words in the dependency tree:
</bodyText>
<equation confidence="0.999407">
PMI(x, y, z) = log p(“x y z”)
p(“x y”)p(“y z”) (4)
</equation>
<bodyText confidence="0.999949">
This kinds of trigram features, for example in MST-
Parser, which can directly capture the sibling and
grandchild features.
We illustrate the PMI features with an example
of dependency parsing tree in Figure 2. In deciding
the dependency between the main verb hit and its ar-
gument headed preposition with, an example of the
N-gram PMI features and the conjoin features with
the baseline are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.958431">
3.2.2 PP-attachment
</subsectionHeader>
<bodyText confidence="0.99996875">
Propositional phrase (PP) attachment is one of
the hardest problems in English dependency pars-
ing. An English sentence consisting of a subject, a
verb, and a nominal object followed by a preposi-
tional phrase is often ambiguous. Ambiguity resolu-
tion reflects the selectional preference between the
verb and noun with their prepositional phrase. For
example, considering the following two examples:
</bodyText>
<listItem confidence="0.998177">
(1) John hit the ball with the bat.
(2) John hit the ball with the red stripe.
</listItem>
<bodyText confidence="0.999936625">
In sentence (1), the preposition with depends on the
main verb hit; but in sentence (2), the prepositional
phrase is a noun attribute and the preposition with
needs to depends on the word ball. To resolve this
kind of ambiguity, there needs to measure the attach-
ment preference. We thus have PP-attachment fea-
tures that determine the PMI association across the
preposition word “IN”7:
</bodyText>
<equation confidence="0.9888275">
PMIIN(x, z) = logp(“x IN z”) (5)
p(x)
</equation>
<bodyText confidence="0.992615">
Here, the preposition word “IN” (e.g., “with”, “in”, ...) is
any token whose part-of-speech is IN
</bodyText>
<subsectionHeader confidence="0.945329">
N-gram feature templates
</subsectionHeader>
<construct confidence="0.954805777777778">
hw, mw, PMI(hw,mw)
hw, ht, mw, PMI(hw,mw)
hw, mw, mt, PMI(hw,mw)
hw, ht, mw, mt, PMI(hw,mw)
� � �
hw, mw, sw
hw, mw, sw, PMI(hw, mw, sw)
hw, mw, gw
hw, mw, gw, PMI(hw, mw, gw)
</construct>
<tableCaption confidence="0.8732175">
Table 2: Examples of N-gram feature templates. Each
entry represents a class of indicator for tuples of informa-
</tableCaption>
<bodyText confidence="0.7956986">
tion. For example, “hw, mw” reprsents a class of indi-
cator features with one feature for each possible combi-
nation of head word and modifier word. Abbreviations:
hw=head word, ht= head POS. st, gt=likewise for sibling
and grandchild.
</bodyText>
<equation confidence="0.9761745">
PMIIN(y, z) = logp(“y IN z”) (6)
p(y)
</equation>
<bodyText confidence="0.9992235">
where the word x and y are usually verb and noun,
z is a noun which directly depends on the preposi-
tion word “IN”. For example in sentence (1), we
would include the features PMIwith(hit, bat) and
PMIwith(ball, bat). If both PMI features exist and
PMIwith(hit, bat) &gt; PMIwith(ball, bat), indicating
to our dependency parsing model that the preposi-
tion word with depends on the verb hit is a good
choice. While in sentence (2), the features include
PMIwith(hit,
</bodyText>
<subsectionHeader confidence="0.999429">
3.3 N-gram feature templates
</subsectionHeader>
<bodyText confidence="0.970038963636363">
We generate N-gram features by mimicking the
template structure of the original baseline features.
For example, the baseline feature set includes indi-
cators for word-to-word and tag-to-tag interactions
between the head and modifier of a dependency. In
the N-gram feature set, we correspondingly intro-
duce N-gram PMI for word-to-word interactions.
stripe) and PMIwith(ball, stripe).
1559
The N-gram feature set for MSTParser is shown and Matsumoto, 2003) to convert the phrase struc-
in Table 2. Following McDonald et al. (2005), ture syntax of the Treebank into a dependency tree
all features are conjoined with the direction of representation, dependency labels were obtained via
attachment as well as the distance between the two the ”Malt” hard-coded setting.8 We split the Tree-
words creating the dependency. In between N-gram bank into a training set (Sections 2-21), a devel-
features, we include the form of word trigrams opment set (Section 22), and several test sets (Sec-
and PMI of the trigrams. The surrounding word tions 0,9 1, 23, and 24). The part-of-speech tags for
N-gram features represent the local context of the the development and test set were automatically as-
selectional preference. Besides, we also present signed by the MXPOST tagger10, where the tagger
the second-order feature templates, including the was trained on the entire training corpus.
sibling and grandchild features. These features are Web page hits for word pairs and trigrams are ob-
designed to disambiguate cases like coordinating tained using a simple heuristic query to the search
conjunctions and prepositional attachment. Con- engine Google.11 Inflected queries are performed
sider the examples we have shown in section 3.2.2, by expanding a bigram or trigram into all its mor-
for sentence (1), the dependency graph path feature phological forms. These forms are then submitted as
ball —* with —* bat should have a lower weight literal queries, and the resulting hits are summed up.
since ball rarely is modified by bat, but is often John Carroll’s suite of morphological tools12 is used
seen through them (e.g., a higher weight should be to generate inflected forms of verbs and nouns. All
associated with hit —* with —* bat). In contrast, the search terms are performed as exact matches by
for sentence (2), our N-gram features will tell us using quotation marks and submitted to the search
that the prepositional phrase is much more likely engines in lower case.
to attach to the noun since the dependency graph We measured the performance of the parsers us-
path feature ball —* with —* stripe should have a ing the following metrics: unlabeled attachment
high weight due to the high strength of selectional score (UAS), labeled attachment score (LAS) and
preference between ball and stripe. complete match (CM), which were defined by Hall
Web-derived selectional preference features et al. (2006). All the metrics are calculated as mean
based on PMI values are trickier to incorporate scores per word, and punctuation tokens are consis-
into the dependency parsing model because they tently excluded.
are continuous rather than discrete. Since all the 4.1 Main results
baseline features used in the literature (McDonald et There are some clear trends in the results of Ta-
al., 2005; Carreras, 2007) take on binary values of 0 ble 3. First, performance increases with the order
or 1, there is a “mis-match” between the continuous of the parser: edge-factored model (dep1) has the
and binary features. Log-linear dependency parsing lowest performance, adding sibling and grandchild
model is sensitive to inappropriately scaled feature. relationships (dep2) significantly increases perfor-
To solve this problem, we transform the PMI mance. Similar observations regarding the effect of
values into a more amenable form by replacing the model order have also been made by Carreras (2007)
PMI values with their z-score. The z-score of a and Koo et al. (2008).
PMI value x is x�� Second, note that the parsers incorporating the N-
� , where p and Q are the mean gram feature sets consistently outperform the mod-
and standard deviation of the PMI distribution, els using the baseline features in all test data sets,
respectively. regardless of model order or label usage. Another
4 Experiments
In order to evaluate the effectiveness of our proposed 8http://w3.msi.vxu.se/ nivre/research/MaltXML.html
approach, we conducted dependency parsing exper- 9We removed a single 249-word sentence from Section 0 for
iments in English. The experiments were performed computational reasons.
on the Penn Treebank (PTB) (Marcus et al., 1993), 10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
</bodyText>
<table confidence="0.957933714285714">
using a standard set of head-selection rules (Yamada 11http://www.google.com/
1560 12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L
00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.42
01 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.37
23 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.77
24 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89
</table>
<tableCaption confidence="0.99641675">
Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:
dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from
the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are
scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
</tableCaption>
<bodyText confidence="0.987137">
finding is that the N-gram features derived from
Google hits are slightly better than Google V1 due
to the large N-gram coverage, we will discuss later.
As a final note, all the comparisons between the inte-
gration of N-gram features and the baseline features
in Table 3 are mildly significant using the Z-test of
Collins et al. (2005) (p &lt; 0.08).
</bodyText>
<table confidence="0.99308075">
Type Systems UAS CM
D Yamada and Matsumoto (2003) 90.3 38.7
McDonald et al. (2005) 90.9 37.5
McDonald and Pereira (2006) 91.5 42.1
Corston-Oliver et al. (2006) 90.9 37.5
Hall et al. (2006) 89.4 36.4
Wang et al. (2007) 89.2 34.4
Carreras et al. (2008) 93.5 -
GoldBerg and Elhadad (2010)† 91.32 40.41
Ours 92.64 46.61
C Nivre and McDonald (2008)† 92.12 44.37
Martins et al. (2008)† 92.87 45.51
Zhang and Clark (2008) 92.1 45.4
S Koo et al. (2008) 93.16 -
Suzuki et al. (2009) 93.79 -
Chen et al. (2009) 93.16 47.15
</table>
<tableCaption confidence="0.997549">
Table 4: Comparison of our final results with other best-
</tableCaption>
<bodyText confidence="0.99603406">
performing systems on the whole Section 23. Type
D, C and S denote discriminative, combined and semi-
supervised systems, respectively. † These papers were
not directly reported the results on this data set, we im-
plemented the experiments in this paper.
To put our results in perspective, we also com-
pare them with other best-performing systems in Ta-
ble 4. To facilitate comparisons with previous work,
we only use Section 23 as the test data. The re-
sults show that our second order model incorpo-
rating the N-gram features (92.64) performs better
than most previously reported discriminative sys-
tems trained on the Treebank. Carreras et al. (2008)
reported a very high accuracy using information of
constituent structure of TAG grammar formalism,
while in our system, we do not use such knowl-
edge. When compared to the combined systems, our
system is better than Nivre and McDonald (2008)
and Zhang and Clark (2008), but a slightly worse
than Martins et al. (2008). We also compare our
method with the semi-supervised approaches, the
semi-supervised approaches achieved very high ac-
curacies by leveraging on large unlabeled data di-
rectly into the systems for joint learning and decod-
ing, while in our method, we only explore the N-
gram features to further improve supervised depen-
dency parsing performance.
Table 5 shows the details of some other N-gram
sources, where NEWS: created from a large set of
news articles including the Reuters and Gigword
(Graff, 2003) corpora. For a given number of unique
N-gram, using any of these sources does not have
significant difference in Figure 3. Google hits is
the largest N-gram data and shows the best perfor-
mance. The other two are smaller ones, accuracies
increase linearly with the log of the number of types
in the auxiliary data set. Similar observations have
been made by Pitler et al. (2010). We see that the
relationship between accuracy and the number of N-
gram is not monotonic for Google V1. The reason
may be that Google V1 does not make detailed pre-
processing, containing many mistakes in the corpus.
Although Google hits is noisier, it has very much
larger coverage of bigrams or trigrams.
Some previous studies also found a log-linear
relationship between unlabeled data (Suzuki and
Isozaki, 2008; Suzuki et al., 2009; Bergsma et al.,
2010; Pitler et al., 2010). We have shown that this
trend continues well for dependency parsing by us-
ing web-scale data (NEWS and Google V1).
</bodyText>
<footnote confidence="0.597502">
13Google indexes about more than 8 billion pages and each
contains about 1,000 words on average.
</footnote>
<page confidence="0.925334">
1561
</page>
<table confidence="0.9684415">
Corpus # of tokens 0 # of types
NEWS 3.2B 1 3.7B
Google V1 1,024.9B 40 3.4B
Google hits13 8,000B 100 -
</table>
<tableCaption confidence="0.9660628">
Table 5: N-gram data, with total number of words in the
original corpus (in billions, B). Following (Brants and
Franz, 2006; Pitler et al., 2010), we set the frequency
threshold to filter the data 0, and total number of unique
N-gram (types) remaining in the data.
</tableCaption>
<figure confidence="0.551517">
Number of Unique N-grams
</figure>
<figureCaption confidence="0.971258">
Figure 3: There is no data like more data. UAS accu-
racy improves with the number of unique N-grams but
still lower than the Google hits.
</figureCaption>
<subsectionHeader confidence="0.961936">
4.2 Improvement relative to dependency length
</subsectionHeader>
<bodyText confidence="0.999942090909091">
The experiments in (McDonald and Nivre, 2007)
showed a negative impact on the dependency pars-
ing performance from too long dependencies. For
our proposed approach, the improvement relative
to dependency length is shown in Figure 4. From
the Figure, it is seen that our method gives observ-
able better performance when dependency lengths
are larger than 3. The results here show that the
proposed approach improves the dependency pars-
ing performance, particularly for long dependency
relationships.
</bodyText>
<subsectionHeader confidence="0.99778">
4.3 Cross-genre testing
</subsectionHeader>
<bodyText confidence="0.999984428571429">
In this section, we present the experiments to vali-
date the robustness the web-derived selectional pref-
erences. The intent is to understand how well the
web-derived selectional preferences transfer to other
sources.
The English experiment evaluates the perfor-
mance of our proposed approach when it is trained
</bodyText>
<figure confidence="0.665578">
Dependency Length
</figure>
<figureCaption confidence="0.999238">
Figure 4: Dependency length vs. Fl score.
</figureCaption>
<bodyText confidence="0.999971391304348">
on annotated data from one genre of text (WSJ) and
is used to parse a test set from a different genre: the
biomedical domain related to cancer (PennBioIE.,
2005) with 2,600 parsed sentences. We divided the
data into 500 for training, 100 for development and
others for testing. We created five sets of train-
ing data with 100, 200, 300, 400, and 500 sen-
tences respectively. Figure 5 plots the UAS ac-
curacy as function of training instances. WSJ is
the performance of our second-order dependency
parser trained on section 2-21; WSJ+N-gram is the
performance of our proposed approach trained on
section 2-21; WSJ+BioMed is the performance of
the parser trained on WSJ and biomedical data.
WSJ+BioMed+N-gram is the performance of our
proposed approach trained on WSJ and biomedical
data. The results show that incorporating the web-
scale N-gram features can significantly improve the
dependency parsing performance, and the improve-
ment is much larger than the in-domain testing pre-
sented in Section 4.1, the reason may be that web-
derived N-gram features do not depend directly on
training data and thus work better on new domains.
</bodyText>
<subsectionHeader confidence="0.986374">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.995902375">
In this paper, we present a novel method to im-
prove dependency parsing by using web-scale data.
Despite the success, there are still some problems
which should be discussed.
(1) Google hits is less sparse than Google V1
in modeling the word-to-word relationships, but
Google hits are likely to be noisier than Google V1.
It is very appealing to carry out a correlation anal-
</bodyText>
<figure confidence="0.999423869565217">
UAS Score (%)
92.7
92.6
92.5
92.4
92.3
92.2
92.1
91.9
1e4 1e5 1e6 1e7 1e8 1e9
92
NEWS
Google V1
Google hits
1 10 20 30
F1 Score (%) 1
0.95
0.9
0.85
0.8
0.75
MST2
MST2+N-gram
</figure>
<page confidence="0.783853">
1562
</page>
<figureCaption confidence="0.998411833333333">
Figure 5: Adapting a WSJ parser to biomedical text.
WSJ: performance of parser trained only on WSJ;
WSJ+N-gram: performance of our proposed approach
trained only on WSJ; WSJ+BioMed: parser trained on
WSJ and biomedical text; WSJ+BioMed+N-gram: our
approach trained on WSJ and biomedical text.
</figureCaption>
<bodyText confidence="0.998930933333333">
ysis to determine whether Google hits and Google
V1 are highly correlated. We will leave it for future
research.
(2) Veronis (2005) pointed out that there had been
a debate about reliability of Google hits due to the
inconsistencies of page hits estimates. However, this
estimate is scale-invariant. Assume that when the
number of pages indexed by Google grows, the num-
ber of pages containing a given search term goes to
a fixed fraction. This means that if pages indexed
by Google doubles, then so do the bigrams or tri-
grams frequencies. Therefore, the estimate becomes
stable when the number of indexed pages grows un-
boundedly. Some details are presented in Cilibrasi
and Vitanyi (2007).
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99998318">
Our approach is to exploit web-derived selectional
preferences to improve the dependency parsing. The
idea of this paper is inspired by the work of Suzuki
et al. (2009) and Pitler et al. (2010). The former uses
the web-scale data explicitly to create more data for
training the model; while the latter explores the web-
scale N-grams data (Lin et al., 2010) for compound
bracketing disambiguation. Our research, however,
applies the web-scale data (Google hits and Google
V1) to model the word-to-word dependency rela-
tionships rather than compound bracketing disam-
biguation.
Several previous studies have exploited the web-
scale data for word pair acquisition. Keller and
Lapata (2003) evaluated the utility of using web
search engine statistics for unseen bigram. Nakov
and Hearst (2005) demonstrated the effectiveness of
using search engine statistics to improve the noun
compound bracketing. Volk (2001) exploited the
WWW as a corpus to resolve PP attachment ambigu-
ities. Turney (2007) measured the semantic orienta-
tion for sentiment classification using co-occurrence
statistics obtained from the search engines. Bergsma
et al. (2010) created robust supervised classifiers
via web-scale N-gram data for adjective ordering,
spelling correction, noun compound bracketing and
verb part-of-speech disambiguation. Our approach,
however, extends these techniques to dependency
parsing, particularly for long dependency relation-
ships, which involves more challenging tasks than
the previous work.
Besides, there are some work exploring the word-
to-word co-occurrence derived from the web-scale
data or a fixed size of corpus (Calvo and Gel-
bukh, 2004; Calvo and Gelbukh, 2006; Yates et al.,
2006; Drabek and Zhou, 2000; van Noord, 2007)
for PP attachment ambiguities or shallow parsing.
Johnson and Riezler (2000) incorporated the lex-
ical selectional preference features derived from
British National Corpus (Graff, 2003) into a stochas-
tic unification-based grammar. Abekawa and Oku-
mura (2006) improved Japanese dependency pars-
ing by using the co-occurrence information derived
from the results of automatic dependency parsing of
large-scale corpora. However, we explore the web-
scale data for dependency parsing, the performance
improves log-linearly with the number of parameters
(unique N-grams). To the best of our knowledge,
web-derived selectional preference has not been suc-
cessfully applied to dependency parsing.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999722428571428">
In this paper, we present a novel method which in-
corporates the web-derived selectional preferences
to improve statistical dependency parsing. The re-
sults show that web-scale data improves the de-
pendency parsing, particularly for long dependency
relationships. There is no data like more data,
performance improves log-linearly with the num-
</bodyText>
<figure confidence="0.952303066666667">
88
87
86
UAS Score (%)
85
82
WSJ
WSJ+N-gram
WSJ+BioMed
WSJ+BioMed+N-gram
81
80
100 150 200 250 300 350 400 450 500
84
83
</figure>
<page confidence="0.943755">
1563
</page>
<bodyText confidence="0.99985925">
ber of parameters (unique N-grams). More impor-
tantly, when operating on new domains, the web-
derived selectional preferences show great potential
for achieving robust performance.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999895875">
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106), and CSIDM project (No. CSIDM-
200805) partially funded by a grant from the Na-
tional Research Foundation (NRF) administered by
the Media Development Authority (MDA) of Singa-
pore. We thank the anonymous reviewers for their
insightful comments.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681764705882">
T. Abekawa and M. Okumura. 2006. Japanese depen-
dency parsing using co-occurrence information and a
combination of case elements. In Proceedings ofACL-
COLING.
S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative
learning of selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59-68.
S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust
supervised classifier via web-scale N-gram data. In
Proceedings of ACL.
T. Brants and Alex Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
H. Calvo and A. Gelbukh. 2004. Acquiring selec-
tional preferences from untagged text for prepositional
phrase attachment disambiguation. In Proceedings of
VLDB.
H. Calvo and A. Gelbukh. 2006. DILUCT: An open-
source Spanish dependency parser based on rules,
heuristics, and selectional preferences. In Lecture
Notes in Computer Science 3999, pages 164-175.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of EMNLP-
CoNLL, pages 957-961.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proceedings of CoNLL.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43.Linguistic Data Consortium.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570-579.
K. W. Church and P. Hanks. 1900. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22-29.
R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google
similarity distance. IEEE Transaction on Knowledge
and Data Engineering, 19(3):2007. pages 370-383.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.
L. Bartlett. 2008. Exponentiated gradient algorithm
for conditional random fields and max-margin markov
networks. Journal of Machine Learning Research,
pages 1775–1822.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings ofACL, pages 531-540.
S. Corston-Oliver, A. Aue, Kevin. Duh, and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of NAACL.
H. Daume III. 2007. Frustrating easy domain adaptation.
In Proceedings of ACL.
E. F. Drabek and Q. Zhou. 2000. Using co-occurrence
statistics as an information source for partial parsing of
Chinese. In Proceedings of Second Chinese Language
Processing Workshop, ACL, pages 22-28.
Y. GoldBerg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of NAACL, pages 742-750.
D. Graff. 2003. English Gigaword, LDC2003T05.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discrimina-
tive classifier for deterministic dependency parsing. In
Proceedings of ACL, pages 316-323.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distribution in stochastic unification-based garmmars.
In Proceedings of NAACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL, pages 595-603.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459-484.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1), pages 1-30.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
D. K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S.
Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K.
Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics.
</reference>
<page confidence="0.872316">
1564
</page>
<reference confidence="0.999390060606061">
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings
of EMNLP, pages 157-166.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of ACL.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic Domain Adapatation for Parsing. In Proceed-
ings of NAACL-HLT.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings ofACL, pages 91-98.
P. Nakov and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: application to noun compound
bracketing. In Proceedings of CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950-958.
G. van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of IWPT, pages 1-10.
PennBioIE. 2005. Mining the bibliome project, 2005.
http:bioie.ldc.upenn.edu/.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886-
894.
P. Resnik. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. thesis,
University of Pennsylvania.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551-560.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In Proceedings of ACL, pages 665-
673.
P. D. Turney. 2003. Measuring praise and criticism:
Inference of semantic orientation from association.
ACM Transactions on Information Systems, 21(4).
J. Veronis. 2005. Web: Google adjusts its counts. Jean
Veronis’ blog: http://aixtal.blogsplot.com/2005/03/
web-google-adjusts-its-count.html.
M. Volk. 2001. Exploiting the WWW as corpus to re-
solve PP attachment ambiguities. In Proceedings of
the Corpus Linguistics.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of IJCAI, pages 1756-1762.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings
of IWPT, pages 195-206.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In Proceedings of EMNLP, pages 27-34.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562-571.
</reference>
<page confidence="0.992497">
1565
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656610">
<title confidence="0.9828765">Exploiting Web-Derived Selectional Preference to Improve Dependency Parsing</title>
<author confidence="0.995165">Jun Liu Zhou</author>
<author confidence="0.995165">Li Cai</author>
<affiliation confidence="0.9881045">National Laboratory of Pattern Institute of Automation, Chinese Academy of</affiliation>
<address confidence="0.864788">95 Zhongguancun East Road, Beijing 100190,</address>
<abstract confidence="0.98995765">In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Abekawa</author>
<author>M Okumura</author>
</authors>
<title>Japanese dependency parsing using co-occurrence information and a combination of case elements.</title>
<date>2006</date>
<booktitle>In Proceedings ofACLCOLING.</booktitle>
<contexts>
<context position="29507" citStr="Abekawa and Okumura (2006)" startWordPosition="4686" endWordPosition="4690">o dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our knowledge, web-derived selectional preference has not been successfully applied to dependency parsing. 6 Conclusion In this paper, we present a novel method which incorporates the web-derived selectional preferences to improve statistical dependency parsing. The results show</context>
</contexts>
<marker>Abekawa, Okumura, 2006</marker>
<rawString>T. Abekawa and M. Okumura. 2006. Japanese dependency parsing using co-occurrence information and a combination of case elements. In Proceedings ofACLCOLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Lin</author>
<author>R Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="9279" citStr="Bergsma et al., 2008" startWordPosition="1419" endWordPosition="1422"> are approximated by Google hits. Another we use is Google V1(Brants and Franz, 2006). This N-gram corpus records how often each unique sequence of words occurs. N-grams appearing 40 2http://groups.csail.mit.edu/nlp/egstra/ 3This kind of feature sets are similar to other feature sets in the literature (McDonald et al., 2005; Carreras, 2007), so we will not attempt to give a exhaustive description. 4Selectional preference tells us which arguments are plausible for a particular predicate, one way to determine the selectional preference is from co-occurrences of predicates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meaning with N-grams, which model the word-to-word relationships, rather than only considering the predicates and arguments relationships. Figure 2: An example of a labeled dependency tree. The tree contains a special token “$” which is always the root of the tree. Each arc is directed from head to modifier and has a label describing the function of the attachment. times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directl</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of EMNLP, pages 59-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>E Pitler</author>
<author>D Lin</author>
</authors>
<title>Creating robust supervised classifier via web-scale N-gram data.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="23125" citStr="Bergsma et al., 2010" startWordPosition="3660" endWordPosition="3663">smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13Google indexes about more than 8 billion pages and each contains about 1,000 words on average. 1561 Corpus # of tokens 0 # of types NEWS 3.2B 1 3.7B Google V1 1,024.9B 40 3.4B Google hits13 8,000B 100 - Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data 0, and total number of unique N-gram (types) remaining in th</context>
<context position="28657" citStr="Bergsma et al. (2010)" startWordPosition="4566" endWordPosition="4569">ationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP</context>
</contexts>
<marker>Bergsma, Pitler, Lin, 2010</marker>
<rawString>S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust supervised classifier via web-scale N-gram data. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>The Google Web 1T 5-gram Corpus Version</booktitle>
<volume>1</volume>
<pages>2006--13</pages>
<contexts>
<context position="4221" citStr="Brants and Franz, 2006" startWordPosition="632" endWordPosition="635">Computational Linguistics Dependency Length Figure 1: F score relative to dependency length. beyond the annotated corpora are needed to capture the bi-lexical relationship at the word-to-word level. Our purpose in this paper is to exploit webderived selectional preferences to improve the supervised statistical dependency parsing. All of our lexical statistics are derived from two kinds of webscale corpus: one is the web, which is the largest data set that is available for NLP (Keller and Lapata, 2003). Another is a web-scale N-gram corpus, which is a N-gram corpus with N-grams of length 1- 5 (Brants and Franz, 2006), we call it Google V1 in this paper. The idea is very simple: web-scale data have large coverage for word pair acquisition. By leveraging some assistant data, the dependency parsing model can directly utilize the additional information to capture the word-to-word level relationships. We address two natural and related questions which some previous studies leave open: Question I: Is there a benefit in incorporating web-derived selectional preference features for statistical dependency parsing, especially for longer dependencies? Question II: How well do web-derived selectional preferences perf</context>
<context position="8743" citStr="Brants and Franz, 2006" startWordPosition="1339" endWordPosition="1342">ent feature sets: a baseline feature seta which draw upon “normal” information source, such as word forms and part-ofspeech (POS) without including the web-derived selectional preference4 features, a feature set conjoins the baseline features and the web-derived selectional preference features. 3.1 Web-scale resources All of our selectional preference features described in this paper rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale resources. one is web, Ngram counts are approximated by Google hits. Another we use is Google V1(Brants and Franz, 2006). This N-gram corpus records how often each unique sequence of words occurs. N-grams appearing 40 2http://groups.csail.mit.edu/nlp/egstra/ 3This kind of feature sets are similar to other feature sets in the literature (McDonald et al., 2005; Carreras, 2007), so we will not attempt to give a exhaustive description. 4Selectional preference tells us which arguments are plausible for a particular predicate, one way to determine the selectional preference is from co-occurrences of predicates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meani</context>
<context position="23591" citStr="Brants and Franz, 2006" startWordPosition="3744" endWordPosition="3747">rams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13Google indexes about more than 8 billion pages and each contains about 1,000 words on average. 1561 Corpus # of tokens 0 # of types NEWS 3.2B 1 3.7B Google V1 1,024.9B 40 3.4B Google hits13 8,000B 100 - Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data 0, and total number of unique N-gram (types) remaining in the data. Number of Unique N-grams Figure 3: There is no data like more data. UAS accuracy improves with the number of unique N-grams but still lower than the Google hits. 4.2 Improvement relative to dependency length The experiments in (McDonald and Nivre, 2007) showed a negative impact on the dependency parsing performance from too long dependencies. For our proposed approach, the improvement relative to dependency length is shown in Figure 4. From the Figure, i</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and Alex Franz. 2006. The Google Web 1T 5-gram Corpus Version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Calvo</author>
<author>A Gelbukh</author>
</authors>
<title>Acquiring selectional preferences from untagged text for prepositional phrase attachment disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of VLDB.</booktitle>
<contexts>
<context position="29164" citStr="Calvo and Gelbukh, 2004" startWordPosition="4635" endWordPosition="4639"> for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly</context>
</contexts>
<marker>Calvo, Gelbukh, 2004</marker>
<rawString>H. Calvo and A. Gelbukh. 2004. Acquiring selectional preferences from untagged text for prepositional phrase attachment disambiguation. In Proceedings of VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Calvo</author>
<author>A Gelbukh</author>
</authors>
<title>DILUCT: An opensource Spanish dependency parser based on rules, heuristics, and selectional preferences.</title>
<date>2006</date>
<booktitle>In Lecture Notes in Computer Science 3999,</booktitle>
<pages>164--175</pages>
<contexts>
<context position="29189" citStr="Calvo and Gelbukh, 2006" startWordPosition="4640" endWordPosition="4643">tion using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of param</context>
</contexts>
<marker>Calvo, Gelbukh, 2006</marker>
<rawString>H. Calvo and A. Gelbukh. 2006. DILUCT: An opensource Spanish dependency parser based on rules, heuristics, and selectional preferences. In Lecture Notes in Computer Science 3999, pages 164-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="6949" citStr="Carreras, 2007" startWordPosition="1049" endWordPosition="1050">s follows. Section 2 gives a brief introduction of dependency parsing. Section 3 describes the web-derived selectional preference features. Experimental evaluation and results are reported in Section 4. Finally, we discuss related work and draw conclusion in Section 5 and Section 6, respectively. 2 Dependency Parsing In dependency parsing, we attempt to build headmodifier (or head-dependent) relations between words in a sentence. The discriminative parser we used in this paper is based on the part-factored model and features of the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007). The parsing model can be defined as a conditional distribution p(y|x; w) over each projective parse tree y for a particular sentence x, parameterized by a vector w. The probability of a parse tree is p(y|x; w) = 1exp{∑w · 4)(x, P) } (1) Z(x; w) P�� where Z(x; w) is the partition function and 4) are part-factored feature functions that include head1 MST1 MST2 1 5 10 15 20 25 30 0.95 0.9 0.85 0.8 0.75 0.7 F1 Score (%) 1557 modifier parts, sibling parts and grandchild parts. Given the training set {(xi, yi)}Ni=1, parameter estimation for log-linear models generally resolve around optimization o</context>
<context position="9000" citStr="Carreras, 2007" startWordPosition="1377" endWordPosition="1378">lectional preference features. 3.1 Web-scale resources All of our selectional preference features described in this paper rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale resources. one is web, Ngram counts are approximated by Google hits. Another we use is Google V1(Brants and Franz, 2006). This N-gram corpus records how often each unique sequence of words occurs. N-grams appearing 40 2http://groups.csail.mit.edu/nlp/egstra/ 3This kind of feature sets are similar to other feature sets in the literature (McDonald et al., 2005; Carreras, 2007), so we will not attempt to give a exhaustive description. 4Selectional preference tells us which arguments are plausible for a particular predicate, one way to determine the selectional preference is from co-occurrences of predicates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meaning with N-grams, which model the word-to-word relationships, rather than only considering the predicates and arguments relationships. Figure 2: An example of a labeled dependency tree. The tree contains a special token “$” which is always the root of the tr</context>
<context position="17577" citStr="Carreras, 2007" startWordPosition="2774" endWordPosition="2775">gh strength of selectional score (UAS), labeled attachment score (LAS) and preference between ball and stripe. complete match (CM), which were defined by Hall Web-derived selectional preference features et al. (2006). All the metrics are calculated as mean based on PMI values are trickier to incorporate scores per word, and punctuation tokens are consisinto the dependency parsing model because they tently excluded. are continuous rather than discrete. Since all the 4.1 Main results baseline features used in the literature (McDonald et There are some clear trends in the results of Taal., 2005; Carreras, 2007) take on binary values of 0 ble 3. First, performance increases with the order or 1, there is a “mis-match” between the continuous of the parser: edge-factored model (dep1) has the and binary features. Log-linear dependency parsing lowest performance, adding sibling and grandchild model is sensitive to inappropriately scaled feature. relationships (dep2) significantly increases perforTo solve this problem, we transform the PMI mance. Similar observations regarding the effect of values into a more amenable form by replacing the model order have also been made by Carreras (2007) PMI values with </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of EMNLPCoNLL, pages 957-961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="20512" citStr="Carreras et al. (2008)" startWordPosition="3218" endWordPosition="3221">eled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspec</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>D Blaheta</author>
<author>N Ge</author>
<author>K Hall</author>
<author>M Johnson</author>
</authors>
<date>2000</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1, LDC No. LDC2000T43.Linguistic Data Consortium.</booktitle>
<marker>Charniak, Blaheta, Ge, Hall, Johnson, 2000</marker>
<rawString>E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson. 2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC No. LDC2000T43.Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>D Kawahara</author>
<author>K Uchimoto</author>
<author>Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="20762" citStr="Chen et al. (2009)" startWordPosition="3265" endWordPosition="3268">res and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate comparisons with previous work, we only use Section 23 as the test data. The results show that our second order model incorporating the N-gram features (92.64) pe</context>
</contexts>
<marker>Chen, Kawahara, Uchimoto, Torisawa, 2009</marker>
<rawString>W. Chen, D. Kawahara, K. Uchimoto, and Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings of EMNLP, pages 570-579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1900</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<contexts>
<context position="10257" citStr="Church and Hanks, 1900" startWordPosition="1574" endWordPosition="1577"> to modifier and has a label describing the function of the attachment. times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics between pairs of words. In this paper we generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) (3) p(“x”)p(“y”) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks.6 The value of these features is the PMI, if it is defined. If the PMI is undefined, following the work of (Pitler et al., 2010), we include one of two binary features: p(“x y”) = 0 or p(“x”) V</context>
</contexts>
<marker>Church, Hanks, 1900</marker>
<rawString>K. W. Church and P. Hanks. 1900. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Cilibrasi</author>
<author>P M B Vitanyi</author>
</authors>
<title>The Google similarity distance.</title>
<date>2007</date>
<journal>IEEE Transaction on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>370--383</pages>
<contexts>
<context position="27505" citStr="Cilibrasi and Vitanyi (2007)" startWordPosition="4390" endWordPosition="4393">ly correlated. We will leave it for future research. (2) Veronis (2005) pointed out that there had been a debate about reliability of Google hits due to the inconsistencies of page hits estimates. However, this estimate is scale-invariant. Assume that when the number of pages indexed by Google grows, the number of pages containing a given search term goes to a fixed fraction. This means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit web-derived selectional preferences to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several pr</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google similarity distance. IEEE Transaction on Knowledge and Data Engineering, 19(3):2007. pages 370-383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>P L Bartlett</author>
</authors>
<title>Exponentiated gradient algorithm for conditional random fields and max-margin markov networks.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1775--1822</pages>
<contexts>
<context position="8037" citStr="Collins et al., 2008" startWordPosition="1231" endWordPosition="1234">hild parts. Given the training set {(xi, yi)}Ni=1, parameter estimation for log-linear models generally resolve around optimization of a regularized conditional log-likelihood objective w* = argminK,L(w) where logp(yiJxi; w) + 2JJwJJ2 (2) 1 The parameter C &gt; 0 is a constant dictating the level of regularization in the model. Since objective function L(w) is smooth and convex, which is convenient for standard gradient-based optimization techniques. In this paper we use the dual exponentiated gradient (EG)2 descent, which is a particularly effective optimization algorithm for log-linear models (Collins et al., 2008). 3 Web-Derived Selectional Preference Features In this paper, we employ two different feature sets: a baseline feature seta which draw upon “normal” information source, such as word forms and part-ofspeech (POS) without including the web-derived selectional preference4 features, a feature set conjoins the baseline features and the web-derived selectional preference features. 3.1 Web-scale resources All of our selectional preference features described in this paper rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale resources. one</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. 2008. Exponentiated gradient algorithm for conditional random fields and max-margin markov networks. Journal of Machine Learning Research, pages 1775–1822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="20249" citStr="Collins et al. (2005)" startWordPosition="3170" endWordPosition="3173">ser with the baseline features; +hits=N-gram features derived from the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL, pages 531-540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duh</author>
<author>E Ringger</author>
</authors>
<title>Multilingual dependency parsing using bayes point machines.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Duh, Ringger, 2006</marker>
<rawString>S. Corston-Oliver, A. Aue, Kevin. Duh, and E. Ringger. 2006. Multilingual dependency parsing using bayes point machines. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustrating easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>H. Daume III. 2007. Frustrating easy domain adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Drabek</author>
<author>Q Zhou</author>
</authors>
<title>Using co-occurrence statistics as an information source for partial parsing of Chinese.</title>
<date>2000</date>
<booktitle>In Proceedings of Second Chinese Language Processing Workshop, ACL,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="29232" citStr="Drabek and Zhou, 2000" startWordPosition="4648" endWordPosition="4651">from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our </context>
</contexts>
<marker>Drabek, Zhou, 2000</marker>
<rawString>E. F. Drabek and Q. Zhou. 2000. Using co-occurrence statistics as an information source for partial parsing of Chinese. In Proceedings of Second Chinese Language Processing Workshop, ACL, pages 22-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y GoldBerg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="20547" citStr="GoldBerg and Elhadad (2010)" startWordPosition="3224" endWordPosition="3227">ng is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with oth</context>
</contexts>
<marker>GoldBerg, Elhadad, 2010</marker>
<rawString>Y. GoldBerg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proceedings of NAACL, pages 742-750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<date>2003</date>
<note>English Gigaword, LDC2003T05.</note>
<contexts>
<context position="22292" citStr="Graff, 2003" startWordPosition="3520" endWordPosition="3521">better than Nivre and McDonald (2008) and Zhang and Clark (2008), but a slightly worse than Martins et al. (2008). We also compare our method with the semi-supervised approaches, the semi-supervised approaches achieved very high accuracies by leveraging on large unlabeled data directly into the systems for joint learning and decoding, while in our method, we only explore the Ngram features to further improve supervised dependency parsing performance. Table 5 shows the details of some other N-gram sources, where NEWS: created from a large set of news articles including the Reuters and Gigword (Graff, 2003) corpora. For a given number of unique N-gram, using any of these sources does not have significant difference in Figure 3. Google hits is the largest N-gram data and shows the best performance. The other two are smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Althoug</context>
<context position="29435" citStr="Graff, 2003" startWordPosition="4678" endWordPosition="4679">guation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our knowledge, web-derived selectional preference has not been successfully applied to dependency parsing. 6 Conclusion In this paper, we present a novel method which incorporates the web-derived selectional</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword, LDC2003T05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Discriminative classifier for deterministic dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="1640" citStr="Hall et al., 2006" startWordPosition="232" endWordPosition="235">show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − i|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score&apos; relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note that the parsers provide very good results for adjace</context>
<context position="20450" citStr="Hall et al. (2006)" startWordPosition="3206" endWordPosition="3209">rent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemente</context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2006</marker>
<rawString>J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative classifier for deterministic dependency parsing. In Proceedings of ACL, pages 316-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Riezler</author>
</authors>
<title>Exploiting auxiliary distribution in stochastic unification-based garmmars.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="29327" citStr="Johnson and Riezler (2000)" startWordPosition="4662" endWordPosition="4665">eb-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our knowledge, web-derived selectional preference has not been successfully applied to dependency p</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>M. Johnson and S. Riezler. 2000. Exploiting auxiliary distribution in stochastic unification-based garmmars. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2905" citStr="Koo et al. (2008)" startWordPosition="433" endWordPosition="436">), while the dependency length increases, the accuracies degrade sharply. These longer dependencies are therefore a major opportunity to improve the overall performance of dependency parsing. Usually, these longer dependencies can be parsed dependent on the specific words involved due to the limited range of features (e.g., a verb and its modifiers). Lexical statistics are therefore needed for resolving ambiguous relationships, yet the lexicalized statistics are sparse and difficult to estimate directly. To solve this problem, some information with different granularity has been investigated. Koo et al. (2008) proposed a semi-supervised dependency parsing by introducing lexical intermediaries at a coarser level than words themselves via a cluster method. This approach, however, ignores the selectional preference for word-to-word interactions, such as head-modifier relationship. Extra resources &apos;Precision represents the percentage of predicted arcs of length d that are correct, and recall measures the percentage of gold-standard arcs of length d that are correctly predicted. Fl = 2 x precision x recall/(precision + recall) 1556 Proceedings of the 49th Annual Meeting of the Association for Computatio</context>
<context position="18230" citStr="Koo et al. (2008)" startWordPosition="2874" endWordPosition="2877">irst, performance increases with the order or 1, there is a “mis-match” between the continuous of the parser: edge-factored model (dep1) has the and binary features. Log-linear dependency parsing lowest performance, adding sibling and grandchild model is sensitive to inappropriately scaled feature. relationships (dep2) significantly increases perforTo solve this problem, we transform the PMI mance. Similar observations regarding the effect of values into a more amenable form by replacing the model order have also been made by Carreras (2007) PMI values with their z-score. The z-score of a and Koo et al. (2008). PMI value x is x�� Second, note that the parsers incorporating the N� , where p and Q are the mean gram feature sets consistently outperform the modand standard deviation of the PMI distribution, els using the baseline features in all test data sets, respectively. regardless of model order or label usage. Another 4 Experiments In order to evaluate the effectiveness of our proposed 8http://w3.msi.vxu.se/ nivre/research/MaltXML.html approach, we conducted dependency parsing exper- 9We removed a single 249-word sentence from Section 0 for iments in English. The experiments were performed comput</context>
<context position="20706" citStr="Koo et al. (2008)" startWordPosition="3253" endWordPosition="3256">the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate comparisons with previous work, we only use Section 23 as the test data. The results show that our second </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL, pages 595-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--3</pages>
<contexts>
<context position="4104" citStr="Keller and Lapata, 2003" startWordPosition="610" endWordPosition="614">ssociation for Computational Linguistics, pages 1556–1565, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Dependency Length Figure 1: F score relative to dependency length. beyond the annotated corpora are needed to capture the bi-lexical relationship at the word-to-word level. Our purpose in this paper is to exploit webderived selectional preferences to improve the supervised statistical dependency parsing. All of our lexical statistics are derived from two kinds of webscale corpus: one is the web, which is the largest data set that is available for NLP (Keller and Lapata, 2003). Another is a web-scale N-gram corpus, which is a N-gram corpus with N-grams of length 1- 5 (Brants and Franz, 2006), we call it Google V1 in this paper. The idea is very simple: web-scale data have large coverage for word pair acquisition. By leveraging some assistant data, the dependency parsing model can directly utilize the additional information to capture the word-to-word level relationships. We address two natural and related questions which some previous studies leave open: Question I: Is there a benefit in incorporating web-derived selectional preference features for statistical depe</context>
<context position="28204" citStr="Keller and Lapata (2003)" startWordPosition="4500" endWordPosition="4503">nces to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb par</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>F. Keller and M. Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>1--30</pages>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata and F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1), pages 1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10064" citStr="Lauer, 1995" startWordPosition="1545" endWordPosition="1546">arguments relationships. Figure 2: An example of a labeled dependency tree. The tree contains a special token “$” which is always the root of the tree. Each arc is directed from head to modifier and has a label describing the function of the attachment. times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics between pairs of words. In this paper we generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) (3) p(“x”)p(“y”) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks.</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Corpus statistics meet the noun compound: some empirical results. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Lin</author>
<author>H Church</author>
<author>S Ji</author>
<author>S Sekine</author>
<author>D Yarowsky</author>
<author>S Bergsma</author>
<author>K Patil</author>
<author>E Pitler</author>
<author>E Lathbury</author>
<author>V Rao</author>
<author>K Dalwani</author>
<author>S Narsale</author>
</authors>
<title>New tools for webscale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="27877" citStr="Lin et al., 2010" startWordPosition="4454" endWordPosition="4457"> means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit web-derived selectional preferences to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachm</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, Patil, Pitler, Lathbury, Rao, Dalwani, Narsale, 2010</marker>
<rawString>D. K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S. Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K. Dalwani, and S. Narsale. 2010. New tools for webscale n-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="1411" citStr="Marcus et al., 1993" startWordPosition="198" endWordPosition="201">ndency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − i|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows </context>
<context position="5974" citStr="Marcus et al., 1993" startWordPosition="904" endWordPosition="907">s. Some studies have investigated domain adaptation for parsers (McClosky et al., 2006; Daume III, 2007; McClosky et al., 2010). These approaches assume that the parsers know which domain it is used, and that it has access to representative data in that domain. However, in practice, these assumptions are unrealistic in many real applications, such as when processing the heterogeneous genre of web texts. In this paper we incorporate the web-derived selectional preference features to design our parsers for robust opendomain testing. We conduct the experiments on the English Penn Treebank (PTB) (Marcus et al., 1993). The results show that web-derived selectional preference can improve the statistical dependency parsing, particularly for long dependency relationships. More importantly, when operating on new domains, the webderived selectional preference features show great potential for achieving robust performance (Section 4.3). The remainder of this paper is divided as follows. Section 2 gives a brief introduction of dependency parsing. Section 3 describes the web-derived selectional preference features. Experimental evaluation and results are reported in Section 4. Finally, we discuss related work and </context>
<context position="18895" citStr="Marcus et al., 1993" startWordPosition="2976" endWordPosition="2979">rsers incorporating the N� , where p and Q are the mean gram feature sets consistently outperform the modand standard deviation of the PMI distribution, els using the baseline features in all test data sets, respectively. regardless of model order or label usage. Another 4 Experiments In order to evaluate the effectiveness of our proposed 8http://w3.msi.vxu.se/ nivre/research/MaltXML.html approach, we conducted dependency parsing exper- 9We removed a single 249-word sentence from Section 0 for iments in English. The experiments were performed computational reasons. on the Penn Treebank (PTB) (Marcus et al., 1993), 10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html using a standard set of head-selection rules (Yamada 11http://www.google.com/ 1560 12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html. Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L 00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.42 01 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.37 23 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.77 24 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89 Table 3: Unlabeled a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>D Das</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="20640" citStr="Martins et al. (2008)" startWordPosition="3240" endWordPosition="3243">he large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate comparisons with previous work, we only </context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing. 2008. Stacking dependency parsers. In Proceedings of EMNLP, pages 157-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5440" citStr="McClosky et al., 2006" startWordPosition="816" endWordPosition="820">rform on new domains? For Question I, we systematically assess the value of using web-scale data in state-of-the-art supervised dependency parsers. We compare dependency parsers that include or exclude selectional preference features obtained from web-scale corpus. To the best of our knowledge, none of the existing studies directly address long dependencies of dependency parsing by using web-scale data. Most statistical parsers are highly domain dependent. For example, the parsers trained on WSJ text perform poorly on Brown corpus. Some studies have investigated domain adaptation for parsers (McClosky et al., 2006; Daume III, 2007; McClosky et al., 2010). These approaches assume that the parsers know which domain it is used, and that it has access to representative data in that domain. However, in practice, these assumptions are unrealistic in many real applications, such as when processing the heterogeneous genre of web texts. In this paper we incorporate the web-derived selectional preference features to design our parsers for robust opendomain testing. We conduct the experiments on the English Penn Treebank (PTB) (Marcus et al., 1993). The results show that web-derived selectional preference can imp</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Automatic Domain Adapatation for Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="5481" citStr="McClosky et al., 2010" startWordPosition="824" endWordPosition="827"> systematically assess the value of using web-scale data in state-of-the-art supervised dependency parsers. We compare dependency parsers that include or exclude selectional preference features obtained from web-scale corpus. To the best of our knowledge, none of the existing studies directly address long dependencies of dependency parsing by using web-scale data. Most statistical parsers are highly domain dependent. For example, the parsers trained on WSJ text perform poorly on Brown corpus. Some studies have investigated domain adaptation for parsers (McClosky et al., 2006; Daume III, 2007; McClosky et al., 2010). These approaches assume that the parsers know which domain it is used, and that it has access to representative data in that domain. However, in practice, these assumptions are unrealistic in many real applications, such as when processing the heterogeneous genre of web texts. In this paper we incorporate the web-derived selectional preference features to design our parsers for robust opendomain testing. We conduct the experiments on the English Penn Treebank (PTB) (Marcus et al., 1993). The results show that web-derived selectional preference can improve the statistical dependency parsing, </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2010. Automatic Domain Adapatation for Parsing. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1764" citStr="McDonald and Nivre, 2007" startWordPosition="247" endWordPosition="251">endency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − i|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score&apos; relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note that the parsers provide very good results for adjacent dependencies (96.89% for dependency length =1), while the dependency length increases, the accuracies degrade sharply. Th</context>
<context position="23986" citStr="McDonald and Nivre, 2007" startWordPosition="3812" endWordPosition="3815">561 Corpus # of tokens 0 # of types NEWS 3.2B 1 3.7B Google V1 1,024.9B 40 3.4B Google hits13 8,000B 100 - Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data 0, and total number of unique N-gram (types) remaining in the data. Number of Unique N-grams Figure 3: There is no data like more data. UAS accuracy improves with the number of unique N-grams but still lower than the Google hits. 4.2 Improvement relative to dependency length The experiments in (McDonald and Nivre, 2007) showed a negative impact on the dependency parsing performance from too long dependencies. For our proposed approach, the improvement relative to dependency length is shown in Figure 4. From the Figure, it is seen that our method gives observable better performance when dependency lengths are larger than 3. The results here show that the proposed approach improves the dependency parsing performance, particularly for long dependency relationships. 4.3 Cross-genre testing In this section, we present the experiments to validate the robustness the web-derived selectional preferences. The intent i</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1620" citStr="McDonald and Pereira, 2006" startWordPosition="227" endWordPosition="231">perating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − i|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score&apos; relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note that the parsers provide very goo</context>
<context position="6932" citStr="McDonald and Pereira, 2006" startWordPosition="1045" endWordPosition="1048">r of this paper is divided as follows. Section 2 gives a brief introduction of dependency parsing. Section 3 describes the web-derived selectional preference features. Experimental evaluation and results are reported in Section 4. Finally, we discuss related work and draw conclusion in Section 5 and Section 6, respectively. 2 Dependency Parsing In dependency parsing, we attempt to build headmodifier (or head-dependent) relations between words in a sentence. The discriminative parser we used in this paper is based on the part-factored model and features of the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007). The parsing model can be defined as a conditional distribution p(y|x; w) over each projective parse tree y for a particular sentence x, parameterized by a vector w. The probability of a parse tree is p(y|x; w) = 1exp{∑w · 4)(x, P) } (1) Z(x; w) P�� where Z(x; w) is the partition function and 4) are part-factored feature functions that include head1 MST1 MST2 1 5 10 15 20 25 30 0.95 0.9 0.85 0.8 0.75 0.7 F1 Score (%) 1557 modifier parts, sibling parts and grandchild parts. Given the training set {(xi, yi)}Ni=1, parameter estimation for log-linear models generally resolve arou</context>
<context position="20382" citStr="McDonald and Pereira (2006)" startWordPosition="3194" endWordPosition="3197"> V1; suffix-L=labeled parser. Unlabeled parsers are scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers we</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1592" citStr="McDonald et al., 2005" startWordPosition="223" endWordPosition="226">ore importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − i|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score&apos; relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note that </context>
<context position="6904" citStr="McDonald et al., 2005" startWordPosition="1041" endWordPosition="1044">tion 4.3). The remainder of this paper is divided as follows. Section 2 gives a brief introduction of dependency parsing. Section 3 describes the web-derived selectional preference features. Experimental evaluation and results are reported in Section 4. Finally, we discuss related work and draw conclusion in Section 5 and Section 6, respectively. 2 Dependency Parsing In dependency parsing, we attempt to build headmodifier (or head-dependent) relations between words in a sentence. The discriminative parser we used in this paper is based on the part-factored model and features of the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007). The parsing model can be defined as a conditional distribution p(y|x; w) over each projective parse tree y for a particular sentence x, parameterized by a vector w. The probability of a parse tree is p(y|x; w) = 1exp{∑w · 4)(x, P) } (1) Z(x; w) P�� where Z(x; w) is the partition function and 4) are part-factored feature functions that include head1 MST1 MST2 1 5 10 15 20 25 30 0.95 0.9 0.85 0.8 0.75 0.7 F1 Score (%) 1557 modifier parts, sibling parts and grandchild parts. Given the training set {(xi, yi)}Ni=1, parameter estimation for log-linear m</context>
<context position="8983" citStr="McDonald et al., 2005" startWordPosition="1373" endWordPosition="1376"> and the web-derived selectional preference features. 3.1 Web-scale resources All of our selectional preference features described in this paper rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale resources. one is web, Ngram counts are approximated by Google hits. Another we use is Google V1(Brants and Franz, 2006). This N-gram corpus records how often each unique sequence of words occurs. N-grams appearing 40 2http://groups.csail.mit.edu/nlp/egstra/ 3This kind of feature sets are similar to other feature sets in the literature (McDonald et al., 2005; Carreras, 2007), so we will not attempt to give a exhaustive description. 4Selectional preference tells us which arguments are plausible for a particular predicate, one way to determine the selectional preference is from co-occurrences of predicates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meaning with N-grams, which model the word-to-word relationships, rather than only considering the predicates and arguments relationships. Figure 2: An example of a labeled dependency tree. The tree contains a special token “$” which is always t</context>
<context position="14817" citStr="McDonald et al. (2005)" startWordPosition="2324" endWordPosition="2327">od choice. While in sentence (2), the features include PMIwith(hit, 3.3 N-gram feature templates We generate N-gram features by mimicking the template structure of the original baseline features. For example, the baseline feature set includes indicators for word-to-word and tag-to-tag interactions between the head and modifier of a dependency. In the N-gram feature set, we correspondingly introduce N-gram PMI for word-to-word interactions. stripe) and PMIwith(ball, stripe). 1559 The N-gram feature set for MSTParser is shown and Matsumoto, 2003) to convert the phrase strucin Table 2. Following McDonald et al. (2005), ture syntax of the Treebank into a dependency tree all features are conjoined with the direction of representation, dependency labels were obtained via attachment as well as the distance between the two the ”Malt” hard-coded setting.8 We split the Treewords creating the dependency. In between N-gram bank into a training set (Sections 2-21), a develfeatures, we include the form of word trigrams opment set (Section 22), and several test sets (Secand PMI of the trigrams. The surrounding word tions 0,9 1, 23, and 24). The part-of-speech tags for N-gram features represent the local context of the</context>
<context position="20344" citStr="McDonald et al. (2005)" startWordPosition="3188" endWordPosition="3191"> features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised sy</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>M Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="28307" citStr="Nakov and Hearst (2005)" startWordPosition="4516" endWordPosition="4519">(2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, part</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>P. Nakov and M. Hearst. 2005. Search engine statistics beyond the n-gram: application to noun compound bracketing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="20605" citStr="Nivre and McDonald (2008)" startWordPosition="3234" endWordPosition="3237">slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate compa</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proceedings of ACL, pages 950-958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>1--10</pages>
<marker>van Noord, 2007</marker>
<rawString>G. van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of IWPT, pages 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PennBioIE</author>
</authors>
<title>Mining the bibliome project,</title>
<date>2005</date>
<note>http:bioie.ldc.upenn.edu/.</note>
<marker>PennBioIE, 2005</marker>
<rawString>PennBioIE. 2005. Mining the bibliome project, 2005. http:bioie.ldc.upenn.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>S Bergsma</author>
<author>D Lin</author>
<author>K Church</author>
</authors>
<title>Using web-scale N-grams to improve base NP parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>886--894</pages>
<contexts>
<context position="10792" citStr="Pitler et al., 2010" startWordPosition="1668" endWordPosition="1671">endency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) (3) p(“x”)p(“y”) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks.6 The value of these features is the PMI, if it is defined. If the PMI is undefined, following the work of (Pitler et al., 2010), we include one of two binary features: p(“x y”) = 0 or p(“x”) V p(“y”) = 0 Besides, we also consider the trigram features be5http://www.google.com/ 6Google only allows automated querying through the Google Web API, this involves obtaining a license key, which then restricts the number of queries to a daily quota of 1000. However, we obtained a quota of 20,000 queries per day by sending a request to api-support@google.com for research purposes. obi det det root subi mod obi L(w) = −C ∑N i=1 1558 PMI(“hit with”) xi-word=“hit”, xj-word=“with”, PMI(“hit with”) xi-word=“hit”, xj-word=“with”, xj-p</context>
<context position="22670" citStr="Pitler et al. (2010)" startWordPosition="3584" endWordPosition="3587">ore the Ngram features to further improve supervised dependency parsing performance. Table 5 shows the details of some other N-gram sources, where NEWS: created from a large set of news articles including the Reuters and Gigword (Graff, 2003) corpora. For a given number of unique N-gram, using any of these sources does not have significant difference in Figure 3. Google hits is the largest N-gram data and shows the best performance. The other two are smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13Googl</context>
<context position="27715" citStr="Pitler et al. (2010)" startWordPosition="4426" endWordPosition="4429">e is scale-invariant. Assume that when the number of pages indexed by Google grows, the number of pages containing a given search term goes to a fixed fraction. This means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit web-derived selectional preferences to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonst</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Using web-scale N-grams to improve base NP parsing performance. In Proceedings of COLING, pages 886-894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and information: a classbased approach to lexical relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10029" citStr="Resnik, 1993" startWordPosition="1539" endWordPosition="1540">only considering the predicates and arguments relationships. Figure 2: An example of a labeled dependency tree. The tree contains a special token “$” which is always the root of the tree. Each arc is directed from head to modifier and has a label describing the function of the attachment. times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics between pairs of words. In this paper we generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) (3) p(“x”)p(“y”) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exac</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Selection and information: a classbased approach to lexical relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="20735" citStr="Suzuki et al. (2009)" startWordPosition="3259" endWordPosition="3262">e integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate comparisons with previous work, we only use Section 23 as the test data. The results show that our second order model incorporating the</context>
<context position="23103" citStr="Suzuki et al., 2009" startWordPosition="3656" endWordPosition="3659">e. The other two are smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13Google indexes about more than 8 billion pages and each contains about 1,000 words on average. 1561 Corpus # of tokens 0 # of types NEWS 3.2B 1 3.7B Google V1 1,024.9B 40 3.4B Google hits13 8,000B 100 - Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data 0, and total number of unique N-gram (</context>
<context position="27690" citStr="Suzuki et al. (2009)" startWordPosition="4421" endWordPosition="4424">es. However, this estimate is scale-invariant. Assume that when the number of pages indexed by Google grows, the number of pages containing a given search term goes to a fixed fraction. This means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit web-derived selectional preferences to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov </context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proceedings of EMNLP, pages 551-560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="23082" citStr="Suzuki and Isozaki, 2008" startWordPosition="3652" endWordPosition="3655"> shows the best performance. The other two are smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13Google indexes about more than 8 billion pages and each contains about 1,000 words on average. 1561 Corpus # of tokens 0 # of types NEWS 3.2B 1 3.7B Google V1 1,024.9B 40 3.4B Google hits13 8,000B 100 - Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data 0, and total numb</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>J. Suzuki and H. Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In Proceedings of ACL, pages 665-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>Turney, 2003</marker>
<rawString>P. D. Turney. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
</authors>
<title>Web: Google adjusts its counts.</title>
<date>2005</date>
<note>Jean Veronis’ blog: http://aixtal.blogsplot.com/2005/03/ web-google-adjusts-its-count.html.</note>
<contexts>
<context position="26948" citStr="Veronis (2005)" startWordPosition="4299" endWordPosition="4300">lation analUAS Score (%) 92.7 92.6 92.5 92.4 92.3 92.2 92.1 91.9 1e4 1e5 1e6 1e7 1e8 1e9 92 NEWS Google V1 Google hits 1 10 20 30 F1 Score (%) 1 0.95 0.9 0.85 0.8 0.75 MST2 MST2+N-gram 1562 Figure 5: Adapting a WSJ parser to biomedical text. WSJ: performance of parser trained only on WSJ; WSJ+N-gram: performance of our proposed approach trained only on WSJ; WSJ+BioMed: parser trained on WSJ and biomedical text; WSJ+BioMed+N-gram: our approach trained on WSJ and biomedical text. ysis to determine whether Google hits and Google V1 are highly correlated. We will leave it for future research. (2) Veronis (2005) pointed out that there had been a debate about reliability of Google hits due to the inconsistencies of page hits estimates. However, this estimate is scale-invariant. Assume that when the number of pages indexed by Google grows, the number of pages containing a given search term goes to a fixed fraction. This means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit</context>
</contexts>
<marker>Veronis, 2005</marker>
<rawString>J. Veronis. 2005. Web: Google adjusts its counts. Jean Veronis’ blog: http://aixtal.blogsplot.com/2005/03/ web-google-adjusts-its-count.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Volk</author>
</authors>
<title>Exploiting the WWW as corpus to resolve PP attachment ambiguities.</title>
<date>2001</date>
<booktitle>In Proceedings of the Corpus Linguistics.</booktitle>
<contexts>
<context position="28425" citStr="Volk (2001)" startWordPosition="4534" endWordPosition="4535">he latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, the</context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>M. Volk. 2001. Exploiting the WWW as corpus to resolve PP attachment ambiguities. In Proceedings of the Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q I Wang</author>
<author>D Lin</author>
<author>D Schuurmans</author>
</authors>
<title>Simple training of dependency parsers via structured boosting.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1756--1762</pages>
<contexts>
<context position="20479" citStr="Wang et al. (2007)" startWordPosition="3212" endWordPosition="3215"> parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this pap</context>
</contexts>
<marker>Wang, Lin, Schuurmans, 2007</marker>
<rawString>Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Simple training of dependency parsers via structured boosting. In Proceedings of IJCAI, pages 1756-1762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yamada</author>
<author>Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="20311" citStr="Yamada and Matsumoto (2003)" startWordPosition="3182" endWordPosition="3185">rived from the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminativ</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada and Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, pages 195-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yates</author>
<author>S Schoenmackers</author>
<author>O Etzioni</author>
</authors>
<title>Detecting parser errors using web-based semantic filters.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>27--34</pages>
<contexts>
<context position="29209" citStr="Yates et al., 2006" startWordPosition="4644" endWordPosition="4647">statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-gram</context>
</contexts>
<marker>Yates, Schoenmackers, Etzioni, 2006</marker>
<rawString>A. Yates, S. Schoenmackers, and O. Etzioni. 2006. Detecting parser errors using web-based semantic filters. In Proceedings of EMNLP, pages 27-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transitionbased dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="20676" citStr="Zhang and Clark (2008)" startWordPosition="3246" endWordPosition="3249">iscuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p &lt; 0.08). Type Systems UAS CM D Yamada and Matsumoto (2003) 90.3 38.7 McDonald et al. (2005) 90.9 37.5 McDonald and Pereira (2006) 91.5 42.1 Corston-Oliver et al. (2006) 90.9 37.5 Hall et al. (2006) 89.4 36.4 Wang et al. (2007) 89.2 34.4 Carreras et al. (2008) 93.5 - GoldBerg and Elhadad (2010)† 91.32 40.41 Ours 92.64 46.61 C Nivre and McDonald (2008)† 92.12 44.37 Martins et al. (2008)† 92.87 45.51 Zhang and Clark (2008) 92.1 45.4 S Koo et al. (2008) 93.16 - Suzuki et al. (2009) 93.79 - Chen et al. (2009) 93.16 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate comparisons with previous work, we only use Section 23 as the test data. The</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: investigating and combining graph-based and transitionbased dependency parsing using beam-search. In Proceedings of EMNLP, pages 562-571.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>