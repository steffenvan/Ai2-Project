<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003858">
<title confidence="0.998766">
A Filter-Based Approach to Detect End-of-Utterances from Prosody in
Dialog Systems
</title>
<author confidence="0.995273">
Olac Fuentes, David Vera and Thamar Solorio
</author>
<affiliation confidence="0.993242">
Computer Science Department
University of Texas at El Paso
</affiliation>
<address confidence="0.459085">
El Paso, TX 79968
</address>
<sectionHeader confidence="0.914696" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999817357142857">
We propose an efficient method to detect
end-of-utterances from prosodic information in
conversational speech. Our method is based
on the application of a large set of binary and
ramp filters to the energy and fundamental fre-
quency signals obtained from the speech sig-
nal. These filter responses, which can be com-
puted very efficiently, are used as input to a
learning algorithm that generates the final de-
tector. Preliminary experiments using data ob-
tained from conversations show that an accu-
rate classifier can be trained efficiently and that
good results can be obtained without requiring
a speech recognition system.
</bodyText>
<sectionHeader confidence="0.995134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998907769230769">
While there have been improvements and a significant
number of methods introduced into the realm of dialog-
based systems, there are aspects of these methods which
can be further improved upon. One such aspect is end-
of-utterance (EOU) detection, which consists of automat-
ically determining when a user has finished his/her turn
and is waiting to receive an answer from the system. Cur-
rent dialog-based systems use a simple pause threshold,
which commonly results in either unnecessary long wait-
ing times or interruptions from the system when the user
makes a pause in the middle of an utterance. These prob-
lems can annoy and discourage users using even simple
dialog systems.
Most previous methods aimed at improving upon
pause thresholds for detecting end-of-utterances use
spectral energy measures (Hariharan et al., 2001; Jia and
Xu, 2002). Other methods use prosodic features with
(Ferrer et al., 2002) and without speech recognition sys-
tems (Ferrer et al., 2003) in conjunction with decision
trees to determine end-of-utterances as quickly as possi-
ble. For this and related problems, the choice of features
is critical. Most common is to use a fixed inventory of
features, chosen based on the linguistics literature and
past experience (Shriberg and Stolcke, 2004). Recently
we have experimented with alternative approaches, in-
cluding features hand-tailored to specific discrimination
</bodyText>
<page confidence="0.86733">
45
</page>
<bodyText confidence="0.999524368421053">
problems (Ward and Al Bayyari, 2006) and random ex-
ploration of the feature space (Solorio et al., 2006). In
this paper we explore yet another approach, using a large
battery of very simple and easy to evaluate features.
In this paper we present a method to improve the ac-
curacy that can be obtained in end-of-utterance detection
that uses prosodic information only, without a speech rec-
ognizer. We adapt and extend a filter-based approach
originally proposed in computer graphics (Crow, 1984)
and later exploited successfully in computer vision (Viola
and Jones, 2001) and music retrieval (Ke et al., 2005).
Our approach consists of applying simple filters, which
can be computed in constant time, in order to generate
attributes to be used by a learning algorithm. After the
attributes have been generated, we test different learning
algorithms to detect end-of-utterances. Our results show
that the features yield good results in combination with
several of the classifiers, with the best result being ob-
tained with bagging ensembles of decision trees.
</bodyText>
<sectionHeader confidence="0.920381" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.99974225">
The first stage in our system is to extract prosodic infor-
mation from the raw audio signal. Using the audio anal-
ysis tool Didi, the log energy and fundamental frequency
signals are extracted from the source sound wave. After
computing log energy and pitch, we apply a large set of
filters in the time domain to the energy and pitch signals
in order to generate attributes suitable for classification.
We compute the filter responses for both signals at every
time step using three types of filters, each applied at many
different times scales.
The first filter type, shown in Figure 1a), is a two-step
binary filter, split approximately in half. The first half
of the filter consists of a sequence of 1’s. The second
half consists of -1’s. The second filter type is a three-step
binary filter (as shown in Figure 1b)), split in approximate
thirds alternating between 1 and -1. Finally, the third filter
is an upward slope ranging from -1 to 1.
Although simple, these filters, in particular when they
are applied at multiple scales, can characterize most of
the prosodic features that are known to be relevant in
identifying dialog phenomena including raises and falls
in pitch and pauses of different lengths.
The response of any of these filters over the signal at
any time is given by the dot product of the filter and signal
</bodyText>
<note confidence="0.8476545">
Proceedings of NAACL HLT 2007, Companion Volume, pages 45–48,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<page confidence="0.634047">
2
</page>
<figure confidence="0.9987089">
1.5
1
0.5
−0.5
−1
−1.5
−2
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
x 104
60
0
40
−20
−2.5
−40
80
−80
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
x 104
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
5 10 15 20 25 30 35 40
a) Type I filter
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
5 10 15 20 25 30 35 40 45 50 55 60
n−1�
i=0
sk+i * fi
F(s, f, k) =
20
0
</figure>
<figureCaption confidence="0.7112565">
Figure 2: Energy, with mean subtracted, and its corre-
sponding integral signal.
</figureCaption>
<figure confidence="0.9989651875">
−60
The filter response F is given by
1
0.8
0.6
0.4
−0.6
−0.8
−1
b) Type II filter
5 10 15 20 25 30 35 40
c) Type III filter
0.2
0
−0.2
−0.4
</figure>
<figureCaption confidence="0.999797">
Figure 1: The three types of filters, the first two being
</figureCaption>
<bodyText confidence="0.946542416666667">
binary only having values -1 or 1, and the last having an
upward slope from -1 to 1.
window of the same length. Computing this dot product
is slow, especially over larger time window sizes. This
cost is even greater when many filter responses are taken
over the course of the entire signal length.
Given the large number of filters and the size of a nor-
mal audio signal, the straightforward dot-product-based
computation of the filter responses is prohibitively expen-
sive. Fortunately, it is possible to device methods to com-
pute these responses efficiently, as explained in the next
subsection.
</bodyText>
<subsectionHeader confidence="0.989322">
2.1 Efficient Filter Computation
</subsectionHeader>
<bodyText confidence="0.999733588235294">
This constant time computation of binary filters for two-
dimensional signals was first presented by Crow (Crow,
1984) in the field of computer graphics and later ap-
plied successfully in computer vision (Viola and Jones,
2001). Here we show how that can be adapted to one-
dimensional signals and extended to the case of non-
binary filters, such as ramps.
Let s be the signal corresponding to either the log en-
ergy or the fundamental frequency. Let f be a filter of
size n (in arbitrary time units) and let k be the time in-
stant for which we want to compute the filter response.
The standard computation of F takes O(n) operations;
however, for the special case of binary filters like the ones
shown in figures 1a) and 1b), we can compute this re-
sponse in constant time with some preprocessing as fol-
lows. Let I be the integral signal, where each element of
I is given by
</bodyText>
<equation confidence="0.991438571428571">
j
Ij = E si
i=0
It can be seen that
k
E sj = Ik − Ij−1
i=j
</equation>
<bodyText confidence="0.9999748">
Thus this summation can be computed with two ac-
cesses to memory, after pre-computing and storing the
values of I in an array. Figure 2 shows an example of a
signal (with its mean subtracted) and the corresponding
integral signal.
Consider a binary filter f such as the one shown in
1a), f = {1n/2, −1n/2}, that is, f consists of n/2 ones
followed by n/2 negative ones. Then the filter response
of a signal can then be computed in constant time using
three references to the integral signal I:
</bodyText>
<equation confidence="0.83099">
F(s, f, k) = 2Ik+n/2−1 − Ik−1 − Ik+n−1
</equation>
<bodyText confidence="0.985283333333333">
Similarly, the response to a filter like the one shown in
Figure 1 b), given by f = {1n/3, −1n/3,1n/3} can be
computed with four memory references.
</bodyText>
<equation confidence="0.7111355">
F(s, f, k) = Ik+n−1 − 2Ik+2n/3−1 + 2Ik+n/3−1 − Ik−1
46
</equation>
<bodyText confidence="0.999803333333333">
The third filter is an upward ramp ranging from -1 to
1. Whereas the binary filters are simple to calculate us-
ing look-up values, and their application to 1-dimensional
signals is a simple adaptation to the 2-D algorithm, a
ramp is more difficult and requires separate preprocess-
ing for filters of different lengths. Regardless, it is still
possible to compute its response in constant time after
preprocessing.
We define a ramp filter of length n as f = {−1, 2
</bodyText>
<equation confidence="0.959139166666667">
n−1 −
1, 4
n−1 − 1, ...,1 − 2
n−1, 11. The response to this filter is
F(s, f, k) = n−1X sk+i * fi
i=0
</equation>
<bodyText confidence="0.962806190476191">
can derive all binary-filter responses from vector I, com-
puting ramp-filter responses requires the pre-computation
of a separate An for every filter length n. Nevertheless,
this cost is small compared to the cost of computing a dot
product for every time instant in the input signal.
The integral signal representations are computed from
the two prosodic feature signals, and filter features are
calculated along the timeframe of the signal. Once the fil-
ter responses are obtained, they are used as attributes for
machine learning algorithms, which are used to generate
the final classifiers. The data is then used to train several
learning algorithms, as implemented in Weka (Witten and
Frank, 2005).
Let Pn−1
i=0 i sk+i be denoted by Ank. Clearly, if Ank
can be computed in constant time, then F(s, f, k) can
also be computed in constant time. This can be done, with
a little preprocessing, as follows. Let An0 be computed
in the standard (O(n) time) way,
Then to compute values of Ank for k &gt; 0 we first
observe that
</bodyText>
<equation confidence="0.9683589">
Ank = sk+1 + 2sk+2 + ... + (n − 1)sk+n−1
and
Ank+1 = sk+2 + 2sk+3 + ... + (n − 1)sk+n
From this, we can see that
Ank+1 = Ank − sk+1 − sk+2 − .. .
−sk+n−1 + (n − 1)sk+n
k+n−1
= Ank − X si + (n − 1)sk+n
i=k+1
= Ank − (Ik+n−1 − Ik) + (n − 1)sk+n
</equation>
<bodyText confidence="0.999459333333333">
Thus after pre-computing vectors I and An, which
takes linear time in the size of the signal, we can compute
any filter response in constant time. However, while we
</bodyText>
<sectionHeader confidence="0.987691" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999705">
Experiments were conducted to test the end-of-utterance
system on pre-recorded conversations, measuring preci-
sion and recall of positive classifications. The conver-
sations used contained speech by both male and female
users to compare the robustness among different vocal
range frequencies.
</bodyText>
<subsectionHeader confidence="0.999044">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9999057">
The training set of data is derived from about 22 min-
utes of conversations, with the audio split such that each
speaker is on a separate voice channel (see (Hollingsed,
2006)). In 17-minutes worth of these, volunteers were
asked to list eight exits on the east side of El Paso on
Interstate 10. This provided a clear way to measure end-
of-utterances as if a system were prompting users for in-
put. This set of conversations contained a large number of
turn-switches, which also simulated voice-portal systems
well. For most of the time in this set, the same person
(a female) is conducting the quiz. However, the speakers
taking the quiz have distinctly different voices and are
mixed in gender.
Five minutes of the training set were taken from a
casual conversation also containing a male and female
speaker combination. The speakers in this conversa-
tion are different from the speakers in the other dataset.
Adding these data balances the training set, reducing the
probability of the system learning only the specific quiz
format used in much of the training data.
Didi was used to extract the prosodic features, and the
filter responses were computed for each of the three fil-
ter types, in sizes ranging from 200ms to 3 seconds in
increments of 50ms, totaling 342 features per time in-
stance. The class was set to 0 or 1, signaling non-end-
of-utterances and a confirmed end-of-utterances, respec-
tively. 992 instances were created for the experiments,
split equally in two between positive examples of end-of-
utterances, and randomly selected negative examples for
both channels in the source audio.
</bodyText>
<equation confidence="0.9972438">
= n−1X 2i − 1)sk+i
i=0 (n − 1
= 2 n−1X isk+i − n−1X sk+i
i=0 i=0
n − 1
i sk+i − (In−1 − Ik−1)
2
=
n−1X
i=0
n − 1
isi
An0 =
n−1X
i=0
</equation>
<page confidence="0.945761">
47
</page>
<tableCaption confidence="0.976713">
Table 1: Experimental results of using different classifiers and
averaging ten ten-fold-cross-validation evaluations with random
seeds per classifier.
</tableCaption>
<table confidence="0.999193857142857">
Recall Precision F-Measure
Dec.Stump 0.623 0.705 0.660
Dec.Table 0.768 0.799 0.783
C4.5 0.792 0.800 0.796
Boost(DS) 0.792 0.820 0.806
Bag(REP) 0.850 0.833 0.841
Bag(C4.5) 0.786 0.797 0.791
</table>
<bodyText confidence="0.9896877">
All instances used for training were randomly cho-
sen. The positive examples were chosen from human-
determined end-of-utterance intervals, which ranged
from the time instant a valid end-of-utterance was
recorded to a point either 1.5 seconds after that instant or
a start-of-utterance that occurred prior to that time. The
negative examples were randomly chosen such that no
time instance was chosen prior to the 3-second-mark of
the audio file used and none was within a marked end-of-
utterance interval.
</bodyText>
<subsectionHeader confidence="0.76625">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999915458333333">
Six combinations of classifiers were generated using the
Weka data mining tool. Each of these classifier combina-
tions was tested using 10-fold cross-validation. The re-
sults reflect the average of ten such cross-validation runs,
each using a different random seed. The final classifier
combinations used are Weka’s implementations of deci-
sion stumps, decision tables, C4.5 (Quinlan, 1993) and
ensembles of decision stumps using boosting and C4.5
and reduced error pruning (REP) decision trees (Quinlan,
1987) using bagging.
The experiments performed yield interesting results.
Table 1 shows that, with the exception of decision
stumps, which are perhaps too simple for this task, all
classifiers performed well, which shows that our filters
produce suitable features for classification. The best re-
sults were obtained using bagging and REP trees, but re-
sults for other methods yield similar precision and recall.
It is almost certain that better results can be obtained
using these methods if bleeding across channels in the
audio streams was reduced. The F0 features do a good job
of filtering out possible mistakes in the system due to the
way the frequencies are calculated. However, bleeding
can still mislead the classifiers into perceiving an end-of-
utterance from another speaker.
</bodyText>
<sectionHeader confidence="0.996479" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999828315789474">
We have shown a new filter-based method for detect-
ing end-of-utterances in conversation using only basic
prosodic information. We adapted and extended previ-
ously described methods for fast computation of filter re-
sponses, which allows our system to be trained quickly
and easily permits real-time performance. Preliminary
experiments in the task of classifying windows in dialog
recordings as being end-of-utterances or not have yielded
very promising results using standard classification algo-
rithms, with an f-measure of 0.84.
Present and future work includes evaluating the
method as a component of a real-time dialog system,
where its usefulness at decreasing waiting time can be
tested. We are also working on methods for feature se-
lection and compression to obtain further speedup, and
finally we are experimenting with larger datasets.
Acknowledgement: The authors would like to thank
NSF for partially supporting this work under grants IIS-
0415150 and 0080940.
</bodyText>
<sectionHeader confidence="0.996328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999314194444445">
F. C. Crow. 1984. Summed-area tables for texture mapping.
In Proceedings of the 11th Annual Conference on Computer
Graphics and Interactive Techniques.
L. Ferrer, E. Shriberg, and A. Stolcke. 2002. Is the speaker
done yet? Faster and more accurate end-of-utterance detec-
tion using prosody. In Proceedings of ICSLP.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-based
approach to end-of-utterance detection that does not require
speech recognition. In Proceedings of IEEE ICASSP.
R. Hariharan, J. Hakkinen, and K. Laurila. 2001. Robust end-
of-utterance detection for real-time speech recognition appli-
cations. In Proceedings of IEEE ICASSP.
T. K. Hollingsed. 2006. Responsive behavior in tutorial spoken
dialogues. Master’s thesis, University of Texas at El Paso.
C. Jia and B. Xu. 2002. An improved entropy-based endpoint
detection algorithm. In Proceedings of ISCSLP.
Y. Ke, D. Hoiem, and R. Sukthankar. 2005. Computer vision
for music identification. In Proceedings of IEEE CVPR.
J. R. Quinlan. 1987. Simplifying decision trees. International
Journal of Man-Machine Studies, 27.
J. R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufman.
E. Shriberg and A. Stolcke. 2004. Direct modeling of prosody:
An overview of applications in automatic speech processing.
In Proceedings of ICSP.
T. Solorio, O. Fuentes, N. Ward, and Y. Al Bayyari. 2006.
Prosodic feature generation for back-channel prediction. In
Proceedings of Interspeech.
P. Viola and M. Jones. 2001. Rapid object detection using a
boosted cascade of simple features. In Proceedings of IEEE
CVPR.
N. Ward and Y. Al Bayyari. 2006. A case study in the identi-
fication of prosodic cues to turn-taking: Back-channeling in
Arabic. In Proceedings of Interspeech.
I. H. Witten and E. Frank. 2005. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufman.
</reference>
<page confidence="0.963435">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.868406">
<title confidence="0.996343">A Filter-Based Approach to Detect End-of-Utterances from Prosody Dialog Systems</title>
<author confidence="0.915563">Olac Fuentes</author>
<author confidence="0.915563">David Vera</author>
<author confidence="0.915563">Thamar</author>
<affiliation confidence="0.9995475">Computer Science University of Texas at El</affiliation>
<address confidence="0.960197">El Paso, TX 79968</address>
<abstract confidence="0.999510933333333">We propose an efficient method to detect end-of-utterances from prosodic information in conversational speech. Our method is based on the application of a large set of binary and ramp filters to the energy and fundamental frequency signals obtained from the speech signal. These filter responses, which can be computed very efficiently, are used as input to a learning algorithm that generates the final detector. Preliminary experiments using data obtained from conversations show that an accurate classifier can be trained efficiently and that good results can be obtained without requiring a speech recognition system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F C Crow</author>
</authors>
<title>Summed-area tables for texture mapping.</title>
<date>1984</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques.</booktitle>
<contexts>
<context position="2733" citStr="Crow, 1984" startWordPosition="429" endWordPosition="430">ently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery of very simple and easy to evaluate features. In this paper we present a method to improve the accuracy that can be obtained in end-of-utterance detection that uses prosodic information only, without a speech recognizer. We adapt and extend a filter-based approach originally proposed in computer graphics (Crow, 1984) and later exploited successfully in computer vision (Viola and Jones, 2001) and music retrieval (Ke et al., 2005). Our approach consists of applying simple filters, which can be computed in constant time, in order to generate attributes to be used by a learning algorithm. After the attributes have been generated, we test different learning algorithms to detect end-of-utterances. Our results show that the features yield good results in combination with several of the classifiers, with the best result being obtained with bagging ensembles of decision trees. 2 Method The first stage in our syste</context>
<context position="6124" citStr="Crow, 1984" startWordPosition="1044" endWordPosition="1045">duct is slow, especially over larger time window sizes. This cost is even greater when many filter responses are taken over the course of the entire signal length. Given the large number of filters and the size of a normal audio signal, the straightforward dot-product-based computation of the filter responses is prohibitively expensive. Fortunately, it is possible to device methods to compute these responses efficiently, as explained in the next subsection. 2.1 Efficient Filter Computation This constant time computation of binary filters for twodimensional signals was first presented by Crow (Crow, 1984) in the field of computer graphics and later applied successfully in computer vision (Viola and Jones, 2001). Here we show how that can be adapted to onedimensional signals and extended to the case of nonbinary filters, such as ramps. Let s be the signal corresponding to either the log energy or the fundamental frequency. Let f be a filter of size n (in arbitrary time units) and let k be the time instant for which we want to compute the filter response. The standard computation of F takes O(n) operations; however, for the special case of binary filters like the ones shown in figures 1a) and 1b</context>
</contexts>
<marker>Crow, 1984</marker>
<rawString>F. C. Crow. 1984. Summed-area tables for texture mapping. In Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferrer</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Is the speaker done yet? Faster and more accurate end-of-utterance detection using prosody.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="1754" citStr="Ferrer et al., 2002" startWordPosition="274" endWordPosition="277">ing when a user has finished his/her turn and is waiting to receive an answer from the system. Current dialog-based systems use a simple pause threshold, which commonly results in either unnecessary long waiting times or interruptions from the system when the user makes a pause in the middle of an utterance. These problems can annoy and discourage users using even simple dialog systems. Most previous methods aimed at improving upon pause thresholds for detecting end-of-utterances use spectral energy measures (Hariharan et al., 2001; Jia and Xu, 2002). Other methods use prosodic features with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this </context>
</contexts>
<marker>Ferrer, Shriberg, Stolcke, 2002</marker>
<rawString>L. Ferrer, E. Shriberg, and A. Stolcke. 2002. Is the speaker done yet? Faster and more accurate end-of-utterance detection using prosody. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferrer</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>A prosody-based approach to end-of-utterance detection that does not require speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE ICASSP.</booktitle>
<contexts>
<context position="1815" citStr="Ferrer et al., 2003" startWordPosition="284" endWordPosition="287">eceive an answer from the system. Current dialog-based systems use a simple pause threshold, which commonly results in either unnecessary long waiting times or interruptions from the system when the user makes a pause in the middle of an utterance. These problems can annoy and discourage users using even simple dialog systems. Most previous methods aimed at improving upon pause thresholds for detecting end-of-utterances use spectral energy measures (Hariharan et al., 2001; Jia and Xu, 2002). Other methods use prosodic features with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery </context>
</contexts>
<marker>Ferrer, Shriberg, Stolcke, 2003</marker>
<rawString>L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-based approach to end-of-utterance detection that does not require speech recognition. In Proceedings of IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hariharan</author>
<author>J Hakkinen</author>
<author>K Laurila</author>
</authors>
<title>Robust endof-utterance detection for real-time speech recognition applications.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE ICASSP.</booktitle>
<contexts>
<context position="1671" citStr="Hariharan et al., 2001" startWordPosition="260" endWordPosition="263">h aspect is endof-utterance (EOU) detection, which consists of automatically determining when a user has finished his/her turn and is waiting to receive an answer from the system. Current dialog-based systems use a simple pause threshold, which commonly results in either unnecessary long waiting times or interruptions from the system when the user makes a pause in the middle of an utterance. These problems can annoy and discourage users using even simple dialog systems. Most previous methods aimed at improving upon pause thresholds for detecting end-of-utterances use spectral energy measures (Hariharan et al., 2001; Jia and Xu, 2002). Other methods use prosodic features with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari,</context>
</contexts>
<marker>Hariharan, Hakkinen, Laurila, 2001</marker>
<rawString>R. Hariharan, J. Hakkinen, and K. Laurila. 2001. Robust endof-utterance detection for real-time speech recognition applications. In Proceedings of IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Hollingsed</author>
</authors>
<title>Responsive behavior in tutorial spoken dialogues. Master’s thesis,</title>
<date>2006</date>
<institution>University of Texas at El Paso.</institution>
<contexts>
<context position="10158" citStr="Hollingsed, 2006" startWordPosition="1793" endWordPosition="1794">ich takes linear time in the size of the signal, we can compute any filter response in constant time. However, while we 3 Experimental Results Experiments were conducted to test the end-of-utterance system on pre-recorded conversations, measuring precision and recall of positive classifications. The conversations used contained speech by both male and female users to compare the robustness among different vocal range frequencies. 3.1 Data The training set of data is derived from about 22 minutes of conversations, with the audio split such that each speaker is on a separate voice channel (see (Hollingsed, 2006)). In 17-minutes worth of these, volunteers were asked to list eight exits on the east side of El Paso on Interstate 10. This provided a clear way to measure endof-utterances as if a system were prompting users for input. This set of conversations contained a large number of turn-switches, which also simulated voice-portal systems well. For most of the time in this set, the same person (a female) is conducting the quiz. However, the speakers taking the quiz have distinctly different voices and are mixed in gender. Five minutes of the training set were taken from a casual conversation also cont</context>
</contexts>
<marker>Hollingsed, 2006</marker>
<rawString>T. K. Hollingsed. 2006. Responsive behavior in tutorial spoken dialogues. Master’s thesis, University of Texas at El Paso.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jia</author>
<author>B Xu</author>
</authors>
<title>An improved entropy-based endpoint detection algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of ISCSLP.</booktitle>
<contexts>
<context position="1690" citStr="Jia and Xu, 2002" startWordPosition="264" endWordPosition="267">nce (EOU) detection, which consists of automatically determining when a user has finished his/her turn and is waiting to receive an answer from the system. Current dialog-based systems use a simple pause threshold, which commonly results in either unnecessary long waiting times or interruptions from the system when the user makes a pause in the middle of an utterance. These problems can annoy and discourage users using even simple dialog systems. Most previous methods aimed at improving upon pause thresholds for detecting end-of-utterances use spectral energy measures (Hariharan et al., 2001; Jia and Xu, 2002). Other methods use prosodic features with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random e</context>
</contexts>
<marker>Jia, Xu, 2002</marker>
<rawString>C. Jia and B. Xu. 2002. An improved entropy-based endpoint detection algorithm. In Proceedings of ISCSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ke</author>
<author>D Hoiem</author>
<author>R Sukthankar</author>
</authors>
<title>Computer vision for music identification.</title>
<date>2005</date>
<booktitle>In Proceedings of IEEE CVPR.</booktitle>
<contexts>
<context position="2847" citStr="Ke et al., 2005" startWordPosition="445" endWordPosition="448">nation 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery of very simple and easy to evaluate features. In this paper we present a method to improve the accuracy that can be obtained in end-of-utterance detection that uses prosodic information only, without a speech recognizer. We adapt and extend a filter-based approach originally proposed in computer graphics (Crow, 1984) and later exploited successfully in computer vision (Viola and Jones, 2001) and music retrieval (Ke et al., 2005). Our approach consists of applying simple filters, which can be computed in constant time, in order to generate attributes to be used by a learning algorithm. After the attributes have been generated, we test different learning algorithms to detect end-of-utterances. Our results show that the features yield good results in combination with several of the classifiers, with the best result being obtained with bagging ensembles of decision trees. 2 Method The first stage in our system is to extract prosodic information from the raw audio signal. Using the audio analysis tool Didi, the log energy</context>
</contexts>
<marker>Ke, Hoiem, Sukthankar, 2005</marker>
<rawString>Y. Ke, D. Hoiem, and R. Sukthankar. 2005. Computer vision for music identification. In Proceedings of IEEE CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Simplifying decision trees.</title>
<date>1987</date>
<journal>International Journal of Man-Machine Studies,</journal>
<volume>27</volume>
<contexts>
<context position="13079" citStr="Quinlan, 1987" startWordPosition="2271" endWordPosition="2272">he 3-second-mark of the audio file used and none was within a marked end-ofutterance interval. 3.2 Results Six combinations of classifiers were generated using the Weka data mining tool. Each of these classifier combinations was tested using 10-fold cross-validation. The results reflect the average of ten such cross-validation runs, each using a different random seed. The final classifier combinations used are Weka’s implementations of decision stumps, decision tables, C4.5 (Quinlan, 1993) and ensembles of decision stumps using boosting and C4.5 and reduced error pruning (REP) decision trees (Quinlan, 1987) using bagging. The experiments performed yield interesting results. Table 1 shows that, with the exception of decision stumps, which are perhaps too simple for this task, all classifiers performed well, which shows that our filters produce suitable features for classification. The best results were obtained using bagging and REP trees, but results for other methods yield similar precision and recall. It is almost certain that better results can be obtained using these methods if bleeding across channels in the audio streams was reduced. The F0 features do a good job of filtering out possible </context>
</contexts>
<marker>Quinlan, 1987</marker>
<rawString>J. R. Quinlan. 1987. Simplifying decision trees. International Journal of Man-Machine Studies, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="12959" citStr="Quinlan, 1993" startWordPosition="2253" endWordPosition="2254">occurred prior to that time. The negative examples were randomly chosen such that no time instance was chosen prior to the 3-second-mark of the audio file used and none was within a marked end-ofutterance interval. 3.2 Results Six combinations of classifiers were generated using the Weka data mining tool. Each of these classifier combinations was tested using 10-fold cross-validation. The results reflect the average of ten such cross-validation runs, each using a different random seed. The final classifier combinations used are Weka’s implementations of decision stumps, decision tables, C4.5 (Quinlan, 1993) and ensembles of decision stumps using boosting and C4.5 and reduced error pruning (REP) decision trees (Quinlan, 1987) using bagging. The experiments performed yield interesting results. Table 1 shows that, with the exception of decision stumps, which are perhaps too simple for this task, all classifiers performed well, which shows that our filters produce suitable features for classification. The best results were obtained using bagging and REP trees, but results for other methods yield similar precision and recall. It is almost certain that better results can be obtained using these method</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Direct modeling of prosody: An overview of applications in automatic speech processing.</title>
<date>2004</date>
<booktitle>In Proceedings of ICSP.</booktitle>
<contexts>
<context position="2117" citStr="Shriberg and Stolcke, 2004" startWordPosition="331" endWordPosition="334">using even simple dialog systems. Most previous methods aimed at improving upon pause thresholds for detecting end-of-utterances use spectral energy measures (Hariharan et al., 2001; Jia and Xu, 2002). Other methods use prosodic features with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery of very simple and easy to evaluate features. In this paper we present a method to improve the accuracy that can be obtained in end-of-utterance detection that uses prosodic information only, without a speech recognizer. We adapt and extend a filter-based approach originally proposed in computer graph</context>
</contexts>
<marker>Shriberg, Stolcke, 2004</marker>
<rawString>E. Shriberg and A. Stolcke. 2004. Direct modeling of prosody: An overview of applications in automatic speech processing. In Proceedings of ICSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
<author>O Fuentes</author>
<author>N Ward</author>
<author>Y Al Bayyari</author>
</authors>
<title>Prosodic feature generation for back-channel prediction.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="2344" citStr="Solorio et al., 2006" startWordPosition="364" endWordPosition="367">ures with (Ferrer et al., 2002) and without speech recognition systems (Ferrer et al., 2003) in conjunction with decision trees to determine end-of-utterances as quickly as possible. For this and related problems, the choice of features is critical. Most common is to use a fixed inventory of features, chosen based on the linguistics literature and past experience (Shriberg and Stolcke, 2004). Recently we have experimented with alternative approaches, including features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery of very simple and easy to evaluate features. In this paper we present a method to improve the accuracy that can be obtained in end-of-utterance detection that uses prosodic information only, without a speech recognizer. We adapt and extend a filter-based approach originally proposed in computer graphics (Crow, 1984) and later exploited successfully in computer vision (Viola and Jones, 2001) and music retrieval (Ke et al., 2005). Our approach consists of applying simple filters, which can be computed in constant time, in or</context>
</contexts>
<marker>Solorio, Fuentes, Ward, Bayyari, 2006</marker>
<rawString>T. Solorio, O. Fuentes, N. Ward, and Y. Al Bayyari. 2006. Prosodic feature generation for back-channel prediction. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Viola</author>
<author>M Jones</author>
</authors>
<title>Rapid object detection using a boosted cascade of simple features.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE CVPR.</booktitle>
<contexts>
<context position="2809" citStr="Viola and Jones, 2001" startWordPosition="438" endWordPosition="441"> features hand-tailored to specific discrimination 45 problems (Ward and Al Bayyari, 2006) and random exploration of the feature space (Solorio et al., 2006). In this paper we explore yet another approach, using a large battery of very simple and easy to evaluate features. In this paper we present a method to improve the accuracy that can be obtained in end-of-utterance detection that uses prosodic information only, without a speech recognizer. We adapt and extend a filter-based approach originally proposed in computer graphics (Crow, 1984) and later exploited successfully in computer vision (Viola and Jones, 2001) and music retrieval (Ke et al., 2005). Our approach consists of applying simple filters, which can be computed in constant time, in order to generate attributes to be used by a learning algorithm. After the attributes have been generated, we test different learning algorithms to detect end-of-utterances. Our results show that the features yield good results in combination with several of the classifiers, with the best result being obtained with bagging ensembles of decision trees. 2 Method The first stage in our system is to extract prosodic information from the raw audio signal. Using the au</context>
<context position="6232" citStr="Viola and Jones, 2001" startWordPosition="1060" endWordPosition="1063">r responses are taken over the course of the entire signal length. Given the large number of filters and the size of a normal audio signal, the straightforward dot-product-based computation of the filter responses is prohibitively expensive. Fortunately, it is possible to device methods to compute these responses efficiently, as explained in the next subsection. 2.1 Efficient Filter Computation This constant time computation of binary filters for twodimensional signals was first presented by Crow (Crow, 1984) in the field of computer graphics and later applied successfully in computer vision (Viola and Jones, 2001). Here we show how that can be adapted to onedimensional signals and extended to the case of nonbinary filters, such as ramps. Let s be the signal corresponding to either the log energy or the fundamental frequency. Let f be a filter of size n (in arbitrary time units) and let k be the time instant for which we want to compute the filter response. The standard computation of F takes O(n) operations; however, for the special case of binary filters like the ones shown in figures 1a) and 1b), we can compute this response in constant time with some preprocessing as follows. Let I be the integral s</context>
</contexts>
<marker>Viola, Jones, 2001</marker>
<rawString>P. Viola and M. Jones. 2001. Rapid object detection using a boosted cascade of simple features. In Proceedings of IEEE CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>Y Al Bayyari</author>
</authors>
<title>A case study in the identification of prosodic cues to turn-taking: Back-channeling in Arabic.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Ward, Bayyari, 2006</marker>
<rawString>N. Ward and Y. Al Bayyari. 2006. A case study in the identification of prosodic cues to turn-taking: Back-channeling in Arabic. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="8933" citStr="Witten and Frank, 2005" startWordPosition="1558" endWordPosition="1561">esponses requires the pre-computation of a separate An for every filter length n. Nevertheless, this cost is small compared to the cost of computing a dot product for every time instant in the input signal. The integral signal representations are computed from the two prosodic feature signals, and filter features are calculated along the timeframe of the signal. Once the filter responses are obtained, they are used as attributes for machine learning algorithms, which are used to generate the final classifiers. The data is then used to train several learning algorithms, as implemented in Weka (Witten and Frank, 2005). Let Pn−1 i=0 i sk+i be denoted by Ank. Clearly, if Ank can be computed in constant time, then F(s, f, k) can also be computed in constant time. This can be done, with a little preprocessing, as follows. Let An0 be computed in the standard (O(n) time) way, Then to compute values of Ank for k &gt; 0 we first observe that Ank = sk+1 + 2sk+2 + ... + (n − 1)sk+n−1 and Ank+1 = sk+2 + 2sk+3 + ... + (n − 1)sk+n From this, we can see that Ank+1 = Ank − sk+1 − sk+2 − .. . −sk+n−1 + (n − 1)sk+n k+n−1 = Ank − X si + (n − 1)sk+n i=k+1 = Ank − (Ik+n−1 − Ik) + (n − 1)sk+n Thus after pre-computing vectors I an</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufman.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>