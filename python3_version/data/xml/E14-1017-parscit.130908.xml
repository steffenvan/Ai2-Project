<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.947766">
Structured and Unstructured Cache Models for SMT Domain Adaptation
</title>
<author confidence="0.983941">
Annie Louis
</author>
<affiliation confidence="0.9980525">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.9863715">
10 Crichton Street
Edinburgh EH8 9AB
</address>
<email confidence="0.998435">
alouis@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999948">
We present a French to English transla-
tion system for Wikipedia biography ar-
ticles. We use training data from out-
of-domain corpora and adapt the system
for biographies. We propose two forms
of domain adaptation. The first biases
the system towards words likely in biogra-
phies and encourages repetition of words
across the document. Since biographies in
Wikipedia follow a regular structure, our
second model exploits this structure as a
sequence of topic segments, where each
segment discusses a narrower subtopic of
the biography domain. In this structured
model, the system is encouraged to use
words likely in the current segment’s topic
rather than in biographies as a whole.
We implement both systems using cache-
based translation techniques. We show
that a system trained on Europarl and news
can be adapted for biographies with 0.5
BLEU score improvement using our mod-
els. Further the structure-aware model out-
performs the system which treats the entire
document as a single segment.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996945">
This paper explores domain adaptation of statisti-
cal machine translation (SMT) systems to contexts
where the target documents have predictable reg-
ularity in topic and document structure. Regular-
ities can take the form of high rates of word rep-
etition across documents, similarities in sentence
syntax, similar subtopics and discourse organiza-
tion. Domain adaptation for such documents can
exploit these similarities. In this paper we focus
on topic (lexical) regularities in a domain. We
present a system that translates Wikipedia biogra-
phies from French to English by adapting a system
</bodyText>
<author confidence="0.732168">
Bonnie Webber
</author>
<affiliation confidence="0.9854065">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.9514675">
10 Crichton Street
Edinburgh EH8 9AB
</address>
<email confidence="0.978102">
bonnie@inf.ed.ac.uk
</email>
<bodyText confidence="0.997983743589744">
trained on Europarl and news commentaries. This
task is interesting for the following two reasons.
Many techniques for SMT domain adaption
have focused on rather diverse domains such as us-
ing systems trained on Europarl or news to trans-
late medical articles (Tiedemann, 2010a), blogs
(Su et al., 2012) and transcribed lectures (Federico
et al., 2012). The main challenge for such systems
is translating out-of-vocabulary words (Carpuat et
al., 2012). In contrast, words in biographies are
closer to a training corpus of news commentaries
and parlimentary proceedings and allow us to ex-
amine how well domain adaptation techniques can
disambiguate lexical choices. Such an analysis is
harder to do on very divergent domains.
In addition, biographies have a fairly regu-
lar discourse structure: a central entity (person
who is the topic of the biography), recurring
subtopics such as ‘childhood’, ‘schooling’, ‘ca-
reer’ and ‘later life’, and a likely chronological
order to these topics. These regularities become
more predictable in documents from sources such
as Wikipedia. This setting allows us to explore the
utility of models which make translation decisions
depending on the discourse structure. Translation
methods for structured documents have only re-
cently been explored in Foster et al. (2010). How-
ever, their system was developed for parlimentary
proceedings and translations were adapted using
separate language models based upon the identity
of the speaker, text type (questions, debate, etc.)
and the year when the proceedings took place.
Biographies constitute a more realistic discourse
context to develop structured models.
This paper introduces a new corpus consisting
of paired French-English translations of biography
articles from Wikipedia.1 We translate this cor-
pus by developing cache-based domain adaptation
methods, a technique recently proposed by Tiede-
</bodyText>
<footnote confidence="0.932442">
1Corpus available at http://homepages.inf.ed.
ac.uk/alouis/wikiBio.html.
</footnote>
<page confidence="0.953385">
155
</page>
<note confidence="0.993236">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155–163,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999734739130435">
mann (2010a). In such methods, cache(s) can be
filled with relevant items for translation and trans-
lation hypotheses that match a greater number of
cache items are scored higher. These cache scores
are used as additional features during decoding.
We use two types of cache—one which encour-
ages the use of words more indicative of the biog-
raphy domain and another which encourages word
repetition in the same document.
We also show how cache models allow
for straightforward implementation of structured
translation by refreshing the cache in response to
topic segment boundaries. We fill caches with
words relevant to the topic of the current segment
which is being translated. The cache contents are
obtained from an unsupervised topic model which
induces clusters of words that are likely to ap-
pear in the same topic segment. Evaluation re-
sults show that cache-based models give upto 0.5
BLEU score improvements over an out-of-domain
system. In addition, models that take topical struc-
ture into account score 0.3 BLEU points higher
than those which ignore discourse structure.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999974975">
The study that is closest to our work is that of
Tiedemann (2010a), which proposed cache mod-
els to adapt a Europarl-trained system to medical
documents. The system used caching in two ways:
a cache-based language model (stores target lan-
guage words from translations of preceding sen-
tences in the same document) and a cache-based
translation model (stores phrase pairs from pre-
ceding sentence translations). These caches en-
couraged the system to imitate the ‘consistency’
aspect of domain-specific texts i.e., the property
that words or phrases are likely to be repeated in a
domain and within the same document.
Cache models developed in later work, Tiede-
mann (2010b) and Gong et al. (2011), were ap-
plied for translating in-domain documents. Gong
et al. (2011) introduced additional caches to store
(i) words and phrase pairs from training docu-
ments most similar to a current source article,
and (ii) words from topical clusters created on the
training set. However, a central issue in these sys-
tems is that caches become noisy over time, since
they ignore topic shifts in the documents. This pa-
per presents cache models which not only take ad-
vantage of likely words in the domain and consis-
tency, but which also adapt to topic shifts.
A different line of work very relevant to our
study is the creation of topic-specific translations
by either inferring a topic for the source document
as a whole, or at the other extreme, finer topics for
individual sentences (Su et al., 2012; Eidelman et
al., 2012). Neither of these granularities seem in-
tuitive in natural discourse. In this work, we pro-
pose that tailoring translations to topics associated
with discourse segments in the article is likely to
be beneficial for two reasons: a) subtopics of such
granularity can be assumed with reasonable con-
fidence to re-occur in documents from the same
domain and b) we can hypothesize that a domain
will have a small number of segment-level topics.
</bodyText>
<sectionHeader confidence="0.982313" genericHeader="method">
3 System adaptation for biographies
</sectionHeader>
<bodyText confidence="0.999916689655173">
We introduce two types of translation systems
adapted for biographies:
General domain models (domain-) that use in-
formation about biographies but treat the docu-
ment as a whole.
Structured models (struct-) that are sensitive to
topic segment boundaries and the specific topic of
the segment currently being translated.
We implement both models using caches. Since
we do not have parallel corpora for the biography
domain, our caches contain items in the target lan-
guage only. We use two types of caches:
Topic cache stores target language words (uni-
grams) likely in a particular topic. Each unigram
has an associated score.
Consistency cache favours repetition of words in
the sentences from the same document. It stores
target language words (unigrams) from the 1-best
translations of previous sentences in the same doc-
ument. Each word is associated with an age value
and a score. Age indicates when a word entered
the cache and introduces a ‘decay effect’. Words
used in immediately previous sentences have a
low age value while higher age values indicate
words from sentences much prior in the document.
Scores are inversely proportional to age.
Both the types of caches are present in both
the general domain and structured models, but the
cache words and scores are computed differently.
</bodyText>
<subsectionHeader confidence="0.998706">
3.1 A general domain model
</subsectionHeader>
<bodyText confidence="0.9997415">
This system seeks to bias translations towards
words which occur often in biography articles.
The topic cache is filled with word unigrams
that are more likely to occur in biographies com-
</bodyText>
<page confidence="0.998106">
156
</page>
<bodyText confidence="0.99997327027027">
pared to general news documents. We compare
the words from 1,475 English Wikipedia biogra-
phies articles to those in a large collection (64,875
articles) of New York Times (NYT) news articles
(taken from the NYT Annotated Corpus (Sand-
haus, 2008)). We use a log-likelihood ratio test
(Lin and Hovy, 2000) to identify words which oc-
cur with significantly higher probability in biogra-
phies compared to NYT. We collect only words
indicated with 0.0001 significance by the test to
be more likely in biographies. We rank this set of
18,597 words in decreasing order of frequency in
the biography article set and assign to each word
a score equal to 1/rank of the word. These words
with their associated scores form the contents of
the topic cache. In the general domain model,
these same words are assumed to be useful for the
full document and so the cache contents remain
constant during translation of the full document.
The consistency cache stores words from the
translations of preceding sentences of the same
document. After each sentence is translated, we
collect the words from the 1-best translation and
filter out punctuation marks and out of vocabu-
lary words. The remaining words are assigned an
age of 1. Words already present in the cache have
their age incremented by one. The new words with
age 1 are added to the cache2 and the scores for
all cache words are recomputed as el/age. The
age therefore gets incremented as each sentence’s
words are inserted into the cache creating a decay.
The cache is cleared at the end of each document.
During decoding, a candidate phrase is split into
unigrams and checked against each cache. Scores
for matching unigrams are summed up to obtain a
score for the phrase. Separate scores are computed
for matches with the topic and consistency caches.
</bodyText>
<subsectionHeader confidence="0.999775">
3.2 A structured model
</subsectionHeader>
<bodyText confidence="0.986935451612903">
Here we consider topic and consistency at a nar-
rower level—within topic segments of the article.
The topic cache is filled with words likely in
individual topic segments of an article. To do this,
we need to identify the topic of smaller segments
of the article and also store a set of most probable
words for each topic. The topics should also have
bilingual mappings which will allow us to infer for
every French document segment, words that are
likely in such a segment in the English language.
We designed and implemented an unsupervised
2If the word already exists in the cache, it is first removed.
topic model based on Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) to induce such word clus-
ters. In a first step, we induce subtopics from
monolingual articles in English and French sep-
arately. The topics are subsequently aligned be-
tween the languages as explained below.
In the first step, we learn a topic model which
incorporates two main ideas a) adds sensitivity
to topic boundaries by assigning a single topic
per topic segment b) allows for additional flex-
ibility by not only drawing the words of a seg-
ment from the segment-level topic, but also al-
lows some words to be either specific to the doc-
ument (such as named entities) or stop words. To
address idea b), we have a “switching variable”
to switch between document-specific word, stop-
word or domain-words.
The generative story to create a monolingual
dataset of biographies is as follows:
</bodyText>
<listItem confidence="0.976620814814815">
• Draw a distribution q for the proportion of the
three word types in the full corpus (domain
subtopic, document-specific, stopwords) —
Dirichlet(γ)
• For each domain subtopic φl, 1 &lt; l &lt; T,
draw a distribution over word vocabulary —
Dirichlet(β)
• Draw a distribution ψ over word vocabulary
for stopwords — Dirichlet(O
• For each document Di:
– Draw a distribution Tri over vocab-
ulary for document-specific words —
Dirichlet(A)
– Draw a distribution Oi giving the mix-
ture of domain subtopics for this docu-
ment — Dirichlet(α)
– For each topic segment Mij in Di:
* Draw a domain subtopic zij —
Multinomial(Oi)
* For each word wijk in segment Mij:
· Draw a word type sijk —
Multinomial(q)
· Depending on the chosen switch
value sijk, draw the word from
the subtopic of the segment φzi,
or document-specific vocabulary
Tri, or stopwords ψ
</listItem>
<bodyText confidence="0.734521">
We use the section markings in the Wikipedia
articles as topic segment boundaries while learn-
ing the model. We use symmetric Dirichlet priors
</bodyText>
<page confidence="0.989756">
157
</page>
<bodyText confidence="0.99996822">
for the vocabulary distributions associated with
domain subtopics, document-specific words and
stopwords. The concentration parameters are set
to 0.001 to encourage sparsity. The distribution
BZ for per-document subtopics is also drawn from
a symmetric Dirichlet distribution with concentra-
tion parameter 0.01. We use asymmetric Dirich-
let priors for q set to (5, 3, 2) for (domain topic,
document-specific, stopwords). The hyperparam-
eter values were minimally tuned so that the differ-
ent vocabulary distributions behaved as intended.
We perform inference using collapsed Gibbs
sampling where we integrate out many multinomi-
als. The sampler chooses a topic zZj for every seg-
ment and then samples a word type sZjk for each
word in the segment. We initialize these variables
randomly and the assignment after 1000 Gibbs it-
erations are taken as the final ones. We create
these models separately for English and French,
in each case obtaining T domain subtopics.
The second step creates an alignment between
the source and target topics using a bilingual dic-
tionary3. For each French topic, we find the top
matching English topic by scoring the number
of dictionary matches. It is unlikely for every
French topic to have a closely corresponding En-
glish topic. Based on observations about the qual-
ity of topic alignment, we select the top 60% (out
of T) pairs of French-English aligned topics only.
Note that our method uses two steps to learn
bilingual topics in contrast to some multilingual
topic models which learn aligned topics directly
from parallel or comparable corpora (Zhao and
Xing, 2006; Boyd-Graber and Blei, 2009; Jagar-
lamudi and Daum´e III, 2010). These methods in-
duce topic-specific translations of words. Rather
we choose a less restrictive pairing of word clus-
ters by topic since (i) we have monolingual bi-
ographies in the two languages which could be
quite heterogenous in the types of personalities
discussed, (ii) we seek to identify words likely in a
topic segment for example ‘career-related’ words
rather than specific translations for source words.
During translation, for each topic segment in the
source document, we infer the French topic most
likely to have produced the segment and find the
corresponding English-side topic. The most prob-
able words for that English topic are then loaded
into the topic cache. The score for a word is its
probability in that topic. When a topic segment
</bodyText>
<footnote confidence="0.937497">
3A filtered set of 13,400 entries from www.dict.cc
</footnote>
<bodyText confidence="0.998937">
boundary is reached, the topic cache is cleared and
the topic words for the new segment are filled.
The consistency cache’s contents are computed
similarly to the general domain case. However, the
cache gets cleared at segment boundaries.
</bodyText>
<sectionHeader confidence="0.900152" genericHeader="method">
4 Training and test data
</sectionHeader>
<bodyText confidence="0.999968976190477">
We distinguish two resources for data. The out-
of-domain system is trained using the WMT’12
datasets comprising Europarl and news commen-
tary texts. It has 2,144,820 parallel French-
English sentence pairs. The language model is
trained using the English side of the training cor-
pus. The tuning set has 2,489 sentence pairs.
Our test set is a corpus of French to En-
glish translations of biographies compiled from
Wikipedia. To create the biography corpus, we
collect articles which are marked with a “Trans-
lation template” in Wikipedia metadata. These
markings indicate a page which is translated from
a corresponding page in a different language and
also contains a link to the source article. (Note
that these article pairs are not those written on
the same topic separately in the two languages.)
We collect pairs of French-English pages with this
template and filter those which do not belong to
the Biography topic (using Wikipedia metadata).
Note, however, that these article pairs are not
very close translations. During translation an edi-
tor may omit or add information and also reorga-
nize parts of the article. So we filter out the paired
documents which differ significantly in length. We
use LFAligner4 to create sentence alignments for
the remaining document pairs. We constrain the
alignments to be within documents but since sec-
tion headings were not maintained in translations,
we did not further constrain alignments within sec-
tions. We manually corrected the resulting align-
ments and keep only documents which have good
alignments and have manually marked topic seg-
ments (Wikipedia section headings). Unaligned
sentences were filtered out. Table 1 shows a sum-
mary of this data and the split for tuning and test.
The articles are 12 to 87 sentences long and con-
tain 5 topic segments on average.
We also collect a larger set of monolingual
French and English Wikipedia biographies to cre-
ate the domain subtopics. We select only articles
that have at least 10 segments (sections) to ensure
</bodyText>
<footnote confidence="0.996149">
4http://sourceforge.net/projects/
aligner/
</footnote>
<page confidence="0.967232">
158
</page>
<table confidence="0.998580833333333">
Tuning Test
No. of article pairs 15 30
Total sentences pairs 430 1008
Min. article size (in sentences) 13 12
Max. article size (in sentences) 59 85
Average no. of segments per article 4.7 5.3
</table>
<tableCaption confidence="0.99984">
Table 1: Summary of Wikipedia biographies data
</tableCaption>
<bodyText confidence="0.681749">
that they are comprehensive ones. This collection
contains 1000 French and 1000 English articles.
</bodyText>
<sectionHeader confidence="0.997163" genericHeader="method">
5 Experimental settings
</sectionHeader>
<bodyText confidence="0.9989385">
We use the Moses phrase-based translation system
(Koehn et al., 2007) to implement our models.
</bodyText>
<subsectionHeader confidence="0.727209">
5.1 Out-of-domain model
</subsectionHeader>
<bodyText confidence="0.999961">
This baseline model is trained on the WMT 2012
training sets described in the previous section and
uses the six standard features from Koehn et al.
(2003). We build a 5-gram language model us-
ing SRILM. The features were tuned using MERT
(Och, 2003) on the WMT 2012 tuning sets. This
system does not use any data about biographies.
</bodyText>
<subsectionHeader confidence="0.991499">
5.2 Biography-adapted models
</subsectionHeader>
<bodyText confidence="0.994239142857143">
First we perform experiments using the manually
marked sections in Wikipedia as topic segments.
We also report results with automatic segmenta-
tion in Section 7.
The domain and structured models have two ex-
tra features ‘topic cache’ and ‘consistency cache’.
For the structured model, topic segment bound-
aries and inferred topic is passed as XML markup
on the source documents. For the consistency
cache, we use a wrapper which passes the 1-
best translation (also using XML markup) of the
preceding sentence and updates the cache before
translating every next sentence.
We tune the weights for these new cache fea-
tures as follows. The weights for the baseline fea-
tures from the out-of-domain model are kept con-
stant. The weights for the new cache features are
set using a grid search. This tuning uses the bi-
ographies documents listed in Table 1 as tuning
data. We run the decoding using the baseline fea-
ture weights and a weight for a cache feature and
compute the (case-insensitive) BLEU (Papineni et
al., 2002) scores of each tuning document. The
weight for the cache feature which maximizes the
average BLEU value over the tuning documents
is chosen. We have not tuned the features us-
ing MERT in this study since a grid search al-
lowed us to quantify the influence of increasing
</bodyText>
<figureCaption confidence="0.9970875">
Figure 1: Effect of feature weights and number of
topics on accuracy for structured topic cache
</figureCaption>
<bodyText confidence="0.998271972222222">
weights on the new features directly. Previous
work has noted that MERT fails to find good set-
tings for cache models (Tiedemann, 2010b). In
future work, we will explore how successful op-
timization of baseline and cache feature weights
could be done jointly. We present the findings
from our grid search below.
The struct-topic cache has two parameters, the
number of topics T and the number of most prob-
able words from each topic which get loaded into
the cache. We ran the tuning for T = 25, 50,
100 and 200 topics (note that 60% of the topics
will be kept after bilingual alignment, see Section
3.2). We also varied the number of topic words
chosen—50, 100, 250 and 500.
The performance did not vary with the number
of topic words used and 50 words gave the same
performance as 500 words for topic models with
any number of topics. This interesting result sug-
gests that only the most likely and basic words
from each topic are useful. The top 50 words from
two topics (one capturing early life and the other
an academic career) taken from the 50-topic model
on English biographies are shown in Table 2.
In Figure 1, we show the performance of sys-
tems using different number of topics. In each
case, the same number of topic words (50) was
added to the cache. We find that 50 topics model
performs best confirming our hypothesis that only
a small number of domain subtopics is plausible.
We choose the 50 topic model with top 50 words
for each topic for the structured topic cache.
The best weights and average document level
BLEU scores on the tuning set are given in Table
3. The scores were computed using the mteval-
v13a.pl script in Moses. BLEU scores for the
</bodyText>
<page confidence="0.997821">
159
</page>
<bodyText confidence="0.997706111111111">
his a s family on life She child St mother
of in married children They death became whom friends attended
and had He that daughter son marriage lived later work
to was born died wife years met couple I age
he her at she father home moved about husband house
of is He received has included National original Academy French
and The by its used works study list book College
his work Award Medal award His Institute life contributed Year
in he are awarded also title Arts Royal edition awards
</bodyText>
<table confidence="0.312485">
s University Prize Society A honorary Library include Sciences recognition
</table>
<tableCaption confidence="0.997005">
Table 2: Top 50 words from 2 topics of the T = 50 topic model
</tableCaption>
<table confidence="0.999912444444444">
Cache type weight BLEU-doc Model BLEU-doc BLEU-sent
Domain-topic 0.075 19.79 Domain-topic 17.63 17.61
Domain-consistency 0.05 19.70 Domain-consistency 17.70 17.75
Domain-topic + consis. 0.05, 0.05 19.80 Domain-topic + consis. 17.63 17.63
Struct-topic (50 topics) 1.75 19.94 Struct-topic (50 topics) 17.76 17.84
Struct-consistency 0.125 19.70 Struct-consistency 17.33 17.34
Struct-topic + consis. 0.4, 0.1 19.84 Struct-topic + consis. 17.47 17.51
Domain-consis. + struct-topic 0.1, 0.25 19.86 Struct-topic + dom-consis. 17.29 17.25
Out-of-domain 19.33 Out-of-domain 17.37 17.43
</table>
<tableCaption confidence="0.937076">
Table 3: Best weights for cache features and
BLEU scores (averaged for tuning documents).
</tableCaption>
<bodyText confidence="0.9503955">
out-of-domain model are shown on the last line.
Note that these scores are overall on a lower scale
for a French-English system due to out-of-domain
differences and because the reference translations
from Wikipedia are not very close ones.
These numbers show that cache models have the
potential to provide better translations compared
to an out-of-domain baseline. The structured topic
model system is the best system outperforming the
out-of-domain system and also the domain-topic
system. Hence, treating documents as composed
of topical segments is a useful setting for auto-
matic translation.
The domain and structured versions of the con-
sistency cache however, show no difference. This
result could arise due to the decay factor incor-
porated in the consistency cache. Higher scores
are given to words from immediately previous
sentences compared to those far off. This decay
implicitly gives lower scores to words from ear-
lier topic segments than those from recent ones.
Explicitly refreshing the cache in the structured
model does not give additional benefits.
When consistency and topic caches are used to-
gether in both general domain and structured set-
tings, the combination is not better than individual
caches. We also tried a setting where the consis-
tency cache is document-range and the topic cache
works at segment level (domain-consis. + struct-
topic). This combination also does not outperform
using the structured topic cache alone.
Table 4: BLEU scores on the test set. ‘doc’ in-
dicates BLEU scores averaged over documents,
‘sent’ indicates sentence-level BLEU
</bodyText>
<sectionHeader confidence="0.986343" genericHeader="method">
6 Results on the test corpus
</sectionHeader>
<bodyText confidence="0.999984310344827">
The best weights chosen on the tuning corpus are
used to decode the biographies test corpus (sum-
marized in Table 1). Table 4 reports the av-
erage BLEU of documents as well as sentence
level BLEU scores of the corpus. We used the
paired bootstrap resampling method (Koehn 2004)
to compute significance.
The struct-topic model gives the highest im-
provement of 0.4 sentence level BLEU over the
out-of-domain model. Struct-topic is also 0.23
BLEU points better compared to the domain-
topic model confirming the usefulness of model-
ing structure regularities. These improvements at
significant at 95% confidence level.
The second best model is the domain-
consistency model (significantly better than out-
of-domain model at 90% confidence level). But
the performance of this cache decreases in the
structured setting. Moreover, combinations of
caches fail to improve over individual caches.
One hypothesis for this result is that biogra-
phy subtopic words which give good performance
in the topic cache differ from the words which
provide benefits in the consistency cache. For
example, words related to named entities and
other document-specific content words could be
ones that are more consistent within the docu-
ment. Then clearing the consistency cache at topic
boundaries would remove such words from the
</bodyText>
<page confidence="0.991875">
160
</page>
<bodyText confidence="0.999955090909091">
cache leading to low performance of the ‘struc-
tured’ version. In our current model, we do not
distinguish between words making up the consis-
tency cache. In future, we plan to experiment
with consistency caches of different ranges and
which hold different types of words. This ap-
proach would require identifying named entities
and parts of speech on the automatic translations
of previous sentences, which is likely to be error-
prone and so require methods for associating a
confidence measure with the cache words.
</bodyText>
<sectionHeader confidence="0.832537" genericHeader="method">
7 Understanding factors that influence
structured cache models
</sectionHeader>
<bodyText confidence="0.999910447368421">
The documents in our test corpus have varying
lengths, number of segments and segment sizes.
This section explores the behavior of structured
models on these different document types. For
this analysis, we compare the BLEU scores from
the domain and the structured versions of the two
caches. We do not consider the out-of-domain sys-
tem here since we are interested in quantifying
gains from using document structure.
For each document in our test corpus, we com-
pute (i) the difference between the BLEU scores
of struct-topic and domain-topic systems (BLEU-
gain-topic), and (ii) the difference in BLEU
scores between the struct-consistency and domain-
consistency systems (BLEU-gain-consis). Table 5
reports the average BLEU gains binned by a) the
document length (in sentences) b) number of topic
segments in the document and c) the average size
of topic segments in a document (in sentences).
The numbers clearly indicate that performance
is not uniform across different types of docu-
ments. The struct-topic cache performs much bet-
ter on longer documents of over 30 sentences giv-
ing 0.3 to 0.4 BLEU points increase compared to
the general domain model. On the other hand, the
performance worsens when the structured cache
is applied on documents with less than 20 sen-
tences. Similarly, the struct-topic cache is benefi-
cial for documents where the average segment size
is larger than 5 sentences and when the number of
topic segments is around 5 to 7.
The struct-consistency cache generally per-
forms worse than the unstructured version and
there does not appear to be a niche set according
to any of the properties—document length, num-
ber of segments and segment size.
Given these findings, it is possible that the
struct-topic cache can benefit by modifying the
</bodyText>
<figure confidence="0.747299705882353">
(a) Average BLEU gains and document length
doc. length no. docs gain-topic gain-consis
12 to 19 7 -0.41 -0.20
20 to 29 10 0.17 -0.63
30 to 49 8 0.44 -0.16
50 to 85 5 0.34 -0.45
(b) Average BLEU gains and no. of topic segments
no. segments no. docs gain-topic gain-consis
3 to 4 9 -0.09 -0.21
5 13 0.24 -0.37
6 to 7 5 0.34 -0.74
9 3 -0.03 -0.26
(c) Average BLEU gains and topic segment size
avg. segment size no. docs gain-topic gain-consis
&lt; 5 10 -0.23 -0.41
5 to 10 18 0.33 -0.37
11 to 17 2 0.39 -0.24
</figure>
<tableCaption confidence="0.755562">
Table 5: Average BLEU score gains from a struc-
tured cache (compared to domain caches) split by
different properties of documents in the test set
</tableCaption>
<bodyText confidence="0.999572352941177">
document structure to match that handled better
by the structured model. We test this hypothe-
sis by segmenting all test documents with an ideal
segment size. The model seems to perform better
when each segment has around 5 to 10 sentences
(longer segments are also preferred but we have
few very long documents in our corpus), so we
try to re-segment the articles to contain approxi-
mately 7 sentences in each segment. We use an
automatic topic segmentation method (Eisenstein
and Barzilay, 2008) to segment the source arti-
cles in our test corpus. For each article we request
(document length)/7 segments to be created.5
We then run the structured topic and consis-
tency models on the automatically segmented cor-
pus using the same feature weights as before. The
results are shown in Table 6.
</bodyText>
<table confidence="0.489743">
Model BLEU (doc) BLEU (sent)
Struct-topic 17.94 17.94
Struct-consistency 17.51 17.46
</table>
<tableCaption confidence="0.9741015">
Table 6: Translation performance on automati-
cally segmented test corpus
</tableCaption>
<bodyText confidence="0.999818555555556">
The struct-topic cache now reaches our best re-
sult of 0.5 BLEU improvement over the out-of-
domain model and 0.3 improvement over the un-
structured domain model. The consistency cache
is also slightly better using the automatic segmen-
tation than the manual sections. Choosing the
right granularity appears to be important for struc-
tured caches and coarse section headers may not
be ideal. This result also shows automatic segmen-
</bodyText>
<footnote confidence="0.857514">
5Note that we only specify the number of segments, but
the system could create long or short segments.
</footnote>
<page confidence="0.987174">
161
</page>
<bodyText confidence="0.988651">
of (42) he (36) his (36) the (22) to (11) in (9) was (7) one (6) a (3) at (3)
head (3) that (3) construction (3) empire office french bases reconstruction only such
all ban marseille main charged have well researchers openness retreat
an two mechanical events army iron class surrender order thirty
and black objectives factory disciple largest close budget part time
as who ceremony figure majority level even sentence project trained
on seat diplomatic wheat working winner life archaeological 9 during
</bodyText>
<tableCaption confidence="0.961436">
Table 7: Impact words computed on the test corpus. The number of times each word was found in the
impact list is indicated within parentheses. Words listed without parentheses appeared once in the list.
</tableCaption>
<figure confidence="0.993842">
(1) (S) Pendant la Premi`ere Guerre mondiale, mobilis´e dans les troupes de marine, il combat dans les Balkans et les
Dardanelles.
(R) During the First World War, conscripted into the navy, he fought in the Balkans and the Dardanelles.
(B) During World War I, mobilized in troops navy, it fight in the Balkans and Dardanelles.
(C) During World War I, mobilized troops in the navy, he fight in the Balkans and the Dardanelles.
(2) (S) A` l’ˆage de 15 ans, elle a ´et´e choisie par la troupe d’op´era de l’arm´ee chinoise pour ˆetre form´ee au chant.
(R) At the age of 15, she was selected by the Chinese Armys Operatic troupe to be trained as a singer.
(B) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be formed the call.
(C) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be trained to call.
(3) (S) La figure de la Corriveau n’a cess´e, depuis, d’inspirer romans, chansons et pi`eces de th´eˆatre et d’alimenter les
controverses.
(R) The figure of Corriveau still inspires novels, songs and plays and is the subject of argument.
(B) The perceived the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
(C) The figure of the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
</figure>
<tableCaption confidence="0.818995">
Table 8: Three examples of impact words in test translations. Abbreviations: S - source sentence, R -
reference translation, B - baseline translation, C - structured topic cache translation
</tableCaption>
<bodyText confidence="0.498799">
tation can be successfully used in these models.
</bodyText>
<sectionHeader confidence="0.651275" genericHeader="method">
8 Changes made by the cache models
</sectionHeader>
<bodyText confidence="0.99997935483871">
Here we examine the kinds of changes made by
the cache models which have lead to the im-
proved BLEU scores. We focus on the the topic
cache since its changes are straightforward to
compute compared to consistency. We analyze
the struct-topic cache translations on automati-
cally segmented documents as that provided the
best performance overall.
To do this analysis, we define the notion of an
impact word. An impact word is one which satis-
fies three conditions: (i) the word is not present in
the out-of-domain translation of a sentence, (ii) it
is present in the translation produced by the topic
cache model (iii) the word matches the reference
translation for the sentence.
These impact words provide a simple (albeit ap-
proximate) way to analyze useful changes made
by the topic cache over the out-of-domain system.
On the test corpus (30 documents), 231 impact
word tokens were found and they come from 70
unique word types. So topic cache model signif-
icantly affects translation decisions and over 200
useful word changes were made in the 30 doc-
uments. The impact word types and counts are
shown in Table 7. Several of these changes relate
to function words and pronouns. For example, the
pronoun ‘he’ and the past tense verb ‘was’ were
correctly introduced in several sentences such as
Example (1) in Table 8. A content word change is
indicated in examples (2) and (3). These changes
appear to be appropriate for biographies.
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999938230769231">
We have introduced a new corpus of biography
translations which we propose as suitable for ex-
amining discourse-motivated SMT methods. We
showed that cache-based techniques which also
take the topic organization into account, make
more appropriate lexical choices for the domain.
In future work, we plan to explore how other do-
main similarities such as sentence syntax and en-
tity reference, for example biographies have a cen-
tral entity (person), can be used to improve transla-
tion performance. We also plan to take advantage
of recent methods to do document level decoding
(Hardmeier et al., 2012).
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999358">
The first author was supported by a Newton Inter-
national Fellowship (NF120479) from the Royal
Society and The British Academy. We also thank
the NLP group at Edinburgh for their comments
and feedback on this work.
</bodyText>
<page confidence="0.996991">
162
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844602409638">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In Proceed-
ings of UAI, pages 75–82.
Marine Carpuat, Hal Daum´e III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Ale&amp;quot;s Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL, pages
115–119.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334–343.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli,
Michael Paul, and Sebastian Stueker. 2012.
Overview of the IWSLT 2012 evaluation campaign.
Proceedings of IWSLT.
George Foster, Pierre Isabelle, and Roland Kuhn.
2010. Translating structured documents. In Pro-
ceedings of AMTA.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of EMNLP, pages
909–919.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the EMNLP-CoNLL, pages 1179–1190.
Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. In Advances in Information Retrieval,
Lecture Notes in Computer Science, pages 444–456.
Springer Berlin Heidelberg.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-HLT, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL meeting on Interac-
tive Poster and Demonstration Sessions, pages 177–
180.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of COLING, pages 495–501.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. Corpus number LDC2008T19, Lin-
guistic Data Consortium, Philadelphia.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings ofACL, pages 459–468.
J¨org Tiedemann. 2010a. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing.
J¨org Tiedemann. 2010b. To cache or not to cache?:
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 189–194.
Bing Zhao and Eric P. Xing. 2006. Bitam: bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING-ACL, pages 969–976.
</reference>
<page confidence="0.999119">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646595">
<title confidence="0.998545">Structured and Unstructured Cache Models for SMT Domain Adaptation</title>
<author confidence="0.95373">Annie</author>
<affiliation confidence="0.994887">School of University of</affiliation>
<address confidence="0.828645">10 Crichton Edinburgh EH8</address>
<email confidence="0.996257">alouis@inf.ed.ac.uk</email>
<abstract confidence="0.999044576923077">We present a French to English translation system for Wikipedia biography articles. We use training data from outof-domain corpora and adapt the system for biographies. We propose two forms of domain adaptation. The first biases the system towards words likely in biographies and encourages repetition of words across the document. Since biographies in Wikipedia follow a regular structure, our second model exploits this structure as a sequence of topic segments, where each segment discusses a narrower subtopic of the biography domain. In this structured model, the system is encouraged to use words likely in the current segment’s topic rather than in biographies as a whole. We implement both systems using cachebased translation techniques. We show that a system trained on Europarl and news can be adapted for biographies with 0.5 BLEU score improvement using our models. Further the structure-aware model outperforms the system which treats the entire document as a single segment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="11142" citStr="Blei et al., 2003" startWordPosition="1780" endWordPosition="1783">thin topic segments of the article. The topic cache is filled with words likely in individual topic segments of an article. To do this, we need to identify the topic of smaller segments of the article and also store a set of most probable words for each topic. The topics should also have bilingual mappings which will allow us to infer for every French document segment, words that are likely in such a segment in the English language. We designed and implemented an unsupervised 2If the word already exists in the cache, it is first removed. topic model based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to induce such word clusters. In a first step, we induce subtopics from monolingual articles in English and French separately. The topics are subsequently aligned between the languages as explained below. In the first step, we learn a topic model which incorporates two main ideas a) adds sensitivity to topic boundaries by assigning a single topic per topic segment b) allows for additional flexibility by not only drawing the words of a segment from the segment-level topic, but also allows some words to be either specific to the document (such as named entities) or stop words. To address idea b</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="14530" citStr="Boyd-Graber and Blei, 2009" startWordPosition="2343" endWordPosition="2346">between the source and target topics using a bilingual dictionary3. For each French topic, we find the top matching English topic by scoring the number of dictionary matches. It is unlikely for every French topic to have a closely corresponding English topic. Based on observations about the quality of topic alignment, we select the top 60% (out of T) pairs of French-English aligned topics only. Note that our method uses two steps to learn bilingual topics in contrast to some multilingual topic models which learn aligned topics directly from parallel or comparable corpora (Zhao and Xing, 2006; Boyd-Graber and Blei, 2009; Jagarlamudi and Daum´e III, 2010). These methods induce topic-specific translations of words. Rather we choose a less restrictive pairing of word clusters by topic since (i) we have monolingual biographies in the two languages which could be quite heterogenous in the types of personalities discussed, (ii) we seek to identify words likely in a topic segment for example ‘career-related’ words rather than specific translations for source words. During translation, for each topic segment in the source document, we infer the French topic most likely to have produced the segment and find the corre</context>
</contexts>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of UAI, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daum´e Alexander Fraser</author>
<author>Chris Quirk</author>
<author>Fabienne Braune</author>
</authors>
<title>Domain adaptation in machine translation: Final report.</title>
<date>2012</date>
<booktitle>In 2012 Johns Hopkins Summer Workshop Final Report.</booktitle>
<location>Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Ale&amp;quot;s Tamchyna, Katharine</location>
<contexts>
<context position="2349" citStr="Carpuat et al., 2012" startWordPosition="357" endWordPosition="360">iographies from French to English by adapting a system Bonnie Webber School of Informatics University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB bonnie@inf.ed.ac.uk trained on Europarl and news commentaries. This task is interesting for the following two reasons. Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles (Tiedemann, 2010a), blogs (Su et al., 2012) and transcribed lectures (Federico et al., 2012). The main challenge for such systems is translating out-of-vocabulary words (Carpuat et al., 2012). In contrast, words in biographies are closer to a training corpus of news commentaries and parlimentary proceedings and allow us to examine how well domain adaptation techniques can disambiguate lexical choices. Such an analysis is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as ‘childhood’, ‘schooling’, ‘career’ and ‘later life’, and a likely chronological order to these topics. These regularities become more predictable in documents from sourc</context>
</contexts>
<marker>Carpuat, Fraser, Quirk, Braune, 2012</marker>
<rawString>Marine Carpuat, Hal Daum´e III, Alexander Fraser, Chris Quirk, Fabienne Braune, Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Ale&amp;quot;s Tamchyna, Katharine Henry, and Rachel Rudinger. 2012. Domain adaptation in machine translation: Final report. In 2012 Johns Hopkins Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>115--119</pages>
<contexts>
<context position="6661" citStr="Eidelman et al., 2012" startWordPosition="1030" endWordPosition="1033">ce article, and (ii) words from topical clusters created on the training set. However, a central issue in these systems is that caches become noisy over time, since they ignore topic shifts in the documents. This paper presents cache models which not only take advantage of likely words in the domain and consistency, but which also adapt to topic shifts. A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. 3 System adaptation for biographies We introduce two types of translation systems adapted for biographies: General domain models (domain-) that use information ab</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of ACL, pages 115–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="29250" citStr="Eisenstein and Barzilay, 2008" startWordPosition="4782" endWordPosition="4785">verage BLEU score gains from a structured cache (compared to domain caches) split by different properties of documents in the test set document structure to match that handled better by the structured model. We test this hypothesis by segmenting all test documents with an ideal segment size. The model seems to perform better when each segment has around 5 to 10 sentences (longer segments are also preferred but we have few very long documents in our corpus), so we try to re-segment the articles to contain approximately 7 sentences in each segment. We use an automatic topic segmentation method (Eisenstein and Barzilay, 2008) to segment the source articles in our test corpus. For each article we request (document length)/7 segments to be created.5 We then run the structured topic and consistency models on the automatically segmented corpus using the same feature weights as before. The results are shown in Table 6. Model BLEU (doc) BLEU (sent) Struct-topic 17.94 17.94 Struct-consistency 17.51 17.46 Table 6: Translation performance on automatically segmented test corpus The struct-topic cache now reaches our best result of 0.5 BLEU improvement over the out-ofdomain model and 0.3 improvement over the unstructured dom</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP, pages 334–343.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcello Federico</author>
</authors>
<institution>Mauro Cettolo, Luisa Bentivogli,</institution>
<marker>Federico, </marker>
<rawString>Marcello Federico, Mauro Cettolo, Luisa Bentivogli,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Sebastian Stueker</author>
</authors>
<title>evaluation campaign.</title>
<date>2012</date>
<journal>Overview of the IWSLT</journal>
<booktitle>Proceedings of IWSLT.</booktitle>
<marker>Paul, Stueker, 2012</marker>
<rawString>Michael Paul, and Sebastian Stueker. 2012. Overview of the IWSLT 2012 evaluation campaign. Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Roland Kuhn</author>
</authors>
<title>Translating structured documents.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="3199" citStr="Foster et al. (2010)" startWordPosition="487" endWordPosition="490"> is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as ‘childhood’, ‘schooling’, ‘career’ and ‘later life’, and a likely chronological order to these topics. These regularities become more predictable in documents from sources such as Wikipedia. This setting allows us to explore the utility of models which make translation decisions depending on the discourse structure. Translation methods for structured documents have only recently been explored in Foster et al. (2010). However, their system was developed for parlimentary proceedings and translations were adapted using separate language models based upon the identity of the speaker, text type (questions, debate, etc.) and the year when the proceedings took place. Biographies constitute a more realistic discourse context to develop structured models. This paper introduces a new corpus consisting of paired French-English translations of biography articles from Wikipedia.1 We translate this corpus by developing cache-based domain adaptation methods, a technique recently proposed by Tiede1Corpus available at ht</context>
</contexts>
<marker>Foster, Isabelle, Kuhn, 2010</marker>
<rawString>George Foster, Pierre Isabelle, and Roland Kuhn. 2010. Translating structured documents. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>909--919</pages>
<contexts>
<context position="5849" citStr="Gong et al. (2011)" startWordPosition="890" endWordPosition="893"> proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (2010b) and Gong et al. (2011), were applied for translating in-domain documents. Gong et al. (2011) introduced additional caches to store (i) words and phrase pairs from training documents most similar to a current source article, and (ii) words from topical clusters created on the training set. However, a central issue in these systems is that caches become noisy over time, since they ignore topic shifts in the documents. This paper presents cache models which not only take advantage of likely words in the domain and consistency, but which also adapt to topic shifts. A different line of work very relevant to our study is</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of EMNLP, pages 909–919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrasebased statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMNLP-CoNLL,</booktitle>
<pages>1179--1190</pages>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrasebased statistical machine translation. In Proceedings of the EMNLP-CoNLL, pages 1179–1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
</authors>
<title>Extracting multilingual topics from unaligned comparable corpora.</title>
<date>2010</date>
<booktitle>In Advances in Information Retrieval, Lecture Notes in Computer Science,</booktitle>
<pages>444--456</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Jagarlamudi, Daum´e, 2010</marker>
<rawString>Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Extracting multilingual topics from unaligned comparable corpora. In Advances in Information Retrieval, Lecture Notes in Computer Science, pages 444–456. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="18316" citStr="Koehn et al. (2003)" startWordPosition="2959" endWordPosition="2962"> of article pairs 15 30 Total sentences pairs 430 1008 Min. article size (in sentences) 13 12 Max. article size (in sentences) 59 85 Average no. of segments per article 4.7 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 Biography-adapted models First we perform experiments using the manually marked sections in Wikipedia as topic segments. We also report results with automatic segmentation in Section 7. The domain and structured models have two extra features ‘topic cache’ and ‘consistency cache’. For the structured model, topic segment boundaries and inferred topic is passed as XML markup on the source documents. For the consistenc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL-HLT, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL meeting on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra</location>
<contexts>
<context position="18112" citStr="Koehn et al., 2007" startWordPosition="2925" endWordPosition="2928">and English Wikipedia biographies to create the domain subtopics. We select only articles that have at least 10 segments (sections) to ensure 4http://sourceforge.net/projects/ aligner/ 158 Tuning Test No. of article pairs 15 30 Total sentences pairs 430 1008 Min. article size (in sentences) 13 12 Max. article size (in sentences) 59 85 Average no. of segments per article 4.7 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 Biography-adapted models First we perform experiments using the manually marked sections in Wikipedia as topic segments. We also report results with automatic segmentation in Section 7. The domain and structured mod</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the ACL meeting on Interactive Poster and Demonstration Sessions, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="8947" citStr="Lin and Hovy, 2000" startWordPosition="1403" endWordPosition="1406">the general domain and structured models, but the cache words and scores are computed differently. 3.1 A general domain model This system seeks to bias translations towards words which occur often in biography articles. The topic cache is filled with word unigrams that are more likely to occur in biographies com156 pared to general news documents. We compare the words from 1,475 English Wikipedia biographies articles to those in a large collection (64,875 articles) of New York Times (NYT) news articles (taken from the NYT Annotated Corpus (Sandhaus, 2008)). We use a log-likelihood ratio test (Lin and Hovy, 2000) to identify words which occur with significantly higher probability in biographies compared to NYT. We collect only words indicated with 0.0001 significance by the test to be more likely in biographies. We rank this set of 18,597 words in decreasing order of frequency in the biography article set and assign to each word a score equal to 1/rank of the word. These words with their associated scores form the contents of the topic cache. In the general domain model, these same words are assumed to be useful for the full document and so the cache contents remain constant during translation of the </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of COLING, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="18410" citStr="Och, 2003" startWordPosition="2978" endWordPosition="2979">cle size (in sentences) 59 85 Average no. of segments per article 4.7 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 Biography-adapted models First we perform experiments using the manually marked sections in Wikipedia as topic segments. We also report results with automatic segmentation in Section 7. The domain and structured models have two extra features ‘topic cache’ and ‘consistency cache’. For the structured model, topic segment boundaries and inferred topic is passed as XML markup on the source documents. For the consistency cache, we use a wrapper which passes the 1- best translation (also using XML markup) of the </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19534" citStr="Papineni et al., 2002" startWordPosition="3165" endWordPosition="3168">tency cache, we use a wrapper which passes the 1- best translation (also using XML markup) of the preceding sentence and updates the cache before translating every next sentence. We tune the weights for these new cache features as follows. The weights for the baseline features from the out-of-domain model are kept constant. The weights for the new cache features are set using a grid search. This tuning uses the biographies documents listed in Table 1 as tuning data. We run the decoding using the baseline feature weights and a weight for a cache feature and compute the (case-insensitive) BLEU (Papineni et al., 2002) scores of each tuning document. The weight for the cache feature which maximizes the average BLEU value over the tuning documents is chosen. We have not tuned the features using MERT in this study since a grid search allowed us to quantify the influence of increasing Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Corpus number LDC2008T19, Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="8889" citStr="Sandhaus, 2008" startWordPosition="1394" endWordPosition="1396"> to age. Both the types of caches are present in both the general domain and structured models, but the cache words and scores are computed differently. 3.1 A general domain model This system seeks to bias translations towards words which occur often in biography articles. The topic cache is filled with word unigrams that are more likely to occur in biographies com156 pared to general news documents. We compare the words from 1,475 English Wikipedia biographies articles to those in a large collection (64,875 articles) of New York Times (NYT) news articles (taken from the NYT Annotated Corpus (Sandhaus, 2008)). We use a log-likelihood ratio test (Lin and Hovy, 2000) to identify words which occur with significantly higher probability in biographies compared to NYT. We collect only words indicated with 0.0001 significance by the test to be more likely in biographies. We rank this set of 18,597 words in decreasing order of frequency in the biography article set and assign to each word a score equal to 1/rank of the word. These words with their associated scores form the contents of the topic cache. In the general domain model, these same words are assumed to be useful for the full document and so the</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus. Corpus number LDC2008T19, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Yidong Chen</author>
<author>Xiaodong Shi</author>
<author>Huailin Dong</author>
<author>Qun Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>459--468</pages>
<contexts>
<context position="2201" citStr="Su et al., 2012" startWordPosition="336" endWordPosition="339">exploit these similarities. In this paper we focus on topic (lexical) regularities in a domain. We present a system that translates Wikipedia biographies from French to English by adapting a system Bonnie Webber School of Informatics University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB bonnie@inf.ed.ac.uk trained on Europarl and news commentaries. This task is interesting for the following two reasons. Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles (Tiedemann, 2010a), blogs (Su et al., 2012) and transcribed lectures (Federico et al., 2012). The main challenge for such systems is translating out-of-vocabulary words (Carpuat et al., 2012). In contrast, words in biographies are closer to a training corpus of news commentaries and parlimentary proceedings and allow us to examine how well domain adaptation techniques can disambiguate lexical choices. Such an analysis is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as ‘childhood’, ‘schooli</context>
<context position="6637" citStr="Su et al., 2012" startWordPosition="1026" endWordPosition="1029">to a current source article, and (ii) words from topical clusters created on the training set. However, a central issue in these systems is that caches become noisy over time, since they ignore topic shifts in the documents. This paper presents cache models which not only take advantage of likely words in the domain and consistency, but which also adapt to topic shifts. A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. 3 System adaptation for biographies We introduce two types of translation systems adapted for biographies: General domain models (domain-)</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, and Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings ofACL, pages 459–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Context adaptation in statistical machine translation using models with exponentially decaying cache.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing.</booktitle>
<contexts>
<context position="2174" citStr="Tiedemann, 2010" startWordPosition="333" endWordPosition="334">on for such documents can exploit these similarities. In this paper we focus on topic (lexical) regularities in a domain. We present a system that translates Wikipedia biographies from French to English by adapting a system Bonnie Webber School of Informatics University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB bonnie@inf.ed.ac.uk trained on Europarl and news commentaries. This task is interesting for the following two reasons. Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles (Tiedemann, 2010a), blogs (Su et al., 2012) and transcribed lectures (Federico et al., 2012). The main challenge for such systems is translating out-of-vocabulary words (Carpuat et al., 2012). In contrast, words in biographies are closer to a training corpus of news commentaries and parlimentary proceedings and allow us to examine how well domain adaptation techniques can disambiguate lexical choices. Such an analysis is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics su</context>
<context position="5222" citStr="Tiedemann (2010" startWordPosition="792" endWordPosition="793">esponse to topic segment boundaries. We fill caches with words relevant to the topic of the current segment which is being translated. The cache contents are obtained from an unsupervised topic model which induces clusters of words that are likely to appear in the same topic segment. Evaluation results show that cache-based models give upto 0.5 BLEU score improvements over an out-of-domain system. In addition, models that take topical structure into account score 0.3 BLEU points higher than those which ignore discourse structure. 2 Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20</context>
<context position="20032" citStr="Tiedemann, 2010" startWordPosition="3254" endWordPosition="3255">line feature weights and a weight for a cache feature and compute the (case-insensitive) BLEU (Papineni et al., 2002) scores of each tuning document. The weight for the cache feature which maximizes the average BLEU value over the tuning documents is chosen. We have not tuned the features using MERT in this study since a grid search allowed us to quantify the influence of increasing Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from each topic which get loaded into the cache. We ran the tuning for T = 25, 50, 100 and 200 topics (note that 60% of the topics will be kept after bilingual alignment, see Section 3.2). We also varied the number of topic words chosen—50, 100, 250 and 500. The performance did not vary with the number of topic words </context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010a. Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>To cache or not to cache?: experiments with adaptive models in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>189--194</pages>
<contexts>
<context position="2174" citStr="Tiedemann, 2010" startWordPosition="333" endWordPosition="334">on for such documents can exploit these similarities. In this paper we focus on topic (lexical) regularities in a domain. We present a system that translates Wikipedia biographies from French to English by adapting a system Bonnie Webber School of Informatics University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB bonnie@inf.ed.ac.uk trained on Europarl and news commentaries. This task is interesting for the following two reasons. Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles (Tiedemann, 2010a), blogs (Su et al., 2012) and transcribed lectures (Federico et al., 2012). The main challenge for such systems is translating out-of-vocabulary words (Carpuat et al., 2012). In contrast, words in biographies are closer to a training corpus of news commentaries and parlimentary proceedings and allow us to examine how well domain adaptation techniques can disambiguate lexical choices. Such an analysis is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics su</context>
<context position="5222" citStr="Tiedemann (2010" startWordPosition="792" endWordPosition="793">esponse to topic segment boundaries. We fill caches with words relevant to the topic of the current segment which is being translated. The cache contents are obtained from an unsupervised topic model which induces clusters of words that are likely to appear in the same topic segment. Evaluation results show that cache-based models give upto 0.5 BLEU score improvements over an out-of-domain system. In addition, models that take topical structure into account score 0.3 BLEU points higher than those which ignore discourse structure. 2 Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20</context>
<context position="20032" citStr="Tiedemann, 2010" startWordPosition="3254" endWordPosition="3255">line feature weights and a weight for a cache feature and compute the (case-insensitive) BLEU (Papineni et al., 2002) scores of each tuning document. The weight for the cache feature which maximizes the average BLEU value over the tuning documents is chosen. We have not tuned the features using MERT in this study since a grid search allowed us to quantify the influence of increasing Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from each topic which get loaded into the cache. We ran the tuning for T = 25, 50, 100 and 200 topics (note that 60% of the topics will be kept after bilingual alignment, see Section 3.2). We also varied the number of topic words chosen—50, 100, 250 and 500. The performance did not vary with the number of topic words </context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010b. To cache or not to cache?: experiments with adaptive models in statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 189–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bitam: bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="14502" citStr="Zhao and Xing, 2006" startWordPosition="2339" endWordPosition="2342">creates an alignment between the source and target topics using a bilingual dictionary3. For each French topic, we find the top matching English topic by scoring the number of dictionary matches. It is unlikely for every French topic to have a closely corresponding English topic. Based on observations about the quality of topic alignment, we select the top 60% (out of T) pairs of French-English aligned topics only. Note that our method uses two steps to learn bilingual topics in contrast to some multilingual topic models which learn aligned topics directly from parallel or comparable corpora (Zhao and Xing, 2006; Boyd-Graber and Blei, 2009; Jagarlamudi and Daum´e III, 2010). These methods induce topic-specific translations of words. Rather we choose a less restrictive pairing of word clusters by topic since (i) we have monolingual biographies in the two languages which could be quite heterogenous in the types of personalities discussed, (ii) we seek to identify words likely in a topic segment for example ‘career-related’ words rather than specific translations for source words. During translation, for each topic segment in the source document, we infer the French topic most likely to have produced th</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. Bitam: bilingual topic admixture models for word alignment. In Proceedings of the COLING-ACL, pages 969–976.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>