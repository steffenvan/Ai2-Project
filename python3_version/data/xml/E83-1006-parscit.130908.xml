<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9845855">
AN EXPERT SYSTEM FOR THE PRODUCTION OF PHONEME STRINGS
FROM UNMARKED ENGLISH TEXT USING MACHINE-INDUCED RULES
</note>
<title confidence="0.915080380952381">
Alberto Maria Segre
University of Illinois
at Urbana-Champaign
Coordinated Science
Laboratory
1101 W. Springfield
Urbana, IL 61801 U.S.A.
Bruce Arne Sherwood
University of Illinois
at Urbana-Champaign
. Computer-based Education
Research Laboratory
103 S. Mathews
Urbana, IL 61801 U.S.A.
Wayne B. Dickerson
University of Illinois
at Urbana-Champaign
English as a Second Language
Foreign Language Building
707 S. Mathews
Urbana, IL 61801 U.S.A.
</title>
<sectionHeader confidence="0.909346" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999879217391304">
The speech synthesis group at the Computer-
Based Education Research Laboratory (CERL) of the
University of Illinois at Urbana-Champaign is
developing a diphone speech synthesis system based
on pitch-adaptive short-time Fourier transforms.
This system accepts the phonemic specification of
an utterance along with pitch, time, and amplitude
warping functions in order to produce high quality
speech output from stored diphone templates.
This paper describes the operation of a
program which operates as a front end for the
diphone speech synthesis system. The UTTER (for
&amp;quot;Unmarked Text Transcription by Expert Rule&amp;quot;)
system maps English text onto a phoneme string,
which is then used as an input to the diphone
speech synthesis system. The program is a two-
tiered Expert System which operates first on the
word level and then on the (vowel or consonant)
cluster level. The system&apos;s knowledge about
pronunciation is organized in two decision trees
automatically generated by an induction algorithm
on a dynamically specified &amp;quot;training set&amp;quot; of
examples.
</bodyText>
<sectionHeader confidence="0.984522" genericHeader="keywords">
I INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9990964">
Most speech synthesis systems in use today
require that eventual utterances be specified in
terms of phoneme strings. The automatic
transformation of normal English texts into
phoneme strings is therefore a useful front-end
process for any speech synthesis unit which
requires such phonemic utterance specification.
Unfortunately, this transcription process is
not nearly as straightforward as one might
initially imagine. It is common knowledge to
nonnative speakers that English poses some
particularly treacherous pronunciation problems.
This is due, in part, to the mixed heritage of the
language, which shares several orthographic
bloodlines.
Past attempts to create orthographically-
based computer algorithms have not met with great
success. Algorithms such as the Naval Research
Laboratory pronunciation algorithm [Elovitz76] are
letter-based instead of linguistically-based. For
this reason, such algorithms are excessively rigid
in that they are often unable to cope with a
letter pattern that maps onto more than one
phoneme pattern. Extreme cases are those words
which, although differing in pronunciation, share
orthographic representations (an analogous problem
exists in speech recognition, where words which
share phonemic representations differ in
orthographic representation, and therefore
possibly in semantic interpretation). A notable
exception is the MIT speech synthesis system
[Allen81] which is linguistically-based, but not
solely phoneme-based.
A desirable feature in any rule-based system
is the ability to automatically acquire or modify
its own rules. Previous work [Oakey81] applies
this automatic inference process to the text-to-
phoneme transcription problem. Unfortunately,
Oakey&apos;s system is strictly letter-based and
suffers from the same deficiencies as other
nonlinguistically-based systems.
The UTTER system is an attempt to provide a
linguistically-based transcription system which
has the ability to automatically acquire its own
rule base.
</bodyText>
<sectionHeader confidence="0.937466" genericHeader="introduction">
II METHOD
</sectionHeader>
<bodyText confidence="0.999758409090909">
The system&apos;s basic goal is the transcription
of input text into phoneme strings. The method
used to accomplish this goal is based on a method
taught to foreign students which enables them to
properly pronounce unknown English words
[DickersonF1, DickersonF2]. The method is
basically a two stage process. The first stage
consists in assigning major stress to one of the
word&apos;s syllables. The second stage maps a vowel or
consonant group with a known stress value uniquely
onto its corresponding phoneme string. It is the
stress-assignment process which distinguishes this
pronunciation method from applying purely letter-
based text-to-speech rules, as in, for example,
the Naval Research Laboratory algorithm
[Elovitz76].
In order to accomplish the transcription of
text into phoneme strings, the system uses a set
of two transcription rules which are machine
generated over a set of sample transcriptions. As
the system transcribes new input texts, any
improper transcriptions (i.e., mispronunciations)
</bodyText>
<page confidence="0.998624">
35
</page>
<bodyText confidence="0.999472214285714">
would be flagged by the user and added to the
sample set for future generations of transcription
rules.
The first stage operates •Ion &amp;quot;words° while
the second stage operates on &amp;quot;clusters&amp;quot; of vowels
or consonants.2 Each word is examined
individually, and &amp;quot;major stress&amp;quot;3 is assigned to
&apos;
one of the &amp;quot;syllables&amp;quot; .&apos;1 Major stress is assigned
on the basis of certain &amp;quot;features&amp;quot; or
&amp;quot;attributes&amp;quot;5 extracted from the word (an example
of a word-level attribute is &amp;quot;suffix-type&amp;quot;). The
assignment of major stress is always made uniquely
for a given word. The assignment process consists
of invoking and applying the &amp;quot;stress-rule&amp;quot;.
The &amp;quot;stress-rule&amp;quot; is one of two machine-
generated transcription rules, the other being the
&amp;quot;cluster-rule&amp;quot;. A transcription rule consists of a
decision tree which, when invoked, is traversed on
the basis of the feature values of the word or
cluster under consideration. The transcription
rule &amp;quot;test&amp;quot;6 is evaluated and the proper branch is
then selected on the basis of values of the word
features. The process is repeated until a leaf
node of the tree is reached. The leaf node
contains the value returned for that invocation of
this transcription rule, which uniquely determines
which syllable is to receive the major stress.
</bodyText>
<sectionHeader confidence="0.478104" genericHeader="method">
1
</sectionHeader>
<bodyText confidence="0.9759280625">
A &amp;quot;word&amp;quot; is delimited by conventional word
separators such as common punctuation or blank
spaces in the input stream.
2 A &amp;quot;cluster&amp;quot; consists of contiguous vowels or
contiguous consonants. The following classificato-
ry scheme is used to determine if a letter is a
vowel (-v-) or a consonant (-c-):
&amp;quot;a&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;i&amp;quot;, and &amp;quot;o&amp;quot; are -v-,
&amp;quot;U&amp;quot; is -v- unless it follows a &amp;quot;g&amp;quot; or &amp;quot;q&amp;quot;,
&amp;quot;1&amp;quot; is a special consonant represented by -1-,
&amp;quot;r&amp;quot; is a special consonant represented by -r-,
&amp;quot;y&amp;quot; is -v- if it follows -v-, -c-, -1- or -r-,
&amp;quot;w&amp;quot; is -v- if it follows -v-.
3 .Major stress&amp;quot; corresponds to that syllable
which receives the most emphasis in spoken En-
glish.
A &amp;quot;syllable&amp;quot; will be taken to be a set of two
adjacent clusters, with the first cluster of the
vowel type and the second cluster of the consonant
type. For syllable division purposes, if the word
begins with a consonant the first syllable in that
word will consist solely of a consonant cluster.
Similarly, if the word ends in a vowel then the
final syllable will consist of a vowel cluster
alone. In all other eases, a syllable will always
consist of a vowel cluster followed by a consonant
cluster.
5 The terms &amp;quot;feature&amp;quot; and &amp;quot;attribute&amp;quot; will be
used interchangeably to refer to some identifiable
element in a word or cluster. For more information
regarding word or cluster attributes see the fol-
lowing section.
</bodyText>
<page confidence="0.843622">
6
</page>
<bodyText confidence="0.995876030303031">
A transcription rule &amp;quot;test&amp;quot; refers to the
branching criteria at the current node.
After word stress is assigned, each cluster
within the word is considered sequentially. The
cluster features are extracted, and the cluster-
rule is invoked and applied to obtain the phonemic
transcription for that particular cluster. Note
that one of the cluster features is the stress of
the particular syllable to which the cluster
belongs. In other words, it is necessary to
determine major stress before it is possible to
transcribe the individual clusters of which the
word is comprised. The value returned from
invoking the cluster rule is the phoneme string
corresponding to the current cluster.
UTTER uses the World English Spelling
[Sherwood78] phonetic alphabet to specify the
forty-odd sounds in the English language. The
major advantage of WES over other phonetic
representations (such as the International
Phonetic Alphabet, normally referred to as IPA) is
that WES does not require special characters to
represent phonemes. In UTTER&apos;s version of WES,
WES uses no more than two Roman alphabet
characters to specify a phoneme.7
The choice of WES over other phoneme
representation systems was also motivated by the
fact that Glinski&apos;s system [Glinski81], with which
UTTER was designed to interface, uses WES. The
choice was strictly implementational, and by no
means excludes the use of a different
representation system for future versions of
UTTER.
</bodyText>
<sectionHeader confidence="0.974594" genericHeader="method">
III SYSTEM ORGANIZATION
</sectionHeader>
<bodyText confidence="0.999078">
The current implementation of UTTER operates
in one of three modes, each of which corresponds
to one of the three tasks required of the system:
</bodyText>
<listItem confidence="0.930467888888889">
(1) execution Aosie.: the transcription of input
text using existing transcription rules.
(2) ITAIJILIK flagging incorrect
transcriptions for inclusion in the next
generation of transcription rules.
(3) inference Azle: automatic induction of a new
set of transcription rules to cover the set
of training examples (including any additions
made in training mode).
</listItem>
<bodyText confidence="0.9821745">
What follows is a more detailed description
of each of these three modes of operation.
</bodyText>
<sectionHeader confidence="0.708001" genericHeader="method">
11. Execution Mode
</sectionHeader>
<bodyText confidence="0.9962838">
Execution mode is UTTER&apos;s normal mode of
operation. While in execution mode, UTTER accepts
English input one sentence at a time and produces
the corresponding pronunciation as a list of
phonemes.
What follows is a detailed description of
each step taken by UTTER when operating in
execution mode.
7 For a complete listing of the World English
Spelling phonetic alphabet see Appendix A.
</bodyText>
<page confidence="0.985347">
36
</page>
<bodyText confidence="0.98594901754386">
(1) The input text is scanned for word and
cluster boundaries, and lists of pointers to
boundary locations in the string are
constructed. The parser also counts the
number of syllables in each word, and
constructs a new representation of the
Original string which consists only of the
letters &apos;v&apos;, &apos;c&apos;, &apos;1&apos;, and &apos;rt.
This new representation, which will be
referred to as the &amp;quot;vowel-consonant mapping,&amp;quot;
or simply &amp;quot;v-c map,&amp;quot; is the same length as
the original input. Therefore, all pointers
to the original string (such as those showing
word and cluster boundaries) are also
applicable to the v-c map. The v-c map will
be used in the extraction of cluster
features.
(2) Each word is now processed individually. The
first step is to determine whether the next
word belongs to the group of &amp;quot;function
words&amp;quot;.8 If the search through the function
word list is successful, it will return the
cross-listed pronunciation for that word.
Table look-up provides time-efficient
transcription for this small class of words
which have a very high frequency of
occurrence in the English language, as well
as highly irregular pronunciations. If the
word is a function word, its pronunciation is
added to the output and processing continues
with the next word.
Positioning of function words provides a
valuable clue to the syntax of the input.
Syntactic information is essential in
disambiguating certain words. Although the
current version of UTTER supports part-of-
speech distinctions, the current version of
the parser fails to supply this information.
A new version of UTTER should include a
better parser which is capable of making
these sorts of part-of-speech distinctions.9
Such a parser need not be very accurate in
terms of the proper assignment of words to
part-of-speech classes. However, it must be
capable of separating identically spelled
words into different classes on the basis of
function. These words often differ in
pronunciation, such as &amp;quot;present&amp;quot; (N) and
&amp;quot;present&amp;quot; (V) or &amp;quot;moderate&amp;quot; (N) and
&amp;quot;moderate&amp;quot; (V). In other words, the parser
need not classify these two words as noun and
verb, as long as it makes some distinction
between them.
(3) Each word is now checked against another list
of words (with their associated
pronunciations) called the &amp;quot;permanent
exception list,&amp;quot; or PEL. The PEL provides the
</bodyText>
<sectionHeader confidence="0.61806" genericHeader="method">
8
</sectionHeader>
<bodyText confidence="0.557456">
For a complete listing of function words see
</bodyText>
<sectionHeader confidence="0.625862" genericHeader="method">
Appendix B.
</sectionHeader>
<bodyText confidence="0.994224728571428">
9 It should be possible to model a new parser
on an existing parser which already makes this
sort of part-of-speech distinction. For example,
the STYLE program developed at Bell Laboratories
provides a tool for analyzing documents [Cherry80]
and yields more part-of-speech classes than would
be required for UTTER&apos;s purposes.
user with the opportunity to specify common
domain-specific words whose transcription
would best be handled by table-look-up,
without reconstructing the pronunciation of
the word each time it is encountered.
The time required to search this list is
relatively small (provided the size of the
list itself is not too large) compared to the
time necessary for UTTER to transcribe the
word normally.
If the word is on the PEL, its pronunciation
is returned by the search routine and added
to the output. Processing continues with the
next word.
(4) At this point the set of word-level features
is extracted. These features are used by the
stress-rule for the assignment of major
stress to a particular syllable in the word.
A major stress assignment is made for each
word.
The set of word level attributes includes:
part-of-speech (assigned by the parser);
key-syllable (in terms of the v-c map
representation);
left-syllable (in terms of the v-c map
representation);
suffix type (neutral, weak or
strong);
prefix/left-syllable overlap
(true or false).
These features are both necessary and
sufficient to assign major stress to any
given word [Dickerson81].
Although a detailed account of the selection
of these features is beyond the scope of this
paper, an example of an input word and the
appropriate attribute values should give the
reader a better grasp of the word-level
feature concept.
Consider the input word &amp;quot;preeminent&amp;quot;.
The weak suffix &amp;quot;ent&amp;quot; is stripped.
Key-syllable (final syllable excluding
suffixes) is &amp;quot;in&amp;quot;.
Left-syllable (left of key-syllable)
is &apos;teem&amp;quot;.
Prefix (&amp;quot;pre&amp;quot;) overlaps left-syllable
(&amp;quot;eem&amp;quot;) since they share an &amp;quot;e&amp;quot;.
Proper stress placement for the word
&amp;quot;preeminent&amp;quot; is on the left-syllable.
(5) The word and its attributes are checked
against a list of exceptions to the current
stress rule (called the &amp;quot;stress exception
list&amp;quot; or SEL). This list is normally empty,
in which case checking does not take place.
Additions to the list can only be made in
training mode (see below).
If the word and its features are indexed on
the SEL, the SEL search returns the proper
stress in terms of the number 0 or -1. If
stress is returned as 0, major stress falls
on the key-syllable. If stress is returned
as -1, major stress falls on the left-
syllable.
</bodyText>
<page confidence="0.996175">
37
</page>
<bodyText confidence="0.970012055555556">
(6) If the word does not appear on the SEL, then
the current stress rule is applied. The
stress rule is essentially a decision tree
which is traversed on the basis of the values
of the word&apos;s word level attributes.
Application of the stress rule also returns
either 0 or -1.
(7) Now processing continues for the current word
on a cluster-by-cluster basis. The cluster-
level attributes are extracted. They include:
cluster type (vowel or consonant);
cluster (orthography);
left neighbor cluster map (from v-c map);
right neighbor cluster (orthography);
right neighbor cluster map
(from v-c map);
cluster position (prefix, suffix, etc.);
stress (distance in syllables from major
stress syllable).
These features are necessary and sufficient
to classify a cluster Dickerson82].
As before, an example of cluster level
attributes is appropriate. Consider the
cluster &amp;quot;ee&amp;quot; (from our sample word
&amp;quot;preeminent&amp;quot;).
The cluster type is &amp;quot;vowel&amp;quot;.
The cluster orthography is &amp;quot;ee&amp;quot;.
The left neighbor cluster map is &amp;quot;cr&amp;quot;
(v-c map of &amp;quot;pr&amp;quot;).
The right neighbor cluster is &amp;quot;m&amp;quot;.
The right neighbor cluster map is &amp;quot;c&amp;quot;
(v-c map of &amp;quot;m&amp;quot;).
The cluster position is
&amp;quot;word-prefix boundary&amp;quot;.
The cluster is inside the syllable
with major stress (see above).
</bodyText>
<listItem confidence="0.889989">
(8) The cluster and its associated attributes are
checked against a list of exceptions to the
cluster rule (called the &amp;quot;cluster exception
list&amp;quot; or CEL). This list is normally empty,
and additions can only be made in training
mode (see below). If the search through the
CEL is successful, it will return the proper
pronunciation for the particular cluster. The
pronunciation (in terms of a WES phoneme
string) is added to the output, and
processing continues with the next cluster in
the current word, or with the next word.
(9) The cluster transcription rule is applied to
the current cluster. As in the case of the
stress rule, the cluster rule is a decision
tree which is traversed on the basis of the
values of the cluster level attributes. The
cluster rule returns the proper pronunciation
for this particular cluster and adds it (in
terms of a WES phoneme string) to the output.
Processing continues with the next cluster in
the current word, or with tts next word in
the input.
</listItem>
<footnote confidence="0.60124">
2rainira &amp;la
</footnote>
<bodyText confidence="0.996704421875">
When UTTER is operating in training mode, the
system allows the user to correct errors in
transcription interactively by specifying the
proper pronunciation for the incorrectly
transcribed word.
The training mode operates in the same manner
as the execution mode with the exception that,
whenever either rule is applied (see steps 6 and 9
above), the user is prompted for a judgement on
the accuracy of the rule. The user functions as
the &amp;quot;oracle&amp;quot; who has the final word on what is to
be considered proper pronunciation.
Let us assume, for example, that the stress
rule applied to a given word yields the result
&amp;quot;stress left-syllable&amp;quot; (in other words, the rule
application routine returns a -1) and the proper
result should be &amp;quot;stress key-syllable&amp;quot; (or a
result of 0). If the system were operating in
execution mode, processing would continue and it
is unlikely that the word would be properly
transcribed. The user could switch to training
mode and repeat the transcription of the problem
word in the same context.
In training mode, the user has the
opportunity to inspect the results from every rule
application, allowing the user to flag incorrect
results. When an incorrect rule result is
detected, the proper result (along with the
current features) will be saved on the appropriate
exception list. In terms of the previous example,
the current word and word-level features would be
saved on the SEL.
If the given word should arise again in the
same context, the SEL would contain the exception
to the transcription rule, prohibiting the
application of the stress rule. The information
from the SEL (and from the CEL at the cluster-
level) will be used to infer the next generation
of transcription rules.
It is important to note that UTTER makes a
given mistake only once. If the transcription
error is spotted and added to the SEL (or CEL,
depending on which transcription rule is at fault)
it will not be repeated as long as the exception
information exists. The SEL (and CEL) can only be
cleared by the rule inference process (see below)
which guarantees that the new generation of rules
will cover any example that is to be removed from
the appropriate exception list.
Inference }lode
Inference mode allows for the generation of
new transcription rules. The inference routine is
based on techniques developed in artificial
intelligence for the purpose of generating
decision trees based on sets of examples and their
respective classifications (Quinlan79]. The basic
idea behind such an inference scheme is that some
set of examples (the &amp;quot;training set&amp;quot;) and their
proper classifications are available. In
addition, a finite set of features which are
sufficient to classify these examples, as well as
some method for extracting these features, are
also available. For example, consider the training
set (dog, cat, eagle, whale, trout] where each
</bodyText>
<page confidence="0.998218">
38
</page>
<bodyText confidence="0.982591702702703">
element is classified as one of [mammal, fish,
- In addition, consider the feature set
[has-fur, lives-in-water, can-fly, is-warm-
blooded] and assume there exists a method for
extracting values for each feature of every entry
in the training set (in this example, values would
be &amp;quot;true&amp;quot; or &amp;quot;false&amp;quot; but this need not always be
so). From this information, the inference routine
would extract a decision tree whose branch nodes
would be tests of the form &amp;quot;if has-fur is true
then branch-left else branch-right&amp;quot; and whose
terminal nodes would be of the form &amp;quot;the animal in
question is a mammal.&amp;quot; The premis is that such a
decision tree would be capable of correctly
classifying not only the examples contained in the
training set but any other example whose feature
values are known or extractable.10
What follows is a step-by-step description of
the inference algorithm as applied to the
generation of the stress transcription rule.
Generation of the cluster transcription rule is
similar, except that the cluster transcription
rule returns a phoneme string rather than a
number. For a more complete discussion of the
inference algorithm, which would be beyond the
scope of this paper, see [Quinlan79].
(1) The current stress exception list is combined
with the training set used to generate the
previous stress transcription rule. The old
training set is referred to as the &amp;quot;stress
classified list,&amp;quot; or SCL, and is stored
following rule generation.11 Since the SCL is
not used again until a new rule is generated,
it can be stored on an inexpensive remote
device, such as magnetic tape. The SCL (as
well as the COL) tends to become quite
large.12
</bodyText>
<sectionHeader confidence="0.527719" genericHeader="method">
10
</sectionHeader>
<subsectionHeader confidence="0.483799">
The inference algorithm need not be time- or
</subsectionHeader>
<bodyText confidence="0.998620133333333">
space-efficient. In fact, in the current implemen-
tation of UTTER, it is neither. This observation
is not particularly alarming, since inference mode
is not used very often, in comparison to execution
or training modes (where space- and time-
efficiency are particularly vital to fast text
transcription). There are some inference systems
[Oakey81] in which the inference routine is some-
what streamlined and not nearly as inefficient as
in the case of the current implementation. Future
versions of UTTER might consider using a more
streamlined inference routine. However, since the
inference routine need not be invoked very often,
its inefficiency does not have any effect on what
the user percieves as transcription time.
</bodyText>
<page confidence="0.668467">
11
</page>
<subsectionHeader confidence="0.568333">
The equivalent list in the cluster tran-
</subsectionHeader>
<bodyText confidence="0.980889">
scription rule case is called the &amp;quot;cluster classi-
fied list,&amp;quot; or CCL.
</bodyText>
<subsectionHeader confidence="0.383197">
12
</subsectionHeader>
<bodyText confidence="0.986190258064516">
It should be possible to use an existing
computer encoded pronunciation dictionary (or a
subset thereof) to provide the initial SCL and
CCL. The current version of UTTER uses null lists
as the initial SCL and CCL, and therefore forces
the user to build these lists via the SEL and CEL.
This implies a rather time consuming process of
running text through UTTER in training mode. An
(2) Features are extracted for each of the
entries in the training set. Features which
cannot be extracted in isolation, such as
the part-of-speech of a given word, are
stored along with the entry and its result in
the SEL. These unextractable attributes rely
on the context the entry appeared in rather
than on the entry itself and, therefore,
cannot be reconstructed &amp;quot;a posteriori.&amp;quot;
The training set now consists of all of the
entries from the SCL and the SEL, as well as
all of the features for each entry. At this
point an initial &amp;quot;window&amp;quot; on the training set
is chosen. Since the inference algorithm&apos;s
execution time increases combinatorially with
the size of the training set, it is wise to
begin the inference procedure with a subset
of the training set. This is acceptable since
there is often a relatively high rate of
redundancy in the training set. The selection
of the window may be done arbitrarily (as in
the current version of UTTER), or one might
try to select an initial window with the
widest possible set of feature values.13
(3) For each &amp;quot;attribute-value&amp;quot;14 in the current
window a &amp;quot;desirability index&amp;quot; is computed.
This index directly reflects the ability of a
test on the attribute-value to split the
window into two relatively even subwindows.
The current version of UTTER uses a
desirability index which is defined as:
# samples with this attribute-value
# distinct final values in this subset.
Different desirability indices might be
substituted to reflect the information
content of attribute-values.
When generating rules using UTTER the user
has the option of using either only a test
for equality in the decision tree, or a
larger set of tests containing &amp;quot;equals,&amp;quot;
&amp;quot;not-equals,&amp;quot; &amp;quot;less-than,&amp;quot; and &amp;quot;greater-
than&amp;quot;. If the larger set of possible tests
is used, then the inference routine takes
existing pronunciation dictionary would allow
training mode to be used rather infrequently, and
then only to make more subtle corrections to the
transcription rules.
13 The selection of all those examples which
have unique combinations of feature values should
reduce the number of iterations required in the
inference routine by eliminating redundant entries
in the training set. This type of training set
pruning should be done at the same time the train-
ing set is scanned for clashes (discussed below).
</bodyText>
<page confidence="0.481752">
14
</page>
<bodyText confidence="0.9851933">
An &amp;quot;attribute-value&amp;quot; refers to the value of
a feature or attribute for the given example. For
instance, let the attribute in question be the
word-level attribute &amp;quot;part-of-speech&amp;quot; and assume
it may take one of five possible values (noun,
verb, adjective, adverb, or function word). If
this attribute appears with only three values
(such as noun, verb, adjective) in the current
window, then only those three attribute-values
need be considered.
</bodyText>
<page confidence="0.998585">
39
</page>
<bodyText confidence="0.943081787878788">
much longer to execute. However, the decision
trees generated using the larger set are
often smaller and therefore usually faster to
traverse.
(4) The attribute-value with the greatest
desirability index is chosen as the next test
in the decision tree. This test is added to
the decision tree. In this manner, examples
occurring most frequently will take the least
amount of time to classify and, thus, to
transcribe.15
(5) The current window is split into two
subwindows. The split is based on which
examples in the window contain the
attribute-value selected as the new test, and
which examples do not.
(6) For each subwindow, it is determined whether
there is only one result value in a given
subwindow (i.e., is the result uniform on the
window?) or whether there is more than one
result.
(7) If there is more than one result in a
subwindow, this procedure is applied
recursively with the subwindow as the new
window.
If there is only one result across a given
subwindow, then generate a &amp;quot;terminal&amp;quot; or
&amp;quot;leaf&amp;quot; node for the decision tree which
returns this singular result as the value of
the tree at that terminal. Terminal nodes
are thus easily recognized since they have
only one distinct result.
(8) When the original window is completely
classified the resulting decision tree is the
new rule which is guaranteed to cover the
original window.
The newly generated rule is applied to the
remaining examples in the training set. From
the examples it fails to correctly classify,
a subset of the failures is chosen for
addition to the previous iteration&apos;s starting
window. The inference algorithm is reapplied
using this new starting window.
(9) When no failures exist, the most recently
generated decision tree completely covers the
training set. In this case, the training set
then becomes the SCL, and is stored in remote
storage until the next rule generating
session. The most recently generated
decision tree becomes the new rule and the
SEL is zeroed.
It is, of course, possible to terminate the
inference algorithm before it completely
classifies the training set. In this case, UTTER
simply places all of the &amp;quot;failures&amp;quot; on the SEL and
all of the properly classified examples from the
training set on the SCL. In this fashion it is
15 In certain pathological cases, the tree gen-
erated is not optimal in terms of traversal time.
This problem has not yet occurred with real tran-
scription data, and, in any case, would still
yield an acceptable, though less than optimal, de-
cision tree.
possible to reduce the size of the SEL without
exhaustively classifying the entire training set.
The procedure for creating a cluster rule is
identical.
In the course of rule generation, an
inconsistency called a &amp;quot;clash&amp;quot; may arise when the
attributes are insufficient to classify two or
more examples. A clash manifests itself as a
window with uniform values for all of the
attributes, but with more than one result present
in the window. The current version of UTTER aborts
the rule generation process when a clash occurs.
Future versions of UTTER should screen the entire
training set for clashes before starting the rule
generation process, as well as allow the user to
remove or correct the entries responsible for the
clash.
Clashes are usually the result of an error
made by the user in training mode. If a clash
should arise which is not the result of a user
error, it would indicate that the attribute set is
insufficient to characterize the set of
transcriptions. Additional attributes would have
to be added to UTTER in order to handle this
event.
For example, the word &amp;quot;read&amp;quot; is pronounced
differently in present tense than it is in past
tense. Since UTTER cannot extract contextual or
semantic information, the distinction cannot be
made. Therefore, two entries in the training set
might be present with the same attributes, but
different transcriptions. This situation results
in a clash which cannot be resolved without the
addition of another attribute, such as &amp;quot;tense.&amp;quot;
Fortunately, such cases account for a very small
portion of the English language.
</bodyText>
<sectionHeader confidence="0.842368" genericHeader="method">
IV CONCLUSION
</sectionHeader>
<bodyText confidence="0.999984230769231">
This paper has described a newly developed
system for the transcription of unmarked English
text into strings of phonemes for eventual
computer speech output. The current
implementation of the system has shown this
technique to be feasible in terms of speed of
execution and storage requirements, and desirable
in terms of transcription accuracy.
One of the unique features of UTTER is the
possibility of creating &amp;quot;mini-implementations&amp;quot; of
UTTER for use on evermore popular micro computers.
These reduced versions of UTTER would only need to
provide execution mode. The two transcription
rules could be developed on a full-scale system,
and provided to the user on floppy diskettes for
use on a micro computer. The micro systems need
not provide a training mode, so no SEL or CEL need
be retained (or checked during the transcription
-process). The PEL should still be provided so the
user could tailor the operation of the system to
the particular application by adding domain-
specific words to this list. The micro systems
need not supply an inference mode which requires
the most processor time and memory space of all
the modes of operation. Updated rules (on floppy
diskettes) could be provided periodically from the
</bodyText>
<page confidence="0.994843">
40
</page>
<bodyText confidence="0.916854111111111">
Phonetics,&amp;quot; IEEE Transactions an Acoustics,
Speech, and =nal _Fronezaing„ Vol 24, p446 -
459: 1976.
main system -- thus keeping memory and storage
requirements well within the capabilities of
today&apos;s micro computers.
Accurate phoneme string transcription from
unmarked text will .become increasingly vital as
speech synthesis technology continues to improve.
Better speech synthesis tools will encourage the
trend from digitally-encoded recorded messages (as
well as other phrase- or word-based computer
speech methods) towards sub-word synthetic speech
methods (such as diphone or phoneme based
synthesis). The UTTER system is an example of a
new approach to this old problem, embodying
features from both the linguistic and artificial
intelligence communities.
</bodyText>
<sectionHeader confidence="0.998414" genericHeader="method">
REFERENCES
</sectionHeader>
<reference confidence="0.9961696">
[Allen81]
Allen, Jonathen, &amp;quot;Linguistic Based Algorithms
Offer Practical Text-to-Speech Systems,&amp;quot;
Speech Technology, pp12-16: Fall 1981.
[Cherry80]
Cherry, L. L. .and Vesterman, W., &amp;quot;Writing
Tools - The STYLE and DICTION Programs,&amp;quot; Oa
Programmer&apos;a Manual, Seventh Ed., Vol. 2C,
Computer Science Division, Department of
Electrical Engineering and Computer Science,
University of California at Berkeley: 1980.
[Dickerson81]
Dickerson, Wayne B., &amp;quot;A Pedagogical
Interpretation of Generative Phonology, II.
The Main Word Stress Rules of English,&amp;quot; =I,
Studies, Vol 4, pp27-93: 1981.
[Dickerson82]
Dickerson, Wayne B., &amp;quot;A Pedagogical
Interpretation of Generative Phonology, III.
Vowels in the Key and Left Syllables,&amp;quot; TESL
Studies, Vol. 5: 1982.
[Glinski81]
Glinski, Stephen C , Diphone Speech Synthesis
jaangd an .a Pitch Adaptive Lhort Time Fourier
Transform, Ph.D. thesis, University of
Illinois at Urbana-Champaign: 1981.
[Kenyon531
Kenyon, John S. and Knott, Thomas A., A
irmmgmmgima Dictionary 91 American English,
G. C. Miriam Company: 1953.
[Oakey81]
Oakey, S. and Cawthorn, R. C., &amp;quot;Inductive
Learning of Pronunciation Rules by Hypothesis
Testing and Correction,&amp;quot; Proceedings 91 the
International Joint Conference san Artificial
Intelligence (IJCAI) 1981, pp109-114: 1981.
[Quinlan79]
Quinlan, J. R., &amp;quot;Discovering Rules by
Induction from Large Collections of
Examples,&amp;quot; Expert Systems In the Bimnm
Electronic Alm (Ed. D. Michie), Edinburgh
University Press, pp168-201: 1979.
[Segre831
Segre, Alberto Maria, A System Ism
Production 9L Phoneme Strings from Unmarked
English Texts, M.S. thesis, University of
Illinois at Urbana-Champaign: 1983.
[Sherwood78]
Sherwood, Bruce Arne, &amp;quot;Fast Text-to-Speech
Algorithms for Esperanto, Spanish, Italian,
Russian, and English,&amp;quot; International Journal&apos;
91 Man)4achine Ltudies •pp669-692: 1978.
[DickersonF1] APPENDIX A - World English Spelling
Dickerson, Wayne B., Lemming English a fat ie tie s set
Pronunciation, Volume III, &amp;quot;Word Stress and aa far j jam sh shed
</reference>
<bodyText confidence="0.962401714285714">
ae Mae k kit t tin
au taut I let th this
b but m met tx thin
ch chum n net u up
d dig ng sing ur fur
e set nk sink uu book
ee see oe toe ux above
er adder oi oil v van
f fat oo too w win
g gum
h hat or for wh when
ou out y yes
i in p pet z zoo
ix engage r run zh vision
</bodyText>
<reference confidence="0.982611333333333">
Vowel Quality,&amp;quot; Part I, forthcoming.
[DickersonF2]
Dickerson, Wayne B., learning Enalish
Pronunciation, Volume IV, &amp;quot;Word Stress and
Vowel Quality,&amp;quot; Part II, forthcoming.
[Elovitz76]
Elovitz, H. S., Johnson, R., McHugh, A. and
Shore, J. E., &amp;quot;Letter-to-Sound Rules for
Automatic Translation of English Text to
</reference>
<page confidence="0.997873">
41
</page>
<sectionHeader confidence="0.672512" genericHeader="method">
APPENDIX B - Function Words
</sectionHeader>
<bodyText confidence="0.804312551724138">
a can I ought under
about could if our unless
across did in ours until
against do into ourselves up
although does is over us
WA down it shall was
among during its she we
an each itself should were
and either like since what
any ever may so whatever
anybody every me some when
anyone everybody might somebody whenever
anything everyone mine someone where
are everything must something wherever
around for my than whether
as from myself that which
at going neither the while
be had never their who
because has no them whom
been have nobody themselves whose
before he noone then why
behind her nor therfore will
below hers not these with
beneath herself nothing they without
beside him off this would
between himself on those you
beyond his one though your
but how onto through yours
by however or . to yourself
</bodyText>
<page confidence="0.995017">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.124604">
<title confidence="0.9904625">AN EXPERT SYSTEM FOR THE PRODUCTION OF PHONEME STRINGS FROM UNMARKED ENGLISH TEXT USING MACHINE-INDUCED RULES</title>
<author confidence="0.999939">Alberto Maria Segre</author>
<affiliation confidence="0.9901305">University of Illinois at Urbana-Champaign Coordinated Science Laboratory</affiliation>
<address confidence="0.988314">1101 W. Springfield Urbana, IL 61801 U.S.A.</address>
<author confidence="0.988523">Bruce Arne Sherwood</author>
<affiliation confidence="0.9820775">University of Illinois at Urbana-Champaign . Computer-based Education Research Laboratory</affiliation>
<address confidence="0.848372">103 S. Mathews Urbana, IL 61801 U.S.A.</address>
<author confidence="0.997463">Wayne B Dickerson</author>
<affiliation confidence="0.99908">University of Illinois</affiliation>
<title confidence="0.730158666666667">at Urbana-Champaign English as a Second Language Foreign Language Building</title>
<author confidence="0.531409">S Mathews</author>
<address confidence="0.716082">Urbana, IL 61801 U.S.A.</address>
<abstract confidence="0.960903791666667">The speech synthesis group at the Computer- Based Education Research Laboratory (CERL) of the University of Illinois at Urbana-Champaign is developing a diphone speech synthesis system based on pitch-adaptive short-time Fourier transforms. This system accepts the phonemic specification of an utterance along with pitch, time, and amplitude warping functions in order to produce high quality speech output from stored diphone templates. This paper describes the operation of a program which operates as a front end for the diphone speech synthesis system. The UTTER (for &amp;quot;Unmarked Text Transcription by Expert Rule&amp;quot;) text onto a phoneme string, which is then used as an input to the diphone speech synthesis system. The program is a twotiered Expert System which operates first on the word level and then on the (vowel or consonant) cluster level. The system&apos;s knowledge about pronunciation is organized in two decision trees automatically generated by an induction algorithm on a dynamically specified &amp;quot;training set&amp;quot; of examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>[Allen81]</note>
<marker></marker>
<rawString>[Allen81]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathen Allen</author>
</authors>
<title>Linguistic Based Algorithms Offer Practical Text-to-Speech Systems,&amp;quot;</title>
<date>1981</date>
<journal>Speech Technology,</journal>
<pages>12--16</pages>
<marker>Allen, 1981</marker>
<rawString>Allen, Jonathen, &amp;quot;Linguistic Based Algorithms Offer Practical Text-to-Speech Systems,&amp;quot; Speech Technology, pp12-16: Fall 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L L and Vesterman Cherry</author>
<author>W</author>
</authors>
<title>Writing Tools - The STYLE and DICTION Programs,&amp;quot; Oa Programmer&apos;a Manual,</title>
<date>1980</date>
<journal>Seventh Ed.,</journal>
<volume>2</volume>
<institution>Computer Science Division, Department of Electrical Engineering and Computer Science, University of California at Berkeley:</institution>
<note>[Dickerson81]</note>
<marker>Cherry, W, 1980</marker>
<rawString>Cherry, L. L. .and Vesterman, W., &amp;quot;Writing Tools - The STYLE and DICTION Programs,&amp;quot; Oa Programmer&apos;a Manual, Seventh Ed., Vol. 2C, Computer Science Division, Department of Electrical Engineering and Computer Science, University of California at Berkeley: 1980. [Dickerson81]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne B Dickerson</author>
</authors>
<title>A Pedagogical Interpretation of Generative Phonology, II. The Main Word Stress Rules of English,&amp;quot;</title>
<date>1981</date>
<journal>I, Studies,</journal>
<volume>4</volume>
<pages>27--93</pages>
<marker>Dickerson, 1981</marker>
<rawString>Dickerson, Wayne B., &amp;quot;A Pedagogical Interpretation of Generative Phonology, II. The Main Word Stress Rules of English,&amp;quot; =I, Studies, Vol 4, pp27-93: 1981.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wayne B Dickerson</author>
</authors>
<title>A Pedagogical Interpretation of Generative Phonology, III. Vowels in the Key and Left Syllables,&amp;quot;</title>
<journal>TESL Studies,</journal>
<volume>5</volume>
<pages>1982</pages>
<marker>Dickerson, </marker>
<rawString>Dickerson, Wayne B., &amp;quot;A Pedagogical Interpretation of Generative Phonology, III. Vowels in the Key and Left Syllables,&amp;quot; TESL Studies, Vol. 5: 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen C Glinski</author>
</authors>
<title>Diphone Speech Synthesis jaangd an .a Pitch Adaptive Lhort Time Fourier Transform,</title>
<date>1981</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Urbana-Champaign:</institution>
<marker>Glinski, 1981</marker>
<rawString>Glinski, Stephen C , Diphone Speech Synthesis jaangd an .a Pitch Adaptive Lhort Time Fourier Transform, Ph.D. thesis, University of Illinois at Urbana-Champaign: 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Kenyon</author>
<author>Thomas A Knott</author>
</authors>
<date>1953</date>
<journal>A irmmgmmgima Dictionary 91 American English, G. C. Miriam Company:</journal>
<marker>Kenyon, Knott, 1953</marker>
<rawString>Kenyon, John S. and Knott, Thomas A., A irmmgmmgima Dictionary 91 American English, G. C. Miriam Company: 1953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oakey</author>
<author>R C Cawthorn</author>
</authors>
<title>Inductive Learning of Pronunciation Rules by Hypothesis Testing and Correction,&amp;quot;</title>
<date>1981</date>
<booktitle>Proceedings 91 the International Joint Conference san Artificial Intelligence (IJCAI)</booktitle>
<pages>109--114</pages>
<marker>Oakey, Cawthorn, 1981</marker>
<rawString>Oakey, S. and Cawthorn, R. C., &amp;quot;Inductive Learning of Pronunciation Rules by Hypothesis Testing and Correction,&amp;quot; Proceedings 91 the International Joint Conference san Artificial Intelligence (IJCAI) 1981, pp109-114: 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Discovering Rules by Induction from Large Collections of Examples,&amp;quot; Expert Systems</title>
<date>1979</date>
<booktitle>In the Bimnm Electronic Alm</booktitle>
<pages>168--201</pages>
<publisher>University Press,</publisher>
<location>Edinburgh</location>
<marker>Quinlan, 1979</marker>
<rawString>Quinlan, J. R., &amp;quot;Discovering Rules by Induction from Large Collections of Examples,&amp;quot; Expert Systems In the Bimnm Electronic Alm (Ed. D. Michie), Edinburgh University Press, pp168-201: 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Maria Segre</author>
</authors>
<title>A System Ism</title>
<date>1983</date>
<booktitle>Production 9L Phoneme Strings from Unmarked English Texts, M.S. thesis, University of Illinois at Urbana-Champaign:</booktitle>
<note>[Sherwood78]</note>
<marker>Segre, 1983</marker>
<rawString>Segre, Alberto Maria, A System Ism Production 9L Phoneme Strings from Unmarked English Texts, M.S. thesis, University of Illinois at Urbana-Champaign: 1983. [Sherwood78]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Arne Sherwood</author>
</authors>
<title>Fast Text-to-Speech Algorithms for Esperanto, Spanish, Italian, Russian, and English,&amp;quot;</title>
<date>1978</date>
<booktitle>International Journal&apos; 91 Man)4achine Ltudies •pp669-692:</booktitle>
<marker>Sherwood, 1978</marker>
<rawString>Sherwood, Bruce Arne, &amp;quot;Fast Text-to-Speech Algorithms for Esperanto, Spanish, Italian, Russian, and English,&amp;quot; International Journal&apos; 91 Man)4achine Ltudies •pp669-692: 1978.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wayne B Dickerson</author>
<author>Lemming English Pronunciation</author>
<author>Volume</author>
</authors>
<title>Word Stress and APPENDIX A - World English Spelling a fat ie tie s set aa far j jam sh shed ae Mae k kit t tin au taut I let th this b but m met tx thin ch chum n net u up d dig ng sing ur fur e set nk sink uu book ee see oe toe ux above er adder oi oil v van f fat oo too w win g gum h hat or for wh when ou out y yes i in p pet z zoo ix engage r run zh vision Vowel Quality,&amp;quot; Part I, forthcoming. [DickersonF2] Dickerson, Wayne B., learning Enalish Pronunciation, Volume IV, &amp;quot;Word Stress and</title>
<marker>Dickerson, Pronunciation, Volume, </marker>
<rawString>[DickersonF1] Dickerson, Wayne B., Lemming English Pronunciation, Volume III, &amp;quot;Word Stress and APPENDIX A - World English Spelling a fat ie tie s set aa far j jam sh shed ae Mae k kit t tin au taut I let th this b but m met tx thin ch chum n net u up d dig ng sing ur fur e set nk sink uu book ee see oe toe ux above er adder oi oil v van f fat oo too w win g gum h hat or for wh when ou out y yes i in p pet z zoo ix engage r run zh vision Vowel Quality,&amp;quot; Part I, forthcoming. [DickersonF2] Dickerson, Wayne B., learning Enalish Pronunciation, Volume IV, &amp;quot;Word Stress and</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>