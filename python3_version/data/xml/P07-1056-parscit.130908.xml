<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009306">
<title confidence="0.9606315">
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification
</title>
<author confidence="0.99762">
John Blitzer Mark Dredze Fernando Pereira
</author>
<affiliation confidence="0.9984445">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<email confidence="0.998773">
{blitzer|mdredze|pereria@cis.upenn.edu}
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941318181818">
Automatic sentiment classification has been
extensively studied and applied in recent
years. However, sentiment is expressed dif-
ferently in different domains, and annotating
corpora for every possible domain of interest
is impractical. We investigate domain adap-
tation for sentiment classifiers, focusing on
online reviews for different types of prod-
ucts. First, we extend to sentiment classifi-
cation the recently-proposed structural cor-
respondence learning (SCL) algorithm, re-
ducing the relative error due to adaptation
between domains by an average of 30% over
the original SCL algorithm and 46% over
a supervised baseline. Second, we identify
a measure of domain similarity that corre-
lates well with the potential for adaptation
of a classifier from one domain to another.
This measure could for instance be used to
select a small set of domains to annotate
whose trained classifiers would transfer well
to many other domains.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999679771428571">
Sentiment detection and classification has received
considerable attention recently (Pang et al., 2002;
Turney, 2002; Goldberg and Zhu, 2004). While
movie reviews have been the most studied domain,
sentiment analysis has extended to a number of
new domains, ranging from stock message boards
to congressional floor debates (Das and Chen, 2001;
Thomas et al., 2006). Research results have been
deployed industrially in systems that gauge market
reaction and summarize opinion from Web pages,
discussion boards, and blogs.
With such widely-varying domains, researchers
and engineers who build sentiment classification
systems need to collect and curate data for each new
domain they encounter. Even in the case of market
analysis, if automatic sentiment classification were
to be used across a wide range of domains, the ef-
fort to annotate corpora for each domain may be-
come prohibitive, especially since product features
change over time. We envision a scenario in which
developers annotate corpora for a small number of
domains, train classifiers on those corpora, and then
apply them to other similar corpora. However, this
approach raises two important questions. First, it
is well known that trained classifiers lose accuracy
when the test data distribution is significantly differ-
ent from the training data distribution 1. Second, it is
not clear which notion of domain similarity should
be used to select domains to annotate that would be
good proxies for many other domains.
We propose solutions to these two questions and
evaluate them on a corpus of reviews for four differ-
ent types of products from Amazon: books, DVDs,
electronics, and kitchen appliances2. First, we show
how to extend the recently proposed structural cor-
</bodyText>
<footnote confidence="0.99957225">
1For surveys of recent research on domain adaptation, see
the ICML 2006 Workshop on Structural Knowledge Transfer
for Machine Learning (http://gameairesearch.uta.
edu/) and the NIPS 2006 Workshop on Learning when test
and training inputs have different distribution (http://ida.
first.fraunhofer.de/projects/different06/)
2The dataset will be made available by the authors at publi-
cation time.
</footnote>
<page confidence="0.917882">
440
</page>
<note confidence="0.9269205">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999654258064516">
respondence learning (SCL) domain adaptation al-
gorithm (Blitzer et al., 2006) for use in sentiment
classification. A key step in SCL is the selection of
pivot features that are used to link the source and tar-
get domains. We suggest selecting pivots based not
only on their common frequency but also according
to their mutual information with the source labels.
For data as diverse as product reviews, SCL can
sometimes misalign features, resulting in degrada-
tion when we adapt between domains. In our second
extension we show how to correct misalignments us-
ing a very small number of labeled instances.
Second, we evaluate the A-distance (Ben-David
et al., 2006) between domains as measure of the loss
due to adaptation from one to the other. The A-
distance can be measured from unlabeled data, and it
was designed to take into account only divergences
which affect classification accuracy. We show that it
correlates well with adaptation loss, indicating that
we can use the A-distance to select a subset of do-
mains to label as sources.
In the next section we briefly review SCL and in-
troduce our new pivot selection method. Section 3
describes datasets and experimental method. Sec-
tion 4 gives results for SCL and the mutual informa-
tion method for selecting pivot features. Section 5
shows how to correct feature misalignments using a
small amount of labeled target domain data. Sec-
tion 6 motivates the A-distance and shows that it
correlates well with adaptability. We discuss related
work in Section 7 and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.957686" genericHeader="introduction">
2 Structural Correspondence Learning
</sectionHeader>
<bodyText confidence="0.999634944444445">
Before reviewing SCL, we give a brief illustrative
example. Suppose that we are adapting from re-
views of computers to reviews of cell phones. While
many of the features of a good cell phone review are
the same as a computer review – the words “excel-
lent” and “awful” for example – many words are to-
tally new, like “reception”. At the same time, many
features which were useful for computers, such as
“dual-core” are no longer useful for cell phones.
Our key intuition is that even when “good-quality
reception” and “fast dual-core” are completely dis-
tinct for each domain, if they both have high correla-
tion with “excellent” and low correlation with “aw-
ful” on unlabeled data, then we can tentatively align
them. After learning a classifier for computer re-
views, when we see a cell-phone feature like “good-
quality reception”, we know it should behave in a
roughly similar manner to “fast dual-core”.
</bodyText>
<subsectionHeader confidence="0.996957">
2.1 Algorithm Overview
</subsectionHeader>
<bodyText confidence="0.999987892857143">
Given labeled data from a source domain and un-
labeled data from both source and target domains,
SCL first chooses a set of m pivot features which oc-
cur frequently in both domains. Then, it models the
correlations between the pivot features and all other
features by training linear pivot predictors to predict
occurrences of each pivot in the unlabeled data from
both domains (Ando and Zhang, 2005; Blitzer et al.,
2006). The Eth pivot predictor is characterized by
its weight vector wt; positive entries in that weight
vector mean that a non-pivot feature (like “fast dual-
core”) is highly correlated with the corresponding
pivot (like “excellent”).
The pivot predictor column weight vectors can be
arranged into a matrix W = [wt]e1. Let 0 E Rkxd
be the top k left singular vectors of W (here d indi-
cates the total number of features). These vectors are
the principal predictors for our weight space. If we
chose our pivot features well, then we expect these
principal predictors to discriminate among positive
and negative words in both domains.
At training and test time, suppose we observe a
feature vector x. We apply the projection 0x to ob-
tain k new real-valued features. Now we learn a
predictor for the augmented instance (x, 0x). If 0
contains meaningful correspondences, then the pre-
dictor which uses 0 will perform well in both source
and target domains.
</bodyText>
<subsectionHeader confidence="0.999522">
2.2 Selecting Pivots with Mutual Information
</subsectionHeader>
<bodyText confidence="0.9999625">
The efficacy of SCL depends on the choice of pivot
features. For the part of speech tagging problem
studied by Blitzer et al. (2006), frequently-occurring
words in both domains were good choices, since
they often correspond to function words such as
prepositions and determiners, which are good indi-
cators of parts of speech. This is not the case for
sentiment classification, however. Therefore, we re-
quire that pivot features also be good predictors of
the source label. Among those features, we then
choose the ones with highest mutual information to
the source label. Table 1 shows the set-symmetric
</bodyText>
<page confidence="0.994043">
441
</page>
<table confidence="0.6684615">
SCL, not SCL-MI SCL-MI, not SCL
book one &lt;num&gt; so all a must a wonderful loved it
very about they like weak don’t waste awful
good when highly recommended and easy
</table>
<tableCaption confidence="0.89962">
Table 1: Top pivots selected by SCL, but not SCL-
MI (left) and vice-versa (right)
</tableCaption>
<bodyText confidence="0.996054">
differences between the two methods for pivot selec-
tion when adapting a classifier from books to kitchen
appliances. We refer throughout the rest of this work
to our method for selecting pivots as SCL-MI.
</bodyText>
<sectionHeader confidence="0.992077" genericHeader="method">
3 Dataset and Baseline
</sectionHeader>
<bodyText confidence="0.979458743589744">
We constructed a new dataset for sentiment domain
adaptation by selecting Amazon product reviews for
four different product types: books, DVDs, electron-
ics and kitchen appliances. Each review consists of
a rating (0-5 stars), a reviewer name and location,
a product name, a review title and date, and the re-
view text. Reviews with rating &gt; 3 were labeled
positive, those with rating &lt; 3 were labeled neg-
ative, and the rest discarded because their polarity
was ambiguous. After this conversion, we had 1000
positive and 1000 negative examples for each do-
main, the same balanced composition as the polarity
dataset (Pang et al., 2002). In addition to the labeled
data, we included between 3685 (DVDs) and 5945
(kitchen) instances of unlabeled data. The size of the
unlabeled data was limited primarily by the number
of reviews we could crawl and download from the
Amazon website. Since we were able to obtain la-
bels for all of the reviews, we also ensured that they
were balanced between positive and negative exam-
ples, as well.
While the polarity dataset is a popular choice in
the literature, we were unable to use it for our task.
Our method requires many unlabeled reviews and
despite a large number of IMDB reviews available
online, the extensive curation requirements made
preparing a large amount of data difficult 3.
For classification, we use linear predictors on un-
igram and bigram features, trained to minimize the
Huber loss with stochastic gradient descent (Zhang,
3For a description of the construction of the polarity
dataset, see http://www.cs.cornell.edu/people/
pabo/movie-review-data/.
2004). On the polarity dataset, this model matches
the results reported by Pang et al. (2002). When we
report results with SCL and SCL-MI, we require that
pivots occur in more than five documents in each do-
main. We set k, the number of singular vectors of the
weight matrix, to 50.
</bodyText>
<sectionHeader confidence="0.981721" genericHeader="method">
4 Experiments with SCL and SCL-MI
</sectionHeader>
<bodyText confidence="0.999901743589743">
Each labeled dataset was split into a training set of
1600 instances and a test set of 400 instances. All
the experiments use a classifier trained on the train-
ing set of one domain and tested on the test set of
a possibly different domain. The baseline is a lin-
ear classifier trained without adaptation, while the
gold standard is an in-domain classifier trained on
the same domain as it is tested.
Figure 1 gives accuracies for all pairs of domain
adaptation. The domains are ordered clockwise
from the top left: books, DVDs, electronics, and
kitchen. For each set of bars, the first letter is the
source domain and the second letter is the target
domain. The thick horizontal bars are the accura-
cies of the in-domain classifiers for these domains.
Thus the first set of bars shows that the baseline
achieves 72.8% accuracy adapting from DVDs to
books. SCL-MI achieves 79.7% and the in-domain
gold standard is 80.4%. We say that the adaptation
loss for the baseline model is 7.6% and the adapta-
tion loss for the SCL-MI model is 0.7%. The relative
reduction in error due to adaptation of SCL-MI for
this test is 90.8%.
We can observe from these results that there is a
rough grouping of our domains. Books and DVDs
are similar, as are kitchen appliances and electron-
ics, but the two groups are different from one an-
other. Adapting classifiers from books to DVDs, for
instance, is easier than adapting them from books
to kitchen appliances. We note that when transfer-
ring from kitchen to electronics, SCL-MI actually
outperforms the in-domain classifier. This is possi-
ble since the unlabeled data may contain information
that the in-domain classifier does not have access to.
At the beginning of Section 2 we gave exam-
ples of how features can change behavior across do-
mains. The first type of behavior is when predictive
features from the source domain are not predictive
or do not appear in the target domain. The second is
</bodyText>
<page confidence="0.989258">
442
</page>
<figure confidence="0.997330563636364">
90
85
80
65
75
70
90
85
80
75
70
65
70.8
72.8
electronics kitchen
77.5
B-&gt;E D-&gt;E K-&gt;E B-&gt;K D-&gt;K E-&gt;K
76.8
D-&gt;B E-&gt;B K-&gt;B B-&gt;D E-&gt;D K-&gt;D
books
75.9
79.7
73.0
84.4
70.7
80.4
74.1
75.4
74.1
75.4
82.7
baseline SCL SCL-MI
70.9 66.1
68.6
83.7
86.8
74.5
87.7
77.2
78.7
74.0
78.9 79.4
75.8
74.0
70.6
74.3
82.4
84.4
81.4 84.0
76.2
72.7
dvd
75.4
76.9
85.9
</figure>
<figureCaption confidence="0.999771">
Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI. Thick black
lines are the accuracies of in-domain classifiers.
</figureCaption>
<bodyText confidence="0.866240578947368">
domain polarity negative positive
books plot &lt;num&gt; pages predictable reader grisham engaging
reading this page &lt;num&gt; must read fascinating
kitchen the plastic poorly designed excellent product espresso
leaking awkward to defective are perfect years now a breeze
Table 2: Correspondences discovered by SCL for books and kitchen appliances. The top row shows features
that only appear in books and the bottom features that only appear in kitchen appliances. The left and right
columns show negative and positive features in correspondence, respectively.
when predictive features from the target domain do
not appear in the source domain. To show how SCL
deals with those domain mismatches, we look at the
adaptation from book reviews to reviews of kitchen
appliances. We selected the top 1000 most infor-
mative features in both domains. In both cases, be-
tween 85 and 90% of the informative features from
one domain were not among the most informative
of the other domain4. SCL addresses both of these
issues simultaneously by aligning features from the
two domains.
</bodyText>
<footnote confidence="0.966238666666667">
4There is a third type, features which are positive in one do-
main but negative in another, but they appear very infrequently
in our datasets.
</footnote>
<bodyText confidence="0.995585538461539">
Table 2 illustrates one row of the projection ma-
trix 0 for adapting from books to kitchen appliances;
the features on each row appear only in the corre-
sponding domain. A supervised classifier trained on
book reviews cannot assign weight to the kitchen
features in the second row of table 2. In con-
trast, SCL assigns weight to these features indirectly
through the projection matrix. When we observe
the feature “predictable” with a negative book re-
view, we update parameters corresponding to the
entire projection, including the kitchen-specific fea-
tures “poorly designed” and “awkward to”.
While some rows of the projection matrix 0 are
</bodyText>
<page confidence="0.997094">
443
</page>
<bodyText confidence="0.999903714285714">
useful for classification, SCL can also misalign fea-
tures. This causes problems when a projection is
discriminative in the source domain but not in the
target. This is the case for adapting from kitchen
appliances to books. Since the book domain is
quite broad, many projections in books model topic
distinctions such as between religious and political
books. These projections, which are uninforma-
tive as to the target label, are put into correspon-
dence with the fewer discriminating projections in
the much narrower kitchen domain. When we adapt
from kitchen to books, we assign weight to these un-
informative projections, degrading target classifica-
tion accuracy.
</bodyText>
<sectionHeader confidence="0.988515" genericHeader="method">
5 Correcting Misalignments
</sectionHeader>
<bodyText confidence="0.9998194">
We now show how to use a small amount of target
domain labeled data to learn to ignore misaligned
projections from SCL-MI. Using the notation of
Ando and Zhang (2005), we can write the supervised
training objective of SCL on the source domain as
</bodyText>
<equation confidence="0.694416333333333">
�min L (w0xi + v0θxi, yi) + λ||w||2 + µ||v||2 ,
w,v
i
</equation>
<bodyText confidence="0.999860384615385">
where y is the label. The weight vector w E Rd
weighs the original features, while v E Rk weighs
the projected features. Ando and Zhang (2005) and
Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which
we have used in our results so far.
Suppose now that we have trained source model
weight vectors ws and vs. A small amount of tar-
get domain data is probably insufficient to signif-
icantly change w, but we can correct v, which is
much smaller. We augment each labeled target in-
stance xj with the label assigned by the source do-
main classifier (Florian et al., 2004; Blitzer et al.,
2006). Then we solve
</bodyText>
<equation confidence="0.805176666666667">
E
minw,v j L (w0xj + v0θxj, yj) + λ||w||2
+µ||v − vs||2 .
</equation>
<bodyText confidence="0.999022857142857">
Since we don’t want to deviate significantly from the
source parameters, we set λ = µ = 10−1.
Figure 2 shows the corrected SCL-MI model us-
ing 50 target domain labeled instances. We chose
this number since we believe it to be a reasonable
amount for a single engineer to label with minimal
effort. For reasons of space, for each target domain
</bodyText>
<table confidence="0.980721571428571">
dom \ model base base scl scl-mi scl-mi
+targ +targ
books 8.9 9.0 7.4 5.8 4.4
dvd 8.9 8.9 7.8 6.1 5.3
electron 8.3 8.5 6.0 5.5 4.8
kitchen 10.2 9.9 7.0 5.6 5.1
average 9.1 9.1 7.1 5.8 4.9
</table>
<tableCaption confidence="0.962968666666667">
Table 3: For each domain, we show the loss due to transfer
for each method, averaged over all domains. The bottom row
shows the average loss over all runs.
</tableCaption>
<bodyText confidence="0.999586868421053">
we show adaptation from only the two domains on
which SCL-MI performed the worst relative to the
supervised baseline. For example, the book domain
shows only results from electronics and kitchen, but
not DVDs. As a baseline, we used the label of the
source domain classifier as a feature in the target, but
did not use any SCL features. We note that the base-
line is very close to just using the source domain
classifier, because with only 50 target domain in-
stances we do not have enough data to relearn all of
the parameters in w. As we can see, though, relearn-
ing the 50 parameters in v is quite helpful. The cor-
rected model always improves over the baseline for
every possible transfer, including those not shown in
the figure.
The idea of using the regularizer of a linear model
to encourage the target parameters to be close to the
source parameters has been used previously in do-
main adaptation. In particular, Chelba and Acero
(2004) showed how this technique can be effective
for capitalization adaptation. The major difference
between our approach and theirs is that we only pe-
nalize deviation from the source parameters for the
weights v of projected features, while they work
with the weights of the original features only. For
our small amount of labeled target data, attempting
to penalize w using ws performed no better than
our baseline. Because we only need to learn to ig-
nore projections that misalign features, we can make
much better use of our labeled data by adapting only
50 parameters, rather than 200,000.
Table 3 summarizes the results of sections 4 and
5. Structural correspondence learning reduces the
error due to transfer by 21%. Choosing pivots by
mutual information allows us to further reduce the
error to 36%. Finally, by adding 50 instances of tar-
get domain data and using this to correct the mis-
aligned projections, we achieve an average relative
</bodyText>
<page confidence="0.997236">
444
</page>
<figure confidence="0.991933566666667">
90
85
80
75
70
65
70.9
E-&gt;B K-&gt;B B-&gt;D K-&gt;D B-&gt;E D-&gt;E B-&gt;K E-&gt;K
76.0
books kitchen
dvd electronics
80.4
70.7
73.2
76.8
base+50-targ SCL-MI+50-targ
78.5
82.4
72.7
76.6
70.8
76.6
84.4
73.0
77.9
74.3
80.7
87.7
84.3
85.9
</figure>
<figureCaption confidence="0.999664">
Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.
</figureCaption>
<bodyText confidence="0.897229">
reduction in error of 46%.
</bodyText>
<sectionHeader confidence="0.931693" genericHeader="method">
6 Measuring Adaptability
</sectionHeader>
<bodyText confidence="0.999859133333333">
Sections 2-5 focused on how to adapt to a target do-
main when you had a labeled source dataset. We
now take a step back to look at the problem of se-
lecting source domain data to label. We study a set-
ting where an engineer knows roughly her domains
of interest but does not have any labeled data yet. In
that case, she can ask the question “Which sources
should I label to obtain the best performance over
all my domains?” On our product domains, for ex-
ample, if we are interested in classifying reviews
of kitchen appliances, we know from sections 4-5
that it would be foolish to label reviews of books or
DVDs rather than electronics. Here we show how to
select source domains using only unlabeled data and
the SCL representation.
</bodyText>
<subsectionHeader confidence="0.997292">
6.1 The A-distance
</subsectionHeader>
<bodyText confidence="0.996990615384615">
We propose to measure domain adaptability by us-
ing the divergence of two domains after the SCL
projection. We can characterize domains by their
induced distributions on instance space: the more
different the domains, the more divergent the distri-
butions. Here we make use of the A-distance (Ben-
David et al., 2006). The key intuition behind the
A-distance is that while two domains can differ in
arbitrary ways, we are only interested in the differ-
ences that affect classification accuracy.
Let A be the family of subsets of Rk correspond-
ing to characteristic functions of linear classifiers
(sets on which a linear classifier returns positive
value). Then the A distance between two probability
distributions is
dA(D, D0) = 2 sup |PrD [A] − PrD, [A] |.
A∈A
That is, we find the subset in A on which the distri-
butions differ the most in the Li sense. Ben-David
et al. (2006) show that computing the A-distance for
a finite sample is exactly the problem of minimiz-
ing the empirical risk of a classifier that discrimi-
nates between instances drawn from D and instances
drawn from D0. This is convenient for us, since it al-
lows us to use classification machinery to compute
the A-distance.
</bodyText>
<subsectionHeader confidence="0.999868">
6.2 Unlabeled Adaptability Measurements
</subsectionHeader>
<bodyText confidence="0.99980175">
We follow Ben-David et al. (2006) and use the Hu-
ber loss as a proxy for the A-distance. Our proce-
dure is as follows: Given two domains, we compute
the SCL representation. Then we create a data set
where each instance Ox is labeled with the identity
of the domain from which it came and train a linear
classifier. For each pair of domains we compute the
empirical average per-instance Huber loss, subtract
it from 1, and multiply the result by 100. We refer
to this quantity as the proxy A-distance. When it is
100, the two domains are completely distinct. When
it is 0, the two domains are indistinguishable using a
linear classifier.
Figure 3 is a correlation plot between the proxy
A-distance and the adaptation error. Suppose we
wanted to label two domains out of the four in such a
</bodyText>
<page confidence="0.998996">
445
</page>
<figureCaption confidence="0.7572486">
Figure 3: The proxy A-distance between each do-
main pair plotted against the average adaptation loss
of as measured by our baseline system. Each pair of
domains is labeled by their first letters: EK indicates
the pair electronics and kitchen.
</figureCaption>
<bodyText confidence="0.999895625">
way as to minimize our error on all the domains. Us-
ing the proxy A-distance as a criterion, we observe
that we would choose one domain from either books
or DVDs, but not both, since then we would not be
able to adequately cover electronics or kitchen appli-
ances. Similarly we would also choose one domain
from either electronics or kitchen appliances, but not
both.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.99998093939394">
Sentiment classification has advanced considerably
since the work of Pang et al. (2002), which we use
as our baseline. Thomas et al. (2006) use discourse
structure present in congressional records to perform
more accurate sentiment classification. Pang and
Lee (2005) treat sentiment analysis as an ordinal
ranking problem. In our work we only show im-
provement for the basic model, but all of these new
techniques also make use of lexical features. Thus
we believe that our adaptation methods could be also
applied to those more refined models.
While work on domain adaptation for senti-
ment classifiers is sparse, it is worth noting that
other researchers have investigated unsupervised
and semisupervised methods for domain adaptation.
The work most similar in spirit to ours that of Tur-
ney (2002). He used the difference in mutual in-
formation with two human-selected features (the
words “excellent” and “poor”) to score features in
a completely unsupervised manner. Then he clas-
sified documents according to various functions of
these mutual information scores. We stress that our
method improves a supervised baseline. While we
do not have a direct comparison, we note that Tur-
ney (2002) performs worse on movie reviews than
on his other datasets, the same type of data as the
polarity dataset.
We also note the work of Aue and Gamon (2005),
who performed a number of empirical tests on do-
main adaptation of sentiment classifiers. Most of
these tests were unsuccessful. We briefly note their
results on combining a number of source domains.
They observed that source domains closer to the tar-
get helped more. In preliminary experiments we
confirmed these results. Adding more labeled data
always helps, but diversifying training data does not.
When classifying kitchen appliances, for any fixed
amount of labeled data, it is always better to draw
from electronics as a source than use some combi-
nation of all three other domains.
Domain adaptation alone is a generally well-
studied area, and we cannot possibly hope to cover
all of it here. As we noted in Section 5, we are
able to significantly outperform basic structural cor-
respondence learning (Blitzer et al., 2006). We also
note that while Florian et al. (2004) and Blitzer et al.
(2006) observe that including the label of a source
classifier as a feature on small amounts of target data
tends to improve over using either the source alone
or the target alone, we did not observe that for our
data. We believe the most important reason for this
is that they explore structured prediction problems,
where labels of surrounding words from the source
classifier may be very informative, even if the cur-
rent label is not. In contrast our simple binary pre-
diction problem does not exhibit such behavior. This
may also be the reason that the model of Chelba and
Acero (2004) did not aid in adaptation.
Finally we note that while Blitzer et al. (2006) did
combine SCL with labeled target domain data, they
only compared using the label of SCL or non-SCL
source classifiers as features, following the work of
Florian et al. (2004). By only adapting the SCL-
related part of the weight vector v, we are able to
make better use of our small amount of unlabeled
data than these previous techniques.
</bodyText>
<figure confidence="0.980100875">
BE,
DK
BK
DE
BD
EK
60 65 70 75 80 85 90 95 100
Proxy A-distance
Adaptation Loss 14
12
10
8
6
4
2
0
</figure>
<page confidence="0.997961">
446
</page>
<sectionHeader confidence="0.999045" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999378851851852">
Sentiment classification has seen a great deal of at-
tention. Its application to many different domains
of discourse makes it an ideal candidate for domain
adaptation. This work addressed two important
questions of domain adaptation. First, we showed
that for a given source and target domain, we can
significantly improve for sentiment classification the
structural correspondence learning model of Blitzer
et al. (2006). We chose pivot features using not only
common frequency among domains but also mutual
information with the source labels. We also showed
how to correct structural correspondence misalign-
ments by using a small amount of labeled target do-
main data.
Second, we provided a method for selecting those
source domains most likely to adapt well to given
target domains. The unsupervised A-distance mea-
sure of divergence between domains correlates well
with loss due to adaptation. Thus we can use the A-
distance to select source domains to label which will
give low target domain error.
In the future, we wish to include some of the more
recent advances in sentiment classification, as well
as addressing the more realistic problem of rank-
ing. We are also actively searching for a larger and
more varied set of domains on which to test our tech-
niques.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999583">
We thank Nikhil Dinesh for helpful advice through-
out the course of this work. This material is based
upon work partially supported by the Defense Ad-
vanced Research Projects Agency (DARPA) un-
der Contract No. NBCHD03001. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA or
the Department of Interior-National BusinessCenter
(DOI-NBC).
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966096">
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. JMLR, 6:1817–1853.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
http://research.microsoft.com/ anthaue/.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Neural Information Processing
Systems (NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In EMNLP.
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: Extracting market sentiment from stock message
boards. In Proceedings of Athe Asia Pacific Finance
Association Annual Conference.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In of HLT-NAACL.
Andrew Goldberg and Xiaojin Zhu. 2004. Seeing
stars when there aren’t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of Association
for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Empiri-
cal Methods in Natural Language Processing.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of Association for
Computational Linguistics.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In International Conference on Machine
Learning (ICML).
</reference>
<page confidence="0.998607">
447
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983646">
<title confidence="0.99821">Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification</title>
<author confidence="0.999815">John Blitzer Mark Dredze Fernando Pereira</author>
<affiliation confidence="0.9993145">Department of Computer and Information Science University of Pennsylvania</affiliation>
<abstract confidence="0.999499782608696">Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>JMLR,</journal>
<pages>6--1817</pages>
<contexts>
<context position="6412" citStr="Ando and Zhang, 2005" startWordPosition="1001" endWordPosition="1004"> align them. After learning a classifier for computer reviews, when we see a cell-phone feature like “goodquality reception”, we know it should behave in a roughly similar manner to “fast dual-core”. 2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains, SCL first chooses a set of m pivot features which occur frequently in both domains. Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al., 2006). The Eth pivot predictor is characterized by its weight vector wt; positive entries in that weight vector mean that a non-pivot feature (like “fast dualcore”) is highly correlated with the corresponding pivot (like “excellent”). The pivot predictor column weight vectors can be arranged into a matrix W = [wt]e1. Let 0 E Rkxd be the top k left singular vectors of W (here d indicates the total number of features). These vectors are the principal predictors for our weight space. If we chose our pivot features well, then we expect these principal predictors to discriminate a</context>
<context position="15558" citStr="Ando and Zhang (2005)" startWordPosition="2537" endWordPosition="2540"> domain is quite broad, many projections in books model topic distinctions such as between religious and political books. These projections, which are uninformative as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain. When we adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy. 5 Correcting Misalignments We now show how to use a small amount of target domain labeled data to learn to ignore misaligned projections from SCL-MI. Using the notation of Ando and Zhang (2005), we can write the supervised training objective of SCL on the source domain as �min L (w0xi + v0θxi, yi) + λ||w||2 + µ||v||2 , w,v i where y is the label. The weight vector w E Rd weighs the original features, while v E Rk weighs the projected features. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which we have used in our results so far. Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller. We augment each labeled </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<note>http://research.microsoft.com/ anthaue/.</note>
<contexts>
<context position="24012" citStr="Aue and Gamon (2005)" startWordPosition="4027" endWordPosition="4030">n adaptation. The work most similar in spirit to ours that of Turney (2002). He used the difference in mutual information with two human-selected features (the words “excellent” and “poor”) to score features in a completely unsupervised manner. Then he classified documents according to various functions of these mutual information scores. We stress that our method improves a supervised baseline. While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset. We also note the work of Aue and Gamon (2005), who performed a number of empirical tests on domain adaptation of sentiment classifiers. Most of these tests were unsuccessful. We briefly note their results on combining a number of source domains. They observed that source domains closer to the target helped more. In preliminary experiments we confirmed these results. Adding more labeled data always helps, but diversifying training data does not. When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains. Domain ada</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. http://research.microsoft.com/ anthaue/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2006</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="4185" citStr="Ben-David et al., 2006" startWordPosition="623" endWordPosition="626">ation algorithm (Blitzer et al., 2006) for use in sentiment classification. A key step in SCL is the selection of pivot features that are used to link the source and target domains. We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels. For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains. In our second extension we show how to correct misalignments using a very small number of labeled instances. Second, we evaluate the A-distance (Ben-David et al., 2006) between domains as measure of the loss due to adaptation from one to the other. The Adistance can be measured from unlabeled data, and it was designed to take into account only divergences which affect classification accuracy. We show that it correlates well with adaptation loss, indicating that we can use the A-distance to select a subset of domains to label as sources. In the next section we briefly review SCL and introduce our new pivot selection method. Section 3 describes datasets and experimental method. Section 4 gives results for SCL and the mutual information method for selecting piv</context>
<context position="20904" citStr="Ben-David et al. (2006)" startWordPosition="3500" endWordPosition="3503">ons. Here we make use of the A-distance (BenDavid et al., 2006). The key intuition behind the A-distance is that while two domains can differ in arbitrary ways, we are only interested in the differences that affect classification accuracy. Let A be the family of subsets of Rk corresponding to characteristic functions of linear classifiers (sets on which a linear classifier returns positive value). Then the A distance between two probability distributions is dA(D, D0) = 2 sup |PrD [A] − PrD, [A] |. A∈A That is, we find the subset in A on which the distributions differ the most in the Li sense. Ben-David et al. (2006) show that computing the A-distance for a finite sample is exactly the problem of minimizing the empirical risk of a classifier that discriminates between instances drawn from D and instances drawn from D0. This is convenient for us, since it allows us to use classification machinery to compute the A-distance. 6.2 Unlabeled Adaptability Measurements We follow Ben-David et al. (2006) and use the Huber loss as a proxy for the A-distance. Our procedure is as follows: Given two domains, we compute the SCL representation. Then we create a data set where each instance Ox is labeled with the identity</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2006</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2006. Analysis of representations for domain adaptation. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3600" citStr="Blitzer et al., 2006" startWordPosition="525" endWordPosition="528"> adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta. edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2The dataset will be made available by the authors at publication time. 440 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics respondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification. A key step in SCL is the selection of pivot features that are used to link the source and target domains. We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels. For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains. In our second extension we show how to correct misalignments using a very small number of labeled instances. Second, we evaluate the A-distance (Ben-David et al., 2006) between domain</context>
<context position="6435" citStr="Blitzer et al., 2006" startWordPosition="1005" endWordPosition="1008">rning a classifier for computer reviews, when we see a cell-phone feature like “goodquality reception”, we know it should behave in a roughly similar manner to “fast dual-core”. 2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains, SCL first chooses a set of m pivot features which occur frequently in both domains. Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al., 2006). The Eth pivot predictor is characterized by its weight vector wt; positive entries in that weight vector mean that a non-pivot feature (like “fast dualcore”) is highly correlated with the corresponding pivot (like “excellent”). The pivot predictor column weight vectors can be arranged into a matrix W = [wt]e1. Let 0 E Rkxd be the top k left singular vectors of W (here d indicates the total number of features). These vectors are the principal predictors for our weight space. If we chose our pivot features well, then we expect these principal predictors to discriminate among positive and negat</context>
<context position="15860" citStr="Blitzer et al. (2006)" startWordPosition="2596" endWordPosition="2599">e adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy. 5 Correcting Misalignments We now show how to use a small amount of target domain labeled data to learn to ignore misaligned projections from SCL-MI. Using the notation of Ando and Zhang (2005), we can write the supervised training objective of SCL on the source domain as �min L (w0xi + v0θxi, yi) + λ||w||2 + µ||v||2 , w,v i where y is the label. The weight vector w E Rd weighs the original features, while v E Rk weighs the projected features. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which we have used in our results so far. Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller. We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006). Then we solve E minw,v j L (w0xj + v0θxj, yj) + λ||w||2 +µ||v − vs||2 . Since we don’t want to deviate significantly from the source parameters, we set λ = µ = 10−1. Figure 2 shows </context>
<context position="24841" citStr="Blitzer et al., 2006" startWordPosition="4163" endWordPosition="4166">served that source domains closer to the target helped more. In preliminary experiments we confirmed these results. Adding more labeled data always helps, but diversifying training data does not. When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains. Domain adaptation alone is a generally wellstudied area, and we cannot possibly hope to cover all of it here. As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al., 2006). We also note that while Florian et al. (2004) and Blitzer et al. (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not. In contrast our simple binary prediction problem does not exhibit such behavior. This may also </context>
<context position="26450" citStr="Blitzer et al. (2006)" startWordPosition="4446" endWordPosition="4449">ake better use of our small amount of unlabeled data than these previous techniques. BE, DK BK DE BD EK 60 65 70 75 80 85 90 95 100 Proxy A-distance Adaptation Loss 14 12 10 8 6 4 2 0 446 8 Conclusion Sentiment classification has seen a great deal of attention. Its application to many different domains of discourse makes it an ideal candidate for domain adaptation. This work addressed two important questions of domain adaptation. First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006). We chose pivot features using not only common frequency among domains but also mutual information with the source labels. We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data. Second, we provided a method for selecting those source domains most likely to adapt well to given target domains. The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation. Thus we can use the Adistance to select source domains to label which will give low target domain error. In the future, we wi</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17977" citStr="Chelba and Acero (2004)" startWordPosition="2992" endWordPosition="2995">e any SCL features. We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters in w. As we can see, though, relearning the 50 parameters in v is quite helpful. The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure. The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation. In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only. For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline. Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000. Table 3 su</context>
<context position="25496" citStr="Chelba and Acero (2004)" startWordPosition="4279" endWordPosition="4282">n et al. (2004) and Blitzer et al. (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not. In contrast our simple binary prediction problem does not exhibit such behavior. This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. Finally we note that while Blitzer et al. (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al. (2004). By only adapting the SCLrelated part of the weight vector v, we are able to make better use of our small amount of unlabeled data than these previous techniques. BE, DK BK DE BD EK 60 65 70 75 80 85 90 95 100 Proxy A-distance Adaptation Loss 14 12 10 8 6 4 2 0 446 8 Conclusion Sentiment classification has seen a great deal of attention. Its a</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of Athe Asia Pacific Finance Association Annual Conference.</booktitle>
<contexts>
<context position="1548" citStr="Das and Chen, 2001" startWordPosition="218" endWordPosition="221">ilarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time. We e</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>Sanjiv Das and Mike Chen. 2001. Yahoo! for amazon: Extracting market sentiment from stock message boards. In Proceedings of Athe Asia Pacific Finance Association Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>H Jing A Ittycheriah</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In of HLT-NAACL.</booktitle>
<contexts>
<context position="16254" citStr="Florian et al., 2004" startWordPosition="2672" endWordPosition="2675"> as �min L (w0xi + v0θxi, yi) + λ||w||2 + µ||v||2 , w,v i where y is the label. The weight vector w E Rd weighs the original features, while v E Rk weighs the projected features. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which we have used in our results so far. Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller. We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006). Then we solve E minw,v j L (w0xj + v0θxj, yj) + λ||w||2 +µ||v − vs||2 . Since we don’t want to deviate significantly from the source parameters, we set λ = µ = 10−1. Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances. We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort. For reasons of space, for each target domain dom \ model base base scl scl-mi scl-mi +targ +targ books 8.9 9.0 7.4 5.8 4.4 dvd 8.9 8.9 7.8 6.1 5.3 electron 8.3 8.5 6.0 5.5 4.8 kitchen 10.2 9.9 7.0 5.6 5.1 a</context>
<context position="24888" citStr="Florian et al. (2004)" startWordPosition="4172" endWordPosition="4175"> helped more. In preliminary experiments we confirmed these results. Adding more labeled data always helps, but diversifying training data does not. When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains. Domain adaptation alone is a generally wellstudied area, and we cannot possibly hope to cover all of it here. As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al., 2006). We also note that while Florian et al. (2004) and Blitzer et al. (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not. In contrast our simple binary prediction problem does not exhibit such behavior. This may also be the reason that the model of Chelba and Acer</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goldberg</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="1347" citStr="Goldberg and Zhu, 2004" startWordPosition="186" endWordPosition="189"> algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic</context>
</contexts>
<marker>Goldberg, Zhu, 2004</marker>
<rawString>Andrew Goldberg and Xiaojin Zhu. 2004. Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22936" citStr="Pang and Lee (2005)" startWordPosition="3848" endWordPosition="3851">on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances. Similarly we would also choose one domain from either electronics or kitchen appliances, but not both. 7 Related Work Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline. Thomas et al. (2006) use discourse structure present in congressional records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation. The work most similar in spirit to ours that of Turney (2002). He used the difference in mutual information with two human-selecte</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1308" citStr="Pang et al., 2002" startWordPosition="180" endWordPosition="183">ral correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in th</context>
<context position="9139" citStr="Pang et al., 2002" startWordPosition="1460" endWordPosition="1463"> a new dataset for sentiment domain adaptation by selecting Amazon product reviews for four different product types: books, DVDs, electronics and kitchen appliances. Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text. Reviews with rating &gt; 3 were labeled positive, those with rating &lt; 3 were labeled negative, and the rest discarded because their polarity was ambiguous. After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al., 2002). In addition to the labeled data, we included between 3685 (DVDs) and 5945 (kitchen) instances of unlabeled data. The size of the unlabeled data was limited primarily by the number of reviews we could crawl and download from the Amazon website. Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well. While the polarity dataset is a popular choice in the literature, we were unable to use it for our task. Our method requires many unlabeled reviews and despite a large number of IMDB reviews available onli</context>
<context position="22756" citStr="Pang et al. (2002)" startWordPosition="3821" endWordPosition="3824">on loss of as measured by our baseline system. Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen. way as to minimize our error on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances. Similarly we would also choose one domain from either electronics or kitchen appliances, but not both. 7 Related Work Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline. Thomas et al. (2006) use discourse structure present in congressional records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1570" citStr="Thomas et al., 2006" startWordPosition="222" endWordPosition="225">tes well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time. We envision a scenario in </context>
<context position="22808" citStr="Thomas et al. (2006)" startWordPosition="3831" endWordPosition="3834">h pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen. way as to minimize our error on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances. Similarly we would also choose one domain from either electronics or kitchen appliances, but not both. 7 Related Work Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline. Thomas et al. (2006) use discourse structure present in congressional records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation. Th</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1322" citStr="Turney, 2002" startWordPosition="184" endWordPosition="185">learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of mark</context>
<context position="23467" citStr="Turney (2002)" startWordPosition="3937" endWordPosition="3939">nal records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation. The work most similar in spirit to ours that of Turney (2002). He used the difference in mutual information with two human-selected features (the words “excellent” and “poor”) to score features in a completely unsupervised manner. Then he classified documents according to various functions of these mutual information scores. We stress that our method improves a supervised baseline. While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset. We also note the work of Aue and Gamon (2005), who performed a number of empirical tests on domain a</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<marker>Zhang, 2004</marker>
<rawString>Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>