<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.710763">
Class-Based n-gram Models of Natural
Language
</title>
<author confidence="0.975625333333333">
Peter F. Brown* Vincent J. Della Pietra*
Peter V. deSouza* Jenifer C. Lai*
Robert L. Mercer*
</author>
<bodyText confidence="0.979219666666667">
IBM T. J. Watson Research Center
We address the problem of predicting a word from previous words in a sample of text. In particular,
we discuss n-gram models based on classes of words. We also discuss several statistical algorithms
for assigning words to classes based on the frequency of their co-occurrence with other words. We
find that we are able to extract classes that have the flavor of either syntactically based groupings
or semantically based groupings, depending on the nature of the underlying statistics.
</bodyText>
<sectionHeader confidence="0.990269" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999902375">
In a number of natural language processing tasks, we face the problem of recovering a
string of English words after it has been garbled by passage through a noisy channel.
To tackle this problem successfully, we must be able to estimate the probability with
which any particular string of English words will be presented as input to the noisy
channel. In this paper, we discuss a method for making such estimates. We also discuss
the related topic of assigning words to classes according to statistical behavior in a
large body of text.
In the next section, we review the concept of a language model and give a defini-
tion of n-gram models. In Section 3, we look at the subset of n-gram models in which
the words are divided into classes. We show that for n = 2 the maximum likelihood
assignment of words to classes is equivalent to the assignment for which the average
mutual information of adjacent classes is greatest. Finding an optimal assignment of
words to classes is computationally hard, but we describe two algorithms for finding a
suboptimal assignment. In Section 4, we apply mutual information to two other forms
of word clustering. First, we use it to find pairs of words that function together as a
single lexical entity. Then, by examining the probability that two words will appear
within a reasonable distance of one another, we use it to find classes that have some
loose semantic coherence.
In describing our work, we draw freely on terminology and notation from the
mathematical theory of communication. The reader who is unfamiliar with this field
or who has allowed his or her facility with some of its concepts to fall into disrepair
may profit from a brief perusal of Feller (1950) and Gallagher (1968). In the first of
these, the reader should focus on conditional probabilities and on Markov chains; in
the second, on entropy and mutual information.
</bodyText>
<note confidence="0.48244425">
* IBM T. J. Watson Research Center, Yorktown Heights, New York 10598.
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 4
Pr (W) Pr (Y I W) = Pr (W, Y)
</note>
<figureCaption confidence="0.980874">
Figure 1
</figureCaption>
<bodyText confidence="0.742299">
Source-channel setup.
</bodyText>
<sectionHeader confidence="0.993989" genericHeader="method">
2. Language Models
</sectionHeader>
<bodyText confidence="0.999962052631579">
Figure 1 shows a model that has long been used in automatic speech recognition (Bahl,
Jelinek, and Mercer 1983) and has recently been proposed for machine translation
(Brown et al. 1990) and for automatic spelling correction (Mays, Demerau, and Mercer
1990). In automatic speech recognition, y is an acoustic signal; in machine translation, y
is a sequence of words in another language; and in spelling correction, y is a sequence
of characters produced by a possibly imperfect typist. In all three applications, given
a signal y, we seek to determine the string of English words, w, which gave rise to
it. In general, many different word strings can give rise to the same signal and so
we cannot hope to recover w successfully in all cases. We can, however, minimize
our probability of error by choosing as our estimate of w that string W for which the
a posteriori probability of W given y is greatest. For a fixed choice of y, this probability
is proportional to the joint probability of * and y which, as shown in Figure 1, is the
product of two terms: the a priori probability of W and the probability that y will
appear at the output of the channel when * is placed at the input. The a priori
probability of W, Pr (W), is the probability that the string W will arise in English. We
do not attempt a formal definition of English or of the concept of arising in English.
Rather, we blithely assume that the production of English text can be characterized by
a set of conditional probabilities, Pr(wk w), in terms of which the probability of a
string of words, w, can be expressed as a product:
</bodyText>
<equation confidence="0.922934">
Pr (w1) = Pr (w1) Pr (w2 I ) • • • Pr (wk I 14-1) (1)
</equation>
<bodyText confidence="0.999076363636364">
Here, wki-1 represents the string wi w2 • • • wk_i . In the conditional probability Pr(wk I
-1 ,
) we call wk-1 the history and wk the prediction. We refer to a computational
mechanism for obtaining these conditional probabilities as a language model.
Often we must choose which of two different language models is the better one.
The performance of a language model in a complete system depends on a delicate
interplay between the language model and other components of the system. One lan-
guage model may surpass another as part of a speech recognition system but perform
less well in a translation system. However, because it is expensive to evaluate a lan-
guage model in the context of a complete system, we are led to seek an intrinsic
measure of the quality of a language model. We might, for example, use each lan-
</bodyText>
<figure confidence="0.9699068">
Source
Language
Model
Channel
Model
</figure>
<page confidence="0.997881">
468
</page>
<subsectionHeader confidence="0.398019">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
</subsectionHeader>
<bodyText confidence="0.999976259259259">
guage model to compute the joint probability of some collection of strings and judge
as better the language model that yields the greater probability The perplexity of a
language model with respect to a sample of text, S. is the reciprocal of the geomet-
ric average of the probabilities of the predictions in S. If S has I S I words, then the
perplexity is Pr (5)-1/1s1. Thus, the language model with the smaller perplexity will be
the one that assigns the larger probability to S. Because the perplexity depends not
only on the language model but also on the text with respect to which it is measured,
it is important that the text be representative of that for which the language model
is intended. Because perplexity is subject to sampling error, making fine distinctions
between language models may require that the perplexity be measured with respect
to a large sample.
In an n-gram language model, we treat two histories as equivalent if they end in
the same n - 1 words, i.e., we assume that for k &gt; n, Pr (wk w1k-1) is equal to
Pr (wk I wifcin1+1). For a vocabulary of size V, a 1-gram model has V - 1 independent
parameters, one for each word minus one for the constraint that all of the probabilities
add up to 1. A 2-gram model has V(V - 1) independent parameters of the form
Pr (102 I wi ) and V - 1 of the form Pr (w) for a total of V2 - 1 independent parameters.
In general, an n-gram model has V&amp;quot; - 1 independent parameters: V&amp;quot;-1(V - 1) of the
form Pr (wn I wr1), which we call the order-n parameters, plus the 17n-1-1 parameters
of an (n - 1)-gram model.
We estimate the parameters of an n-gram model by examining a sample of text,
tf, which we call the training text, in a process called training. If C(w) is the number
of times that the string w occurs in the string 1-T, then for a 1-gram language model
the maximum likelihood estimate for the parameter Pr (w) is C(w)/T. To estimate the
parameters of an n-gram model, we estimate the parameters of the (n -1)-gram model
that it contains and then choose the order-n parameters so as to maximize Pr (tnT trii -1).
Thus, the order-n parameters are
</bodyText>
<equation confidence="0.645417">
Pr (wn I w7-1) c(zdit-lw) •
</equation>
<bodyText confidence="0.99986915">
We call this method of parameter estimation sequential maximum likelihood estimation.
We can think of the order-n parameters of an n-gram model as constituting the
transition matrix of a Markov model the states of which are sequences of n - 1 words.
Thus, the probability of a transition between the state W1W2 • • &apos; Wn-1 and the state
w2w3 • • • wn is Pr (w I W1102 • • • wn-i ) . The steady-state distribution for this transition
matrix assigns a probability to each (n - 1)-gram, which we denote S(w7-1). We say
that an n-gram language model is consistent if, for each string w7-1, the probability that
the model assigns to win-1 is S(win-1). Sequential maximum likelihood estimation does
not, in general, lead to a consistent model, although for large values of T, the model
will be very nearly consistent. Maximum likelihood estimation of the parameters of a
consistent n-gram language model is an interesting topic, but is beyond the scope of
this paper.
The vocabulary of English is very large and so, even for small values of n, the
number of parameters in an n-gram model is enormous. The IBM Tangora speech
recognition system has a vocabulary of about 20,000 words and employs a 3-gram
language model with over eight trillion parameters (Averbuch et al. 1987). We can
illustrate the problems attendant to parameter estimation for a 3-gram language model
with the data in Table 1. Here, we show the number of 1-, 2-, and 3-grams appearing
with various frequencies in a sample of 365,893,263 words of English text from a
variety of sources. The vocabulary consists of the 260,740 different words plus a special
</bodyText>
<equation confidence="0.51641">
c(w7-1wn)
(2)
</equation>
<page confidence="0.99903">
469
</page>
<table confidence="0.998211125">
Computational Linguistics Volume 18, Number 4
Count 1-grams 2-grams 3-grams
1 36,789 8,045,024 53,737,350
2 20,269 2,065,469 9,229,958
3 13,123 970,434 3,653,791
&gt; 3 135,335 3,413,290 8,728,789
&gt; 0 205,516 14,494,217 75,349,888
&gt; 0 260,741 6.799 x 1010 1.773 x 1016
</table>
<tableCaption confidence="0.999146">
Table 1
</tableCaption>
<bodyText confidence="0.972945111111111">
Number of n-grams with various frequencies in 365,893,263 words of running text.
unknown word into which all other words are mapped. Of the 6.799 x 1010 2-grams that
might have occurred in the data, only 14,494,217 actually did occur and of these,
8,045,024 occurred only once each. Similarly, of the 1.773 x 1016 3-grams that might
have occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurred
only once each. From these data and Turing&apos;s formula (Good 1953), we can expect
that maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for
2.2 percent of the 2-grams in a new sample of English text. We can be confident that
any 3-gram that does not appear in our sample is, in fact, rare, but there are so many
of them that their aggregate probability is substantial.
As n increases, the accuracy of an n-gram model increases, but the reliability of our
parameter estimates, drawn as they must be from a limited training text, decreases.
Jelinek and Mercer (1980) describe a technique called interpolated estimation that com-
bines the estimates of several language models so as to use the estimates of the more
accurate models where they are reliable and, where they are unreliable, to fall back on
the more reliable estimates of less accurate models. If Pri (w I &amp;I.-1) is the conditional
probability as determined by the jth language model, then the interpolated estimate,
Pr(wi I w&apos;i-1), is given by
</bodyText>
<equation confidence="0.916468">
Pr (wi I = E (w11-1)Pr(l) (wz (3)
</equation>
<bodyText confidence="0.999981153846154">
Given values for Pr(i) 0, the A1(w) are chosen, with the help of the EM algorithm,
so as to maximize the probability of some additional sample of text called the held-out
data (Baum 1972; Dempster, Laird, and Rubin 1977; Jelinek and Mercer 1980). When we
use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models,
we choose the As to depend on the history, w1, only through the count of the 2-
gram, We expect that where the count of the 2-gram is high, the 3-gram
estimates will be reliable, and, where the count is low, the estimates will be unreliable.
We have constructed an interpolated 3-gram model in which we have divided the As
into 1,782 different sets according to the 2-gram counts. We estimated these As from a
held-out sample of 4,630,934 words. We measure the performance of our model on the
Brown corpus, which contains a variety of English text and is not included in either
our training or held-out data (Kiera and Francis 1967). The Brown corpus contains
1,014,312 words and has a perplexity of 244 with respect to our interpolated model.
</bodyText>
<sectionHeader confidence="0.934035" genericHeader="method">
3. Word Classes
</sectionHeader>
<bodyText confidence="0.998269666666667">
Clearly, some words are similar to other words in their meaning and syntactic function.
We would not be surprised to learn that the probability distribution of words in the
vicinity of Thursday is very much like that for words in the vicinity of Friday. Of
</bodyText>
<page confidence="0.986233">
470
</page>
<bodyText confidence="0.998408375">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
course, they will not be identical: we rarely hear someone say Thank God it&apos;s Thursday!
or worry about Thursday the 13th. If we can successfully assign words to classes, it
may be possible to make more reasonable predictions for histories that we have not
previously seen by assuming that they are similar to other histories that we have seen.
Suppose that we partition a vocabulary of V words into C classes using a function,
7r, which maps a word, wi, into its class, ci. We say that a language model is an n-
gram class model if it is an n-gram language model and if, in addition, for 1 &lt; k &lt; n,
</bodyText>
<equation confidence="0.8700965">
Pr (wk w1)
= Pr (wk I ck) Pr (ck I 4-1). An n-gram class model has C&amp;quot; -1 + V - C
</equation>
<bodyText confidence="0.991951">
independent parameters: V - C of the form Pr (w j c,), plus the C&amp;quot; - 1 independent
parameters of an n-gram language model for a vocabulary of size C. Thus, except in
the trivial cases in which C --= V or n 1, an n-gram class language model always has
fewer independent parameters than a general n-gram language model.
Given training text, tr, the maximum likelihood estimates of the parameters of a
1-gram class model are
</bodyText>
<equation confidence="0.999319">
Pr (w c) = Cc(ro) , (4)
Pr (c) C(c)
T
</equation>
<bodyText confidence="0.9968018">
where by C(c) we mean the number of words in tf for which the class is c. From these
equations, we see that, since c = r(w), Pr (w) = Pr (w I c) Pr (c) = C(w)/T. For a 1-gram
class model, the choice of the mapping it has no effect. For a 2-gram class model,
the sequential maximum likelihood estimates of the order-2 parameters maximize
Pr (tII ti) or, equivalently, log Pr(tr I t1) and are given by
</bodyText>
<equation confidence="0.99512">
C(cic2)
Pr (c2 I ci
Ec c(cic) •
</equation>
<bodyText confidence="0.9651195">
By definition, Pr (ci c2) = Pr (ci) Pr (c2 I ci), and so, for sequential maximum likelihood
estimation, we have
</bodyText>
<equation confidence="0.97187">
,
Pr (C1C2) = C(C1C2)x C(Ci)
Ec c(cio • (7)
T
</equation>
<bodyText confidence="0.99892825">
Since C(ci ) and Ec c(cio are the numbers of words for which the class is ci in the
strings ti. and tiT-1 respectively, the final term in this equation tends to 1 as T tends to
infinity. Thus, Pr (ci c2) tends to the relative frequency of ci c2 as consecutive classes in
the training text.
</bodyText>
<equation confidence="0.957081888888889">
Let L(r) = (T - 1)-1 log Pr (tr I ti). Then
L(7r) = E c(wiw2) T - log Pr (c2 I ci) Pr (w2 I c)
1
wiw2
EC(cic2) i„, (2 I C1) y` Ew C(WW2) log Pr (w2 c2 Pr (c2) (8)
T - 1 -1&apos; Pr (c2) T - 1
w2
cicz
Pr(w2)
</equation>
<bodyText confidence="0.99913">
Therefore, since Ew c(ww2)/(T— 1) tends to the relative frequency of w2 in the training
text, and hence to Pr (w2), we must have, in the limit,
</bodyText>
<equation confidence="0.8357885">
L(7) = E Pr (w) log Pr (w) + E Pr (ci c2) Pr (c2 ci)
log Pr (c2)
cie2
= -H(w) + /(ci , c2),
and
(9)
</equation>
<page confidence="0.964356">
471
</page>
<note confidence="0.512827">
Computational Linguistics Volume 18, Number 4
</note>
<bodyText confidence="0.999971647058824">
where H(w) is the entropy of the 1-gram word distribution and /(ci , c2) is the average
mutual information of adjacent classes. Because L(7r) depends on 7r only through this
average mutual information, the partition that maximizes L(7r) is, in the limit, also the
partition that maximizes the average mutual information of adjacent classes.
We know of no practical method for finding one of the partitions that maximize the
average mutual information. Indeed, given such a partition, we know of no practical
method for demonstrating that it does, in fact, maximize the average mutual informa-
tion. We have, however, obtained interesting results using a greedy algorithm. Initially,
we assign each word to a distinct class and compute the average mutual information
between adjacent classes. We then merge that pair of classes for which the loss in
average mutual information is least. After V — C of these merges, C classes remain.
Often, we find that for classes obtained in this way the average mutual information
can be made larger by moving some words from one class to another. Therefore, after
having derived a set of classes from successive merges, we cycle through the vocabu-
lary moving each word to the class for which the resulting partition has the greatest
average mutual information. Eventually no potential reassignment of a word leads to
a partition with greater average mutual information. At this point, we stop. It may
be possible to find a partition with higher average mutual information by simultane-
ously reassigning two or more words, but we regard such a search as too costly to be
feasible.
To make even this suboptimal algorithm practical one must exercise a certain care
in implementation. There are approximately (V-02/2 merges that we must investigate
to carry out the ith step. The average mutual information remaining after any one of
them is the sum of (V — 02 terms, each of which involves a logarithm. Since altogether
we must make V — C merges, this straightforward approach to the computation is of
order V5. We cannot seriously contemplate such a calculation except for very small
values of V. A more frugal organization of the computation must take advantage of
the redundancy in this straightforward calculation. As we shall see, we can make the
computation of the average mutual information remaining after a merge in constant
time, independent of V.
Suppose that we have already made V —k merges, resulting in classes Ck(1), Ck (2),
, Ck (k) and that we now wish to investigate the merge of Ck (i) with Ck (j for 1 &lt;
i &lt;j &lt; k. Let pk(1, m) -= Pr (Ck (0, Ck(m)), i.e., the probability that a word in class Ck (m)
follows a word in class Ck(1). Let
</bodyText>
<equation confidence="0.981457666666667">
pik(/)=E pk(1, m),
let
prk(m)=Epk(1,m),
</equation>
<bodyText confidence="0.536768">
and let
</bodyText>
<equation confidence="0.9989045">
q(1,m) = pk(1, m) log Pk (1&apos; m)
plk (1)prk(m) •
</equation>
<bodyText confidence="0.833329">
The average mutual information remaining after V — k merges is
</bodyText>
<equation confidence="0.8318955">
Ik = Eqk(1,m).
1,m
</equation>
<bodyText confidence="0.95671">
We use the notation i+ j to represent the cluster obtained by merging Ck(i) and Ck(i)•
</bodyText>
<page confidence="0.723431">
(13)
472
</page>
<table confidence="0.847132111111111">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
Thus, for example, pk(i + j, m) = pk(i,m) + pk(j,m) and
qk +1, m) Pk(i +j, m) log Pk(i m)
Plk(i + l)Prk(m)•
The average mutual information remaining after we merge Ck(i) and Ck(j) is then
where Ik(i,j)= Ik - Sk(i) - Sk(i) qk(i, + qk(j,i) + qk(i + j,i +j)
+ DO, + + E qkci
moij
so) Eqk(1,0+ qk(i, m) - qk(i, i). (16)
</table>
<bodyText confidence="0.996825533333333">
If we know Ik. SO), and sk(j), then the majority of the time involved in computing
Ik(i,j) is devoted to computing the sums on the second line of equation (15). Each of
these sums has approximately V - k terms and so we have reduced the problem of
evaluating Ik(i,j) from one of order V2 to one of order V. We can improve this further
by keeping track of those pairs 1,m for which pk(1,m) is different from 0. We recall
from Table 1, for example, that of the 6.799 x 1010 2-grams that might have occurred
in the training data, only 14,494,217 actually did occur. Thus, in this case, the sums
required in equation (15) have, on average, only about 56 non-zero terms instead of
260,741, as we might expect from the size of the vocabulary
By examining all pairs, we can find that pair, i &lt;j, for which the loss in average
mutual information, Lk (i, j) - Ik(i, j), is least. We complete the step by merging
Ck(i) and Ck(j) to form a new cluster Ck_i (i). If j k, we rename Ck(k) as Ck_i (i) and
for 1 i,j, we set Ck-i (1) to Ck(/). Obviously, Ik-i = Ik(i,j). The values of Pk-1,
prk_i, and qk_...1 can be obtained easily from Pk, plk, prk, and qk. If 1 and m both denote
indices neither of which is equal to either i or j, then it is easy to establish that
</bodyText>
<equation confidence="0.999963571428572">
sk_1(1) = sk(1) — q(1,i) — cik(i,1) — qk(1,)) — qk(1,1)+qk_1(1,1)+qk—i(i,1)
sk—i(i) = sk(k)— qk(k,1)— qk(1,k) — qk(k,j)— qk(i,k) + qk_1(1,1)+qt—i(1,j)
M) = Lk(1, in) — qk(/ — qk(i,1 + m) - qk(1+ m, j) - 1k(j,1 + m)
+qk--1(1 + + + In)
Lk_1(1,j) = 14(1,k) - qk(1 + k,i) - qk(i,1 + k) - qk(1 + k, j) - qk(j,l+k)
+q1c-1(1+ + qk-1(i11+
, 1) = 14-1(1, (17)
</equation>
<bodyText confidence="0.9992775">
Finally, we must evaluate sk_1(i) and Lk_1(/, i) from equations 15 and 16. Thus, the
entire update process requires something on the order of V2 computations in the
course of which we will determine the next pair of clusters to merge. The algorithm,
then, is of order V3.
Although we have described this algorithm as one for finding clusters, we actually
determine much more. If we continue the algorithm for V - 1 merges, then we will
have a single cluster which, of course, will be the entire vocabulary. The order in which
clusters are merged, however, determines a binary tree the root of which corresponds
</bodyText>
<page confidence="0.998542">
473
</page>
<table confidence="0.919012761904762">
Computational Linguistics Volume 18, Number 4
plan evaluation
letter assessment
request
memo
case
analysis
understanding
opinion
conversation
discussion I
question --I
charge --I
statement
draft --I
day accounts
year people
week customers
month _Th] individuals
quarter employees
half students I
</table>
<bodyText confidence="0.399597">
reps
representatives
representative
rep
</bodyText>
<figureCaption confidence="0.905856">
Figure 2
</figureCaption>
<bodyText confidence="0.987151173913043">
Sample subtrees from a 1,000-word mutual information tree.
to this single cluster and the leaves of which correspond to the words in the vocabulary.
Intermediate nodes of the tree correspond to groupings of words intermediate between
single words and the entire vocabulary. Words that are statistically similar with respect
to their immediate neighbors in running text will be close together in the tree. We
have applied this tree-building algorithm to vocabularies of up to 5,000 words. Figure
2 shows some of the substructures in a tree constructed in this manner for the 1,000
most frequent words in a collection of office correspondence.
Beyond 5,000 words this algorithm also fails of practicality. To obtain clusters for
larger vocabularies, we proceed as follows. We arrange the words in the vocabulary
in order of frequency with the most frequent words first and assign each of the first
C words to its own, distinct class. At the first step of the algorithm, we assign the
(C + 1)st most probable word to a new class and merge that pair among the resulting
C + 1 classes for which the loss in average mutual information is least. At the kth step
of the algorithm, we assign the (C + k)th most probable word to a new class. This
restores the number of classes to C + 1, and we again merge that pair for which the
loss in average mutual information is least. After V — C steps, each of the words in
the vocabulary will have been assigned to one of C classes.
We have used this algorithm to divide the 260,741-word vocabulary of Table 1 into
1,000 classes. Table 2 contains examples of classes that we find particularly interesting.
Table 3 contains examples that were selected at random. Each of the lines in the tables
contains members of a different class. The average class has 260 words and so to
make the table manageable, we include only words that occur at least ten times and
</bodyText>
<page confidence="0.998425">
474
</page>
<table confidence="0.942701772727273">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
Friday Monday Thursday Wednesday Tuesday Saturday Sunday weekends Sundays Saturdays
June March July April January December October November September August
people guys folks fellows CEOs chaps doubters commies unfortunates blokes
down backwards ashore sideways southward northward overboard aloft downwards adrift
water gas coal liquid acid sand carbon steam shale iron
great big vast sudden mere sheer gigantic lifelong scant colossal
man woman boy girl lawyer doctor guy farmer teacher citizen
American Indian European Japanese German African Catholic Israeli Italian Arab
pressure temperature permeability density porosity stress velocity viscosity gravity tension
mother wife father son husband brother daughter sister boss uncle
machine device controller processor CPU printer spindle subsystem compiler plotter
John George James Bob Robert Paul William Jim David Mike
anyone someone anybody somebody
feet miles pounds degrees inches barrels tons acres meters bytes
director chief professor commissioner commander treasurer founder superintendent dean cus-
todian
liberal conservative parliamentary royal progressive Tory provisional separatist federalist PQ
had hadn&apos;t hath would&apos;ve could&apos;ve should&apos;ve must&apos;ve might&apos;ve
asking telling wondering instructing informing kidding reminding bothering thanking deposing
that tha theat
head body hands eyes voice arm seat eye hair mouth
</table>
<tableCaption confidence="0.998101">
Table 2
</tableCaption>
<subsectionHeader confidence="0.560203">
Classes from a 260,741-word vocabulary.
</subsectionHeader>
<bodyText confidence="0.99994425">
we include no more than the ten most frequent words of any class (the other two
months would appear with the class of months if we extended this limit to twelve).
The degree to which the classes capture both syntactic and semantic aspects of English
is quite surprising given that they were constructed from nothing more than counts
of bigrams. The class {that tha theat} is interesting because although tha and theat are
not English words, the computer has discovered that in our data each of them is most
often a mistyped that.
Table 4 shows the number of class 1-, 2-, and 3-grams occurring in the text with
various frequencies. We can expect from these data that maximum likelihood estimates
will assign a probability of 0 to about 3.8 percent of the class 3-grams and to about
.02 percent of the class 2-grams in a new sample of English text. This is a substantial
improvement over the corresponding numbers for a 3-gram language model, which
are 14.7 percent for word 3-grams and 2.2 percent for word 2-grams, but we have
achieved this at the expense of precision in the model. With a class model, we distin-
guish between two different words of the same class only according to their relative
frequencies in the text as a whole. Looking at the classes in Tables 2 and 3, we feel that
</bodyText>
<page confidence="0.997243">
475
</page>
<table confidence="0.695592619047619">
Computational Linguistics Volume 18, Number 4
little prima moment&apos;s trifle tad Litle minute&apos;s tinker&apos;s hornet&apos;s teammate&apos;s
6
ask remind instruct urge interrupt invite congratulate commend warn applaud
object apologize apologise avow whish
cost expense risk profitability deferral earmarks capstone cardinality mintage reseller
B dept. AA Whitey CL pi Namerow PA Mgr. LaRose
# Rel rel. #S Shree
S Gens nai Matsuzawa ow Kageyama Nishida Sumit Zollner Mallik
research training education science advertising arts medicine machinery Art AIDS
rise focus depend rely concentrate dwell capitalize embark intrude typewriting
Minister mover Sydneys Minster Miniter
3
running moving playing setting holding carrying passing cutting driving fighting
court judge jury slam Edelstein magistrate marshal AbeIla Scalia larceny
annual regular monthly daily weekly quarterly periodic Good yearly convertible
aware unaware unsure cognizant apprised mindful partakers
force ethic stoppage force&apos;s conditioner stoppages conditioners waybill forwarder Atonabee
systems magnetics loggers products&apos; coupler Econ databanks Centre inscriber correctors
industry producers makers fishery Arabia growers addiction medalist inhalation addict
brought moved opened picked caught tied gathered cleared hung lifted
</table>
<tableCaption confidence="0.998599">
Table 3
</tableCaption>
<table confidence="0.989055875">
Randomly selected word classes.
Count 1-gram 2-grams 3-grams
1 0 81,171 13,873,192
2 0 57,056 4,109,998
3 0 43,752 2,012,394
&gt; 3 1,000 658,564 6,917,746
&gt; 0 1,000 840,543 26,913,330
&gt; 0 1,000 1,000,000 1.000 x 109
</table>
<tableCaption confidence="0.998576">
Table 4
</tableCaption>
<subsectionHeader confidence="0.740665">
Number of class n-grams with various frequencies in 365,893,263 words of running text.
</subsectionHeader>
<bodyText confidence="0.995347555555556">
this is reasonable for pairs like John and George or liberal and conservative but perhaps
less so for pairs like little and prima or Minister and mover.
We used these classes to construct an interpolated 3-gram class model using the
same training text and held-out data as we used for the word-based language model
we discussed above. We measured the perplexity of the Brown corpus with respect to
this model and found it to be 271. We then interpolated the class-based estimators with
the word-based estimators and found the perplexity of the test data to be 236, which
is a small improvement over the perplexity of 244 we obtained with the word-based
model.
</bodyText>
<page confidence="0.996871">
476
</page>
<note confidence="0.738491">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
</note>
<sectionHeader confidence="0.302041" genericHeader="method">
4. Sticky Pairs and Semantic Classes
</sectionHeader>
<bodyText confidence="0.995092">
In the previous section, we discussed some methods for grouping words together
according to the statistical similarity of their surroundings. Here, we discuss two ad-
ditional types of relations between words that can be discovered by examining various
co-occurrence statistics.
The mutual information of the pair w1 and w2 as adjacent words is
</bodyText>
<equation confidence="0.7137905">
Pr (wi w2) (18)
log Pr (wi) Pr (w2).
</equation>
<bodyText confidence="0.999762153846154">
If w2 follows wi less often than we would expect on the basis of their independent
frequencies, then the mutual information is negative. If w2 follows wi more often than
we would expect, then the mutual information is positive. We say that the pair w1 w2 is
sticky if the mutual information for the pair is substantially greater than 0. In Table 5,
we list the 20 stickiest pairs of words found in a 59,537,595-word sample of text from
the Canadian parliament. The mutual information for each pair is given in bits, which
corresponds to using 2 as the base of the logarithm in equation 18. Most of the pairs
are proper names such as Pontius Pilate or foreign phrases that have been adopted into
English such as mutatis mutandis and avant garde. The mutual information for Hum pty
Dumpty, 22.5 bits, means that the pair occurs roughly 6,000,000 times more than one
would expect from the individual frequencies of Hum pty and Dumpty. Notice that the
property of being a sticky pair is not symmetric and so, while Hum pty Dumpty forms
a sticky pair, Dumpty Hum pty does not.
</bodyText>
<table confidence="0.999949961538462">
Word pair Mutual Information
Humpty Dumpty 22.5
Klux Klan 22.2
Ku Klux 22.2
Chah Nulth 22.2
Lao Bao 22.2
Nuu Chah 22.1
Tse Tung 22.1
avant garde 22.1
Carena Bancorp 22.0
gizzard shad 22.0
Bobby Orr 22.0
Warnock Hersey 22.0
mutatis mutandis 21.9
Taj Mahal 21.8
Pontius Pilate 21.7
ammonium nitrate 21.7
jiggery pokery 21.6
Pitney Bowes 21.6
Lubor Zink 21.5
anciens combattants 21.5
Abu Dhabi 21.4
Aldo Moro 21.4
fuddle duddle 21.4
helter skelter 21.4
mumbo jumbo 21.4
</table>
<tableCaption confidence="0.9449205">
Table 5
Sticky word pairs.
</tableCaption>
<page confidence="0.977572">
477
</page>
<note confidence="0.530422">
Computational Linguistics Volume 18, Number 4
</note>
<tableCaption confidence="0.687847684210526">
we our us ourselves ours
question questions asking answer answers answering
performance performed perform performs performing
tie jacket suit
write writes writing written wrote pen
morning noon evening night nights midnight bed
attorney counsel trial court judge
problems problem solution solve analyzed solved solving
letter addressed enclosed letters correspondence
large size small larger smaller
operations operations operating operate operated
school classroom teaching grade math
street block avenue corner blocks
table tables dining chairs plate
published publication author publish writer titled
wall ceiling walls enclosure roof
sell buy selling buying sold
Table 6
Semantic clusters.
</tableCaption>
<bodyText confidence="0.999931642857143">
Instead of seeking pairs of words that occur next to one another more than we
would expect, we can seek pairs of words that simply occur near one another more
than we would expect. We avoid finding sticky pairs again by not considering pairs of
words that occur too close to one another. To be precise, let Prnear (w1 w2) be the proba-
bility that a word chosen at random from the text is w1 and that a second word, chosen
at random from a window of 1,001 words centered on wi but excluding the words in
a window of 5 centered on w1, is w2. We say that w1 and w2 are semantically sticky if
Prnear (W1W2) is much larger than Pr (w1) Pr (w2) . Unlike stickiness, semantic stickiness
is symmetric so that if w1 sticks semantically to w2, then w2 sticks semantically to w1.
In Table 6, we show some interesting classes that we constructed, using Prnear (w1 w2),
in a manner similar to that described in the preceding section. Some classes group to-
gether words having the same morphological stem, such as performance, performed,
perform, performs, and performing. Other classes contain words that are semantically
related but have different stems, such as attorney, counsel, trial, court, and judge.
</bodyText>
<sectionHeader confidence="0.964429" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.9999743">
We have described several methods here that we feel clearly demonstrate the value of
simple statistical techniques as allies in the struggle to tease from words their linguistic
secrets. However, we have not as yet demonstrated the full value of the secrets thus
gleaned. At the expense of a slightly greater perplexity, the 3-gram model with word
classes requires only about one-third as much storage as the 3-gram language model
in which each word is treated as a unique individual (see Tables 1 and 4). Even when
we combine the two models, we are not able to achieve much improvement in the
perplexity. Nonetheless, we are confident that we will eventually be able to make
significant improvements to 3-gram language models with the help of classes of the
kind that we have described here.
</bodyText>
<page confidence="0.994868">
478
</page>
<note confidence="0.472078">
Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language
</note>
<sectionHeader confidence="0.807636" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.984998666666667">
The authors would like to thank John
Lafferty for his assistance in constructing
word classes described in this paper.
</bodyText>
<sectionHeader confidence="0.993446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999684321428572">
Averbuch, A.; Bahl, L.; Bakis, R.; Brown, P.;
Cole, A.; Daggett, G.; Das, S.; Davies, K.;
Gennaro, S. De.; de Souza, P.; Epstein, E.;
Fraleigh, D.; Jelinek, F.; Moorhead, J.;
Lewis, B.; Mercer, R.; Nadas, A.;
Nahamoo, D.; Picheny, M.; Shichman, G.;
Spinelli, P.; Van Compernolle, D.; and
Wilkens, H. (1987). &amp;quot;Experiments with the
Tangora 20,000 word speech recognizer.&amp;quot;
In Proceedings, IEEE International Conference
on Acoustics, Speech and Signal Processing.
Dallas, Texas, 701-704.
Bahl, L. R.; Jelinek, F.; and Mercer, R. L.
(1983). &amp;quot;A maximum likelihood approach
to continuous speech recognition.&amp;quot; IEEE
Transactions on Pattern Analysis and Machine
Intelligence, PAMI-5(2), 179-190.
Baum, L. (1972). &amp;quot;An inequality and
associated maximization technique in
statistical estimation of probabilistic
functions of a Markov process.&amp;quot;
Inequalities, 3,1-8.
Brown, P. F.; Cocke, J.; DellaPietra, S. A.;
DellaPietra, V. J.; Jelinek, F.; Lafferty, J. D.;
Mercer, R. L.; and Roossin, P. S. (1990). &amp;quot;A
statistical approach to machine
translation.&amp;quot; Computational Linguistics,
16(2), 79-85.
Dempster, A.; Laird, N.; and Rubin, D.
(1977). &amp;quot;Maximum likelihood from
incomplete data via the EM algorithm.&amp;quot;
Journal of the Royal Statistical Society, 39(B),
1-38.
Feller, W. (1950). An Introduction to
Probability Theory and its Applications,
Volume I. John Wiley &amp; Sons, Inc.
Gallagher, R. G. (1968). Information Theory
and Reliable Communication. John Wiley &amp;
Sons, Inc.
Good, I. (1953). &amp;quot;The population frequencies
of species and the estimation of
population parameters.&amp;quot; Biometrika,
40(3-4), 237-264.
Jelinek, F., and Mercer, R. L. (1980).
&amp;quot;Interpolated estimation of Markov
source parameters from sparse data.&amp;quot; In
Proceedings, Workshop on Pattern Recognition
in Practice, Amsterdam, The Netherlands,
381-397.
Kue&apos;era, H., and Francis, W. (1967).
Computational Analysis of Present Day
American English. Brown University Press.
Mays, E.; Damerau, E J.; and Mercer, R. L.
(1990). &amp;quot;Context-based spelling
correction.&amp;quot; In Proceedings, IBM Natural
Language ITL. Paris, France, 517-522.
</reference>
<page confidence="0.99916">
479
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833094">
<title confidence="0.943864">Class-Based n-gram Models of Natural Language</title>
<author confidence="0.996975333333333">Peter F Brown Vincent J Della Pietra Peter V deSouza Jenifer C Lai Robert L Mercer</author>
<affiliation confidence="0.990965">IBM T. J. Watson Research Center</affiliation>
<abstract confidence="0.9896464">We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>A Averbuch</author>
<author>L Bahl</author>
<author>R Bakis</author>
<author>P Brown</author>
<author>A Cole</author>
<author>G Daggett</author>
<author>S Das</author>
<author>K Davies</author>
<author>S De Gennaro</author>
<author>P de Souza</author>
<author>E Epstein</author>
<author>D Fraleigh</author>
<author>F Jelinek</author>
<author>J Moorhead</author>
<author>B Lewis</author>
<author>R Mercer</author>
<author>A Nadas</author>
<author>D Nahamoo</author>
<author>M Picheny</author>
<author>G Shichman</author>
<author>P Spinelli</author>
<author>D Van Compernolle</author>
<author>H Wilkens</author>
</authors>
<title>Experiments with the Tangora 20,000 word speech recognizer.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<pages>701--704</pages>
<location>Dallas, Texas,</location>
<marker>Averbuch, Bahl, Bakis, Brown, Cole, Daggett, Das, Davies, Gennaro, de Souza, Epstein, Fraleigh, Jelinek, Moorhead, Lewis, Mercer, Nadas, Nahamoo, Picheny, Shichman, Spinelli, Van Compernolle, Wilkens, 1987</marker>
<rawString>Averbuch, A.; Bahl, L.; Bakis, R.; Brown, P.; Cole, A.; Daggett, G.; Das, S.; Davies, K.; Gennaro, S. De.; de Souza, P.; Epstein, E.; Fraleigh, D.; Jelinek, F.; Moorhead, J.; Lewis, B.; Mercer, R.; Nadas, A.; Nahamoo, D.; Picheny, M.; Shichman, G.; Spinelli, P.; Van Compernolle, D.; and Wilkens, H. (1987). &amp;quot;Experiments with the Tangora 20,000 word speech recognizer.&amp;quot; In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing. Dallas, Texas, 701-704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.&amp;quot;</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>179--190</pages>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, L. R.; Jelinek, F.; and Mercer, R. L. (1983). &amp;quot;A maximum likelihood approach to continuous speech recognition.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2), 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.&amp;quot;</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="11112" citStr="Baum 1972" startWordPosition="1950" endWordPosition="1951">on that combines the estimates of several language models so as to use the estimates of the more accurate models where they are reliable and, where they are unreliable, to fall back on the more reliable estimates of less accurate models. If Pri (w I &amp;I.-1) is the conditional probability as determined by the jth language model, then the interpolated estimate, Pr(wi I w&apos;i-1), is given by Pr (wi I = E (w11-1)Pr(l) (wz (3) Given values for Pr(i) 0, the A1(w) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data (Baum 1972; Dempster, Laird, and Rubin 1977; Jelinek and Mercer 1980). When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the As to depend on the history, w1, only through the count of the 2- gram, We expect that where the count of the 2-gram is high, the 3-gram estimates will be reliable, and, where the count is low, the estimates will be unreliable. We have constructed an interpolated 3-gram model in which we have divided the As into 1,782 different sets according to the 2-gram counts. We estimated these As from a held-out sample of 4,630,934 words. </context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L. (1972). &amp;quot;An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.&amp;quot; Inequalities, 3,1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A DellaPietra</author>
<author>V J DellaPietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>79--85</pages>
<contexts>
<context position="2982" citStr="Brown et al. 1990" startWordPosition="497" endWordPosition="500">f Feller (1950) and Gallagher (1968). In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. * IBM T. J. Watson Research Center, Yorktown Heights, New York 10598. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 4 Pr (W) Pr (Y I W) = Pr (W, Y) Figure 1 Source-channel setup. 2. Language Models Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 1990) and for automatic spelling correction (Mays, Demerau, and Mercer 1990). In automatic speech recognition, y is an acoustic signal; in machine translation, y is a sequence of words in another language; and in spelling correction, y is a sequence of characters produced by a possibly imperfect typist. In all three applications, given a signal y, we seek to determine the string of English words, w, which gave rise to it. In general, many different word strings can give rise to the same signal and so we cannot hope to recover w successfully in all cases. We can, however, minimize our probability of</context>
</contexts>
<marker>Brown, Cocke, DellaPietra, DellaPietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, P. F.; Cocke, J.; DellaPietra, S. A.; DellaPietra, V. J.; Jelinek, F.; Lafferty, J. D.; Mercer, R. L.; and Roossin, P. S. (1990). &amp;quot;A statistical approach to machine translation.&amp;quot; Computational Linguistics, 16(2), 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.&amp;quot;</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.; Laird, N.; and Rubin, D. (1977). &amp;quot;Maximum likelihood from incomplete data via the EM algorithm.&amp;quot; Journal of the Royal Statistical Society, 39(B), 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Feller</author>
</authors>
<title>An Introduction to Probability Theory and its Applications, Volume I.</title>
<date>1950</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="2379" citStr="Feller (1950)" startWordPosition="400" endWordPosition="401">l information to two other forms of word clustering. First, we use it to find pairs of words that function together as a single lexical entity. Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence. In describing our work, we draw freely on terminology and notation from the mathematical theory of communication. The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of Feller (1950) and Gallagher (1968). In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. * IBM T. J. Watson Research Center, Yorktown Heights, New York 10598. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 4 Pr (W) Pr (Y I W) = Pr (W, Y) Figure 1 Source-channel setup. 2. Language Models Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 19</context>
</contexts>
<marker>Feller, 1950</marker>
<rawString>Feller, W. (1950). An Introduction to Probability Theory and its Applications, Volume I. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R G Gallagher</author>
</authors>
<title>Information Theory and Reliable Communication.</title>
<date>1968</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="2400" citStr="Gallagher (1968)" startWordPosition="403" endWordPosition="404">wo other forms of word clustering. First, we use it to find pairs of words that function together as a single lexical entity. Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence. In describing our work, we draw freely on terminology and notation from the mathematical theory of communication. The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of Feller (1950) and Gallagher (1968). In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. * IBM T. J. Watson Research Center, Yorktown Heights, New York 10598. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 4 Pr (W) Pr (Y I W) = Pr (W, Y) Figure 1 Source-channel setup. 2. Language Models Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 1990) and for automatic</context>
</contexts>
<marker>Gallagher, 1968</marker>
<rawString>Gallagher, R. G. (1968). Information Theory and Reliable Communication. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.&amp;quot;</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<issue>3</issue>
<pages>237--264</pages>
<contexts>
<context position="9929" citStr="Good 1953" startWordPosition="1742" endWordPosition="1743">70,434 3,653,791 &gt; 3 135,335 3,413,290 8,728,789 &gt; 0 205,516 14,494,217 75,349,888 &gt; 0 260,741 6.799 x 1010 1.773 x 1016 Table 1 Number of n-grams with various frequencies in 365,893,263 words of running text. unknown word into which all other words are mapped. Of the 6.799 x 1010 2-grams that might have occurred in the data, only 14,494,217 actually did occur and of these, 8,045,024 occurred only once each. Similarly, of the 1.773 x 1016 3-grams that might have occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurred only once each. From these data and Turing&apos;s formula (Good 1953), we can expect that maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for 2.2 percent of the 2-grams in a new sample of English text. We can be confident that any 3-gram that does not appear in our sample is, in fact, rare, but there are so many of them that their aggregate probability is substantial. As n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases. Jelinek and Mercer (1980) describe a technique called interpolated estimation that combines the estim</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. (1953). &amp;quot;The population frequencies of species and the estimation of population parameters.&amp;quot; Biometrika, 40(3-4), 237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings, Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<location>Amsterdam, The</location>
<contexts>
<context position="10453" citStr="Jelinek and Mercer (1980)" startWordPosition="1833" endWordPosition="1836">ur and of these, 53,737,350 occurred only once each. From these data and Turing&apos;s formula (Good 1953), we can expect that maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for 2.2 percent of the 2-grams in a new sample of English text. We can be confident that any 3-gram that does not appear in our sample is, in fact, rare, but there are so many of them that their aggregate probability is substantial. As n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases. Jelinek and Mercer (1980) describe a technique called interpolated estimation that combines the estimates of several language models so as to use the estimates of the more accurate models where they are reliable and, where they are unreliable, to fall back on the more reliable estimates of less accurate models. If Pri (w I &amp;I.-1) is the conditional probability as determined by the jth language model, then the interpolated estimate, Pr(wi I w&apos;i-1), is given by Pr (wi I = E (w11-1)Pr(l) (wz (3) Given values for Pr(i) 0, the A1(w) are chosen, with the help of the EM algorithm, so as to maximize the probability of some ad</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, F., and Mercer, R. L. (1980). &amp;quot;Interpolated estimation of Markov source parameters from sparse data.&amp;quot; In Proceedings, Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands, 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kue&apos;era</author>
<author>W Francis</author>
</authors>
<title>Computational Analysis of Present Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<marker>Kue&apos;era, Francis, 1967</marker>
<rawString>Kue&apos;era, H., and Francis, W. (1967). Computational Analysis of Present Day American English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mays</author>
<author>E J Damerau</author>
<author>R L Mercer</author>
</authors>
<title>Context-based spelling correction.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, IBM Natural Language ITL.</booktitle>
<pages>517--522</pages>
<location>Paris,</location>
<marker>Mays, Damerau, Mercer, 1990</marker>
<rawString>Mays, E.; Damerau, E J.; and Mercer, R. L. (1990). &amp;quot;Context-based spelling correction.&amp;quot; In Proceedings, IBM Natural Language ITL. Paris, France, 517-522.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>