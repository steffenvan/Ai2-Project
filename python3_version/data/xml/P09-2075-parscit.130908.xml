<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000868">
<title confidence="0.981856">
Multi-Document Summarization using Sentence-based Topic Models
</title>
<author confidence="0.970749">
Dingding Wang 1 Shenghuo Zhu 2 Tao Li 1 Yihong Gong 2
</author>
<affiliation confidence="0.940153">
1. School of Computer Science, Florida International University, Miami, FL, 33199
</affiliation>
<address confidence="0.500212">
2. NEC Laboratories America, Cupertino, CA 95014, USA.
</address>
<email confidence="0.993014">
{dwang003,taoli}@cs.fiu.edu {zsh,ygong}@sv.nec-labs.com
</email>
<sectionHeader confidence="0.99371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816789473684">
Most of the existing multi-document
summarization methods decompose the
documents into sentences and work
directly in the sentence space using a
term-sentence matrix. However, the
knowledge on the document side, i.e. the
topics embedded in the documents, can
help the context understanding and guide
the sentence selection in the summariza-
tion procedure. In this paper, we propose a
new Bayesian sentence-based topic model
for summarization by making use of both
the term-document and term-sentence
associations. An efficient variational
Bayesian algorithm is derived for model
parameter estimation. Experimental
results on benchmark data sets show the
effectiveness of the proposed model for
the multi-document summarization task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962235294118">
With the continuing growth of online text
resources, document summarization has found
wide-ranging applications in information retrieval
and web search. Many multi-document summa-
rization methods have been developed to extract
the most important sentences from the documents.
These methods usually represent the documents
as term-sentence matrices (where each row rep-
resents a sentence and each column represents a
term) or graphs (where each node is a sentence
and each edge represents the pairwise relationship
among corresponding sentences), and ranks the
sentences according to their scores calculated by a
set of predefined features, such as term frequency-
inverse sentence frequency (TF-ISF) (Radev et al.,
2004; Lin and Hovy, 2002), sentence or term
position (Yih et al., 2007), and number of key-
words (Yih et al., 2007). Typical existing summa-
rization methods include centroid-based methods
(e.g., MEAD (Radev et al., 2004)), graph-ranking
based methods (e.g., LexPageRank (Erkan and
Radev, 2004)), non-negative matrix factorization
(NMF) based methods (e.g., (Lee and Seung,
2001)), Conditional random field (CRF) based
summarization (Shen et al., 2007), and LSA based
methods (Gong and Liu, 2001).
There are two limitations with most of the exist-
ing multi-document summarization methods: (1)
They work directly in the sentence space and many
methods treat the sentences as independent of each
other. Although few work tries to analyze the
context or sequence information of the sentences,
the document side knowledge, i.e. the topics em-
bedded in the documents are ignored. (2) An-
other limitation is that the sentence scores calcu-
lated from existing methods usually do not have
very clear and rigorous probabilistic interpreta-
tions. Many if not all of the sentence scores
are computed using various heuristics as few re-
search efforts have been reported on using genera-
tive models for document summarization.
In this paper, to address the above issues,
we propose a new Bayesian sentence-based topic
model for multi-document summarization by mak-
ing use of both the term-document and term-
sentence associations. Our proposal explicitly
models the probability distributions of selecting
sentences given topics and provides a principled
way for the summarization task. An efficient vari-
ational Bayesian algorithm is derived for estimat-
ing model parameters.
</bodyText>
<sectionHeader confidence="0.9967" genericHeader="method">
2 Bayesian Sentence-based Topic Models
(BSTM)
</sectionHeader>
<subsectionHeader confidence="0.999366">
2.1 Model Formulation
</subsectionHeader>
<bodyText confidence="0.995557666666667">
The entire document set is denoted by D. For each
document d E D, we consider its unigram lan-
guage model,
</bodyText>
<page confidence="0.962053">
297
</page>
<note confidence="0.9312135">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297–300,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.9956955">
p(W1 n |ed) = Yn p(Wi|ed),
i=1
</equation>
<bodyText confidence="0.996816166666667">
where θd denotes the model parameter for docu-
ment d, W1n denotes the sequence of words {Wi E
W}ni�1, i.e. the content of the document. W is the
vocabulary. As topic models, we further assume
the unigram model as a mixture of several topic
unigram models,
</bodyText>
<equation confidence="0.995871">
p(Wi|ed) = X p(Wi|Ti)p(Ti|ed),
Ti∈T
</equation>
<bodyText confidence="0.999582">
where T is the set of topics. Here, we assume
that given a topic, generating words is independent
from the document, i.e.
</bodyText>
<equation confidence="0.85062">
p(Wi|Ti, ed) = p(Wi|Ti).
</equation>
<bodyText confidence="0.99601325">
Instead of freely choosing topic unigram mod-
els, we further assume that topic unigram models
are mixtures of some existing base unigram mod-
els, i.e.
</bodyText>
<equation confidence="0.99804">
p(Wi|Ti) = X p(Wi|Si = s)p(Si = s|Ti),
s∈S
</equation>
<bodyText confidence="0.9997438">
where S is the set of base unigram models. Here,
we use sentence language models as the base mod-
els. One benefit of this assumption is that each
topic is represented by meaningful sentences, in-
stead of directly by keywords. Thus we have
</bodyText>
<equation confidence="0.9988995">
p(Wi|ed) = X X p(Wi|Si = s)p(Si = s|Ti = t)p(Ti = t|ed).
t∈T s∈S
</equation>
<bodyText confidence="0.999990777777778">
Here we use parameter Ust for the probability
of choosing base model s given topic t, p(Si =
s|Ti = t) = Ust, where Es Ust = 1. We use
parameters {θd} for the probability of choosing
topic t given document d, where Et Odt = 1.
We assume that the parameters of base models,
{Bws}, are given, i.e. p(Wi = w|Si = s) = Bws,
where Ew Bws = 1. Usually, we obtain Bws by
empirical distribution words of sentence s.
</bodyText>
<subsectionHeader confidence="0.998084">
2.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9999757">
For summarization task, we concern how to de-
scribe each topic with the given sentences. This
can be answered by the parameter of choosing
base model s given topic t, Ust. Comparing to
parameter Ust, we concern less about the topic
distribution of each document, i.e. Odt. Thus
we choose Bayesian framework to estimate Ust by
marginalizing Odt. To do so, we assume a Dirich-
let prior for Od· — Dir(α), where vector α is a
hyperparameter. Thus the likelihood is
</bodyText>
<equation confidence="0.953185285714286">
Y Z Y
f(U; Y) = p(Yid|ed)π(ed|α)ded
d i
Z Y (1)
Y
= B(α)−D [BUΘ&gt;]Yid Θαk−1
id × dk d Θ.
</equation>
<bodyText confidence="0.871964166666667">
id dk
As Eq. (1) is intractable, LDA (Blei et al., 2001)
applies variational Bayesian, which is to maximize
a variational bound of the integrated likelihood.
Here we write the variational bound.
Definition 1 The variational bound is
</bodyText>
<equation confidence="0.9264474">
B(α + yd,·)µ Bwv U d�
B(α) Y
vkwd φvk;wd J
vk 1 Ywvk;wd
(2)
</equation>
<bodyText confidence="0.988998">
where the domain of V isV = {V ∈ RD×K : Pk Vdk =
</bodyText>
<equation confidence="0.773353">
+
1}, φvk;wd = BwvUvkVdk/[BUV&gt;]wd, γdk = Pwv Ywdφvk;wd.
</equation>
<bodyText confidence="0.999623833333333">
We have the following proposition.
Proposition 1 f(U; Y) &gt; supVEV �f(U, V; Y).
Actually the optimum of this variational bound is
the same as that obtained variational Bayesian ap-
proach. Due to the space limit, the proof of the
proposition is omitted.
</bodyText>
<sectionHeader confidence="0.983178" genericHeader="method">
3 The Iterative Algorithm
</sectionHeader>
<bodyText confidence="0.999956888888889">
The LDA algorithm (Blei et al., 2001) em-
ployed the variational Bayesian paradigm, which
estimates the optimal variation bound for each U.
The algorithm requires an internal Expectation-
Maximization (EM) procedure to find the optimal
variational bound. The nested EM slows down
the optimization procedure. To avoid the internal
EM loop, we can directly optimize the variational
bound to obtain the update rules.
</bodyText>
<subsectionHeader confidence="0.995683">
3.1 Algorithm Derivation
</subsectionHeader>
<bodyText confidence="0.86294925">
First, we define the concept of Dirichlet adjust-
ment, which is used in the algorithm for vari-
ational update rules involving Dirichlet distribu-
tion. Then, we define some notations for the up-
date rules.
Definition 2 We call vector y of size K is the
Dirichlet adjustment of vector x of size K with re-
spect to Dirichlet distribution DK(α) if
</bodyText>
<equation confidence="0.885589">
X (αl + xl))),
yk = exp(Ψ(αk + xk) − Ψ(
l
where IF(·) is digamma function. We denote it by
y = PD(x; α).
</equation>
<bodyText confidence="0.92143">
We denote element-wise product of matrix X and
matrix Y by X o Y, element-wise division by
�, obtaining Y via normalizing of each column
�
of X as Y + X, and obtaining Y via Dirich-
let adjustment PD(·; α) and normalization of each
row of X as PD(·;α),2
</bodyText>
<equation confidence="0.896852">
←− , i.e., z = PD((Xd,·)&gt;; α) and
Yd,k = zk/ Pk zk. The following is the update rules
for LDA:
Y
ef(U, V; Y) =
d
· Y ¸
U ← 1 B&gt; Ve ◦Ue (3)
BUe eV&gt;
V PD(·;α ),2 r Y 1 &gt; BU o V 4
L J ( ) O
BUVT
</equation>
<page confidence="0.966446">
298
</page>
<table confidence="0.784553916666667">
Algorithm 1 Iterative Algorithm
Input: Y : term-document matrix
B : term-sentence matrix
K : the number of latent topics
Output: U : sentence-topic matrix
V : auxiliary document-topic matrix
1: Randomly initialize U and V, and normalize them
2: repeat
3: Update U using Eq. (3);
4: Update V_using Eq. (4);
5: Compute f using Eq. (2);
6: until f_ converges.
</table>
<subsectionHeader confidence="0.997894">
3.2 Algorithm Procedure
</subsectionHeader>
<bodyText confidence="0.99990675">
The detail procedure is listed as Algorithm 1.
¿From the sentence-topic matrix U, we include
the sentence with the highest probability in each
topic into the summary.
</bodyText>
<sectionHeader confidence="0.990386" genericHeader="method">
4 Relations with Other Models
</sectionHeader>
<bodyText confidence="0.999853222222222">
In this section, we discuss the connections and
differences of our BSTM model with two related
models.
Recently, a new language model, factorization
with sentence bases (FGB) (Wang et al., 2008) is
proposed for document clustering and summariza-
tion by making use of both term-document matrix
Y and term-sentence matrix B. The FGB model
computes two matrices U and V by optimizing
</bodyText>
<equation confidence="0.98600475">
U, V = arg min
U,V
�YIIBUV��
`(U, V) = KL - ln Pr(U, V).
</equation>
<bodyText confidence="0.999981117647059">
Here, Kullback-Leibler divergence is used to mea-
sure the difference between the distributions of Y
and the estimated BUVT. Our BSTM is similar
to the FGB summarization since they are all based
on sentence-based topic model. The difference is
that the document-topic allocation V is marginal-
ized out in BSTM. The marginalization increases
the stability of the estimation of the sentence-topic
parameters. Actually, from the algorithm we can
see that the difference lies in the Dirichlet adjust-
ment. Experimental results show that our BSTM
achieves better summarization results than FGB
model.
Our BSTM model is also related to 3-
factor non-negative matrix factorization (NMF)
model (Ding et al., 2006) where the problem is to
solve U and V by minimizing T 2
</bodyText>
<equation confidence="0.999539">
`F (U,
V) = IIY - BUV IIF. (5)
</equation>
<bodyText confidence="0.999927416666667">
Both BSTM and NMF models are used for solv-
ing U and V and have similar multiplicative up-
date rules. Note that if the matrix B is the identity
matrix, Eq. (5) leads to the derivation of the NMF
algorithm with Frobenius norm in (Lee and Seung,
2001). However, our BSTM model is a generative
probabilistic model and makes use of Dirichlet ad-
justment. The results obtained in our model have
clear and rigorous probabilistic interpretations that
the NMF model lacks. In addition, by marginaliz-
ing out V, our BSTM model leads to better sum-
marization results.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="conclusions">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999309">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999968333333333">
To evaluate the summarization results empirically,
we use the DUC2002 and DUC2004 data sets,
both of which are open benchmark data sets from
Document Understanding Conference (DUC) for
generic automatic summarization evaluation. Ta-
ble 1 gives a brief description of the data sets.
</bodyText>
<table confidence="0.995102857142857">
DUC2002 DUC2004
number of 59 50
document collections
number of documents -10 10
in each collection
data source TREC TDT
summary length 200 words 665bytes
</table>
<tableCaption confidence="0.880818">
Table 1: Description of the data sets for multi-document
summarization
</tableCaption>
<table confidence="0.999974625">
Random 0.38475 0.11692 0.37218 0.18057
Centroid 0.45379 0.19181 0.43237 0.23629
LexPageRank 0.47963 0.22949 0.44332 0.26198
LSA 0.43078 0.15022 0.40507 0.20226
NMF 0.44587 0.16280 0.41513 0.21687
KM 0.43156 0.15135 0.40376 0.20144
FGB 0.48507 0.24103 0.45080 0.26860
BSTM 0.48812 0.24571 0.45516 0.27018
</table>
<tableCaption confidence="0.826114">
Table 2: Overall performance comparison on DUC2002
data using ROUGE evaluation methods.
</tableCaption>
<table confidence="0.9984278">
Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU
DUC Best 0.38224 0.09216 0.38687 0.13233
Random 0.31865 0.06377 0.34521 0.11779
Centroid 0.36728 0.07379 0.36182 0.12511
LexPageRank 0.37842 0.08572 0.37531 0.13097
LSA 0.34145 0.06538 0.34973 0.11946
NMF 0.36747 0.07261 0.36749 0.12918
KM 0.34872 0.06937 0.35882 0.12115
FGB 0.38724 0.08115 0.38423 0.12957
BSTM 0.39065 0.09010 0.38799 0.13218
</table>
<tableCaption confidence="0.9445045">
Table 3: Overall performance comparison on DUC2004 data using
ROUGE evaluation methods.
</tableCaption>
<subsectionHeader confidence="0.998768">
5.2 Implemented Systems
</subsectionHeader>
<bodyText confidence="0.9980258">
We implement the following most widely used
document summarization methods as the base-
line systems to compare with our proposed BSTM
method. (1) Random: The method selects sen-
tences randomly for each document collection.
</bodyText>
<figure confidence="0.990136666666667">
Systems
ROUGE-1
ROUGE-2
ROUGE-L
ROUGE-SU
DUC Best
0.49869
0.25229
0.46803
0.28406
`(U, V),
where
</figure>
<page confidence="0.992394">
299
</page>
<bodyText confidence="0.985370190476191">
(2) Centroid: The method applies MEAD algo-
rithm (Radev et al., 2004) to extract sentences ac-
cording to the following three parameters: cen-
troid value, positional value, and first-sentence
overlap. (3) LexPageRank: The method first con-
structs a sentence connectivity graph based on
cosine similarity and then selects important sen-
tences based on the concept of eigenvector cen-
trality (Erkan and Radev, 2004). (4) LSA: The
method performs latent semantic analysis on terms
by sentences matrix to select sentences having
the greatest combined weights across all impor-
tant topics (Gong and Liu, 2001). (5) NMF: The
method performs non-negative matrix factoriza-
tion (NMF) on terms by sentences matrix and then
ranks the sentences by their weighted scores (Lee
and Seung, 2001). (6) KM: The method performs
K-means algorithm on terms by sentences matrix
to cluster the sentences and then chooses the cen-
troids for each sentence cluster. (7) FGB: The
FGB method is proposed in (Wang et al., 2008).
</bodyText>
<subsectionHeader confidence="0.977354">
5.3 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999986555555556">
We use ROUGE toolkit (version 1.5.5) to measure
the summarization performance, which is widely
applied by DUC for performance evaluation. It
measures the quality of a summary by counting the
unit overlaps between the candidate summary and
a set of reference summaries. The full explanation
of the evaluation toolkit can be found in (Lin and
E.Hovy, 2003). In general, the higher the ROUGE
scores, the better summarization performance.
</bodyText>
<subsectionHeader confidence="0.957039">
5.4 Result Analysis
</subsectionHeader>
<bodyText confidence="0.999782782608696">
Table 2 and Table 3 show the comparison results
between BSTM and other implemented systems.
From the results, we have the follow observa-
tions: (1) Random has the worst performance.
The results of LSA, KM, and NMF are similar
and they are slightly better than those of Random.
Note that LSA and NMF provide continuous so-
lutions to the same K-means clustering problem
while LSA relaxes the non-negativity of the clus-
ter indicator of K-means and NMF relaxes the
orthogonality of the cluster indicator (Ding and
He, 2004; Ding et al., 2005). Hence all these
three summarization methods perform clustering-
based summarization: they first generate sentence
clusters and then select representative sentences
from each sentence cluster. (2) The Centroid sys-
tem outperforms clustering-based summarization
methods in most cases. This is mainly because
the Centroid based algorithm takes into account
positional value and first-sentence overlap which
are not used in clustering-based summarization.
(3) LexPageRank outperforms Centroid. This is
due to the fact that LexPageRank ranks the sen-
tence using eigenvector centrality which implic-
itly accounts for information subsumption among
all sentences (Erkan and Radev, 2004). (4) FGB
performs better than LexPageRank. Note that
FGB model makes use of both term-document and
term-sentence matrices. Our BSTM model outper-
forms FGB since the document-topic allocation is
marginalized out in BSTM and the marginaliza-
tion increases the stability of the estimation of the
sentence-topic parameters. (5) Our BSTM method
outperforms all other implemented systems and its
performance is close to the results of the best team
in the DUC competition. Note that the good per-
formance of the best team in DUC benefits from
their preprocessing on the data using deep natural
language analysis which is not applied in our im-
plemented systems.
The experimental results provide strong evi-
dence that our BSTM is a viable method for docu-
ment summarization.
Acknowledgement: The work is partially
supported by NSF grants IIS-0546280, DMS-
0844513 and CCF-0830659.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999373264705882">
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. In Advances
in Neural Information Processing Systems 14.
C. Ding and X. He. K-means clustering and principal component analysis. In
Prodeedings ofICML 2004.
Chris Ding, Xiaofeng He, and Horst Simon. 2005. On the equivalence of
nonnegative matrix factorization and spectral clustering. In Proceedings of
Siam Data Mining.
Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonneg-
ative matrix tri-factorizations for clustering. In Proceedings of SIGKDD
2006.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in multi-document text
summarization. In Proceedings of EMNLP 2004.
Y. Gong and X. Liu. 2001. Generic text summarization using relevance mea-
sure and latent semantic analysis. In Proceedings of SIGIR.
Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix
factorization. In Advances in Neural Information Processing Systems 13.
C-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings ofNLT-NAACL 2003.
C-Y. Lin and E. Hovy. 2002. From single to multi-document summarization:
A prototype system and its evaluation. In Proceedings ofACL 2002.
I. Mani. 2001. Automatic summarization. John Benjamins Publishing Com-
pany.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and Management, pages
919–938.
B. Ricardo and R. Berthier. 1999. Modern information retrieval. ACM Press.
D. Shen, J-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document summariza-
tion using conditional random fields. In Proceedings of IJCAI2007.
Dingding Wang, Shenghuo Zhu, Tao Li, Yun Chi, and Yihong Gong. 2008.
Integrating clustering and multi-document summarization to improve doc-
ument understanding. In Proceedings of CIKM 2008.
W-T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-
document summarization by maximizing informative content-words. In
Proceedings ofIJCAI2007.
</reference>
<page confidence="0.997974">
300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733423">
<title confidence="0.99905">Multi-Document Summarization using Sentence-based Topic Models</title>
<author confidence="0.80304">Wang Shenghuo Zhu Tao Li Yihong Gong</author>
<address confidence="0.9405545">1. School of Computer Science, Florida International University, Miami, FL, 33199 2. NEC Laboratories America, Cupertino, CA 95014, USA.</address>
<abstract confidence="0.99973015">Most of the existing multi-document summarization methods decompose the documents into sentences and work directly in the sentence space using a term-sentence matrix. However, the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. In this paper, we propose a new Bayesian sentence-based topic model for summarization by making use of both the term-document and term-sentence associations. An efficient variational Bayesian algorithm is derived for model parameter estimation. Experimental results on benchmark data sets show the effectiveness of the proposed model for the multi-document summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<booktitle>In Advances in Neural Information Processing Systems 14.</booktitle>
<marker>Blei, Ng, Jordan, </marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. In Advances in Neural Information Processing Systems 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ding</author>
<author>X He</author>
</authors>
<title>K-means clustering and principal component analysis.</title>
<date>2004</date>
<booktitle>In Prodeedings ofICML</booktitle>
<contexts>
<context position="13775" citStr="Ding and He, 2004" startWordPosition="2268" endWordPosition="2271">. In general, the higher the ROUGE scores, the better summarization performance. 5.4 Result Analysis Table 2 and Table 3 show the comparison results between BSTM and other implemented systems. From the results, we have the follow observations: (1) Random has the worst performance. The results of LSA, KM, and NMF are similar and they are slightly better than those of Random. Note that LSA and NMF provide continuous solutions to the same K-means clustering problem while LSA relaxes the non-negativity of the cluster indicator of K-means and NMF relaxes the orthogonality of the cluster indicator (Ding and He, 2004; Ding et al., 2005). Hence all these three summarization methods perform clusteringbased summarization: they first generate sentence clusters and then select representative sentences from each sentence cluster. (2) The Centroid system outperforms clustering-based summarization methods in most cases. This is mainly because the Centroid based algorithm takes into account positional value and first-sentence overlap which are not used in clustering-based summarization. (3) LexPageRank outperforms Centroid. This is due to the fact that LexPageRank ranks the sentence using eigenvector centrality wh</context>
</contexts>
<marker>Ding, He, 2004</marker>
<rawString>C. Ding and X. He. K-means clustering and principal component analysis. In Prodeedings ofICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Xiaofeng He</author>
<author>Horst Simon</author>
</authors>
<title>On the equivalence of nonnegative matrix factorization and spectral clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of Siam Data Mining.</booktitle>
<contexts>
<context position="13795" citStr="Ding et al., 2005" startWordPosition="2272" endWordPosition="2275">igher the ROUGE scores, the better summarization performance. 5.4 Result Analysis Table 2 and Table 3 show the comparison results between BSTM and other implemented systems. From the results, we have the follow observations: (1) Random has the worst performance. The results of LSA, KM, and NMF are similar and they are slightly better than those of Random. Note that LSA and NMF provide continuous solutions to the same K-means clustering problem while LSA relaxes the non-negativity of the cluster indicator of K-means and NMF relaxes the orthogonality of the cluster indicator (Ding and He, 2004; Ding et al., 2005). Hence all these three summarization methods perform clusteringbased summarization: they first generate sentence clusters and then select representative sentences from each sentence cluster. (2) The Centroid system outperforms clustering-based summarization methods in most cases. This is mainly because the Centroid based algorithm takes into account positional value and first-sentence overlap which are not used in clustering-based summarization. (3) LexPageRank outperforms Centroid. This is due to the fact that LexPageRank ranks the sentence using eigenvector centrality which implicitly accou</context>
</contexts>
<marker>Ding, He, Simon, 2005</marker>
<rawString>Chris Ding, Xiaofeng He, and Horst Simon. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In Proceedings of Siam Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
<author>Haesun Park</author>
</authors>
<title>Orthogonal nonnegative matrix tri-factorizations for clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGKDD</booktitle>
<contexts>
<context position="9394" citStr="Ding et al., 2006" startWordPosition="1577" endWordPosition="1580">tributions of Y and the estimated BUVT. Our BSTM is similar to the FGB summarization since they are all based on sentence-based topic model. The difference is that the document-topic allocation V is marginalized out in BSTM. The marginalization increases the stability of the estimation of the sentence-topic parameters. Actually, from the algorithm we can see that the difference lies in the Dirichlet adjustment. Experimental results show that our BSTM achieves better summarization results than FGB model. Our BSTM model is also related to 3- factor non-negative matrix factorization (NMF) model (Ding et al., 2006) where the problem is to solve U and V by minimizing T 2 `F (U, V) = IIY - BUV IIF. (5) Both BSTM and NMF models are used for solving U and V and have similar multiplicative update rules. Note that if the matrix B is the identity matrix, Eq. (5) leads to the derivation of the NMF algorithm with Frobenius norm in (Lee and Seung, 2001). However, our BSTM model is a generative probabilistic model and makes use of Dirichlet adjustment. The results obtained in our model have clear and rigorous probabilistic interpretations that the NMF model lacks. In addition, by marginalizing out V, our BSTM mode</context>
</contexts>
<marker>Ding, Li, Peng, Park, 2006</marker>
<rawString>Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative matrix tri-factorizations for clustering. In Proceedings of SIGKDD 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="2068" citStr="Erkan and Radev, 2004" startWordPosition="290" endWordPosition="293">ence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). There are two limitations with most of the existing multi-document summarization methods: (1) They work directly in the sentence space and many methods treat the sentences as independent of each other. Although few work tries to analyze the context or sequence information of the sentences, the document side knowledge, i.e. the topics embedded in the documents are ignored. (2) Another limitatio</context>
<context position="12195" citStr="Erkan and Radev, 2004" startWordPosition="2010" endWordPosition="2013">are with our proposed BSTM method. (1) Random: The method selects sentences randomly for each document collection. Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU DUC Best 0.49869 0.25229 0.46803 0.28406 `(U, V), where 299 (2) Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. (3) LexPageRank: The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). (4) LSA: The method performs latent semantic analysis on terms by sentences matrix to select sentences having the greatest combined weights across all important topics (Gong and Liu, 2001). (5) NMF: The method performs non-negative matrix factorization (NMF) on terms by sentences matrix and then ranks the sentences by their weighted scores (Lee and Seung, 2001). (6) KM: The method performs K-means algorithm on terms by sentences matrix to cluster the sentences and then chooses the centroids for each sentence cluster. (7) FGB: The FGB method is proposed in (Wang et al., 2008). 5.3 Evaluation </context>
<context position="14470" citStr="Erkan and Radev, 2004" startWordPosition="2364" endWordPosition="2367"> clusteringbased summarization: they first generate sentence clusters and then select representative sentences from each sentence cluster. (2) The Centroid system outperforms clustering-based summarization methods in most cases. This is mainly because the Centroid based algorithm takes into account positional value and first-sentence overlap which are not used in clustering-based summarization. (3) LexPageRank outperforms Centroid. This is due to the fact that LexPageRank ranks the sentence using eigenvector centrality which implicitly accounts for information subsumption among all sentences (Erkan and Radev, 2004). (4) FGB performs better than LexPageRank. Note that FGB model makes use of both term-document and term-sentence matrices. Our BSTM model outperforms FGB since the document-topic allocation is marginalized out in BSTM and the marginalization increases the stability of the estimation of the sentence-topic parameters. (5) Our BSTM method outperforms all other implemented systems and its performance is close to the results of the best team in the DUC competition. Note that the good performance of the best team in DUC benefits from their preprocessing on the data using deep natural language analy</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>X Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2270" citStr="Gong and Liu, 2001" startWordPosition="319" endWordPosition="322">r scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). There are two limitations with most of the existing multi-document summarization methods: (1) They work directly in the sentence space and many methods treat the sentences as independent of each other. Although few work tries to analyze the context or sequence information of the sentences, the document side knowledge, i.e. the topics embedded in the documents are ignored. (2) Another limitation is that the sentence scores calculated from existing methods usually do not have very clear and rigorous probabilistic interpretations. Many if not all of the sentence scores are computed using variou</context>
<context position="12385" citStr="Gong and Liu, 2001" startWordPosition="2040" endWordPosition="2043">406 `(U, V), where 299 (2) Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. (3) LexPageRank: The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). (4) LSA: The method performs latent semantic analysis on terms by sentences matrix to select sentences having the greatest combined weights across all important topics (Gong and Liu, 2001). (5) NMF: The method performs non-negative matrix factorization (NMF) on terms by sentences matrix and then ranks the sentences by their weighted scores (Lee and Seung, 2001). (6) KM: The method performs K-means algorithm on terms by sentences matrix to cluster the sentences and then chooses the centroids for each sentence cluster. (7) FGB: The FGB method is proposed in (Wang et al., 2008). 5.3 Evaluation Measures We use ROUGE toolkit (version 1.5.5) to measure the summarization performance, which is widely applied by DUC for performance evaluation. It measures the quality of a summary by cou</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 13. C-Y. Lin</booktitle>
<marker>Lee, Seung, 2003</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13. C-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings ofNLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>From single to multi-document summarization: A prototype system and its evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="1802" citStr="Lin and Hovy, 2002" startWordPosition="250" endWordPosition="253">formation retrieval and web search. Many multi-document summarization methods have been developed to extract the most important sentences from the documents. These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). There are two limitations with most of the existing multi-document summarization methods: (1) They work directly in the sentence s</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>C-Y. Lin and E. Hovy. 2002. From single to multi-document summarization: A prototype system and its evaluation. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents. Information Processing and Management,</title>
<date>2004</date>
<pages>919--938</pages>
<contexts>
<context position="1781" citStr="Radev et al., 2004" startWordPosition="246" endWordPosition="249">g applications in information retrieval and web search. Many multi-document summarization methods have been developed to extract the most important sentences from the documents. These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). There are two limitations with most of the existing multi-document summarization methods: (1) They work direc</context>
<context position="11857" citStr="Radev et al., 2004" startWordPosition="1959" endWordPosition="1962"> 0.12918 KM 0.34872 0.06937 0.35882 0.12115 FGB 0.38724 0.08115 0.38423 0.12957 BSTM 0.39065 0.09010 0.38799 0.13218 Table 3: Overall performance comparison on DUC2004 data using ROUGE evaluation methods. 5.2 Implemented Systems We implement the following most widely used document summarization methods as the baseline systems to compare with our proposed BSTM method. (1) Random: The method selects sentences randomly for each document collection. Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU DUC Best 0.49869 0.25229 0.46803 0.28406 `(U, V), where 299 (2) Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. (3) LexPageRank: The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). (4) LSA: The method performs latent semantic analysis on terms by sentences matrix to select sentences having the greatest combined weights across all important topics (Gong and Liu, 2001). (5) NMF: The method performs non-negative matrix factorization (NMF) o</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ricardo</author>
<author>R Berthier</author>
</authors>
<title>Modern information retrieval.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI2007.</booktitle>
<publisher>ACM</publisher>
<marker>Ricardo, Berthier, 1999</marker>
<rawString>B. Ricardo and R. Berthier. 1999. Modern information retrieval. ACM Press. D. Shen, J-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document summarization using conditional random fields. In Proceedings of IJCAI2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yun Chi</author>
<author>Yihong Gong</author>
</authors>
<title>Integrating clustering and multi-document summarization to improve document understanding.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="8450" citStr="Wang et al., 2008" startWordPosition="1422" endWordPosition="1425"> V : auxiliary document-topic matrix 1: Randomly initialize U and V, and normalize them 2: repeat 3: Update U using Eq. (3); 4: Update V_using Eq. (4); 5: Compute f using Eq. (2); 6: until f_ converges. 3.2 Algorithm Procedure The detail procedure is listed as Algorithm 1. ¿From the sentence-topic matrix U, we include the sentence with the highest probability in each topic into the summary. 4 Relations with Other Models In this section, we discuss the connections and differences of our BSTM model with two related models. Recently, a new language model, factorization with sentence bases (FGB) (Wang et al., 2008) is proposed for document clustering and summarization by making use of both term-document matrix Y and term-sentence matrix B. The FGB model computes two matrices U and V by optimizing U, V = arg min U,V �YIIBUV�� `(U, V) = KL - ln Pr(U, V). Here, Kullback-Leibler divergence is used to measure the difference between the distributions of Y and the estimated BUVT. Our BSTM is similar to the FGB summarization since they are all based on sentence-based topic model. The difference is that the document-topic allocation V is marginalized out in BSTM. The marginalization increases the stability of th</context>
<context position="12778" citStr="Wang et al., 2008" startWordPosition="2106" endWordPosition="2109"> centrality (Erkan and Radev, 2004). (4) LSA: The method performs latent semantic analysis on terms by sentences matrix to select sentences having the greatest combined weights across all important topics (Gong and Liu, 2001). (5) NMF: The method performs non-negative matrix factorization (NMF) on terms by sentences matrix and then ranks the sentences by their weighted scores (Lee and Seung, 2001). (6) KM: The method performs K-means algorithm on terms by sentences matrix to cluster the sentences and then chooses the centroids for each sentence cluster. (7) FGB: The FGB method is proposed in (Wang et al., 2008). 5.3 Evaluation Measures We use ROUGE toolkit (version 1.5.5) to measure the summarization performance, which is widely applied by DUC for performance evaluation. It measures the quality of a summary by counting the unit overlaps between the candidate summary and a set of reference summaries. The full explanation of the evaluation toolkit can be found in (Lin and E.Hovy, 2003). In general, the higher the ROUGE scores, the better summarization performance. 5.4 Result Analysis Table 2 and Table 3 show the comparison results between BSTM and other implemented systems. From the results, we have t</context>
</contexts>
<marker>Wang, Zhu, Li, Chi, Gong, 2008</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, Yun Chi, and Yihong Gong. 2008. Integrating clustering and multi-document summarization to improve document understanding. In Proceedings of CIKM 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-T Yih</author>
<author>J Goodman</author>
<author>L Vanderwende</author>
<author>H Suzuki</author>
</authors>
<title>Multidocument summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proceedings ofIJCAI2007.</booktitle>
<contexts>
<context position="1848" citStr="Yih et al., 2007" startWordPosition="258" endWordPosition="261">ocument summarization methods have been developed to extract the most important sentences from the documents. These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequencyinverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). There are two limitations with most of the existing multi-document summarization methods: (1) They work directly in the sentence space and many methods treat the sentences as i</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>W-T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multidocument summarization by maximizing informative content-words. In Proceedings ofIJCAI2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>