<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003119">
<title confidence="0.998118">
Learning Semantic Links from a Corpus of
Parallel Temporal and Causal Relations
</title>
<author confidence="0.999431">
Steven Bethard
</author>
<affiliation confidence="0.93513525">
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
</affiliation>
<email confidence="0.997535">
steven.bethard@colorado.edu
</email>
<author confidence="0.994044">
James H. Martin
</author>
<affiliation confidence="0.9349525">
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
</affiliation>
<email confidence="0.99862">
james.martin@colorado.edu
</email>
<sectionHeader confidence="0.991595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.7122298125">
Finding temporal and causal relations is cru-
cial to understanding the semantic structure
of a text. Since existing corpora provide no
parallel temporal and causal annotations, we
annotated 1000 conjoined event pairs, achiev-
ing inter-annotator agreement of 81.2% on
temporal relations and 77.8% on causal re-
lations. We trained machine learning mod-
els using features derived from WordNet and
the Google N-gram corpus, and they out-
performed a variety of baselines, achieving
an F-measure of 49.0 for temporals and 52.4
for causals. Analysis of these models sug-
gests that additional data will improve perfor-
mance, and that temporal information is cru-
cial to causal relation identification.
</bodyText>
<sectionHeader confidence="0.998408" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996642892857143">
Working out how events are tied together temporally
and causally is a crucial component for successful
natural language understanding. Consider the text:
(1) I ate a bad tuna sandwich, got food poisoning
and had to have a shot in my shoulder. wsj 0409
To understand the semantic structure here, a system
must order events along a timeline, recognizing that
getting food poisoning occurred BEFORE having a
shot. The system must also identify when an event
is not independent of the surrounding events, e.g.
got food poisoning was CAUSED by eating a bad
sandwich. Recognizing these temporal and causal
relations is crucial for applications like question an-
swering which must face queries like How did he get
food poisoning? or What was the treatment?
Currently, no existing resource has all the neces-
sary pieces for investigating parallel temporal and
causal phenomena. The TimeBank (Pustejovsky et
al., 2003) links events with BEFORE and AFTER
relations, but includes no causal links. PropBank
(Kingsbury and Palmer, 2002) identifies ARGM-TMP
and ARGM-CAU relations, but arguments may only
be temporal or causal, never both. Thus existing
corpora are missing some crucial pieces for study-
ing temporal-causal interactions. Our research aims
to fill these gaps by building a corpus of parallel
temporal and causal relations and exploring machine
learning approaches to extracting these relations.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999735052631579">
Much recent work on temporal relations revolved
around the TimeBank and TempEval (Verhagen et
al., 2007). These works annotated temporal relations
between events and times, but low inter-annotator
agreement made many TimeBank and TempEval
tasks difficult (Boguraev and Ando, 2005; Verha-
gen et al., 2007). Still, TempEval showed that on a
constrained tense identification task, systems could
achieve accuracies in the 80s, and Bethard and col-
leagues (Bethard et al., 2007) showed that temporal
relations between a verb and a complement clause
could be identified with accuracies of nearly 90%.
Recent work on causal relations has also found
that arbitrary relations in text are difficult to annotate
and give poor system performance (Reitter, 2003).
Girju and colleagues have made progress by select-
ing constrained pairs of events using web search pat-
terns. Both manually generated Cause-Effect pat-
terns (Girju et al., 2007) and patterns based on nouns
</bodyText>
<page confidence="0.979571">
177
</page>
<reference confidence="0.2345215">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 177–180,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<table confidence="0.999151666666667">
Full Train Test
Documents 556 344 212
Event pairs 1000 697 303
BEFORE relations 313 232 81
AFTER relations 16 11 5
CAUSAL relations 271 207 64
</table>
<tableCaption confidence="0.997072">
Table 1: Contents of the corpus and its train/test sections
</tableCaption>
<table confidence="0.999148">
Task Agreement Kappa F
Temporals 81.2 0.715 71.9
Causals 77.8 0.556 66.5
</table>
<tableCaption confidence="0.999538">
Table 2: Inter-annotator agreement by task.
</tableCaption>
<bodyText confidence="0.997677">
linked causally in WordNet (Girju, 2003) were used
to collect examples for annotation, with the result-
ing corpora allowing machine learning models to
achieve performance in the 70s and 80s.
</bodyText>
<sectionHeader confidence="0.993538" genericHeader="method">
3 Conjoined Events Corpus
</sectionHeader>
<bodyText confidence="0.994738777777778">
Prior work showed that finding temporal and causal
relations is more tractable in carefully selected cor-
pora. Thus we chose a simple construction that
frequently expressed both temporal and causal rela-
tions, and accounted for 10% of all adjacent verbal
events: events conjoined by the word and.
Our temporal annotation guidelines were based
on the guidelines for TimeBank and TempEval, aug-
mented with the guidelines of (Bethard et al., 2008).
Annotators used the labels:
BEFORE The first event fully precedes the second
AFTER The second event fully precedes the first
NO-REL Neither event clearly precedes the other
Our causal annotation guidelines were based on
paraphrasing rather than the intuitive notions of
cause used in prior work (Girju, 2003; Girju et al.,
2007). Annotators selected the best paraphrase of
“and” from the following options:
CAUSAL and as a result, and as a consequence,
and enabled by that
NO-REL and independently, and for similar reasons
To build the corpus, we first identified verbs
that represented events by running the system of
(Bethard and Martin, 2006) on the TreeBank. We
then used a set of tree-walking rules to identify con-
joined event pairs. 1000 pairs were annotated by
two annotators and adjudicated by a third. Table 1
</bodyText>
<figureCaption confidence="0.9983645">
Figure 1: Syntactic tree from wsj 0450 with events took
and began highlighted.
</figureCaption>
<bodyText confidence="0.999959428571428">
and Table 2 give statistics for the resulting corpus1.
The annotators had substantial agreement on tem-
porals (81.2%) and moderate agreement on causals
(77.8%). We also report F-measure agreement, since
BEFORE, AFTER and CAUSAL relations are more in-
teresting than NO-REL. Annotators had F-measure
agreement of 71.9 on temporals and 66.5 causals.
</bodyText>
<sectionHeader confidence="0.995675" genericHeader="method">
4 Machine Learning Methods
</sectionHeader>
<bodyText confidence="0.9776645">
We used our corpus for machine learning experi-
ments where relation identification was viewed as
pair-wise classification. Consider the sentence:
(2) The man who had brought it in for an esti-
mate had [EVENT returned] to collect it and was
[EVENT waiting] in the hall. wsj 0450
A temporal classifier should label returned-waiting
with BEFORE since returned occurred first, and a
causal classifier should label it CAUSAL since this
and can be paraphrased as and as a result.
We identified both syntactic and semantic features
for our task. These will be described using the ex-
ample event pair in Figure 1. Our syntactic features
characterized surrounding surface structures:
</bodyText>
<listItem confidence="0.996017625">
• The event words, lemmas and part-of-speech tags,
e.g. took, take, VBD and began, begin, VBD.
• All words, lemmas and part-of-speech tags in the
verb phrases of each event, e.g. took, take, VBD
and began, to, trade, begin, trade, VBD,TO,VB.
• The syntactic paths from the first event to
the common ancestor to the second event, e.g.
VBD&gt;VP, VP and VP&lt;VBD.
</listItem>
<bodyText confidence="0.649527">
1Train: wsj 0416-wsj 0759. Test: wsj 0760-wsj 0971.
verbs.colorado.edu/∼bethard/treebank-verb-conj-anns.xml
</bodyText>
<figure confidence="0.999169794117647">
NP
PP
Acapulco
VB
to
trade
some of it
for cocaine
S
VP
VP CC VP
VBD NP
took DT
ADVP
RB
Then
PRP
they
NP
PP
and
VBD
NN
TO
NP
began
S
VP
the
art
to
NNP
VP
TO
</figure>
<page confidence="0.960715">
178
</page>
<listItem confidence="0.991246666666667">
• All words before, between and after the event pair,
e.g. Then, they plus the, art, to, Acapulco, and
plus to, trade, some, of, it, for, cocaine.
</listItem>
<bodyText confidence="0.7987965">
Our semantic features encoded surrounding word
meanings. We used WordNet (Fellbaum, 1998) root
synsets (roots) and lexicographer file names (lex-
names) to derive the following features:
</bodyText>
<listItem confidence="0.994931916666667">
• All event roots and lexnames, e.g. take#33,
move#1 ... body, change ... for took and be#0,
begin#1 ... change, communication ... for began.
• All lexnames before, between and after the event
pair, e.g. all plus artifact, location, etc. plus pos-
session, artifact, etc.
• All roots and lexnames shared by both events, e.g.
took and began were both act#0, be#0 and change,
communication, etc.
• The least common ancestor (LCA) senses shared
by both events, e.g. took and began meet only at
their roots, so the LCA senses are act#0 and be#0.
</listItem>
<bodyText confidence="0.960291">
We also extracted temporal and causal word associ-
ations from the Google N-gram corpus (Brants and
Franz, 2006), using &lt;keyword&gt; &lt;pronoun&gt;
&lt;word&gt; patterns, where before and after were the
keywords for temporals, and because was the key-
word for causals. Word scores were assigned as:
</bodyText>
<equation confidence="0.999457">
(Nke��ord(w) �
score(w) = log N(w)
</equation>
<bodyText confidence="0.837574625">
where Nkeyword(w) is the number of times the word
appeared in the keyword’s pattern, and N(w) is the
number of times the word was in the corpus. The
following features were derived from these scores:
• Whether the event score was in at least the Nth
percentile, e.g. took’s −6.1 because score placed
it above 84% of the scores, so the feature was true
for N = 70 and N = 80, but false for N = 90.
</bodyText>
<listItem confidence="0.903115">
• Whether the first event score was greater than the
second by at least N, e.g. took and began have
after scores of −6.3 and −6.2 so the feature was
true for N = −1, but false for N = 0 and N = 1.
</listItem>
<sectionHeader confidence="0.999754" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999394">
We trained SVMperf classifiers (Joachims, 2005) for
the temporal and causal relation tasks2 using the
</bodyText>
<footnote confidence="0.991756333333333">
2We built multi-class SVMs using the one-vs-rest approach
and used 5-fold cross-validation on the training data to set pa-
rameters. For temporals, C=0.1 (for syntactic-only models),
</footnote>
<table confidence="0.999903">
Model Temporals P Causals F1
P R F1 R
BEFORE 26.7 94.2 41.6 - - -
CAUSAL - - - 21.1 100.0 34.8
1st Event 35.0 24.4 28.8 31.0 20.3 24.5
2nd Event 36.1 30.2 32.9 22.4 17.2 19.5
POS Pair 46.7 8.1 13.9 30.0 4.7 8.1
Syntactic 36.5 53.5 43.4 24.4 79.7 37.4
Semantic 35.8 55.8 43.6 27.2 64.1 38.1
All 43.6 55.8 49.0 27.0 59.4 37.1
All+Tmp - - - 46.9 59.4 52.4
</table>
<tableCaption confidence="0.989975666666667">
Table 3: Performance of the temporal relation identifica-
tion models: (A)ccuracy, (P)recision, (R)ecall and (F1)-
measure. The null label is NO-REL.
</tableCaption>
<bodyText confidence="0.983044068965517">
train/test split from Table 1 and the feature sets:
Syntactic The syntactic features from Section 4.
Semantic The semantic features from Section 4.
All Both syntactic and semantic features.
All+Tmp (Causals Only) Syntactic and semantic
features, plus the gold-standard temporal label.
We compared our models against several baselines,
using precision, recall and F-measure since the NO-
REL labels were uninteresting. Two simple baselines
had 0% recall: a lookup table of event word pairs3,
and the majority class (NO-REL) label for causals.
We therefore considered the following baselines:
BEFORE Classify all instances as BEFORE, the ma-
jority class label for temporals.
CAUSAL Classify all instances as CAUSAL.
1st Event Use a lookup table of 1st words and the
labels they were assigned in the training data.
2nd Event As 1st Event, but using 2nd words.
POS Pair As 1st Event, but using part of speech tag
pairs. POS tags encode tense, so this suggests the
performance of a tense-based classifier.
The results on our test data are shown in Table 3. For
temporal relations, the F-measures of all SVM mod-
els exceeded all baselines, with the combination of
syntactic and semantic features performing 5 points
better (43.6% precision and 55.8% recall) than either
feature set individually. This suggests that our syn-
tactic and semantic features encoded complemen-
tary information for the temporal relation task. For
</bodyText>
<footnote confidence="0.66927775">
C=1.0 (for all other models), and loss-function=F1 (for all
models). For causals, C=0.1 and loss-function=precision/recall
break even point (for all models).
3Only 3 word pairs from training were seen during testing.
</footnote>
<page confidence="0.99519">
179
</page>
<figureCaption confidence="0.989605666666667">
Figure 2: Model precisions (dotted lines) and percent of
events in the test data seen during training (solid lines),
given increasing fractions of the training data.
</figureCaption>
<bodyText confidence="0.999646666666667">
causal relations, all SVM models again exceeded all
baselines, but combining syntactic features with se-
mantic ones gained little. However, knowing about
underlying temporal relations boosted performance
to 46.9% precision and 59.4% recall. This shows
that progress in causal relation identification will re-
quire knowledge of temporal relations.
We examined the effect of corpus size on our
models by training them on increasing fractions of
the training data and evaluating them on the test
data. The precisions of the resulting models are
shown as dotted lines in Figure 2. The models im-
prove steadily, and the causals precision can be seen
to follow the solid curves which show how event
coverage increases with increased training data. A
logarithmic trendline fit to these seen-event curves
suggests that annotating all 5,013 event pairs in the
Penn TreeBank could move event coverage up from
the mid 50s to the mid 80s. Thus annotating addi-
tional data should provide a substantial benefit to our
temporal and causal relation identification systems.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999985875">
Our research fills a gap in existing corpora and NLP
systems, examining parallel temporal and causal re-
lations. We annotated 1000 event pairs conjoined
by the word and, assigning each pair both a tempo-
ral and causal relation. Annotators achieved 81.2%
agreement on temporal relations and 77.8% agree-
ment on causal relations. Using features based on
WordNet and the Google N-gram corpus, we trained
support vector machine models that achieved 49.0
F on temporal relations, and 37.1 F on causal rela-
tions. Providing temporal information to the causal
relations classifier boosted its results to 52.4 F. Fu-
ture work will investigate increasing the size of the
corpus and developing more statistical approaches
like the Google N-gram scores to take advantage of
large-scale resources to characterize word meaning.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998216">
This research was performed in part under an ap-
pointment to the U.S. Department of Homeland Se-
curity (DHS) Scholarship and Fellowship Program.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883375">
S. Bethard and J. H. Martin. 2006. Identification of event
mentions and their semantic class. In EMNLP-2006.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007.
Timelines from text: Identification of syntactic tem-
poral relations. In ICSC-2007.
S. Bethard, W. Corvey, S. Klingenstein, and J. H. Martin.
2008. Building a corpus of temporal-causal structure.
In LREC-2008.
B. Boguraev and R. K. Ando. 2005. Timebank-
driven timeml analysis. In Annotating, Extracting
and Reasoning about Time and Events. IBFI, Schloss
Dagstuhl, Germany.
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium, Philadelphia.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney,
and D. Yuret. 2007. Semeval-2007 task 04: Classi-
fication of semantic relations between nominals. In
SemEval-2007.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL Workshop on Multi-
lingual Summarization and Question Answering.
T. Joachims. 2005. A support vector method for multi-
variate performance measures. In ICML-2005.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC-2002.
J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647–656.
D. Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature sup-
port vector models. LDV-Forum, GLDV-Journal for
Computational Linguistics and Language Technology,
18(1/2):38–52.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. Semeval-2007
task 15: Tempeval temporal relation identification. In
SemEval-2007.
</reference>
<page confidence="0.997755">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923948">
<title confidence="0.999942">Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations</title>
<author confidence="0.999935">Steven Bethard</author>
<affiliation confidence="0.998997">Institute for Cognitive Science Department of Computer Science University of Colorado</affiliation>
<address confidence="0.999931">Boulder, CO 80309, USA</address>
<email confidence="0.99976">steven.bethard@colorado.edu</email>
<author confidence="0.999584">James H Martin</author>
<affiliation confidence="0.998523666666667">Institute for Cognitive Science Department of Computer Science University of Colorado</affiliation>
<address confidence="0.999954">Boulder, CO 80309, USA</address>
<email confidence="0.99986">james.martin@colorado.edu</email>
<abstract confidence="0.995837764705882">Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>177--180</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 177–180,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<title>Identification of event mentions and their semantic class.</title>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics S. Bethard and J. H. Martin. 2006. Identification of event mentions and their semantic class. In EMNLP-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>J H Martin</author>
<author>S Klingenstein</author>
</authors>
<title>Timelines from text: Identification of syntactic temporal relations.</title>
<date>2007</date>
<booktitle>In ICSC-2007.</booktitle>
<marker>Bethard, Martin, Klingenstein, 2007</marker>
<rawString>S. Bethard, J. H. Martin, and S. Klingenstein. 2007. Timelines from text: Identification of syntactic temporal relations. In ICSC-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>W Corvey</author>
<author>S Klingenstein</author>
<author>J H Martin</author>
</authors>
<title>Building a corpus of temporal-causal structure.</title>
<date>2008</date>
<booktitle>In LREC-2008.</booktitle>
<marker>Bethard, Corvey, Klingenstein, Martin, 2008</marker>
<rawString>S. Bethard, W. Corvey, S. Klingenstein, and J. H. Martin. 2008. Building a corpus of temporal-causal structure. In LREC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R K Ando</author>
</authors>
<title>Timebankdriven timeml analysis.</title>
<date>2005</date>
<booktitle>In Annotating, Extracting and Reasoning about Time and Events. IBFI, Schloss Dagstuhl,</booktitle>
<location>Germany.</location>
<contexts>
<context position="2781" citStr="Boguraev and Ando, 2005" startWordPosition="414" endWordPosition="417"> but arguments may only be temporal or causal, never both. Thus existing corpora are missing some crucial pieces for studying temporal-causal interactions. Our research aims to fill these gaps by building a corpus of parallel temporal and causal relations and exploring machine learning approaches to extracting these relations. 2 Related Work Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007). These works annotated temporal relations between events and times, but low inter-annotator agreement made many TimeBank and TempEval tasks difficult (Boguraev and Ando, 2005; Verhagen et al., 2007). Still, TempEval showed that on a constrained tense identification task, systems could achieve accuracies in the 80s, and Bethard and colleagues (Bethard et al., 2007) showed that temporal relations between a verb and a complement clause could be identified with accuracies of nearly 90%. Recent work on causal relations has also found that arbitrary relations in text are difficult to annotate and give poor system performance (Reitter, 2003). Girju and colleagues have made progress by selecting constrained pairs of events using web search patterns. Both manually generate</context>
</contexts>
<marker>Boguraev, Ando, 2005</marker>
<rawString>B. Boguraev and R. K. Ando. 2005. Timebankdriven timeml analysis. In Annotating, Extracting and Reasoning about Time and Events. IBFI, Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1t 5-gram version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1t 5-gram version 1. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>P Nakov</author>
<author>V Nastase</author>
<author>S Szpakowicz</author>
<author>P Turney</author>
<author>D Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In SemEval-2007.</booktitle>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In ACL Workshop on Multilingual Summarization and Question Answering.</booktitle>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for question answering. In ACL Workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>A support vector method for multivariate performance measures.</title>
<date>2005</date>
<booktitle>In ICML-2005.</booktitle>
<marker>Joachims, 2005</marker>
<rawString>T. Joachims. 2005. A support vector method for multivariate performance measures. In ICML-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In LREC-2002.</booktitle>
<contexts>
<context position="2114" citStr="Kingsbury and Palmer, 2002" startWordPosition="317" endWordPosition="320">E having a shot. The system must also identify when an event is not independent of the surrounding events, e.g. got food poisoning was CAUSED by eating a bad sandwich. Recognizing these temporal and causal relations is crucial for applications like question answering which must face queries like How did he get food poisoning? or What was the treatment? Currently, no existing resource has all the necessary pieces for investigating parallel temporal and causal phenomena. The TimeBank (Pustejovsky et al., 2003) links events with BEFORE and AFTER relations, but includes no causal links. PropBank (Kingsbury and Palmer, 2002) identifies ARGM-TMP and ARGM-CAU relations, but arguments may only be temporal or causal, never both. Thus existing corpora are missing some crucial pieces for studying temporal-causal interactions. Our research aims to fill these gaps by building a corpus of parallel temporal and causal relations and exploring machine learning approaches to extracting these relations. 2 Related Work Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007). These works annotated temporal relations between events and times, but low inter-annotator agreement made </context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In LREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>R Sauri</author>
<author>A See</author>
<author>R Gaizauskas</author>
<author>A Setzer</author>
<author>D Radev</author>
<author>B Sundheim</author>
<author>D Day</author>
<author>L Ferro</author>
<author>M Lazo</author>
</authors>
<title>The timebank corpus. In Corpus Linguistics,</title>
<date>2003</date>
<pages>647--656</pages>
<contexts>
<context position="2000" citStr="Pustejovsky et al., 2003" startWordPosition="300" endWordPosition="303">ucture here, a system must order events along a timeline, recognizing that getting food poisoning occurred BEFORE having a shot. The system must also identify when an event is not independent of the surrounding events, e.g. got food poisoning was CAUSED by eating a bad sandwich. Recognizing these temporal and causal relations is crucial for applications like question answering which must face queries like How did he get food poisoning? or What was the treatment? Currently, no existing resource has all the necessary pieces for investigating parallel temporal and causal phenomena. The TimeBank (Pustejovsky et al., 2003) links events with BEFORE and AFTER relations, but includes no causal links. PropBank (Kingsbury and Palmer, 2002) identifies ARGM-TMP and ARGM-CAU relations, but arguments may only be temporal or causal, never both. Thus existing corpora are missing some crucial pieces for studying temporal-causal interactions. Our research aims to fill these gaps by building a corpus of parallel temporal and causal relations and exploring machine learning approaches to extracting these relations. 2 Related Work Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al.,</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, and M. Lazo. 2003. The timebank corpus. In Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Reitter</author>
</authors>
<title>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. LDV-Forum, GLDV-Journal for Computational Linguistics and Language Technology,</title>
<date>2003</date>
<pages>18--1</pages>
<marker>Reitter, 2003</marker>
<rawString>D. Reitter. 2003. Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. LDV-Forum, GLDV-Journal for Computational Linguistics and Language Technology, 18(1/2):38–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>R Gaizauskas</author>
<author>F Schilder</author>
<author>M Hepple</author>
<author>G Katz</author>
<author>J Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In SemEval-2007.</booktitle>
<contexts>
<context position="2606" citStr="Verhagen et al., 2007" startWordPosition="390" endWordPosition="393">ky et al., 2003) links events with BEFORE and AFTER relations, but includes no causal links. PropBank (Kingsbury and Palmer, 2002) identifies ARGM-TMP and ARGM-CAU relations, but arguments may only be temporal or causal, never both. Thus existing corpora are missing some crucial pieces for studying temporal-causal interactions. Our research aims to fill these gaps by building a corpus of parallel temporal and causal relations and exploring machine learning approaches to extracting these relations. 2 Related Work Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007). These works annotated temporal relations between events and times, but low inter-annotator agreement made many TimeBank and TempEval tasks difficult (Boguraev and Ando, 2005; Verhagen et al., 2007). Still, TempEval showed that on a constrained tense identification task, systems could achieve accuracies in the 80s, and Bethard and colleagues (Bethard et al., 2007) showed that temporal relations between a verb and a complement clause could be identified with accuracies of nearly 90%. Recent work on causal relations has also found that arbitrary relations in text are difficult to annotate and g</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In SemEval-2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>