<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000742">
<title confidence="0.998178">
Multi-Class Composite N-gram Language Model for Spoken Language
Processing Using Multiple Word Clusters
</title>
<author confidence="0.928588">
Hirofumi Yamamoto Shuntaro Isogai Yoshinori Sagisaka
</author>
<affiliation confidence="0.865267">
ATR SLT Waseda University GITI / ATR SLT
</affiliation>
<address confidence="0.975308">
2-2-2 Hikaridai Seika-cho 3-4-1 Okubo, Shinjuku-ku 1-3-10 Nishi-Waseda
Soraku-gun, Kyoto-fu, Japan Tokyo-to, Japan Shinjuku-ku, Tokyo-to, Japan
</address>
<email confidence="0.998884">
yama@slt.atr.co.jp isogai@shirai.info.waseda.ac.jp sagisaka@slt.atr.co.jp
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999806">
In this paper, a new language model, the
Multi-Class Composite N-gram, is pro-
posed to avoid a data sparseness prob-
lem for spoken language in that it is
difficult to collect training data. The
Multi-Class Composite N-gram main-
tains an accurate word prediction ca-
pability and reliability for sparse data
with a compact model size based on
multiple word clusters, called Multi-
Classes. In the Multi-Class, the statisti-
cal connectivity at each position of the
N-grams is regarded as word attributes,
and one word cluster each is created to
represent the positional attributes. Fur-
thermore, by introducing higher order
word N-grams through the grouping of
frequent word successions, Multi-Class
N-grams are extended to Multi-Class
Composite N-grams. In experiments,
the Multi-Class Composite N-grams re-
sult in 9.5% lower perplexity and a 16%
lower word error rate in speech recogni-
tion with a 40% smaller parameter size
than conventional word 3-grams.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999662775510204">
Word N-grams have been widely used as a sta-
tistical language model for language processing.
Word N-grams are models that give the transition
probability of the next word from the previous
N — 1 word sequence based on a statistical analy-
sis of the huge text corpus. Though word N-grams
are more effective and flexible than rule-based
grammatical constraints in many cases, their per-
formance strongly depends on the size of training
data, since they are statistical models.
In word N-grams, the accuracy of the word
prediction capability will increase according to
the number of the order N, but also the num-
ber of word transition combinations will exponen-
tially increase. Moreover, the size of training data
for reliable transition probability values will also
dramatically increase. This is a critical problem
for spoken language in that it is difficult to col-
lect training data sufficient enough for a reliable
model. As a solution to this problem, class N-
grams are proposed.
In class N-grams, multiple words are mapped
to one word class, and the transition probabilities
from word to word are approximated to the proba-
bilities from word class to word class. The perfor-
mance and model size of class N-grams strongly
depend on the definition of word classes. In fact,
the performance of class N-grams based on the
part-of-speech (POS) word class is usually quite
a bit lower than that of word N-grams. Based on
this fact, effective word class definitions are re-
quired for high performance in class N-grams.
In this paper, the Multi-Class assignment is
proposed for effective word class definitions. The
word class is used to represent word connectiv-
ity, i.e. which words will appear in a neigh-
boring position with what probability. In Multi-
Class assignment, the word connectivity in each
position of the N-grams is regarded as a differ-
ent attribute, and multiple classes corresponding
to each attribute are assigned to each word. For
the word clustering of each Multi-Class for each
word, a method is used in which word classes are
formed automatically and statistically from a cor-
pus, not using a priori knowledge as POS infor-
mation. Furthermore, by introducing higher order
word N-grams through the grouping of frequent
word successions, Multi-Class N-grams are ex-
tended to Multi-Class Composite N-grams.
</bodyText>
<sectionHeader confidence="0.956446" genericHeader="introduction">
2 N-gram Language Models Based on
Multiple Word Classes
</sectionHeader>
<subsectionHeader confidence="0.997145">
2.1 Class N-grams
</subsectionHeader>
<bodyText confidence="0.99981725">
Word N-grams are models that statistically give
the transition probability of the next word from
the previous N —1 word sequence. This transition
probability is given in the next formula.
</bodyText>
<equation confidence="0.998959">
p(wijwi-N+1; :::; wi-2; wi-1) (1)
</equation>
<bodyText confidence="0.999653466666667">
In word N-grams, accurate word prediction can be
expected, since a word dependent, unique connec-
tivity from word to word can be represented. On
the other hand, the number of estimated param-
eters, i.e., the number of combinations of word
transitions, is V N in vocabulary V . As V N will
exponentially increase according to N, reliable
estimations of each word transition probability
are difficult under a large N.
Class N-grams are proposed to resolve the
problem that a huge number of parameters is re-
quired in word N-grams. In class N-grams, the
transition probability of the next word from the
previous N —1 word sequence is given in the next
formula.
</bodyText>
<equation confidence="0.998118">
p(cijci—N+1; :::; ci-2; ci-1)p(wijci) (2)
</equation>
<bodyText confidence="0.9848822">
Where, ci represents the word class to which the
word wi belongs.
In class N-grams with C classes, the number
of estimated parameters is decreased from V N
to CN. However, accuracy of the word predic-
tion capability will be lower than that of word N-
grams with a sufficient size of training data, since
the representation capability of the word depen-
dent, unique connectivity attribute will be lost for
the approximation base word class.
</bodyText>
<subsectionHeader confidence="0.9900085">
2.2 Problems in the Definition of Word
Classes
</subsectionHeader>
<bodyText confidence="0.998983952380952">
In class N-grams, word classes are used to repre-
sent the connectivity between words. In the con-
ventional word class definition, word connectiv-
ity for which words follow and that for which
word precedes are treated as the same neighbor-
ing characteristics without distinction. Therefore,
only the words that have the same word connec-
tivity for the following words and the preceding
word belong to the same word class, and this word
class definition cannot represent the word connec-
tivity attribute efficiently. Take ”a” and ”an” as an
example. Both are classified by POS as an Indef-
inite Article, and are assigned to the same word
class. In this case, information about the differ-
ence with the following word connectivity will be
lost. On the other hand, a different class assign-
ment for both words will cause the information
about the community in the preceding word con-
nectivity to be lost. This directional distinction is
quite crucial for languages with reflection such as
French and Japanese.
</bodyText>
<subsectionHeader confidence="0.992065">
2.3 Multi-Class and Multi-Class N-grams
</subsectionHeader>
<bodyText confidence="0.999995714285714">
As in the previous example of ”a” and ”an”, fol-
lowing and preceding word connectivity are not
always the same. Let’s consider the case of dif-
ferent connectivity for the words that precede and
follow. Multiple word classes are assigned to
each word to represent the following and preced-
ing word connectivity. As the connectivity of the
word preceding ”a” and ”an” is the same, it is ef-
ficient to assign them to the same word class to
represent the preceding word connectivity, if as-
signing different word classes to represent the fol-
lowing word connectivity at the same time. To
apply these word class definitions to formula (2),
the next formula is given.
</bodyText>
<equation confidence="0.99584225">
p(ct ijcfN�1
iN+1; :::; cf2
i2; cf1
i1)p(wijct i) (3)
</equation>
<bodyText confidence="0.89224325">
In the above formula, cti represents the word class
in the target position to which the word w i be-
longs, and cfN
i represents the word class in the
N-th position in a conditional word sequence.
We call this multiple word class definition, a
Multi-Class. Similarly, we call class N-grams
based on the Multi-Class, Multi-Class N-grams
(Yamamoto and Sagisaka, 1999).
3 Automatic Extraction of Word probability of the succeeding class-word 2-
Clusters gram or word 2-gram, while pf is the same
for the preceding one.
</bodyText>
<subsectionHeader confidence="0.8760765">
3.1 Word Clustering for Multi-Class
2-grams
</subsectionHeader>
<bodyText confidence="0.9999331">
For word clustering in class N-grams, POS in-
formation is sometimes used. Though POS in-
formation can be used for words that do not ap-
pear in the corpus, this is not always an optimal
word classification for N-grams. The POS in-
formation does not accurately represent the sta-
tistical word connectivity characteristics. Better
word-clustering is to be considered based on word
connectivity by the reflection neighboring charac-
teristics in the corpus. In this paper, vectors are
used to represent word neighboring characteris-
tics. The elements of the vectors are forward or
backward word 2-gram probabilities to the clus-
tering target word after being smoothed. And we
consider that word pairs that have a small distance
between vectors also have similar word neighbor-
ing characteristics (Brown et al., 1992) (Bai et
al., 1998). In this method, the same vector is
assigned to words that do not appear in the cor-
pus, and the same word cluster will be assigned to
these words. To avoid excessively rough cluster-
ing over different POS, we cluster the words un-
der the condition that only words with the same
POS can belong to the same cluster. Parts-of-
speech that have the same connectivity in each
Multi-Class are merged. For example, if differ-
ent parts-of-speeche are assigned to ”a” and ”an”,
these parts-of-speeche are regarded as the same
for the preceding word cluster. Word clustering is
thus performed in the following manner.
</bodyText>
<listItem confidence="0.95346175">
1. Assign one unique class per word.s.
2. Assign a vector to each class or to each word
X. This represents the word connectivity at-
tribute.
</listItem>
<equation confidence="0.985969">
vt(x) = [pt(w1jx); pt(w2jx); :::; pt(wNjx)]
vf(x) = [pf(w1jx); pf(w2jx);:::;pf(wNjx)]
</equation>
<bodyText confidence="0.654531714285714">
Where, vt(x) represents the preceding word
connectivity, vf(x) represents the following
word connectivity, and pt is the value of the
3. Merge the two classes. We choose classes
whose dispersion weighted with the 1-gram
probability results in the lowest rise, and
merge these two classes:
</bodyText>
<equation confidence="0.9742435">
Unew = � (p(w)D(v(cnew(w)); v(w)))
w
Uold = (p(w)D(v(cold(w)); v(w)))
w
</equation>
<bodyText confidence="0.98351875">
where we merge the classes whose merge
cost Unew — Uold is the lowest. D(ve; vw)
represents the square of the Euclidean dis-
tance between vector ve and vw, cold repre-
sents the classes before merging, and cnew
represents the classes after merging.
4. Repeat step 2 until the number of classes is
reduced to the desired number.
</bodyText>
<subsectionHeader confidence="0.9748015">
3.2 Word Clustering for Multi-Class
3-grams
</subsectionHeader>
<bodyText confidence="0.99997815">
To apply the multiple clustering for 2-grams to
3-grams, the clustering target in the conditional
part is extended to a word pair from the single
word in 2-grams. Number of clustering targets in
the preceding class increases to V 2 from V in 2-
grams, and the length of the vector in the succeed-
ing class also increase to V 2. Therefore, efficient
word clustering is needed to keep the reliability
of 3-grams after the clustering and a reasonable
calculation cost.
To avoid losing the reliability caused by the
data sparseness of the word pair in the history
of 3-grams, approximation is employed using
distance-2 2-grams. The authority of this ap-
proximation is based on a report that the asso-
ciation of word 2-grams and distance-2 2-grams
based on the maximum entropy method gives a
good approximation of word 3-grams (Zhang et
al., 1999). The vector for clustering is given in
the next equation.
</bodyText>
<equation confidence="0.999389666666667">
vf2(x) = [pf 2(w1jx); pf 2(w2jx); :::; pf2(wNjx)]
(8)
cf(A + B + C) = cf(C) (12)
</equation>
<bodyText confidence="0.996542">
Applying these relations to equation (10), the next
equation is obtained.
Where, pf 2(yjx) represents the distance-2 2-gram
value from word x to word y. And the POS con-
straints for clustering are the same as in the clus-
tering for preceding words.
</bodyText>
<sectionHeader confidence="0.997426" genericHeader="method">
4 Multi-Class Composite N-grams
</sectionHeader>
<subsectionHeader confidence="0.964689">
4.1 Multi-Class Composite 2-grams
Introducing Variable Length Word
Sequences
</subsectionHeader>
<bodyText confidence="0.9999055625">
Let’s consider the condition such that only word
sequence (A, B, C) has sufficient frequency in
sequence (X, A, B, C, D). In this case, the value
of word 2-gram p(BjA) can be used as a reli-
able value for the estimation of word B, as the
frequency of sequence (A, B) is sufficient. The
value of word 3-gram p(CjA, B) can be used
for the estimation of word C for the same rea-
son. For the estimation of words A and D, it is
reasonable to use the value of the class 2-gram,
since the value of the word N-gram is unreli-
able (note that the frequency of word sequences
(X, A) and (C, D) is insufficient). Based on this
idea, the transition probability of word sequence
(A, B, C, D) from word X is given in the next
equation in the Multi-Class 2-gram.
</bodyText>
<equation confidence="0.999946">
P = p(ct(A)jcf(X))p(Ajct(A)))
X p(BjA)
X p(CjA, B)
X p(ct(D)jcf(C))p(Djct(D)) (9)
</equation>
<bodyText confidence="0.9989802">
When word succession A+B+C is introduced as
a variable length word sequence (A, B, C), equa-
tion (9) can be changed exactly to the next equa-
tion (Deligne and Bimbot, 1995) (Masataki et al.,
1996).
</bodyText>
<equation confidence="0.9998515">
P = p(ct(A)jcf(X))p(A + B + Cjct(A))
X p(ct(D)jcf(C))p(Djct(D)) (10)
</equation>
<bodyText confidence="0.999978125">
Here, we find the following properties. The pre-
ceding word connectivity of word succession A+
B + C is the same as the connectivity of word A,
the first word of A + B + C. The following con-
nectivity is the same as the last word C. In these
assignments, no new cluster is required. But con-
ventional class N-grams require a new cluster for
the new word succession.
</bodyText>
<equation confidence="0.9999744">
ct(A+B + C) = ct(A) (11)
P = p(ct(A + B + C)jcf(X))
X p(A + B + Cjct(A + B + C))
X p(ct(D)jcf(A + B + C))
X p(Djct(D)) (13)
</equation>
<bodyText confidence="0.99562125">
Equation(13) means that if the frequency of the
N word sequence is sufficient, we can partially
introduce higher order word N-grams using N
length word succession, thus maintaining the re-
liability of the estimated probability and forma-
tion of the Multi-Class 2-grams. We call Multi-
Class Composite 2-grams that are created by par-
tially introducing higher order word N-grams by
word succession, Multi-Class 2-grams. In addi-
tion, equation (13) shows that number of param-
eters will not be increased so match when fre-
quent word successions are added to the word en-
try. Only a 1-gram of word succession A+B +C
should be added to the conventional N-gram pa-
rameters. Multi-Class Composite 2-grams are
created in the following manner.
</bodyText>
<listItem confidence="0.908133">
1. Assign a Multi-Class 2-gram, for state ini-
tialization.
2. Find a word pair whose frequency is above
the threshold.
3. Create a new word succession entry for the
frequent word pair and add it to a lexicon.
The following connectivity class of the word
succession is the same as the following class
of the first word in the pair, and its preceding
class is the same as the preceding class of the
last word in it.
4. Replace the frequent word pair in training
data to word succession, and recalculate the
frequency of the word or word succession
pair. Therefore, the summation of probabil-
ity is always kept to 1.
5. Repeat step 2 with the newly added word
succession, until no more word pairs are
found.
</listItem>
<subsectionHeader confidence="0.8729045">
4.2 Extension to Multi-Class Composite
3-grams
</subsectionHeader>
<bodyText confidence="0.999284">
Next, we put the word succession into the for-
mulation of Multi-Class 3-grams. The transition
probability to word sequence (A; B; C; D; E; F)
from word pair (X; Y ) is given in the next equa-
tion.
</bodyText>
<equation confidence="0.916278454545455">
P = p(ct(A+B+C+ D)jcf2(X); cf1(Y ))
X p(A + B + C + Djct(A + B + C + D))
X p(ct(E)jcf2(Y ); cf1(A + B + C + D))
X p(Ejct(E))
X p(ct(F)jcf2(A + B + C + D); cf1(E))
X p(Fjct(F)) (14)
Where, the Multi-Classes for word succession
A + B + C + D are given by the next equations.
ct(A + B + C + D) = ct(A)
cf2(A + B + C + D) = cf2(D)
cf1(A + B + C + D) = cf2(C); cf1(D)
</equation>
<bodyText confidence="0.99852675">
In equation (17), please notice that the class se-
quence (not single class) is assigned to the pre-
ceding class of the word successions. the class
sequence is the preceding class of the last word of
the word succession and the pre-preceding class
of the second from the last word. Applying these
class assignments to equation (14) gives the next
equation.
</bodyText>
<equation confidence="0.999964833333333">
P = p(ct(A)jcf2(X); cf1(Y ))
X p(A + B + C + Djct(A))
X p(ct(E)jcf2(C); cf1(D))
X p(Ejct(E))
X p(ct(F)jcf2(D); cf1(E))
X p(Fjct(F)) (18)
</equation>
<bodyText confidence="0.99982525">
In the above formation, the parameter increase
from the Multi-class 3-gram is p(A + B + C +
Djct(A)). After expanding this term, the next
equation is given.
</bodyText>
<equation confidence="0.999879777777778">
P = p(ct(A)jcf2(X); cf1(Y ))
X p(Ajct(A))
X p(BjA)
X p(CjA; B)
x p(DjA; B; C)
X p(ct(E)jcf2(C); cf1(D))
X p(Ejct(E))
X p(ct(F)jcf2(D); cf1(E))
X p(Fjct(F)) (19)
</equation>
<bodyText confidence="0.9990556">
In equation (19), the words without B are es-
timated by the same or more accurate models
than Multi-Class 3-grams (Multi-Class 3-grams
for words A, E and F, and word 3-gram and word
4-gram for words C and D ). However, for word
B, a word 2-gram is used instead of the Multi-
Class 3-grams though its accuracy is lower than
the Multi-Class 3-grams. To prevent this decrease
in the accuracy of estimation, the next process is
introduced.
</bodyText>
<equation confidence="0.944537166666667">
First, the 3-gram entry p(ct(E)jcf2(Y ); A +
B +C +D) is removed. After this deletion, back-
off smoothing is applied to this entry as follows.
p(ct(E)jcf2(Y ); cf1(A + B + C + D))
= b(cf2(Y ); cf1(A + B + C + D))
X p(ct(E)jcf1(A + B + C + D)) (20)
</equation>
<bodyText confidence="0.999949">
Next, we assign the following value to the
back-off parameter in equation (20). And this
value is used to correct the decrease in the accu-
racy of the estimation of word B.
</bodyText>
<equation confidence="0.999946">
b(cf2(Y ); cf1(A + B + C + D))
= p(ct(B)jcf2(Y ); cf1(A))
X p(Bjct(B))=p(BjA) (21)
</equation>
<bodyText confidence="0.998489888888889">
After this assignment, the probabilities of words
B and E are locally incorrect. However, the total
probability is correct, since the back-off parame-
ter is used to correct the decrease in the accuracy
of the estimation of word B. In fact, applying
equations (20) and (21) to equation (14) accord-
ing to the above definition gives the next equa-
tion. In this equation, the probability for word B
is changed from a word 2-gram to a class 3-gram.
</bodyText>
<equation confidence="0.9999267">
P = p(ct(A)jcf2(X); cf1(Y ))
X p(Ajct(A))
X p(ct(B)jcf2(Y ); cf1(A))
X p(Bjct(B))
X p(CjA; B)
X p(DIA, B, C)
X p(ct(E)Icf2(C), cf1(D))
X p(EIct(E))
X p(ct(F)Icf2(D), cf1(E))
X p(FIct(F)) (22)
</equation>
<bodyText confidence="0.9999546">
In the above process, only 2 parameters are ad-
ditionally used. One is word 1-grams of word
successions as p(A + B + C + D). And the
other is word 2-grams of the first two words of
the word successions. The number of combina-
tions for the first two words of the word succes-
sions is at most the number of word successions.
Therefore, the number of increased parameters in
the Multi-Class Composite 3-gram is at most the
number of introduced word successions times 2.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="method">
5 Evaluation Experiments
</sectionHeader>
<subsectionHeader confidence="0.999968">
5.1 Evaluation of Multi-Class N-grams
</subsectionHeader>
<bodyText confidence="0.999978">
We have evaluated Multi-Class N-grams in per-
plexity as the next equations.
</bodyText>
<equation confidence="0.9997575">
1 � lo92(p(wi)) (23)
Entropy =
N .
Perplexity = 2Entropy (24)
</equation>
<bodyText confidence="0.999514136363636">
The Good-Turing discount is used for smooth-
ing. The perplexity is compared with those of
word 2-grams and word 3-grams. The evaluation
data set is the ATR Spoken Language Database
(Takezawa et al., 1998). The total number of
words in the training set is 1,387,300, the vocab-
ulary size is 16,531, and 5,880 words in 42 con-
versations which are not included in the training
set are used for the evaluation.
Figure1 shows the perplexity of Multi-Class 2-
grams for each number of classes. In the Multi-
Class, the numbers of following and preceding
classes are fixed to the same value just for com-
parison. As shown in the figure, the Multi-Class
2-gram with 1,200 classes gives the lowest per-
plexity of 22.70, and it is smaller than the 23.93
in the conventional word 2-gram.
Figure 2 shows the perplexity of Multi-Class
3-grams for each number of classes. The num-
ber of following and preceding classes is 1,200
(which gives the lowest perplexity in Multi-Class
2-grams). The number of pre-preceding classes is
</bodyText>
<tableCaption confidence="0.9223585">
Table 1: Evaluation of Multi-Class Composite N-
grams in Perplexity
</tableCaption>
<table confidence="0.999760454545455">
Kind of model Perplexity Number of
parameters
Word 2-gram 23.93 181,555
Multi-Class 2-gram 22.70 81,556
Multi-Class 19.81 92,761
Composite 2-gram
Word 3-gram 17.88 713,154
Multi-Class 3-gram 17.38 438,130
Multi-Class 16.20 455,431
Composite 3-gram
Word 4-gram 17.45 1,703,207
</table>
<bodyText confidence="0.9984672">
changed from 100 to 1,500. As shown in this fig-
ure, Multi-Class 3-grams result in lower perplex-
ity than the conventional word 3-gram, indicating
the reasonability of word clustering based on the
distance-2 2-gram.
</bodyText>
<subsectionHeader confidence="0.9968">
5.2 Evaluation of Multi-Class Composite
N-grams
</subsectionHeader>
<bodyText confidence="0.99989275">
We have also evaluated Multi-Class Composite
N-grams in perplexity under the same conditions
as the Multi-Class N-grams stated in the previ-
ous section. The Multi-Class 2-gram is used for
the initial condition of the Multi-Class Compos-
ite 2-gram. The threshold of frequency for in-
troducing word successions is set to 10 based on
a preliminary experiment. The same word suc-
cession set as that of the Multi-Class Composite
2-gram is used for the Multi-Class Composite 3-
gram. The evaluation results are shown in Table
1. Table 1 shows that the Multi-Class Compos-
ite 3-gram results in 9.5% lower perplexity with a
40% smaller parameter size than the conventional
word 3-gram, and that it is in fact a compact and
high-performance model.
</bodyText>
<sectionHeader confidence="0.614559" genericHeader="method">
5.3 Evaluation in Continuous Speech
Recognition
</sectionHeader>
<bodyText confidence="0.9998695">
Though perplexity is a good measure for the per-
formance of language models, it does not al-
ways have a direct bearing on performance in lan-
guage processing. We have evaluated the pro-
posed model in continuous speech recognition.
The experimental conditions are as follows:
</bodyText>
<figure confidence="0.95183232">
• Evaluation set
• Decoder (Shimizu et al., 1996)
– 1st pass: frame-synchronized viterbi
search
(W: Number of correct words, D: Deletion error,
I : Insertion error, S: Substitution error)
——
% W D S
Correct = W
X 100
– The same 42 conversations as used in
the evaluation of perplexity
• Acoustic features
– Sampling rate 16kHz
– Frame shift 10msec
– Mel-cepstrum 12 + power and their
delta, total 26
• Acoustic models
– 800-state 5-mixture HMnet model
based on ML-SSS (Ostendorf and
Singer, 1997)
– Automatic selection of gender depen-
dent models
– 2nd pass: full search after changing the
language model and LM scale
</figure>
<figureCaption confidence="0.701452076923077">
The Multi-Class Composite 2-gram and 3-
gram are compared with those of the word 2-
gram, Multi-Class 2-gram, word 3-gram and
Multi-Class 3-gram. The number of classes is
1,200 through all class-based models. For the
evaluation of each 2-gram, a 2-gram is used at
both the 1st and the 2nd pass in decoder. For
the 3-gram, each 2-gram is changed to the cor-
responding 3-gram in the 2nd pass. The evalu-
ation measures are conventional word accuracy
and %correct calculated as follows.
WordAccuracy = W DW I — SX 100
Figure 2: Perplexity of Multi-Class 3-grams
</figureCaption>
<figure confidence="0.993229333333333">
100 300 500 700 900 1100 1300 1500
Number of Classes
17 17.5 18 18.5 19 19.5 20
Multi-Class 3-gram
word 3-gram
Perplexity
</figure>
<figureCaption confidence="0.995841">
Figure 1: Perplexity of Multi-Class 2-grams
</figureCaption>
<figure confidence="0.966935166666667">
400 600 800 1000 1200 1400 1600
Number of Classes
22.5 23 23.5 24 24.5 25
Multi-Class 2-gram
word 2-gram
Perplexity
</figure>
<tableCaption confidence="0.8888595">
Table 2: Evaluation of Multi-Class Composite N-
grams in Continuous Speech Recognition
</tableCaption>
<table confidence="0.997617444444444">
Kind of Model Word Acc. %Correct
Word 2-gram 84.15 88.42
Multi-Class 2-gram 85.45 88.80
Multi-Class 88.00 90.84
Composite 2-gram
Word 3-gram 86.07 89.76
Multi-Class 3-gram 87.11 90.50
Multi-Class 88.30 91.48
Composite 3-gram
</table>
<bodyText confidence="0.9992808">
lower word error rate in continuous speech recog-
nition with a 40% smaller model size than the
conventional word 3-gram. And it is confirmed
that high performance with a small model size can
be created for Multi-Class Composite 3-grams.
</bodyText>
<sectionHeader confidence="0.996556" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999375">
We would like to thank Michael Paul and Rainer
Gruhn for their assistance in writing some of the
explanations in this paper.
Table 2 shows the evaluation results. As in the
perplexity results, the Multi-Class Composite 3-
gram shows the highest performance of all mod-
els, and its error reduction from the conventional
word 3-gram is 16%.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999945275862069">
This paper proposes an effective word clustering
method called Multi-Class. In the Multi-Class
method, multiple classes are assigned to each
word by clustering the following and preceding
word characteristics separately. This word clus-
tering is performed based on the word connec-
tivity in the corpus. Therefore, the Multi-Class
N-grams based on Multi-Class can improve reli-
ability with a compact model size without losing
accuracy.
Furthermore, Multi-Class N-grams are ex-
tended to Multi-Class Composite N-grams. In
the Multi-Class Composite N-grams, higher or-
der word N-grams are introduced through the
grouping of frequent word successions. There-
fore, these have accuracy in higher order word
N-grams added to reliability in the Multi-Class
N-grams. And the number of increased param-
eters with the introduction of word successions
is at most the number of word successions times
2. Therefore, Multi-Class Composite 3-grams can
maintain a compact model size in the Multi-Class
N-grams. Nevertheless, Multi-Class Composite
3-grams are represented by the usual formation
of 3-grams. This formation is easily handled by a
language processor, especially that requires huge
calculation cost as speech recognitions.
In experiments, the Multi-Class Composite 3-
gram resulted in 9.5% lower perplexity and 16%
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941297297297">
Shuanghu Bai, Haizhou Li, and Baosheng Yuan.
1998. Building class-based language models with
contextual statistics. In Proc. ICASSP, pages 173–
176.
P.F. Brown, V.J.D. Pietra, P.V. de Souza, J.C. Lai, and
R.L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467–479.
Sabine Deligne and Frederic Bimbot. 1995. Language
modeling by variable length sequences. Proc.
ICASSP, pages 169–172.
Hirokazu Masataki, Shoichi Matsunaga, and Yosinori
Sagusaka. 1996. Variable-order n-gram genera-
tion by word-class splitting and consecutive word
grouping. Proc. ICASSP, pages 188–191.
M. Ostendorf and H. Singer. 1997. HMM topol-
ogy design using maximum likelihood successive
state splitting. Computer Speech and Language,
11(1):17–41.
Tohru Shimizu, Hirofumi Yamamoto, Hirokazu Masa-
taki, Shoichi Matsunaga, and Yoshinori Sagusaka.
1996. Spontaneous dialogue speech recognition
using cross-word context constrained word graphs.
Proc. ICASSP, pages 145–148.
Toshiyuki Takezawa, Tsuyoshi Morimoto, and Yoshi-
nori Sagisaka. 1998. Speech and language
databases for speech translation research in ATR.
In Proc. of the 1st International Workshop on East-
Asian Language Resource and Evaluation, pages
148–155.
Hirofumi Yamamoto and Yoshinori Sagisaka. 1999.
Multi-class composite n-gram based on connection
direction. Proc. ICASSP, pages 533–536.
S. Zhang, H. Singer, D. Wu, and Y. Sagisaka. 1999.
Improving n-gram modeling using distance-related
unit association maximum entropy language mod-
eling. In Proc. EuroSpeech, pages 1611–1614.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591200">
<title confidence="0.9995835">Multi-Class Composite N-gram Language Model for Spoken Language Processing Using Multiple Word Clusters</title>
<author confidence="0.992806">Hirofumi Yamamoto Shuntaro Isogai Yoshinori Sagisaka</author>
<affiliation confidence="0.97805">ATR SLT Waseda University GITI / ATR SLT</affiliation>
<address confidence="0.8759785">2-2-2 Hikaridai Seika-cho 3-4-1 Okubo, Shinjuku-ku 1-3-10 Nishi-Waseda Soraku-gun, Kyoto-fu, Japan Tokyo-to, Japan Shinjuku-ku, Tokyo-to, Japan</address>
<email confidence="0.803793">yama@slt.atr.co.jpisogai@shirai.info.waseda.ac.jpsagisaka@slt.atr.co.jp</email>
<abstract confidence="0.995185653846154">In this paper, a new language model, the Multi-Class Composite N-gram, is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data. The Multi-Class Composite N-gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi- Classes. In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent the positional attributes. Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams. In experiments, the Multi-Class Composite N-grams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shuanghu Bai</author>
<author>Haizhou Li</author>
<author>Baosheng Yuan</author>
</authors>
<title>Building class-based language models with contextual statistics.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>173--176</pages>
<contexts>
<context position="8294" citStr="Bai et al., 1998" startWordPosition="1340" endWordPosition="1343">r N-grams. The POS information does not accurately represent the statistical word connectivity characteristics. Better word-clustering is to be considered based on word connectivity by the reflection neighboring characteristics in the corpus. In this paper, vectors are used to represent word neighboring characteristics. The elements of the vectors are forward or backward word 2-gram probabilities to the clustering target word after being smoothed. And we consider that word pairs that have a small distance between vectors also have similar word neighboring characteristics (Brown et al., 1992) (Bai et al., 1998). In this method, the same vector is assigned to words that do not appear in the corpus, and the same word cluster will be assigned to these words. To avoid excessively rough clustering over different POS, we cluster the words under the condition that only words with the same POS can belong to the same cluster. Parts-ofspeech that have the same connectivity in each Multi-Class are merged. For example, if different parts-of-speeche are assigned to ”a” and ”an”, these parts-of-speeche are regarded as the same for the preceding word cluster. Word clustering is thus performed in the following mann</context>
</contexts>
<marker>Bai, Li, Yuan, 1998</marker>
<rawString>Shuanghu Bai, Haizhou Li, and Baosheng Yuan. 1998. Building class-based language models with contextual statistics. In Proc. ICASSP, pages 173– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>P V de Souza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>P.F. Brown, V.J.D. Pietra, P.V. de Souza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Frederic Bimbot</author>
</authors>
<title>Language modeling by variable length sequences.</title>
<date>1995</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>169--172</pages>
<contexts>
<context position="12185" citStr="Deligne and Bimbot, 1995" startWordPosition="2014" endWordPosition="2017">eason. For the estimation of words A and D, it is reasonable to use the value of the class 2-gram, since the value of the word N-gram is unreliable (note that the frequency of word sequences (X, A) and (C, D) is insufficient). Based on this idea, the transition probability of word sequence (A, B, C, D) from word X is given in the next equation in the Multi-Class 2-gram. P = p(ct(A)jcf(X))p(Ajct(A))) X p(BjA) X p(CjA, B) X p(ct(D)jcf(C))p(Djct(D)) (9) When word succession A+B+C is introduced as a variable length word sequence (A, B, C), equation (9) can be changed exactly to the next equation (Deligne and Bimbot, 1995) (Masataki et al., 1996). P = p(ct(A)jcf(X))p(A + B + Cjct(A)) X p(ct(D)jcf(C))p(Djct(D)) (10) Here, we find the following properties. The preceding word connectivity of word succession A+ B + C is the same as the connectivity of word A, the first word of A + B + C. The following connectivity is the same as the last word C. In these assignments, no new cluster is required. But conventional class N-grams require a new cluster for the new word succession. ct(A+B + C) = ct(A) (11) P = p(ct(A + B + C)jcf(X)) X p(A + B + Cjct(A + B + C)) X p(ct(D)jcf(A + B + C)) X p(Djct(D)) (13) Equation(13) means</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>Sabine Deligne and Frederic Bimbot. 1995. Language modeling by variable length sequences. Proc. ICASSP, pages 169–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirokazu Masataki</author>
<author>Shoichi Matsunaga</author>
<author>Yosinori Sagusaka</author>
</authors>
<title>Variable-order n-gram generation by word-class splitting and consecutive word grouping.</title>
<date>1996</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>188--191</pages>
<contexts>
<context position="12209" citStr="Masataki et al., 1996" startWordPosition="2018" endWordPosition="2021">f words A and D, it is reasonable to use the value of the class 2-gram, since the value of the word N-gram is unreliable (note that the frequency of word sequences (X, A) and (C, D) is insufficient). Based on this idea, the transition probability of word sequence (A, B, C, D) from word X is given in the next equation in the Multi-Class 2-gram. P = p(ct(A)jcf(X))p(Ajct(A))) X p(BjA) X p(CjA, B) X p(ct(D)jcf(C))p(Djct(D)) (9) When word succession A+B+C is introduced as a variable length word sequence (A, B, C), equation (9) can be changed exactly to the next equation (Deligne and Bimbot, 1995) (Masataki et al., 1996). P = p(ct(A)jcf(X))p(A + B + Cjct(A)) X p(ct(D)jcf(C))p(Djct(D)) (10) Here, we find the following properties. The preceding word connectivity of word succession A+ B + C is the same as the connectivity of word A, the first word of A + B + C. The following connectivity is the same as the last word C. In these assignments, no new cluster is required. But conventional class N-grams require a new cluster for the new word succession. ct(A+B + C) = ct(A) (11) P = p(ct(A + B + C)jcf(X)) X p(A + B + Cjct(A + B + C)) X p(ct(D)jcf(A + B + C)) X p(Djct(D)) (13) Equation(13) means that if the frequency o</context>
</contexts>
<marker>Masataki, Matsunaga, Sagusaka, 1996</marker>
<rawString>Hirokazu Masataki, Shoichi Matsunaga, and Yosinori Sagusaka. 1996. Variable-order n-gram generation by word-class splitting and consecutive word grouping. Proc. ICASSP, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>H Singer</author>
</authors>
<title>HMM topology design using maximum likelihood successive state splitting.</title>
<date>1997</date>
<journal>Computer Speech and Language,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="21001" citStr="Ostendorf and Singer, 1997" startWordPosition="3578" endWordPosition="3581">age processing. We have evaluated the proposed model in continuous speech recognition. The experimental conditions are as follows: • Evaluation set • Decoder (Shimizu et al., 1996) – 1st pass: frame-synchronized viterbi search (W: Number of correct words, D: Deletion error, I : Insertion error, S: Substitution error) —— % W D S Correct = W X 100 – The same 42 conversations as used in the evaluation of perplexity • Acoustic features – Sampling rate 16kHz – Frame shift 10msec – Mel-cepstrum 12 + power and their delta, total 26 • Acoustic models – 800-state 5-mixture HMnet model based on ML-SSS (Ostendorf and Singer, 1997) – Automatic selection of gender dependent models – 2nd pass: full search after changing the language model and LM scale The Multi-Class Composite 2-gram and 3- gram are compared with those of the word 2- gram, Multi-Class 2-gram, word 3-gram and Multi-Class 3-gram. The number of classes is 1,200 through all class-based models. For the evaluation of each 2-gram, a 2-gram is used at both the 1st and the 2nd pass in decoder. For the 3-gram, each 2-gram is changed to the corresponding 3-gram in the 2nd pass. The evaluation measures are conventional word accuracy and %correct calculated as follows</context>
</contexts>
<marker>Ostendorf, Singer, 1997</marker>
<rawString>M. Ostendorf and H. Singer. 1997. HMM topology design using maximum likelihood successive state splitting. Computer Speech and Language, 11(1):17–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tohru Shimizu</author>
<author>Hirofumi Yamamoto</author>
<author>Hirokazu Masataki</author>
<author>Shoichi Matsunaga</author>
<author>Yoshinori Sagusaka</author>
</authors>
<title>Spontaneous dialogue speech recognition using cross-word context constrained word graphs.</title>
<date>1996</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>145--148</pages>
<contexts>
<context position="20554" citStr="Shimizu et al., 1996" startWordPosition="3499" endWordPosition="3502">luation results are shown in Table 1. Table 1 shows that the Multi-Class Composite 3-gram results in 9.5% lower perplexity with a 40% smaller parameter size than the conventional word 3-gram, and that it is in fact a compact and high-performance model. 5.3 Evaluation in Continuous Speech Recognition Though perplexity is a good measure for the performance of language models, it does not always have a direct bearing on performance in language processing. We have evaluated the proposed model in continuous speech recognition. The experimental conditions are as follows: • Evaluation set • Decoder (Shimizu et al., 1996) – 1st pass: frame-synchronized viterbi search (W: Number of correct words, D: Deletion error, I : Insertion error, S: Substitution error) —— % W D S Correct = W X 100 – The same 42 conversations as used in the evaluation of perplexity • Acoustic features – Sampling rate 16kHz – Frame shift 10msec – Mel-cepstrum 12 + power and their delta, total 26 • Acoustic models – 800-state 5-mixture HMnet model based on ML-SSS (Ostendorf and Singer, 1997) – Automatic selection of gender dependent models – 2nd pass: full search after changing the language model and LM scale The Multi-Class Composite 2-gram</context>
</contexts>
<marker>Shimizu, Yamamoto, Masataki, Matsunaga, Sagusaka, 1996</marker>
<rawString>Tohru Shimizu, Hirofumi Yamamoto, Hirokazu Masataki, Shoichi Matsunaga, and Yoshinori Sagusaka. 1996. Spontaneous dialogue speech recognition using cross-word context constrained word graphs. Proc. ICASSP, pages 145–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Tsuyoshi Morimoto</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Speech and language databases for speech translation research in ATR.</title>
<date>1998</date>
<booktitle>In Proc. of the 1st International Workshop on EastAsian Language Resource and Evaluation,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="18046" citStr="Takezawa et al., 1998" startWordPosition="3093" endWordPosition="3096">o words of the word successions is at most the number of word successions. Therefore, the number of increased parameters in the Multi-Class Composite 3-gram is at most the number of introduced word successions times 2. 5 Evaluation Experiments 5.1 Evaluation of Multi-Class N-grams We have evaluated Multi-Class N-grams in perplexity as the next equations. 1 � lo92(p(wi)) (23) Entropy = N . Perplexity = 2Entropy (24) The Good-Turing discount is used for smoothing. The perplexity is compared with those of word 2-grams and word 3-grams. The evaluation data set is the ATR Spoken Language Database (Takezawa et al., 1998). The total number of words in the training set is 1,387,300, the vocabulary size is 16,531, and 5,880 words in 42 conversations which are not included in the training set are used for the evaluation. Figure1 shows the perplexity of Multi-Class 2- grams for each number of classes. In the MultiClass, the numbers of following and preceding classes are fixed to the same value just for comparison. As shown in the figure, the Multi-Class 2-gram with 1,200 classes gives the lowest perplexity of 22.70, and it is smaller than the 23.93 in the conventional word 2-gram. Figure 2 shows the perplexity of </context>
</contexts>
<marker>Takezawa, Morimoto, Sagisaka, 1998</marker>
<rawString>Toshiyuki Takezawa, Tsuyoshi Morimoto, and Yoshinori Sagisaka. 1998. Speech and language databases for speech translation research in ATR. In Proc. of the 1st International Workshop on EastAsian Language Resource and Evaluation, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Multi-class composite n-gram based on connection direction.</title>
<date>1999</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>533--536</pages>
<contexts>
<context position="7278" citStr="Yamamoto and Sagisaka, 1999" startWordPosition="1176" endWordPosition="1179">e preceding word connectivity, if assigning different word classes to represent the following word connectivity at the same time. To apply these word class definitions to formula (2), the next formula is given. p(ct ijcfN�1 iN+1; :::; cf2 i2; cf1 i1)p(wijct i) (3) In the above formula, cti represents the word class in the target position to which the word w i belongs, and cfN i represents the word class in the N-th position in a conditional word sequence. We call this multiple word class definition, a Multi-Class. Similarly, we call class N-grams based on the Multi-Class, Multi-Class N-grams (Yamamoto and Sagisaka, 1999). 3 Automatic Extraction of Word probability of the succeeding class-word 2- Clusters gram or word 2-gram, while pf is the same for the preceding one. 3.1 Word Clustering for Multi-Class 2-grams For word clustering in class N-grams, POS information is sometimes used. Though POS information can be used for words that do not appear in the corpus, this is not always an optimal word classification for N-grams. The POS information does not accurately represent the statistical word connectivity characteristics. Better word-clustering is to be considered based on word connectivity by the reflection n</context>
</contexts>
<marker>Yamamoto, Sagisaka, 1999</marker>
<rawString>Hirofumi Yamamoto and Yoshinori Sagisaka. 1999. Multi-class composite n-gram based on connection direction. Proc. ICASSP, pages 533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhang</author>
<author>H Singer</author>
<author>D Wu</author>
<author>Y Sagisaka</author>
</authors>
<title>Improving n-gram modeling using distance-related unit association maximum entropy language modeling.</title>
<date>1999</date>
<booktitle>In Proc. EuroSpeech,</booktitle>
<pages>1611--1614</pages>
<contexts>
<context position="10693" citStr="Zhang et al., 1999" startWordPosition="1744" endWordPosition="1747">ases to V 2 from V in 2- grams, and the length of the vector in the succeeding class also increase to V 2. Therefore, efficient word clustering is needed to keep the reliability of 3-grams after the clustering and a reasonable calculation cost. To avoid losing the reliability caused by the data sparseness of the word pair in the history of 3-grams, approximation is employed using distance-2 2-grams. The authority of this approximation is based on a report that the association of word 2-grams and distance-2 2-grams based on the maximum entropy method gives a good approximation of word 3-grams (Zhang et al., 1999). The vector for clustering is given in the next equation. vf2(x) = [pf 2(w1jx); pf 2(w2jx); :::; pf2(wNjx)] (8) cf(A + B + C) = cf(C) (12) Applying these relations to equation (10), the next equation is obtained. Where, pf 2(yjx) represents the distance-2 2-gram value from word x to word y. And the POS constraints for clustering are the same as in the clustering for preceding words. 4 Multi-Class Composite N-grams 4.1 Multi-Class Composite 2-grams Introducing Variable Length Word Sequences Let’s consider the condition such that only word sequence (A, B, C) has sufficient frequency in sequence</context>
</contexts>
<marker>Zhang, Singer, Wu, Sagisaka, 1999</marker>
<rawString>S. Zhang, H. Singer, D. Wu, and Y. Sagisaka. 1999. Improving n-gram modeling using distance-related unit association maximum entropy language modeling. In Proc. EuroSpeech, pages 1611–1614.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>