<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.933922">
Semantic Role Labeling Using Different Syntactic Views*
</title>
<author confidence="0.910313">
Daniel Jurafsky
</author>
<affiliation confidence="0.98905">
Department of Linguistics,
</affiliation>
<address confidence="0.8048555">
Stanford University,
Stanford, CA 94305
</address>
<email confidence="0.998529">
jurafsky@stanford.edu
</email>
<author confidence="0.9634145">
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
</author>
<affiliation confidence="0.9965565">
Center for Spoken Language Research,
University of Colorado,
</affiliation>
<address confidence="0.820227">
Boulder, CO 80303
</address>
<email confidence="0.998695">
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
</email>
<sectionHeader confidence="0.995635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997896">
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
</bodyText>
<sectionHeader confidence="0.999329" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993624358974359">
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
* This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al., 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al., 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system’s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system’s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
</bodyText>
<page confidence="0.973396">
581
</page>
<note confidence="0.991585">
Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999862944444444">
features, including features extracted from a parser
that provides a different syntactic view – a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views – one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser – Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
</bodyText>
<sectionHeader confidence="0.98134" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.999751">
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al., 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al., 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al.,
2003; Pradhan et al., 2003; Pradhan et al., 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
</bodyText>
<footnote confidence="0.999560666666667">
1http://www.cis.upenn.edu/˜ace/
2http://chasen.org/˜taku/software/TinySVM/
3http://chasen.org/˜taku/software/yamcha/
</footnote>
<equation confidence="0.915350272727273">
S
� � � � ����
NP VP
� � � ���
VBD VP
�� � ���
VBN PP
in � �September
���
ARGM−TMP
[ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September].
</equation>
<figureCaption confidence="0.9990515">
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
</figureCaption>
<bodyText confidence="0.965619090909091">
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al., (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
</bodyText>
<table confidence="0.99808452">
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the headword
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
</table>
<tableCaption confidence="0.999828">
Table 1: Features used in the Baseline system
</tableCaption>
<bodyText confidence="0.99833675">
As described in (Pradhan et al., 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
</bodyText>
<listItem confidence="0.9850895">
• Argument Identification – This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
</listItem>
<equation confidence="0.591757857142857">
�� � � ����
The acquisition
ARG1
was
NULL
completed
predicate
</equation>
<page confidence="0.987259">
582
</page>
<listItem confidence="0.975578">
• Argument Classification – Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
• Argument Identification and Classification –
A combination of the above two tasks.
</listItem>
<table confidence="0.9969425">
ALL ARGs Task P R Fl A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
</table>
<tableCaption confidence="0.809186333333333">
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
</tableCaption>
<bodyText confidence="0.994083565217392">
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors – about
7% are due to missing constituents, and the other
half – about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
</bodyText>
<sectionHeader confidence="0.979591" genericHeader="method">
3 Additional Features
</sectionHeader>
<subsectionHeader confidence="0.991285">
3.1 CCG Parse Features
</subsectionHeader>
<bodyText confidence="0.9997935">
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al., 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
</bodyText>
<listItem confidence="0.865830307692308">
• Phrase type – This is the category of the max-
imal projection between the two words – the
predicate and the dependent word.
• Categorial Path – This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
• Tree Path – This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
</listItem>
<bodyText confidence="0.999831823529412">
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
</bodyText>
<subsectionHeader confidence="0.99888">
3.2 Other Features
</subsectionHeader>
<bodyText confidence="0.9999935">
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
</bodyText>
<footnote confidence="0.9637835">
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
</footnote>
<page confidence="0.993802">
583
</page>
<table confidence="0.997956833333333">
ALL ARGs Task P R Fr
( %) ( %)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
</table>
<tableCaption confidence="0.9028185">
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
</tableCaption>
<bodyText confidence="0.891169">
an important feature in argument identification (Ha-
cioglu et al., 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
</bodyText>
<tableCaption confidence="0.686953">
Table 4
</tableCaption>
<table confidence="0.999585409090909">
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an “*”.
For example, the path NP↑S↑VP↑SBAR↑NP↑VP↓VBD
becomes NP↑S↑*S↑*↑*↓VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP↑S↑S↓VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP↑S↑NP↑VP↓VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP↑S↑VP↑SBAR↑NP↑VP↓VBD becomes:
NP↑S↑VP, S↑VP↑SBAR, VP↑SBAR↑NP, SBAR↑NP↑VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
</table>
<tableCaption confidence="0.995697">
Table 4: Other Features
</tableCaption>
<sectionHeader confidence="0.954251" genericHeader="method">
4 Feature Selection and Calibration
</sectionHeader>
<bodyText confidence="0.999829875">
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the x2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
</bodyText>
<table confidence="0.998914166666667">
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
</table>
<tableCaption confidence="0.883951">
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
</tableCaption>
<bodyText confidence="0.999318666666667">
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al.,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt’s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARCM-TmP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
</bodyText>
<page confidence="0.98882">
584
</page>
<figure confidence="0.998242107142857">
Before Calibration After Calibration
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
True Probability
True Probability
0
Predicted Probability Predicted Probability
</figure>
<figureCaption confidence="0.804555">
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
</figureCaption>
<table confidence="0.9258564">
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
</table>
<tableCaption confidence="0.991676">
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
</tableCaption>
<sectionHeader confidence="0.985939" genericHeader="method">
5 Alternative Syntactic Views
</sectionHeader>
<bodyText confidence="0.999779863636363">
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can’t recover from the situation
where the parse tree being used for classification
doesn’t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al., 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
</bodyText>
<subsectionHeader confidence="0.997486">
5.1 Minipar-based Semantic Labeler
</subsectionHeader>
<bodyText confidence="0.999753703703704">
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar’s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
</bodyText>
<page confidence="0.997779">
585
</page>
<table confidence="0.841083181818182">
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
</table>
<tableCaption confidence="0.9703355">
Table 7: Features used in the Baseline system using
Minipar parses.
</tableCaption>
<table confidence="0.99921975">
Task P R Fl
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
</table>
<tableCaption confidence="0.9950485">
Table 8: Baseline system performance on all tasks
using Minipar parses.
</tableCaption>
<bodyText confidence="0.978628375">
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
on the PropBank training data.
</bodyText>
<table confidence="0.999478344827586">
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ”before”, ”after” and ”-” (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
</table>
<tableCaption confidence="0.997832">
Table 10: Features used by chunk based classifier.
</tableCaption>
<bodyText confidence="0.97979625">
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
</bodyText>
<figure confidence="0.916464888888889">
P
(%)
Fr
R
(%)
Id. and Classification
72.6
66.9
69.6
</figure>
<table confidence="0.936328333333333">
Task P R Fl
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
</table>
<tableCaption confidence="0.99532825">
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
Table 9: Head-word based performance using Char-
niak and Minipar parses.
</tableCaption>
<subsectionHeader confidence="0.999618">
5.2 Chunk-based Semantic Labeler
</subsectionHeader>
<bodyText confidence="0.9981095">
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al., 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
</bodyText>
<footnote confidence="0.9999655">
5http://chasen.org/˜taku/software/TinySVM/
6http://chasen.org/˜taku/software/yamcha/
</footnote>
<page confidence="0.99677">
586
</page>
<sectionHeader confidence="0.901854" genericHeader="method">
6 Combining Semantic Labelers
</sectionHeader>
<bodyText confidence="0.999418428571429">
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
</bodyText>
<table confidence="0.99787875">
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
</table>
<tableCaption confidence="0.99197">
Table 12: Constituent-based best system perfor-
</tableCaption>
<bodyText confidence="0.912354909090909">
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
</bodyText>
<table confidence="0.972344090909091">
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
</table>
<tableCaption confidence="0.988241">
Table 13: Performance improvement on parses
</tableCaption>
<bodyText confidence="0.976027304347826">
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
</bodyText>
<table confidence="0.9997635">
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+Ca Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+Ca Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
</table>
<tableCaption confidence="0.991058">
Table 14: Performance improvement on head word
</tableCaption>
<bodyText confidence="0.7856224">
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
</bodyText>
<table confidence="0.9997784">
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+Ca Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+Ca Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
</table>
<tableCaption confidence="0.897613333333333">
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
</tableCaption>
<page confidence="0.996473">
587
</page>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999971888888889">
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999838647058824">
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger – IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959102941176">
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132–139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303–313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings ofACL, pages 512–520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings ofACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task – Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings ofHLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335–
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings ofLREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142–144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation ofParsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings ofHLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
ofACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings ofEMNLP, Barcelona, Spain.
</reference>
<page confidence="0.997338">
588
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903113">
<title confidence="0.999291">Role Labeling Using Different Syntactic</title>
<author confidence="0.999794">Daniel Jurafsky</author>
<affiliation confidence="0.9999565">Department of Linguistics, Stanford University,</affiliation>
<address confidence="0.999023">Stanford, CA 94305</address>
<email confidence="0.99975">jurafsky@stanford.edu</email>
<author confidence="0.974536">Sameer Pradhan</author>
<author confidence="0.974536">Wayne Ward</author>
<author confidence="0.974536">Kadri Hacioglu</author>
<author confidence="0.974536">James H Martin</author>
<affiliation confidence="0.9996455">Center for Spoken Language Research, University of Colorado,</affiliation>
<address confidence="0.99998">Boulder, CO 80303</address>
<abstract confidence="0.997970346153846">Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R E Barlow</author>
<author>D J Bartholomew</author>
<author>J M Bremmer</author>
<author>H D Brunk</author>
</authors>
<title>Statistical Inference under Order Restrictions.</title>
<date>1972</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="15701" citStr="Barlow et al., 1972" startWordPosition="2476" endWordPosition="2479">fiers used the same set of features, fitting all scores to a single sigmoid was found to give the best performance. Since different feature sets are now used by the classifiers, we trained a separate sigmoid for each classifier. Raw Scores Probabilities After lattice-rescoring Uncalibrated Calibrated (%) (%) (%) Same Feat. same sigmoid 74.7 74.7 75.4 Selected Feat. diff. sigmoids 75.4 75.1 76.2 Table 5: Performance improvement on selecting features per argument and calibrating the probabilities on 10k training data. Foster and Stine (2004) show that the pooladjacent-violators (PAV) algorithm (Barlow et al., 1972) provides a better method for converting raw classifier scores to probabilities when Platt’s algorithm fails. The probabilities resulting from either conversions may not be properly calibrated. So, we binned the probabilities and trained a warping function to calibrate them. For each argument classifier, we used both the methods for converting raw SVM scores into probabilities and calibrated them using a development set. Then, we visually inspected the calibrated plots for each classifier and chose the method that showed better calibration as the calibration procedure for that classifier. Plot</context>
</contexts>
<marker>Barlow, Bartholomew, Bremmer, Brunk, 1972</marker>
<rawString>R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statistical Inference under Order Restrictions. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="4065" citStr="Charniak, 2000" startWordPosition="624" endWordPosition="625">luding features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected Tr</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistics features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1783" citStr="Chen and Rambow, 2003" startWordPosition="260" endWordPosition="263">tactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%. On th</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistics features for the recognition and labeling of semantic arguments. In Proceedings of the EMNLP, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dean P Foster</author>
<author>Robert A Stine</author>
</authors>
<title>Variable selection in data mining: building a predictive model for bankruptcy.</title>
<date>2004</date>
<journal>Journal of American Statistical Association,</journal>
<volume>99</volume>
<pages>303--313</pages>
<contexts>
<context position="15626" citStr="Foster and Stine (2004)" startWordPosition="2465" endWordPosition="2468">ert the SVM scores into probabilities by fitting to a sigmoid. When all classifiers used the same set of features, fitting all scores to a single sigmoid was found to give the best performance. Since different feature sets are now used by the classifiers, we trained a separate sigmoid for each classifier. Raw Scores Probabilities After lattice-rescoring Uncalibrated Calibrated (%) (%) (%) Same Feat. same sigmoid 74.7 74.7 75.4 Selected Feat. diff. sigmoids 75.4 75.1 76.2 Table 5: Performance improvement on selecting features per argument and calibrating the probabilities on 10k training data. Foster and Stine (2004) show that the pooladjacent-violators (PAV) algorithm (Barlow et al., 1972) provides a better method for converting raw classifier scores to probabilities when Platt’s algorithm fails. The probabilities resulting from either conversions may not be properly calibrated. So, we binned the probabilities and trained a warping function to calibrate them. For each argument classifier, we used both the methods for converting raw SVM scores into probabilities and calibrated them using a development set. Then, we visually inspected the calibrated plots for each classifier and chose the method that showe</context>
</contexts>
<marker>Foster, Stine, 2004</marker>
<rawString>Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining: building a predictive model for bankruptcy. Journal of American Statistical Association, 99, pages 303–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying semantic roles using combinatory categorial grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1813" citStr="Gildea and Hockenmaier, 2003" startWordPosition="264" endWordPosition="267"> chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%. On the other hand, the system’s per</context>
<context position="9899" citStr="Gildea and Hockenmaier (2003)" startWordPosition="1548" endWordPosition="1551">feature selection and calibration, and iii) combining parses from different syntactic representations. 3 Additional Features 3.1 CCG Parse Features While the Path feature has been identified to be very important for the argument identification task, it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al., 2004; Xue and Palmer, 2004). A dependency grammar should generate shorter paths from the predicate to dependent words in the sentence, and could be a more robust complement to the phrase structure grammar paths extracted from the Charniak parse tree. Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. We evaluated features from a CCG parser combined with our baseline feature set. We used three features that were introduced by Gildea and Hockenmaier (2003): • Phrase type – This is the category of the maximal projection between the two words – the predicate and the dependent word. • Categorial Path – This is a feature formed by concatenating the following three values: i) category to which the dependent word belongs, ii) the direction of de</context>
<context position="21083" citStr="Gildea and Hockenmaier (2003)" startWordPosition="3350" endWordPosition="3354">OSITION Table 7: Features used in the Baseline system using Minipar parses. Task P R Fl (%) (%) Id. 73.5 43.8 54.6 Id. + Classification 66.2 36.7 47.2 Table 8: Baseline system performance on all tasks using Minipar parses. ments do not have corresponding constituents that match its boundaries. In experiments reported by Hacioglu (Hacioglu, 2004), a mismatch of about 8% was introduced in the transformation from handcorrected constituent trees to dependency trees. Using an errorful automatically generated tree, a still higher mismatch would be expected. In case of the CCG parses, as reported by Gildea and Hockenmaier (2003), the mismatch was about 23%. A more realistic way to score the performance is to score tags assigned to head words of constituents, rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003). The results for this system are shown in Table 9. on the PropBank training data. WORDS PREDICATE LEMMAS PART OF SPEECH TAGS BP POSITIONS: The position of a token in a BP using the IOB2 representation (e.g. B-NP, I-NP, O, etc.) CLAUSE TAGS: The tags that mark token positions in a sentence with respect to clauses. NAMED ENTITIES: The IOB tags of named enti</context>
</contexts>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using combinatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>512--520</pages>
<location>Hong Kong,</location>
<contexts>
<context position="1660" citStr="Gildea and Jurafsky, 2000" startWordPosition="239" endWordPosition="242">ituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system pe</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles. In Proceedings ofACL, pages 512–520, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1687" citStr="Gildea and Jurafsky, 2002" startWordPosition="243" endWordPosition="246">e correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline sys</context>
<context position="6090" citStr="Gildea and Jurafsky (2002)" startWordPosition="941" endWordPosition="944">en.org/˜taku/software/TinySVM/ 3http://chasen.org/˜taku/software/yamcha/ S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. PREDICATE LEMMA PATH: Path from the constituent to the predicate in the parse tree. POSITION: Whether the constituent is before or after the predicate. VOICE PREDICATE SUB-CATEGORIZATION PREDICATE CLUSTER HEAD WORD: Head word of the constituent. HEAD WORD POS: POS of the headword NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the </context>
<context position="14257" citStr="Gildea and Jurafsky, 2002" startWordPosition="2250" endWordPosition="2253">uent being classified. Table 4: Other Features 4 Feature Selection and Calibration In the baseline system, we used the same set of features for all the n binary ONE VS ALL classifiers. Error analysis showed that some features specifically suited for one argument class, for example, core arguments, tend to hurt performance on some adjunctive arguments. Therefore, we thought that selecting subsets of features for each argument class might improve performance. To achieve this, we performed a simple feature selection procedure. For each argument, we started with the set of features introduced by (Gildea and Jurafsky, 2002). We pruned this set by training classifiers after leaving out one feature at a time and checking its performance on a development set. We used the x2 significance while making pruning decisions. Following that, we added each of the other features one at a time to the pruned baseline set of features and selected ones that showed significantly improved performance. Since the feature selection experiments were computationally intensive, we performed them using 10k training examples. SVMs output distances not probabilities. These distances may not be comparable across classifiers, especially if d</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of syntactic parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1712" citStr="Gildea and Palmer, 2002" startWordPosition="247" endWordPosition="250">r to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the tas</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for predicate argument recognition. In Proceedings ofACL, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>Semantic role labeling using dependency trees.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1852" citStr="Hacioglu, 2004" startWordPosition="272" endWordPosition="273">baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%. On the other hand, the system’s performance on the identification task is </context>
<context position="4215" citStr="Hacioglu, 2004" startWordPosition="647" endWordPosition="648">man, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE </context>
<context position="19310" citStr="Hacioglu, 2004" startWordPosition="3060" endWordPosition="3061">outputs dependencies between a word called head and another called modifier. Each word can modify at most one word. The dependency relationships form a dependency tree. The set of words under each node in Minipar’s dependency tree form a contiguous segment in the original sentence and correspond to the constituent in a constituent tree. We formulate the semantic labeling problem in the same way as in a constituent structure parse, except we classify the nodes that represent head words of constituents. A similar formulation using dependency trees derived from TreeBank was reported in Hacioglu (Hacioglu, 2004). In that experiment, the dependency trees were derived from hand-corrected TreeBank trees using head word rules. Here, an SVM is trained to assign PropBank argument labels to nodes in Minipar dependency trees using the following features: Table 8 shows the performance of the Miniparbased semantic parser. Minipar performance on the PropBank corpus is substantially worse than the Charniak based system. This is understandable from the fact that Minipar is not designed to produce constituents that would exactly match the constituent segmentation used in TreeBank. In the test set, about 37% of the</context>
<context position="20801" citStr="Hacioglu, 2004" startWordPosition="3307" endWordPosition="3308">the tree. DEPENDENCY PATH: Each word that is connected to the head word has a particular dependency relationship to the word. These are represented as labels on the arc between the words. This feature is the dependencies along the path that connects two words. VOICE POSITION Table 7: Features used in the Baseline system using Minipar parses. Task P R Fl (%) (%) Id. 73.5 43.8 54.6 Id. + Classification 66.2 36.7 47.2 Table 8: Baseline system performance on all tasks using Minipar parses. ments do not have corresponding constituents that match its boundaries. In experiments reported by Hacioglu (Hacioglu, 2004), a mismatch of about 8% was introduced in the transformation from handcorrected constituent trees to dependency trees. Using an errorful automatically generated tree, a still higher mismatch would be expected. In case of the CCG parses, as reported by Gildea and Hockenmaier (2003), the mismatch was about 23%. A more realistic way to score the performance is to score tags assigned to head words of constituents, rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003). The results for this system are shown in Table 9. on the PropBank training</context>
</contexts>
<marker>Hacioglu, 2004</marker>
<rawString>Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
</authors>
<title>Target word detection and semantic role chunking using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1760" citStr="Hacioglu and Ward, 2003" startWordPosition="256" endWordPosition="259">parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic </context>
</contexts>
<marker>Hacioglu, Ward, 2003</marker>
<rawString>Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role chunking using support vector machines. In Proceedings of HLT/NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2003</date>
<tech>Technical Report TR-CSLR-2003-1,</tech>
<institution>Center for Spoken Language Research,</institution>
<location>Boulder, Colorado.</location>
<contexts>
<context position="5333" citStr="Hacioglu et al., 2003" startWordPosition="823" endWordPosition="826">ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1http://www.cis.upenn.edu/˜ace/ 2http://chasen.org/˜taku/software/TinySVM/ 3http://chasen.org/˜taku/software/yamcha/ S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained,</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling by tagging syntactic chunks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004, Shared Task – Semantic Role Labeling.</booktitle>
<contexts>
<context position="12179" citStr="Hacioglu et al., 2004" startWordPosition="1928" endWordPosition="1932">entioned in parentheses. 3.2 Other Features We added several other features to the system. Position of the clause node (S, SBAR) seems to be 4Many thanks to Julia Hockenmaier for providing us with the CCG bank as well as the StatCCG parser. 583 ALL ARGs Task P R Fr ( %) ( %) HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0) Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4) AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3) Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8) Table 3: Performance improvement upon adding CCG features to the Baseline system. an important feature in argument identification (Hacioglu et al., 2004) therefore we experimented with four clause-based path feature variations. We added the predicate context to capture predicate sense variations. For some adjunctive arguments, punctuation plays an important role, so we added some punctuation features. All the new features are shown in Table 4 CLAUSE-BASED PATH VARIATIONS: I. Replacing all the nodes in a path other than clause nodes with an “*”. For example, the path NP↑S↑VP↑SBAR↑NP↑VP↓VBD becomes NP↑S↑*S↑*↑*↓VBD II. Retaining only the clause nodes in the path, which for the above example would produce NP↑S↑S↓VBD, III. Adding a binary feature t</context>
<context position="18090" citStr="Hacioglu et al., 2004" startWordPosition="2862" endWordPosition="2865">ses. 5 Alternative Syntactic Views Adding new features can improve performance when the syntactic representation being used for classification contains the correct constituents. Additional features can’t recover from the situation where the parse tree being used for classification doesn’t contain the correct constituent representing an argument. Such parse errors account for about 7% absolute of the errors (or, about half of 12.7%) for the Charniak parse based system. To address these errors, we added two additional parse representations: i) Minipar dependency parser, and ii) chunking parser (Hacioglu et al., 2004). The hope is that these parsers will produce different errors than the Charniak parser since they represent different syntactic views. The Charniak parser is trained on the Penn TreeBank corpus. Minipar is a rule based dependency parser. The chunking parser is trained on PropBank and produces a flat syntactic representation that is very different from the full parse tree produced by Charniak. A combination of the three different parses could produce better results than any single one. 5.1 Minipar-based Semantic Labeler Minipar (Lin, 1998; Lin and Pantel, 2001) is a rulebased dependency parser</context>
<context position="23668" citStr="Hacioglu et al., 2004" startWordPosition="3788" endWordPosition="3791"> that have already been assigned to the tokens contained in the linguistic context. A 5-token sliding window is used for the context. P (%) Fr R (%) Id. and Classification 72.6 66.9 69.6 Task P R Fl (%) (%) CHARNIAK Id. 92.2 87.5 89.8 Id. + Classification 85.9 81.6 83.7 MINIPAR Id. 83.3 61.1 70.5 Id. + Classification 72.9 53.5 61.7 Table 11: Semantic chunker performance on the combined task of Id. and classification. Table 9: Head-word based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5http://chasen.o</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Jurafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceedings of CoNLL-2004, Shared Task – Semantic Role Labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>A lightweight semantic chunking model based on tagging.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1852" citStr="Hacioglu, 2004" startWordPosition="272" endWordPosition="273">baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%. On the other hand, the system’s performance on the identification task is </context>
<context position="4215" citStr="Hacioglu, 2004" startWordPosition="647" endWordPosition="648">man, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE </context>
<context position="19310" citStr="Hacioglu, 2004" startWordPosition="3060" endWordPosition="3061">outputs dependencies between a word called head and another called modifier. Each word can modify at most one word. The dependency relationships form a dependency tree. The set of words under each node in Minipar’s dependency tree form a contiguous segment in the original sentence and correspond to the constituent in a constituent tree. We formulate the semantic labeling problem in the same way as in a constituent structure parse, except we classify the nodes that represent head words of constituents. A similar formulation using dependency trees derived from TreeBank was reported in Hacioglu (Hacioglu, 2004). In that experiment, the dependency trees were derived from hand-corrected TreeBank trees using head word rules. Here, an SVM is trained to assign PropBank argument labels to nodes in Minipar dependency trees using the following features: Table 8 shows the performance of the Miniparbased semantic parser. Minipar performance on the PropBank corpus is substantially worse than the Charniak based system. This is understandable from the fact that Minipar is not designed to produce constituents that would exactly match the constituent segmentation used in TreeBank. In the test set, about 37% of the</context>
<context position="20801" citStr="Hacioglu, 2004" startWordPosition="3307" endWordPosition="3308">the tree. DEPENDENCY PATH: Each word that is connected to the head word has a particular dependency relationship to the word. These are represented as labels on the arc between the words. This feature is the dependencies along the path that connects two words. VOICE POSITION Table 7: Features used in the Baseline system using Minipar parses. Task P R Fl (%) (%) Id. 73.5 43.8 54.6 Id. + Classification 66.2 36.7 47.2 Table 8: Baseline system performance on all tasks using Minipar parses. ments do not have corresponding constituents that match its boundaries. In experiments reported by Hacioglu (Hacioglu, 2004), a mismatch of about 8% was introduced in the transformation from handcorrected constituent trees to dependency trees. Using an errorful automatically generated tree, a still higher mismatch would be expected. In case of the CCG parses, as reported by Gildea and Hockenmaier (2003), the mismatch was about 23%. A more realistic way to score the performance is to score tags assigned to head words of constituents, rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003). The results for this system are shown in Table 9. on the PropBank training</context>
</contexts>
<marker>Hacioglu, 2004</marker>
<rawString>Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tagging. In Proceedings ofHLT/NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with combinatory grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>335--342</pages>
<contexts>
<context position="3611" citStr="Hockenmaier and Steedman, 2002" startWordPosition="552" endWordPosition="556">s organized as follows. We first describe a baseline system based on the best published techniques. We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis. In the first set of experiments we explore new 581 Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, </context>
<context position="10912" citStr="Hockenmaier and Steedman, 2002" startWordPosition="1717" endWordPosition="1720"> the two words – the predicate and the dependent word. • Categorial Path – This is a feature formed by concatenating the following three values: i) category to which the dependent word belongs, ii) the direction of dependence and iii) the slot in the category filled by the dependent word. • Tree Path – This is the categorial analogue of the path feature in the Charniak parse based system, which traces the path from the dependent word to the predicate through the binary CCG tree. Parallel to the hand-corrected TreeBank parses, we also had access to correct CCG parses derived from the TreeBank (Hockenmaier and Steedman, 2002a). We performed two sets of experiments. One using the correct CCG parses, and the other using parses obtained using StatCCG4 parser (Hockenmaier and Steedman, 2002). We incorporated these features in the systems based on hand-corrected TreeBank parses and Charniak parses respectively. For each constituent in the Charniak parse tree, if there was a dependency between the head word of the constituent and the predicate, then the corresponding CCG features for those words were added to the features for that constituent. Table 3 shows the performance of the system when these features were added. </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with combinatory grammars. In Proceedings of the ACL, pages 335– 342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="3611" citStr="Hockenmaier and Steedman, 2002" startWordPosition="552" endWordPosition="556">s organized as follows. We first describe a baseline system based on the best published techniques. We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis. In the first set of experiments we explore new 581 Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, </context>
<context position="10912" citStr="Hockenmaier and Steedman, 2002" startWordPosition="1717" endWordPosition="1720"> the two words – the predicate and the dependent word. • Categorial Path – This is a feature formed by concatenating the following three values: i) category to which the dependent word belongs, ii) the direction of dependence and iii) the slot in the category filled by the dependent word. • Tree Path – This is the categorial analogue of the path feature in the Charniak parse based system, which traces the path from the dependent word to the predicate through the binary CCG tree. Parallel to the hand-corrected TreeBank parses, we also had access to correct CCG parses derived from the TreeBank (Hockenmaier and Steedman, 2002a). We performed two sets of experiments. One using the correct CCG parses, and the other using parses obtained using StatCCG4 parser (Hockenmaier and Steedman, 2002). We incorporated these features in the systems based on hand-corrected TreeBank parses and Charniak parses respectively. For each constituent in the Charniak parse tree, if there was a dependency between the head word of the constituent and the predicate, then the corresponding CCG features for those words were added to the features for that constituent. Table 3 shows the performance of the system when these features were added. </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC, Las Palmas, Canary Islands,</booktitle>
<contexts>
<context position="4399" citStr="Kingsbury and Palmer, 2002" startWordPosition="675" endWordPosition="678">lities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings ofLREC, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL and LLL,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="24081" citStr="Kudo and Matsumoto, 2000" startWordPosition="3859" endWordPosition="3862">fication. Table 9: Head-word based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5http://chasen.org/˜taku/software/TinySVM/ 6http://chasen.org/˜taku/software/yamcha/ 586 6 Combining Semantic Labelers We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted. Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were sel</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. In Proceedings of CoNLL and LLL, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="5802" citStr="Kudo and Matsumoto, 2001" startWordPosition="891" endWordPosition="894">ing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1http://www.cis.upenn.edu/˜ace/ 2http://chasen.org/˜taku/software/TinySVM/ 3http://chasen.org/˜taku/software/yamcha/ S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. PREDICATE LEMMA PATH: Path from the constituent to the predicate in the parse tree. POSITION: Whether the constituent is before or after the </context>
<context position="24108" citStr="Kudo and Matsumoto, 2001" startWordPosition="3863" endWordPosition="3866">rd based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5http://chasen.org/˜taku/software/TinySVM/ 6http://chasen.org/˜taku/software/yamcha/ 586 6 Combining Semantic Labelers We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted. Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were selected. In the chunked syste</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="18657" citStr="Lin and Pantel, 2001" startWordPosition="2951" endWordPosition="2954">arser, and ii) chunking parser (Hacioglu et al., 2004). The hope is that these parsers will produce different errors than the Charniak parser since they represent different syntactic views. The Charniak parser is trained on the Penn TreeBank corpus. Minipar is a rule based dependency parser. The chunking parser is trained on PropBank and produces a flat syntactic representation that is very different from the full parse tree produced by Charniak. A combination of the three different parses could produce better results than any single one. 5.1 Minipar-based Semantic Labeler Minipar (Lin, 1998; Lin and Pantel, 2001) is a rulebased dependency parser. It outputs dependencies between a word called head and another called modifier. Each word can modify at most one word. The dependency relationships form a dependency tree. The set of words under each node in Minipar’s dependency tree form a contiguous segment in the original sentence and correspond to the constituent in a constituent tree. We formulate the semantic labeling problem in the same way as in a constituent structure parse, except we classify the nodes that represent head words of constituents. A similar formulation using dependency trees derived fr</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR. In</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation ofParsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="4130" citStr="Lin, 1998" startWordPosition="634" endWordPosition="635">actic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, wher</context>
<context position="18634" citStr="Lin, 1998" startWordPosition="2949" endWordPosition="2950">ependency parser, and ii) chunking parser (Hacioglu et al., 2004). The hope is that these parsers will produce different errors than the Charniak parser since they represent different syntactic views. The Charniak parser is trained on the Penn TreeBank corpus. Minipar is a rule based dependency parser. The chunking parser is trained on PropBank and produces a flat syntactic representation that is very different from the full parse tree produced by Charniak. A combination of the three different parses could produce better results than any single one. 5.1 Minipar-based Semantic Labeler Minipar (Lin, 1998; Lin and Pantel, 2001) is a rulebased dependency parser. It outputs dependencies between a word called head and another called modifier. Each word can modify at most one word. The dependency relationships form a dependency tree. The set of words under each node in Minipar’s dependency tree form a contiguous segment in the original sentence and correspond to the constituent in a constituent tree. We formulate the semantic labeling problem in the same way as in a constituent structure parse, except we classify the nodes that represent head words of constituents. A similar formulation using depe</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop on the Evaluation ofParsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<contexts>
<context position="4571" citStr="Marcus et al., 1994" startWordPosition="705" endWordPosition="708">ntic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 fo</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<note>To appear Computational Linguistics.</note>
<contexts>
<context position="4421" citStr="Palmer et al., 2005" startWordPosition="679" endWordPosition="682">periments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1 shows a syntax tree a</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. To appear Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Probabilities for support vector machines.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers.</booktitle>
<editor>In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14995" citStr="Platt, 2000" startWordPosition="2365" endWordPosition="2366">ment set. We used the x2 significance while making pruning decisions. Following that, we added each of the other features one at a time to the pruned baseline set of features and selected ones that showed significantly improved performance. Since the feature selection experiments were computationally intensive, we performed them using 10k training examples. SVMs output distances not probabilities. These distances may not be comparable across classifiers, especially if different features are used to train each binary classifier. In the baseline system, we used the algorithm described by Platt (Platt, 2000) to convert the SVM scores into probabilities by fitting to a sigmoid. When all classifiers used the same set of features, fitting all scores to a single sigmoid was found to give the best performance. Since different feature sets are now used by the classifiers, we trained a separate sigmoid for each classifier. Raw Scores Probabilities After lattice-rescoring Uncalibrated Calibrated (%) (%) (%) Same Feat. same sigmoid 74.7 74.7 75.4 Selected Feat. diff. sigmoids 75.4 75.1 76.2 Table 5: Performance improvement on selecting features per argument and calibrating the probabilities on 10k trainin</context>
</contexts>
<marker>Platt, 2000</marker>
<rawString>John Platt. 2000. Probabilities for support vector machines. In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Semantic role parsing: Adding semantic structure to unstructured text.</title>
<date>2003</date>
<booktitle>In Proceedings of ICDM,</booktitle>
<location>Melbourne, Florida.</location>
<contexts>
<context position="5355" citStr="Pradhan et al., 2003" startWordPosition="827" endWordPosition="830">0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1http://www.cis.upenn.edu/˜ace/ 2http://chasen.org/˜taku/software/TinySVM/ 3http://chasen.org/˜taku/software/yamcha/ S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of </context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic structure to unstructured text. In Proceedings of ICDM, Melbourne, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1835" citStr="Pradhan et al., 2004" startWordPosition="268" endWordPosition="271">ion with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%. On the other hand, the system’s performance on the identi</context>
<context position="5378" citStr="Pradhan et al., 2004" startWordPosition="831" endWordPosition="834">ARG1 is the PROTO-PATIENT, etc. In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1http://www.cis.upenn.edu/˜ace/ 2http://chasen.org/˜taku/software/TinySVM/ 3http://chasen.org/˜taku/software/yamcha/ S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes includ</context>
<context position="7386" citStr="Pradhan et al., 2004" startWordPosition="1146" endWordPosition="1149">formation from PropBank HEAD WORD OF PP: Head of PP replaced by head word of NP inside it, and PP replaced by PP-preposition FIRST AND LAST WORD/POS IN CONSTITUENT ORDINAL CONSTITUENT POSITION CONSTITUENT TREE DISTANCE CONSTITUENT RELATIVE FEATURES: Nine features representing the phrase type, head word and head word part of speech of the parent, and left and right siblings of the constituent. TEMPORAL CUE WORDS DYNAMIC CLASS CONTEXT SYNTACTIC FRAME CONTENT WORD FEATURES: Content word, its POS and named entities in the content word Table 1: Features used in the Baseline system As described in (Pradhan et al., 2004), we postprocess the n-best hypotheses using a trigram language model of the argument sequence. We analyze the performance on three tasks: • Argument Identification – This is the process of identifying the parsed constituents in the sentence that represent semantic arguments of a given predicate. �� � � ���� The acquisition ARG1 was NULL completed predicate 582 • Argument Classification – Given constituents known to represent arguments of a predicate, assign the appropriate argument labels to them. • Argument Identification and Classification – A combination of the above two tasks. ALL ARGs Ta</context>
<context position="9623" citStr="Pradhan et al., 2004" startWordPosition="1504" endWordPosition="1507"> identification performance for automatic parses, we examined a number of techniques for improving argument identification. We made a number of changes to the system which resulted in improved performance. The changes fell into three categories: i) new features, ii) feature selection and calibration, and iii) combining parses from different syntactic representations. 3 Additional Features 3.1 CCG Parse Features While the Path feature has been identified to be very important for the argument identification task, it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al., 2004; Xue and Palmer, 2004). A dependency grammar should generate shorter paths from the predicate to dependent words in the sentence, and could be a more robust complement to the phrase structure grammar paths extracted from the Charniak parse tree. Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. We evaluated features from a CCG parser combined with our baseline feature set. We used three features that were introduced by Gildea and Hockenmaier (2003): • Phrase ty</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings ofHLT/NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1735" citStr="Surdeanu et al., 2003" startWordPosition="251" endWordPosition="255">, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 1 Introduction Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004). The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role. A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance. The baseline system’s accuracy on the task of labeling nodes kno</context>
<context position="6159" citStr="Surdeanu et al., (2003)" startWordPosition="953" endWordPosition="956"> S � � � � ���� NP VP � � � ��� VBD VP �� � ��� VBN PP in � �September ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. PREDICATE LEMMA PATH: Path from the constituent to the predicate in the parse tree. POSITION: Whether the constituent is before or after the predicate. VOICE PREDICATE SUB-CATEGORIZATION PREDICATE CLUSTER HEAD WORD: Head word of the constituent. HEAD WORD POS: POS of the headword NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. VERB SENSE INFORMATION: Oracle verb se</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings ofACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6226" citStr="Xue and Palmer, 2004" startWordPosition="963" endWordPosition="966">r ��� ARGM−TMP [ARG1 The acquisition] was [predicate completed] [ARGM_TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. PREDICATE LEMMA PATH: Path from the constituent to the predicate in the parse tree. POSITION: Whether the constituent is before or after the predicate. VOICE PREDICATE SUB-CATEGORIZATION PREDICATE CLUSTER HEAD WORD: Head word of the constituent. HEAD WORD POS: POS of the headword NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. VERB SENSE INFORMATION: Oracle verb sense information from PropBank HEAD WORD OF PP: Head of PP replaced </context>
<context position="9646" citStr="Xue and Palmer, 2004" startWordPosition="1508" endWordPosition="1511">mance for automatic parses, we examined a number of techniques for improving argument identification. We made a number of changes to the system which resulted in improved performance. The changes fell into three categories: i) new features, ii) feature selection and calibration, and iii) combining parses from different syntactic representations. 3 Additional Features 3.1 CCG Parse Features While the Path feature has been identified to be very important for the argument identification task, it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al., 2004; Xue and Palmer, 2004). A dependency grammar should generate shorter paths from the predicate to dependent words in the sentence, and could be a more robust complement to the phrase structure grammar paths extracted from the Charniak parse tree. Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. We evaluated features from a CCG parser combined with our baseline feature set. We used three features that were introduced by Gildea and Hockenmaier (2003): • Phrase type – This is the catego</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings ofEMNLP, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>