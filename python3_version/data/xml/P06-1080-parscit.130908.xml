<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000507">
<title confidence="0.99167">
Self-Organizing n-gram Model for Automatic Word Spacing
</title>
<author confidence="0.983851">
Seong-Bae Park Yoon-Shik Tae Se-Young Park
</author>
<affiliation confidence="0.993403">
Department of Computer Engineering
Kyungpook National University
</affiliation>
<address confidence="0.805718">
Daegu 702-701, Korea
</address>
<email confidence="0.999289">
{sbpark,ystae,syparkl@sejong.knu.ac.kr
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971464285714">
An automatic word spacing is one of the
important tasks in Korean language pro-
cessing and information retrieval. Since
there are a number of confusing cases in
word spacing of Korean, there are some
mistakes in many texts including news ar-
ticles. This paper presents a high-accurate
method for automatic word spacing based
on self-organizing n-gram model. This
method is basically a variant of n-gram
model, but achieves high accuracy by au-
tomatically adapting context size.
In order to find the optimal context size,
the proposed method automatically in-
creases the context size when the contex-
tual distribution after increasing it dose
not agree with that of the current context.
It also decreases the context size when
the distribution of reduced context is sim-
ilar to that of the current context. This
approach achieves high accuracy by con-
sidering higher dimensional data in case
of necessity, and the increased compu-
tational cost are compensated by the re-
duced context size. The experimental re-
sults show that the self-organizing struc-
ture of n-gram model enhances the basic
model.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955545454546">
Even though Korean widely uses Chinese charac-
ters, the ideograms, it has a word spacing model
unlike Chinese and Japanese. The word spacing of
Korean, however, is not a simple task, though the
basic rule for it is simple. The basic rule asserts
that all content words should be spaced. However,
there are a number of exceptions due to various
postpositions and endings. For instance, it is diffi-
cult to distinguish some postpositions from incom-
plete nouns. Such exceptions induce many mis-
takes of word spacing even in news articles.
The problem of the inaccurate word spacing is
that they are fatal in language processing and in-
formation retrieval. The incorrect word spacing
would result in the incorrect morphological analy-
sis. For instance, let us consider a famous Korean
sentence: “�������������������������.” The true
word spacing for this sentence is “4tl1--714#&apos;:a
011#-!T�-017}Vr}.” whose meaning is that my fa-
ther entered the room. If the sentence is written
as “° 7}VL}.”, it means that
my father entered the bag, which is totally dif-
ferent from the original meaning. That is, since
the morphological analysis is the first-step in most
NLP applications, the sentences with incorrect
word spacing must be corrected for their further
processing. In addition, the wrong word spacing
would result in the incorrect index for terms in in-
formation retrieval. Thus, correcting the sentences
with incorrect word spacing is a critical task in Ko-
rean information processing.
One of the most simple and strong models for
automatic word spacing is n-gram model. In spite
of the advantages of the n-gram model, its prob-
lem should be also considered for achieving high
performance. The main problem of the model is
that it is usually modeled with fixed window size,
n. The small value for n represents the narrow
context in modeling, which results in poor per-
formance in general. However, it is also difficult
to increase n for better performance due to data
sparseness. Since the corpus size is physically lim-
ited, it is highly possible that many n-grams which
do not appear in the corpus exist in the real world.
</bodyText>
<page confidence="0.989341">
633
</page>
<note confidence="0.533244">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 633–640,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999888095238095">
The goal of this paper is to provide a new
method for processing automatic word spacing
with an n-gram model. The proposed method au-
tomatically adapts the window size n. That is, this
method begins with a bigram model, and it shrinks
to an unigram model when data sparseness occurs.
It also grows up to a trigram, fourgram, and so
on when it requires more specific information in
determining word spacing. In a word, the pro-
posed model organizes the windows size n online,
and achieves high accuracy by removing both data
sparseness and information lack.
The rest of the paper is organized as follows.
Section 2 surveys the previous work on automatic
word spacing and the smoothing methods for n-
gram models. Section 3 describes the general way
to automatic word spacing by an n-gram model,
and Section 4 proposes a self-organizing n-gram
model to overcome some drawbacks of n-gram
models. Section 5 presents the experimental re-
sults. Finally, Section 6 draws conclusions.
</bodyText>
<sectionHeader confidence="0.996143" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99999117721519">
Many previous work has explored the possibility
of automatic word spacing. While most of them
reported high accuracy, they can be categorized
into two parts in methodology: analytic approach
and statistical approach. The analytic approach
is based on the results of morphological analysis.
Kang used the fundamental morphological analy-
sis techniques (Kang, 2000), and Kim et al. distin-
guished each word by the morphemic information
of postpositions and endings (Kim et al., 1998).
The main drawbacks of this approach are that (i)
the analytic step is very complex, and (ii) it is
expensive to construct and maintain the analytic
knowledge.
In the other hand, the statistical approach ex-
tracts from corpora the probability that a space is
put between two syllables. Since this approach can
obtain the necessary information automatically, it
does require neither the linguistic knowledge on
syllable composition nor the costs for knowledge
construction and maintenance. In addition, the
fact that it does not use a morphological analyzer
produces solid results even for unknown words.
Many previous studies using corpora are based on
bigram information. According to (Kang, 2004),
the number of syllables used in modern Korean is
about 104, which implies that the number of bi-
grams reaches 108. In order to obtain stable statis-
tics for all bigrams, a great large volume of cor-
pora will be required. If higher order n-gram is
adopted for better accuracy, the volume of corpora
required will be increased exponentially.
The main drawback of n-gram model is that
it suffers from data sparseness however large the
corpus is. That is, there are many n-grams of
which frequency is zero. To avoid this problem,
many smoothing techniques have been proposed
for construction of n-gram models (Chen and
Goodman, 1996). Most of them belongs to one
of two categories. One is to pretend each n-gram
occurs once more than it actually did (Mitchell,
1996). The other is to interpolate n-grams with
lower dimensional data (Jelinek and Mercer, 1980;
Katz, 1987). However, these methods artificially
modify the original distribution of corpus. Thus,
the final probabilities used in learning with n-
grams are the ones distorted by a smoothing tech-
nique.
A maximum entropy model can be considered
as another way to avoid zero probability in n-gram
models (Rosenfeld, 1996). Instead of construct-
ing separate models and then interpolate them, it
builds a single, combined model to capture all
the information provided by various knowledge
sources. Even though a maximum entropy ap-
proach is simple, general, and strong, it is com-
putationally very expensive. In addition, its per-
formance is mainly dependent on the relevance
of knowledge sources, since the prior knowledge
on the target problem is very important (Park and
Zhang, 2002). Thus, when prior knowledge is not
clear and computational cost is an important fac-
tor, n-gram models are more suitable than a maxi-
mum entropy model.
Adapting features or contexts has been an im-
portant issue in language modeling (Siu and Os-
tendorf, 2000). In order to incorporate long-
distance features into a language model, (Rosen-
feld, 1996) adopted triggers, and (Mochihashi and
Mastumoto, 2006) used a particle filter. However,
these methods are restricted to a specific language
model. Instead of long-distance features, some
other researchers tried local context extension. For
this purpose, (Sch¨utze and Singer, 1994) adopted
a variable memory Markov model proposed by
(Ron et al., 1996), (Kim et al., 2003) applied se-
lective extension of features to POS tagging, and
(Dickinson and Meurers, 2005) expanded context
of n-gram models to find errors in syntactic anno-
</bodyText>
<page confidence="0.998257">
634
</page>
<bodyText confidence="0.999682333333333">
tation. In these methods, only neighbor words or
features of the target n-grams became candidates
to be added into the context. Since they required
more information for better performance or detect-
ing errors, only the context extension was consid-
ered.
</bodyText>
<sectionHeader confidence="0.9450415" genericHeader="method">
3 Automatic Word Spacing by n-gram
Model
</sectionHeader>
<bodyText confidence="0.999975705882353">
The problem of automatic word spacing can be re-
garded as a binary classification task. Let a sen-
tence be given as S = w1w2 ... wN. If i.i.d. sam-
pling is assumed, the data from this sentence are
given as D =&lt; (x1, y1), ... , (xN, yN) &gt; where
xi E R&amp;quot; and yi E {true, false}. In this rep-
resentation, xi is a contextual representation of a
syllable wi. If a space should be put after wi, then
yi, the class of xi, is true. It is false otherwise.
Therefore, the automatic word spacing is to esti-
mate a function f : IIS&amp;quot; —� {true, false}. That
is, our task is to determine whether a space should
be put after a syllable wi expressed as xi with its
context.
The probabilistic method is one of the strong
and most widely used methods for estimating f.
That is, for each wi,
</bodyText>
<equation confidence="0.980684">
f(xi) = arg max P(yiIxi),
YiE{truejalsel
</equation>
<bodyText confidence="0.803264">
where P (yiIxi) is rewritten as
</bodyText>
<equation confidence="0.9988145">
P(xiIyi)P(yi)
P(yiIxi) = P(xi) .
</equation>
<bodyText confidence="0.997103666666667">
Since P (xi) is independent of finding the class of
xi, f(xi) is determined by multiplying P(xiIyi)
and P (yi). That is,
</bodyText>
<equation confidence="0.964276">
f(xi) = arg max P (xiIyi)P (yi).
YiE{truejalsel
</equation>
<bodyText confidence="0.9999644">
In n-gram model, xi is expressed with n neigh-
bor syllables around wi. Typically, n is taken
to be two or three, corresponding to a bigram or
trigram respectively. xi corresponds to wi_twi
when n = 2. In the same way, it is wi_2wi_1wi
when n = 3. The simple and easy way to esti-
mate P (xiIyi) is to use maximum likelihood esti-
mate with a large corpus. For instance, consider
the case n = 2. Then, the probability P (xiIyi) is
represented as P (wi_1wiIyi), and is computed by
</bodyText>
<equation confidence="0.874507">
P(wi_�wi&amp;yi)
P(wi_�wiIyi) = (1)
P(yi)
C(wi_1wi&amp;yi)
= C(yi) ,
0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06
No. of Training Examples
</equation>
<figureCaption confidence="0.867877">
Figure 1: The performance of n-gram models ac-
cording to the values of n in automatic word spac-
ing.
</figureCaption>
<bodyText confidence="0.9999287">
where C is a counting function.
Determining the context size, the value of n, in
n-gram models is closely related with the corpus
size. The larger is n, the larger corpus is required
to avoid data sparseness. In contrast, though low-
order n-grams do not suffer from data sparseness
severely, they do not reflect the language charac-
teristics well, either. Typically researchers have
used n = 2 or n = 3, and achieved high perfor-
mance in many tasks (Bengio et al., 2003). Fig-
ure 1 supports that bigram and trigram outper-
form low-order (n = 1) and high-order (n &gt; 4)
n-grams in automatic word spacing. All the ex-
perimental settings for this figure follows those
in Section 5. In this figure, bigram model shows
the best accuracy and trigram achieves the second
best, whereas unigram model results in the worst
accuracy. Since the bigram model is best, a self-
organizing n-gram model explained below starts
from bigram.
</bodyText>
<sectionHeader confidence="0.987876" genericHeader="method">
4 Self-Organizing n-gram Model
</sectionHeader>
<bodyText confidence="0.999986">
To tackle the problem of fixed window size in n-
gram models, we propose a self-organizing struc-
ture for them.
</bodyText>
<subsectionHeader confidence="0.995109">
4.1 Expanding n-grams
</subsectionHeader>
<bodyText confidence="0.999764166666667">
When n-grams are compared with (n + 1)-grams,
their performance in many tasks is lower than that
of (n + 1)-grams (Charniak, 1993). Simultane-
ously the computational cost for (n + 1)-grams
is far higher than that for n-grams. Thus, it can
be justified to use (n + 1)-grams instead of n-
</bodyText>
<figure confidence="0.999157466666667">
unigram
bigram
trigram
4-gram
5-gram
6-gram
7-gram
8-gram
9-gram
10-gram
Accuracy (%) 0.9
0.85
0.8
0.75
0.7
</figure>
<page confidence="0.88286">
635
</page>
<table confidence="0.647210333333333">
Function HowLargeExpand(x,)
Input: xn: n-grams
Output: an integer for expanding size
</table>
<listItem confidence="0.7950496">
1. Retrieve (n + 1)-grams xn+1 for xn.
2. Compute
D = KL(P(YIxn)IIP(YIxn+1))•
3. If D &lt; BEXP Then return 0.
4. return HowLargeExpand(xn+1) + 1.
</listItem>
<figureCaption confidence="0.9922795">
Figure 2: A function that determines how large a
window size should be.
</figureCaption>
<figure confidence="0.925803777777778">
Function HowSmallShrink(xn)
Input: xn: n-grams
Output: an integer for shrinking size
1. If n &lt; 1 Then return 0.
2. Retrieve (n — 1)-grams xn-1 for xn.
3. Compute
D = KL(P(YIxn)IIP(YIxn-1))•
4. If D &gt; BSHR Then return 0.
5. return HowSmallShrink(xn-1) - 1.
</figure>
<bodyText confidence="0.997982642857143">
grams only when higher performance is expected.
In other words, (n + 1)-grams should be different
from n-grams. Otherwise, the performance would
not be different. Since our task is attempted with
a probabilistic method, the difference can be mea-
sured with conditional distributions. If the condi-
tional distributions of n-grams and (n + 1)-grams
are similar each other, there is no reason to adopt
(n + 1)-grams.
Let P (y2lxn2) be a class-conditional probabil-
ity by n-grams and P(YzIx(n+1)z) that by (n + 1)-
grams. Then, the difference D(n, n + 1) between
them is measured by Kullback-Leibler divergence.
That is,
</bodyText>
<equation confidence="0.85255775">
D(n, n + 1) = KL(P(yzlxnz)IIP(Yzlx(n+1)i),
which is computed by
� p(zIxni) log p(Zlxni) (2)
n��o� p(Zl x(n+1)z)
</equation>
<bodyText confidence="0.964568642857143">
D(n, n + 1) that is larger than a predefined
threshold BEXP implies that P (YzIx(n+1)z) is dif-
ferent from P(YzIxnz). In this case, (n + 1)-grams
is used instead of n-grams.
Figure 2 depicts an algorithm that determines
how large n-grams should be used. It recursively
finds the optimal expanding window size. For in-
stance, let bigrams (n = 2) be used at first. When
the difference between bigrams and trigrams (n =
3) is larger than BEXP, that between trigrams and
fourgrams (n = 4) is checked again. If it is less
than BEXP, then this function returns 1 and tri-
grams are used instead of bigrams. Otherwise, it
considers higher n-grams again.
</bodyText>
<figureCaption confidence="0.978629">
Figure 3: A function that determines how small a
window size should be used.
</figureCaption>
<subsectionHeader confidence="0.987715">
4.2 Shrinking n-grams
</subsectionHeader>
<bodyText confidence="0.999525625">
Shrinking n-grams is accomplished in the direc-
tion opposite to expanding n-grams. After com-
paring n-grams with (n—1)-grams, (n—1)-grams
are used instead of n-grams only when they are
similar enough. The difference D(n, n — 1) be-
tween n-grams and (n — 1)-grams is, once again,
measured by Kullback-Leibler divergence. That
is,
</bodyText>
<equation confidence="0.973989">
D(n,n-1) = KL(P(yi1xnz)IIP(Yz1x(n-1)z)•
</equation>
<bodyText confidence="0.998938285714286">
If D(n, n — 1) is smaller than another predefined
threshold BSHR, then (n — 1)-grams are used in-
stead of n-grams.
Figure 3 shows an algorithm which determines
how deeply the shrinking is occurred. The main
stream of this algorithm is equivalent to that in
Figure 2. It also recursively finds the optimal
shrinking window size, but can not be further re-
duced when the current model is an unigram.
The merit of shrinking n-grams is that it can
construct a model with a lower dimensionality.
Since the maximum likelihood estimate is used in
calculating probabilities, this helps obtaining sta-
ble probabilities. According to the well-known
curse of dimensionality, the data density required
is reduced exponentially by reducing dimensions.
Thus, if the lower dimensional model is not differ-
ent so much from the higher dimensional one, it
is highly possible that the probabilities from lower
dimensional space are more stable than those from
higher dimensional space.
</bodyText>
<page confidence="0.995328">
636
</page>
<table confidence="0.79155">
Function ChangingWindowSize(xn)
Input: xn: n-grams
Output: an integer for changing window size
</table>
<listItem confidence="0.9904512">
1. Set exp := HowLargeExpand(xn).
2. If exp &gt; 0 Then return exp.
3. Set shr := HowSmallShrink(xn).
4. If shr &lt; 0 Then return shr.
5. return 0.
</listItem>
<subsectionHeader confidence="0.998582">
4.4 Sequence Tagging
</subsectionHeader>
<bodyText confidence="0.999493">
Since natural language sentences are sequential as
their nature, the word spacing can be considered
as a special POS tagging task (Lee et al., 2002) for
which a hidden Markov model is usually adopted.
The best sequence of word spacing for the sen-
tence is defined as
</bodyText>
<equation confidence="0.9987586">
y�1,N = arg max P(y1,NIx1,N)
Y1,N
= arg max
Y1,N P(X1,NIy1,N)P(y1,N)
P(xl,N)
</equation>
<figureCaption confidence="0.99979">
Figure 4: A function that determines the changing = arg max P(X1,NIy1,N)P(y1,N)
window size of n-grams. Y1,N
</figureCaption>
<subsectionHeader confidence="0.996303">
4.3 Overall Self-Organizing Structure
</subsectionHeader>
<bodyText confidence="0.996321605263158">
For a given i.i.d. sample xi, there are three pos-
sibilities on changing n-grams. First one is not
to change n-grams. It is obvious when n-grams
are not changed. This occurs when both D(n, n +
1) &gt; BEXP and D(n, n — 1) &lt; BSHR are met.
This is when the expanding results in too similar
distribution to that of the current n-grams and the
distribution after shrinking is too different from
that of the current n-grams.
The remaining possibilities are then expand-
ing and shrinking. The application order be-
tween them can affect the performance of the pro-
posed method. In this paper, an expanding is
checked prior to a shrinking as shown in Figure
4. The function ChangingWindowSize first calls
HowLargeExpand. The non-zero return value of
HowLargeExpand implies that the window size
of the current n-grams should be enlarged. Oth-
erwise, ChangingWindowSize checks if the win-
dow size should be shrinked by calling HowSmall-
Shrink. If HowSmallShrink returns a negative in-
teger, the window size should be shrinked to (n +
shr). If both functions return zero, the window
size should not be changed.
The reason why HowLargeExpand is called
prior to HowSmallShrink is that the expanded n-
grams handle more specific data. (n + 1)-grams,
in general, help obtaining higher accuracy than n-
grams, since (n + 1)-gram data are more specific
than n-gram ones. However, it is time-consuming
to consider higher-order data, since the number of
kinds of data increases. The time increased due
to expanding is compensated by shrinking. Af-
ter shrinking, only lower-oder data are considered,
and then processing time for them decreases.
by where N is a sentence length.
If we assume that the syllables are independent
of each other, P(x1,N Iy1,N) is given by
</bodyText>
<equation confidence="0.997706">
N
P(x1,NIy1,N) = P(xiIyi),
i=1
</equation>
<bodyText confidence="0.99456">
which can be computed using Equation (1). In ad-
dition, by Markov assumption, the probability of
a current tag yi conditionally depends on only the
previous k tags. That is,
</bodyText>
<equation confidence="0.992924428571429">
N
P(y1,N) = P(yiIyi—k,i-7)•
i=1
Thus, the best sequence is determined by
N
y�1,N = arg max P(xiIyi) - P(yiIyi—k,i-1)• (3)
Y1,N i=1
</equation>
<bodyText confidence="0.999537">
Since this equation follows Markov assumption,
the best sequence is found by applying the Viterbi
algorithm.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99033">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999972888888889">
The data set used in this paper is the HANTEC cor-
pora version 2.0 distributed by KISTI1. From this
corpus, we extracted only the HKIB94 part which
consists of 22,000 news articles in 1994 from Han-
kook Ilbo. The reason why HKIB94 is chosen is
that the word spacing of news articles is relatively
more accurate than other texts. Even though this
data set is composed of totally 12,523,688 Korean
syllables, the number of unique syllables is just
</bodyText>
<footnote confidence="0.990466">
1http://www.kisti.re.kr
</footnote>
<page confidence="0.989184">
637
</page>
<figure confidence="0.887481428571429">
Order
No. of Errors
Expanding then Shrinking
Shrinking then Expanding
108,831
114,343
Methods
baseline
bigram
trigram
self-organizing bigram
decision tree
Accuracy (%)
72.19
88.34
87.59
91.31 Table 2: The number of errors caused by the appli-
cation order of context expanding and shrinking.
88.68
89.10
support vector machine
</figure>
<tableCaption confidence="0.992127">
Table 1: The experimental results of various meth-
ods for automatic word spacing.
</tableCaption>
<bodyText confidence="0.954651384615385">
2,037 after removing all special symbols, digits,
and English alphabets.
The data set is divided into three parts: train-
ing (70%), held-out (20%), and test (10%). The
held-out set is used only to estimate BEXP and
BSHR. The number of instances in the training set
is 8,766,578, that in the held-out set is 2,504,739,
and that in test set is 1,252,371. Among the
1,252,371 test cases, the number of positive in-
stances is 348,278, and that of negative instances
is 904,093. Since about 72% of test cases are neg-
ative, this is the baseline of the automatic word
spacing.
</bodyText>
<subsectionHeader confidence="0.998957">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999980701754386">
To evaluate the performance of the proposed
method, two well-known machine learning algo-
rithms are compared together. The tested machine
learning algorithms are (i) decision tree and (ii)
support vector machines. We use C4.5 release 8
(Quinlan, 1993) for decision tree induction and
SVMtight (Joachims, 1998) for support vector
machines. For all experiments with decision trees
and support vector machines, the context size is
set to two since the bigram shows the best perfor-
mance in Figure 1.
Table 1 gives the experimental results of various
methods including machine learning algorithms
and self-organizing n-gram model. The ‘self-
organizing bigram’ in this table is the one pro-
posed in this paper. The normal n-grams achieve
an accuracy of around 88%, while decision tree
and support vector machine produce that of around
89%. The self-organizing n-gram model achieves
91.31%. The accuracy improvement by the self-
organizing n-gram model is about 19% over the
baseline, about 3% over the normal n-gram model,
and 2% over decision trees and support vector ma-
chines.
In order to organize the context size for n-grams
online, two operations of expanding and shrinking
were proposed. Table 2 shows how much the num-
ber of errors is affected by their application order.
The number of errors made by expanding first is
108,831 while that by shrinking first is 114,343.
That is, if shrinking is applied ahead of expand-
ing, 5,512 additional errors are made. Thus, it is
clear that expanding should be considered first.
The errors by expanding can be explained with
two reasons: (i) the expression power of the
model and (ii) data sparseness. Since Korean is a
partially-free word order language and the omis-
sion of words are very frequent, n-gram model
that captures local information could not express
the target task sufficiently. In addition, the class-
conditional distribution after expanding could be
very different from that before expanding due to
data sparseness. In such cases, the expanding
should not be applied since the distribution after
expanding is not trustworthy. However, only the
difference between two distributions is considered
in the proposed method, and the errors could be
made by data sparseness.
Figure 5 shows that the number of training in-
stances does not matter in computing probabilities
of n-grams. Even though the accuracy increases
slightly, the accuracy difference after 900,000 in-
stances is not significant. It implies that the er-
rors made by the proposed method is not from the
lack of training instance but from the lack of its
expression power for the target task. This result
also complies with Figure 1.
</bodyText>
<subsectionHeader confidence="0.999873">
5.3 Effect of Right Context
</subsectionHeader>
<bodyText confidence="0.981897285714286">
All the experiments above considered left context
only. However, Kang reported that the probabilis-
tic model using both left and right context outper-
forms the one that uses left context only (Kang,
2004). In his work, the word spacing probabil-
ity P(wi, wi+1) between two adjacent syllables wi
and wi+1 is given as
</bodyText>
<equation confidence="0.95279">
P(wi, wi+1) = 0.25 x PL(wi-1, wi)
0.5 x Pm(wi, wi+1) (4)
0.25 x PR(wi+1, wi+2),
</equation>
<page confidence="0.995088">
638
</page>
<figure confidence="0.999053222222222">
1
0.95
0.9
0.85
0.8
0.75
0.7
0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06
No. of Training Examples
</figure>
<figureCaption confidence="0.9963375">
Figure 5: The effect of the number of training ex-
amples in the self-organizing n-gram model.
</figureCaption>
<figure confidence="0.93208475">
Context Accuracy (%)
Left Context Only 91.31
Right Context Only 88.26
Both Contexts 92.54
</figure>
<tableCaption confidence="0.961082">
Table 3: The effect of using both left and right
context.
</tableCaption>
<bodyText confidence="0.999738571428571">
where PR, PM, PL are computed respectively
based on the syllable frequency.
In order to reflect the idea of bidirectional con-
text in the proposed model, the model is enhanced
by modifying P (w2—1w2Iy2) in Equation (1). That
is, the likelihood of P (w2—iw2Iy2) is expanded to
be
</bodyText>
<equation confidence="0.999427666666667">
P�(w2—lw2ly2) = at x P(w2—W2—lly2)
a2 x P(w2—lw2ly2)
a3 x P (w2, w2+lly2).
</equation>
<bodyText confidence="0.9943116">
Since the coefficients of Equation (4) were deter-
mined arbitrarily (Kang, 2004), they are replaced
with parameters a2 of which values are determined
using a held-out data.
The change of accuracy by the context is shown
in Table 3. When only the right context is used,
the accuracy gets 88.26% which is worse than the
left context only. That is, the original n-gram
is a relatively good model. However, when both
left and right context are used, the accuracy be-
comes 92.54%. The accuracy improvement by
using additional right context is 1.23%. This re-
sults coincide with the previous report (Lee et
al., 2002). The a2’s to achieve this accuracy are
al = 0.5,a2 = 0.3, and a3 = 0.2.
</bodyText>
<table confidence="0.940686666666667">
Method Accuracy(%)
Normal HMM 92.37
Self-Organizing HMM 94.71
</table>
<tableCaption confidence="0.999723">
Table 4: The effect of considering a tag sequence.
</tableCaption>
<subsectionHeader confidence="0.97107">
5.4 Effect of Considering Tag Sequence
</subsectionHeader>
<bodyText confidence="0.999962869565217">
The state-of-the-art performance on Korean word
spacing is to use the hidden Markov model. Ac-
cording to the previous work (Lee et al., 2002), the
hidden Markov model shows the best performance
when it sees two previous tags and two previous
syllables.
For the simplicity in the experiments, the value
for k in Equation (3) is set to be one. The
performance comparison between normal HMM
and the proposed method is given in Table 4.
The proposed method considers the various num-
ber of previous syllables, whereas the normal
HMM has the fixed context. Thus, the proposed
method in Table 4 is specified as ‘self-organizing
HMM.’ The accuracy of the self-organizing HMM
is 94.71%, while that of the normal HMM is just
92.37%. Even though the normal HMM consid-
ers more previous tags (k = 2), the accuracy of
the self-organizing model is 2.34% higher than
that of the normal HMM. Therefore, the proposed
method that considers the sequence of word spac-
ing tags achieves higher accuracy than any other
methods reported ever.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999707">
In this paper we have proposed a new method to
learn word spacing in Korean by adaptively orga-
nizing context size. Our method is based on the
simple n-gram model, but the context size n is
changed as needed. When the increased context
is much different from the current one, the context
size is increased. In the same way, the context is
decreased, if the decreased context is not so much
different from the current one. The benefits of this
method are that it can consider wider context by
increasing context size as required, and save the
computational cost due to the reduced context.
The experiments on HANTEC corpora showed
that the proposed method improves the accuracy of
the trigram model by 3.72%. Even compared with
some well-known machine learning algorithms, it
achieved the improvement of 2.63% over decision
trees and 2.21% over support vector machines. In
addition, we showed two ways for improving the
</bodyText>
<equation confidence="0.55269">
Accuracy (%)
</equation>
<page confidence="0.9952">
639
</page>
<bodyText confidence="0.999871736842105">
proposed method: considering right context and
word spacing sequence. By considering left and
right context at the same time, the accuracy is im-
proved by 1.23%, and the consideration of word
spacing sequence gives the accuracy improvement
of 2.34%.
The n-gram model is one of the most widely
used methods in natural language processing and
information retrieval. Especially, it is one of the
successful language models, which is a key tech-
nique in language and speech processing. There-
fore, the proposed method can be applied to not
only word spacing but also many other tasks. Even
though word spacing is one of the important tasks
in Korean information processing, it is just a sim-
ple task in many other languages such as English,
German, and French. However, due to its gener-
ality, the importance of the proposed method yet
does hold in such languages.
</bodyText>
<sectionHeader confidence="0.999025" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997811666666667">
This work was supported by the Korea Research
Foundation Grant funded by the Korean Govern-
ment (KRF-2005-202-D00465).
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994276">
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A Neural Probabilistic Language Model.
Journal of Machine Learning Research, Vol. 3, pp.
1137–1155.
E. Charniak. 1993. Statistical Language Learning.
MIT Press.
S. Chen and J. Goodman. 1996. An Empirical Study of
Smoothing Techniques for Language Modeling. In
Proceedings of the 34th Annual Meeting of the Asso-
ciationfor Computational Linguistics, pp. 310–318.
M. Dickinson and W. Meurers. 2005. Detecting Er-
rors in Discontinuous Structural Annotation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 322–329.
F. Jelinek and R. Mercer. 1980. Interpolated Estima-
tion of Markov Source Parameters from Sparse Data.
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice.
T. Joachims. 1998. Making Large-Scale SVM Learn-
ing Practical. LS8, Universitat Dortmund.
S.-S. Kang, 2000. Eojeol-Block Bidirectional Algo-
rithm for Automatic Word Spacing of Hangul Sen-
tences. Journal of KISS, Vol. 27, No. 4, pp. 441–
447. (in Korean)
S.-S. Kang. 2004. Improvement of Automatic Word
Segmentation of Korean by Simplifying Syllable Bi-
gram. In Proceedings of the 15th Conference on
Korean Language and Information Processing, pp.
227–231. (in Korean)
S. Katz. 1987. Estimation of Probabilities from
Sparse Data for the Language Model Component of
a Speech Recognizer. IEEE Transactions on Acous-
tics, Speech and Signal Processing. Vol. 35, No. 3,
pp. 400–401.
K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998. Three-
Stage Spacing System for Korean in Sentence with
No Word Boundaries. Journal ofKISS, Vol. 25, No.
12, pp. 1838–1844. (in Korean)
J.-D. Kim, H.-C. Rim, and J. Tsujii. 2003. Self-
Organizing Markov Models and Their Application
to Part-of-Speech Tagging. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pp. 296–302.
D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim, 2002.
Automatic Word Spacing Using Hidden Markov
Model for Refining Korean Text Corpora. In Pro-
ceedings of the 3rd Workshop on Asian Language
Resources and International Standardization, pp.
51–57.
T. Mitchell. 1997. Machine Learning. McGraw Hill.
D. Mochihashi and Y. Matsumoto. 2006. Context as
Filtering. Advances in Neural Information Process-
ing Systems 18, pp. 907–914.
S.-B. Park and B.-T. Zhang. 2002. A Boosted Max-
imum Entropy Model for Learning Text Chunking.
In Proceedings of the 19th International Conference
on Machine Learning, pp. 482–489.
R. Quinlan. 1993. C4.5: Program for Machine Learn-
ing. Morgan Kaufmann Publishers.
D. Ron, Y. Singer, and N. Tishby. 1996. The Power
of Amnesia: Learning Probabilistic Automata with
Variable Memory Length. Machine Learning, Vol.
25, No. 2, pp. 117–149.
R. Rosenfeld. 1996. A Maximum Entropy Approach
to Adaptive Statistical Language Modeling. Com-
puter, Speech and Language, Vol. 10, pp. 187– 228.
H. Sch¨utze and Y. Singer. 1994. Part-of-Speech Tag-
ging Using a Variable Memory Markov Model. In
Proceedings of the 32nd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 181–
187.
M. Siu and M. Ostendorf. 2000. Variable N-Grams
and Extensions for Conversational Speech Language
Modeling. IEEE Transactions on Speech and Audio
Processing, Vol. 8, No. 1, pp. 63–75.
</reference>
<page confidence="0.997699">
640
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872688">
<title confidence="0.999928">Model for Automatic Word Spacing</title>
<author confidence="0.994405">Seong-Bae Park Yoon-Shik Tae Se-Young Park</author>
<affiliation confidence="0.99862">Department of Computer Engineering Kyungpook National University</affiliation>
<address confidence="0.931042">Daegu 702-701, Korea</address>
<abstract confidence="0.998017689655172">An automatic word spacing is one of the important tasks in Korean language processing and information retrieval. Since there are a number of confusing cases in word spacing of Korean, there are some mistakes in many texts including news articles. This paper presents a high-accurate method for automatic word spacing based self-organizing model. This is basically a variant of model, but achieves high accuracy by automatically adapting context size. In order to find the optimal context size, the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context. It also decreases the context size when the distribution of reduced context is similar to that of the current context. This approach achieves high accuracy by considering higher dimensional data in case of necessity, and the increased computational cost are compensated by the reduced context size. The experimental results show that the self-organizing strucof model enhances the basic model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="10829" citStr="Bengio et al., 2003" startWordPosition="1794" endWordPosition="1797">e+06 5e+06 6e+06 7e+06 8e+06 No. of Training Examples Figure 1: The performance of n-gram models according to the values of n in automatic word spacing. where C is a counting function. Determining the context size, the value of n, in n-gram models is closely related with the corpus size. The larger is n, the larger corpus is required to avoid data sparseness. In contrast, though loworder n-grams do not suffer from data sparseness severely, they do not reflect the language characteristics well, either. Typically researchers have used n = 2 or n = 3, and achieved high performance in many tasks (Bengio et al., 2003). Figure 1 supports that bigram and trigram outperform low-order (n = 1) and high-order (n &gt; 4) n-grams in automatic word spacing. All the experimental settings for this figure follows those in Section 5. In this figure, bigram model shows the best accuracy and trigram achieves the second best, whereas unigram model results in the worst accuracy. Since the bigram model is best, a selforganizing n-gram model explained below starts from bigram. 4 Self-Organizing n-gram Model To tackle the problem of fixed window size in ngram models, we propose a self-organizing structure for them. 4.1 Expanding</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, Vol. 3, pp. 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11568" citStr="Charniak, 1993" startWordPosition="1922" endWordPosition="1923">acing. All the experimental settings for this figure follows those in Section 5. In this figure, bigram model shows the best accuracy and trigram achieves the second best, whereas unigram model results in the worst accuracy. Since the bigram model is best, a selforganizing n-gram model explained below starts from bigram. 4 Self-Organizing n-gram Model To tackle the problem of fixed window size in ngram models, we propose a self-organizing structure for them. 4.1 Expanding n-grams When n-grams are compared with (n + 1)-grams, their performance in many tasks is lower than that of (n + 1)-grams (Charniak, 1993). Simultaneously the computational cost for (n + 1)-grams is far higher than that for n-grams. Thus, it can be justified to use (n + 1)-grams instead of nunigram bigram trigram 4-gram 5-gram 6-gram 7-gram 8-gram 9-gram 10-gram Accuracy (%) 0.9 0.85 0.8 0.75 0.7 635 Function HowLargeExpand(x,) Input: xn: n-grams Output: an integer for expanding size 1. Retrieve (n + 1)-grams xn+1 for xn. 2. Compute D = KL(P(YIxn)IIP(YIxn+1))• 3. If D &lt; BEXP Then return 0. 4. return HowLargeExpand(xn+1) + 1. Figure 2: A function that determines how large a window size should be. Function HowSmallShrink(xn) Input</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>E. Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="6440" citStr="Chen and Goodman, 1996" startWordPosition="1032" endWordPosition="1035"> number of syllables used in modern Korean is about 104, which implies that the number of bigrams reaches 108. In order to obtain stable statistics for all bigrams, a great large volume of corpora will be required. If higher order n-gram is adopted for better accuracy, the volume of corpora required will be increased exponentially. The main drawback of n-gram model is that it suffers from data sparseness however large the corpus is. That is, there are many n-grams of which frequency is zero. To avoid this problem, many smoothing techniques have been proposed for construction of n-gram models (Chen and Goodman, 1996). Most of them belongs to one of two categories. One is to pretend each n-gram occurs once more than it actually did (Mitchell, 1996). The other is to interpolate n-grams with lower dimensional data (Jelinek and Mercer, 1980; Katz, 1987). However, these methods artificially modify the original distribution of corpus. Thus, the final probabilities used in learning with ngrams are the ones distorted by a smoothing technique. A maximum entropy model can be considered as another way to avoid zero probability in n-gram models (Rosenfeld, 1996). Instead of constructing separate models and then inter</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. Chen and J. Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pp. 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
<author>W Meurers</author>
</authors>
<title>Detecting Errors in Discontinuous Structural Annotation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="8248" citStr="Dickinson and Meurers, 2005" startWordPosition="1320" endWordPosition="1323">xts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate longdistance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (Sch¨utze and Singer, 1994) adopted a variable memory Markov model proposed by (Ron et al., 1996), (Kim et al., 2003) applied selective extension of features to POS tagging, and (Dickinson and Meurers, 2005) expanded context of n-gram models to find errors in syntactic anno634 tation. In these methods, only neighbor words or features of the target n-grams became candidates to be added into the context. Since they required more information for better performance or detecting errors, only the context extension was considered. 3 Automatic Word Spacing by n-gram Model The problem of automatic word spacing can be regarded as a binary classification task. Let a sentence be given as S = w1w2 ... wN. If i.i.d. sampling is assumed, the data from this sentence are given as D =&lt; (x1, y1), ... , (xN, yN) &gt; w</context>
</contexts>
<marker>Dickinson, Meurers, 2005</marker>
<rawString>M. Dickinson and W. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Interpolated Estimation of Markov Source Parameters from Sparse Data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice.</booktitle>
<contexts>
<context position="6664" citStr="Jelinek and Mercer, 1980" startWordPosition="1070" endWordPosition="1073">r order n-gram is adopted for better accuracy, the volume of corpora required will be increased exponentially. The main drawback of n-gram model is that it suffers from data sparseness however large the corpus is. That is, there are many n-grams of which frequency is zero. To avoid this problem, many smoothing techniques have been proposed for construction of n-gram models (Chen and Goodman, 1996). Most of them belongs to one of two categories. One is to pretend each n-gram occurs once more than it actually did (Mitchell, 1996). The other is to interpolate n-grams with lower dimensional data (Jelinek and Mercer, 1980; Katz, 1987). However, these methods artificially modify the original distribution of corpus. Thus, the final probabilities used in learning with ngrams are the ones distorted by a smoothing technique. A maximum entropy model can be considered as another way to avoid zero probability in n-gram models (Rosenfeld, 1996). Instead of constructing separate models and then interpolate them, it builds a single, combined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensiv</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. Mercer. 1980. Interpolated Estimation of Markov Source Parameters from Sparse Data. In Proceedings of the Workshop on Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1998</date>
<tech>LS8,</tech>
<institution>Universitat Dortmund.</institution>
<contexts>
<context position="19922" citStr="Joachims, 1998" startWordPosition="3313" endWordPosition="3314">the held-out set is 2,504,739, and that in test set is 1,252,371. Among the 1,252,371 test cases, the number of positive instances is 348,278, and that of negative instances is 904,093. Since about 72% of test cases are negative, this is the baseline of the automatic word spacing. 5.2 Experimental Results To evaluate the performance of the proposed method, two well-known machine learning algorithms are compared together. The tested machine learning algorithms are (i) decision tree and (ii) support vector machines. We use C4.5 release 8 (Quinlan, 1993) for decision tree induction and SVMtight (Joachims, 1998) for support vector machines. For all experiments with decision trees and support vector machines, the context size is set to two since the bigram shows the best performance in Figure 1. Table 1 gives the experimental results of various methods including machine learning algorithms and self-organizing n-gram model. The ‘selforganizing bigram’ in this table is the one proposed in this paper. The normal n-grams achieve an accuracy of around 88%, while decision tree and support vector machine produce that of around 89%. The self-organizing n-gram model achieves 91.31%. The accuracy improvement by</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Making Large-Scale SVM Learning Practical. LS8, Universitat Dortmund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-S Kang</author>
</authors>
<title>Eojeol-Block Bidirectional Algorithm for Automatic Word Spacing of Hangul Sentences.</title>
<date>2000</date>
<journal>Journal of KISS,</journal>
<volume>27</volume>
<pages>441--447</pages>
<note>(in Korean)</note>
<contexts>
<context position="4997" citStr="Kang, 2000" startWordPosition="799" endWordPosition="800"> way to automatic word spacing by an n-gram model, and Section 4 proposes a self-organizing n-gram model to overcome some drawbacks of n-gram models. Section 5 presents the experimental results. Finally, Section 6 draws conclusions. 2 Previous Work Many previous work has explored the possibility of automatic word spacing. While most of them reported high accuracy, they can be categorized into two parts in methodology: analytic approach and statistical approach. The analytic approach is based on the results of morphological analysis. Kang used the fundamental morphological analysis techniques (Kang, 2000), and Kim et al. distinguished each word by the morphemic information of postpositions and endings (Kim et al., 1998). The main drawbacks of this approach are that (i) the analytic step is very complex, and (ii) it is expensive to construct and maintain the analytic knowledge. In the other hand, the statistical approach extracts from corpora the probability that a space is put between two syllables. Since this approach can obtain the necessary information automatically, it does require neither the linguistic knowledge on syllable composition nor the costs for knowledge construction and mainten</context>
</contexts>
<marker>Kang, 2000</marker>
<rawString>S.-S. Kang, 2000. Eojeol-Block Bidirectional Algorithm for Automatic Word Spacing of Hangul Sentences. Journal of KISS, Vol. 27, No. 4, pp. 441– 447. (in Korean)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-S Kang</author>
</authors>
<title>Improvement of Automatic Word Segmentation of Korean by Simplifying Syllable Bigram.</title>
<date>2004</date>
<booktitle>In Proceedings of the 15th Conference on Korean Language and Information Processing,</booktitle>
<pages>227--231</pages>
<note>(in Korean)</note>
<contexts>
<context position="5812" citStr="Kang, 2004" startWordPosition="927" endWordPosition="928">nd (ii) it is expensive to construct and maintain the analytic knowledge. In the other hand, the statistical approach extracts from corpora the probability that a space is put between two syllables. Since this approach can obtain the necessary information automatically, it does require neither the linguistic knowledge on syllable composition nor the costs for knowledge construction and maintenance. In addition, the fact that it does not use a morphological analyzer produces solid results even for unknown words. Many previous studies using corpora are based on bigram information. According to (Kang, 2004), the number of syllables used in modern Korean is about 104, which implies that the number of bigrams reaches 108. In order to obtain stable statistics for all bigrams, a great large volume of corpora will be required. If higher order n-gram is adopted for better accuracy, the volume of corpora required will be increased exponentially. The main drawback of n-gram model is that it suffers from data sparseness however large the corpus is. That is, there are many n-grams of which frequency is zero. To avoid this problem, many smoothing techniques have been proposed for construction of n-gram mod</context>
<context position="22483" citStr="Kang, 2004" startWordPosition="3732" endWordPosition="3733">g instances does not matter in computing probabilities of n-grams. Even though the accuracy increases slightly, the accuracy difference after 900,000 instances is not significant. It implies that the errors made by the proposed method is not from the lack of training instance but from the lack of its expression power for the target task. This result also complies with Figure 1. 5.3 Effect of Right Context All the experiments above considered left context only. However, Kang reported that the probabilistic model using both left and right context outperforms the one that uses left context only (Kang, 2004). In his work, the word spacing probability P(wi, wi+1) between two adjacent syllables wi and wi+1 is given as P(wi, wi+1) = 0.25 x PL(wi-1, wi) 0.5 x Pm(wi, wi+1) (4) 0.25 x PR(wi+1, wi+2), 638 1 0.95 0.9 0.85 0.8 0.75 0.7 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06 No. of Training Examples Figure 5: The effect of the number of training examples in the self-organizing n-gram model. Context Accuracy (%) Left Context Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context. where PR, PM, PL are computed respectively based on the syll</context>
</contexts>
<marker>Kang, 2004</marker>
<rawString>S.-S. Kang. 2004. Improvement of Automatic Word Segmentation of Korean by Simplifying Syllable Bigram. In Proceedings of the 15th Conference on Korean Language and Information Processing, pp. 227–231. (in Korean)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing.</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="6677" citStr="Katz, 1987" startWordPosition="1074" endWordPosition="1075">for better accuracy, the volume of corpora required will be increased exponentially. The main drawback of n-gram model is that it suffers from data sparseness however large the corpus is. That is, there are many n-grams of which frequency is zero. To avoid this problem, many smoothing techniques have been proposed for construction of n-gram models (Chen and Goodman, 1996). Most of them belongs to one of two categories. One is to pretend each n-gram occurs once more than it actually did (Mitchell, 1996). The other is to interpolate n-grams with lower dimensional data (Jelinek and Mercer, 1980; Katz, 1987). However, these methods artificially modify the original distribution of corpus. Thus, the final probabilities used in learning with ngrams are the ones distorted by a smoothing technique. A maximum entropy model can be considered as another way to avoid zero probability in n-gram models (Rosenfeld, 1996). Instead of constructing separate models and then interpolate them, it builds a single, combined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensive. In additio</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing. Vol. 35, No. 3, pp. 400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-S Kim</author>
<author>H-J Lee</author>
<author>S-J Lee</author>
</authors>
<title>ThreeStage Spacing System for Korean in Sentence with No Word Boundaries.</title>
<date>1998</date>
<journal>Journal ofKISS,</journal>
<volume>25</volume>
<pages>1838--1844</pages>
<note>(in Korean)</note>
<contexts>
<context position="5114" citStr="Kim et al., 1998" startWordPosition="817" endWordPosition="820">rcome some drawbacks of n-gram models. Section 5 presents the experimental results. Finally, Section 6 draws conclusions. 2 Previous Work Many previous work has explored the possibility of automatic word spacing. While most of them reported high accuracy, they can be categorized into two parts in methodology: analytic approach and statistical approach. The analytic approach is based on the results of morphological analysis. Kang used the fundamental morphological analysis techniques (Kang, 2000), and Kim et al. distinguished each word by the morphemic information of postpositions and endings (Kim et al., 1998). The main drawbacks of this approach are that (i) the analytic step is very complex, and (ii) it is expensive to construct and maintain the analytic knowledge. In the other hand, the statistical approach extracts from corpora the probability that a space is put between two syllables. Since this approach can obtain the necessary information automatically, it does require neither the linguistic knowledge on syllable composition nor the costs for knowledge construction and maintenance. In addition, the fact that it does not use a morphological analyzer produces solid results even for unknown wor</context>
</contexts>
<marker>Kim, Lee, Lee, 1998</marker>
<rawString>K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998. ThreeStage Spacing System for Korean in Sentence with No Word Boundaries. Journal ofKISS, Vol. 25, No. 12, pp. 1838–1844. (in Korean)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>H-C Rim</author>
<author>J Tsujii</author>
</authors>
<title>SelfOrganizing Markov Models and Their Application to Part-of-Speech Tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>296--302</pages>
<contexts>
<context position="8158" citStr="Kim et al., 2003" startWordPosition="1306" endWordPosition="1309">dels are more suitable than a maximum entropy model. Adapting features or contexts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate longdistance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (Sch¨utze and Singer, 1994) adopted a variable memory Markov model proposed by (Ron et al., 1996), (Kim et al., 2003) applied selective extension of features to POS tagging, and (Dickinson and Meurers, 2005) expanded context of n-gram models to find errors in syntactic anno634 tation. In these methods, only neighbor words or features of the target n-grams became candidates to be added into the context. Since they required more information for better performance or detecting errors, only the context extension was considered. 3 Automatic Word Spacing by n-gram Model The problem of automatic word spacing can be regarded as a binary classification task. Let a sentence be given as S = w1w2 ... wN. If i.i.d. sampl</context>
</contexts>
<marker>Kim, Rim, Tsujii, 2003</marker>
<rawString>J.-D. Kim, H.-C. Rim, and J. Tsujii. 2003. SelfOrganizing Markov Models and Their Application to Part-of-Speech Tagging. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pp. 296–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D-G Lee</author>
<author>S-Z Lee</author>
<author>H-C Rim</author>
<author>H-S Lim</author>
</authors>
<title>Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization,</booktitle>
<pages>51--57</pages>
<contexts>
<context position="15599" citStr="Lee et al., 2002" startWordPosition="2595" endWordPosition="2598">mensional model is not different so much from the higher dimensional one, it is highly possible that the probabilities from lower dimensional space are more stable than those from higher dimensional space. 636 Function ChangingWindowSize(xn) Input: xn: n-grams Output: an integer for changing window size 1. Set exp := HowLargeExpand(xn). 2. If exp &gt; 0 Then return exp. 3. Set shr := HowSmallShrink(xn). 4. If shr &lt; 0 Then return shr. 5. return 0. 4.4 Sequence Tagging Since natural language sentences are sequential as their nature, the word spacing can be considered as a special POS tagging task (Lee et al., 2002) for which a hidden Markov model is usually adopted. The best sequence of word spacing for the sentence is defined as y�1,N = arg max P(y1,NIx1,N) Y1,N = arg max Y1,N P(X1,NIy1,N)P(y1,N) P(xl,N) Figure 4: A function that determines the changing = arg max P(X1,NIy1,N)P(y1,N) window size of n-grams. Y1,N 4.3 Overall Self-Organizing Structure For a given i.i.d. sample xi, there are three possibilities on changing n-grams. First one is not to change n-grams. It is obvious when n-grams are not changed. This occurs when both D(n, n + 1) &gt; BEXP and D(n, n — 1) &lt; BSHR are met. This is when the expandi</context>
<context position="23984" citStr="Lee et al., 2002" startWordPosition="3993" endWordPosition="3996">2+lly2). Since the coefficients of Equation (4) were determined arbitrarily (Kang, 2004), they are replaced with parameters a2 of which values are determined using a held-out data. The change of accuracy by the context is shown in Table 3. When only the right context is used, the accuracy gets 88.26% which is worse than the left context only. That is, the original n-gram is a relatively good model. However, when both left and right context are used, the accuracy becomes 92.54%. The accuracy improvement by using additional right context is 1.23%. This results coincide with the previous report (Lee et al., 2002). The a2’s to achieve this accuracy are al = 0.5,a2 = 0.3, and a3 = 0.2. Method Accuracy(%) Normal HMM 92.37 Self-Organizing HMM 94.71 Table 4: The effect of considering a tag sequence. 5.4 Effect of Considering Tag Sequence The state-of-the-art performance on Korean word spacing is to use the hidden Markov model. According to the previous work (Lee et al., 2002), the hidden Markov model shows the best performance when it sees two previous tags and two previous syllables. For the simplicity in the experiments, the value for k in Equation (3) is set to be one. The performance comparison between</context>
</contexts>
<marker>Lee, Lee, Rim, Lim, 2002</marker>
<rawString>D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim, 2002. Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization, pp. 51–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<date>1997</date>
<journal>Machine</journal>
<booktitle>Context as Filtering. Advances in Neural Information Processing Systems 18,</booktitle>
<pages>907--914</pages>
<marker>Mitchell, 1997</marker>
<rawString>T. Mitchell. 1997. Machine Learning. McGraw Hill. D. Mochihashi and Y. Matsumoto. 2006. Context as Filtering. Advances in Neural Information Processing Systems 18, pp. 907–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-B Park</author>
<author>B-T Zhang</author>
</authors>
<title>A Boosted Maximum Entropy Model for Learning Text Chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="7443" citStr="Park and Zhang, 2002" startWordPosition="1193" endWordPosition="1196">the ones distorted by a smoothing technique. A maximum entropy model can be considered as another way to avoid zero probability in n-gram models (Rosenfeld, 1996). Instead of constructing separate models and then interpolate them, it builds a single, combined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensive. In addition, its performance is mainly dependent on the relevance of knowledge sources, since the prior knowledge on the target problem is very important (Park and Zhang, 2002). Thus, when prior knowledge is not clear and computational cost is an important factor, n-gram models are more suitable than a maximum entropy model. Adapting features or contexts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate longdistance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (S</context>
</contexts>
<marker>Park, Zhang, 2002</marker>
<rawString>S.-B. Park and B.-T. Zhang. 2002. A Boosted Maximum Entropy Model for Learning Text Chunking. In Proceedings of the 19th International Conference on Machine Learning, pp. 482–489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Program for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="19864" citStr="Quinlan, 1993" startWordPosition="3305" endWordPosition="3306">r of instances in the training set is 8,766,578, that in the held-out set is 2,504,739, and that in test set is 1,252,371. Among the 1,252,371 test cases, the number of positive instances is 348,278, and that of negative instances is 904,093. Since about 72% of test cases are negative, this is the baseline of the automatic word spacing. 5.2 Experimental Results To evaluate the performance of the proposed method, two well-known machine learning algorithms are compared together. The tested machine learning algorithms are (i) decision tree and (ii) support vector machines. We use C4.5 release 8 (Quinlan, 1993) for decision tree induction and SVMtight (Joachims, 1998) for support vector machines. For all experiments with decision trees and support vector machines, the context size is set to two since the bigram shows the best performance in Figure 1. Table 1 gives the experimental results of various methods including machine learning algorithms and self-organizing n-gram model. The ‘selforganizing bigram’ in this table is the one proposed in this paper. The normal n-grams achieve an accuracy of around 88%, while decision tree and support vector machine produce that of around 89%. The self-organizing</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Program for Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>25</volume>
<pages>117--149</pages>
<contexts>
<context position="8138" citStr="Ron et al., 1996" startWordPosition="1302" endWordPosition="1305">nt factor, n-gram models are more suitable than a maximum entropy model. Adapting features or contexts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate longdistance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (Sch¨utze and Singer, 1994) adopted a variable memory Markov model proposed by (Ron et al., 1996), (Kim et al., 2003) applied selective extension of features to POS tagging, and (Dickinson and Meurers, 2005) expanded context of n-gram models to find errors in syntactic anno634 tation. In these methods, only neighbor words or features of the target n-grams became candidates to be added into the context. Since they required more information for better performance or detecting errors, only the context extension was considered. 3 Automatic Word Spacing by n-gram Model The problem of automatic word spacing can be regarded as a binary classification task. Let a sentence be given as S = w1w2 ...</context>
</contexts>
<marker>Ron, Singer, Tishby, 1996</marker>
<rawString>D. Ron, Y. Singer, and N. Tishby. 1996. The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length. Machine Learning, Vol. 25, No. 2, pp. 117–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<volume>10</volume>
<pages>187--228</pages>
<contexts>
<context position="6984" citStr="Rosenfeld, 1996" startWordPosition="1122" endWordPosition="1123">been proposed for construction of n-gram models (Chen and Goodman, 1996). Most of them belongs to one of two categories. One is to pretend each n-gram occurs once more than it actually did (Mitchell, 1996). The other is to interpolate n-grams with lower dimensional data (Jelinek and Mercer, 1980; Katz, 1987). However, these methods artificially modify the original distribution of corpus. Thus, the final probabilities used in learning with ngrams are the ones distorted by a smoothing technique. A maximum entropy model can be considered as another way to avoid zero probability in n-gram models (Rosenfeld, 1996). Instead of constructing separate models and then interpolate them, it builds a single, combined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensive. In addition, its performance is mainly dependent on the relevance of knowledge sources, since the prior knowledge on the target problem is very important (Park and Zhang, 2002). Thus, when prior knowledge is not clear and computational cost is an important factor, n-gram models are more suitable than a maximum entro</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, Vol. 10, pp. 187– 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
<author>Y Singer</author>
</authors>
<title>Part-of-Speech Tagging Using a Variable Memory Markov Model.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>181--187</pages>
<marker>Sch¨utze, Singer, 1994</marker>
<rawString>H. Sch¨utze and Y. Singer. 1994. Part-of-Speech Tagging Using a Variable Memory Markov Model. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pp. 181– 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
<author>M Ostendorf</author>
</authors>
<title>Variable N-Grams and Extensions for Conversational Speech Language Modeling.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<pages>63--75</pages>
<contexts>
<context position="7698" citStr="Siu and Ostendorf, 2000" startWordPosition="1236" endWordPosition="1240">ombined model to capture all the information provided by various knowledge sources. Even though a maximum entropy approach is simple, general, and strong, it is computationally very expensive. In addition, its performance is mainly dependent on the relevance of knowledge sources, since the prior knowledge on the target problem is very important (Park and Zhang, 2002). Thus, when prior knowledge is not clear and computational cost is an important factor, n-gram models are more suitable than a maximum entropy model. Adapting features or contexts has been an important issue in language modeling (Siu and Ostendorf, 2000). In order to incorporate longdistance features into a language model, (Rosenfeld, 1996) adopted triggers, and (Mochihashi and Mastumoto, 2006) used a particle filter. However, these methods are restricted to a specific language model. Instead of long-distance features, some other researchers tried local context extension. For this purpose, (Sch¨utze and Singer, 1994) adopted a variable memory Markov model proposed by (Ron et al., 1996), (Kim et al., 2003) applied selective extension of features to POS tagging, and (Dickinson and Meurers, 2005) expanded context of n-gram models to find errors </context>
</contexts>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>M. Siu and M. Ostendorf. 2000. Variable N-Grams and Extensions for Conversational Speech Language Modeling. IEEE Transactions on Speech and Audio Processing, Vol. 8, No. 1, pp. 63–75.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>