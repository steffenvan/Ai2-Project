<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.988933">
Modeling Review Comments
</title>
<author confidence="0.999028">
Arjun Mukherjee Bing Liu
</author>
<affiliation confidence="0.925733">
Department of Computer Science Department of Computer Science
University of Illinois at Chicago University of Illinois at Chicago
Chicago, IL 60607, USA Chicago, IL 60607, USA
</affiliation>
<email confidence="0.998075">
arjun4787@gmail.com liub@cs.uic.edu
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929789473684">
Writing comments about news articles,
blogs, or reviews have become a popular
activity in social media. In this paper, we
analyze reader comments about reviews.
Analyzing review comments is important
because reviews only tell the experiences
and evaluations of reviewers about the
reviewed products or services. Comments,
on the other hand, are readers’ evaluations
of reviews, their questions and concerns.
Clearly, the information in comments is
valuable for both future readers and brands.
This paper proposes two latent variable
models to simultaneously model and
extract these key pieces of information.
The results also enable classification of
comments accurately. Experiments using
Amazon review comments demonstrate the
effectiveness of the proposed models.
</bodyText>
<sectionHeader confidence="0.99836" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999905357142857">
Online reviews enable consumers to evaluate the
products and services that they have used. These
reviews are also used by other consumers and
businesses as a valuable source of opinions.
However, reviews only give the evaluations and
experiences of the reviewers. Often a reviewer may
not be an expert of the product and may misuse the
product or make other mistakes. There may also be
aspects of the product that the reviewer did not
mention but a reader wants to know. Some
reviewers may even write fake reviews to promote
some products, which is called opinion spamming
(Jindal and Liu 2008). To improve the online
review system and user experience, some review
hosting sites allow readers to write comments
about reviews (apart from just providing a
feedback by clicking whether the review is helpful
or not). Many reviews receive a large number of
comments. It is difficult for a reader to read them
to get a gist of them. An automated comment
analysis would be very helpful. Review comments
mainly contain the following information:
Thumbs-up or thumbs-down: Some readers may
comment on whether they find the review
useful in helping them make a buying decision.
Agreement or disagreement: Some readers who
comment on a review may be users of the
product themselves. They often state whether
they agree or disagree with the review. Such
comments are valuable as they provide a second
opinion, which may even identify fake reviews
because a genuine user often can easily spot
reviewers who have never used the product.
Question and answer: A commenter may ask for
clarification or about some aspects of the
product that are not covered in the review.
In this paper, we use statistical modeling to model
review comments. Two new generative models are
proposed. The first model is called the Topic and
Multi-Expression model (TME). It models topics
and different types of expressions, which represent
different types of comment posts:
</bodyText>
<listItem confidence="0.995103333333333">
1. Thumbs-up (e.g., “review helped me”)
2. Thumbs-down (e.g., “poor review”)
3. Question (e.g., “how to”)
</listItem>
<page confidence="0.987851">
320
</page>
<note confidence="0.9811195">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 320–329,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.8924065">
4. Answer acknowledgement (e.g., “thank you for
clarifying”). Note that we have no expressions
for answers to questions as there are usually no
specific phrases indicating that a post answers
a question except starting with the name of the
person who asked the question. However, there
are typical phrases for acknowledging answers,
thus answer acknowledgement expressions.
</bodyText>
<listItem confidence="0.9045195">
5. Disagreement (contention) (e.g., “I disagree”)
6. Agreement (e.g., “I agree”).
</listItem>
<bodyText confidence="0.999800458333334">
For ease of presentation, we call these
expressions the comment expressions (or C-
expressions). TME provides a basic model for
extracting these pieces of information and topics.
Its generative process separates topics and C-
expression types using a switch variable and treats
posts as random mixtures over latent topics and C-
expression types. The second model, called ME-
TME, improves TME by using Maximum-Entropy
priors to guide topic/expression switching. In short,
the two models provide a principled and integrated
approach to simultaneously discover topics and C-
expressions, which is the goal of this work. Note
that topics are usually product aspects in this work.
The extracted C-expressions and topics from
review comments are very useful in practice. First
of all, C-expressions enable us to perform more
accurate classification of comments, which can
give us a good evaluation of the review quality and
credibility. For example, a review with many
Disagreeing and Thumbs-down comments is
dubious. Second, the extracted C-expressions and
topics help identify the key product aspects that
people are troubled with in disagreements and in
questions. Our experimental results in Section 5
will demonstrate these capabilities of our models.
With these pieces of information, comments for
a review can be summarized. The summary may
include, but not limited to, the following: (1)
percent of people who give the review thumbs-up
or thumbs-down; (2) percent of people who agree
or disagree (or contend) with the reviewer; (3)
contentious (disagreed) aspects (or topics); (4)
aspects about which people often have questions.
To the best of our knowledge, there is no
reported work on such a fine-grained modeling of
review comments. The related works are mainly in
sentiment analysis (Pang and Lee, 2008; Liu
2012), e.g., topic and sentiment modeling, review
quality prediction and review spam detection.
However, our work is different from them. We will
compare with them in detail in Section 2.
The proposed models have been evaluated both
qualitatively and quantitatively using a large
number of review comments from Amazon.com.
Experimental results show that both TME and ME-
TME are effective in performing their tasks. ME-
TME also outperforms TME significantly.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.984012697674419">
We believe that this work is the first attempt to
model review comments for fine-grained analysis.
There are, however, several general research areas
that are related to our work.
Topic models such as LDA (Latent Dirichlet
Allocation) (Blei et al., 2003) have been used to
mine topics in large text collections. There have
been various extensions to multi-grain (Titov and
McDonald, 2008a), labeled (Ramage et al., 2009),
partially-labeled (Ramage et al., 2011), constrained
(Andrzejewski et al., 2009) models, etc. These
models produce only topics but not multiple types
of expressions together with topics. Note that in
labeled models, each document is labeled with one
or multiple labels. For our work, there is no label
for each comment. Our labeling is on topical terms
and C-expressions with the purpose of obtaining
some priors to separate topics and C-expressions.
In sentiment analysis, researchers have jointly
modeled topics and sentiment words (Lin and He,
2009; Mei et al., 2007; Lu and Zhai, 2008; Titov
and McDonald, 2008b; Lu et al., 2009; Brody and
Elhadad, 2010; Wang et al., 2010; Jo and Oh,
2011; Maghaddam and Ester, 2011; Sauper et al.,
2011; Mukherjee and Liu, 2012a). Our model is
more related to the ME-LDA model in (Zhao et al.,
2010), which used a switch variable trained with
Maximum-Entropy to separate topic and sentiment
words. We also use such a variable. However,
unlike sentiments and topics in reviews, which are
emitted in the same sentence, C-expressions often
interleave with topics across sentences and the
same comment post may also have multiple types
of C-expressions. Additionally, C-expressions are
mostly phrases rather than individual words. Thus,
a different model is required to model them.
There have also been works aimed at putting
authors in debate into support/oppose camps, e.g.,
(Galley et al., 2004; Agarwal et al., 2003;
Murakami and Raymond, 2010), modeling debate
discussions considering reply relations (Mukherjee
and Liu, 2012b), and identifying stances in debates
(Somasundaran and Wiebe, 2009; Thomas et al.,
</bodyText>
<page confidence="0.997779">
321
</page>
<bodyText confidence="0.9977181875">
2006; Burfoot et al., 2011). (Yano and Smith,
2010) also modeled the relationship of a blog post
and the number of comments it receives. These
works are different as they do not mine C-
expressions or discover the points of contention
and questions in comments.
In (Kim et al., 2006; Zhang and Varadarajan,
2006; Ghose and Ipeirotis, 2007; Liu et al., 2007;
Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur
and Rappoport 2009), various classification and
regression approaches were taken to assess the
quality of reviews. (Jindal and Liu, 2008; Lim et
al., 2010; Li et al. 2011; Ott et al., 2011;
Mukherjee et al., 2012) detect fake reviews and
reviewers. However, all these works are not
concerned with review comments.
</bodyText>
<sectionHeader confidence="0.958819" genericHeader="method">
3. The Basic TME Model
</sectionHeader>
<bodyText confidence="0.998752151515151">
This section discusses TME. The next section
discusses ME-TME, which improves TME. These
models belong to the family of generative models
for text where words and phrases (n-grams) are
viewed as random variables, and a document is
viewed as a bag of n-grams and each n-gram takes
a value from a predefined vocabulary. In this work,
we use up to 4-grams, i.e., n = 1, 2, 3, 4. For
simplicity, we use terms to denote both words
(unigrams or 1-grams) and phrases (n-grams). We
denote the entries in our vocabulary by ݒଵ...௏ where
ܸ is the number of unique terms in the vocabulary.
The entire corpus contains ݀ଵ...஽ documents. A
document (e.g., comment post) ݀ is represented as
a vector of terms ࢝ࢊ with ܰௗ entries. ܹ is the set of
all observed terms with cardinality, |ܹ |ൌ ∑ௗ ܰௗ .
The TME (Topic and Multi-Expression) model is
a hierarchical generative model motivated by the
joint occurrence of various types of expressions
indicating Thumbs-up, Thumbs-down, Question,
Answer acknowledgement, Agreement, and
Disagreement and topics in comment posts. As
before, these expressions are collectively called C-
expressions. A typical comment post mentions a
few topics (using semantically related topical
terms) and expresses some viewpoints with one or
more C-expression types (using semantically
related expressions). This observation motivates
the generative process of our model where
documents (posts) are represented as random
mixtures of latent topics and C-expression types.
Each topic or C-expression type is characterized by
a distribution over terms (words/phrases). Assume
</bodyText>
<figure confidence="0.955481">
(a) TME Model (b) ME-TME Model
</figure>
<figureCaption confidence="0.999082">
Figure 1: Graphical Models in plate notations.
</figureCaption>
<bodyText confidence="0.99221125">
we have ݐଵ...் topics and ݁ଵ...ா expression types in
our corpus. Note that in our case of Amazon
review comments, based on reading various posts,
we hypothesize that E = 6 as in such review
discussions, we mostly find 6 expression types
(more details in Section 5.1). Let ߰ௗ denote the
distribution of topics and C-expressions in a
document ݀ with ݎௗ,௝ א ሼݐ̂, ݁̂ሽ denoting the binary
indicator variable (topic or C-expression) for the
݆௧௛ term of ݀, ݓௗ,௝. ݖௗ,௝denotes the appropriate
topic or C-expression type index to which ݓௗ,௝
belongs. We parameterize multinomials over topics
using a matrix Θ஽ൈ்
் whose elements ߠௗ,௧
் signify the
probability of document ݀ exhibiting topic ݐ. For
simplicity of notation, we will drop the latter
subscript (ݐ in this case) when convenient and use
ߠௗ் to stand for the ݀௧௛ row of Θ். Similarly, we
define multinomials over C-expression types using
a matrix Θ஽ൈா
ா . The multinomials over terms
associated with each topic are parameterized by a
matrix Φ்ൈ௏
் , whose elements ߮௧,௩
் denote the
probability of generating ݒ from topic ݐ. Likewise,
multinomials over terms associated with each C-
expression type are parameterized by a matrix
Φாൈ௏
ா . We now define the generative process of
TME (see Figure 1(a)).
</bodyText>
<figure confidence="0.955363547619048">
A. For each C-expression type ݁, draw ߮௘ா~ܦ݅ݎሺߚாሻ
B. For each topic t, draw ߮௧்~ܦ݅ݎሺߚ்ሻ
C. For each comment post ݀ א ሼ1 ... ܦሽ:
i. Draw ߰ௗ~ܤ݁ݐܽሺߛ࢛ሻ
ii. Draw ߠௗா~ܦ݅ݎሺߙாሻ
iii. Draw ߠௗ்~ܦ݅ݎሺߙ்ሻ
iv. For each term ݓௗ,௝, ݆ א ሼ1 ... ܰௗሽ:
a. Draw ݎௗ,௝~ܤ݁ݎ݊݋ݑ݈݈݅ሺ߰ௗሻ
b. if (ݎௗ,௝ ൌ ݁̂ // ݓௗ,௝is a C-expression term
Draw ݖௗ,௝~ ܯݑ݈ݐሺߠௗா)
else // ݎௗ,௝ ൌݐ̂, ݓௗ,௝is a topical term
Draw ݖௗ,௝~ ܯݑ݈ݐሺߠௗ்)
c. Emit ݓௗ,௝~ܯݑ݈ݐሺ߮௭೏,ೕ
௥೏,ೕ)
flT
αT BT
�T
T
w
Nd
Z BE αE
r
y u
�E
E
V/
D
flE
flT
αT BT
�T
T
w Nd
Z BE αE
r
�E
E
X
V/
D
flE
λ
</figure>
<page confidence="0.989919">
322
</page>
<bodyText confidence="0.9998569375">
To learn the TME model from data, as exact
inference is not possible, we resort to approximate
inference using collapsed Gibbs sampling
(GrifÞths and Steyvers, 2004). Gibbs sampling is a
form of Markov Chain Monte Carlo method where
a Markov chain is constructed to have a particular
stationary distribution. In our case, we want to
construct a Markov chain which converges to the
posterior distribution over ܴ and ܼ conditioned on
the data. We only need to sample ݖ and ݎ as we use
collapsed Gibbs sampling and the dependencies of
ߠ and ߮ have been integrated out analytically in the
joint. Denoting the random variables ሼݓ, ݖ, ݎሽ by
singular subscriptsሼݓ௞, ݖ௞, ݎ௞ሽ, ݇ଵ...௄, where ܭ ൌ
∑ௗ ܰௗ, a single iteration consists of performing the
following sampling:
</bodyText>
<equation confidence="0.995840714285714">
݌ሺݖ௞ ൌ ݐ, ݎ௞ ൌ ݐ̂|ܹ൓௞, ܼ൓௞, ܴ൓௞, ݓ௞ ൌ ݒሻ ן
݌ሺݖ௞ ൌ ݁, ݎ௞ ൌ ݁̂|ܹ൓௞, ܼ൓௞, ܴ൓௞, ݓ௞ ൌ ݒሻ ן
ವಶ
௡೏ ಶ ൓ೖାఊ್ ௡೏,೐ ൓ೖାఈಶ ൈ
௡೏ ೅ା௡೏ ಶ ൓ೖାఊೌାఊ್ ൈ ௡೏,ሺ൉ሻ ାாఈಶ
ವಶ
൓ೖ
</equation>
<bodyText confidence="0.982051857142857">
where ݇ ൌ ሺ݀, ݆ሻ denotes the ݆௧௛ term of document
݀ and the subscript ൓݇ denotes assignments
excluding the term at ሺ݀, ݆ሻ. Counts݊௧,௩
஼் and ݊௘,௩
஼ா
denote the number of times term ݒ was assigned to
topic ݐ and expression type ݁ respectively. ݊ௗ,௧
</bodyText>
<equation confidence="0.472612">
஽் and
݊ௗ,௘
</equation>
<bodyText confidence="0.9999812">
஽ா denote the number of terms in document ݀ that
were assigned to topic ݐ and C-expression type ݁
respectively. Lastly, ݊ௗ் and ݊ௗா are the number of
terms in ݀ that were assigned to topics and C-
expression types respectively. Omission of the
latter index denoted by ሺ൉ሻ represents the
marginalized sum over the latter index. We employ
a blocked sampler jointly sampling ݎ and ݖ as this
improves convergence and reduces autocorrelation
of the Gibbs sampler (Rosen-Zvi et al., 2004).
Asymmetric Beta priors: Based on our initial
experiments with TME, we found that properly
setting the smoothing hyper-parameter ߛ࢛ is
crucial as it governs the topic/expression switch.
According to the generative process, ߰ௗ is the
(success) probability (of the Bernoulli distribution)
of emitting a topical/aspect term in a comment post
݀ and1 െ ߰ௗ, the probability of emitting a C-
expression term in ݀. Without loss of generality,
we draw ߰ௗ~ܤ݁ݐܽሺߛ࢛ሻ where ߛ is the
concentration parameter and ࢛ ൌ ሾݑܽ, ݑܾሿ is the
base measure. Without any prior belief, one resorts
to uniform base measure ݑ௔ ൌ ݑ௕ ൌ0.5 (i.e.,
assumes that both topical and C-expression terms
are equally likely to be emitted in a comment post).
This results in symmetric Beta priors
߰ௗ~ܤ݁ݐܽሺߛ௔,ߛ௕ሻ where ߛ௔ ൌ ߛݑ௔, ߛ௕ ൌ ߛݑ௕ and
ߛ௔ ൌ ߛ௕ ൌ ߛ/2. However, knowing the fact that
topics are more likely to be emitted than
expressions in a post apriori motivates us to take
guidance from asymmetric priors (i.e., we now
have a non-uniform base measure࢛). This
asymmetric setting of ߛ ensures that samples of ߰ௗ
are more close to the actual distribution of topical
terms in posts based on some domain knowledge.
Symmetric γ cannot utilize any prior knowledge. In
(Lin and He, 2009), a method was proposed to
incorporate domain knowledge during Gibbs
sampling initialization, but its effect becomes weak
as the sampling progresses (Jo and Oh, 2011).
For asymmetric priors, we estimate the hyper-
parameters from labeled data. Given a labeled set
ܦ௅, where we know the per post probability of C-
expression emission (1 െ ߰ௗሻ, we use the method
of moments to estimate ߛ ൌ ሾߛ௔, ߛ௕ሿ as follows:
</bodyText>
<equation confidence="0.998749">
ߛ௔ ൌ ߤ ቀఓሺଵିఓሻ
ఙ െ 1ቁ ,ߛ௕ ൌ ߛ௔ ቀଵ ఓ െ 1ቁ; ߤ ൌ ܧሾ߰ௗሿ,ߪ ൌ ܸܽݎሾ߰ௗሿ (3)
</equation>
<sectionHeader confidence="0.998642" genericHeader="method">
4. ME-TME Model
</sectionHeader>
<bodyText confidence="0.999957375">
The guidance of Beta priors, although helps, is still
relatively coarse and weak. We can do better to
produce clearer separation of topical and C-
expression terms. An alternative strategy is to
employ Maximum-Entropy (Max-Ent) priors
instead of Beta priors. The Max-Ent parameters
can be learned from a small number of labeled
topical and C-expression terms (words and
phrases) which can serve as good priors. The idea
is motivated by the following observation: topical
and C-expression terms typically play different
syntactic roles in a sentence. Topical terms (e.g.
“ipod” “cell phone”, “macro lens”, “kindle”, etc.)
tend to be noun and noun phrases while expression
terms (“I refute”, “how can you say”, “great
review”) usually contain pronouns, verbs, wh-
determiners, adjectives, and modals. In order to
utilize the part-of-speech (POS) tag information,
we move the topic/C-expression distribution ߰ௗ
(the prior over the indicator variable ݎௗ,௝) from the
document plate to the word plate (see Figure 1 (b))
and draw it from a Max-Ent model conditioned on
the observed feature vector ݔௗ,ఫሬሬሬሬԦ associated with
ݓௗ,௝ and the learned Max-Ent parameters ߣ. ݔௗ,௝ can
</bodyText>
<equation confidence="0.590612642857143">
಴೅
௡೟,ೡ൓ೖାఉ೅
௡೟,ሺ൉ሻ
಴೅ ା௏ఉ೅
൓ೖ
௡೏ ೅ ൓ೖା௡೏ ಶାఊೌାఊ್ ൈ ௡೏,ሺ൉ሻ൓ೖା்ఈ೅
ವ೅
௡೏೅൓ೖାఊೌ ௡೏,೟ ൓ೖାఈ೅ ൈ
ವ೅
௡೐,ೡ
಴ಶ ൓ೖାఉಶ
ఉಶ
௡೐ ಴ಶ ା௏,ሺ൉ሻ
൓ೖ
</equation>
<page confidence="0.988455">
323
</page>
<bodyText confidence="0.918585">
encode arbitrary contextual features for learning.
With Max-Ent priors, we have the new model ME-
TME. In this work, we encode both lexical and
POS features of the previous, current and next POS
tags/lexemes of the term ݓௗ,௝. More specifically,
ݔௗ,ఫሬሬሬሬሬሬԦ ൌ ቂܱܲܵ௪೏,ೕିଵ, ܱܲܵ௪೏,ೕ, ܱܲܵ௪೏,ೕାଵ, ݓௗ,௝ െ 1, ݓௗ,௝, ݓௗ,௝ ൅ 1ቃ
For phrasal terms (n-grams), all POS tags and
lexemes of ݓௗ,௝are considered as features.
Incorporating Max-Ent priors, the Gibbs sampler
of ME-TME is given by:
</bodyText>
<equation confidence="0.996331181818182">
݌ሺݖ௞ ൌ ݐ, ݎ௞ ൌ ݐ̂|ܹ൓௞, ܼ൓௞, ܴ൓௞, ݓ௞ ൌ ݒሻ ן
௘௫௣൫∑೙൯
೔సభ ఒ೔௙೔൫௫೏,ೕ,௧መ
ൈ
∑೤אሼ೐ෝ,೟෠ሽ௘௫௣൫∑೔సభఒ೔௙೔൫௫೏,ೕ,௬൯೙൯
݌ሺݖ௞ ൌ ݁, ݎ௞ ൌ ݁̂|ܹ൓௞,ܼ൓௞,ܴ൓௞, ݓ௞ ൌ ݒሻ ן
௘௫௣൫∑೔సభఒ೔௙೔൫௫೏,ೕ,௘̂൯೙൯ ൈ ವ ௡೐,ೡ
∑೤אሼ೐ෝ,೟෠ሽ௘௫௣൫∑೔సభఒ೔௙೔൫௫೏,ೕ,௬൯೙൯ ௡೏,ಶ೐൓ೖାఈಶ ൈ ಴ಶ ൓ೖାఉಶ (5)
௡೏,ሺ൉ሻ ಴ಶ ఉಶ
ವಶ ାாఈಶ ௡೐,ሺ൉ሻ൓ೖା௏
൓ೖ
</equation>
<bodyText confidence="0.997500333333333">
where ߣଵ...௡ are the parameters of the learned Max-
Ent model corresponding to the ݊ binary feature
functions ݂ଵ...௡ from Max-Ent.
</bodyText>
<sectionHeader confidence="0.996853" genericHeader="evaluation">
5. Evaluation
</sectionHeader>
<bodyText confidence="0.9893415">
We now evaluate the proposed TME and ME-TME
models. Specifically, we evaluate the discovered
C-expressions, contentious aspects, and aspects
often mentioned in questions.
</bodyText>
<subsectionHeader confidence="0.611798">
5.1 Dataset and Experiment SettingsWe crawled
</subsectionHeader>
<bodyText confidence="0.999487615384615">
comments of reviews in Amazon.com for a variety
of products. For each comment we extracted its id,
the comment author id, the review id on which it
commented, and the review author id. Our
database consisted of 21,316 authors, 37,548
reviews, and 88,345 comments with an average of
124 words per comment post.
For all our experiments, the hyper-parameters
for TME and ME-TME were set to the heuristic
values αT = 50/T, αE = 50/E, βT = βE = 0.1 as
suggested in (GrifÞths and Steyvers, 2004). For ߛ,
we estimated the asymmetric Beta priors using the
method of moments discussed in Section 3. We
sampled 1000 random posts and for each post we
identified the C-expressions emitted. We thus
computed the per-post probability of C-expression
emission (1 െ ߰ௗሻ and used Eq. (3) to get the final
estimates, ߛ௔ = 3.66, ߛ௕= 1.21. To learn the Max-
Ent parameters ߣ, we randomly sampled 500 terms
from our corpus appearing at least 10 times and
labeled them as topical (332) or C-expressions
(168) and used the corresponding feature vector of
each term (in the context of posts where it occurs)
to train the Max-Ent model. We set the number of
topics, T = 100 and the number of C-expression
types, E = 6 (Thumbs-up, Thumbs-down, Question,
Answer acknowledgement, Agreement and
Disagreement) as in review comments, we usually
find these six dominant expression types. Note that
knowing the exact number of topics, T and
expression types, E in a corpus is difficult. While
non-parametric Bayesian approaches (Teh et al.,
2006) aim to estimate T from the corpus, in this
work the heuristic values obtained from our initial
experiments produced good results. We also tried
increasing E to 7, 8, etc. However, it did not
produce any new dominant expression type.
Instead, the expression types became less specific
as the expression term space became sparser.
</bodyText>
<subsectionHeader confidence="0.97319">
5.2 C-Expression Evaluation
</subsectionHeader>
<bodyText confidence="0.99961625">
We now evaluate the discovered C-expressions.
We first evaluate them qualitatively in Tables 1
and 2. Table 1 shows the top terms of all
expression types using the TME model. We find
that TME can discover and cluster many correct C-
expressions, e.g., “great review”, “review helped
me” in Thumbs-up; “poor review”, “very unfair
review” in Thumbs-down; “how do I”, “help me
decide” in Question; “good reply”, “thank you for
clarifying” in Answer Acknowledgement; “I
disagree”, “I refute” in Disagreement; and “I
agree”, “true in fact” in Agreement. However, with
the guidance of Max-Ent priors, ME-TME did
much better (Table 2). For example, we find “level
headed review”, “review convinced me” in
Thumbs-up; “biased review”, “is flawed” in
Thumbs-down; “any clues”, “I was wondering
how” in Question; “clears my”, “valid answer” in
Answer-acknowledgement; “I don’t buy your”,
“sheer nonsense” in Disagreement; “agree
completely”, “well said” in Agreement. These
newly discovered phrases by ME-TME are marked
in blue in Table 3. ME-TME also has fewer errors.
Next, we evaluate them quantitatively using the
metric precision @ n, which gives the precision at
different rank positions. This metric is appropriate
here because the C-expressions (according to top
terms in ΦE) produced by TME and ME-TME are
rankings. Table 3 reports the precisions @ top 25,
50, 75, and 100 rank positions for all six
expression types across both models. We evaluated
till top 100 positions because it is usually
</bodyText>
<figure confidence="0.6590242">
௡ ವ೅ ௡೟,ೡ಴೅ ൓ೖାఉ೅ ା௏ఉ೅ (4)
೏,೟ ൓ೖାఈ೅ ൈ ௡೟,ሺ൉ሻ
௡೏,ሺ൉ሻ ಴೅
ವ೅ ା்ఈ೅ ൓ೖ
൓ೖ
</figure>
<page confidence="0.990263">
324
</page>
<construct confidence="0.8510032">
Thumbs-up (e1): review, thanks, great review, nice review, time,
best review, appreciate, you, your review helped, nice, terrific,
review helped me, good critique, very, assert, wrong, useful
review, don’t, misleading, thanks a lot, ...
Thumbs-down (e2): review, no, poor review, imprecise, you,
</construct>
<figureCaption confidence="0.810519">
Figure 5: Precision @ top 50
</figureCaption>
<construct confidence="0.508273285714286">
complaint, very, suspicious, bogus review, absolutely, credible,
very unfair review, criticisms, true, disregard this review, disagree
with, judgment, without owning, ...
Question (e3): question, my, I, how do I, why isn’t, please explain,
good answer, clarify, don’t understand, my doubts, I’m confused,
does not, understand, help me decide, how to, yes, answer, how
can I, can’t explain, ...
</construct>
<bodyText confidence="0.438281">
Answer Acknowledgement (e4): my, informative, answer, good
reply, thank you for clarifying, answer doesn’t, good answer,
vague, helped me choose, useful suggestion, don’t understand,
cannot explain, your answer, doubts, answer isn’t, ...
</bodyText>
<tableCaption confidence="0.992273974358974">
Disagreement (e5): disagree, I, don’t, I disagree, argument claim, I
reject, I refute, I refuse, oppose, debate, accept, don’t agree, quote,
sense, would disagree, assertions, I doubt, right, your, really,
you, I’d disagree, cannot, nonsense,...
Agreement (e6): yes, do, correct, indeed, no, right, I agree, you,
agree, I accept, very, yes indeed, true in fact, indeed correct, I’d
agree, completely, true, but, doesn’t, don’t, definitely, false,
completely agree, agree with your, true, ...
Table 1: Top terms (comma delimited) of six expression types
e1, e2, e3, e4, e5, e6 (ΦE) using TME model. Red (bold) colored
terms denote possible errors
Thumbs-up (e1): review, you, great review, I&apos;m glad I read, best
review, review convinced me, review helped me, good review, terrific
review, job, thoughtful review, awesome review, level headed review,
good critique, good job, video review,...
Thumbs-down (e2): review, you, bogus review, con, useless review,
ridiculous, biased review, very unfair review, is flawed, completely,
skeptical, badmouth, misleading review, cynical review, wrong,
disregard this review, seemingly honest, ...
Question (e3): question, I, how do I, why isn’t, please explain, clarify,
any clues, answer, please explain, help me decide, vague, how to, how
do I, where can I, how to set, I was wondering how, could you explain,
how can I, can I use, ...
Answer Acknowledgement (e4): my, good reply, , answer, reply,
helped me choose, clears my, valid answer, answer doesn’t,
satisfactory answer, can you clarify, informative answer, useful
suggestion, perfect answer, thanks for your reply, doubts, ...
Disagreement (e5): disagree, I, don’t, I disagree, doesn’t, I don’t buy
your, credible, I reject, I doubt, I refuse, I oppose, sheer nonsense,
hardly, don’t agree, can you prove, you have no clue, how do you say,
sense, you fail, contradiction, ...
Agreement (e6): I, do, agree, point, yes, really, would agree, you,
agree, I accept, claim, agree completely, personally agree, true in fact,
indeed correct, well said, valid point, correct, never meant, might not,
definitely agree,...
Table 2: Top terms (comma delimited) of six expression types
using ME-TME model. Red (bold) terms denote possible errors.
Blue (italics) terms denote those newly discovered by the model;
rest (black) were used in Max-Ent training.
</tableCaption>
<bodyText confidence="0.9999607">
important to see whether a model can discover and
rank those major expressions of a type at the top.
We believe that top 100 are sufficient for most
applications. From Table 3, we observe that ME-
TME consistently outperforms TME in precisions
across all expression types and all rank positions.
This shows that Max-Ent priors are more effective
in discovering expressions than Beta priors. Note
that we couldn’t compare with an existing baseline
because there is no reported study on this problem.
</bodyText>
<subsectionHeader confidence="0.995819">
5.3 Comment Classification
</subsectionHeader>
<bodyText confidence="0.992025791666666">
Here we show that the discovered C-expressions
can help comment classification. Note that since a
comment can belong to one or more types (e.g., a
comment can belong to both Thumbs-up and
Agreement types), this task is an instance of multi-
label classification, i.e., an instance can have more
than one class label. In order to evaluate all the
expression types, we follow the binary approach
which is an extension of one-against-all method for
multi-label classification. Thus, for each label, we
build a binary classification problem. Instances
associated with that label are in one class and the
rest are in the other class. To perform this task, we
randomly sampled 2000 comments, and labeled
each of them into one or more of the following 8
labels: Thumbs-up, Thumbs-down, Disagreement,
Agreement, Question, Answer-Acknowledgement,
Answer, and None, which have 432, 401, 309, 276,
305, 201, 228, and 18 comments respectively. We
disregard the None category due to its small size.
This labeling is a fairly easy task as one can almost
certainly make out to which type a comment
belongs. Thus we didn’t use multiple labelers. The
distribution reveals that the labels are overlapping.
For instance, we found many comments belonging
to both Thumbs-down and Disagreement, Thumbs-up
with Acknowledgement and with Question.
For supervised classification, the choice of
feature is a key issue. While word and POS n-
grams are traditional features, such features may
not be the best for our task. We now compare such
features with the C-expressions discovered by the
proposed models. We used the top 1000 terms
from each of the 6 C-expression rankings as
features. As comments in Question type mostly use
the punctuation “?”, we added it in our feature set.
We use precision, recall and F1 as our metric to
compare classification performance using a trained
SVM (linear kernel). All results (Table 4) were
computed using 10-fold cross-validation (CV). We
also tried Naïve Bayes and Logistic Regression
classifiers, but they were poorer than SVM. Hence
their results are not reported due to space
constraints. As a separate experiment (not shown
here also due to space constraints), we analyzed
the classification performance by varying the
number of top terms from 200, 400,..., 1000, 1200,
etc. and found that the F1 scores stabilized after top
</bodyText>
<page confidence="0.996513">
325
</page>
<table confidence="0.999970125">
C-Expression Type P@25 P@50 P@75 P@100
TME ME-TME TME ME-TME TME ME-TME TME ME-TME
Thumbs-up 0.60 0.80 0.66 0.78 0.60 0.69 0.55 0.64
Thumbs-down 0.68 0.84 0.70 0.80 0.63 0.67 0.60 0.65
Question 0.64 0.80 0.68 0.76 0.65 0.72 0.61 0.67
Answer-Acknowledgement 0.68 0.76 0.62 0.72 0.57 0.64 0.54 0.58
Disagreement 0.76 0.88 0.74 0.80 0.68 0.73 0.65 0.70
Agreement 0.72 0.80 0.64 0.74 0.61 0.70 0.60 0.69
</table>
<tableCaption confidence="0.997412">
Table 3: Precision @ top 25, 50, 75, and 100 rank positions for all C-expression types.
</tableCaption>
<table confidence="0.999865875">
Features Thumbs-up Thumbs-down Question Answer-Ack. Disagreement Agreement Answer
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
W+POS 1-gram 0.68 0.66 0.67 0.65 0.65 0.65 0.71 0.68 0.69 0.64 0.61 0.62 0.73 0.72 0.72 0.67 0.65 0.66 0.58 0.57 0.57
W+POS 1-2 gram 0.72 0.69 0.70 0.68 0.67 0.67 0.74 0.69 0.71 0.69 0.63 0.65 0.76 0.75 0.75 0.71 0.69 0.70 0.60 0.57 0.58
W+POS, 1-3 gram 0.73 0.71 0.72 0.69 0.68 0.68 0.75 0.69 0.72 0.70 0.64 0.66 0.76 0.76 0.76 0.72 0.70 0.71 0.61 0.58 0.59
W+POS, 1-4 gram 0.74 0.72 0.73 0.71 0.68 0.69 0.75 0.70 0.72 0.70 0.65 0.67 0.77 0.76 0.76 0.73 0.70 0.71 0.61 0.58 0.59
C-Expr. OE, TME 0.82 0.74 0.78 0.77 0.71 0.74 0.83 0.75 0.78 0.75 0.72 0.73 0.83 0.80 0.81 0.78 0.75 0.76 0.66 0.61 0.63
C-Expr. OE, ME-TME 0.87 0.79 0.83 0.80 0.73 0.76 0.87 0.76 0.81 0.77 0.72 0.74 0.86 0.81 0.83 0.81 0.77 0.79 0.67 0.61 0.64
</table>
<tableCaption confidence="0.989674">
Table 4: Precision (P), Recall (R), and F1 scores of binary classification using SVM and different features. The
improvements of our models are significant (p&lt;0.001) over paired t-test across 10-fold cross validation.
</tableCaption>
<table confidence="0.99997025">
D OE + Noun/Noun Phrase TME ME-TME
J1 J2 J1 J2 J1 J2
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
D1 0.62 0.70 0.66 0.58 0.67 0.62 0.66 0.75 0.70 0.62 0.70 0.66 0.67 0.79 0.73 0.64 0.74 0.69
D2 0.61 0.67 0.64 0.57 0.63 0.60 0.66 0.72 0.69 0.62 0.67 0.64 0.68 0.75 0.71 0.64 0.71 0.67
D3 0.60 0.69 0.64 0.56 0.64 0.60 0.64 0.73 0.68 0.60 0.67 0.63 0.67 0.76 0.71 0.63 0.72 0.67
D4 0.59 0.68 0.63 0.55 0.65 0.60 0.63 0.71 0.67 0.59 0.68 0.63 0.65 0.73 0.69 0.62 0.71 0.66
Avg. 0.61 0.69 0.64 0.57 0.65 0.61 0.65 0.73 0.69 0.61 0.68 0.64 0.67 0.76 0.71 0.63 0.72 0.67
</table>
<tableCaption confidence="0.995405">
Table 5 (a)
</tableCaption>
<table confidence="0.999972875">
D OE + Noun/Noun Phrase TME ME-TME
J1 J2 J1 J2 J1 J2
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
D1 0.57 0.65 0.61 0.54 0.63 0.58 0.61 0.69 0.65 0.58 0.66 0.62 0.64 0.73 0.68 0.61 0.70 0.65
D2 0.61 0.66 0.63 0.58 0.61 0.59 0.64 0.68 0.66 0.60 0.64 0.62 0.68 0.70 0.69 0.65 0.69 0.67
D3 0.60 0.68 0.64 0.57 0.64 0.60 0.64 0.71 0.67 0.62 0.68 0.65 0.67 0.72 0.69 0.64 0.69 0.66
D4 0.56 0.67 0.61 0.55 0.65 0.60 0.60 0.72 0.65 0.58 0.68 0.63 0.63 0.75 0.68 0.61 0.71 0.66
Avg. 0.59 0.67 0.62 0.56 0.63 0.59 0.62 0.70 0.66 0.60 0.67 0.63 0.66 0.73 0.69 0.63 0.70 0.66
</table>
<tableCaption confidence="0.7597195">
Table 5 (b)
Table 5: Points of Contention (a), Questioned aspects (b). D1: Ipod, D2: Kindle, D3: Nikon, D4: Garmin. We report the
average precision (P), recall (R), and F1 score over 100 comments for each particular domain.
Statistical significance: Differences between Nearest Noun Phrase and TME for both judges (J1, J2) across all domains were
significant at 97% confidence level (p&lt;0.03). Differences among TME and ME-TME for both judges (J1, J2) across all
domains were significant at 95% confidence level (p&lt;0.05). A paired t-test was used for testing significance.
</tableCaption>
<bodyText confidence="0.989842178571429">
1000 terms. From Table 4, we see that F1 scores
dramatically increase with C-expression (Φா)
features for all expression types. TME and ME-
TME progressively improve the classification.
Improvements of TME and ME-TME being
significant (p&lt;0.001) using a paired t-test across
10-fold cross validations shows that the discovered
C-expressions are of high quality and useful.
We note that the annotation resulted in a new
label “Answer” which consists of mostly replies to
comments with questions. Since an “answer” to a
question usually does not show any specific
expression, it does not attain very good F1 scores.
Thus, to improve the performance of the Answer
type comments, we added three binary features for
each comment c on top of C-expression features:
i) Is the author of c the review author too? The
idea here is that most of the times the reviewer
answers the questions raised in comments.
ii) Is there any comment posted before c by some
author a which has been previously classified
as a question post?
iii) Is there any comment posted after c by author
a that replies to c (using @name) and is an
Answer-Acknowledgement comment (which
again has been previously classified as such)?
Using these additional features, we obtained a
precision of 0.78 and a recall of 0.73 yielding an F1
</bodyText>
<page confidence="0.99773">
326
</page>
<bodyText confidence="0.999332">
score of 0.75 which is a dramatic increase beyond
0.64 achieved by ME-TME in Table 4.
</bodyText>
<subsectionHeader confidence="0.998996">
5.4 Contention Points and Questioned Aspects
</subsectionHeader>
<bodyText confidence="0.9902717">
We now turn to the task of discovering points of
contention in disagreement comments and aspects
(or topics) raised in questions. By “points”, we
mean the topical terms on which some contentions
or disagreements have been expressed. Topics
being the product aspects are also indirectly
evaluated in this task. We employ the TME and
ME-TME models in the following manner.
We only detail the approach for disagreement
comments. The same method is applied to question
comments. Given a disagreement comment post ݀,
we first select the top k topics that are mentioned in
d according to its topic distribution, ߠௗ். Let ܶௗ be
the set of these top ݇ topics in ݀. Then, for each
ா
disagreement expression ݓ א ݀ ת ߮௘ୀ஽௜௦௔௚௥௘௘௠௘௡௧ ,
we emit the topical terms (words/phrases) of topics
in ܶௗwhich appear within a word window of ݍ from
ݓ in ݀. More precisely, we emit the set ܣ ൌ ሼݓ|ݓ א
݀ ת ߮௧், ݐ א ܶௗ, |݌݋ݏ݅ሺݓሻ െ ݌݋ݏ݅ሺݒሻ |൑ ݍሽ, where
posi(·) returns the position index of the word or
phrase in document ݀. To compute the intersection
ݓ א ݀ ת ߮௧், we need a threshold. This is so
because the Dirichlet distribution has a smoothing
effect which assigns some non-zero probability
mass to every term in the vocabulary for each topic
ݐ. So for computing the intersection, we considered
only terms in ߮௧் which have ݌ሺݒ|ݐሻ ൌ ߮௧,௩
் &gt; 0.001
as probability masses lower than 0.001 are more
due to the smoothing effect of the Dirichlet
distribution than true correlation. In an actual
application, the values for ݇ and ݍ can be set
according to the user’s need. In our experiment, we
used ݇ = 3 and ݍ = 5, which are reasonable because
a post normally does not talk about many topics
(݇), and the contention points (aspect terms) appear
quite close to the disagreement expressions.
For comparison, we also designed a baseline.
For each disagreement (or question) expression
</bodyText>
<equation confidence="0.873946">
ா
ݓ א ݀ ת ߮௘ୀ஽௜௦௔௚௥௘௘௠௘௡௧
</equation>
<bodyText confidence="0.999306128205128">
ா (߮௘ୀொ௨௘௦௧௜௢௡), we emit the
nouns and noun phrases within the same window ݍ
as the points of contention (question) in ݀. This
baseline is reasonable because topical terms are
usually nouns and noun phrases and are near
disagreement (question) expressions. We note that
this baseline cannot stand alone because it has to
rely on our expression models Φா of ME-TME.
Next, to evaluate the performance of these
methods in discovering points of contention, we
randomly selected 100 disagreement (contentious)
(and 100 question) comment posts on reviews from
each of the 4 product domains: Ipod, Kindle,
Nikon Cameras, and Garmin GPS in our database
and employed the aforementioned methods to
discover the points of contention (question) in each
post. Then we asked two human judges (graduate
students fluent in English) to manually judge the
results produced by each method for each post. We
asked them to report the precision of the
discovered terms for a post by judging them as
being indeed valid points of contention and report
recall in a post by judging how many of actually
contentious points in the post were discovered. In
Table 5 (a), we report the average precision and
recall for 100 posts in each domain by the two
judges J1 and J2 for different methods on the task
of discovering points (aspects) of contention. In
Table 5 (b), similar results are reported for the task
of discovering questioned aspects in 100 question
comments for each product domain. Since this
judging task is subjective, the differences in the
results from the two judges are not surprising. Our
judges were made to work in isolation to prevent
any bias. We observe that across all domains, ME-
TME again performs the best consistently. Note
that agreement study using Kappa is not used here
as our problem is not to label a fixed set of items
categorically by the judges.
</bodyText>
<sectionHeader confidence="0.99868" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999902923076923">
This paper proposed the problem of modeling
review comments, and presented two models TME
and ME-TME to model and to extract topics
(aspects) and various comment expressions. These
expressions enable us to classify comments more
accurately, and to find contentious aspects and
questioned aspects. These pieces of information
also allow us to produce a simple summary of
comments for each review as discussed in Section
1. To our knowledge, this is the first attempt to
analyze comments in such details. Our experiments
demonstrated the efficacy of the models. ME-TME
also outperformed TME significantly.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<copyright confidence="0.3765745">
This work is supported in part by National Science
Foundation (NSF) under grant no. IIS-1111092.
</copyright>
<page confidence="0.994845">
327
</page>
<sectionHeader confidence="0.990117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752836734694">
Agarwal, R., S. Rajagopalan, R. Srikant, Y. Xu. 2003.
Mining newsgroups using networks arising from
social behavior. Proceedings of International
Conference on World Wide Web 2003.
Andrzejewski, D., X. Zhu, M. Craven. 2009.
Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. Proceedings of
International Conference on Machine Learning.
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. Journal of Machine Learning Research.
Brody, S. and S. Elhadad. 2010. An Unsupervised
Aspect-Sentiment Model for Online Reviews.
Proceedings of the Annual Conference of the North
American Chapter of the ACL.
Burfoot, C., S. Bird, and T. Baldwin. 2011. Collective
Classification of Congressional Floor-Debate
Transcripts. Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Galley, M., K. McKeown, J. Hirschberg, E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of Bayesian networks to
model pragmatic dependencies. Proceedings of the
42th Annual Meeting of the Association of
Computational Linguistics.
Ghose, A. and P. Ipeirotis. 2007. Designing novel
review ranking systems: predicting the usefulness
and impact of reviews. Proceedings of International
Conference on Electronic Commerce.
GrifÞths, T. and M. Steyvers. 2004. Finding scientific
topics. Proceedings of National Academy of
Sciences.
Kim, S., P. Pantel, T. Chklovski, and M. Pennacchiotti.
2006. Automatically assessing review helpfulness.
Proceedings of Empirical Methods in Natural
Language Processing.
Jindal, N. and B. Liu. 2008. Opinion spam and analysis.
Proceedings of the ACM International Conference on
Web Search and Web Data Mining.
Jo, Y. and A. Oh. 2011. Aspect and sentiment
unification model for online review analysis.
Proceedings of the ACM International Conference
on Web Search and Web Data Mining.
Li, F., M. Huang, Y. Yang, and X. Zhu. 2011. Learning
to Identify Review Spam. in Proceedings of the
International Joint Conference on Artificial
Intelligence.
Lim, E., V. Nguyen, N. Jindal, B. Liu, and H. Lauw.
2010. Detecting Product Review Spammers using
Rating Behaviors. Proceedings of the ACM
International Conference on Information and
Knowledge Management.
Lin, C. and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. Proceedings of the ACM
International Conference on Information and
Knowledge Management.
Liu, J., Y. Cao, C. Lin, Y. Huang, and M. Zhou. 2007.
Low-quality product review detection in opinion
summarization. Proceedings of Empirical Methods in
Natural Language Processing.
Liu, B. 2012. Sentiment Analysis and Opinion Mining.
Morgan &amp; Claypool publishers (to appear in June
2012).
Liu, Y., X. Huang, A. An, and X. Yu. 2008. Modeling
and predicting the helpfulness of online reviews.
Proceedings of IEEE International Conference on
Data Mining.
Lu, Y. and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. Proceedings of
International Conference on World Wide Web.
Lu, Y., C. Zhai, and N. Sundaresan. 2009. Rated aspect
summarization of short comments. Proceedings of
International Conference on World Wide.
Mei, Q. X. Ling, M. Wondra, H. Su and C. Zhai. 2007.
Topic sentiment mixture: modeling facets and
opinions in weblogs. Proceedings of International
Conference on World Wide.
Moghaddam, S. and M. Ester. 2011. ILDA:
interdependent LDA model for learning latent
aspects and their ratings from online product reviews.
Proceedings of Annual ACM SIGIR Conference on
Research and Development in Information Retrieval.
Mukherjee, A. and B. Liu. 2012a. Aspect Extraction
through Semi-Supervised Modeling. Proceedings of
50th Annual Meeting of Association for
Computational Linguistics (to appear in July 2012).
Mukherjee, A. and B. Liu. 2012b. Mining Contentions
from Discussions and Debates. Proceedings of ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (to appear in August
2012).
Mukherjee, A., B. Liu and N. Glance. 2012. Spotting
Fake Reviewer Groups in Consumer Reviews.
Proceedings of International World Wide Web
Conference.
Murakami A., and R. Raymond, 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions.
Proceedings of International Conference on
</reference>
<page confidence="0.984376">
328
</page>
<reference confidence="0.999363102941177">
Computational Linguistics.
O&apos;Mahony, M. P. and B. Smyth. 2009. Learning to
recommend helpful hotel reviews. Proceedings of the
third ACM conference on Recommender systems.
Ott, M., Y. Choi, C. Cardie, and J. T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Pang, B. and L. Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Ramage, D., D. Hall, R. Nallapati, and C. Manning.
2009. Labeled LDA: A supervised topic model for
credit attribution in multi-labeled corpora.
Proceedings of Empirical Methods in Natural
Language Processing.
Ramage, D., C. Manning, and S. Dumais. 2011 Partially
labeled topic models for interpretable text mining.
Proceedings of the 17th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smith.
2004. The author-topic model for authors and
documents. Uncertainty in Artificial Intelligence.
Sauper, C. A. Haghighi and R. Barzilay. 2011. Content
models with attitude. Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics.
Somasundaran, S., J. Wiebe. 2009. Recognizing stances
in online debates. Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the
AFNLP
Teh, Y., M. Jordan, M. Beal and D. Blei. 2006.
Hierarchical Dirichlet Processes. Journal of the
American Statistical Association.
Thomas, M., B. Pang and L. Lee. 2006. Get out the
vote: Determining support or opposition from
Congressional ßoor-debate transcripts. Proceedings
of Empirical Methods in Natural Language
Processing.
Titov, I. and R. McDonald. 2008a. Modeling online
reviews with multi-grain topic models. Proceedings
of International Conference on World Wide Web.
Titov, I. and R. McDonald. 2008b. A joint model of text
and aspect ratings for sentiment summarization.
Proceedings of Annual Meeting of the Association
for Computational Linguistics.
Tsur, O. and A. Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. Proceedings of the International AAAI
Conference on Weblogs and Social Media.
Wang, H., Y. Lu, and C. Zhai. 2010. Latent aspect
rating analysis on review text data: a rating
regression approach. Proceedings of ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining.
Yano, T and N. Smith. 2010. What’s Worthy of
Comment? Content and Comment Volume in
Political Blogs. Proceedings of the International
AAAI Conference on Weblogs and Social Media.
Zhang, Z. and B. Varadarajan. 2006. Utility scoring of
product reviews. Proceedings of ACM International
Conference on Information and Knowledge
Management.
Zhao, X., J. Jiang, H. Yan, and X. Li. 2010. Jointly
modeling aspects and opinions with a MaxEnt-LDA
hybrid. Proceedings of Empirical Methods in Natural
Language Processing.
</reference>
<page confidence="0.999188">
329
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983544">
<title confidence="0.999993">Modeling Review Comments</title>
<author confidence="0.997094">Arjun Mukherjee Bing Liu</author>
<affiliation confidence="0.999931">Department of Computer Science Department of Computer Science University of Illinois at Chicago University of Illinois at Chicago</affiliation>
<address confidence="0.994826">Chicago, IL 60607, USA Chicago, IL 60607, USA</address>
<email confidence="0.998545">arjun4787@gmail.comliub@cs.uic.edu</email>
<abstract confidence="0.99961905">Writing comments about news articles, blogs, or reviews have become a popular activity in social media. In this paper, we analyze reader comments about reviews. Analyzing review comments is important because reviews only tell the experiences and evaluations of reviewers about the reviewed products or services. Comments, on the other hand, are readers’ evaluations of reviews, their questions and concerns. Clearly, the information in comments is valuable for both future readers and brands. This paper proposes two latent variable models to simultaneously model and extract these key pieces of information. The results also enable classification of comments accurately. Experiments using Amazon review comments demonstrate the effectiveness of the proposed models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agarwal</author>
<author>S Rajagopalan</author>
<author>R Srikant</author>
<author>Y Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>Proceedings of International Conference on World Wide Web</booktitle>
<contexts>
<context position="7908" citStr="Agarwal et al., 2003" startWordPosition="1229" endWordPosition="1232">010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; </context>
</contexts>
<marker>Agarwal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Agarwal, R., S. Rajagopalan, R. Srikant, Y. Xu. 2003. Mining newsgroups using networks arising from social behavior. Proceedings of International Conference on World Wide Web 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Andrzejewski</author>
<author>X Zhu</author>
<author>M Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6534" citStr="Andrzejewski et al., 2009" startWordPosition="1006" endWordPosition="1009"> that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>Andrzejewski, D., X. Zhu, M. Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="6286" citStr="Blei et al., 2003" startWordPosition="970" endWordPosition="973"> work is different from them. We will compare with them in detail in Section 2. The proposed models have been evaluated both qualitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>S Elhadad</author>
</authors>
<title>An Unsupervised Aspect-Sentiment Model for Online Reviews.</title>
<date>2010</date>
<booktitle>Proceedings of the Annual Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="7111" citStr="Brody and Elhadad, 2010" startWordPosition="1101" endWordPosition="1104">2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individua</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Brody, S. and S. Elhadad. 2010. An Unsupervised Aspect-Sentiment Model for Online Reviews. Proceedings of the Annual Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burfoot</author>
<author>S Bird</author>
<author>T Baldwin</author>
</authors>
<title>Collective Classification of Congressional Floor-Debate Transcripts.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8135" citStr="Burfoot et al., 2011" startWordPosition="1261" endWordPosition="1264">pressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect f</context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>Burfoot, C., S. Bird, and T. Baldwin. 2011. Collective Classification of Congressional Floor-Debate Transcripts. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>Proceedings of the 42th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="7886" citStr="Galley et al., 2004" startWordPosition="1225" endWordPosition="1228">el in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Ma</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Galley, M., K. McKeown, J. Hirschberg, E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies. Proceedings of the 42th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghose</author>
<author>P Ipeirotis</author>
</authors>
<title>Designing novel review ranking systems: predicting the usefulness and impact of reviews.</title>
<date>2007</date>
<booktitle>Proceedings of International Conference on Electronic Commerce.</booktitle>
<contexts>
<context position="8444" citStr="Ghose and Ipeirotis, 2007" startWordPosition="1314" endWordPosition="1317">ors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as</context>
</contexts>
<marker>Ghose, Ipeirotis, 2007</marker>
<rawString>Ghose, A. and P. Ipeirotis. 2007. Designing novel review ranking systems: predicting the usefulness and impact of reviews. Proceedings of International Conference on Electronic Commerce.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T GrifÞths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of National Academy of Sciences.</booktitle>
<contexts>
<context position="12435" citStr="GrifÞths and Steyvers, 2004" startWordPosition="1996" endWordPosition="1999">or each topic t, draw ߮௧்~ܦ݅ݎሺߚ்ሻ C. For each comment post ݀ א ሼ1 ... ܦሽ: i. Draw ߰ௗ~ܤ݁ݐܽሺߛሻ ii. Draw ߠௗா~ܦ݅ݎሺߙாሻ iii. Draw ߠௗ்~ܦ݅ݎሺߙ்ሻ iv. For each term ݓௗ,, ݆ א ሼ1 ... ܰௗሽ: a. Draw ݎௗ,~ܤ݁ݎ݊ݑ݈݈݅ሺ߰ௗሻ b. if (ݎௗ, ൌ ݁̂ // ݓௗ,is a C-expression term Draw ݖௗ,~ ܯݑ݈ݐሺߠௗா) else // ݎௗ, ൌݐ̂, ݓௗ,is a topical term Draw ݖௗ,~ ܯݑ݈ݐሺߠௗ்) c. Emit ݓௗ,~ܯݑ݈ݐሺ߮௭,ೕ ,ೕ) flT αT BT �T T w Nd Z BE αE r y u �E E V/ D flE flT αT BT �T T w Nd Z BE αE r �E E X V/ D flE λ 322 To learn the TME model from data, as exact inference is not possible, we resort to approximate inference using collapsed Gibbs sampling (GrifÞths and Steyvers, 2004). Gibbs sampling is a form of Markov Chain Monte Carlo method where a Markov chain is constructed to have a particular stationary distribution. In our case, we want to construct a Markov chain which converges to the posterior distribution over ܴ and ܼ conditioned on the data. We only need to sample ݖ and ݎ as we use collapsed Gibbs sampling and the dependencies of ߠ and ߮ have been integrated out analytically in the joint. Denoting the random variables ሼݓ, ݖ, ݎሽ by singular subscriptsሼݓ, ݖ, ݎሽ, ݇ଵ..., where ܭ ൌ ∑ௗ ܰௗ, a single iteration consists of performing the following sampling: ሺݖ ൌ</context>
<context position="18541" citStr="GrifÞths and Steyvers, 2004" startWordPosition="3042" endWordPosition="3045">the discovered C-expressions, contentious aspects, and aspects often mentioned in questions. 5.1 Dataset and Experiment SettingsWe crawled comments of reviews in Amazon.com for a variety of products. For each comment we extracted its id, the comment author id, the review id on which it commented, and the review author id. Our database consisted of 21,316 authors, 37,548 reviews, and 88,345 comments with an average of 124 words per comment post. For all our experiments, the hyper-parameters for TME and ME-TME were set to the heuristic values αT = 50/T, αE = 50/E, βT = βE = 0.1 as suggested in (GrifÞths and Steyvers, 2004). For ߛ, we estimated the asymmetric Beta priors using the method of moments discussed in Section 3. We sampled 1000 random posts and for each post we identified the C-expressions emitted. We thus computed the per-post probability of C-expression emission (1 െ ߰ௗሻ and used Eq. (3) to get the final estimates, ߛ = 3.66, ߛ= 1.21. To learn the MaxEnt parameters ߣ, we randomly sampled 500 terms from our corpus appearing at least 10 times and labeled them as topical (332) or C-expressions (168) and used the corresponding feature vector of each term (in the context of posts where it occurs) to trai</context>
</contexts>
<marker>GrifÞths, Steyvers, 2004</marker>
<rawString>GrifÞths, T. and M. Steyvers. 2004. Finding scientific topics. Proceedings of National Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>P Pantel</author>
<author>T Chklovski</author>
<author>M Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8388" citStr="Kim et al., 2006" startWordPosition="1306" endWordPosition="1309">here have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models </context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Kim, S., P. Pantel, T. Chklovski, and M. Pennacchiotti. 2006. Automatically assessing review helpfulness. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>Proceedings of the ACM International Conference on Web Search and Web Data Mining.</booktitle>
<contexts>
<context position="1648" citStr="Jindal and Liu 2008" startWordPosition="245" endWordPosition="248">d models. 1. Introduction Online reviews enable consumers to evaluate the products and services that they have used. These reviews are also used by other consumers and businesses as a valuable source of opinions. However, reviews only give the evaluations and experiences of the reviewers. Often a reviewer may not be an expert of the product and may misuse the product or make other mistakes. There may also be aspects of the product that the reviewer did not mention but a reader wants to know. Some reviewers may even write fake reviews to promote some products, which is called opinion spamming (Jindal and Liu 2008). To improve the online review system and user experience, some review hosting sites allow readers to write comments about reviews (apart from just providing a feedback by clicking whether the review is helpful or not). Many reviews receive a large number of comments. It is difficult for a reader to read them to get a gist of them. An automated comment analysis would be very helpful. Review comments mainly contain the following information: Thumbs-up or thumbs-down: Some readers may comment on whether they find the review useful in helping them make a buying decision. Agreement or disagreement</context>
<context position="8649" citStr="Jindal and Liu, 2008" startWordPosition="1347" endWordPosition="1350">entifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. For simplicity, we use </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Jindal, N. and B. Liu. 2008. Opinion spam and analysis. Proceedings of the ACM International Conference on Web Search and Web Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jo</author>
<author>A Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>Proceedings of the ACM International Conference on Web Search and Web Data Mining.</booktitle>
<contexts>
<context position="7147" citStr="Jo and Oh, 2011" startWordPosition="1109" endWordPosition="1112">9) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is </context>
<context position="15329" citStr="Jo and Oh, 2011" startWordPosition="2507" endWordPosition="2510"> and ߛ ൌ ߛ ൌ ߛ/2. However, knowing the fact that topics are more likely to be emitted than expressions in a post apriori motivates us to take guidance from asymmetric priors (i.e., we now have a non-uniform base measure). This asymmetric setting of ߛ ensures that samples of ߰ௗ are more close to the actual distribution of topical terms in posts based on some domain knowledge. Symmetric γ cannot utilize any prior knowledge. In (Lin and He, 2009), a method was proposed to incorporate domain knowledge during Gibbs sampling initialization, but its effect becomes weak as the sampling progresses (Jo and Oh, 2011). For asymmetric priors, we estimate the hyperparameters from labeled data. Given a labeled set ܦ, where we know the per post probability of Cexpression emission (1 െ ߰ௗሻ, we use the method of moments to estimate ߛ ൌ ሾߛ, ߛሿ as follows: ߛ ൌ ߤ ቀఓሺଵିఓሻ ఙ െ 1ቁ ,ߛ ൌ ߛ ቀଵ ఓ െ 1ቁ; ߤ ൌ ܧሾ߰ௗሿ,ߪ ൌ ܸܽݎሾ߰ௗሿ (3) 4. ME-TME Model The guidance of Beta priors, although helps, is still relatively coarse and weak. We can do better to produce clearer separation of topical and Cexpression terms. An alternative strategy is to employ Maximum-Entropy (Max-Ent) priors instead of Beta priors. The Max-Ent paramete</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Jo, Y. and A. Oh. 2011. Aspect and sentiment unification model for online review analysis. Proceedings of the ACM International Conference on Web Search and Web Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>M Huang</author>
<author>Y Yang</author>
<author>X Zhu</author>
</authors>
<title>Learning to Identify Review Spam.</title>
<date>2011</date>
<booktitle>in Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8683" citStr="Li et al. 2011" startWordPosition="1355" endWordPosition="1358">an and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. For simplicity, we use terms to denote both words (unigra</context>
</contexts>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Li, F., M. Huang, Y. Yang, and X. Zhu. 2011. Learning to Identify Review Spam. in Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lim</author>
<author>V Nguyen</author>
<author>N Jindal</author>
<author>B Liu</author>
<author>H Lauw</author>
</authors>
<title>Detecting Product Review Spammers using Rating Behaviors.</title>
<date>2010</date>
<booktitle>Proceedings of the ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="8667" citStr="Lim et al., 2010" startWordPosition="1351" endWordPosition="1354">ebates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. For simplicity, we use terms to denote bo</context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>Lim, E., V. Nguyen, N. Jindal, B. Liu, and H. Lauw. 2010. Detecting Product Review Spammers using Rating Behaviors. Proceedings of the ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>Y He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>Proceedings of the ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="7005" citStr="Lin and He, 2009" startWordPosition="1081" endWordPosition="1084">rain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also </context>
<context position="15163" citStr="Lin and He, 2009" startWordPosition="2482" endWordPosition="2485"> both topical and C-expression terms are equally likely to be emitted in a comment post). This results in symmetric Beta priors ߰ௗ~ܤ݁ݐܽሺߛ,ߛሻ where ߛ ൌ ߛݑ, ߛ ൌ ߛݑ and ߛ ൌ ߛ ൌ ߛ/2. However, knowing the fact that topics are more likely to be emitted than expressions in a post apriori motivates us to take guidance from asymmetric priors (i.e., we now have a non-uniform base measure). This asymmetric setting of ߛ ensures that samples of ߰ௗ are more close to the actual distribution of topical terms in posts based on some domain knowledge. Symmetric γ cannot utilize any prior knowledge. In (Lin and He, 2009), a method was proposed to incorporate domain knowledge during Gibbs sampling initialization, but its effect becomes weak as the sampling progresses (Jo and Oh, 2011). For asymmetric priors, we estimate the hyperparameters from labeled data. Given a labeled set ܦ, where we know the per post probability of Cexpression emission (1 െ ߰ௗሻ, we use the method of moments to estimate ߛ ൌ ሾߛ, ߛሿ as follows: ߛ ൌ ߤ ቀఓሺଵିఓሻ ఙ െ 1ቁ ,ߛ ൌ ߛ ቀଵ ఓ െ 1ቁ; ߤ ൌ ܧሾ߰ௗሿ,ߪ ൌ ܸܽݎሾ߰ௗሿ (3) 4. ME-TME Model The guidance of Beta priors, although helps, is still relatively coarse and weak. We can do better to produce c</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Lin, C. and Y. He. 2009. Joint sentiment/topic model for sentiment analysis. Proceedings of the ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>Y Cao</author>
<author>C Lin</author>
<author>Y Huang</author>
<author>M Zhou</author>
</authors>
<title>Low-quality product review detection in opinion summarization.</title>
<date>2007</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8462" citStr="Liu et al., 2007" startWordPosition="1318" endWordPosition="1321">oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables,</context>
</contexts>
<marker>Liu, Cao, Lin, Huang, Zhou, 2007</marker>
<rawString>Liu, J., Y. Cao, C. Lin, Y. Huang, and M. Zhou. 2007. Low-quality product review detection in opinion summarization. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool</publisher>
<note>publishers (to appear in</note>
<contexts>
<context position="5565" citStr="Liu 2012" startWordPosition="859" endWordPosition="860">rate these capabilities of our models. With these pieces of information, comments for a review can be summarized. The summary may include, but not limited to, the following: (1) percent of people who give the review thumbs-up or thumbs-down; (2) percent of people who agree or disagree (or contend) with the reviewer; (3) contentious (disagreed) aspects (or topics); (4) aspects about which people often have questions. To the best of our knowledge, there is no reported work on such a fine-grained modeling of review comments. The related works are mainly in sentiment analysis (Pang and Lee, 2008; Liu 2012), e.g., topic and sentiment modeling, review quality prediction and review spam detection. However, our work is different from them. We will compare with them in detail in Section 2. The proposed models have been evaluated both qualitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several genera</context>
<context position="7220" citStr="Liu, 2012" startWordPosition="1123" endWordPosition="1124">essions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting autho</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Liu, B. 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool publishers (to appear in June 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>X Huang</author>
<author>A An</author>
<author>X Yu</author>
</authors>
<title>Modeling and predicting the helpfulness of online reviews.</title>
<date>2008</date>
<booktitle>Proceedings of IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="8480" citStr="Liu et al., 2008" startWordPosition="1322" endWordPosition="1325">, (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is</context>
</contexts>
<marker>Liu, Huang, An, Yu, 2008</marker>
<rawString>Liu, Y., X. Huang, A. An, and X. Yu. 2008. Modeling and predicting the helpfulness of online reviews. Proceedings of IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lu</author>
<author>C Zhai</author>
</authors>
<title>Opinion integration through semi-supervised topic modeling.</title>
<date>2008</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="7042" citStr="Lu and Zhai, 2008" startWordPosition="1089" endWordPosition="1092">beled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions.</context>
</contexts>
<marker>Lu, Zhai, 2008</marker>
<rawString>Lu, Y. and C. Zhai. 2008. Opinion integration through semi-supervised topic modeling. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lu</author>
<author>C Zhai</author>
<author>N Sundaresan</author>
</authors>
<title>Rated aspect summarization of short comments.</title>
<date>2009</date>
<booktitle>Proceedings of International Conference on World Wide.</booktitle>
<contexts>
<context position="7086" citStr="Lu et al., 2009" startWordPosition="1097" endWordPosition="1100"> (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phra</context>
</contexts>
<marker>Lu, Zhai, Sundaresan, 2009</marker>
<rawString>Lu, Y., C. Zhai, and N. Sundaresan. 2009. Rated aspect summarization of short comments. Proceedings of International Conference on World Wide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q X Ling Mei</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>Proceedings of International Conference on World Wide.</booktitle>
<contexts>
<context position="7023" citStr="Mei et al., 2007" startWordPosition="1085" endWordPosition="1088">Donald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple type</context>
</contexts>
<marker>Mei, Wondra, Su, Zhai, 2007</marker>
<rawString>Mei, Q. X. Ling, M. Wondra, H. Su and C. Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. Proceedings of International Conference on World Wide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moghaddam</author>
<author>M Ester</author>
</authors>
<title>ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews.</title>
<date>2011</date>
<booktitle>Proceedings of Annual ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<marker>Moghaddam, Ester, 2011</marker>
<rawString>Moghaddam, S. and M. Ester. 2011. ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews. Proceedings of Annual ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Aspect Extraction through Semi-Supervised Modeling.</title>
<date>2012</date>
<booktitle>Proceedings of 50th Annual Meeting of Association for Computational Linguistics</booktitle>
<note>to appear in</note>
<contexts>
<context position="7220" citStr="Mukherjee and Liu, 2012" startWordPosition="1121" endWordPosition="1124"> types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting autho</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and B. Liu. 2012a. Aspect Extraction through Semi-Supervised Modeling. Proceedings of 50th Annual Meeting of Association for Computational Linguistics (to appear in July 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Mining Contentions from Discussions and Debates.</title>
<date>2012</date>
<booktitle>Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</booktitle>
<note>to appear in</note>
<contexts>
<context position="7220" citStr="Mukherjee and Liu, 2012" startWordPosition="1121" endWordPosition="1124"> types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting autho</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and B. Liu. 2012b. Mining Contentions from Discussions and Debates. Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (to appear in August 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
<author>N Glance</author>
</authors>
<title>Spotting Fake Reviewer Groups in Consumer Reviews.</title>
<date>2012</date>
<booktitle>Proceedings of International World Wide Web Conference.</booktitle>
<contexts>
<context position="8726" citStr="Mukherjee et al., 2012" startWordPosition="1363" endWordPosition="1366"> 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. For simplicity, we use terms to denote both words (unigrams or 1-grams) and phrases (n-grams). We de</context>
</contexts>
<marker>Mukherjee, Liu, Glance, 2012</marker>
<rawString>Mukherjee, A., B. Liu and N. Glance. 2012. Spotting Fake Reviewer Groups in Consumer Reviews. Proceedings of International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Murakami</author>
<author>R Raymond</author>
</authors>
<title>Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions.</title>
<date>2010</date>
<booktitle>Proceedings of International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="7937" citStr="Murakami and Raymond, 2010" startWordPosition="1233" endWordPosition="1236">tch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), var</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>Murakami A., and R. Raymond, 2010. Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions. Proceedings of International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P O&apos;Mahony</author>
<author>B Smyth</author>
</authors>
<title>Learning to recommend helpful hotel reviews.</title>
<date>2009</date>
<booktitle>Proceedings of the third ACM conference on Recommender systems.</booktitle>
<marker>O&apos;Mahony, Smyth, 2009</marker>
<rawString>O&apos;Mahony, M. P. and B. Smyth. 2009. Learning to recommend helpful hotel reviews. Proceedings of the third ACM conference on Recommender systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ott</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>J T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8701" citStr="Ott et al., 2011" startWordPosition="1359" endWordPosition="1362">09; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. For simplicity, we use terms to denote both words (unigrams or 1-grams) and</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Ott, M., Y. Choi, C. Cardie, and J. T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</title>
<date>2008</date>
<contexts>
<context position="5554" citStr="Pang and Lee, 2008" startWordPosition="855" endWordPosition="858">ction 5 will demonstrate these capabilities of our models. With these pieces of information, comments for a review can be summarized. The summary may include, but not limited to, the following: (1) percent of people who give the review thumbs-up or thumbs-down; (2) percent of people who agree or disagree (or contend) with the reviewer; (3) contentious (disagreed) aspects (or topics); (4) aspects about which people often have questions. To the best of our knowledge, there is no reported work on such a fine-grained modeling of review comments. The related works are mainly in sentiment analysis (Pang and Lee, 2008; Liu 2012), e.g., topic and sentiment modeling, review quality prediction and review spam detection. However, our work is different from them. We will compare with them in detail in Section 2. The proposed models have been evaluated both qualitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, sev</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, B. and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6452" citStr="Ramage et al., 2009" startWordPosition="996" endWordPosition="999">a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov an</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Ramage, D., D. Hall, R. Nallapati, and C. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>C Manning</author>
<author>S Dumais</author>
</authors>
<title>Partially labeled topic models for interpretable text mining.</title>
<date>2011</date>
<booktitle>Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="6493" citStr="Ramage et al., 2011" startWordPosition="1001" endWordPosition="1004">azon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody</context>
</contexts>
<marker>Ramage, Manning, Dumais, 2011</marker>
<rawString>Ramage, D., C. Manning, and S. Dumais. 2011 Partially labeled topic models for interpretable text mining. Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smith</author>
</authors>
<title>The author-topic model for authors and documents. Uncertainty in Artificial Intelligence.</title>
<date>2004</date>
<contexts>
<context position="13921" citStr="Rosen-Zvi et al., 2004" startWordPosition="2271" endWordPosition="2274">, ݆ሻ. Counts݊௧,௩ ் and ݊,௩ ா denote the number of times term ݒ was assigned to topic ݐ and expression type ݁ respectively. ݊ௗ,௧ ் and ݊ௗ, ா denote the number of terms in document ݀ that were assigned to topic ݐ and C-expression type ݁ respectively. Lastly, ݊ௗ் and ݊ௗா are the number of terms in ݀ that were assigned to topics and Cexpression types respectively. Omission of the latter index denoted by ሺሻ represents the marginalized sum over the latter index. We employ a blocked sampler jointly sampling ݎ and ݖ as this improves convergence and reduces autocorrelation of the Gibbs sampler (Rosen-Zvi et al., 2004). Asymmetric Beta priors: Based on our initial experiments with TME, we found that properly setting the smoothing hyper-parameter ߛ is crucial as it governs the topic/expression switch. According to the generative process, ߰ௗ is the (success) probability (of the Bernoulli distribution) of emitting a topical/aspect term in a comment post ݀ and1 െ ߰ௗ, the probability of emitting a Cexpression term in ݀. Without loss of generality, we draw ߰ௗ~ܤ݁ݐܽሺߛሻ where ߛ is the concentration parameter and  ൌ ሾݑܽ, ݑܾሿ is the base measure. Without any prior belief, one resorts to uniform base measure ݑ ൌ ݑ</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smith, 2004</marker>
<rawString>Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smith. 2004. The author-topic model for authors and documents. Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Haghighi Sauper</author>
<author>R Barzilay</author>
</authors>
<title>Content models with attitude.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Sauper, Barzilay, 2011</marker>
<rawString>Sauper, C. A. Haghighi and R. Barzilay. 2011. Content models with attitude. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</booktitle>
<contexts>
<context position="8087" citStr="Somasundaran and Wiebe, 2009" startWordPosition="1252" endWordPosition="1255">n reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ot</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Somasundaran, S., J. Wiebe. 2009. Recognizing stances in online debates. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association.</journal>
<contexts>
<context position="19563" citStr="Teh et al., 2006" startWordPosition="3214" endWordPosition="3217">orpus appearing at least 10 times and labeled them as topical (332) or C-expressions (168) and used the corresponding feature vector of each term (in the context of posts where it occurs) to train the Max-Ent model. We set the number of topics, T = 100 and the number of C-expression types, E = 6 (Thumbs-up, Thumbs-down, Question, Answer acknowledgement, Agreement and Disagreement) as in review comments, we usually find these six dominant expression types. Note that knowing the exact number of topics, T and expression types, E in a corpus is difficult. While non-parametric Bayesian approaches (Teh et al., 2006) aim to estimate T from the corpus, in this work the heuristic values obtained from our initial experiments produced good results. We also tried increasing E to 7, 8, etc. However, it did not produce any new dominant expression type. Instead, the expression types became less specific as the expression term space became sparser. 5.2 C-Expression Evaluation We now evaluate the discovered C-expressions. We first evaluate them qualitatively in Tables 1 and 2. Table 1 shows the top terms of all expression types using the TME model. We find that TME can discover and cluster many correct Cexpressions</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Teh, Y., M. Jordan, M. Beal and D. Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional ßoor-debate transcripts.</title>
<date>2006</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Thomas, M., B. Pang and L. Lee. 2006. Get out the vote: Determining support or opposition from Congressional ßoor-debate transcripts. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="6419" citStr="Titov and McDonald, 2008" startWordPosition="991" endWordPosition="994">alitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Titov, I. and R. McDonald. 2008a. Modeling online reviews with multi-grain topic models. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>Proceedings of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6419" citStr="Titov and McDonald, 2008" startWordPosition="991" endWordPosition="994">alitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Titov, I. and R. McDonald. 2008b. A joint model of text and aspect ratings for sentiment summarization. Proceedings of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews.</title>
<date>2009</date>
<booktitle>Proceedings of the International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="8532" citStr="Tsur and Rappoport 2009" startWordPosition="1330" endWordPosition="1333">; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phrases (n-grams) are viewed as random variables, and a document is viewed as a bag of n-grams and each n-gram takes a </context>
</contexts>
<marker>Tsur, Rappoport, 2009</marker>
<rawString>Tsur, O. and A. Rappoport. 2009. Revrank: A fully unsupervised algorithm for selecting the most helpful book reviews. Proceedings of the International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>Y Lu</author>
<author>C Zhai</author>
</authors>
<title>Latent aspect rating analysis on review text data: a rating regression approach.</title>
<date>2010</date>
<booktitle>Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="7130" citStr="Wang et al., 2010" startWordPosition="1105" endWordPosition="1108">ejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a di</context>
</contexts>
<marker>Wang, Lu, Zhai, 2010</marker>
<rawString>Wang, H., Y. Lu, and C. Zhai. 2010. Latent aspect rating analysis on review text data: a rating regression approach. Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yano</author>
<author>N Smith</author>
</authors>
<title>What’s Worthy of Comment? Content and Comment Volume in Political Blogs.</title>
<date>2010</date>
<booktitle>Proceedings of the International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="8159" citStr="Yano and Smith, 2010" startWordPosition="1265" endWordPosition="1268">ve with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewer</context>
</contexts>
<marker>Yano, Smith, 2010</marker>
<rawString>Yano, T and N. Smith. 2010. What’s Worthy of Comment? Content and Comment Volume in Political Blogs. Proceedings of the International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>B Varadarajan</author>
</authors>
<title>Utility scoring of product reviews.</title>
<date>2006</date>
<booktitle>Proceedings of ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="8417" citStr="Zhang and Varadarajan, 2006" startWordPosition="1310" endWordPosition="1313">n works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 321 2006; Burfoot et al., 2011). (Yano and Smith, 2010) also modeled the relationship of a blog post and the number of comments it receives. These works are different as they do not mine Cexpressions or discover the points of contention and questions in comments. In (Kim et al., 2006; Zhang and Varadarajan, 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur and Rappoport 2009), various classification and regression approaches were taken to assess the quality of reviews. (Jindal and Liu, 2008; Lim et al., 2010; Li et al. 2011; Ott et al., 2011; Mukherjee et al., 2012) detect fake reviews and reviewers. However, all these works are not concerned with review comments. 3. The Basic TME Model This section discusses TME. The next section discusses ME-TME, which improves TME. These models belong to the family of generative models for text where words and phra</context>
</contexts>
<marker>Zhang, Varadarajan, 2006</marker>
<rawString>Zhang, Z. and B. Varadarajan. 2006. Utility scoring of product reviews. Proceedings of ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhao</author>
<author>J Jiang</author>
<author>H Yan</author>
<author>X Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid.</title>
<date>2010</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7292" citStr="Zhao et al., 2010" startWordPosition="1135" endWordPosition="1138"> document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agar</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Zhao, X., J. Jiang, H. Yan, and X. Li. 2010. Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>