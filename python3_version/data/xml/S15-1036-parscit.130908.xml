<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.994373">
Graph-based Coherence Modeling For Assessing Readability
</title>
<author confidence="0.96176">
Mohsen Mesgar and Michael Strube
</author>
<affiliation confidence="0.948869">
Heidelberg Institute for Theoretical Studies gGmbH
</affiliation>
<address confidence="0.9219945">
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.909918">
(mohsen.mesgar|michael.strube)@h-its.org
</email>
<sectionHeader confidence="0.99421" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708538461538">
Readability depends on many factors rang-
ing from shallow features like word length
to semantic ones like coherence. We intro-
duce novel graph-based coherence features
based on frequent subgraphs and compare
their ability to assess the readability of Wall
Street Journal articles. In contrast to Pitler
and Nenkova (2008) some of our graph-based
features are significantly correlated with hu-
man judgments. We outperform Pitler and
Nenkova (2008) in the readability ranking task
by more than 5% accuracy thus establishing a
new state-of-the-art on this dataset.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950285714286">
Readability depends on many factors which enable
readers to process a text. These factors can be used
by readability assessment methods to quantify the
difficulty of text understanding. Possible applica-
tions of readability assessment are automatic text
summarization and simplification systems. Measur-
ing readability can also be used in question answer-
ing and knowledge extraction systems to prune texts
with low readability (Kate et al., 2010).
Many different text features have been used to
assess readability. They include shallow features
(Flesch, 1948; Kincaid et al., 1975), language
modeling features (Si and Callan, 2001; Collins-
Thompson and Callan, 2004), syntactic features
(Schwarm and Ostendorf, 2005) and text flow or
coherence (Barzilay and Lapata, 2008; Pitler and
Nenkova, 2008). In a coherent text each sentence
has some connections with other sentences. Al-
though these local connections make the text more
readable, the corresponding coherence features used
in Pitler and Nenkova (2008) (Section 2) are not
strongly correlated with human judgments.
The main goal of this paper is to introduce novel
graph-based coherence features for assessing read-
ability. To achieve this goal, we use the entity graph
coherence model by Guinaudeau and Strube (2013)
(Section 3.1.1) and follow two ideas. The first main
idea is to use a graph representation of rhetorical re-
lations between sentences of a text (Section 3.1.2)
and to merge the entity graph and the rhetorical
graph (Section 3.1.3). Hence we enrich the entity
graph and consequently consider the distribution of
two aspects of coherence (i.e. entities and discourse
relations) simultaneously. The second main idea is
to apply subgraph mining algorithms to find frequent
subgraphs (i.e. patterns) in texts (Section 3.2). Sub-
graph mining has been successfully applied to other
tasks, e.g. image processing (Nowozin et al., 2007)
and language modeling (Biemann et al., 2012). We
hypothesize that text coherence correlates with fre-
quent subgraphs (vaguely reminding us of coherence
patterns (Daneˇs, 1974)) and that the mined patterns
are good predictors for readability ratings.
Our study is novel in introducing new and infor-
mative graph-based coherence features. We examine
the predictive power of these feature in two experi-
ments: first, readability rating prediction, and sec-
ond, ranking texts according to the readability (Sec-
tion 5).
</bodyText>
<page confidence="0.987171">
309
</page>
<note confidence="0.96174">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 309–318,
Denver, Colorado, June 4–5, 2015.
</note>
<figure confidence="0.9950475">
s1 s2 s3 s4 s5
... ... ... ...
OFFICIALS
GEAR
HOURS
NEWS
PRESS
SENSE
HUGO
TABLE
MAKERS
MAYOR
POLICY
ASSOCIATED
FUNCTIONS
WASHINGTON
CHARLESTON
BUREAUCRACY
</figure>
<figureCaption confidence="0.999973">
Figure 1: The entity graph representation of the text in Table 1. Dark entities are shared by the sentences.
</figureCaption>
<sectionHeader confidence="0.977416" genericHeader="method">
2 Readability Assessment
</sectionHeader>
<bodyText confidence="0.9999828">
The quality of a text depends on different factors
which make the text easier to read. These factors
range from shallow features like word length to se-
mantic features like coherence. Readability assess-
ment leads to two problems: distinguishing and rec-
ognizing readability levels of texts and predicting
human readability ratings.
Pitler and Nenkova (2008) use all entity transi-
tions of the entity grid model (Barzilay and Lapata,
2008) as coherence features. They compute the cor-
relation between them and readability ratings and
show that none of them is significantly correlated
with human readability judgments. Indeed, none of
these features on its own is a good predictor to mea-
sure coherence and to predict readability as well.
</bodyText>
<sectionHeader confidence="0.996377" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9999825">
We introduce the graph representation of a text and
propose to use these graphs to model coherence.
</bodyText>
<subsectionHeader confidence="0.9457215">
3.1 Graphs
3.1.1 Entity Graph
</subsectionHeader>
<bodyText confidence="0.996494967741935">
Guinaudeau and Strube (2013) describe a graph-
based version of the entity grid (Barzilay and Lap-
ata, 2008) which models the interaction between en-
tities and sentences as a bipartite graph. This graph
contains two sets of nodes: sentences and entities.
Sentence and entity nodes are connected if and only
if the entity is mentioned in the sentence (Figure 1).
Edges are weighted according to the grammatical
role of the entity mentioned in the sentence.
Guinaudeau and Strube (2013) model entity tran-
sitions between sentences via a one-mode projec-
tion of the entity graph. The one-mode projec-
tion is a graph consisting of sentence nodes that
are connected if and only if they have at least one
entity in common in the entity graph. One-mode
projections are directed as they follow the text or-
der. Hence, backward edges never occur. Guin-
audeau and Strube (2013) introduce three kinds of
projections. The unweighted projection PER umodels
the existence of the entity connections between sen-
tences. The weighted projection PER
w uses the num-
ber of shared entities by sentences as a weight for the
corresponding edge (Figure 2). PER
acc takes the gram-
matical function of entities in sentences into account
as edge weights. Guinaudeau and Strube (2013)
show that PER
acc does not perform well for readabil-
ity assessment. It does not outperform PER
w in our
</bodyText>
<listItem confidence="0.921142375">
S1: The [Associated] [Press]’s [earthquake] [coverage]
drew [attention] to a [phenomenon] that deserves some
[thought] by public [officials] and other [policy] [mak-
ers].
S2: Private [relief] [agencies], such as the [Salvation]
[Army] and [Red] [Cross], mobilized almost instantly
to help [people], while the [Washington] [bureaucracy]
”took [hours] getting into [gear].”
S3: One [news] show we saw [yesterday] even displayed
25 federal [officials] meeting around a [table].
S4: We recall that the [mayor] of [Charleston] com-
plained bitterly about the federal [bureaucracy]’s re-
sponse to [Hurricane Hugo].
S5: The [sense] grows that modern public [bureaucra-
cies] simply don’t perform their assigned [functions]
well.
</listItem>
<tableCaption confidence="0.8369725">
Table 1: A sample text from the Wall Street Journal
dataset (Pitler and Nenkova, 2008).
</tableCaption>
<page confidence="0.988541">
310
</page>
<figure confidence="0.9941405">
PER PER
u w
</figure>
<figureCaption confidence="0.9390055">
Figure 2: PER
u : unweighted, and PER
</figureCaption>
<bodyText confidence="0.9781056">
w : weighted projec-
tion graphs. In the weighted projection all edge weights
are equal to one, because all sentences share one entity.
experiments as well. Thus, we do explain further
details of PER where.
</bodyText>
<subsectionHeader confidence="0.782724">
3.1.2 Discourse Relation Graph
</subsectionHeader>
<bodyText confidence="0.999664866666667">
Lin et al. (2011) and Lin (2011) use Rhetorical
Structure Theory (RST) to describe and model co-
herence by considering the transitions between dis-
course relations. Inspired by the entity grid they ex-
pand the relation sequence into a two-dimensional
matrix whose rows and columns are sentences and
entities, respectively. The cell (si,ej) corresponds
to the set of discourse relations entity ej is involved
with in sentence si. These methods are based on en-
tity transitions which, however, are intuitively im-
plausible, because discourse relations connect sen-
tences (or elementary discourse units).
Since discourse relations capture interactions be-
tween sentences (Table 2), we model these relations
with a graph.
</bodyText>
<tableCaption confidence="0.902892">
Table 2: PDTB-style discourse relations (Prasad et al.,
2008) of the sample text in Table 1
</tableCaption>
<bodyText confidence="0.810134">
A discourse relation graph is PDR
</bodyText>
<equation confidence="0.403042">
u = (V,R), where
</equation>
<bodyText confidence="0.99183325">
V is the set of sentence nodes and R is the edge
set which represents all discourse relations in the
text. Two sentence nodes are adjacent if and only
if they are connected by at least one discourse rela-
tion. Intra-sentential discourse relations are repre-
sented as self-edges. We define PDR
w as a weighted
discourse relation graph whose edge weights are
</bodyText>
<figure confidence="0.872503">
PDR PDR
u w
</figure>
<figureCaption confidence="0.903721">
Figure 3: PDR
</figureCaption>
<equation confidence="0.466653">
u : unweighted, and PDR
w : weighted discourse
relation graphs.
</equation>
<bodyText confidence="0.8674875">
the number of discourse relations between sentence
nodes (Figure 3).
</bodyText>
<subsectionHeader confidence="0.9411865">
3.1.3 Combined Entity and Discourse Relation
Graphs
</subsectionHeader>
<bodyText confidence="0.9999236">
Both projection and discourse relation graphs rep-
resent different types of connections. These graphs
can be merged by employing basic operators.
We use the V operator (logical OR) to combine
the projection graph PER uwith the PDR
</bodyText>
<listItem confidence="0.83589325">
u graph. The
V operator takes two sentence nodes and creates an
edge between them if they are connected at least
by one connection, whether entity transition (PER
u )
or discourse relations (PDR
u ). The other basic logi-
cal operators (e.g. ∧ or ®) lose connections. Hence
we do not report on their performance. Inspired by
linear regression models we combine the weighted
graphs by adding (+) the edge weights in PER
w and
</listItem>
<figure confidence="0.854214">
PDR
w (Figure 4).
PER uV PDR PER w+ PDR
u w
</figure>
<figureCaption confidence="0.999569">
Figure 4: Combined entity and discourse relation graphs.
</figureCaption>
<subsectionHeader confidence="0.999416">
3.2 Coherence Features
</subsectionHeader>
<bodyText confidence="0.9988692">
We use the proposed graphs to introduce novel co-
herence features.
Average outdegree. Measures to which extent a
sentence is connected with other sentences (Guin-
audeau and Strube, 2013):
</bodyText>
<figure confidence="0.94731845">
AvgOutDegree(P) = IISII
s1
s2
s5
s3
s4
s1
s2
1 1
s5
1
1
s3
s4
Relation
Arg1 Arg2
Implicit Expansion
Explicit Comparison
Implicit Expansion
Implicit Temporal
Implicit Contingency
S1 S2
S2 S2
S2 S3
S3 S4
S4 S5
s1
s2
s5
s3
s4
1
s1
s2
1
s5
1
1
s3
s4
1
s1
s2
s5
s3
s4
1
s1
s2
1
1
s5
1
1
1
2
s3
s4
1
∑sES OutDegree(s)
</figure>
<page confidence="0.994409">
311
</page>
<bodyText confidence="0.9999035">
where OutDegree(s) is the sum of the weights as-
sociated with edges that leave node s and ISI is the
number of sentences in the text.
Number of components. The projection graph
can be disconnected. A graph is disconnected if
there are at least two nodes which are not reachable
from each other (like s1 and s2 in Figure 2). A max-
imal non-empty connected subgraph in a graph is
called component. Each projection graph in Figure
2 contains two components. Intuitively, projection
graphs of a more coherent text should contain fewer
number of components. The outdegree does not cap-
ture this type of connectivity. E.g., in Figure 5 the
average outdegree of the two graphs is equal, while
the left graph contains more components and should
be less coherent.
</bodyText>
<figureCaption confidence="0.9825995">
Figure 5: Two graphs with the same outdegree value.
Graph (a) has two components. It is less coherent.
</figureCaption>
<bodyText confidence="0.999609571428572">
Frequent subgraphs. We hypothesize that par-
ticular coherence patterns show a correlation with
readability. These patterns are encoded as subgraphs
in graphs. An advantage is that coherence can be
measured beyond simple sentence or node connec-
tivity. We first define the graph concepts employed.
Isomorphic. Two graphs G and G&apos; are isomorphic,
if they fulfill two conditions: there should be a one-
to-one association between nodes of G&apos; and those of
G, and two nodes of G&apos; should be connected, if and
only if their associated nodes in G are connected.
Subgraph. Graph G&apos; is a subgraph of graph G, if
G&apos; is isomorphic to a graph whose nodes and edges
are in G.
</bodyText>
<listItem confidence="0.5314175">
k-node subgraph. A subgraph with k nodes is
called k-node subgraph.
</listItem>
<bodyText confidence="0.920986">
Induced subgraph. The graph G&apos; is an induced
subgraph of graph G, if G&apos; is a subgraph of G whose
nodes are connected by all edges which connect the
corresponding nodes in G (Figure 6). We always
mean induced subgraphs when using the term sub-
graph.
Frequent subgraph &amp; minimum support. Let ζ =
{G1,G2,··· ,Gn} be a database of n graphs. For
</bodyText>
<figureCaption confidence="0.9802565">
Figure 6: Both graphs (b) and (c) are subgraphs of (a).
Only (c) is an induced subgraph of (a).
</figureCaption>
<bodyText confidence="0.981499111111111">
each subgraph sg, support(sg) denotes the number
of graphs (in ζ) which contain sg as a subgraph. A
subgraph sg is a frequent subgraph if and only if
support(sg) &gt; λ, where λ is called minimum sup-
port.
Graph signature. Given a set of fre-
quent subgraphs {sg1,sg2,...,sgm}, a graph
signature for G E ζ is the vector Φ(G) =
(ϕ(sg1,G),ϕ(sg2,G),...,ϕ(sgm,G)), where
</bodyText>
<equation confidence="0.9970975">
count(sgi,G)
ϕ(sgi,G) = ∑sgjE(sg1,sg2,...,sgm)count(sgj,G)
</equation>
<bodyText confidence="0.9976934">
Here count(sgi,G) is the number of occurrences
of sgi in graph G. We use the relative frequency
ϕ(sgi,G) because it compares graphs with different
numbers of nodes and different numbers of edges.
Subgraph features are divided into two categories:
basic subgraphs and frequent large subgraphs.
Basic subgraphs. Instead of frequent subgraphs
all possible 3-node subgraphs (Figure 7) are used as
basic subgraphs because they are the smallest mean-
ingful subgraphs that can model coherence patterns.
</bodyText>
<figureCaption confidence="0.998776">
Figure 7: All possible directed 3-node subgraphs.
</figureCaption>
<bodyText confidence="0.9930135">
Because backward edges never occur in one-
mode projections, only four subgraphs are feasible
(Figure 8).
We interpret these subgraphs as follows:
</bodyText>
<listItem confidence="0.99473025">
• sg1: The connection between a sentence and
subsequent ones. In other words, at least two
entities are mentioned in one sentence and the
subsequent ones are about these entities.
</listItem>
<figure confidence="0.935226571428571">
(a) (b)
s1 s2 s3 s4
s6 s5
s
1 s2 s3 s4
s6 s5
(a) (b) (c)
</figure>
<page confidence="0.778002">
312
</page>
<figureCaption confidence="0.88804425">
Figure 8: Feasible 3-node subgraph coherence features.
Node labels illustrate the order of sentences. Sentence st
occurs before sentence su, and sentence su occurs before
sentence sv (i.e. t &lt; u &lt; v).
</figureCaption>
<listItem confidence="0.998802923076923">
• sg2: Indicates that entities in st and su get con-
nected to each other in sv.
• sg3: Each sentence tends to refer to the most
prominent entity (focus of attention) in pre-
ceding sentences (Sidner, 1983; Grosz et al.,
1995). The absence of a connection between
st and sv indicates that the entity connecting st
and su is different from the entity connecting su
and sv. Therefore this subgraph approximately
corresponds to the shift of the focus of atten-
tion.
• sg4: Merges sg1 and sg3 and represents all con-
nections of these two subgraphs.
</listItem>
<bodyText confidence="0.999990041666666">
We use these feasible 3-node subgraphs and com-
pute the graph signature, Φ, of each G E �. We pro-
pose each (p E Φ (i.e. relative frequency of each sub-
graph in G) as a connectivity feature of graph G to
measure text coherence.
Frequent large subgraphs. Since we observe a
strong correlation between basic subgraphs and hu-
man readability ratings (Table 4), we mine frequent
large subgraphs of projection graphs. Our intuition
is that larger subgraphs are more informative coher-
ence patterns. Hence, we extend the coherence fea-
tures from all feasible 3-node subgraphs to frequent
k-node subgraphs. We first use an efficient subgraph
mining algorithm to extract all subgraphs with size k
and then compute the count of each subgraph as an
induced subgraph in each graph G E �. We retain a
subgraph sg, if it is frequent (i.e. support(sg) &gt; X).
The result of these steps is a two-dimensional ma-
trix whose rows represent graphs in � and columns
represent frequent subgraphs with size k. The cell
(Gi,sgj) shows the count of sgj in graph Gi. Given
this matrix, we compute the graph signature of each
G E � and take each element of the graph signature
as a coherence feature.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.952577">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999382692307692">
We use the dataset created by Pitler and Nenkova
(2008) which consists of randomly selected articles
from the Wall Street Journal corpus. The articles
were rated by three humans on a scale from 1 to 5
for readability based on quality measures that are de-
signed to estimate the coherence of articles. The fi-
nal readability score of each article is the average of
these three ratings.
We exclude three files from this dataset: wsj-
-0382 does not exist in the Penn Treebank (Mar-
cus et al., 1994)1. wsj-2090 does not exist in
the Penn Discource Treebank (Prasad et al., 2008).
wsj-1398 is a poem.
</bodyText>
<subsectionHeader confidence="0.947865">
4.2 Settings
</subsectionHeader>
<bodyText confidence="0.99762072">
Entity graph. We use the gold parse trees in the
Penn Treebank (Marcus et al., 1994) to extract all
nouns in a document as mentions. We consider
nouns with identical stem2 as coreferent. We divide
the edge weight between two sentence nodes si and
sj by their distance j − i to decrease the importance
of links that exist between non-adjacent sentences.
Discourse relation graph. We use gold PDTB-style
discourse relations (Prasad et al., 2008). We filter
out EntRel and NoRel relations.
Number of components. For counting the number
of components in each projection graph, the Sage-
Math3 package is used. This feature is computed on
unweighted projections (i.e. PER
u ).
Frequent subgraphs. Since subgraph mining is
an NP-complete problem, different algorithms have
been introduced to improve the performance of sub-
graph mining. We use the gSpan4 algorithm (Yan
and Han, 2002) to mine subgraphs of a graph
database which contains PER uprojections. An advan-
tage of using efficient subgraph mining algorithms is
that we can exhaustively search very large subgraph
spaces. A graph with IEI edges, however, poten-
tially has O(2IEI) subgraphs. Having sparse graphs
</bodyText>
<footnote confidence="0.974455428571429">
1Pitler and Nenkova (2008) also remove one file from their
experiments. We assume that it is wsj-0382.
2We use Stanford CoreNLP (http://nlp.stanford.
edu/software/corenlp.shtml)
3http://sagemath.org/download-linux.html
4We use the Java package: http://www.cs.ucsb.
edu/˜xyan/software/gSpan.htm
</footnote>
<figure confidence="0.957324666666667">
sg1 sg2 sg3 sg4
st su
sv
st su
sv
st su
sv
st su
sv
</figure>
<page confidence="0.923627">
313
</page>
<figureCaption confidence="0.8478895">
Figure 9: Frequent subgraphs with four nodes where t &lt;
u &lt; v &lt; w.
</figureCaption>
<bodyText confidence="0.999719333333333">
and using efficient subgraph mining algorithm lets
us to search trough this space. We mine subgraphs
with k = 4 and X = 0 (Figure 9).
</bodyText>
<subsectionHeader confidence="0.992707">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999784909090909">
We evaluate on the following benchmark tasks.
Readability assessment. We use the Pearson cor-
relation coefficient to find features correlated with
readability scores. It takes feature values and read-
ability scores of all articles and returns −1 &lt; p &lt;
+1. A high value of |p |shows a strong correlation.
We report statistical significance on the 0.05-level5.
Readability as ranking. We rank texts pairwise
with respect to their readability. We define a clas-
sification problem with a set of text pairs and a la-
bel, which indicates whether the first text in a pair
</bodyText>
<footnote confidence="0.96716">
5The results written in bold face (Section 5).
</footnote>
<table confidence="0.945586866666667">
p p value
Entity Graph 0.949
PER −0.013 0.452
u 0.455
PER 0.151
w
PER 0.150
acc
Discourse Relation Graph 0.455
PDR 0.150 0.440
u
PDR 0.155
w
Combination of Entity and Discourse Relation
PER uV PDR 0.083 0.681
</table>
<figure confidence="0.7139656">
u
PER w+PDR 0.185 0.356
u
PER w+ PDR 0.187 0.350
w
</figure>
<tableCaption confidence="0.961531">
Table 3: The correlation of the average outdegree of dif-
ferent graphs with human readability ratings.
</tableCaption>
<bodyText confidence="0.997438833333333">
is more readable. We use every two texts whose hu-
man readability scores differ by at least 0.5. Each
text is represented with its graph-based coherence
features. We employ WEKA’s linear support vector
implementation (SMO) to classify the pairs. Perfor-
mance is evaluated using 10-fold cross-validation.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999793523809524">
Readability assessment. We report the correlation
of our coherence models encoded in graph features
and compare them with Guinaudeau and Strube’s
(2013) entity graph as the state-of-the-art coherence
model. Pitler and Nenkova (2008) show that the en-
tity transition features extracted from the entity grid
model (Barzilay and Lapata, 2008) on its own do
not significantly predict human readability ratings.
So we do not describe their results here.
The results for the outdegree feature is shown
in Table 3. The average outdegree of PER
w is
highly correlated with human readability ratings.
This confirms the readability results of Guinaudeau
and Strube (2013) on the Encyclopedia Britannica
dataset. The outdegrees of discourse relation graphs
are more strongly correlated with human readability
ratings than the outdegree of the projections in the
entity graph, suggesting that efficient graph-based
encoding of discourse relations can measure read-
ability well. The outdegree of the combined graph
</bodyText>
<equation confidence="0.979462">
PER
w + PDR
</equation>
<bodyText confidence="0.9971466">
w is highly correlated, showing that the
interaction of entity connections and discourse re-
lations is important for text coherence. However,
none of the outdegree measures in this table are
significantly correlated with human readability rat-
</bodyText>
<figure confidence="0.999551">
sg1 sg2 sg3 sg4
st su
st su
st su
st su
sw sv
sw sv
sw sv
sw
sv
sg5 sg6 sg7 sg8
st su
st su
st su
st su
sw sv
sw sv
sw sv
sw
sv
sg9 sg10 sg11 sg12
st su
st su
st su
st su
sw
sv
sw
sv
sw
sv
sw
sv
sg13 sg14 sg15 sg16
st su
st su
st su
st su
sw
sv
sw
sv
sw
sv
sw
sv
sg17 sg18 sg19 sg20
st su
st su
st su
st su
sw sv
sw sv
sw sv
sw
sv
sg21 sg22 sg23 sg24
st su
st su
st su
st su
sw
sv
sw
sv
sw
sv
sw
sv
</figure>
<page confidence="0.998285">
314
</page>
<bodyText confidence="0.993380666666667">
ings, confirming the intuition that outdegree only
measures node connectivity in graphs and it is not
enough to measure readability.
</bodyText>
<table confidence="0.900833428571429">
ρ p value
Number of Components −0.391 0.044
Relative frequency of 3-node Subgraphs
sg1 0.310 0.116
sg2 −0.325 0.098
sg3 −0.384 0.048
sg4 0.108 0.592
</table>
<tableCaption confidence="0.996129">
Table 4: Number of components and subgraph sg3 are
significantly correlated to readability.
</tableCaption>
<bodyText confidence="0.989181875">
Table 4 shows the correlation of two features
of projections6: The number of components has
a strong and significant negative correlation with
human readability ratings7, suggesting that simple
properties of graphs measure text coherence. The
lower part of Table 4 shows the correlation of the rel-
ative frequency of 3-node subgraphs (see Figure 8).
More readable articles have many sg1 and few num-
ber of sg2 patterns. Pattern sg3 is significantly and
negatively correlated with human readability judg-
ments, confirming the intuition that many shifts in
focus of attention make texts difficult to read.
Table 5 shows the correlation between the rela-
tive frequency of 4-node subgraphs and readabil-
ity ratings. First, most subgraphs with less than
four edges are negatively correlated with readabil-
ity, except sg20 and sg24 which are weakly corre-
lated with readability. Few connections between
sentences make the text difficult to read.
Second, the highest positive and significant cor-
relation of sg12 and the most negatively correlated
subgraph sg11 show that different patterns of edges
in subgraphs capture readability judgments. Stod-
dard (1991, p.29) explains this by the ambiguity
node phenomenon: “[...] in some cases, there may
be more than one logical, possible node for a given
cohesive element in a text, in which case, a reader
may see the resulting ambiguity but not be able to
6Although, the proposed features can be applied on all kind
of presented graphs, we evaluate them (except outdegree) only
on projections of the entity graph model. We leave the applica-
tion to the other graph representations for future work.
</bodyText>
<footnote confidence="0.984057333333333">
7This supports Karamanis et al. (2009) who report that
NOCB transitions in the centering model can be used for the
sentence ordering task.
</footnote>
<table confidence="0.99953096">
number of edges ρ p value
sg1 6 0.103 0.609
sg2 5 −0.212 0.288
sg3 5 −0.176 0.380
sg4 4 −0.257 0.196
sg5 5 −0.140 0. 486
sg6 5 0.200 0.317
sg7 5 −0.402 0.038
sg8 4 −0.317 0.107
sg9 5 0.153 0.446
sg10 4 −0.238 0.232
sg11 4 −0.509 0.007
sg12 4 0.449 0.019
sg13 4 −0.045 0.824
sg14 4 −0.033 0.870
sg15 3 −0.358 0.067
sg16 4 −0.068 0.736
sg17 3 −0.308 0.118
sg18 3 −0.546 0.003
sg19 3 −0.601 0.001
sg20 3 0.094 0.641
sg21 4 0.068 0.736
sg22 3 −0.374 0.055
sg23 3 −0.314 0.111
sg24 3 0.100 0.620
</table>
<tableCaption confidence="0.893746">
Table 5: The correlation between the relative frequency
of 4-node subgraphs and readability ratings.
</tableCaption>
<bodyText confidence="0.999971363636364">
decide between the choices”. E.g., in sg11 a reader
may make a decision about the focus of attention
in sw, while in sg12 the focus of attention of sw is
the same as the focus of attention of st. This phe-
nomenon can also be observed in all positively cor-
related subgraphs. If readers have to return to one
point in the text, they prefer to return to a sentence
which is the core of the preceding sentences. How-
ever, we should refrain of interpreting too much into
these patterns.
Finally, we conclude that in all strongly negative
correlated subgraphs, a subgraph suffers either from
edge shortage or the ambiguity node phenomenon
like sg7.
Considering the correlation of 3-node subgraphs
in Table 4 and 4-node subgraphs in Table 5, two
results are noticeable. First, in large subgraphs
there are more strongly correlated subgraphs than
3-node subgraphs, confirming our hypothesis that
larger subgraphs convey coherence patterns with
higher quality. Second, sg12 in 4-node subgraphs is
more strongly and positively correlated than sg4 in
</bodyText>
<page confidence="0.998322">
315
</page>
<bodyText confidence="0.999552571428571">
3-node subgraphs, because s912 captures more cir-
cumstances about st. The relative frequency of s912
is more informative than s94’s relative frequency.
Readability as ranking. Results of the readability
ranking problem are shown in Table 6. Baseline fea-
tures are entity transition features which are used as
coherence features by Pitler and Nenkova (2008)8.
</bodyText>
<table confidence="0.997374">
Features Accuracy
Baselines
None (Majority class) 47.85%
Baseline features (Pitler and Nenkova, 2008) 83.25%
Graph-based Features
Number of components 61.72%
Basic subgraphs (3-node) 79.43%
Frequent large subgraphs (4-node) 89.00%
Frequent basic + large subgraphs 88.52%
Baseline features + frequent large subgraphs 93.30%
</table>
<tableCaption confidence="0.996554">
Table 6: SVM prediction accuracy.
</tableCaption>
<bodyText confidence="0.999745941176471">
When classifying with graph signatures based on
basic subgraphs, accuracy is lower than with the
baseline coherence features. This is probably related
to the entity grid features which represent gram-
matical role transitions of entities, while the basic
subgraphs only models the occurrence of entities
across sentences. Graph signatures based on large
subgraphs improve the performance of basic sub-
graphs by around 10%. This high accuracy ver-
ifies that larger subgraphs capture coherence pat-
terns with high quality. Combining basic (3-node)
and large subgraphs (4-node) cannot improve the
performance of the large subgraphs features. This
probably is because basic subgraphs are implicitly
included in larger subgraphs. The combination of
coherence baseline features and frequent large sub-
graphs improves the accuracy.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999880833333333">
There is a research tradition developing metrics for
readability and using these metrics to quantify how
difficult it is to understand a document. Shallow fea-
tures such as word, sentence and text length, which
only capture superficial properties of a text, have
been used traditionally (Flesch, 1948; Kincaid et al.,
</bodyText>
<footnote confidence="0.945677">
8The accuracy reported in their paper is 79.42%. Our reim-
plementation achieves higher accuracy, because our dataset has
three articles less.
</footnote>
<bodyText confidence="0.9988696875">
1975). De Clercq et al. (2014) use traditional shal-
low features and apply these to a new corpus anno-
tated with two different methodologies. However,
some studies indicate that shallow features do not
precisely predict the readability of a text (Feng et al.,
2009; Petersen and Ostendorf, 2009). Later studies
introduce deeper (more semantic) features such as
those obtained by language models (Si and Callan,
2001; Collins-Thompson and Callan, 2004) and syn-
tactic features like the number of NPs in sentences
or the height of the sentence’s parse tree (Schwarm
and Ostendorf, 2005; Heilman et al., 2007). Barzi-
lay and Lapata (2008) propose an entity-based co-
herence model which operationalizes some of the
intuitions behind the centering model (Grosz et al.,
1995). Although this model works well on the sen-
tence ordering and summary coherence rating tasks,
it does not work well for readability assessment.
Only when combining the entity grid with features
taken from Schwarm and Ostendorf (2005) the en-
tity grid performs competitively.
While most of these studies predict the readabil-
ity level of documents, Pitler and Nenkova (2008)
present a new readability dataset with Wall Street
Journal articles, where each article is assigned hu-
man readability ratings. They analyze the correla-
tion between different readability features and hu-
man readability scores. They show no correla-
tion between entity-transition features and readabil-
ity scores. In contrast to them we are able to report
a statistically significant correlation between some
entity-based features and human readability ratings.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999973923076923">
We proposed graph-based coherence features based
on the notion of frequent subgraphs. We analyzed
these features on the dataset created by Pitler and
Nenkova (2008) which associates human readabil-
ity ratings with each document. We have shown that
frequent subgraphs represent coherence patterns in
a text. Larger subgraphs obtain a high and statisti-
cally significant correlation with human readability
ratings.
Pitler and Nenkova (2008) did not achieve statis-
tically significant (positive or negative) correlations
between their features derived from the entity grid
and human readability ratings. In contrast, some of
</bodyText>
<page confidence="0.997391">
316
</page>
<bodyText confidence="0.999885307692308">
our automatically induced subgraphs have a strong
statistically significant correlation. We also outper-
form Pitler and Nenkova (2008) in the readability
ranking task by more than 5% accuracy thus estab-
lishing a new state-of-the-art on this dataset. We
conclude that the graph-based representation (Guin-
audeau and Strube, 2013) is a better and more infor-
mative starting point for assessing readability.
In future work, we plan to induce common sub-
graphs and apply our method to different datasets
(e.g. the dataset created by De Clercq et al. (2014))
combined with other readability features (Schwarm
and Ostendorf, 2005).
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999893428571428">
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first author
has been supported by a HITS Ph.D. scholarship.
We would like to thank our colleagues Benjamin
Heinzerling, Yufang Hou, Sebastian Martschat,
Nafise Moosavi, and Daraksha Parveen who com-
mented on earlier drafts of this paper.
</bodyText>
<sectionHeader confidence="0.998141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999465192307692">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Chris Biemann, Stefanie Roos, and Karsten Weihe. 2012.
Quantifying semantics using complex network analy-
sis. In Proceedings of the 24th International Confer-
ence on Computational Linguistics, Mumbai, India, 8–
15 December 2012, pages 263–278.
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Boston, Mass., 2–7 May 2004, pages 193–200.
Franti&amp;quot;sek Dane&amp;quot;s. 1974. Functional sentence perspec-
tive and the organization of the text. In F. Dane&amp;quot;s, edi-
tor, Papers on Functional Sentence Perspective, pages
106–128. Prague: Academia.
Orph´ee De Clercq, V´eronique Hoste, Bart Desmet, Philip
Van Oosten, Martine De Cock, and Lieve Macken.
2014. Using the crowd for readability prediction. Nat-
ural Language Engineering, 20(3):293–325.
Lijun Feng, No´emie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, Athens, Greece, 30 March – 3 April
2009, pages 229–237.
Rudolf Flesch. 1948. A new readability yardstick. Jour-
nal of Applied Psychlogy, 32:221–233.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Camille Guinaudeau and Michael Strube. 2013. Graph-
based local coherence modeling. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), Sofia,
Bulgaria, 4–9 August 2013, pages 93–103.
Michael J. Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
lexical and grammatical features to improve readabil-
ity measures for first and second language texts. In
Proceedings of Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics, Rochester,
N.Y., 22–27 April 2007, pages 460–467.
Nikiforos Karamanis, Chris Mellish, Massimo Poesio,
and Jon Oberlander. 2009. Evaluating centering for
information ordering using corpora. Computational
Linguistics, 35(1):29–46.
Rohit Kate, Xiaoqiang Luo, Siddharth Patwardhan, Mar-
tin Franz, Radu Florian, Raymond Mooney, Salim
Roukos, and Chris Welty. 2010. Learning to pre-
dict readability using diverse linguistic features. In
Proceedings of the 23rd International Conference on
Computational Linguistics, Beijing, China, 23–27 Au-
gust 2010, pages 546–554.
J. Peter Kincaid, Robert P. Jr. Fishburne, Richard L.
Rogers, and Brad S. Chisson. 1975. Derivation of new
readability formulas (automated readability index, Fog
count and Flesch reading ease formula) for navy en-
listed personnel. Technical Report 8-75, Naval Tech-
nical Training Command, Naval Air Station Memphis-
Millington, Tenn., February.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), Portland, Oreg., 19–24 June
2011, pages 997–1006.
Ziheng Lin. 2011. Discourse parsing: Inferring dis-
course structure, modeling coherence, and its appli-
cations. Ph.D. thesis, Dept. of Computer Science,
School of Computing, National University of Singa-
pore.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
</reference>
<page confidence="0.983014">
317
</page>
<reference confidence="0.999654897959184">
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
Proceedings of ARPA Speech and Natural Language
Workshop.
Sebastian Nowozin, Koji Tsuda, Takeaki Uno, Taku
Kudo, and Gokhan BakIr. 2007. Weighted substruc-
ture mining for image analysis. In Proceedings of
the 2007 IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition, Minneapolis,
Minn., 18-23 June 2007, pages 1–8.
Sarah E. Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter Speech and Language, 23(1):89–106.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, Waikiki, Honolulu, Hawaii, 25–27 October 2008,
pages 186–195.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, 26 May – 1 June 2008.
Sarah E. Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25–30 June
2005, pages 523–530.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the ACM 10th
Conference on Information and Knowledge Manage-
ment, Atlanta, Georgia, 5–10 November 2001, pages
574–576.
Candace L. Sidner. 1983. Focusing in the com-
prehension of definite anaphora. In M. Brady and
R.C. Berwick, editors, Computational Models of Dis-
course, pages 267–330. Cambridge, Mass.: MIT
Press. Reprinted in: Grosz, Barbara J. et al. (Eds.)
(1986). Readings in Natural Language Processing.
Morgan Kaufman: Los Altos, Cal., pp.363-394.
Sally Stoddard. 1991. Text and Texture: Patterns of Co-
hesion. Ablex, Norwood, N.J.
Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-based
substructure pattern mining. In Proceedings of the
International Conference on Data Mining, Maebashi
City, Japan, 9–12 December 2002, pages 721–724.
</reference>
<page confidence="0.997836">
318
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.288084">
<title confidence="0.996581">Graph-based Coherence Modeling For Assessing Readability</title>
<author confidence="0.501119">Mesgar</author>
<affiliation confidence="0.5022915">Heidelberg Institute for Theoretical Studies Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.99868">69118 Heidelberg, Germany</address>
<email confidence="0.999047">(mohsen.mesgar|michael.strube)@h-its.org</email>
<abstract confidence="0.999092642857143">Readability depends on many factors ranging from shallow features like word length to semantic ones like coherence. We introduce novel graph-based coherence features based on frequent subgraphs and compare their ability to assess the readability of Wall Street Journal articles. In contrast to Pitler and Nenkova (2008) some of our graph-based features are significantly correlated with human judgments. We outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1585" citStr="Barzilay and Lapata, 2008" startWordPosition="220" endWordPosition="223">ntify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated with human judgments. The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhet</context>
<context position="4094" citStr="Barzilay and Lapata, 2008" startWordPosition="610" endWordPosition="613">Y ASSOCIATED FUNCTIONS WASHINGTON CHARLESTON BUREAUCRACY Figure 1: The entity graph representation of the text in Table 1. Dark entities are shared by the sentences. 2 Readability Assessment The quality of a text depends on different factors which make the text easier to read. These factors range from shallow features like word length to semantic features like coherence. Readability assessment leads to two problems: distinguishing and recognizing readability levels of texts and predicting human readability ratings. Pitler and Nenkova (2008) use all entity transitions of the entity grid model (Barzilay and Lapata, 2008) as coherence features. They compute the correlation between them and readability ratings and show that none of them is significantly correlated with human readability judgments. Indeed, none of these features on its own is a good predictor to measure coherence and to predict readability as well. 3 Method We introduce the graph representation of a text and propose to use these graphs to model coherence. 3.1 Graphs 3.1.1 Entity Graph Guinaudeau and Strube (2013) describe a graphbased version of the entity grid (Barzilay and Lapata, 2008) which models the interaction between entities and sentenc</context>
<context position="18919" citStr="Barzilay and Lapata, 2008" startWordPosition="3117" endWordPosition="3120"> every two texts whose human readability scores differ by at least 0.5. Each text is represented with its graph-based coherence features. We employ WEKA’s linear support vector implementation (SMO) to classify the pairs. Performance is evaluated using 10-fold cross-validation. 5 Results Readability assessment. We report the correlation of our coherence models encoded in graph features and compare them with Guinaudeau and Strube’s (2013) entity graph as the state-of-the-art coherence model. Pitler and Nenkova (2008) show that the entity transition features extracted from the entity grid model (Barzilay and Lapata, 2008) on its own do not significantly predict human readability ratings. So we do not describe their results here. The results for the outdegree feature is shown in Table 3. The average outdegree of PER w is highly correlated with human readability ratings. This confirms the readability results of Guinaudeau and Strube (2013) on the Encyclopedia Britannica dataset. The outdegrees of discourse relation graphs are more strongly correlated with human readability ratings than the outdegree of the projections in the entity graph, suggesting that efficient graph-based encoding of discourse relations can </context>
<context position="26648" citStr="Barzilay and Lapata (2008)" startWordPosition="4405" endWordPosition="4409">. De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most of these studies predict the readability level of documents, Pitler and Nenkova (2008) present a new readability dataset with Wall Street Journal articles, where each article is assi</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Stefanie Roos</author>
<author>Karsten Weihe</author>
</authors>
<title>Quantifying semantics using complex network analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<volume>8</volume>
<pages>263--278</pages>
<location>Mumbai,</location>
<contexts>
<context position="2756" citStr="Biemann et al., 2012" startWordPosition="403" endWordPosition="406">main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently consider the distribution of two aspects of coherence (i.e. entities and discourse relations) simultaneously. The second main idea is to apply subgraph mining algorithms to find frequent subgraphs (i.e. patterns) in texts (Section 3.2). Subgraph mining has been successfully applied to other tasks, e.g. image processing (Nowozin et al., 2007) and language modeling (Biemann et al., 2012). We hypothesize that text coherence correlates with frequent subgraphs (vaguely reminding us of coherence patterns (Daneˇs, 1974)) and that the mined patterns are good predictors for readability ratings. Our study is novel in introducing new and informative graph-based coherence features. We examine the predictive power of these feature in two experiments: first, readability rating prediction, and second, ranking texts according to the readability (Section 5). 309 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 309–318, Denver, Colorado, Ju</context>
</contexts>
<marker>Biemann, Roos, Weihe, 2012</marker>
<rawString>Chris Biemann, Stefanie Roos, and Karsten Weihe. 2012. Quantifying semantics using complex network analysis. In Proceedings of the 24th International Conference on Computational Linguistics, Mumbai, India, 8– 15 December 2012, pages 263–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>James P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<location>Boston, Mass.,</location>
<contexts>
<context position="26466" citStr="Collins-Thompson and Callan, 2004" startWordPosition="4374" endWordPosition="4377">raditionally (Flesch, 1948; Kincaid et al., 8The accuracy reported in their paper is 79.42%. Our reimplementation achieves higher accuracy, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most </context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Kevyn Collins-Thompson and James P. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, Mass., 2–7 May 2004, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frantisek Danes</author>
</authors>
<title>Functional sentence perspective and the organization of the text.</title>
<date>1974</date>
<booktitle>Papers on Functional Sentence Perspective,</booktitle>
<pages>106--128</pages>
<editor>In F. Dane&amp;quot;s, editor,</editor>
<publisher>Academia.</publisher>
<location>Prague:</location>
<marker>Danes, 1974</marker>
<rawString>Franti&amp;quot;sek Dane&amp;quot;s. 1974. Functional sentence perspective and the organization of the text. In F. Dane&amp;quot;s, editor, Papers on Functional Sentence Perspective, pages 106–128. Prague: Academia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Orph´ee De Clercq</author>
<author>V´eronique Hoste</author>
<author>Bart Desmet</author>
<author>Philip Van Oosten</author>
<author>Martine De Cock</author>
<author>Lieve Macken</author>
</authors>
<title>Using the crowd for readability prediction.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>De Clercq, Hoste, Desmet, Van Oosten, De Cock, Macken, 2014</marker>
<rawString>Orph´ee De Clercq, V´eronique Hoste, Bart Desmet, Philip Van Oosten, Martine De Cock, and Lieve Macken. 2014. Using the crowd for readability prediction. Natural Language Engineering, 20(3):293–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
<author>No´emie Elhadad</author>
<author>Matt Huenerfauth</author>
</authors>
<title>Cognitively motivated features for readability assessment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>229--237</pages>
<location>Athens,</location>
<contexts>
<context position="26279" citStr="Feng et al., 2009" startWordPosition="4348" endWordPosition="4351">w difficult it is to understand a document. Shallow features such as word, sentence and text length, which only capture superficial properties of a text, have been used traditionally (Flesch, 1948; Kincaid et al., 8The accuracy reported in their paper is 79.42%. Our reimplementation achieves higher accuracy, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it d</context>
</contexts>
<marker>Feng, Elhadad, Huenerfauth, 2009</marker>
<rawString>Lijun Feng, No´emie Elhadad, and Matt Huenerfauth. 2009. Cognitively motivated features for readability assessment. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece, 30 March – 3 April 2009, pages 229–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of Applied Psychlogy,</journal>
<pages>32--221</pages>
<contexts>
<context position="1374" citStr="Flesch, 1948" startWordPosition="191" endWordPosition="192"> a new state-of-the-art on this dataset. 1 Introduction Readability depends on many factors which enable readers to process a text. These factors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated with human judgments. The main goal of this paper is to introduce novel graph-based coherence features for asse</context>
<context position="25858" citStr="Flesch, 1948" startWordPosition="4282" endWordPosition="4283">ning basic (3-node) and large subgraphs (4-node) cannot improve the performance of the large subgraphs features. This probably is because basic subgraphs are implicitly included in larger subgraphs. The combination of coherence baseline features and frequent large subgraphs improves the accuracy. 6 Related Work There is a research tradition developing metrics for readability and using these metrics to quantify how difficult it is to understand a document. Shallow features such as word, sentence and text length, which only capture superficial properties of a text, have been used traditionally (Flesch, 1948; Kincaid et al., 8The accuracy reported in their paper is 79.42%. Our reimplementation achieves higher accuracy, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Calla</context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>Rudolf Flesch. 1948. A new readability yardstick. Journal of Applied Psychlogy, 32:221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="13465" citStr="Grosz et al., 1995" startWordPosition="2200" endWordPosition="2203">subsequent ones. In other words, at least two entities are mentioned in one sentence and the subsequent ones are about these entities. (a) (b) s1 s2 s3 s4 s6 s5 s 1 s2 s3 s4 s6 s5 (a) (b) (c) 312 Figure 8: Feasible 3-node subgraph coherence features. Node labels illustrate the order of sentences. Sentence st occurs before sentence su, and sentence su occurs before sentence sv (i.e. t &lt; u &lt; v). • sg2: Indicates that entities in st and su get connected to each other in sv. • sg3: Each sentence tends to refer to the most prominent entity (focus of attention) in preceding sentences (Sidner, 1983; Grosz et al., 1995). The absence of a connection between st and sv indicates that the entity connecting st and su is different from the entity connecting su and sv. Therefore this subgraph approximately corresponds to the shift of the focus of attention. • sg4: Merges sg1 and sg3 and represents all connections of these two subgraphs. We use these feasible 3-node subgraphs and compute the graph signature, Φ, of each G E �. We propose each (p E Φ (i.e. relative frequency of each subgraph in G) as a connectivity feature of graph G to measure text coherence. Frequent large subgraphs. Since we observe a strong correl</context>
<context position="26781" citStr="Grosz et al., 1995" startWordPosition="4426" endWordPosition="4429">er, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most of these studies predict the readability level of documents, Pitler and Nenkova (2008) present a new readability dataset with Wall Street Journal articles, where each article is assigned human readability ratings. They analyze the correlation between different readability features and human readability scores. The</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Guinaudeau</author>
<author>Michael Strube</author>
</authors>
<title>Graphbased local coherence modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>93--103</pages>
<location>Sofia, Bulgaria, 4–9</location>
<contexts>
<context position="2086" citStr="Guinaudeau and Strube (2013)" startWordPosition="298" endWordPosition="301">mpson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated with human judgments. The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently consider the distribution of two aspects of coherence (i.e. entities and discourse relations) simultaneously. The second main idea is to apply subgraph mining algorithms to find frequent subgraphs (i.e. patterns) in texts (Section 3.2). Subgraph mining has been successfully applied to other tasks, e.g. image processi</context>
<context position="4559" citStr="Guinaudeau and Strube (2013)" startWordPosition="686" endWordPosition="689">levels of texts and predicting human readability ratings. Pitler and Nenkova (2008) use all entity transitions of the entity grid model (Barzilay and Lapata, 2008) as coherence features. They compute the correlation between them and readability ratings and show that none of them is significantly correlated with human readability judgments. Indeed, none of these features on its own is a good predictor to measure coherence and to predict readability as well. 3 Method We introduce the graph representation of a text and propose to use these graphs to model coherence. 3.1 Graphs 3.1.1 Entity Graph Guinaudeau and Strube (2013) describe a graphbased version of the entity grid (Barzilay and Lapata, 2008) which models the interaction between entities and sentences as a bipartite graph. This graph contains two sets of nodes: sentences and entities. Sentence and entity nodes are connected if and only if the entity is mentioned in the sentence (Figure 1). Edges are weighted according to the grammatical role of the entity mentioned in the sentence. Guinaudeau and Strube (2013) model entity transitions between sentences via a one-mode projection of the entity graph. The one-mode projection is a graph consisting of sentence</context>
<context position="9356" citStr="Guinaudeau and Strube, 2013" startWordPosition="1474" endWordPosition="1478"> at least by one connection, whether entity transition (PER u ) or discourse relations (PDR u ). The other basic logical operators (e.g. ∧ or ®) lose connections. Hence we do not report on their performance. Inspired by linear regression models we combine the weighted graphs by adding (+) the edge weights in PER w and PDR w (Figure 4). PER uV PDR PER w+ PDR u w Figure 4: Combined entity and discourse relation graphs. 3.2 Coherence Features We use the proposed graphs to introduce novel coherence features. Average outdegree. Measures to which extent a sentence is connected with other sentences (Guinaudeau and Strube, 2013): AvgOutDegree(P) = IISII s1 s2 s5 s3 s4 s1 s2 1 1 s5 1 1 s3 s4 Relation Arg1 Arg2 Implicit Expansion Explicit Comparison Implicit Expansion Implicit Temporal Implicit Contingency S1 S2 S2 S2 S2 S3 S3 S4 S4 S5 s1 s2 s5 s3 s4 1 s1 s2 1 s5 1 1 s3 s4 1 s1 s2 s5 s3 s4 1 s1 s2 1 1 s5 1 1 1 2 s3 s4 1 ∑sES OutDegree(s) 311 where OutDegree(s) is the sum of the weights associated with edges that leave node s and ISI is the number of sentences in the text. Number of components. The projection graph can be disconnected. A graph is disconnected if there are at least two nodes which are not reachable from </context>
<context position="19241" citStr="Guinaudeau and Strube (2013)" startWordPosition="3169" endWordPosition="3172">t the correlation of our coherence models encoded in graph features and compare them with Guinaudeau and Strube’s (2013) entity graph as the state-of-the-art coherence model. Pitler and Nenkova (2008) show that the entity transition features extracted from the entity grid model (Barzilay and Lapata, 2008) on its own do not significantly predict human readability ratings. So we do not describe their results here. The results for the outdegree feature is shown in Table 3. The average outdegree of PER w is highly correlated with human readability ratings. This confirms the readability results of Guinaudeau and Strube (2013) on the Encyclopedia Britannica dataset. The outdegrees of discourse relation graphs are more strongly correlated with human readability ratings than the outdegree of the projections in the entity graph, suggesting that efficient graph-based encoding of discourse relations can measure readability well. The outdegree of the combined graph PER w + PDR w is highly correlated, showing that the interaction of entity connections and discourse relations is important for text coherence. However, none of the outdegree measures in this table are significantly correlated with human readability ratsg1 sg2</context>
<context position="28573" citStr="Guinaudeau and Strube, 2013" startWordPosition="4691" endWordPosition="4695">s obtain a high and statistically significant correlation with human readability ratings. Pitler and Nenkova (2008) did not achieve statistically significant (positive or negative) correlations between their features derived from the entity grid and human readability ratings. In contrast, some of 316 our automatically induced subgraphs have a strong statistically significant correlation. We also outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset. We conclude that the graph-based representation (Guinaudeau and Strube, 2013) is a better and more informative starting point for assessing readability. In future work, we plan to induce common subgraphs and apply our method to different datasets (e.g. the dataset created by De Clercq et al. (2014)) combined with other readability features (Schwarm and Ostendorf, 2005). Acknowledgments This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a HITS Ph.D. scholarship. We would like to thank our colleagues Benjamin Heinzerling, Yufang Hou, Sebastian Martschat, Nafise Moosavi, and Daraksha Parveen who commented</context>
</contexts>
<marker>Guinaudeau, Strube, 2013</marker>
<rawString>Camille Guinaudeau and Michael Strube. 2013. Graphbased local coherence modeling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, 4–9 August 2013, pages 93–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>460--467</pages>
<location>Rochester, N.Y.,</location>
<contexts>
<context position="26620" citStr="Heilman et al., 2007" startWordPosition="4401" endWordPosition="4404">ee articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most of these studies predict the readability level of documents, Pitler and Nenkova (2008) present a new readability dataset with Wall Street Journal articles</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, N.Y., 22–27 April 2007, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Chris Mellish</author>
<author>Massimo Poesio</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centering for information ordering using corpora.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="22276" citStr="Karamanis et al. (2009)" startWordPosition="3702" endWordPosition="3705">sg11 show that different patterns of edges in subgraphs capture readability judgments. Stoddard (1991, p.29) explains this by the ambiguity node phenomenon: “[...] in some cases, there may be more than one logical, possible node for a given cohesive element in a text, in which case, a reader may see the resulting ambiguity but not be able to 6Although, the proposed features can be applied on all kind of presented graphs, we evaluate them (except outdegree) only on projections of the entity graph model. We leave the application to the other graph representations for future work. 7This supports Karamanis et al. (2009) who report that NOCB transitions in the centering model can be used for the sentence ordering task. number of edges ρ p value sg1 6 0.103 0.609 sg2 5 −0.212 0.288 sg3 5 −0.176 0.380 sg4 4 −0.257 0.196 sg5 5 −0.140 0. 486 sg6 5 0.200 0.317 sg7 5 −0.402 0.038 sg8 4 −0.317 0.107 sg9 5 0.153 0.446 sg10 4 −0.238 0.232 sg11 4 −0.509 0.007 sg12 4 0.449 0.019 sg13 4 −0.045 0.824 sg14 4 −0.033 0.870 sg15 3 −0.358 0.067 sg16 4 −0.068 0.736 sg17 3 −0.308 0.118 sg18 3 −0.546 0.003 sg19 3 −0.601 0.001 sg20 3 0.094 0.641 sg21 4 0.068 0.736 sg22 3 −0.374 0.055 sg23 3 −0.314 0.111 sg24 3 0.100 0.620 Table 5:</context>
</contexts>
<marker>Karamanis, Mellish, Poesio, Oberlander, 2009</marker>
<rawString>Nikiforos Karamanis, Chris Mellish, Massimo Poesio, and Jon Oberlander. 2009. Evaluating centering for information ordering using corpora. Computational Linguistics, 35(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kate</author>
<author>Xiaoqiang Luo</author>
<author>Siddharth Patwardhan</author>
<author>Martin Franz</author>
<author>Radu Florian</author>
<author>Raymond Mooney</author>
<author>Salim Roukos</author>
<author>Chris Welty</author>
</authors>
<title>Learning to predict readability using diverse linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>546--554</pages>
<location>Beijing, China, 23–27</location>
<contexts>
<context position="1262" citStr="Kate et al., 2010" startWordPosition="173" endWordPosition="176">ts. We outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset. 1 Introduction Readability depends on many factors which enable readers to process a text. These factors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated</context>
</contexts>
<marker>Kate, Luo, Patwardhan, Franz, Florian, Mooney, Roukos, Welty, 2010</marker>
<rawString>Rohit Kate, Xiaoqiang Luo, Siddharth Patwardhan, Martin Franz, Radu Florian, Raymond Mooney, Salim Roukos, and Chris Welty. 2010. Learning to predict readability using diverse linguistic features. In Proceedings of the 23rd International Conference on Computational Linguistics, Beijing, China, 23–27 August 2010, pages 546–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Rogers Fishburne</author>
<author>Brad S Chisson</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, Fog count and Flesch reading ease formula) for navy enlisted personnel.</title>
<date>1975</date>
<tech>Technical Report 8-75,</tech>
<institution>Naval Technical Training Command, Naval Air Station MemphisMillington,</institution>
<location>Tenn.,</location>
<marker>Fishburne, Chisson, 1975</marker>
<rawString>J. Peter Kincaid, Robert P. Jr. Fishburne, Richard L. Rogers, and Brad S. Chisson. 1975. Derivation of new readability formulas (automated readability index, Fog count and Flesch reading ease formula) for navy enlisted personnel. Technical Report 8-75, Naval Technical Training Command, Naval Air Station MemphisMillington, Tenn., February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>997--1006</pages>
<location>Portland, Oreg.,</location>
<contexts>
<context position="6992" citStr="Lin et al. (2011)" startWordPosition="1082" endWordPosition="1085">he [mayor] of [Charleston] complained bitterly about the federal [bureaucracy]’s response to [Hurricane Hugo]. S5: The [sense] grows that modern public [bureaucracies] simply don’t perform their assigned [functions] well. Table 1: A sample text from the Wall Street Journal dataset (Pitler and Nenkova, 2008). 310 PER PER u w Figure 2: PER u : unweighted, and PER w : weighted projection graphs. In the weighted projection all edge weights are equal to one, because all sentences share one entity. experiments as well. Thus, we do explain further details of PER where. 3.1.2 Discourse Relation Graph Lin et al. (2011) and Lin (2011) use Rhetorical Structure Theory (RST) to describe and model coherence by considering the transitions between discourse relations. Inspired by the entity grid they expand the relation sequence into a two-dimensional matrix whose rows and columns are sentences and entities, respectively. The cell (si,ej) corresponds to the set of discourse relations entity ej is involved with in sentence si. These methods are based on entity transitions which, however, are intuitively implausible, because discourse relations connect sentences (or elementary discourse units). Since discourse relat</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Portland, Oreg., 19–24 June 2011, pages 997–1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
</authors>
<title>Discourse parsing: Inferring discourse structure, modeling coherence, and its applications.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computer Science, School of Computing, National University of Singapore.</institution>
<contexts>
<context position="7007" citStr="Lin (2011)" startWordPosition="1087" endWordPosition="1088">ton] complained bitterly about the federal [bureaucracy]’s response to [Hurricane Hugo]. S5: The [sense] grows that modern public [bureaucracies] simply don’t perform their assigned [functions] well. Table 1: A sample text from the Wall Street Journal dataset (Pitler and Nenkova, 2008). 310 PER PER u w Figure 2: PER u : unweighted, and PER w : weighted projection graphs. In the weighted projection all edge weights are equal to one, because all sentences share one entity. experiments as well. Thus, we do explain further details of PER where. 3.1.2 Discourse Relation Graph Lin et al. (2011) and Lin (2011) use Rhetorical Structure Theory (RST) to describe and model coherence by considering the transitions between discourse relations. Inspired by the entity grid they expand the relation sequence into a two-dimensional matrix whose rows and columns are sentences and entities, respectively. The cell (si,ej) corresponds to the set of discourse relations entity ej is involved with in sentence si. These methods are based on entity transitions which, however, are intuitively implausible, because discourse relations connect sentences (or elementary discourse units). Since discourse relations capture in</context>
</contexts>
<marker>Lin, 2011</marker>
<rawString>Ziheng Lin. 2011. Discourse parsing: Inferring discourse structure, modeling coherence, and its applications. Ph.D. thesis, Dept. of Computer Science, School of Computing, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of ARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="15462" citStr="Marcus et al., 1994" startWordPosition="2552" endWordPosition="2556">te the graph signature of each G E � and take each element of the graph signature as a coherence feature. 4 Experiments 4.1 Data We use the dataset created by Pitler and Nenkova (2008) which consists of randomly selected articles from the Wall Street Journal corpus. The articles were rated by three humans on a scale from 1 to 5 for readability based on quality measures that are designed to estimate the coherence of articles. The final readability score of each article is the average of these three ratings. We exclude three files from this dataset: wsj-0382 does not exist in the Penn Treebank (Marcus et al., 1994)1. wsj-2090 does not exist in the Penn Discource Treebank (Prasad et al., 2008). wsj-1398 is a poem. 4.2 Settings Entity graph. We use the gold parse trees in the Penn Treebank (Marcus et al., 1994) to extract all nouns in a document as mentions. We consider nouns with identical stem2 as coreferent. We divide the edge weight between two sentence nodes si and sj by their distance j − i to decrease the importance of links that exist between non-adjacent sentences. Discourse relation graph. We use gold PDTB-style discourse relations (Prasad et al., 2008). We filter out EntRel and NoRel relations.</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn treebank: Annotating predicate argument structure. In Proceedings of ARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Nowozin</author>
<author>Koji Tsuda</author>
<author>Takeaki Uno</author>
<author>Taku Kudo</author>
<author>Gokhan BakIr</author>
</authors>
<title>Weighted substructure mining for image analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<location>Minneapolis, Minn.,</location>
<contexts>
<context position="2711" citStr="Nowozin et al., 2007" startWordPosition="396" endWordPosition="399">ction 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently consider the distribution of two aspects of coherence (i.e. entities and discourse relations) simultaneously. The second main idea is to apply subgraph mining algorithms to find frequent subgraphs (i.e. patterns) in texts (Section 3.2). Subgraph mining has been successfully applied to other tasks, e.g. image processing (Nowozin et al., 2007) and language modeling (Biemann et al., 2012). We hypothesize that text coherence correlates with frequent subgraphs (vaguely reminding us of coherence patterns (Daneˇs, 1974)) and that the mined patterns are good predictors for readability ratings. Our study is novel in introducing new and informative graph-based coherence features. We examine the predictive power of these feature in two experiments: first, readability rating prediction, and second, ranking texts according to the readability (Section 5). 309 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*S</context>
</contexts>
<marker>Nowozin, Tsuda, Uno, Kudo, BakIr, 2007</marker>
<rawString>Sebastian Nowozin, Koji Tsuda, Takeaki Uno, Taku Kudo, and Gokhan BakIr. 2007. Weighted substructure mining for image analysis. In Proceedings of the 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Minneapolis, Minn., 18-23 June 2007, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Petersen</author>
<author>Mari Ostendorf</author>
</authors>
<title>A machine learning approach to reading level assessment.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="26310" citStr="Petersen and Ostendorf, 2009" startWordPosition="4352" endWordPosition="4355">o understand a document. Shallow features such as word, sentence and text length, which only capture superficial properties of a text, have been used traditionally (Flesch, 1948; Kincaid et al., 8The accuracy reported in their paper is 79.42%. Our reimplementation achieves higher accuracy, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readabili</context>
</contexts>
<marker>Petersen, Ostendorf, 2009</marker>
<rawString>Sarah E. Petersen and Mari Ostendorf. 2009. A machine learning approach to reading level assessment. Computer Speech and Language, 23(1):89–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<location>Waikiki, Honolulu, Hawaii,</location>
<contexts>
<context position="687" citStr="Pitler and Nenkova (2008)" startWordPosition="87" endWordPosition="90">n Mesgar and Michael Strube Heidelberg Institute for Theoretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany (mohsen.mesgar|michael.strube)@h-its.org Abstract Readability depends on many factors ranging from shallow features like word length to semantic ones like coherence. We introduce novel graph-based coherence features based on frequent subgraphs and compare their ability to assess the readability of Wall Street Journal articles. In contrast to Pitler and Nenkova (2008) some of our graph-based features are significantly correlated with human judgments. We outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset. 1 Introduction Readability depends on many factors which enable readers to process a text. These factors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text fea</context>
<context position="4014" citStr="Pitler and Nenkova (2008)" startWordPosition="596" endWordPosition="599">... ... ... OFFICIALS GEAR HOURS NEWS PRESS SENSE HUGO TABLE MAKERS MAYOR POLICY ASSOCIATED FUNCTIONS WASHINGTON CHARLESTON BUREAUCRACY Figure 1: The entity graph representation of the text in Table 1. Dark entities are shared by the sentences. 2 Readability Assessment The quality of a text depends on different factors which make the text easier to read. These factors range from shallow features like word length to semantic features like coherence. Readability assessment leads to two problems: distinguishing and recognizing readability levels of texts and predicting human readability ratings. Pitler and Nenkova (2008) use all entity transitions of the entity grid model (Barzilay and Lapata, 2008) as coherence features. They compute the correlation between them and readability ratings and show that none of them is significantly correlated with human readability judgments. Indeed, none of these features on its own is a good predictor to measure coherence and to predict readability as well. 3 Method We introduce the graph representation of a text and propose to use these graphs to model coherence. 3.1 Graphs 3.1.1 Entity Graph Guinaudeau and Strube (2013) describe a graphbased version of the entity grid (Barz</context>
<context position="6683" citStr="Pitler and Nenkova, 2008" startWordPosition="1026" endWordPosition="1029">vate [relief] [agencies], such as the [Salvation] [Army] and [Red] [Cross], mobilized almost instantly to help [people], while the [Washington] [bureaucracy] ”took [hours] getting into [gear].” S3: One [news] show we saw [yesterday] even displayed 25 federal [officials] meeting around a [table]. S4: We recall that the [mayor] of [Charleston] complained bitterly about the federal [bureaucracy]’s response to [Hurricane Hugo]. S5: The [sense] grows that modern public [bureaucracies] simply don’t perform their assigned [functions] well. Table 1: A sample text from the Wall Street Journal dataset (Pitler and Nenkova, 2008). 310 PER PER u w Figure 2: PER u : unweighted, and PER w : weighted projection graphs. In the weighted projection all edge weights are equal to one, because all sentences share one entity. experiments as well. Thus, we do explain further details of PER where. 3.1.2 Discourse Relation Graph Lin et al. (2011) and Lin (2011) use Rhetorical Structure Theory (RST) to describe and model coherence by considering the transitions between discourse relations. Inspired by the entity grid they expand the relation sequence into a two-dimensional matrix whose rows and columns are sentences and entities, re</context>
<context position="15026" citStr="Pitler and Nenkova (2008)" startWordPosition="2475" endWordPosition="2478"> subgraph mining algorithm to extract all subgraphs with size k and then compute the count of each subgraph as an induced subgraph in each graph G E �. We retain a subgraph sg, if it is frequent (i.e. support(sg) &gt; X). The result of these steps is a two-dimensional matrix whose rows represent graphs in � and columns represent frequent subgraphs with size k. The cell (Gi,sgj) shows the count of sgj in graph Gi. Given this matrix, we compute the graph signature of each G E � and take each element of the graph signature as a coherence feature. 4 Experiments 4.1 Data We use the dataset created by Pitler and Nenkova (2008) which consists of randomly selected articles from the Wall Street Journal corpus. The articles were rated by three humans on a scale from 1 to 5 for readability based on quality measures that are designed to estimate the coherence of articles. The final readability score of each article is the average of these three ratings. We exclude three files from this dataset: wsj-0382 does not exist in the Penn Treebank (Marcus et al., 1994)1. wsj-2090 does not exist in the Penn Discource Treebank (Prasad et al., 2008). wsj-1398 is a poem. 4.2 Settings Entity graph. We use the gold parse trees in the P</context>
<context position="16760" citStr="Pitler and Nenkova (2008)" startWordPosition="2763" endWordPosition="2766">ection graph, the SageMath3 package is used. This feature is computed on unweighted projections (i.e. PER u ). Frequent subgraphs. Since subgraph mining is an NP-complete problem, different algorithms have been introduced to improve the performance of subgraph mining. We use the gSpan4 algorithm (Yan and Han, 2002) to mine subgraphs of a graph database which contains PER uprojections. An advantage of using efficient subgraph mining algorithms is that we can exhaustively search very large subgraph spaces. A graph with IEI edges, however, potentially has O(2IEI) subgraphs. Having sparse graphs 1Pitler and Nenkova (2008) also remove one file from their experiments. We assume that it is wsj-0382. 2We use Stanford CoreNLP (http://nlp.stanford. edu/software/corenlp.shtml) 3http://sagemath.org/download-linux.html 4We use the Java package: http://www.cs.ucsb. edu/˜xyan/software/gSpan.htm sg1 sg2 sg3 sg4 st su sv st su sv st su sv st su sv 313 Figure 9: Frequent subgraphs with four nodes where t &lt; u &lt; v &lt; w. and using efficient subgraph mining algorithm lets us to search trough this space. We mine subgraphs with k = 4 and X = 0 (Figure 9). 4.3 Evaluation We evaluate on the following benchmark tasks. Readability ass</context>
<context position="18813" citStr="Pitler and Nenkova (2008)" startWordPosition="3100" endWordPosition="3103">ion of the average outdegree of different graphs with human readability ratings. is more readable. We use every two texts whose human readability scores differ by at least 0.5. Each text is represented with its graph-based coherence features. We employ WEKA’s linear support vector implementation (SMO) to classify the pairs. Performance is evaluated using 10-fold cross-validation. 5 Results Readability assessment. We report the correlation of our coherence models encoded in graph features and compare them with Guinaudeau and Strube’s (2013) entity graph as the state-of-the-art coherence model. Pitler and Nenkova (2008) show that the entity transition features extracted from the entity grid model (Barzilay and Lapata, 2008) on its own do not significantly predict human readability ratings. So we do not describe their results here. The results for the outdegree feature is shown in Table 3. The average outdegree of PER w is highly correlated with human readability ratings. This confirms the readability results of Guinaudeau and Strube (2013) on the Encyclopedia Britannica dataset. The outdegrees of discourse relation graphs are more strongly correlated with human readability ratings than the outdegree of the p</context>
<context position="24365" citStr="Pitler and Nenkova (2008)" startWordPosition="4063" endWordPosition="4066">large subgraphs there are more strongly correlated subgraphs than 3-node subgraphs, confirming our hypothesis that larger subgraphs convey coherence patterns with higher quality. Second, sg12 in 4-node subgraphs is more strongly and positively correlated than sg4 in 315 3-node subgraphs, because s912 captures more circumstances about st. The relative frequency of s912 is more informative than s94’s relative frequency. Readability as ranking. Results of the readability ranking problem are shown in Table 6. Baseline features are entity transition features which are used as coherence features by Pitler and Nenkova (2008)8. Features Accuracy Baselines None (Majority class) 47.85% Baseline features (Pitler and Nenkova, 2008) 83.25% Graph-based Features Number of components 61.72% Basic subgraphs (3-node) 79.43% Frequent large subgraphs (4-node) 89.00% Frequent basic + large subgraphs 88.52% Baseline features + frequent large subgraphs 93.30% Table 6: SVM prediction accuracy. When classifying with graph signatures based on basic subgraphs, accuracy is lower than with the baseline coherence features. This is probably related to the entity grid features which represent grammatical role transitions of entities, whi</context>
<context position="27152" citStr="Pitler and Nenkova (2008)" startWordPosition="4485" endWordPosition="4488">r the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most of these studies predict the readability level of documents, Pitler and Nenkova (2008) present a new readability dataset with Wall Street Journal articles, where each article is assigned human readability ratings. They analyze the correlation between different readability features and human readability scores. They show no correlation between entity-transition features and readability scores. In contrast to them we are able to report a statistically significant correlation between some entity-based features and human readability ratings. 7 Conclusions We proposed graph-based coherence features based on the notion of frequent subgraphs. We analyzed these features on the dataset </context>
<context position="28380" citStr="Pitler and Nenkova (2008)" startWordPosition="4662" endWordPosition="4665">ated by Pitler and Nenkova (2008) which associates human readability ratings with each document. We have shown that frequent subgraphs represent coherence patterns in a text. Larger subgraphs obtain a high and statistically significant correlation with human readability ratings. Pitler and Nenkova (2008) did not achieve statistically significant (positive or negative) correlations between their features derived from the entity grid and human readability ratings. In contrast, some of 316 our automatically induced subgraphs have a strong statistically significant correlation. We also outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset. We conclude that the graph-based representation (Guinaudeau and Strube, 2013) is a better and more informative starting point for assessing readability. In future work, we plan to induce common subgraphs and apply our method to different datasets (e.g. the dataset created by De Clercq et al. (2014)) combined with other readability features (Schwarm and Ostendorf, 2005). Acknowledgments This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first autho</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25–27 October 2008, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<volume>1</volume>
<location>Marrakech,</location>
<contexts>
<context position="7747" citStr="Prasad et al., 2008" startWordPosition="1196" endWordPosition="1199">relations. Inspired by the entity grid they expand the relation sequence into a two-dimensional matrix whose rows and columns are sentences and entities, respectively. The cell (si,ej) corresponds to the set of discourse relations entity ej is involved with in sentence si. These methods are based on entity transitions which, however, are intuitively implausible, because discourse relations connect sentences (or elementary discourse units). Since discourse relations capture interactions between sentences (Table 2), we model these relations with a graph. Table 2: PDTB-style discourse relations (Prasad et al., 2008) of the sample text in Table 1 A discourse relation graph is PDR u = (V,R), where V is the set of sentence nodes and R is the edge set which represents all discourse relations in the text. Two sentence nodes are adjacent if and only if they are connected by at least one discourse relation. Intra-sentential discourse relations are represented as self-edges. We define PDR w as a weighted discourse relation graph whose edge weights are PDR PDR u w Figure 3: PDR u : unweighted, and PDR w : weighted discourse relation graphs. the number of discourse relations between sentence nodes (Figure 3). 3.1.</context>
<context position="15541" citStr="Prasad et al., 2008" startWordPosition="2566" endWordPosition="2569">re as a coherence feature. 4 Experiments 4.1 Data We use the dataset created by Pitler and Nenkova (2008) which consists of randomly selected articles from the Wall Street Journal corpus. The articles were rated by three humans on a scale from 1 to 5 for readability based on quality measures that are designed to estimate the coherence of articles. The final readability score of each article is the average of these three ratings. We exclude three files from this dataset: wsj-0382 does not exist in the Penn Treebank (Marcus et al., 1994)1. wsj-2090 does not exist in the Penn Discource Treebank (Prasad et al., 2008). wsj-1398 is a poem. 4.2 Settings Entity graph. We use the gold parse trees in the Penn Treebank (Marcus et al., 1994) to extract all nouns in a document as mentions. We consider nouns with identical stem2 as coreferent. We divide the edge weight between two sentence nodes si and sj by their distance j − i to decrease the importance of links that exist between non-adjacent sentences. Discourse relation graph. We use gold PDTB-style discourse relations (Prasad et al., 2008). We filter out EntRel and NoRel relations. Number of components. For counting the number of components in each projection</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco, 26 May – 1 June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<location>Ann Arbor, Mich.,</location>
<contexts>
<context position="1531" citStr="Schwarm and Ostendorf, 2005" startWordPosition="211" endWordPosition="214">tors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated with human judgments. The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The fi</context>
<context position="26597" citStr="Schwarm and Ostendorf, 2005" startWordPosition="4397" endWordPosition="4400">, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid performs competitively. While most of these studies predict the readability level of documents, Pitler and Nenkova (2008) present a new readability dataset with Wall </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah E. Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Mich., 25–30 June 2005, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luo Si</author>
<author>Jamie Callan</author>
</authors>
<title>A statistical model for scientific readability.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM 10th Conference on Information and Knowledge Management,</booktitle>
<pages>574--576</pages>
<location>Atlanta,</location>
<contexts>
<context position="1446" citStr="Si and Callan, 2001" startWordPosition="200" endWordPosition="203">ty depends on many factors which enable readers to process a text. These factors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence has some connections with other sentences. Although these local connections make the text more readable, the corresponding coherence features used in Pitler and Nenkova (2008) (Section 2) are not strongly correlated with human judgments. The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coheren</context>
<context position="26430" citStr="Si and Callan, 2001" startWordPosition="4370" endWordPosition="4373">ext, have been used traditionally (Flesch, 1948; Kincaid et al., 8The accuracy reported in their paper is 79.42%. Our reimplementation achieves higher accuracy, because our dataset has three articles less. 1975). De Clercq et al. (2014) use traditional shallow features and apply these to a new corpus annotated with two different methodologies. However, some studies indicate that shallow features do not precisely predict the readability of a text (Feng et al., 2009; Petersen and Ostendorf, 2009). Later studies introduce deeper (more semantic) features such as those obtained by language models (Si and Callan, 2001; Collins-Thompson and Callan, 2004) and syntactic features like the number of NPs in sentences or the height of the sentence’s parse tree (Schwarm and Ostendorf, 2005; Heilman et al., 2007). Barzilay and Lapata (2008) propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model (Grosz et al., 1995). Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. Only when combining the entity grid with features taken from Schwarm and Ostendorf (2005) the entity grid</context>
</contexts>
<marker>Si, Callan, 2001</marker>
<rawString>Luo Si and Jamie Callan. 2001. A statistical model for scientific readability. In Proceedings of the ACM 10th Conference on Information and Knowledge Management, Atlanta, Georgia, 5–10 November 2001, pages 574–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Focusing in the comprehension of definite anaphora.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>267--330</pages>
<editor>In M. Brady and R.C. Berwick, editors,</editor>
<publisher>MIT Press. Reprinted</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="13444" citStr="Sidner, 1983" startWordPosition="2198" endWordPosition="2199"> sentence and subsequent ones. In other words, at least two entities are mentioned in one sentence and the subsequent ones are about these entities. (a) (b) s1 s2 s3 s4 s6 s5 s 1 s2 s3 s4 s6 s5 (a) (b) (c) 312 Figure 8: Feasible 3-node subgraph coherence features. Node labels illustrate the order of sentences. Sentence st occurs before sentence su, and sentence su occurs before sentence sv (i.e. t &lt; u &lt; v). • sg2: Indicates that entities in st and su get connected to each other in sv. • sg3: Each sentence tends to refer to the most prominent entity (focus of attention) in preceding sentences (Sidner, 1983; Grosz et al., 1995). The absence of a connection between st and sv indicates that the entity connecting st and su is different from the entity connecting su and sv. Therefore this subgraph approximately corresponds to the shift of the focus of attention. • sg4: Merges sg1 and sg3 and represents all connections of these two subgraphs. We use these feasible 3-node subgraphs and compute the graph signature, Φ, of each G E �. We propose each (p E Φ (i.e. relative frequency of each subgraph in G) as a connectivity feature of graph G to measure text coherence. Frequent large subgraphs. Since we ob</context>
</contexts>
<marker>Sidner, 1983</marker>
<rawString>Candace L. Sidner. 1983. Focusing in the comprehension of definite anaphora. In M. Brady and R.C. Berwick, editors, Computational Models of Discourse, pages 267–330. Cambridge, Mass.: MIT Press. Reprinted in: Grosz, Barbara J. et al. (Eds.) (1986). Readings in Natural Language Processing. Morgan Kaufman: Los Altos, Cal., pp.363-394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Stoddard</author>
</authors>
<title>Text and Texture: Patterns of Cohesion. Ablex,</title>
<date>1991</date>
<location>Norwood, N.J.</location>
<contexts>
<context position="21754" citStr="Stoddard (1991" startWordPosition="3614" endWordPosition="3616">on that many shifts in focus of attention make texts difficult to read. Table 5 shows the correlation between the relative frequency of 4-node subgraphs and readability ratings. First, most subgraphs with less than four edges are negatively correlated with readability, except sg20 and sg24 which are weakly correlated with readability. Few connections between sentences make the text difficult to read. Second, the highest positive and significant correlation of sg12 and the most negatively correlated subgraph sg11 show that different patterns of edges in subgraphs capture readability judgments. Stoddard (1991, p.29) explains this by the ambiguity node phenomenon: “[...] in some cases, there may be more than one logical, possible node for a given cohesive element in a text, in which case, a reader may see the resulting ambiguity but not be able to 6Although, the proposed features can be applied on all kind of presented graphs, we evaluate them (except outdegree) only on projections of the entity graph model. We leave the application to the other graph representations for future work. 7This supports Karamanis et al. (2009) who report that NOCB transitions in the centering model can be used for the s</context>
</contexts>
<marker>Stoddard, 1991</marker>
<rawString>Sally Stoddard. 1991. Text and Texture: Patterns of Cohesion. Ablex, Norwood, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xifeng Yan</author>
<author>Jiawei Han</author>
</authors>
<title>gSpan: Graph-based substructure pattern mining.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Data Mining,</booktitle>
<pages>721--724</pages>
<location>Maebashi City,</location>
<contexts>
<context position="16451" citStr="Yan and Han, 2002" startWordPosition="2715" endWordPosition="2718">r distance j − i to decrease the importance of links that exist between non-adjacent sentences. Discourse relation graph. We use gold PDTB-style discourse relations (Prasad et al., 2008). We filter out EntRel and NoRel relations. Number of components. For counting the number of components in each projection graph, the SageMath3 package is used. This feature is computed on unweighted projections (i.e. PER u ). Frequent subgraphs. Since subgraph mining is an NP-complete problem, different algorithms have been introduced to improve the performance of subgraph mining. We use the gSpan4 algorithm (Yan and Han, 2002) to mine subgraphs of a graph database which contains PER uprojections. An advantage of using efficient subgraph mining algorithms is that we can exhaustively search very large subgraph spaces. A graph with IEI edges, however, potentially has O(2IEI) subgraphs. Having sparse graphs 1Pitler and Nenkova (2008) also remove one file from their experiments. We assume that it is wsj-0382. 2We use Stanford CoreNLP (http://nlp.stanford. edu/software/corenlp.shtml) 3http://sagemath.org/download-linux.html 4We use the Java package: http://www.cs.ucsb. edu/˜xyan/software/gSpan.htm sg1 sg2 sg3 sg4 st su s</context>
</contexts>
<marker>Yan, Han, 2002</marker>
<rawString>Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-based substructure pattern mining. In Proceedings of the International Conference on Data Mining, Maebashi City, Japan, 9–12 December 2002, pages 721–724.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>