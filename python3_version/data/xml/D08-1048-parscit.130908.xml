<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000246">
<title confidence="0.983029">
Automatic induction of FrameNet lexical units
</title>
<author confidence="0.989424">
Marco Pennacchiotti(t), Diego De Cao($), Roberto Basili($), Danilo Croce($), Michael Roth(t)
</author>
<affiliation confidence="0.9699735">
(†) Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.655134">
Saarbr¨ucken, Germany
</address>
<email confidence="0.979754">
{pennacchiotti,mroth}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.994296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999341">
Most attempts to integrate FrameNet in NLP
systems have so far failed because of its lim-
ited coverage. In this paper, we investigate the
applicability of distributional and WordNet-
based models on the task of lexical unit induc-
tion, i.e. the expansion of FrameNet with new
lexical units. Experimental results show that
our distributional and WordNet-based models
achieve good level of accuracy and coverage,
especially when combined.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999668074074074">
Most inference-based NLP tasks require a large
amount of semantic knowledge at the predicate-
argument level. This type of knowledge allows to
identify meaning-preserving transformations, such
as active/passive, verb alternations and nominal-
izations, which are crucial in several linguistic in-
ferences. Recently, the integration of NLP sys-
tems with manually-built resources at the predi-
cate argument-level, such as FrameNet (Baker et
al., 1998) and PropBank (Palmer et al., 2005) has
received growing interest. For example, Shen and
Lapata (2007) show the potential improvement that
FrameNet can bring on the performance of a Ques-
tion Answering (QA) system. Similarly, several
other studies (e.g. (Bar-Haim et al., 2005; Garoufi,
2007)) indicate that frame semantics plays a central
role in Recognizing Textual Entailment (RTE). Un-
fortunately, most attempts to integrate FrameNet or
similar resources in QA and RTE systems have so
far failed, as reviewed respectively in (Shen and La-
pata, 2007) and (Burchardt and Frank, 2006). These
studies indicate limited coverage as the main reason
of insuccess. Indeed, the FrameNet database only
contains 10,000 lexical units (LUs), far less than
the 210,000 entries in WordNet 3.0. Also, frames
are based on more complex information than word
senses, so that their manual development is much
</bodyText>
<listItem confidence="0.327307">
(�) DISP
</listItem>
<affiliation confidence="0.6104945">
University of Roma Tor Vergata
Roma, Italy
</affiliation>
<email confidence="0.74465">
{decao,basili,croce}@info.uniroma2.it
</email>
<bodyText confidence="0.99994336">
more demanding (Burchardt et al., 2006; Subirats
and Petruck, 2003).
Therefore, there is nowadays a pressing need to
adopt learning approaches to extend the coverage
of the FrameNet lexicon by automatically acquiring
new LUs, a task we call LU induction, as recently
proposed at SemEval-2007 (Baker et al., 2007). Un-
fortunately, research in this area is still somehow
limited and fragmentary. The aim of our study is
to pioneer in this field by proposing two unsuper-
vised models for LU induction, one based on dis-
tributional techniques and one using WordNet as a
support; and a combined model which mixes the
two. The goal is to investigate to what extent distri-
butional and WordNet-based models can be used to
induce frame semantic knowledge in order to safely
extend FrameNet, thus limiting the high costs of
manual annotation.
In Section 2 we introduce the LU induction task
and present related work. In Sections 3, 4 and 5 we
present our distributional, WordNet-based and com-
bined models. Then, in Section 6 we report experi-
mental results and comparative evaluations. Finally,
in Section 7 we draw final conclusions and outline
future work.
</bodyText>
<sectionHeader confidence="0.855413" genericHeader="method">
2 Task Definition and Related Work
</sectionHeader>
<bodyText confidence="0.999989461538462">
As defined in (Fillmore, 1985), a frame is a con-
ceptual structure modeling a prototypical situation,
evoked in texts through the occurrence of its lex-
ical units. A lexical unit (LU) is a predicate that
linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-
guments. For example the frame KILLING has lex-
ical units such as assassin, assassinate, blood-bath,
fatal, murderer, kill, suicide that share semantic ar-
guments such as KILLER, INSTRUMENT, CAUSE,
VICTIM. Building on this frame-semantic model,
the Berkeley FrameNet project (Baker et al., 1998)
has been developing a frame-semantic lexicon for
</bodyText>
<page confidence="0.975403">
457
</page>
<note confidence="0.9620725">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457–465,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999902465517241">
the core vocabulary of English since 1997. The
current FrameNet release contains 795 frames and
about 10,000 LUs. Part of FrameNet is also a cor-
pus of 135,000 annotated example sentences from
the British National Corpus (BNC).
LU induction is a fairly new task. Formally,
it can be defined as the task of assigning a
generic lexical unit not yet present in the FrameNet
database (hereafter called unknown LU) to the cor-
rect frame(s). As the number of frames is very
large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then
be mapped to more than one frame (for example
the word tea could map both to FOOD and SO-
CIAL EVENT). Also, even unambiguous words can
be assigned to more than one frame – e.g. child maps
to both KINSHIP and PEOPLE BY AGE.
LU induction is relevant to many NLP tasks, such
as the semi-automatic creation of new FrameNets,
and semantic role labelling. LU induction has been
integrated at SemEval-2007 as part of the Frame Se-
mantic Structure Extraction shared task (Baker et
al., 2007), where systems are requested to assign
the correct frame to a given LU, even when the
LU is not yet present in FrameNet. Johansson and
Nugues (2007) approach the task as a machine learn-
ing problem: a Support Vector Machine trained on
existing LUs is applied to assign unknown LUs to
the correct frame, using features derived from the
WordNet hierarchy. Tested on the FrameNet gold
standard, the method achieves an accuracy of 0.78,
at the cost of a low coverage of 31% (i.e. many LUs
are not assigned). Johansson and Nugues (2007)
also experiment with a simple model based on stan-
dard WordNet similarity measures (Pedersen et al.,
2004), achieving lower performance. Burchardt and
colleagues (2005) present Detour, a rule-based sys-
tem using words in a WordNet relation with the un-
known LU to find the correct frame. The system
achieves an accuracy of 0.39 and a coverage of 87%.
Unfortunately this algorithm requires the LU to be
previously disambiguated, either by hand or using
contextual information.
In a departure from previous work, our first model
leverages distributional properties to induce LUs, in-
stead of relying on pre-existing lexical resources as
WordNet. This guarantees two main advantages.
First, it can predict a frame for any unknown LU,
while WordNet based approaches can be applied
only to words having a WordNet entry. Second, it
allows to induce LUs in languages for which Word-
Net is not available or has limited coverage. Our
second WordNet-based model uses sense informa-
tion to characterize the frame membership for un-
known LU, by adopting a semantic similarity mea-
sure which is sensitive to all the known LUs of a
frame.
</bodyText>
<sectionHeader confidence="0.997881" genericHeader="method">
3 Distributional model
</sectionHeader>
<bodyText confidence="0.999983428571429">
The basic idea behind the distributional approach is
to induce new LUs by modelling existing frames and
unknown LUs in a semantic space, where they are
represented as distributional co-occurrence vectors
computed over a corpus.
Semantic spaces are widely used in NLP for rep-
resenting the meaning of words or other lexical en-
tities. They have been successfully applied in sev-
eral tasks, such as information retrieval (Salton et al.,
1975) and harvesting thesauri (Lin, 1998). The intu-
ition is that the meaning of a word can be described
by the set of textual contexts in which it appears
(Distributional Hypothesis (Harris, 1964)), and that
words with similar vectors are semantically related.
In our setting, the goal is to find a semantic space
model able to capture the notion of frame – i.e. the
property of “being characteristic of a frame”. In
such a model, an unknown LU is induced by first
computing the similarity between its vector and the
vectors of the existing frames, and then assigning the
LU to the frame with the highest similarity.
</bodyText>
<subsectionHeader confidence="0.999942">
3.1 Assigning unknown LUs to frames
</subsectionHeader>
<bodyText confidence="0.9971994">
In our model, a LU l is represented by a vector l
whose dimensions represent the set of contexts C
of the semantic space. The value of each dimen-
sion is given by the co-occurrence value of the LU
with a contextual feature c E C, computed over a
large corpus using an association measure. We ex-
periment with two different association measures:
normalized frequency and pointwise mutual infor-
mation. We approximate these measures by using
Maximum Likelihood Estimation, as follows:
</bodyText>
<page confidence="0.935624">
458
</page>
<equation confidence="0.979367555555556">
F (l, c) =MLE |l, c|
|∗, ∗ |(1)
MI(l,c) =MLE |l, c||∗, ∗|
|∗, c||l, ∗|
where |l, c |denotes the co-occurrence counts
of the pair (l, c) in the corpus, |∗, c |=
/DlEL |l,c|, |l, ∗ |= EcEC |l, c |and finally |∗, ∗ |=
ElEL,cEC |l, c|.
~
</equation>
<bodyText confidence="0.928106571428571">
A frame f is modeled by a vector f, representing
the distributional profile of the frame in the seman-
tic space. We here assume that a frame can be fully
described by the set of its lexical units F. We imple-
~
ment this intuition by computing f as the weighted
centroid of the set F, as follows:
</bodyText>
<equation confidence="0.9973885">
f~ = � wlf ∗ l~ (2)
lEF
</equation>
<bodyText confidence="0.999908">
where wlf is a weighting factor, accounting for
the relevance of a given lexical unit with respect to
the frame, estimated as:
</bodyText>
<equation confidence="0.997843">
wlf = |l ||l |(3)
lEF
</equation>
<bodyText confidence="0.965472808510638">
where |l |denotes the counts of l in the corpus.
From a more cognitive perspective, the vector f~ rep-
resents the prototypical lexical unit of the frame.
Given the set of all frames N and an unknown lex-
ical unit ul, we assign ul to the frame fmaxul which
is distributionally most similar – i.e. we intuitively
map an unknown lexical unit to the frame whose
prototypical lexical unit f~ has the highest similarity
~
with
a high similarity value simD would be then given
both to assassinate/kill and to assassin/kill. We ex-
plore three spaces which seem to capture the above
property well:
Word-based space: Contexts are words appear-
ing in a n-window of the lexical unit. Such spaces
model a generic notion of semantic relatedness.
Two LUs close in the space are likely to be re-
lated by some type of generic semantic relation,
either paradigmatic (e.g. synonymy, hyperonymy,
antonymy) or syntagmatic (e.g. meronymy, concep-
tual and phrasal association).1
Syntax-based space: Contexts are syntactic re-
lations (e.g. X-VSubj-man where X is the LU), as
described in (Pad´o, 2007). These spaces are good
at modeling semantic similarity. Two LUs close in
the space are likely to be in a paradigmatic relation,
i.e. to be close in a is-a hierarchy (Budanitsky and
Hirst, 2006; Lin, 1998; Pad´o, 2007). Indeed, as con-
texts are syntactic relations, targets with the same
part of speech are much closer than targets of differ-
ent types.
Mixed space: In a combination of the two above
spaces, contexts are words connected to the LU by a
dependency path of at most length n. Unlike word-
based spaces, contexts are selected in a more princi-
pled way: only syntactically related words are con-
texts, while other (possibly noisy) material is filtered
out. Unlike syntax-based spaces, the context c does
not explicitly state the type of syntactic relation with
the LU: this usually allows to capture both paradig-
matic and syntagmatic relations.
ul:
fmaxul = argmaxfEArsimD( ~ul, ~f) (4) 4 WordNet-based model
In our model, we used the traditional cosine simi-
larity:
simc,,(ul, f) =
</bodyText>
<subsectionHeader confidence="0.999873">
3.2 Choosing the space
</subsectionHeader>
<bodyText confidence="0.9999421">
Different types of contexts C define spaces with dif-
ferent semantic properties. We are here looking for
a space able to capture the properties which charac-
terise a frame. The most relevant of these properties
is that LUs in the same frame tend to be either co-
occurring or substitutional words (e.g. assassin/kill
or assassinate/kill) – i.e. they are either in paradig-
matic and syntagmatic relation. In an ideal space,
In a departure from previous work, our WordNet-
based model does not rely on standard WordNet sim-
ilarity measures (Pedersen et al., 2004), as these
measures can only be applied to pairs of words,
while we here need to capture the meaning of whole
frames, which typically consist of larger sets of LUs.
Our intuition is that senses able to evoke a frame can
be detected via WordNet, by jointly considering the
WordNet synsets activated by all LUs of the frame.
We implement this intuition in a weakly-
supervised model, where each frame f is repre-
sented as a set of specific sub-graphs of the WordNet
</bodyText>
<equation confidence="0.8833774">
1See (Pad´o, 2007; Sahlgren, 2006) for an in depth analysis.
(5)
|~ul |∗  |~f|
f
~ul ·
</equation>
<page confidence="0.978727">
459
</page>
<bodyText confidence="0.999814333333333">
hyponymy hierarchy. As different parts of speech
have different WordNet hierarchies, we build a sub-
graph for each of them: Snf for nouns, Sf for verbs
and Sf for adjectives.2 These sub-graphs repre-
sent the lexical semantic properties characterizing
the frame. An unknown LU ul of a given part of
speech is assigned to the frame whose correspond-
ing sub-graph is semantically most similar to one of
the senses of ul:
</bodyText>
<equation confidence="0.97664">
fmaxul = argmaxfEArsimWN(ul, f) (6)
</equation>
<bodyText confidence="0.999684735294118">
where simWN is a WordNet-based similarity
measure. In the following subsections we will de-
scribe how we build sub-graphs and model the sim-
ilarity measure for the different part of speech.
Figure 1 reports an excerpt of the noun sub-
graph for the frame PEOPLE BY AGE, cover-
ing the suitable senses of its nominal LUs
{adult, baby, boy, kid, youngster, youth}. The
relevant senses (e.g. sense 1 of youth out of the 6
potential ones) are generally selected, as they share
the most specific generalizations in WordNet with
the other words.
Nouns. To compute similarity for nouns we adopt
conceptual density (cd) (Agirre and Rigau, 1996),
a semantic similarity model previously applied to
word sense disambiguation tasks.
Given a frame f and its set of nominal lexical
units Fn, the nominal subgraph Snf is built as fol-
lows. All senses of all words in Fn are activated
in WordNet. All hypernyms Hnf of these senses are
then retrieved. Every synset Q E Hnf is given a cd
score, representing the density of the WordNet sub-
hierarchy rooted at Q in representing the set of nouns
Fn. The intuition behind this model is that the larger
the number of LUs in Fn that are generalized by Q is,
the better it captures the lexical semantics intended
by the frame f. Broader generalizations are penal-
ized as they give rise to bigger hierarchies, not well
correlated with the full set of targets Fn.
To build the final sub-graph Snf , we apply the
greedy algorithm proposed by Basili and colleagues
(2004). It first computes the set of WordNet synsets
that generalize at least two LUs in Fn, and then se-
lects the subset of most dense ones Snf C Hnf that
</bodyText>
<footnote confidence="0.8998675">
2Our WordNet model does not cover the limited number of
LUs which are not nouns, verbs or adjectives.
</footnote>
<bodyText confidence="0.91246605">
cover Fn. If a LU has no common hypernym with
other members of Fn, it is not represented in Snf , and
its similarity is set to 0 . Snf disambiguates words in
Fn as only the lexical senses with at least one hyper-
nym in Snf are considered.
Figure 1 shows the nominal sub-graph automati-
cally derived using conceptual density for the frame
PEOPLE BY AGE. The word boy is successfully dis-
ambiguated, as its only hypernym in the sub-graph
refers to its third sense (a male human offspring)
which correctly maps to the given frame. Notice
that this model departs from the first sense heuris-
tics largely successful in word sense disambigua-
tion: most frames in fact are characterized by non
predominant senses. The only questionable disam-
biguation is for the word adult: the wrong sense
(adult mammal) is selected. However, even in these
cases, the cd values are very low (about 10−4), so
that they do not impact much on the quality of the
resulting inference.
</bodyText>
<figureCaption confidence="0.961718333333333">
Figure 1: The noun sub-graph for the frame PEO-
PLE BY AGE as evoked by a subset of the words. Sense
numbers #n refers to WordNet 2.0.
</figureCaption>
<bodyText confidence="0.9962">
Using this model, LU induction is performed as
follows. Given an unknown lexical unit ul, for each
frame f E N we first build the sub-graph Snf from
the set Fn U {ul}. We then compute simWN(f, ul)
as the maximal cd of any synset Q E Snf that gener-
alizes one of the lexical senses of ul. In the example
baby would receive a score of 0.117 according to its
first sense in WordNet 2.0 (“baby,babe,infant”). In
a final step, we assign the LU to the most similar
frame, according to Eq. 6
Verbs and Adjectives. As the conceptual density
algorithm can be used only for nouns, we apply dif-
ferent similarity measures for verbs and adjectives.
</bodyText>
<page confidence="0.995016">
460
</page>
<bodyText confidence="0.87803075">
For verbs we exploit the co-hyponymy relation:
the sub-graph Sf is given by all hyponyms of all
verbs F„ in the frame f. Similarity simWN(f, ul)
is computed as follows:
</bodyText>
<equation confidence="0.9988764">
1 iff ElK C F such that
|K |&gt; T AND
dl E K, l is a co-hyponym of ul
2 otherwise
(7)
</equation>
<bodyText confidence="0.99777475">
As for adjectives, WordNet does not provide a hy-
ponymy hierarchy. We then compute similarity sim-
ply on the basis of the synonymy relation, as fol-
lows:
</bodyText>
<equation confidence="0.817119">
1 iff Ell E F such that
l is a synonym of ul (8)
2 otherwise
</equation>
<sectionHeader confidence="0.989349" genericHeader="method">
5 Combined model
</sectionHeader>
<bodyText confidence="0.999907846153846">
The methods presented so far use two independent
information sources to induce LUs: distributional
similarity simD and WordNet similarity simWN.
We also build a joint model, leveraging both ap-
proaches: we expect the combination of different
information to raise the overall performance. We
here choose to combine the two approaches using a
simple back-off model, that uses the WordNet-based
model as a default and backs-off to the distributional
one when no frame is proposed by the former. The
intuition is that WordNet should guarantee the high-
est precision in the assignment, while distributional
similarity should recover cases of low coverage.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99797">
In this section we present a comparative evaluation
of our models on the task of inducing LUs, in a
leave-one-out setting over a reference gold standard.
</bodyText>
<subsectionHeader confidence="0.981116">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999213255319149">
Our gold standard is the FrameNet 1.3 database,
containing 795 frames and a set L of 7,522 unique
LUs (in all there are 10,196 LUs possibly assigned
to more than one frame). Given a lexical unit l E L,
we simulate the induction task by executing a leave-
one-out procedure, similarly to Burchardt and col-
leagues (2005). First, we remove l from all its origi-
nal frames. Then, we ask our models to reassign it to
the most similar frame(s) f, according to the simi-
larity measure3. We repeat this procedure for all lex-
ical units. Though our experiment is not completely
realistic (we test over LUs already in FrameNet), it
has the advantage of a reliable gold standard pro-
duced by expert annotators. A second, more re-
alistic, small-scale experiment is described in Sec-
tion 6.2.
We compute accuracy as the fraction of LUs in L
that are correctly re-assigned to the original frame.
Accuracy is computed at different levels k: a LU l is
correctly assigned if its gold standard frame appears
among the best-k frames f ranked by the model us-
ing the sim(l, f) measure. As LUs can have more
than one correct frame, we deem as correct an as-
signment for which at least one of the correct frames
is among the best-k.
We also measure coverage, intended as the per-
centage of LUs that have been assigned to at least
one frame by the model. Notice that when no
sense preference can be found above the threshold E,
the WordNet-based model cannot predict any frame,
thus decreasing coverage.
We present results for the following models and
parametrizations (further parametrizations have re-
vealed comparable performance).
Dist-word : the word-based space described in
Section 3. Contextual features correspond to the
set of the 4,000 most frequent words in the BNC.4
The association measure between LUs and contexts
is the pointwise mutual information. Valid contexts
for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described
in Section 3. Context features are the 10,000 most
frequent syntactic relations in the BNC5. As associ-
ation measure we apply log-likelihood ratio (Dun-
ning, 1993) to normalized frequency. Syntactic rela-
tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec-
</bodyText>
<footnote confidence="0.682115571428571">
3In the distributional model, we recompute the centroids for
each frame f in which the LU appeared, applying Eq. 2 to the
set F − {l}.
4We didn’t use the FrameNet corpus directly, as it is too
small to obtain reliable statistics.
5Specifically, we use the minimum context selection func-
tion and the plain path value function described in Pado (2007).
</footnote>
<equation confidence="0.9795365">
simWN(ul, f) _ I
simWN(ul, f) _ I
</equation>
<page confidence="0.989621">
461
</page>
<bodyText confidence="0.998911555555556">
tion 3. As for the Dist-word model, contextual fea-
tures are 4,000 and pointwise mutual information is
the association measure. The maximal dependency
path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described
in Section 4.
WNet-bsense : this model is computed as WNet-
full but using only the most frequent sense for each
LU as defined in WordNet.
Combined : the combined method presented in
Section 5. Specifically, it uses WNet-full as a default
and Dist-word as back-off.
Baseline-rnd : a baseline model, randomly as-
signing LUs to frames.
Baseline-mostfreq : a model predicting as best-k
frames the most likely ones in FrameNet – i.e. those
containing the highest number of LUs.
</bodyText>
<subsectionHeader confidence="0.998903">
6.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999872166666667">
Table 1 reports accuracy and coverage results for the
different models, considering only 6792 LUs with
frequency higher than 5 in the BNC, and frames
with more than 2 lexical units (to allow better gen-
eralizations in all models). Results show that all our
models largely outperform both baselines, achieving
a good level of accuracy and high coverage. In
particular, accuracy for the best-10 frames is high
enoungh to support tasks such as the semi-automatic
creation of new FrameNets. This claim is supported
by a further task-driven experiment, in which we
asked 3 annotators to assign 60 unknown LUs (from
the Detour system log) to frames, with and without
the support of the Dist-word model’s predictions as
suggestions6. We verified that our model guarantee
an annotation speed-up of 25% – i.e. in average an
annotator saves 25% of annotation time by using
the system’s suggestions.
</bodyText>
<subsectionHeader confidence="0.447945">
Distributional vs. WordNet-based models.
</subsectionHeader>
<bodyText confidence="0.999795833333333">
WordNet-based models are significantly better than
distributional ones, for several reasons. First, distri-
butional models acquire information only from the
contexts in the corpus. As we do not use a FrameNet
annotated corpus, there is no guarantee that the us-
age of a LU in the texts reflects exactly the semantic
</bodyText>
<footnote confidence="0.953363">
6For this purpose, the dataset is evenly split in two parts.
</footnote>
<bodyText confidence="0.999956446808511">
properties of the LU in FrameNet. In the extreme
cases of polysemous LUs, it may happen that the
textual contexts refer to senses which are not ac-
counted for in FrameNet. In our study, we explicitly
ignore the issue of polisemy, which is a notoriously
hard task to solve in semantics spaces (see (Sch¨utze,
1998)), as the occurrences of different word senses
need to be clustered separately. We will approach
the problem in future work. The WordNet-based
model suffers from the problem of polisemy to a
much lesser extent, as all senses are explicitly rep-
resented and separated in WordNet, including those
related to the FrameNet gold standard.
A second issue regards data sparseness. The vec-
torial representation of LUs with few occurrences in
the corpus is likely to be semantically incomplete,
as not enough statistical evidence is available. Par-
ticularly skewed distributions can be found when
some frames are very rarely represented in the cor-
pus. A more in-depth descussion on these two issues
is given later in this section.
Regarding the WordNet-based models, WNet-full
in most cases outperforms WNet-bsense. The first
sense heuristic does not seem to be as effective as
in other tasks, such as Word Sense Disambigua-
tion. Although sense preferences (or predominance)
across two general purpose resources, such as Word-
Net and FrameNet, should be a useful hint, the con-
ceptual density algorithm seems to produce better
distributions (i.e. higher accuracy), especially when
several solutions are considered. Indeed, for many
LUs the first WordNet sense is not the one repre-
sented in the FrameNet database.
As for distributional models, results show that the
Dist-word model performs best. In general, syntac-
tic relations (Dist-syntax model) do not help to cap-
ture frame semantic properties better than a simple
window-based approach. This seems to indicate that
LUs in a same frame are related both by paradig-
matic and syntagmatic relations, in accordance to
the definition given in Section 3.2 – i.e. they are
mostly semantically related, but not similar.
Coverage. Distributional models show a coverage
15% higher than WordNet-based ones. Indeed, as far
as corpus evidence is available (i.e. the unknown LU
appears in the corpus), distributional methods are al-
ways able to predict a frame. WordNet-based mod-
</bodyText>
<page confidence="0.997723">
462
</page>
<table confidence="0.999899888888889">
MODEL B-1 B-2 B-3 B-4 B-5 B-6 B-7 B-8 B-9 B-10 COVERAGE
Dist-word 0.27 0.36 0.42 0.46 0.49 0.51 0.53 0.55 0.56 0.57 95%
Dist-syntax 0.22 0.29 0.34 0.38 0.41 0.44 0.46 0.48 0.50 0.51 95%
Dist-mixed 0.25 0.35 0.40 0.44 0.47 0.49 0.51 0.53 0.54 0.56 95%
WNet-full 0.47 0.59 0.65 0.69 0.72 0.73 0.75 0.76 0.77 0.78 80%
WNet-bsense 0.52 0.61 0.64 0.66 0.67 0.68 0.69 0.69 0.70 0.70 72%
Combined 0.43 0.54 0.60 0.64 0.66 0.68 0.70 0.71 0.72 0.73 95%
Baseline-rnd 0.02 0.03 0.05 0.06 0.08 0.10 0.11 0.12 0.14 0.15
Baseline-mostfreq 0.02 0.05 0.07 0.08 0.10 0.11 0.13 0.14 0.15 0.17
</table>
<tableCaption confidence="0.9164725">
Table 1: Accuracy and coverage of different models on best-k ranking with frequency threshold 5 and frame threshold
2
</tableCaption>
<bodyText confidence="0.999627066666667">
els cannot make predictions in two specific cases.
First, when the LU is not present in WordNet. Sec-
ond, when the function simWN does not has suffi-
cient relational information to find a similar frame.
This second factor is particularly evident for adjec-
tives, as Eq. 8 assigns a frame only when a synonym
of the unknown LU is found. It is then not surpris-
ing that 68% of the missed assignment are indeed
adjectives.
Results for the Combined model suggest that
the integration of distributional and WordNet-based
methods can offer a viable solution to the cover-
age problem, as it achieves an accuracy comparable
to the pure WordNet approaches, while keeping the
coverage high.
</bodyText>
<figureCaption confidence="0.9899925">
Figure 2: Dist-word model accuracy at different LU fre-
quency cuts.
</figureCaption>
<bodyText confidence="0.998752954545455">
Data Sparseness. A major issue when using dis-
tributional approaches is that words with low fre-
quency tend to have a very sparse non-meaningful
representation in the vector space. This highly im-
pacts on the accuracy of the models. To measure
the impact of data sparseness, we computed the ac-
curacy at different frequency cuts – i.e. we exclude
LUs below a given frequency threshold from cen-
troid computation and evaluation. Figure 2 reports
the results for best-10 assignment at different cuts,
for the Dist-word model. As expected, accuracy im-
proves by excluding infrequent LUs. Only at a fre-
quency cut of 200 performance becomes stable, as
statistical evidence is enough for a reliable predic-
tion. Yet, in a real setting the improvement in accu-
racy implies a lower coverage, as the system would
not classify LUs below the threshold. For example,
by discarding LUs occurring less than 200 times in
the corpus, we obtain a +0.12 improvement in accu-
racy, but the coverage decreases to 57%. However,
uncovered LUs are also the most rare ones and their
relevance in an application may be negligible.
</bodyText>
<subsectionHeader confidence="0.660983">
Lexical Semantics, Ambiguity and Plausible As-
</subsectionHeader>
<bodyText confidence="0.999986">
signments. The overall accuracies achieved by
our methods are “pessimistic”, in the sense that they
should be intended as lower-bounds. Indeed, a qual-
itative analysis of erroneous predictions reveals that
in many cases the frame assignments produced by
the models are semantically plausible, even if they
are considered incorrect in the leave-one-out test.
Consider for example the LU guerrilla, assigned in
FrameNet to the frame PEOPLE BY VOCATION. Our
mixed model proposes as two most similar frames
MILITARY and TERRORISM, which could still be
considered plausible assignment. The same holds
for the LU caravan, for which the most similar
frame is VEHICLE, while in FrameNet the LU is as-
signed only to the frame BUILDINGS. These cases
are due to the low FrameNet coverage, i.e LUs are
not fully annotated and they appear only in a subset
of their potential frames. The real accuracy of our
</bodyText>
<page confidence="0.999028">
463
</page>
<bodyText confidence="0.99995180952381">
models is therefore expected to be higher.
To explore the issue, we carried out a qualita-
tive analysis of 5 words (i.e. abandon.v, accuse.v,
body.n, charge.v and partner.n). For each of them,
we randomly picked 60 sentences from the BNC
corpus, and asked two human annotators to assign
to the correct frame the occurrence of the word in
the given sentence. For 2 out of 5 words, no frame
could be found for most of the sentences, suggesting
that the most frequent frames for these words were
missing from FrameNet7. We can then conclude that
100% accuracy cannot be considered as the upper-
bound of our experiment, as word usage in texts is
not well reflected in the FrameNet modelling.
Further experiments. We also tested our models
on a realistic gold-standard set of 24 unknown LUs
extracted from the SemEval-2007 corpus (Baker et
al., 2007). These are words not present in FrameNet
1.3 which have been assigned by human annotators
to an existing frame8. WNet-full achieves an accu-
racy of 0.25 for best-1 and 0.69 for best-10, with a
coverage of 67%. A qualitative analysis showed that
the lower performance wrt to our main experiment is
due to higher ambiguity of the LUs (e.g. we assign
tea to SOCIAL EVENT instead of FOOD).
Comparison to other approaches. We compare
our models to the system presented by Johans-
son and Nugues (2007) and Burchardt and col-
leagues (2005). Johansson and Nugues (2007) eval-
uate their machine learning system using 7,000
unique LUs to train the Support Vector Machine, and
the remaining LUs as test. They measure accuracy at
different coverage levels. At 80% coverage accuracy
is about 0.42, 10 points below our best WordNet-
based system. At 90% coverage, the system shows
an accuracy below 0.10 and is significantly out-
performed by both our distributional and combined
methods. These results confirm that WordNet-based
approaches, while being highly accurate wrt dis-
tributional ones, present strong weaknesses as far
as coverage is concerned. Furthermore, Johansson
and Nugues (2007) show that their machine learn-
</bodyText>
<footnote confidence="0.9873246">
7Note that the need of new frames to account for seman-
tic phenomena in free texts has been also demonstrated by the
SemEval-2007 competition.
8The set does not contain 4 LUs which have no frame in
FrameNet.
</footnote>
<bodyText confidence="0.999977888888889">
ing approach outperforms a simple approach based
on WordNet similarity: thus, our results indirectly
prove that our WordNet-based method is more ef-
fective than the application of the similarity measure
presented in (Pedersen et al., 2004).
We also compare our results to those reported
by Burchardt and colleagues (2005) for Detour.
Though the experimental setting is slightly different
(LU assignment is done at the text-level), they use
the same gold standard and leave-one-out technique,
reporting a best-1 accuracy of 0.38 and a coverage
of 87%. Our WordNet-based models significantly
outperform Detour on best-1 accuracy, at the cost of
lower coverage. Yet,our combined model is signifi-
cantly better both on accuracy (+5%) and coverage
(+8%). Also, in most cases Detour cannot predict
more than one frame (best-1), while our accuracies
can be improved by relaxing to any best-k level.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999616857142857">
In this paper we presented an original approach for
FrameNet LU induction. Results show that mod-
els combining distributional and WordNet informa-
tion offer the most viable solution to model the no-
tion of frame, as they allow to achieve a reasonable
trade-off between accuracy and coverage. We also
showed that in contrast to previous work, simple se-
mantic spaces are more helpful than complex syn-
tactic ones. Results are accurate enough to support
the creation and the development of new FrameNets.
As future work, we will evaluate new types of
spaces (e.g. dimensionality reduction methods) to
improve the generalization capabilities of the space
models. We will also address the data sparseness is-
sue, by testing smoothing techniques to better model
low frequency LUs. Finally, we will implement
the presented models in a complex architecture for
semi-supervised FrameNets development, both for
specializing the existing English FrameNet in spe-
cific domains, and for creating new FrameNets in
other languages.
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98222075">
This work has partly been funded by the German Re-
search Foundation DFG (grant PI 154/9-3). Thanks
to Richard Johansson and Aljoscha Burchardt for
providing the data of their systems.
</bodyText>
<page confidence="0.999404">
464
</page>
<sectionHeader confidence="0.993905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998103">
E. Agirre and G. Rigau. 1996. Word Sense Disam-
biguation using Conceptual Density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99–104, Prague, Czech Republic, June.
Roy Bar-Haim, Idan Szpektor, and Oren Glickman.
2005. Definition and Analysis of Intermediate Entail-
ment Levels. In ACL-05 Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A
semantic similarity measure for unsupervised semantic
disambiguation. In Proceedings of LREC-04, Lisbon,
Portugal.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13–47.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of PASCAL RTE2 Workshop.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet Detour to FrameNet. In Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8 of Computer Studies in Language
and Speech. Peter Lang, Frankfurt/Main.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC, Genova,
Italy.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 18(1):61–74.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222–254.
K. Garoufi. 2007. Towards a better understanding of
applied textual entailment: Annotation and evaluation
of the rte-2 dataset. M.Sc. thesis, saarland university.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Phi-
losophy of Linguistics, New York. Oxford University
Press.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, Tartu, Estonia, May 24.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Sebastian Pad´o. 2007. Cross-Lingual Annotation Projec-
tion Models for Role-Semantic Information. Saarland
University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1).
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18:613620.
Hinrich Sch¨utze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97–124.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of EMNLP-CoNLL, pages 12–21, Prague.
C. Subirats and M. Petruck. 2003. Surprise! Spanish
FrameNet! In Proceedings of the Workshop on Frame
Semantics at the XVII. International Congress of Lin-
guists, Prague.
</reference>
<page confidence="0.999592">
465
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943389">
<title confidence="0.999784">Automatic induction of FrameNet lexical units</title>
<author confidence="0.998549">Diego De_Roberto Danilo Michael</author>
<affiliation confidence="0.999131">Computational Linguistics Saarland University</affiliation>
<address confidence="0.979572">Saarbr¨ucken, Germany</address>
<abstract confidence="0.996916818181818">Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetmodels on the task of unit induci.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>G Rigau</author>
</authors>
<title>Word Sense Disambiguation using Conceptual Density.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="13445" citStr="Agirre and Rigau, 1996" startWordPosition="2230" endWordPosition="2233">imWN is a WordNet-based similarity measure. In the following subsections we will describe how we build sub-graphs and model the similarity measure for the different part of speech. Figure 1 reports an excerpt of the noun subgraph for the frame PEOPLE BY AGE, covering the suitable senses of its nominal LUs {adult, baby, boy, kid, youngster, youth}. The relevant senses (e.g. sense 1 of youth out of the 6 potential ones) are generally selected, as they share the most specific generalizations in WordNet with the other words. Nouns. To compute similarity for nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996), a semantic similarity model previously applied to word sense disambiguation tasks. Given a frame f and its set of nominal lexical units Fn, the nominal subgraph Snf is built as follows. All senses of all words in Fn are activated in WordNet. All hypernyms Hnf of these senses are then retrieved. Every synset Q E Hnf is given a cd score, representing the density of the WordNet subhierarchy rooted at Q in representing the set of nouns Fn. The intuition behind this model is that the larger the number of LUs in Fn that are generalized by Q is, the better it captures the lexical semantics intended</context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>E. Agirre and G. Rigau. 1996. Word Sense Disambiguation using Conceptual Density. In Proceedings of COLING-96, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1147" citStr="Baker et al., 1998" startWordPosition="154" endWordPosition="157"> lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies in</context>
<context position="3903" citStr="Baker et al., 1998" startWordPosition="592" endWordPosition="595">tion and Related Work As defined in (Fillmore, 1985), a frame is a conceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lexical units. A lexical unit (LU) is a predicate that linguistically expresses the situation of the frame. Lexical units of the same frame share semantic arguments. For example the frame KILLING has lexical units such as assassin, assassinate, blood-bath, fatal, murderer, kill, suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM. Building on this frame-semantic model, the Berkeley FrameNet project (Baker et al., 1998) has been developing a frame-semantic lexicon for 457 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457–465, Honolulu, October 2008.c�2008 Association for Computational Linguistics the core vocabulary of English since 1997. The current FrameNet release contains 795 frames and about 10,000 LUs. Part of FrameNet is also a corpus of 135,000 annotated example sentences from the British National Corpus (BNC). LU induction is a fairly new task. Formally, it can be defined as the task of assigning a generic lexical unit not yet present in the FrameNet d</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 19: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>99--104</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2438" citStr="Baker et al., 2007" startWordPosition="352" endWordPosition="355">meNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual development is much (�) DISP University of Roma Tor Vergata Roma, Italy {decao,basili,croce}@info.uniroma2.it more demanding (Burchardt et al., 2006; Subirats and Petruck, 2003). Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed at SemEval-2007 (Baker et al., 2007). Unfortunately, research in this area is still somehow limited and fragmentary. The aim of our study is to pioneer in this field by proposing two unsupervised models for LU induction, one based on distributional techniques and one using WordNet as a support; and a combined model which mixes the two. The goal is to investigate to what extent distributional and WordNet-based models can be used to induce frame semantic knowledge in order to safely extend FrameNet, thus limiting the high costs of manual annotation. In Section 2 we introduce the LU induction task and present related work. In Secti</context>
<context position="5238" citStr="Baker et al., 2007" startWordPosition="812" endWordPosition="815"> task is intuitively hard to solve. A further complexity regards multiple assignments. Lexical units are sometimes ambiguous and can then be mapped to more than one frame (for example the word tea could map both to FOOD and SOCIAL EVENT). Also, even unambiguous words can be assigned to more than one frame – e.g. child maps to both KINSHIP and PEOPLE BY AGE. LU induction is relevant to many NLP tasks, such as the semi-automatic creation of new FrameNets, and semantic role labelling. LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al., 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Johansson and Nugues (2007) approach the task as a machine learning problem: a Support Vector Machine trained on existing LUs is applied to assign unknown LUs to the correct frame, using features derived from the WordNet hierarchy. Tested on the FrameNet gold standard, the method achieves an accuracy of 0.78, at the cost of a low coverage of 31% (i.e. many LUs are not assigned). Johansson and Nugues (2007) also experiment with a simple model based on standard WordNet simila</context>
<context position="28934" citStr="Baker et al., 2007" startWordPosition="4868" endWordPosition="4871">from the BNC corpus, and asked two human annotators to assign to the correct frame the occurrence of the word in the given sentence. For 2 out of 5 words, no frame could be found for most of the sentences, suggesting that the most frequent frames for these words were missing from FrameNet7. We can then conclude that 100% accuracy cannot be considered as the upperbound of our experiment, as word usage in texts is not well reflected in the FrameNet modelling. Further experiments. We also tested our models on a realistic gold-standard set of 24 unknown LUs extracted from the SemEval-2007 corpus (Baker et al., 2007). These are words not present in FrameNet 1.3 which have been assigned by human annotators to an existing frame8. WNet-full achieves an accuracy of 0.25 for best-1 and 0.69 for best-10, with a coverage of 67%. A qualitative analysis showed that the lower performance wrt to our main experiment is due to higher ambiguity of the LUs (e.g. we assign tea to SOCIAL EVENT instead of FOOD). Comparison to other approaches. We compare our models to the system presented by Johansson and Nugues (2007) and Burchardt and colleagues (2005). Johansson and Nugues (2007) evaluate their machine learning system u</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 99–104, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Idan Szpektor</author>
<author>Oren Glickman</author>
</authors>
<title>Definition and Analysis of Intermediate Entailment Levels.</title>
<date>2005</date>
<booktitle>In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1422" citStr="Bar-Haim et al., 2005" startWordPosition="197" endWordPosition="200">ment level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual develo</context>
</contexts>
<marker>Bar-Haim, Szpektor, Glickman, 2005</marker>
<rawString>Roy Bar-Haim, Idan Szpektor, and Oren Glickman. 2005. Definition and Analysis of Intermediate Entailment Levels. In ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M Cammisa</author>
<author>F M Zanzotto</author>
</authors>
<title>A semantic similarity measure for unsupervised semantic disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-04,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Basili, Cammisa, Zanzotto, 2004</marker>
<rawString>R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A semantic similarity measure for unsupervised semantic disambiguation. In Proceedings of LREC-04, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="10443" citStr="Budanitsky and Hirst, 2006" startWordPosition="1716" endWordPosition="1719"> n-window of the lexical unit. Such spaces model a generic notion of semantic relatedness. Two LUs close in the space are likely to be related by some type of generic semantic relation, either paradigmatic (e.g. synonymy, hyperonymy, antonymy) or syntagmatic (e.g. meronymy, conceptual and phrasal association).1 Syntax-based space: Contexts are syntactic relations (e.g. X-VSubj-man where X is the LU), as described in (Pad´o, 2007). These spaces are good at modeling semantic similarity. Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in a is-a hierarchy (Budanitsky and Hirst, 2006; Lin, 1998; Pad´o, 2007). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. Mixed space: In a combination of the two above spaces, contexts are words connected to the LU by a dependency path of at most length n. Unlike wordbased spaces, contexts are selected in a more principled way: only syntactically related words are contexts, while other (possibly noisy) material is filtered out. Unlike syntax-based spaces, the context c does not explicitly state the type of syntactic relation with the LU: this usually allows</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of semantic distance. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
</authors>
<title>Approximating Textual Entailment with LFG and FrameNet Frames.</title>
<date>2006</date>
<booktitle>In Proceedings of PASCAL RTE2 Workshop.</booktitle>
<contexts>
<context position="1729" citStr="Burchardt and Frank, 2006" startWordPosition="245" endWordPosition="248">level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual development is much (�) DISP University of Roma Tor Vergata Roma, Italy {decao,basili,croce}@info.uniroma2.it more demanding (Burchardt et al., 2006; Subirats and Petruck, 2003). Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatica</context>
</contexts>
<marker>Burchardt, Frank, 2006</marker>
<rawString>Aljoscha Burchardt and Anette Frank. 2006. Approximating Textual Entailment with LFG and FrameNet Frames. In Proceedings of PASCAL RTE2 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
</authors>
<title>A WordNet Detour to FrameNet.</title>
<date>2005</date>
<booktitle>In Sprachtechnologie, mobile Kommunikation und linguistische Resourcen,</booktitle>
<volume>8</volume>
<marker>Burchardt, Erk, Frank, 2005</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005. A WordNet Detour to FrameNet. In Sprachtechnologie, mobile Kommunikation und linguistische Resourcen, volume 8 of Computer Studies in Language and Speech. Peter Lang, Frankfurt/Main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Genova, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of LREC, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="19684" citStr="Dunning, 1993" startWordPosition="3334" endWordPosition="3336">resent results for the following models and parametrizations (further parametrizations have revealed comparable performance). Dist-word : the word-based space described in Section 3. Contextual features correspond to the set of the 4,000 most frequent words in the BNC.4 The association measure between LUs and contexts is the pointwise mutual information. Valid contexts for LUs are fixed to a 20-window. Dist-syntax : the syntax-based space described in Section 3. Context features are the 10,000 most frequent syntactic relations in the BNC5. As association measure we apply log-likelihood ratio (Dunning, 1993) to normalized frequency. Syntactic relations are extracted using the Minipar parser. Dist-mixed : the mixed space described in Sec3In the distributional model, we recompute the centroids for each frame f in which the LU appeared, applying Eq. 2 to the set F − {l}. 4We didn’t use the FrameNet corpus directly, as it is too small to obtain reliable statistics. 5Specifically, we use the minimum context selection function and the plain path value function described in Pado (2007). simWN(ul, f) _ I simWN(ul, f) _ I 461 tion 3. As for the Dist-word model, contextual features are 4,000 and pointwise </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 18(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frames and the semantics of understanding.</title>
<date>1985</date>
<journal>Quaderni di Semantica,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="3336" citStr="Fillmore, 1985" startWordPosition="505" endWordPosition="506">ich mixes the two. The goal is to investigate to what extent distributional and WordNet-based models can be used to induce frame semantic knowledge in order to safely extend FrameNet, thus limiting the high costs of manual annotation. In Section 2 we introduce the LU induction task and present related work. In Sections 3, 4 and 5 we present our distributional, WordNet-based and combined models. Then, in Section 6 we report experimental results and comparative evaluations. Finally, in Section 7 we draw final conclusions and outline future work. 2 Task Definition and Related Work As defined in (Fillmore, 1985), a frame is a conceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lexical units. A lexical unit (LU) is a predicate that linguistically expresses the situation of the frame. Lexical units of the same frame share semantic arguments. For example the frame KILLING has lexical units such as assassin, assassinate, blood-bath, fatal, murderer, kill, suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM. Building on this frame-semantic model, the Berkeley FrameNet project (Baker et al., 1998) has been developing a frame-sema</context>
</contexts>
<marker>Fillmore, 1985</marker>
<rawString>Charles J. Fillmore. 1985. Frames and the semantics of understanding. Quaderni di Semantica, 4(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Garoufi</author>
</authors>
<title>Towards a better understanding of applied textual entailment: Annotation and evaluation of the rte-2 dataset. M.Sc. thesis, saarland university.</title>
<date>2007</date>
<contexts>
<context position="1438" citStr="Garoufi, 2007" startWordPosition="201" endWordPosition="202">f knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual development is much (�</context>
</contexts>
<marker>Garoufi, 2007</marker>
<rawString>K. Garoufi. 2007. Towards a better understanding of applied textual entailment: Annotation and evaluation of the rte-2 dataset. M.Sc. thesis, saarland university.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1964</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<editor>In Jerrold J. Katz and Jerry A. Fodor, editors,</editor>
<publisher>University Press.</publisher>
<location>New York. Oxford</location>
<contexts>
<context position="7538" citStr="Harris, 1964" startWordPosition="1196" endWordPosition="1197">behind the distributional approach is to induce new LUs by modelling existing frames and unknown LUs in a semantic space, where they are represented as distributional co-occurrence vectors computed over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vectors are semantically related. In our setting, the goal is to find a semantic space model able to capture the notion of frame – i.e. the property of “being characteristic of a frame”. In such a model, an unknown LU is induced by first computing the similarity between its vector and the vectors of the existing frames, and then assigning the LU to the frame with the highest similarity. 3.1 Assigning unknown LUs to frames In our model, a LU l is represented by a vector l whose dimensions represent the set of contexts C of the semantic space. The value of each dim</context>
</contexts>
<marker>Harris, 1964</marker>
<rawString>Zellig Harris. 1964. Distributional structure. In Jerrold J. Katz and Jerry A. Fodor, editors, The Philosophy of Linguistics, New York. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Using WordNet to extend FrameNet coverage.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA,</booktitle>
<location>Tartu, Estonia,</location>
<contexts>
<context position="5387" citStr="Johansson and Nugues (2007)" startWordPosition="839" endWordPosition="842">mapped to more than one frame (for example the word tea could map both to FOOD and SOCIAL EVENT). Also, even unambiguous words can be assigned to more than one frame – e.g. child maps to both KINSHIP and PEOPLE BY AGE. LU induction is relevant to many NLP tasks, such as the semi-automatic creation of new FrameNets, and semantic role labelling. LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al., 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Johansson and Nugues (2007) approach the task as a machine learning problem: a Support Vector Machine trained on existing LUs is applied to assign unknown LUs to the correct frame, using features derived from the WordNet hierarchy. Tested on the FrameNet gold standard, the method achieves an accuracy of 0.78, at the cost of a low coverage of 31% (i.e. many LUs are not assigned). Johansson and Nugues (2007) also experiment with a simple model based on standard WordNet similarity measures (Pedersen et al., 2004), achieving lower performance. Burchardt and colleagues (2005) present Detour, a rule-based system using words i</context>
<context position="29428" citStr="Johansson and Nugues (2007)" startWordPosition="4953" endWordPosition="4957">also tested our models on a realistic gold-standard set of 24 unknown LUs extracted from the SemEval-2007 corpus (Baker et al., 2007). These are words not present in FrameNet 1.3 which have been assigned by human annotators to an existing frame8. WNet-full achieves an accuracy of 0.25 for best-1 and 0.69 for best-10, with a coverage of 67%. A qualitative analysis showed that the lower performance wrt to our main experiment is due to higher ambiguity of the LUs (e.g. we assign tea to SOCIAL EVENT instead of FOOD). Comparison to other approaches. We compare our models to the system presented by Johansson and Nugues (2007) and Burchardt and colleagues (2005). Johansson and Nugues (2007) evaluate their machine learning system using 7,000 unique LUs to train the Support Vector Machine, and the remaining LUs as test. They measure accuracy at different coverage levels. At 80% coverage accuracy is about 0.42, 10 points below our best WordNetbased system. At 90% coverage, the system shows an accuracy below 0.10 and is significantly outperformed by both our distributional and combined methods. These results confirm that WordNet-based approaches, while being highly accurate wrt distributional ones, present strong weakn</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Using WordNet to extend FrameNet coverage. In Proceedings of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA, Tartu, Estonia, May 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar word.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7383" citStr="Lin, 1998" startWordPosition="1169" endWordPosition="1170">hip for unknown LU, by adopting a semantic similarity measure which is sensitive to all the known LUs of a frame. 3 Distributional model The basic idea behind the distributional approach is to induce new LUs by modelling existing frames and unknown LUs in a semantic space, where they are represented as distributional co-occurrence vectors computed over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vectors are semantically related. In our setting, the goal is to find a semantic space model able to capture the notion of frame – i.e. the property of “being characteristic of a frame”. In such a model, an unknown LU is induced by first computing the similarity between its vector and the vectors of the existing frames, and then assigning the LU to the frame with the highest similarity. 3.1 Assigning unknown LU</context>
<context position="10454" citStr="Lin, 1998" startWordPosition="1720" endWordPosition="1721">t. Such spaces model a generic notion of semantic relatedness. Two LUs close in the space are likely to be related by some type of generic semantic relation, either paradigmatic (e.g. synonymy, hyperonymy, antonymy) or syntagmatic (e.g. meronymy, conceptual and phrasal association).1 Syntax-based space: Contexts are syntactic relations (e.g. X-VSubj-man where X is the LU), as described in (Pad´o, 2007). These spaces are good at modeling semantic similarity. Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in a is-a hierarchy (Budanitsky and Hirst, 2006; Lin, 1998; Pad´o, 2007). Indeed, as contexts are syntactic relations, targets with the same part of speech are much closer than targets of different types. Mixed space: In a combination of the two above spaces, contexts are words connected to the LU by a dependency path of at most length n. Unlike wordbased spaces, contexts are selected in a more principled way: only syntactically related words are contexts, while other (possibly noisy) material is filtered out. Unlike syntax-based spaces, the context c does not explicitly state the type of syntactic relation with the LU: this usually allows to capture</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar word. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>Cross-Lingual Annotation Projection Models for Role-Semantic Information.</title>
<date>2007</date>
<institution>Saarland University.</institution>
<marker>Pad´o, 2007</marker>
<rawString>Sebastian Pad´o. 2007. Cross-Lingual Annotation Projection Models for Role-Semantic Information. Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1182" citStr="Palmer et al., 2005" startWordPosition="160" endWordPosition="163">ts show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concept.</title>
<date>2004</date>
<booktitle>In Proc. of 5th NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="5875" citStr="Pedersen et al., 2004" startWordPosition="922" endWordPosition="925"> are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. Johansson and Nugues (2007) approach the task as a machine learning problem: a Support Vector Machine trained on existing LUs is applied to assign unknown LUs to the correct frame, using features derived from the WordNet hierarchy. Tested on the FrameNet gold standard, the method achieves an accuracy of 0.78, at the cost of a low coverage of 31% (i.e. many LUs are not assigned). Johansson and Nugues (2007) also experiment with a simple model based on standard WordNet similarity measures (Pedersen et al., 2004), achieving lower performance. Burchardt and colleagues (2005) present Detour, a rule-based system using words in a WordNet relation with the unknown LU to find the correct frame. The system achieves an accuracy of 0.39 and a coverage of 87%. Unfortunately this algorithm requires the LU to be previously disambiguated, either by hand or using contextual information. In a departure from previous work, our first model leverages distributional properties to induce LUs, instead of relying on pre-existing lexical resources as WordNet. This guarantees two main advantages. First, it can predict a fram</context>
<context position="11813" citStr="Pedersen et al., 2004" startWordPosition="1946" endWordPosition="1949">raditional cosine similarity: simc,,(ul, f) = 3.2 Choosing the space Different types of contexts C define spaces with different semantic properties. We are here looking for a space able to capture the properties which characterise a frame. The most relevant of these properties is that LUs in the same frame tend to be either cooccurring or substitutional words (e.g. assassin/kill or assassinate/kill) – i.e. they are either in paradigmatic and syntagmatic relation. In an ideal space, In a departure from previous work, our WordNetbased model does not rely on standard WordNet similarity measures (Pedersen et al., 2004), as these measures can only be applied to pairs of words, while we here need to capture the meaning of whole frames, which typically consist of larger sets of LUs. Our intuition is that senses able to evoke a frame can be detected via WordNet, by jointly considering the WordNet synsets activated by all LUs of the frame. We implement this intuition in a weaklysupervised model, where each frame f is represented as a set of specific sub-graphs of the WordNet 1See (Pad´o, 2007; Sahlgren, 2006) for an in depth analysis. (5) |~ul |∗ |~f| f ~ul · 459 hyponymy hierarchy. As different parts of speech </context>
<context position="30582" citStr="Pedersen et al., 2004" startWordPosition="5136" endWordPosition="5139"> being highly accurate wrt distributional ones, present strong weaknesses as far as coverage is concerned. Furthermore, Johansson and Nugues (2007) show that their machine learn7Note that the need of new frames to account for semantic phenomena in free texts has been also demonstrated by the SemEval-2007 competition. 8The set does not contain 4 LUs which have no frame in FrameNet. ing approach outperforms a simple approach based on WordNet similarity: thus, our results indirectly prove that our WordNet-based method is more effective than the application of the similarity measure presented in (Pedersen et al., 2004). We also compare our results to those reported by Burchardt and colleagues (2005) for Detour. Though the experimental setting is slightly different (LU assignment is done at the text-level), they use the same gold standard and leave-one-out technique, reporting a best-1 accuracy of 0.38 and a coverage of 87%. Our WordNet-based models significantly outperform Detour on best-1 accuracy, at the cost of lower coverage. Yet,our combined model is significantly better both on accuracy (+5%) and coverage (+8%). Also, in most cases Detour cannot predict more than one frame (best-1), while our accuraci</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concept. In Proc. of 5th NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<institution>Department of Linguistics, Stockholm University.</institution>
<contexts>
<context position="12308" citStr="Sahlgren, 2006" startWordPosition="2036" endWordPosition="2037">from previous work, our WordNetbased model does not rely on standard WordNet similarity measures (Pedersen et al., 2004), as these measures can only be applied to pairs of words, while we here need to capture the meaning of whole frames, which typically consist of larger sets of LUs. Our intuition is that senses able to evoke a frame can be detected via WordNet, by jointly considering the WordNet synsets activated by all LUs of the frame. We implement this intuition in a weaklysupervised model, where each frame f is represented as a set of specific sub-graphs of the WordNet 1See (Pad´o, 2007; Sahlgren, 2006) for an in depth analysis. (5) |~ul |∗ |~f| f ~ul · 459 hyponymy hierarchy. As different parts of speech have different WordNet hierarchies, we build a subgraph for each of them: Snf for nouns, Sf for verbs and Sf for adjectives.2 These sub-graphs represent the lexical semantic properties characterizing the frame. An unknown LU ul of a given part of speech is assigned to the frame whose corresponding sub-graph is semantically most similar to one of the senses of ul: fmaxul = argmaxfEArsimWN(ul, f) (6) where simWN is a WordNet-based similarity measure. In the following subsections we will descr</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Department of Linguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<pages>18--613620</pages>
<contexts>
<context position="7347" citStr="Salton et al., 1975" startWordPosition="1162" endWordPosition="1165"> information to characterize the frame membership for unknown LU, by adopting a semantic similarity measure which is sensitive to all the known LUs of a frame. 3 Distributional model The basic idea behind the distributional approach is to induce new LUs by modelling existing frames and unknown LUs in a semantic space, where they are represented as distributional co-occurrence vectors computed over a corpus. Semantic spaces are widely used in NLP for representing the meaning of words or other lexical entities. They have been successfully applied in several tasks, such as information retrieval (Salton et al., 1975) and harvesting thesauri (Lin, 1998). The intuition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vectors are semantically related. In our setting, the goal is to find a semantic space model able to capture the notion of frame – i.e. the property of “being characteristic of a frame”. In such a model, an unknown LU is induced by first computing the similarity between its vector and the vectors of the existing frames, and then assigning the LU to the frame with the highest </context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18:613620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>12--21</pages>
<location>Prague.</location>
<contexts>
<context position="1249" citStr="Shen and Lapata (2007)" startWordPosition="170" endWordPosition="173">good level of accuracy and coverage, especially when combined. 1 Introduction Most inference-based NLP tasks require a large amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominalizations, which are crucial in several linguistic inferences. Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) has received growing interest. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. Similarly, several other studies (e.g. (Bar-Haim et al., 2005; Garoufi, 2007)) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE). Unfortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 1</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Subirats</author>
<author>M Petruck</author>
</authors>
<title>Surprise! Spanish FrameNet!</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Frame Semantics at the XVII. International Congress of Linguists,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2193" citStr="Subirats and Petruck, 2003" startWordPosition="313" endWordPosition="316">tegrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively in (Shen and Lapata, 2007) and (Burchardt and Frank, 2006). These studies indicate limited coverage as the main reason of insuccess. Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0. Also, frames are based on more complex information than word senses, so that their manual development is much (�) DISP University of Roma Tor Vergata Roma, Italy {decao,basili,croce}@info.uniroma2.it more demanding (Burchardt et al., 2006; Subirats and Petruck, 2003). Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed at SemEval-2007 (Baker et al., 2007). Unfortunately, research in this area is still somehow limited and fragmentary. The aim of our study is to pioneer in this field by proposing two unsupervised models for LU induction, one based on distributional techniques and one using WordNet as a support; and a combined model which mixes the two. The goal is to investigate to what extent distributio</context>
</contexts>
<marker>Subirats, Petruck, 2003</marker>
<rawString>C. Subirats and M. Petruck. 2003. Surprise! Spanish FrameNet! In Proceedings of the Workshop on Frame Semantics at the XVII. International Congress of Linguists, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>