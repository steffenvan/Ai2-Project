<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.385561">
Book Reviews Communicating with Databases in Natural Language
</title>
<note confidence="0.794264">
COMMUNICATING WITH DATABASES IN NATURAL
LANGUAGE
(Ellis Horwood series in artificial intelligence)
Mark Wallace
</note>
<address confidence="0.249811">
Chichester: Ellis Horwood, 1984, 170 pp.
ISBN 0-85312-639-9; $29.95
</address>
<bodyText confidence="0.998387634615384">
[Also distributed by Halsted Press/John Wiley &amp; Sons as
ISBN 0-470-20105-3]
As a programming language for computational linguistics,
Prolog is a relative newcomer. Mark Wallace, however,
demonstrates very clearly in this timely book the value of
this important tool, especially as it relates to the building
of natural language front ends and interfaces to database
systems. He takes a very practical approach which should
appeal to anyone who has had to contend with the diffi-
culties of designing and implementing natural language
interfaces.
Wallace begins by providing some background on
natural language interfaces. He surveys most of the
conceptual issues, but generously intersperses concrete
references to major research papers and projects. Some
might find his survey too shallow and broad in certain
respects, but I personally found his treatment fair and
complete.
In subsequent chapters, Wallace introduces a formal
query language, called D&amp;Qs, based on referring phrases
(Descriptions) and qualifying phrases (Qualifiers). He
then uses this formalism as a representational vehicle in
the development and Prolog implementation of a natural
language interface, called QPROC. D&amp;Qs is based on
predicate calculus, suitably restricted to provide an
adequate relational query language. Queries in D&amp;Qs
can either be cast into Prolog (as in his &amp;quot;pilot&amp;quot; version)
or converted by Prolog to an underlying query language.
In the Prolog version, each simple qualifier is handled
through facts, each relation maps into a predicate, and
each tuple of the relation ends up as a Prolog clause for
that predicate.
Although the parser is treated in a domain-indepen-
dent fashion, semantics adopts a fairly conventional
relation and attribute style, with verbs, of course, playing
the major roles. Some major issues of semantics are
clearly identified for the reader, including ambiguity,
several matters involving reference and qualification, and
ways to handle the verb to be.
The reader should not, however, view this book as
something it does not claim to be — namely, a book that
provides a thorough and adequate introduction to the
areas of database systems, natural language understand-
ing, or Prolog (although the latter is discussed at some
length in an appendix). Such readers will be disap-
pointed. A moderate level of competence in these areas is
certainly assumed. The interested reader should find the
extensive bibliography also very useful.
I recommend this book to those who feel competent
with Prolog, have a basic understanding of relational
database systems, and can reason through a modest
amount of predicate calculus.
</bodyText>
<author confidence="0.62581">
Stan C. Kwasny
</author>
<affiliation confidence="0.766595666666667">
Department of Computer Science
Indiana University
Bloomington, IN 47405
</affiliation>
<sectionHeader confidence="0.9020465" genericHeader="method">
THE MENTAL REPRESENTATION OF GRAMMATICAL
RELATIONS
</sectionHeader>
<subsectionHeader confidence="0.674860333333333">
(MIT Press series on cognitive theory and mental repre-
sentation)
Joan Bresnan, Editor
</subsectionHeader>
<bodyText confidence="0.592486">
Cambridge: The MIT Press, 1983, 1E+874 pp.
ISBN 0-262-02158-7, $35.00
</bodyText>
<sectionHeader confidence="0.851965" genericHeader="method">
LEXICAL-FUNCTIONAL GRAMMAR
</sectionHeader>
<subsectionHeader confidence="0.775175">
(Trends in linguistics, Studies and monographs 21)
George M. Horn
</subsectionHeader>
<bodyText confidence="0.982765111111111">
Berlin: Mouton Publishers, 1983, 394 pp.
ISBN 90-279-3169-0
One of the curiosities of practical linguistics has long
been the chasm between linguistics, as practiced by, for
example, computational linguists, and linguistic theory.
Lexical-functional grammar deserves some attention
from the practical linguist if only because it contains an
explicit attempt to bridge this chasm.
Lexical-functional grammar is firmly planted in
Chomskyan tradition. It builds on all the accepted
conclusions of that theory through X-bar theory. Bres-
nan began lexical-functional grammar in a 1978 publica-
tion, and she has continued it since then in a number of
publications. The Mental Representation of Grammatical
Relations brings together this work and the work of a
number of collaborators into a synthesis that includes
many interesting innovations. Lexical-Functional Gram-
mar is an individual work that is not &amp;quot;orthodox&amp;quot;, in the
sense that it deviates from Bresnan&apos;s own theory in some
non-trivial ways. In this review, I will concentrate on
Bresnan&apos;s theory and discuss Horn&apos;s contributions only in
passing.
I will begin by trying to describe lexical-functional
grammar for the outsider. It will be necessary to squeeze
a lot into a few sentences, but the reader needs to under-
stand some of the machinery (for instance, the striking
up-arrow and down-arrow notation) to understand
anything about the theory.
Grammar is divided into several modules that function
independently of one another to process an utterance out
of the mind into speech, or in the other direction.
Theoreticians like to ignore the production of sentences
and move directly to a module, having no generally
Computational Linguistics, Volume 12, Numbers 2, April-June 1986 127
Book Reviews The Mental Representation of Grammatical Relations
agreed-upon name, which generates all possible constitu-
ent structures by exercising a context-free phrase struc-
ture grammar (or an equivalent such as a recursive
transition network). This leads to a string of lexical cate-
gories. Then a lexical module takes the category string
as input and fills each category with a choice from the
lexicon. This leads to a string of (abstract) morphemes.
Then a phonological module takes the morpheme string
and converts it into an utterance. Various schools of
thought have inserted transformational modules between
the lexical and phonological modules or even between
phrase structure and lexicon.
This model of speech has been criticized for leaving no
place for meaning. Chomsky has defended it by insisting
that it is a model of grammar and not a model of human
thought; there is good reason to believe that he thinks
that it is a tool, perhaps the only tool, for examining
thought scientifically. But even government-and-binding
theory has never really addressed this problem. The
generative semanticists identified semantics with the
phrase structure module; Chomsky explicitly rejected this
idea and, probably, there are, nowadays, no actual gener-
ative semanticists left. Lexical-functional grammar is
another attempt, quite different in spirit, to address the
problem of meaning.
In lexical-functional grammar, the lexical module is
expanded to produce a second output — a functional
structure — as well as the constituent structure that it
passes to the phonology. The functional structure is the
input to a semantic module that determines the meaning.
The process of producing an utterance remains shadowy,
but it is very clear how understanding must be carried
out. In order to drive the lexical functional processing,
the phrase structure is enriched with a set of functional
statements that parallel the category statements.
For example, the initial phrase-structure rule of
English is expanded into:
</bodyText>
<equation confidence="0.931389">
S NP VP
(f subj)=4 f=l
</equation>
<bodyText confidence="0.991548863013699">
In this notation, the f is a variable to be filled with the
identifier of the node being expanded (here S) and the
is a variable to be filled with the identifier of the new
node (here NP or VP). An interesting effect of this
particular rule is the equation S = VP, so that the VP is
identified with the sentence and its parts become
sentence parts. I presume that VP is retained in the anal-
ysis because it reappears elsewhere in the phrase struc-
ture, but it clearly has no separate part to play in the
meaning.
Hence as an utterance leaves the phrase structure it
has a conventional category string and a set of equations
like these:
(fl subj) = f2; fl = f3
The lexical module adds more of these equations on the
basis of the lexical items it inserts into the category
string. For example:
(f2 pred) = girl
The entire set of equations is a functional description,
and it must be solved to provide a functional structure.
The word &amp;quot;function&amp;quot; is not being used lightly here. It
is the intent of the theory that all the unknowns, the vari-
ous fs, are indeed functions in the technical sense of that
word. That is, they are sets of ordered pairs of argument
and value such that each argument occurs no more than
once. The description has been solved when each
unknown function has been worked out and the value
corresponding to each argument determined. In general,
this is not too hard if the sentence is well-formed. And a
failure in the solution indicates that the sentence is not
well-formed. Grammaticality is automatic.
What makes the functional structure interesting is that
a value can be another function. For example, the entire
sentence might be a function with four arguments, subj,
obj, tense, pred, and the value of the argument subj might
be a three-place function with arguments spec, num, pred,
where the three values are a, singular, girl. This has
advantages; in a reflexive sentence the same function can
be the value for both of the arguments subj and obj. In
more complex sentences the interrelationship can be
quite complicated.
The claim made by lexical-functional grammar is that
functional structure is an adequate representation of
mental processes. This is a strong claim that will need a
significant amount of testing before it can be accepted.
Since a number of persons appear to already believe that
thought is, in fact, based on a mixture of predicates and
their arguments, this is not an outrageous claim. Lexi-
cal-functional grammar can support a clearly formulated
version of predicate and argument logic. It can also
support considerably more complicated structures unless,
as all of our writers are careful to do, one argument of
every function is a distinguished &amp;quot;predicate&amp;quot;. This is not
however required by any other part of lexical-functional
grammar, and represents a separate claim.
Lexical-functional grammar has no transformations.
This means that it must face the problems of anaphorical
behavior that government and binding was developed to
handle. Some of this is relatively easy because relation-
ships that are distant in terms of phonological strings can
easily turn up as immediate after the functional
description is solved. But neither Bresnan nor Horn can
solve all the problems in this way, and they have to fall
back onto special devices. The reader need not be
concerned about specific devices, because they are sure
to be changed in each treatment of the subject. Some
transformations may yet be re-introduced.
Another difficulty which is almost trivial but matters
because it impacts on the name of the theory is that
&amp;quot;function&amp;quot; is probably a misnomer. One situation Bres-
nan answers with a special device is the component, like
an English temporal adjunct, which can occur an arbi-
trary number of times. The device used is to allow a
</bodyText>
<page confidence="0.924659">
128 Computational Linguistics, Volume 12, Numbers 2, April-June 1986
</page>
<note confidence="0.495787">
Book Reviews Books Received
</note>
<bodyText confidence="0.999917646341464">
value to be a set of functions rather than just one. This is
a rather transparent trick to avoid saying that &amp;quot;adjunct&amp;quot;
occurs several times as an argument and the set of
ordered pairs is not, in fact, a function. Either we have
to do with something we might call a &amp;quot;near-function&amp;quot;, or
we have to reformulate the well-formedness rules.
Both of the books being reviewed are mainly devoted
to proving that this framework does not fall apart when
presented with certain standard problems. Horn&apos;s book
is less rigorous and closer to conventional linguistic theo-
ry. The main difference in the two theories seems to be
that Horn admits function predicates (that is, values of
the argument &amp;quot;predicate&amp;quot;) that are themselves functions
rather than atomic logical predicates. This is interesting
from the mental representation point of view and
deserves more discussion from that point of view than it
gets in either book.
Bresnan has collected a number of very competent
co-workers, most notably Ronald M. Kaplan, and Mental
Representation is a collection of different contributions
with a rather thin thread of continuity. Bresnan is the
author or co-author of six of the thirteen chapters. The
material is not restricted to English, and there are chap-
ters on French, Russian, Icelandic, and Malayalam. All
of the work is intended to prove applicability of the theo-
ry, rather than present the results of complete implemen-
tations.
Part III of Mental Representation is entitled &amp;quot;Cognitive
Processing of Grammatical Representations&amp;quot;; it contains
three chapters. &amp;quot;A Theory of the Acquisition of Lexical
Interpretive Grammars&amp;quot; by Steven Pinker discusses
acquisition by children. This reviewer admits to no long-
er being able to understand articles in this increasingly
specialized sub-field of linguistics. &amp;quot;A Competence-
Based Theory of Syntactic Closure&amp;quot; by Marilyn Ford,
Bresnan, and Kaplan is, to a computationally-oriented
linguist, the high point of the book. It contains a theory,
supported by experimental evidence, of how real people
analyze real utterances; and it ends up with a perfectly
feasible plan for computerization (in terms of Kaplan&apos;s
(1981) General Syntactic Processor) of lexical-functional
grammars. &amp;quot;Sentence Planning Units: Implications for
the Speaker&apos;s Representation of Meaningful Relaticths
Underlying Sentences&amp;quot; by Ford discusses production of
utterances and presents some valuable experimental data.
It is not possible, of course, in a review to do anything
like justice to the wealth of detail in either of these
books. For example, Horn systematically compares his
English analysis with a parallel analysis of Polish. In fact,
all of the authors wrestle with the problem of free word
order and seem to achieve victories. In a chapter on
&amp;quot;Control and Complementation&amp;quot;, Bresnan tackles word
order and even worse problems with some success. The
language-specific chapters contain much of value that
cannot be discussed here; and so on.
Lexical-functional grammar appears to be flourishing,
but it has not swept away all the other schools of
thought. Still other alternatives can be visualized. For
example, we might question the asymmetry between
phonology and semantics, and change the model to place
them on a common basis. We might use phrase structure
and lexicon to generate something called &amp;quot;deep
structure&amp;quot; and two sets of descriptions, one on each side,
to be solved.
Horn tries to formulate the construction of functional
structures with something very like transformations rath-
er than the arrow formalism. It is true, in an empty way,
that every mapping of this kind is a transformation. But
the process of solving the description wreaks havoc with
conventional simplicity metrics. It is not, of course, fash-
ionable any longer to mention simplicity metrics, but a
glance at the argumentation procedures of any theoretical
linguist will disclose that they are alive and well, if never
acknowledged. Theoretical linguists choose, continually,
between models on the basis of what mathematicians call
elegance. This is unobjectionable because it is simply
another version of Occam&apos;s Razor; but mathematicians
live in a world where anything, notationally speaking,
goes. Linguists have been, generally speaking, captive to
their notational devices. It may well be that the most
valuable contribution of lexical-functional grammar is to
introduce the implicit function theorem into linguistics.
</bodyText>
<subsectionHeader confidence="0.189177">
David Kleinecke
</subsectionHeader>
<bodyText confidence="0.362718">
Santa Barbara, California
</bodyText>
<sectionHeader confidence="0.899434" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.986134">
Bresnan, J. 1978 A realistic transformational grammar. In Halle, M.,
Bresnan, J., and Miller, G.A., Eds., Linguistic Theory and Psycholog-
ical Reality. Cambridge, Massachusetts: The MIT Press.
Kaplan, R.M. 1981 Active chart parsing. Xerox Palo Alto Research
Center.
</reference>
<subsectionHeader confidence="0.482385">
BOOKS RECEIVED
</subsectionHeader>
<bodyText confidence="0.937153">
The following books have been received.
</bodyText>
<subsubsectionHeader confidence="0.792078">
Data Bases in the Humanities and Social Sciences 2
</subsubsectionHeader>
<bodyText confidence="0.934944">
(Papers from the 1983 International Conference on Data Bases
in the Humanities and Social Sciences)
</bodyText>
<sectionHeader confidence="0.452748" genericHeader="method">
Robert F. Allen
</sectionHeader>
<reference confidence="0.94314875">
Osprey, Florida: Paradigm Press, 1985, 434 pp. [ISBN
0-931351-006 (paper) $41; (cloth) $64]
Language Sound Structure: Studies in Phonology
Presented to Morris Halle by his Teacher and Students
Mark Aronoff and Richard T. Oehrle with Frances
Kelley and Bonnie Stephens Wilker (Editors)
The MIT Press, 1984 [$35.00]
Rationality and Intelligence
Jonathan Baron
Cambridge University Press, 1985, viii-i-299 pp.
[ISBN 0-521-26717-X, $32.50]
Computational Linguistics, Volume 12, Numbers 2, April-June 1986 129
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.182322">
<title confidence="0.90408375">Book Reviews Communicating with Databases in Natural Language COMMUNICATING WITH DATABASES IN NATURAL LANGUAGE (Ellis Horwood series in artificial intelligence)</title>
<author confidence="0.997874">Mark Wallace</author>
<note confidence="0.946683">Chichester: Ellis Horwood, 1984, 170 pp. ISBN 0-85312-639-9; $29.95 [Also distributed by Halsted Press/John Wiley &amp; Sons as</note>
<abstract confidence="0.971032375">ISBN 0-470-20105-3] As a programming language for computational linguistics, Prolog is a relative newcomer. Mark Wallace, however, demonstrates very clearly in this timely book the value of this important tool, especially as it relates to the building of natural language front ends and interfaces to database systems. He takes a very practical approach which should appeal to anyone who has had to contend with the difficulties of designing and implementing natural language interfaces. Wallace begins by providing some background on natural language interfaces. He surveys most of the conceptual issues, but generously intersperses concrete references to major research papers and projects. Some might find his survey too shallow and broad in certain respects, but I personally found his treatment fair and complete. In subsequent chapters, Wallace introduces a formal query language, called D&amp;Qs, based on referring phrases (Descriptions) and qualifying phrases (Qualifiers). He then uses this formalism as a representational vehicle in the development and Prolog implementation of a natural interface, called is based on predicate calculus, suitably restricted to provide an adequate relational query language. Queries in D&amp;Qs can either be cast into Prolog (as in his &amp;quot;pilot&amp;quot; version) or converted by Prolog to an underlying query language. In the Prolog version, each simple qualifier is handled through facts, each relation maps into a predicate, and each tuple of the relation ends up as a Prolog clause for that predicate. Although the parser is treated in a domain-independent fashion, semantics adopts a fairly conventional relation and attribute style, with verbs, of course, playing the major roles. Some major issues of semantics are clearly identified for the reader, including ambiguity, several matters involving reference and qualification, and to handle the verb be. The reader should not, however, view this book as something it does not claim to be — namely, a book that</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>A realistic transformational grammar. In</title>
<date>1978</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<marker>Bresnan, 1978</marker>
<rawString>Bresnan, J. 1978 A realistic transformational grammar. In Halle, M., Bresnan, J., and Miller, G.A., Eds., Linguistic Theory and Psychological Reality. Cambridge, Massachusetts: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>Active chart parsing. Xerox Palo Alto Research Center.</title>
<date>1981</date>
<marker>Kaplan, 1981</marker>
<rawString>Kaplan, R.M. 1981 Active chart parsing. Xerox Palo Alto Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florida Osprey</author>
</authors>
<date>1985</date>
<pages>434--0</pages>
<publisher>Paradigm Press,</publisher>
<note>(paper) $41; (cloth) $64] Language Sound Structure: Studies in Phonology</note>
<marker>Osprey, 1985</marker>
<rawString>Osprey, Florida: Paradigm Press, 1985, 434 pp. [ISBN 0-931351-006 (paper) $41; (cloth) $64] Language Sound Structure: Studies in Phonology</rawString>
</citation>
<citation valid="true">
<title>Presented to Morris Halle by his Teacher and Students Mark Aronoff and Richard T. Oehrle with Frances Kelley and Bonnie Stephens Wilker (Editors) The</title>
<date>1984</date>
<pages>35--00</pages>
<publisher>MIT Press,</publisher>
<marker>1984</marker>
<rawString>Presented to Morris Halle by his Teacher and Students Mark Aronoff and Richard T. Oehrle with Frances Kelley and Bonnie Stephens Wilker (Editors) The MIT Press, 1984 [$35.00]</rawString>
</citation>
<citation valid="false">
<institution>Rationality and Intelligence</institution>
<marker></marker>
<rawString>Rationality and Intelligence</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Baron</author>
</authors>
<date>1985</date>
<pages>299--0</pages>
<publisher>Cambridge University Press,</publisher>
<marker>Baron, 1985</marker>
<rawString>Jonathan Baron Cambridge University Press, 1985, viii-i-299 pp. [ISBN 0-521-26717-X, $32.50]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1986</date>
<volume>12</volume>
<pages>129</pages>
<marker>Linguistics, 1986</marker>
<rawString>Computational Linguistics, Volume 12, Numbers 2, April-June 1986 129</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>