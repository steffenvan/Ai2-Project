<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000728">
<title confidence="0.98417">
Better Word Alignments with Supervised ITG Models
</title>
<author confidence="0.995075">
Aria Haghighi, John Blitzer, John DeNero and Dan Klein
</author>
<affiliation confidence="0.998837">
Computer Science Division, University of California at Berkeley
</affiliation>
<email confidence="0.981232">
faria42,blitzer,denero,kleinl@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997164" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99971235">
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999707315789474">
Inversion transduction grammar (ITG) con-
straints (Wu, 1997) provide coherent structural
constraints on the relationship between a sentence
and its translation. ITG has been extensively
explored in unsupervised statistical word align-
ment (Zhang and Gildea, 2005; Cherry and
Lin, 2007a; Zhang et al., 2008) and machine
translation decoding (Cherry and Lin, 2007b;
Petrov et al., 2008). In this work, we investigate
large-scale, discriminative ITG word alignment.
Past work on discriminative word alignment
has focused on the family of at-most-one-to-one
matchings (Melamed, 2000; Taskar et al., 2005;
Moore et al., 2006). An exception to this is the
work of Cherry and Lin (2006), who discrim-
inatively trained one-to-one ITG models, albeit
with limited feature sets. As they found, ITG
approaches offer several advantages over general
matchings. First, the additional structural con-
straint can result in superior alignments. We con-
firm and extend this result, showing that one-to-
one ITG models can perform as well as, or better
than, general one-to-one matching models, either
using heuristic weights or using rich, learned fea-
tures.
A second advantage of ITG approaches is that
they admit a range of training options. As with
general one-to-one matchings, we can optimize
margin-based objectives. However, unlike with
general matchings, we can also efficiently com-
pute expectations over the set of ITG derivations,
enabling the training of conditional likelihood
models. A major challenge in both cases is that
our training alignments are often not one-to-one
ITG alignments. Under such conditions, directly
training to maximize margin is unstable, and train-
ing to maximize likelihood is ill-defined, since the
target alignment derivations don’t exist in our hy-
pothesis class. We show how to adapt both margin
and likelihood objectives to learn good ITG align-
ers.
In the case of likelihood training, two innova-
tions are presented. The simple, two-rule ITG
grammar exponentially over-counts certain align-
ment structures relative to others. Because of this,
Wu (1997) and Zens and Ney (2003) introduced a
normal form ITG which avoids this over-counting.
We extend this normal form to null productions
and give the first extensive empirical comparison
of simple and normal form ITGs, for posterior de-
coding under our likelihood models. Additionally,
we show how to deal with training instances where
the gold alignments are outside of the hypothesis
class by instead optimizing the likelihood of a set
of minimum-loss alignments.
Perhaps the greatest advantage of ITG mod-
els is that they straightforwardly permit block-
</bodyText>
<page confidence="0.979471">
923
</page>
<note confidence="0.999612">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999909095238095">
structured alignments (i.e. phrases), which gen-
eral matchings cannot efficiently do. The need for
block alignments is especially acute in Chinese-
English data, where oracle AERs drop from 10.2
without blocks to around 1.2 with them. Indeed,
blocks are the primary reason for gold alignments
being outside the space of one-to-one ITG align-
ments. We show that placing linear potential func-
tions on many-to-one blocks can substantially im-
prove performance.
Finally, to scale up our system, we give a com-
bination of pruning techniques that allows us to
sum ITG alignments two orders of magnitude
faster than naive inside-outside parsing.
All in all, our discriminatively trained, block
ITG models produce alignments which exhibit
the best AER on the NIST 2002 Chinese-English
alignment data set. Furthermore, they result in
a 1.1 BLEU-point improvement over GIZA++
alignments in an end-to-end Hiero (Chiang, 2007)
machine translation system.
</bodyText>
<sectionHeader confidence="0.918789" genericHeader="method">
2 Alignment Families
</sectionHeader>
<bodyText confidence="0.99951175">
In order to structurally restrict attention to rea-
sonable alignments, word alignment models must
constrain the set of alignments considered. In this
section, we discuss and compare alignment fami-
lies used to train our discriminative models.
Initially, as in Taskar et al. (2005) and Moore
et al. (2006), we assume the score a of a potential
alignment a) decomposes as
</bodyText>
<equation confidence="0.993905">
�s(a) = �sij + �siE + sEj (1)
(i,j)Ea i�a j�a
</equation>
<bodyText confidence="0.999991">
where sij are word-to-word potentials and siE and
sEj represent English null and foreign null poten-
tials, respectively.
We evaluate our proposed alignments (a)
against hand-annotated alignments, which are
marked with sure (s) and possible (p) alignments.
The alignment error rate (AER) is given by,
</bodyText>
<equation confidence="0.962305">
AER(a, s, p) = 1 − |a n s |+ |a n p|
|a |+ |s|
</equation>
<subsectionHeader confidence="0.935766">
2.1 1-to-1 Matchings
</subsectionHeader>
<bodyText confidence="0.9995315">
The class of at most 1-to-1 alignment match-
ings, A1-1, has been considered in several works
(Melamed, 2000; Taskar et al., 2005; Moore et al.,
2006). The alignment that maximizes a set of po-
tentials factored as in Equation (1) can be found
in O(n3) time using a bipartite matching algo-
rithm (Kuhn, 1955).1 On the other hand, summing
over A1-1 is #P-hard (Valiant, 1979).
Initially, we consider heuristic alignment poten-
tials given by Dice coefficients
</bodyText>
<equation confidence="0.9588465">
Dice(e, f) =
Ce+Cf
</equation>
<bodyText confidence="0.999724">
where Cef is the joint count of words (e, f) ap-
pearing in aligned sentence pairs, and Ce and Cf
are monolingual unigram counts.
We extracted such counts from 1.1 million
French-English aligned sentence pairs of Hansards
data (see Section 6.1). For each sentence pair in
the Hansards test set, we predicted the alignment
from A1-1 which maximized the sum of Dice po-
tentials. This yielded 30.6 AER.
</bodyText>
<subsectionHeader confidence="0.99265">
2.2 Inversion Transduction Grammar
</subsectionHeader>
<bodyText confidence="0.99389124137931">
Wu (1997)’s inversion transduction grammar
(ITG) is a synchronous grammar formalism in
which derivations of sentence pairs correspond to
alignments. In its original formulation, there is a
single non-terminal X spanning a bitext cell with
an English and foreign span. There are three rule
types: Terminal unary productions X —* (e, f),
where e and f are an aligned English and for-
eign word pair (possibly with one being null);
normal binary rules X _* X(L)X(R), where the
English and foreign spans are constructed from
the children as (X(L)X(R), X(L)X(R)i; and in-
verted binary rules X --* X(L)X(R), where the
foreign span inverts the order of the children
(X (L)X(R), X(R)X(L)i.2 In general, we will call
a bitext cell a normal cell if it was constructed with
a normal rule and inverted if constructed with an
inverted rule.
Each ITG derivation yields some alignment.
The set of such ITG alignments, AITG, are a strict
subset of A1-1 (Wu, 1997). Thus, we will view
ITG as a constraint on A1-1 which we will ar-
gue is generally beneficial. The maximum scor-
ing alignment from AITG can be found in O(n6)
time with synchronous CFG parsing; in practice,
we can make ITG parsing efficient using a variety
of pruning techniques. One computational advan-
tage of AITG over A1-1 alignments is that sum-
mation over AITG is tractable. The corresponding
</bodyText>
<footnote confidence="0.9791762">
1We shall use n throughout to refer to the maximum of
foreign and English sentence lengths.
2The superscripts on non-terminals are added only to in-
dicate correspondence of child symbols.
2Cef
</footnote>
<page confidence="0.986859">
924
</page>
<figure confidence="0.960081857142857">
印 印
尼 尼
Q会 Q会
议长 议长
出庭 出庭
受审 受审
(a) Max-Matching Alignment (b) Block ITG Alignment
</figure>
<figureCaption confidence="0.94635">
Figure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively. The 1-1
matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is rendered
as two words in Chinese or that in court is rendered as a single word in Chinese.
</figureCaption>
<bodyText confidence="0.998485">
dynamic program allows us to utilize likelihood-
based objectives for learning alignment models
(see Section 4).
Using the same heuristic Dice potentials on
the Hansards test set, the maximal scoring align-
ment from AITG yields 28.4 AER—2.4 better
than A1-1 —indicating that ITG can be beneficial
as a constraint on heuristic alignments.
</bodyText>
<subsectionHeader confidence="0.999357">
2.3 Block ITG
</subsectionHeader>
<bodyText confidence="0.999860391304348">
An important alignment pattern disallowed by
A1-1 is the many-to-one alignment block. While
not prevalent in our hand-aligned French Hansards
dataset, blocks occur frequently in our hand-
aligned Chinese-English NIST data. Figure 1
contains an example. Extending A1-1 to include
blocks is problematic, because finding a maximal
1-1 matching over phrases is NP-hard (DeNero
and Klein, 2008).
With ITG, it is relatively easy to allow contigu-
ous many-to-one alignment blocks without added
complexity.3 This is accomplished by adding ad-
ditional unary terminal productions aligning a for-
eign phrase to a single English terminal or vice
versa. We will use BITG to refer to this block
ITG variant and ABITG to refer to the alignment
family, which is neither contained in nor contains
A1-1. For this alignment family, we expand the
alignment potential decomposition in Equation (1)
to incorporate block potentials sef and sef which
represent English and foreign many-to-one align-
ment blocks, respectively.
One way to evaluate alignment families is to
</bodyText>
<footnote confidence="0.561809">
3In our experiments we limited the block size to 4.
</footnote>
<bodyText confidence="0.999834666666667">
consider their oracle AER. In the 2002 NIST
Chinese-English hand-aligned data (see Sec-
tion 6.2), we constructed oracle alignment poten-
tials as follows: sij is set to +1 if (i, j) is a sure
or possible alignment in the hand-aligned data, -
1 otherwise. All null potentials (si, and s�j) are
set to 0. A max-matching under these potentials is
generally a minimal loss alignment in the family.
The oracle AER computed in this was is 10.1 for
A1-1 and 10.2 for AITG. The ABITG alignment
family has an oracle AER of 1.2. These basic ex-
periments show that AITG outperforms A1-1 for
heuristic alignments, and ABITG provide a much
closer fit to true Chinese-English alignments than
A1-1.
</bodyText>
<sectionHeader confidence="0.995379" genericHeader="method">
3 Margin-Based Training
</sectionHeader>
<bodyText confidence="0.9997585">
In this and the next section, we discuss learning
alignment potentials. As input, we have a training
set D = (x1, a∗1), ... , (x, a∗�) of hand-aligned
data, where x refers to a sentence pair. We will as-
sume the score of a alignment is given as a linear
function of a feature vector φ(x, a). We will fur-
ther assume the feature representation of an align-
ment, φ(x, a) decomposes as in Equation (1),
</bodyText>
<equation confidence="0.9994935">
E φij(x) + � φi�(x) + � φo(x)
(i,j)∈a i�a j �a
</equation>
<bodyText confidence="0.996905666666667">
In the framework of loss-augmented margin
learning, we seek a w such that w · φ(x, a∗) is
larger than w · φ(x, a) + L(a, a∗) for all a in an
alignment family, where L(a, a∗) is the loss be-
tween a proposed alignment a and the gold align-
ment a∗. As in Taskar et al. (2005), we utilize a
</bodyText>
<page confidence="0.992056">
925
</page>
<bodyText confidence="0.9998175">
loss that decomposes across alignments. Specif-
ically, for each alignment cell (i, j) which is not
a possible alignment in a*, we incur a loss of 1
when azo =6 a*zo; note that if (i, j) is a possible
alignment, our loss is indifferent to its presence in
the proposal alignment.
A simple loss-augmented learning pro-
cedure is the margin infused relaxed algo-
rithm (MIRA) (Crammer et al., 2006). MIRA
is an online procedure, where at each time step
</bodyText>
<equation confidence="0.839539333333333">
t + 1, we update our weights as follows:
wt+1 = argminw||w − wt||2 (2)
2
</equation>
<bodyText confidence="0.943918545454545">
s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*)
where a� = arg max
aEA
In our data sets, many a* are not in A1-1 (and
thus not in AITG), implying the minimum in-
family loss must exceed 0. Since MIRA oper-
ates in an online fashion, this can cause severe
stability problems. On the Hansards data, the
simple averaging technique described by Collins
(2002) yields a reasonable model. On the Chinese
NIST data, however, where almost no alignment
is in A1-1, the update rule from Equation (2) is
completely unstable, and even the averaged model
does not yield high-quality results.
We instead use a variant of MIRA similar to
Chiang et al. (2008). First, rather than update
towards the hand-labeled alignment a*, we up-
date towards an alignment which achieves mini-
mal loss within the family.4 We call this best-
in-class alignment a*�. Second, we perform loss-
augmented inference to obtain a. This yields the
modified QP,
</bodyText>
<equation confidence="0.971504333333333">
wt+1 = argminw||w − wt||2 (3)
2
s.t. w · O(x, a*�) ≥ w · O(x, a) + L(a, a*�)
</equation>
<bodyText confidence="0.934929166666667">
where a� = arg max
aEA wt · O(x, a) + AL(a, a*�)
By setting A = 0, we recover the MIRA update
from Equation (2). As A grows, we increase our
preference that a� have high loss (relative to a*�)
rather than high model score. With this change,
MIRA is stable, but still performs suboptimally.
The reason is that initially the score for all align-
ments is low, so we are biased toward only using
very high loss alignments in our constraint. This
slows learning and prevents us from finding a use-
ful weight vector. Instead, in all the experiments
</bodyText>
<footnote confidence="0.785282">
4There might be several alignments which achieve this
minimal loss; we choose arbitrarily among them.
</footnote>
<bodyText confidence="0.842525">
we report here, we begin with A = 0 and slowly
increase it to A = 0.5.
</bodyText>
<sectionHeader confidence="0.988364" genericHeader="method">
4 Likelihood Objective
</sectionHeader>
<bodyText confidence="0.9969895">
An alternative to margin-based training is a likeli-
hood objective, which learns a conditional align-
ment distribution Pw(a|x) parametrized as fol-
lows,
</bodyText>
<equation confidence="0.9787245">
log Pw(a|x)=w·O(x,a)−log � exp(w·O(x,a�))
a�EA
</equation>
<bodyText confidence="0.99565075">
where the log-denominator represents a sum over
the alignment family A. This alignment probabil-
ity only places mass on members of A. The likeli-
hood objective is given by,
</bodyText>
<equation confidence="0.96816">
max
w � log Pw(a*|x)
(x,a*)EA
</equation>
<bodyText confidence="0.9999198">
Optimizing this objective with gradient methods
requires summing over alignments. For AITG and
ABITG, we can efficiently sum over the set of ITG
derivations in 0(n6) time using the inside-outside
algorithm. However, for the ITG grammar pre-
sented in Section 2.2, each alignment has multiple
grammar derivations. In order to correctly sum
over the set of ITG alignments, we need to alter
the grammar to ensure a bijective correspondence
between alignments and derivations.
</bodyText>
<subsectionHeader confidence="0.907495">
4.1 ITG Normal Form
</subsectionHeader>
<bodyText confidence="0.999974227272727">
There are two ways in which ITG derivations dou-
ble count alignments. First, n-ary productions are
not binarized to remove ambiguity; this results in
an exponential number of derivations for diagonal
alignments. This source of overcounting is con-
sidered and fixed by Wu (1997) and Zens and Ney
(2003), which we briefly review here. The result-
ing grammar, which does not handle null align-
ments, consists of a symbol N to represent a bi-
text cell produced by a normal rule and I for a cell
formed by an inverted rule; alignment terminals
can be either N or I. In order to ensure unique
derivations, we stipulate that a N cell can be con-
structed only from a sequence of smaller inverted
cells I. Binarizing the rule N → I2+ introduces
the intermediary symbol N (see Figure 2(a)). Sim-
ilarly for inverse cells, we insist an I cell only be
built by an inverted combination of N cells; bina-
rization of I --* N2+ requires the introduction of
the intermediary symbol I (see Figure 2(b)).
Null productions are also a source of double
counting, as there are many possible orders in
</bodyText>
<equation confidence="0.939094">
wt · O(x, a)
</equation>
<page confidence="0.806972">
926
</page>
<figure confidence="0.968836095238095">
N → IN
N → IN
N—*I
N
I
N
I
I N
N
I
I
N
I
N
I&amp;quot;NI
I &amp;quot; NI
I &amp;quot; N
N → I2+
I --, N2+
(a) Normal Domain Rules (b) Inverted Domain Rules
Nil
</figure>
<figureCaption confidence="0.955169">
Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar
without nulls (presented in Wu (1997) and Zens and Ney (2003)). In (c) and (d), we present a normal form grammar
that accounts for null alignments.
</figureCaption>
<figure confidence="0.964867205128205">
(c) Normal Domain with Null Rules (d) Inverted Domain with Null Rules
Nil — (·,f)N11
Nil — Nio
Noo
Nio — N10(e, )
Nio — Noo
N
N
Ill
Ill
Ioo
I00 N11I
I N11I
I Noo
N00 → I11N
N—,I11N
N → Ioo
Ill — (, f)Ill
Ill — (&apos;, f)�I10
Ill Ilo
Ilo Ilo(e,&apos;)
I10 — I00(e, ���
I10 — I00
I00 — NliN00
Nil — (·,f)*N10
TN10 — N00(e, ·)*
N00 → IliI00
Noo Nio Nio
Nil
I00
Ioo
Iio Iio
In
In
Noo
I
I
Nil
Nil
</figure>
<bodyText confidence="0.99920044">
which to attach null alignments to a bitext cell;
we address this by adapting the grammar to force
a null attachment order. We introduce symbols
N00, N10, and N11 to represent whether a normal
cell has taken no nulls, is accepting foreign nulls,
or is accepting English nulls, respectively. We also
introduce symbols I00, I10, and I11 to represent
inverse cells at analogous stages of taking nulls.
As Figures 2 (c) and (d) illustrate, the directions
in which nulls are attached to normal and inverse
cells differ. The N00 symbol is constructed by
one or more ‘complete’ inverted cells I11 termi-
nated by a no-null I00. By placing I00 in the lower
right hand corner, we allow the larger N00 to un-
ambiguously attach nulls. N00 transitions to the
N10 symbol and accepts any number of (e, ·) En-
glish terminal alignments. Then N10 transitions to
N11 and accepts any number of (·, f) foreign ter-
minal alignments. An analogous set of grammar
rules exists for the inverted case (see Figure 2(d)
for an illustration). Given this normal form, we
can efficiently compute model expectations over
ITG alignments without double counting.5 To our
knowledge, the alteration of the normal form to
accommodate null emissions is novel to this work.
</bodyText>
<footnote confidence="0.941855">
5The complete grammar adds sentinel symbols to the up-
per left and lower right, and the root symbol is constrained to
be a Noo.
</footnote>
<subsectionHeader confidence="0.991579">
4.2 Relaxing the Single Target Assumption
</subsectionHeader>
<bodyText confidence="0.999993090909091">
A crucial obstacle for using the likelihood objec-
tive is that a given a* may not be in the alignment
family. As in our alteration to MIRA (Section 3),
we could replace a* with a minimal loss in-class
alignment a*�. However, in contrast to MIRA, the
likelihood objective will implicitly penalize pro-
posed alignments which have loss equal to a*�. We
opt instead to maximize the probability of the set
of alignments M(a*) which achieve the same op-
timal in-class loss. Concretely, let m* be the min-
imal loss achievable relative to a* in A. Then,
</bodyText>
<equation confidence="0.741376">
M(a*) = {a E A|L(a, a*) = m*}
</equation>
<bodyText confidence="0.983705">
When a* is an ITG alignment (i.e., m* is 0),
M(a*) consists only of alignments which have all
the sure alignments in a*, but may have some sub-
set of the possible alignments in a*. See Figure 3
for a specific example where m* = 1.
Our modified likelihood objective is given by,
</bodyText>
<equation confidence="0.969369333333333">
�max
w
(x,a*)ED
</equation>
<bodyText confidence="0.9996032">
Note that this objective is no longer convex, as it
involves a logarithm of a summation, however we
still utilize gradient-based optimization. Summing
and obtaining feature expectations over M(a*)
can be done efficiently using a constrained variant
</bodyText>
<equation confidence="0.9797555">
log � Pw(a|x)
aEM(a*)
</equation>
<page confidence="0.99459">
927
</page>
<table confidence="0.9996855">
MIRA Likelihood
1-1 ITG ITG-S ITG-N
Features P R AER P R AER P R AER P R AER
Dice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6
+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4
+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1
</table>
<tableCaption confidence="0.992048">
Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows
indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar
(Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.
</tableCaption>
<figure confidence="0.809831666666667">
Gold Alignment a* Target Alignments M(a*)
Se
ne
est
pas
suffisant
</figure>
<figureCaption confidence="0.992404">
Figure 3: Often, the gold alignment a* isn’t in our
alignment family, here ABITG. For the likelihood ob-
jective (Section 4.2), we maximize the probability of
the set M(a*) consisting of alignments ABITG which
achieve minimal loss relative to a*. In this example,
the minimal loss is 1, and we have a choice of remov-
ing either of the sure alignments to the English word
not. We also have the choice of whether to include the
possible alignment, yielding 4 alignments in M(a*).
</figureCaption>
<bodyText confidence="0.998488142857143">
of the inside-outside algorithm where sure align-
ments not present in a* are disallowed, and the
number of missing sure alignments is appended to
the state of the bitext cell.6
One advantage of the likelihood-based objec-
tive is that we can obtain posteriors over individual
alignment cells,
</bodyText>
<equation confidence="0.9591035">
PIV((i,j)|x) = � PIV(a|x)
aEA:(iJ)Ea
</equation>
<bodyText confidence="0.9999594">
We obtain posterior ITG alignments by including
all alignment cells (i, j) such that PIV((i, j)|x) ex-
ceeds a fixed threshold t. Posterior thresholding
allows us to easily trade-off precision and recall in
our alignments by raising or lowering t.
</bodyText>
<sectionHeader confidence="0.994139" genericHeader="method">
5 Dynamic Program Pruning
</sectionHeader>
<bodyText confidence="0.845186333333333">
Both discriminative methods require repeated
model inference: MIRA depends upon loss-
augmented Viterbi parsing, while conditional like-
</bodyText>
<footnote confidence="0.968801666666667">
6Note that alignments that achieve the minimal loss would
not introduce any alignments not either sure or possible, so it
suffices to keep track only of the number of sure recall errors.
</footnote>
<bodyText confidence="0.999806515151515">
lihood uses the inside-outside algorithm for com-
puting cell posteriors. Exhaustive computation
of these quantities requires an O(n6) dynamic
program that is prohibitively slow even on small
supervised training sets. However, most of the
search space can safely be pruned using posterior
predictions from a simpler alignment models. We
use posteriors from two jointly estimated HMM
models to make pruning decisions during ITG in-
ference (Liang et al., 2006). Our first pruning tech-
nique is broadly similar to Cherry and Lin (2007a).
We select high-precision alignment links from the
HMM models: those word pairs that have a pos-
terior greater than 0.9 in either model. Then, we
prune all bitext cells that would invalidate more
than 8 of these high-precision alignments.
Our second pruning technique is to prune all
one-by-one (word-to-word) bitext cells that have a
posterior below 10−4 in both HMM models. Prun-
ing a one-by-one cell also indirectly prunes larger
cells containing it. To take maximal advantage of
this indirect pruning, we avoid explicitly attempt-
ing to build each cell in the dynamic program. In-
stead, we track bounds on the spans for which we
have successfully built ITG cells, and we only iter-
ate over larger spans that fall within those bounds.
The details of a similar bounding approach appear
in DeNero et al. (2009).
In all, pruning reduces MIRA iteration time
from 175 to 5 minutes on the NIST Chinese-
English dataset with negligible performance loss.
Likelihood training time is reduced by nearly two
orders of magnitude.
</bodyText>
<sectionHeader confidence="0.929894" genericHeader="method">
6 Alignment Quality Experiments
</sectionHeader>
<bodyText confidence="0.998992166666667">
We present results which measure the quality of
our models on two hand-aligned data sets. Our
first is the English-French Hansards data set from
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003). Here we use the same 337/100
train/test split of the labeled data as Taskar et al.
</bodyText>
<page confidence="0.9924">
928
</page>
<table confidence="0.999780333333333">
MIRA Likelihood
1-1 ITG BITG BITG-S BITG-N
Features P R AER P R AER P R AER P R AER P R AER
Dice, dist, 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1
blcks, dict, lex
+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4
</table>
<tableCaption confidence="0.94009625">
Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment
family. The first row represents our best model without external alignment models and the second row includes
features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N
uses the normal form grammar (Section 4.1).
</tableCaption>
<bodyText confidence="0.999470571428572">
(2005); we compute external features from the
same unlabeled data, 1.1 million sentence pairs.
Our second is the Chinese-English hand-aligned
portion of the 2002 NIST MT evaluation set. This
dataset has 491 sentences, which we split into a
training set of 150 and a test set of 191. When we
trained external Chinese models, we used the same
unlabeled data set as DeNero and Klein (2007), in-
cluding the bilingual dictionary.
For likelihood based models, we set the L2 reg-
ularization parameter, U2, to 100 and the thresh-
old for posterior decoding to 0.33. We report re-
sults using the simple ITG grammar (ITG-S, Sec-
tion 2.2) where summing over derivations dou-
ble counts alignments, as well as the normal form
ITG grammar (ITG-N,Section 4.1) which does
not double count. We ran our annealed loss-
augmented MIRA for 15 iterations, beginning
with A at 0 and increasing it linearly to 0.5. We
compute Viterbi alignments using the averaged
weight vector from this procedure.
</bodyText>
<subsectionHeader confidence="0.998046">
6.1 French Hansards Results
</subsectionHeader>
<bodyText confidence="0.999889611111111">
The French Hansards data are well-studied data
sets for discriminative word alignment (Taskar et
al., 2005; Cherry and Lin, 2006; Lacoste-Julien
et al., 2006). For this data set, it is not clear
that improving alignment error rate beyond that of
GIZA++ is useful for translation (Ganchev et al.,
2008). Table 1 illustrates results for the Hansards
data set. The first row uses dice and the same dis-
tance features as Taskar et al. (2005). The first
two rows repeat the experiments of Taskar et al.
(2005) and Cherry and Lin (2006), but adding ITG
models that are trained to maximize conditional
likelihood. The last row includes the posterior of
the jointly-trained HMM of Liang et al. (2006)
as a feature. This model alone achieves an AER
of 5.4. No model significantly improves over the
HMM alone, which is consistent with the results
of Taskar et al. (2005).
</bodyText>
<subsectionHeader confidence="0.998289">
6.2 Chinese NIST Results
</subsectionHeader>
<bodyText confidence="0.9999912">
Chinese-English alignment is a much harder task
than French-English alignment. For example, the
HMM aligner achieves an AER of 20.7 when us-
ing the competitive thresholding heuristic of DeN-
ero and Klein (2007). On this data set, our block
ITG models make substantial performance im-
provements over the HMM, and moreover these
results do translate into downstream improve-
ments in BLEU score for the Chinese-English lan-
guage pair. Because of this, we will briefly de-
scribe the features used for these models in de-
tail. For features on one-by-one cells, we con-
sider Dice, the distance features from (Taskar et
al., 2005), dictionary features, and features for the
50 most frequent lexical pairs. We also trained an
HMM aligner as described in DeNero and Klein
(2007) and used the posteriors of this model as fea-
tures. The first two columns of Table 2 illustrate
these features for ITG and one-to-one matchings.
For our block ITG models, we include all of
these features, along with variants designed for
many-to-one blocks. For example, we include the
average Dice of all the cells in a block. In addi-
tion, we also created three new block-specific fea-
tures types. The first type comprises bias features
for each block length. The second type comprises
features computed from N-gram statistics gathered
from a large monolingual corpus. These include
features such as the number of occurrences of the
phrasal (multi-word) side of a many-to-one block,
as well as pointwise mutual information statistics
for the multi-word parts of many-to-one blocks.
These features capture roughly how “coherent” the
multi-word side of a block is.
The final block feature type consists of phrase
shape features. These are designed as follows: For
each word in a potential many-to-one block align-
ment, we map an individual word to X if it is not
one of the 25 most frequent words. Some example
features of this type are,
</bodyText>
<page confidence="0.992354">
929
</page>
<listItem confidence="0.998474">
• English Block: [the X, X], [in X of, X]
• Chinese Block: [ --- X, X] [X A, X]
</listItem>
<bodyText confidence="0.999962">
For English blocks, for example, these features
capture the behavior of phrases such as in spite
of or in front of that are rendered as one word in
Chinese. For Chinese blocks, these features cap-
ture the behavior of phrases containing classifier
phrases like --- ^ or --- %, which are rendered as
English indefinite determiners.
The right-hand three columns in Table 2 present
supervised results on our Chinese English data set
using block features. We note that almost all of
our performance gains (relative to both the HMM
and 1-1 matchings) come from BITG and block
features. The maximum likelihood-trained nor-
mal form ITG model outperforms the HMM, even
without including any features derived from the
unlabeled data. Once we include the posteriors
of the HMM as a feature, the AER decreases to
14.4. The previous best AER result on this data set
is 15.9 from Ayan and Dorr (2006), who trained
stacked neural networks based on GIZA++ align-
ments. Our results are not directly comparable
(they used more labeled data, but did not have the
HMM posteriors as an input feature).
</bodyText>
<subsectionHeader confidence="0.977519">
6.3 End-To-End MT Experiments
</subsectionHeader>
<bodyText confidence="0.999039">
We further evaluated our alignments in an end-to-
end Chinese to English translation task using the
publicly available hierarchical pipeline JosHUa
(Li and Khudanpur, 2008). The pipeline extracts
a Hiero-style synchronous context-free grammar
(Chiang, 2007), employs suffix-array based rule
extraction (Lopez, 2007), and tunes model pa-
rameters with minimum error rate training (Och,
2003). We trained on the FBIS corpus using sen-
tences up to length 40, which includes 2.7 million
English words. We used a 5-gram language model
trained on 126 million words of the Xinhua section
of the English Gigaword corpus, estimated with
SRILM (Stolcke, 2002). We tuned on 300 sen-
tences of the NIST MT04 test set.
Results on the NIST MT05 test set appear in
Table 3. We compared four sets of alignments.
The GIZA++ alignments7 are combined across di-
rections with the grow-diag-final heuristic, which
outperformed the union. The joint HMM align-
ments are generated from competitive posterior
</bodyText>
<footnote confidence="0.885786">
7We used a standard training regimen: 5 iterations of
model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3
iterations of Model 4.
</footnote>
<table confidence="0.999563166666667">
Alignments Translations
Model Prec Rec Rules BLEU
GIZA++ 62 84 1.9M 23.22
Joint HMM 79 77 4.0M 23.05
Viterbi ITG 90 80 3.8M 24.28
Posterior ITG 81 83 4.2M 24.32
</table>
<tableCaption confidence="0.988098333333333">
Table 3: Results on the NIST MT05 Chinese-English
test set show that our ITG alignments yield improve-
ments in translation quality.
</tableCaption>
<bodyText confidence="0.99784225">
thresholding (DeNero and Klein, 2007). The ITG
Viterbi alignments are the Viterbi output of the
ITG model with all features, trained to maximize
log likelihood. The ITG Posterior alignments
result from applying competitive thresholding to
alignment posteriors under the ITG model. Our
supervised ITG model gave a 1.1 BLEU increase
over GIZA++.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985153846154">
This work presented the first large-scale applica-
tion of ITG to discriminative word alignment. We
empirically investigated the performance of con-
ditional likelihood training of ITG word aligners
under simple and normal form grammars. We
showed that through the combination of relaxed
learning objectives, many-to-one block alignment
potential, and efficient pruning, ITG models can
yield state-of-the art word alignments, even when
the underlying gold alignments are highly non-
ITG. Our models yielded the lowest published er-
ror for Chinese-English alignment and an increase
in downstream translation performance.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997458133333333">
Necip Fazil Ayan and Bonnie Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In ACL.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In ACL.
Colin Cherry and Dekang Lin. 2007a. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In NAACL-HLT 2007.
Colin Cherry and Dekang Lin. 2007b. A scalable in-
version transduction grammar for joint phrasal trans-
lation modeling. In SSST Workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
</reference>
<page confidence="0.969492">
930
</page>
<reference confidence="0.999828970149254">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai S. Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In ACL Short Paper
Track.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In NAACL.
Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008.
Better alignments = better translations? In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word alignment via
quadratic assignment. In NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
SSST Workshop at ACL.
Percy Liang, Dan Klein, and Dan Klein. 2006. Align-
ment by agreement. In NAACL-HLT.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT/NAACL
Workshop on Building and Using Parallel Texts.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In ACL-COLING.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Empirical Methods in Nat-
ural Language Processing.
Andreas Stolcke. 2002. Srilm: An extensible language
modeling toolkit. In ICSLP 2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In NAACL-HLT.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189–
201.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Dan Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment.
In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
</reference>
<page confidence="0.998152">
931
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.843253">
<title confidence="0.995754">Better Word Alignments with Supervised ITG Models</title>
<author confidence="0.985262">John Blitzer Haghighi</author>
<author confidence="0.985262">John DeNero Klein</author>
<affiliation confidence="0.99953">Computer Science Division, University of California at Berkeley</affiliation>
<abstract confidence="0.983828333333333">This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28164" citStr="Ayan and Dorr (2006)" startWordPosition="4783" endWordPosition="4786"> or --- %, which are rendered as English indefinite determiners. The right-hand three columns in Table 2 present supervised results on our Chinese English data set using block features. We note that almost all of our performance gains (relative to both the HMM and 1-1 matchings) come from BITG and block features. The maximum likelihood-trained normal form ITG model outperforms the HMM, even without including any features derived from the unlabeled data. Once we include the posteriors of the HMM as a feature, the AER decreases to 14.4. The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1764" citStr="Cherry and Lin (2006)" startWordPosition="254" endWordPosition="257">97) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features. A second advantage of ITG approaches is that they admit a range of training options. As with general one-to-one matchings, we can optimize margin</context>
<context position="24542" citStr="Cherry and Lin, 2006" startWordPosition="4165" endWordPosition="4168">0 and the threshold for posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with A at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achie</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In NAACL-HLT</booktitle>
<contexts>
<context position="1369" citStr="Cherry and Lin, 2007" startWordPosition="192" endWordPosition="195">tences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural const</context>
<context position="21428" citStr="Cherry and Lin (2007" startWordPosition="3638" endWordPosition="3641">her sure or possible, so it suffices to keep track only of the number of sure recall errors. lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models. We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006). Our first pruning technique is broadly similar to Cherry and Lin (2007a). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly attempting to build each cell in the dynamic program. Instead, we track b</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007a. Inversion transduction grammar for joint phrasal translation modeling. In NAACL-HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A scalable inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In SSST Workshop at ACL.</booktitle>
<contexts>
<context position="1369" citStr="Cherry and Lin, 2007" startWordPosition="192" endWordPosition="195">tences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural const</context>
<context position="21428" citStr="Cherry and Lin (2007" startWordPosition="3638" endWordPosition="3641">her sure or possible, so it suffices to keep track only of the number of sure recall errors. lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models. We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006). Our first pruning technique is broadly similar to Cherry and Lin (2007a). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly attempting to build each cell in the dynamic program. Instead, we track b</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007b. A scalable inversion transduction grammar for joint phrasal translation modeling. In SSST Workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12452" citStr="Chiang et al. (2008)" startWordPosition="2056" endWordPosition="2059">a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aEA In our data sets, many a* are not in A1-1 (and thus not in AITG), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1, the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a*, we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a*�. Second, we perform lossaugmented inference to obtain a. This yields the modified QP, wt+1 = argminw||w − wt||2 (3) 2 s.t. w · O(x, a*�) ≥ w · O(x, a) + L(a, a*�) where a� = arg max aEA wt · O(x, a) + AL(a, a*�) By setting A = 0, we recover the MIRA update from Equation (2). As A grows, we increase our preference that a� have high loss (relative to a*�) rather than high model score. With this change, MIRA is st</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="4768" citStr="Chiang, 2007" startWordPosition="724" endWordPosition="725"> outside the space of one-to-one ITG alignments. We show that placing linear potential functions on many-to-one blocks can substantially improve performance. Finally, to scale up our system, we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside-outside parsing. All in all, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set. Furthermore, they result in a 1.1 BLEU-point improvement over GIZA++ alignments in an end-to-end Hiero (Chiang, 2007) machine translation system. 2 Alignment Families In order to structurally restrict attention to reasonable alignments, word alignment models must constrain the set of alignments considered. In this section, we discuss and compare alignment families used to train our discriminative models. Initially, as in Taskar et al. (2005) and Moore et al. (2006), we assume the score a of a potential alignment a) decomposes as �s(a) = �sij + �siE + sEj (1) (i,j)Ea i�a j�a where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively. We evaluate our </context>
<context position="28643" citStr="Chiang, 2007" startWordPosition="4856" endWordPosition="4857">rs of the HMM as a feature, the AER decreases to 14.4. The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the gro</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12164" citStr="Collins (2002)" startWordPosition="2009" endWordPosition="2010"> in the proposal alignment. A simple loss-augmented learning procedure is the margin infused relaxed algorithm (MIRA) (Crammer et al., 2006). MIRA is an online procedure, where at each time step t + 1, we update our weights as follows: wt+1 = argminw||w − wt||2 (2) 2 s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aEA In our data sets, many a* are not in A1-1 (and thus not in AITG), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1, the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a*, we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a*�. Second, we perform lossaugmented inference to obtain a. This yields the modified QP, wt+1 = argminw||w − wt||2 (3) 2 s.t. w ·</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Shai S Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="11690" citStr="Crammer et al., 2006" startWordPosition="1912" endWordPosition="1915">ek a w such that w · φ(x, a∗) is larger than w · φ(x, a) + L(a, a∗) for all a in an alignment family, where L(a, a∗) is the loss between a proposed alignment a and the gold alignment a∗. As in Taskar et al. (2005), we utilize a 925 loss that decomposes across alignments. Specifically, for each alignment cell (i, j) which is not a possible alignment in a*, we incur a loss of 1 when azo =6 a*zo; note that if (i, j) is a possible alignment, our loss is indifferent to its presence in the proposal alignment. A simple loss-augmented learning procedure is the margin infused relaxed algorithm (MIRA) (Crammer et al., 2006). MIRA is an online procedure, where at each time step t + 1, we update our weights as follows: wt+1 = argminw||w − wt||2 (2) 2 s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aEA In our data sets, many a* are not in A1-1 (and thus not in AITG), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1, the update rule from Equa</context>
</contexts>
<marker>Crammer, Dekel, Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Shai S. Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23806" citStr="DeNero and Klein (2007)" startWordPosition="4044" endWordPosition="4047">sents our best model without external alignment models and the second row includes features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N uses the normal form grammar (Section 4.1). (2005); we compute external features from the same unlabeled data, 1.1 million sentence pairs. Our second is the Chinese-English hand-aligned portion of the 2002 NIST MT evaluation set. This dataset has 491 sentences, which we split into a training set of 150 and a test set of 191. When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. For likelihood based models, we set the L2 regularization parameter, U2, to 100 and the threshold for posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with A at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 French Hansards </context>
<context position="25508" citStr="DeNero and Klein (2007)" startWordPosition="4328" endWordPosition="4332">eriments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005). 6.2 Chinese NIST Results Chinese-English alignment is a much harder task than French-English alignment. For example, the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein (2007). On this data set, our block ITG models make substantial performance improvements over the HMM, and moreover these results do translate into downstream improvements in BLEU score for the Chinese-English language pair. Because of this, we will briefly describe the features used for these models in detail. For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs. We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as featu</context>
<context position="29830" citStr="DeNero and Klein, 2007" startWordPosition="5054" endWordPosition="5057">bined across directions with the grow-diag-final heuristic, which outperformed the union. The joint HMM alignments are generated from competitive posterior 7We used a standard training regimen: 5 iterations of model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. Alignments Translations Model Prec Rec Rules BLEU GIZA++ 62 84 1.9M 23.22 Joint HMM 79 77 4.0M 23.05 Viterbi ITG 90 80 3.8M 24.28 Posterior ITG 81 83 4.2M 24.32 Table 3: Results on the NIST MT05 Chinese-English test set show that our ITG alignments yield improvements in translation quality. thresholding (DeNero and Klein, 2007). The ITG Viterbi alignments are the Viterbi output of the ITG model with all features, trained to maximize log likelihood. The ITG Posterior alignments result from applying competitive thresholding to alignment posteriors under the ITG model. Our supervised ITG model gave a 1.1 BLEU increase over GIZA++. 7 Conclusion This work presented the first large-scale application of ITG to discriminative word alignment. We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars. We showed that through the combination of rela</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In ACL Short Paper Track.</booktitle>
<contexts>
<context position="9158" citStr="DeNero and Klein, 2008" startWordPosition="1458" endWordPosition="1461">ing the same heuristic Dice potentials on the Hansards test set, the maximal scoring alignment from AITG yields 28.4 AER—2.4 better than A1-1 —indicating that ITG can be beneficial as a constraint on heuristic alignments. 2.3 Block ITG An important alignment pattern disallowed by A1-1 is the many-to-one alignment block. While not prevalent in our hand-aligned French Hansards dataset, blocks occur frequently in our handaligned Chinese-English NIST data. Figure 1 contains an example. Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). With ITG, it is relatively easy to allow contiguous many-to-one alignment blocks without added complexity.3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa. We will use BITG to refer to this block ITG variant and ABITG to refer to the alignment family, which is neither contained in nor contains A1-1. For this alignment family, we expand the alignment potential decomposition in Equation (1) to incorporate block potentials sef and sef which represent English and foreign many-to-one alignment blocks, respe</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In ACL Short Paper Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="22237" citStr="DeNero et al. (2009)" startWordPosition="3774" endWordPosition="3777">ate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly attempting to build each cell in the dynamic program. Instead, we track bounds on the spans for which we have successfully built ITG cells, and we only iterate over larger spans that fall within those bounds. The details of a similar bounding approach appear in DeNero et al. (2009). In all, pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss. Likelihood training time is reduced by nearly two orders of magnitude. 6 Alignment Quality Experiments We present results which measure the quality of our models on two hand-aligned data sets. Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). Here we use the same 337/100 train/test split of the labeled data as Taskar et al. 928 MIRA Likelihood 1-1 ITG BITG BITG-S BITG-N Features P R AER P R AER P </context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations?</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24715" citStr="Ganchev et al., 2008" startWordPosition="4194" endWordPosition="4197">nts, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with A at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005). 6.2 Chinese NIST Results Chinese-English</context>
</contexts>
<marker>Ganchev, Graca, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008. Better alignments = better translations? In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem. Naval Research Logistic Quarterly.</title>
<date>1955</date>
<contexts>
<context position="5902" citStr="Kuhn, 1955" startWordPosition="919" endWordPosition="920">sent English null and foreign null potentials, respectively. We evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The alignment error rate (AER) is given by, AER(a, s, p) = 1 − |a n s |+ |a n p| |a |+ |s| 2.1 1-to-1 Matchings The class of at most 1-to-1 alignment matchings, A1-1, has been considered in several works (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). The alignment that maximizes a set of potentials factored as in Equation (1) can be found in O(n3) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P-hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients Dice(e, f) = Ce+Cf where Cef is the joint count of words (e, f) appearing in aligned sentence pairs, and Ce and Cf are monolingual unigram counts. We extracted such counts from 1.1 million French-English aligned sentence pairs of Hansards data (see Section 6.1). For each sentence pair in the Hansards test set, we predicted the alignment from A1-1 which maximized the sum of Dice potentials. This yielded 30.6 AER. 2.2 Inversion Transduction Grammar </context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistic Quarterly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="24572" citStr="Lacoste-Julien et al., 2006" startWordPosition="4169" endWordPosition="4172">r posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with A at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model si</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>Simon Lacoste-Julien, Ben Taskar, Dan Klein, and Michael Jordan. 2006. Word alignment via quadratic assignment. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In SSST Workshop at ACL.</booktitle>
<contexts>
<context position="28558" citStr="Li and Khudanpur, 2008" startWordPosition="4844" endWordPosition="4847">en without including any features derived from the unlabeled data. Once we include the posteriors of the HMM as a feature, the AER decreases to 14.4. The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four s</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In SSST Workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="21356" citStr="Liang et al., 2006" startWordPosition="3625" endWordPosition="3628">hat achieve the minimal loss would not introduce any alignments not either sure or possible, so it suffices to keep track only of the number of sure recall errors. lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models. We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006). Our first pruning technique is broadly similar to Cherry and Lin (2007a). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly a</context>
<context position="25105" citStr="Liang et al. (2006)" startWordPosition="4262" endWordPosition="4265">ord alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005). 6.2 Chinese NIST Results Chinese-English alignment is a much harder task than French-English alignment. For example, the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein (2007). On this data set, our block ITG models make substantial performance improvements over the HMM, and moreover these results do translate into downstream improvements in BLEU score for the Chinese-E</context>
</contexts>
<marker>Liang, Klein, Klein, 2006</marker>
<rawString>Percy Liang, Dan Klein, and Dan Klein. 2006. Alignment by agreement. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28701" citStr="Lopez, 2007" startWordPosition="4863" endWordPosition="4864">previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The </context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words. Computational Linguistics.</title>
<date>2000</date>
<contexts>
<context position="1663" citStr="Melamed, 2000" startWordPosition="236" endWordPosition="237">er GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features. A second advantage of ITG approaches is that</context>
<context position="5705" citStr="Melamed, 2000" startWordPosition="883" endWordPosition="884">005) and Moore et al. (2006), we assume the score a of a potential alignment a) decomposes as �s(a) = �sij + �siE + sEj (1) (i,j)Ea i�a j�a where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively. We evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The alignment error rate (AER) is given by, AER(a, s, p) = 1 − |a n s |+ |a n p| |a |+ |s| 2.1 1-to-1 Matchings The class of at most 1-to-1 alignment matchings, A1-1, has been considered in several works (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). The alignment that maximizes a set of potentials factored as in Equation (1) can be found in O(n3) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P-hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients Dice(e, f) = Ce+Cf where Cef is the joint count of words (e, f) appearing in aligned sentence pairs, and Ce and Cf are monolingual unigram counts. We extracted such counts from 1.1 million French-English aligned sentence pairs of Hansards data (see Secti</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In HLT/NAACL Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="22678" citStr="Mihalcea and Pedersen, 2003" startWordPosition="3844" endWordPosition="3848"> which we have successfully built ITG cells, and we only iterate over larger spans that fall within those bounds. The details of a similar bounding approach appear in DeNero et al. (2009). In all, pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss. Likelihood training time is reduced by nearly two orders of magnitude. 6 Alignment Quality Experiments We present results which measure the quality of our models on two hand-aligned data sets. Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). Here we use the same 337/100 train/test split of the labeled data as Taskar et al. 928 MIRA Likelihood 1-1 ITG BITG BITG-S BITG-N Features P R AER P R AER P R AER P R AER P R AER Dice, dist, 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1 blcks, dict, lex +HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4 Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment family. The first row represents our best model without external alignment models and the second row includes features fro</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In HLT/NAACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In ACL-COLING.</booktitle>
<contexts>
<context position="1705" citStr="Moore et al., 2006" startWordPosition="242" endWordPosition="245"> Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features. A second advantage of ITG approaches is that they admit a range of training options. A</context>
<context position="5120" citStr="Moore et al. (2006)" startWordPosition="776" endWordPosition="779">ll, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set. Furthermore, they result in a 1.1 BLEU-point improvement over GIZA++ alignments in an end-to-end Hiero (Chiang, 2007) machine translation system. 2 Alignment Families In order to structurally restrict attention to reasonable alignments, word alignment models must constrain the set of alignments considered. In this section, we discuss and compare alignment families used to train our discriminative models. Initially, as in Taskar et al. (2005) and Moore et al. (2006), we assume the score a of a potential alignment a) decomposes as �s(a) = �sij + �siE + sEj (1) (i,j)Ea i�a j�a where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively. We evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The alignment error rate (AER) is given by, AER(a, s, p) = 1 − |a n s |+ |a n p| |a |+ |s| 2.1 1-to-1 Matchings The class of at most 1-to-1 alignment matchings, A1-1, has been considered in several works (Melamed, 2000; Taskar et al.</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28774" citStr="Och, 2003" startWordPosition="4875" endWordPosition="4876">, who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The joint HMM alignments are generated from competitive posterior 7We used a </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1469" citStr="Petrov et al., 2008" startWordPosition="208" endWordPosition="211">ne block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone I</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm: An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP</booktitle>
<contexts>
<context position="29032" citStr="Stolcke, 2002" startWordPosition="4919" endWordPosition="4920">ments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The joint HMM alignments are generated from competitive posterior 7We used a standard training regimen: 5 iterations of model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. Alignments Translations Model Prec Rec Rules BLEU GIZA++ 62 84 1.9M 23.22 Joint HMM 79 77 4.0M 23.05 Viterbi ITG 90 80 3.8M 24.28 P</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm: An extensible language modeling toolkit. In ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="1684" citStr="Taskar et al., 2005" startWordPosition="238" endWordPosition="241">ments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features. A second advantage of ITG approaches is that they admit a range o</context>
<context position="5096" citStr="Taskar et al. (2005)" startWordPosition="771" endWordPosition="774">outside parsing. All in all, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set. Furthermore, they result in a 1.1 BLEU-point improvement over GIZA++ alignments in an end-to-end Hiero (Chiang, 2007) machine translation system. 2 Alignment Families In order to structurally restrict attention to reasonable alignments, word alignment models must constrain the set of alignments considered. In this section, we discuss and compare alignment families used to train our discriminative models. Initially, as in Taskar et al. (2005) and Moore et al. (2006), we assume the score a of a potential alignment a) decomposes as �s(a) = �sij + �siE + sEj (1) (i,j)Ea i�a j�a where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively. We evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The alignment error rate (AER) is given by, AER(a, s, p) = 1 − |a n s |+ |a n p| |a |+ |s| 2.1 1-to-1 Matchings The class of at most 1-to-1 alignment matchings, A1-1, has been considered in several works (Mela</context>
<context position="11282" citStr="Taskar et al. (2005)" startWordPosition="1839" endWordPosition="1842">aining set D = (x1, a∗1), ... , (x, a∗�) of hand-aligned data, where x refers to a sentence pair. We will assume the score of a alignment is given as a linear function of a feature vector φ(x, a). We will further assume the feature representation of an alignment, φ(x, a) decomposes as in Equation (1), E φij(x) + � φi�(x) + � φo(x) (i,j)∈a i�a j �a In the framework of loss-augmented margin learning, we seek a w such that w · φ(x, a∗) is larger than w · φ(x, a) + L(a, a∗) for all a in an alignment family, where L(a, a∗) is the loss between a proposed alignment a and the gold alignment a∗. As in Taskar et al. (2005), we utilize a 925 loss that decomposes across alignments. Specifically, for each alignment cell (i, j) which is not a possible alignment in a*, we incur a loss of 1 when azo =6 a*zo; note that if (i, j) is a possible alignment, our loss is indifferent to its presence in the proposal alignment. A simple loss-augmented learning procedure is the margin infused relaxed algorithm (MIRA) (Crammer et al., 2006). MIRA is an online procedure, where at each time step t + 1, we update our weights as follows: wt+1 = argminw||w − wt||2 (2) 2 s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aE</context>
<context position="24520" citStr="Taskar et al., 2005" startWordPosition="4161" endWordPosition="4164"> parameter, U2, to 100 and the threshold for posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with A at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. </context>
<context position="25915" citStr="Taskar et al., 2005" startWordPosition="4398" endWordPosition="4401">ults Chinese-English alignment is a much harder task than French-English alignment. For example, the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein (2007). On this data set, our block ITG models make substantial performance improvements over the HMM, and moreover these results do translate into downstream improvements in BLEU score for the Chinese-English language pair. Because of this, we will briefly describe the features used for these models in detail. For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs. We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. The first two columns of Table 2 illustrate these features for ITG and one-to-one matchings. For our block ITG models, we include all of these features, along with variants designed for many-to-one blocks. For example, we include the average Dice of all the cells in a block. In addition, we also created three new block-specific features types. The first type comprises bias features for each block le</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L G Valiant</author>
</authors>
<title>The complexity of computing the permanent.</title>
<date>1979</date>
<journal>Theoretical Computer Science,</journal>
<volume>8</volume>
<pages>201</pages>
<contexts>
<context position="5968" citStr="Valiant, 1979" startWordPosition="930" endWordPosition="931"> evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The alignment error rate (AER) is given by, AER(a, s, p) = 1 − |a n s |+ |a n p| |a |+ |s| 2.1 1-to-1 Matchings The class of at most 1-to-1 alignment matchings, A1-1, has been considered in several works (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). The alignment that maximizes a set of potentials factored as in Equation (1) can be found in O(n3) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P-hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients Dice(e, f) = Ce+Cf where Cef is the joint count of words (e, f) appearing in aligned sentence pairs, and Ce and Cf are monolingual unigram counts. We extracted such counts from 1.1 million French-English aligned sentence pairs of Hansards data (see Section 6.1). For each sentence pair in the Hansards test set, we predicted the alignment from A1-1 which maximized the sum of Dice potentials. This yielded 30.6 AER. 2.2 Inversion Transduction Grammar Wu (1997)’s inversion transduction grammar (ITG) is a synchronous </context>
</contexts>
<marker>Valiant, 1979</marker>
<rawString>L. G. Valiant. 1979. The complexity of computing the permanent. Theoretical Computer Science, 8:189– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<contexts>
<context position="1146" citStr="Wu, 1997" startWordPosition="162" endWordPosition="163">w that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Che</context>
<context position="3151" citStr="Wu (1997)" startWordPosition="468" endWordPosition="469">lihood models. A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit block923 Proceedings of the 47th Annual Meeting of the </context>
<context position="6511" citStr="Wu (1997)" startWordPosition="1019" endWordPosition="1020">.1 On the other hand, summing over A1-1 is #P-hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients Dice(e, f) = Ce+Cf where Cef is the joint count of words (e, f) appearing in aligned sentence pairs, and Ce and Cf are monolingual unigram counts. We extracted such counts from 1.1 million French-English aligned sentence pairs of Hansards data (see Section 6.1). For each sentence pair in the Hansards test set, we predicted the alignment from A1-1 which maximized the sum of Dice potentials. This yielded 30.6 AER. 2.2 Inversion Transduction Grammar Wu (1997)’s inversion transduction grammar (ITG) is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments. In its original formulation, there is a single non-terminal X spanning a bitext cell with an English and foreign span. There are three rule types: Terminal unary productions X —* (e, f), where e and f are an aligned English and foreign word pair (possibly with one being null); normal binary rules X _* X(L)X(R), where the English and foreign spans are constructed from the children as (X(L)X(R), X(L)X(R)i; and inverted binary rules X --* X(L)X(R), where the </context>
<context position="14702" citStr="Wu (1997)" startWordPosition="2441" endWordPosition="2442">in 0(n6) time using the inside-outside algorithm. However, for the ITG grammar presented in Section 2.2, each alignment has multiple grammar derivations. In order to correctly sum over the set of ITG alignments, we need to alter the grammar to ensure a bijective correspondence between alignments and derivations. 4.1 ITG Normal Form There are two ways in which ITG derivations double count alignments. First, n-ary productions are not binarized to remove ambiguity; this results in an exponential number of derivations for diagonal alignments. This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. The resulting grammar, which does not handle null alignments, consists of a symbol N to represent a bitext cell produced by a normal rule and I for a cell formed by an inverted rule; alignment terminals can be either N or I. In order to ensure unique derivations, we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I. Binarizing the rule N → I2+ introduces the intermediary symbol N (see Figure 2(a)). Similarly for inverse cells, we insist an I cell only be built by an inverted combination of N cells</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3175" citStr="Zens and Ney (2003)" startWordPosition="471" endWordPosition="474"> A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit block923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP o</context>
<context position="14726" citStr="Zens and Ney (2003)" startWordPosition="2444" endWordPosition="2447">using the inside-outside algorithm. However, for the ITG grammar presented in Section 2.2, each alignment has multiple grammar derivations. In order to correctly sum over the set of ITG alignments, we need to alter the grammar to ensure a bijective correspondence between alignments and derivations. 4.1 ITG Normal Form There are two ways in which ITG derivations double count alignments. First, n-ary productions are not binarized to remove ambiguity; this results in an exponential number of derivations for diagonal alignments. This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. The resulting grammar, which does not handle null alignments, consists of a symbol N to represent a bitext cell produced by a normal rule and I for a cell formed by an inverted rule; alignment terminals can be either N or I. In order to ensure unique derivations, we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I. Binarizing the rule N → I2+ introduces the intermediary symbol N (see Figure 2(a)). Similarly for inverse cells, we insist an I cell only be built by an inverted combination of N cells; binarization of I --* </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Dan Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1347" citStr="Zhang and Gildea, 2005" startWordPosition="188" endWordPosition="191">er allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the addit</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Dan Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1391" citStr="Zhang et al., 2008" startWordPosition="196" endWordPosition="199">gnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in su</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>