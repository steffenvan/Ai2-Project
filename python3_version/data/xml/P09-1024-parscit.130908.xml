<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9990445">
Automatically Generating Wikipedia Articles:
A Structure-Aware Approach
</title>
<author confidence="0.989562">
Christina Sauper and Regina Barzilay
</author>
<affiliation confidence="0.9974455">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.999384">
{csauper,regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994152631579">
In this paper, we investigate an ap-
proach for creating a comprehensive tex-
tual overview of a subject composed of in-
formation drawn from the Internet. We use
the high-level structure of human-authored
texts to automatically induce a domain-
specific template for the topic structure of
a new overview. The algorithmic innova-
tion of our work is a method to learn topic-
specific extractors for content selection
jointly for the entire template. We aug-
ment the standard perceptron algorithm
with a global integer linear programming
formulation to optimize both local fit of
information into each topic and global co-
herence across the entire overview. The
results of our evaluation confirm the bene-
fits of incorporating structural information
into the content selection process.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999678036363637">
In this paper, we consider the task of automatically
creating a multi-paragraph overview article that
provides a comprehensive summary of a subject of
interest. Examples of such overviews include ac-
tor biographies from IMDB and disease synopses
from Wikipedia. Producing these texts by hand is
a labor-intensive task, especially when relevant in-
formation is scattered throughout a wide range of
Internet sources. Our goal is to automate this pro-
cess. We aim to create an overview of a subject –
e.g., 3-M Syndrome – by intelligently combining
relevant excerpts from across the Internet.
As a starting point, we can employ meth-
ods developed for multi-document summarization.
However, our task poses additional technical chal-
lenges with respect to content planning. Gen-
erating a well-rounded overview article requires
proactive strategies to gather relevant material,
such as searching the Internet. Moreover, the chal-
lenge of maintaining output readability is mag-
nified when creating a longer document that dis-
cusses multiple topics.
In our approach, we explore how the high-
level structure of human-authored documents can
be used to produce well-formed comprehensive
overview articles. We select relevant material for
an article using a domain-specific automatically
generated content template. For example, a tem-
plate for articles about diseases might contain di-
agnosis, causes, symptoms, and treatment. Our
system induces these templates by analyzing pat-
terns in the structure of human-authored docu-
ments in the domain of interest. Then, it produces
a new article by selecting content from the Internet
for each part of this template. An example of our
system’s output1 is shown in Figure 1.
The algorithmic innovation of our work is a
method for learning topic-specific extractors for
content selection jointly across the entire template.
Learning a single topic-specific extractor can be
easily achieved in a standard classification frame-
work. However, the choices for different topics
in a template are mutually dependent; for exam-
ple, in a multi-topic article, there is potential for
redundancy across topics. Simultaneously learn-
ing content selection for all topics enables us to
explicitly model these inter-topic connections.
We formulate this task as a structured classifica-
tion problem. We estimate the parameters of our
model using the perceptron algorithm augmented
with an integer linear programming (ILP) formu-
lation, run over a training set of example articles
in the given domain.
The key features of this structure-aware ap-
proach are twofold:
</bodyText>
<footnote confidence="0.9985545">
1This system output was added to Wikipedia at http://
en.wikipedia.org/wiki/3-M syndrome on June
26, 2008. The page’s history provides examples of changes
performed by human editors to articles created by our system.
</footnote>
<page confidence="0.888168">
208
</page>
<note confidence="0.830282363636364">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 208–216,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
Diagnosis ... No laboratories offering molecular genetic testing for prenatal diagnosis of 3-M syndrome are listed in the
GeneTests Laboratory Directory. However, prenatal testing may be available for families in which the disease-causing mutations
have been identified in an affected family member in a research or clinical laboratory.
Causes Three M syndrome is thought to be inherited as an autosomal recessive genetic trait. Human traits, including the classic
genetic diseases, are the product of the interaction of two genes, one received from the father and one from the mother. In recessive
disorders, the condition does not occur unless an individual inherits the same defective gene for the same trait from each parent... .
Symptoms ... Many of the symptoms and physical features associated with the disorder are apparent at birth (congenital). In
some cases, individuals who carry a single copy of the disease gene (heterozygotes) may exhibit mild symptoms associated with
Three M syndrome.
</note>
<figureCaption confidence="0.8865958">
Treatment ... Genetic counseling will be of benefit for affected individuals and their families. Family members of affected indi-
viduals should also receive regular clinical evaluations to detect any symptoms and physical characteristics that may be potentially
associated with Three M syndrome or heterozygosity for the disorder. Other treatment for Three M syndrome is symptomatic and
supportive.
Figure 1: A fragment from the automatically created article for 3-M Syndrome.
</figureCaption>
<listItem confidence="0.983274153846154">
• Automatic template creation: Templates
are automatically induced from human-
authored documents. This ensures that the
overview article will have the breadth ex-
pected in a comprehensive summary, with
content drawn from a wide variety of Inter-
net sources.
• Joint parameter estimation for content se-
lection: Parameters are learned jointly for
all topics in the template. This procedure op-
timizes both local relevance of information
for each topic and global coherence across
the entire article.
</listItem>
<bodyText confidence="0.9993624">
We evaluate our approach by creating articles in
two domains: Actors and Diseases. For a data set,
we use Wikipedia, which contains articles simi-
lar to those we wish to produce in terms of length
and breadth. An advantage of this data set is that
Wikipedia articles explicitly delineate topical sec-
tions, facilitating structural analysis. The results
of our evaluation confirm the benefits of structure-
aware content selection over approaches that do
not explicitly model topical structure.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999922425531915">
Concept-to-text generation and text-to-text gener-
ation take very different approaches to content se-
lection. In traditional concept-to-text generation,
a content planner provides a detailed template for
what information should be included in the output
and how this information should be organized (Re-
iter and Dale, 2000). In text-to-text generation,
such templates for information organization are
not available; sentences are selected based on their
salience properties (Mani and Maybury, 1999).
While this strategy is robust and portable across
domains, output summaries often suffer from co-
herence and coverage problems.
In between these two approaches is work on
domain-specific text-to-text generation. Instances
of these tasks are biography generation in sum-
marization and answering definition requests in
question-answering. In contrast to a generic sum-
marizer, these applications aim to characterize
the types of information that are essential in a
given domain. This characterization varies greatly
in granularity. For instance, some approaches
coarsely discriminate between biographical and
non-biographical information (Zhou et al., 2004;
Biadsy et al., 2008), while others go beyond binary
distinction by identifying atomic events – e.g., oc-
cupation and marital status – that are typically in-
cluded in a biography (Weischedel et al., 2004;
Filatova and Prager, 2005; Filatova et al., 2006).
Commonly, such templates are specified manually
and are hard-coded for a particular domain (Fujii
and Ishikawa, 2004; Weischedel et al., 2004).
Our work is related to these approaches; how-
ever, content selection in our work is driven by
domain-specific automatically induced templates.
As our experiments demonstrate, patterns ob-
served in domain-specific training data provide
sufficient constraints for topic organization, which
is crucial for a comprehensive text.
Our work also relates to a large body of recent
work that uses Wikipedia material. Instances of
this work include information extraction, ontology
induction and resource acquisition (Wu and Weld,
2007; Biadsy et al., 2008; Nastase, 2008; Nastase
and Strube, 2008). Our focus is on a different task
— generation of new overview articles that follow
the structure of Wikipedia articles.
</bodyText>
<page confidence="0.999202">
209
</page>
<sectionHeader confidence="0.993324" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9996315">
The goal of our system is to produce a compre-
hensive overview article given a title – e.g., Can-
cer. We assume that relevant information on the
subject is available on the Internet but scattered
among several pages interspersed with noise.
We are provided with a training corpus consist-
ing of n documents d1 ... do in the same domain
– e.g., Diseases. Each document di has a title and
a set of delineated sections2 si1 ... sim. The num-
ber of sections m varies between documents. Each
section sij also has a corresponding heading hij –
e.g., Treatment.
Our overview article creation process consists
of three parts. First, a preprocessing step creates
a template and searches for a number of candidate
excerpts from the Internet. Next, parameters must
be trained for the content selection algorithm us-
ing our training data set. Finally, a complete ar-
ticle may be created by combining a selection of
candidate excerpts.
</bodyText>
<listItem confidence="0.595846">
1. Preprocessing (Section 3.1) Our prepro-
</listItem>
<bodyText confidence="0.955786">
cessing step leverages previous work in topic
segmentation and query reformulation to pre-
pare a template and a set of candidate ex-
cerpts for content selection. Template gen-
eration must occur once per domain, whereas
search occurs every time an article is gener-
ated in both learning and application.
</bodyText>
<listItem confidence="0.8522454">
(a) Template Induction To create a con-
tent template, we cluster all section
headings hi1 ... him for all documents
di. Each cluster is labeled with the most
common heading hij within the clus-
ter. The largest k clusters are selected to
become topics t1 ... tk, which form the
domain-specific content template.
(b) Search For each document that we
wish to create, we retrieve from the In-
ternet a set of r excerpts ej1 ... ejr for
each topic tj from the template. We de-
fine appropriate search queries using the
requested document title and topics tj.
2. Learning Content Selection (Section 3.2)
For each topic tj, we learn the corresponding
topic-specific parameters wj to determine the
2In data sets where such mark-up is not available, one can
employ topical segmentation algorithms as an additional pre-
processing step.
</listItem>
<bodyText confidence="0.992551352941176">
quality of a given excerpt. Using the percep-
tron framework augmented with an ILP for-
mulation for global optimization, the system
is trained to select the best excerpt for each
document di and each topic tj. For train-
ing, we assume the best excerpt is the original
human-authored text sij.
3. Application (Section 3.2) Given the title of
a requested document, we select several ex-
cerpts from the candidate vectors returned by
the search procedure (1b) to create a com-
prehensive overview article. We perform the
decoding procedure jointly using learned pa-
rameters w1 ... wk and the same ILP formu-
lation for global optimization as in training.
The result is a new document with k excerpts,
one for each topic.
</bodyText>
<subsectionHeader confidence="0.997873">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99991003125">
Template Induction A content template speci-
fies the topical structure of documents in one do-
main. For instance, the template for articles about
actors consists of four topics t1 ... t4: biography,
early life, career, and personal life. Using this
template to create the biography of a new actor
will ensure that its information coverage is con-
sistent with existing human-authored documents.
We aim to derive these templates by discovering
common patterns in the organization of documents
in a domain of interest. There has been a sizable
amount of research on structure induction ranging
from linear segmentation (Hearst, 1994) to content
modeling (Barzilay and Lee, 2004). At the core
of these methods is the assumption that fragments
of text conveying similar information have simi-
lar word distribution patterns. Therefore, often a
simple segment clustering across domain texts can
identify strong patterns in content structure (Barzi-
lay and Elhadad, 2003). Clusters containing frag-
ments from many documents are indicative of top-
ics that are essential for a comprehensive sum-
mary. Given the simplicity and robustness of this
approach, we utilize it for template induction.
We cluster all section headings hi1 ... him from
all documents di using a repeated bisectioning
algorithm (Zhao et al., 2005). As a similarity
function, we use cosine similarity weighted with
TF*IDF. We eliminate any clusters with low in-
ternal similarity (i.e., smaller than 0.5), as we as-
sume these are “miscellaneous” clusters that will
not yield unified topics.
</bodyText>
<page confidence="0.988389">
210
</page>
<bodyText confidence="0.99998453125">
We determine the average number of sections
k over all documents in our training set, then se-
lect the k largest section clusters as topics. We or-
der these topics as t1 ... tk using a majority order-
ing algorithm (Cohen et al., 1998). This algorithm
finds a total order among clusters that is consistent
with a maximal number of pairwise relationships
observed in our data set.
Each topic tj is identified by the most frequent
heading found within the cluster – e.g., Causes.
This set of topics forms the content template for a
domain.
Search To retrieve relevant excerpts, we must
define appropriate search queries for each topic
t1 ... tk. Query reformulation is an active area of
research (Agichtein et al., 2001). We have exper-
imented with several of these methods for draw-
ing search queries from representative words in the
body text of each topic; however, we find that the
best performance is provided by deriving queries
from a conjunction of the document title and topic
– e.g., “3-M syndrome” diagnosis.
Using these queries, we search using Yahoo!
and retrieve the first ten result pages for each topic.
From each of these pages, we extract all possible
excerpts consisting of chunks of text between stan-
dardized boundary indicators (such as &lt;p&gt; tags).
In our experiments, there are an average of 6 ex-
cerpts taken from each page. For each topic tj of
each document we wish to create, the total number
of excerpts r found on the Internet may differ. We
label the excerpts ej1 ... ejr.
</bodyText>
<subsectionHeader confidence="0.998584">
3.2 Selection Model
</subsectionHeader>
<bodyText confidence="0.99993421875">
Our selection model takes the content template
t1 ... tk and the candidate excerpts ej1 ... ejr for
each topic tj produced in the previous steps. It
then selects a series of k excerpts, one from each
topic, to create a coherent summary.
One possible approach is to perform individ-
ual selections from each set of excerpts ej1 ... ejr
and then combine the results. This strategy is
commonly used in multi-document summariza-
tion (Barzilay et al., 1999; Goldstein et al., 2000;
Radev et al., 2000), where the combination step
eliminates the redundancy across selected ex-
cerpts. However, separating the two steps may not
be optimal for this task — the balance between
coverage and redundancy is harder to achieve
when a multi-paragraph summary is generated. In
addition, a more discriminative selection strategy
is needed when candidate excerpts are drawn di-
rectly from the web, as they may be contaminated
with noise.
We propose a novel joint training algorithm that
learns selection criteria for all the topics simulta-
neously. This approach enables us to maximize
both local fit and global coherence. We implement
this algorithm using the perceptron framework, as
it can be easily modified for structured prediction
while preserving convergence guarantees (Daum´e
III and Marcu, 2005; Snyder and Barzilay, 2007).
In this section, we first describe the structure
and decoding procedure of our model. We then
present an algorithm to jointly learn the parame-
ters of all topic models.
</bodyText>
<subsectionHeader confidence="0.898648">
3.2.1 Model Structure
</subsectionHeader>
<bodyText confidence="0.996477">
The model inputs are as follows:
</bodyText>
<listItem confidence="0.9357358">
• The title of the desired document
• t1 ... tk — topics from the content template
• ej1 ... ejr — candidate excerpts for each
topic tj
In addition, we define feature and parameter
vectors:
• 0(ejl) — feature vector for the lth candidate
excerpt for topic tj
• w1 ... wk — parameter vectors, one for each
of the topics t1 ... tk
</listItem>
<bodyText confidence="0.999740714285714">
Our model constructs a new article by following
these two steps:
Ranking First, we attempt to rank candidate
excerpts based on how representative they are of
each individual topic. For each topic tj, we induce
a ranking of the excerpts ej1 ... ejr by mapping
each excerpt ejl to a score:
</bodyText>
<equation confidence="0.998366">
scorej(ejl) = 0(ejl) · wj
</equation>
<bodyText confidence="0.999863916666667">
Candidates for each topic are ranked from high-
est to lowest score. After this procedure, the posi-
tion l of excerpt ejl within the topic-specific can-
didate vector is the excerpt’s rank.
Optimizing the Global Objective To avoid re-
dundancy between topics, we formulate an opti-
mization problem using excerpt rankings to create
the final article. Given k topics, we would like to
select one excerpt ejl for each topic tj, such that
the rank is minimized; that is, scorej(ejl) is high.
To select the optimal excerpts, we employ inte-
ger linear programming (ILP). This framework is
</bodyText>
<page confidence="0.992357">
211
</page>
<bodyText confidence="0.995645636363636">
commonly used in generation and summarization
applications where the selection process is driven
by multiple constraints (Marciniak and Strube,
2005; Clarke and Lapata, 2007).
We represent excerpts included in the output
using a set of indicator variables, xjl. For each
excerpt ejl, the corresponding indicator variable
xjl = 1 if the excerpt is included in the final doc-
ument, and xjl = 0 otherwise.
Our objective is to minimize the ranks of the
excerpts selected for the final document:
</bodyText>
<table confidence="0.8930734">
Feature Value
UNI wordi count of word occurrences
POS wordi first position of word in excerpt
BI wordi wordi+1 count of bigram occurrences
SENT count of all sentences
EXCL count of exclamations
QUES count of questions
WORD count of all words
NAME count of title mentions
DATE count of dates
PROP count of proper nouns
PRON count of pronouns
NUM count of numbers
FIRST word1 1*
FIRST word1 word2 1†
</table>
<equation confidence="0.965321333333333">
k r
min l · xjl
j=1 l=1
</equation>
<bodyText confidence="0.9162856">
We augment this formulation with two types of
constraints.
Exclusivity Constraints We want to ensure that
exactly one indicator xjl is nonzero for each topic
tj. These constraints are formulated as follows:
</bodyText>
<equation confidence="0.872394">
r
xjl = 1 bj E 11 ... k}
l=1
Redundancy Constraints We also want to pre-
</equation>
<bodyText confidence="0.9997462">
vent redundancy across topics. We define
sim(ejl, ej,l,) as the cosine similarity between ex-
cerpts ejl from topic tj and ej,l, from topic tj,.
We introduce constraints that ensure no pair of ex-
cerpts has similarity above 0.5:
</bodyText>
<equation confidence="0.905932">
(xjl + xj,l,) · sim(ejl, ej,l,) G 1
bj, j&apos; = 1... k bl, l&apos; = 1... r
</equation>
<bodyText confidence="0.99938225">
If excerpts ejl and ej,l, have cosine similarity
sim(ejl, ej,l,) &gt; 0.5, only one excerpt may be
selected for the final document – i.e., either xjl
or xj,l, may be 1, but not both. Conversely, if
sim(ejl, ej,l,) G 0.5, both excerpts may be se-
lected.
Solving the ILP Solving an integer linear pro-
gram is NP-hard (Cormen et al., 1992); however,
in practice there exist several strategies for solving
certain ILPs efficiently. In our study, we employed
lp solve,3 an efficient mixed integer programming
solver which implements the Branch-and-Bound
algorithm. On a larger scale, there are several al-
ternatives to approximate the ILP results, such as a
dynamic programming approximation to the knap-
sack problem (McDonald, 2007).
</bodyText>
<footnote confidence="0.826929">
3http://lpsolve.sourceforge.net/5.5/
</footnote>
<tableCaption confidence="0.99723">
Table 1: Features employed in the ranking model.
</tableCaption>
<bodyText confidence="0.866780428571429">
* Defined as the first unigram in the excerpt.
† Defined as the first bigram in the excerpt.
$ Defined as excerpts with cosine similarity &gt; 0.5
Features As shown in Table 1, most of the fea-
tures we select in our model have been employed
in previous work on summarization (Mani and
Maybury, 1999). All features except the SIMS
feature are defined for individual excerpts in isola-
tion. For each excerpt ejl, the value of the SIMS
feature is the count of excerpts ejl, in the same
topic tj for which sim(ejl, ejl,) &gt; 0.5. This fea-
ture quantifies the degree of repetition within a
topic, often indicative of an excerpt’s accuracy and
relevance.
</bodyText>
<subsectionHeader confidence="0.765789">
3.2.2 Model Training
</subsectionHeader>
<bodyText confidence="0.997657684210526">
Generating Training Data For training, we are
given n original documents d1 ... dn, a content
template consisting of topics t1 ... tk, and a set of
candidate excerpts eij1 ... eijr for each document
di and topic tj. For each section of each docu-
ment, we add the gold excerpt sij to the corre-
sponding vector of candidate excerpts eij1 ... eijr.
This excerpt represents the target for our training
algorithm. Note that the algorithm does not re-
quire annotated ranking data; only knowledge of
this “optimal” excerpt is required. However, if
the excerpts provided in the training data have low
quality, noise is introduced into the system.
Training Procedure Our algorithm is a
modification of the perceptron ranking algo-
rithm (Collins, 2002), which allows for joint
learning across several ranking problems (Daum´e
III and Marcu, 2005; Snyder and Barzilay, 2007).
Pseudocode for this algorithm is provided in
</bodyText>
<figureCaption confidence="0.562442">
Figure 2.
</figureCaption>
<bodyText confidence="0.599571">
First, we define Rank(eij1 ... eijr, wj), which
SIMS count of similar excerpts$
</bodyText>
<page confidence="0.986806">
212
</page>
<bodyText confidence="0.999966023809524">
ranks all excerpts from the candidate excerpt
vector eij1 ... eijr for document di and topic
tj. Excerpts are ordered by scorej(ejl) using
the current parameter values. We also define
Optimize(eij1 ... eijr), which finds the optimal
selection of excerpts (one per topic) given ranked
lists of excerpts eij1 ... eijr for each document di
and topic tj. These functions follow the ranking
and optimization procedures described in Section
3.2.1. The algorithm maintains k parameter vec-
tors w1 ... wk, one associated with each topic tj
desired in the final article. During initialization,
all parameter vectors are set to zeros (line 2).
To learn the optimal parameters, this algorithm
iterates over the training set until the parameters
converge or a maximum number of iterations is
reached (line 3). For each document in the train-
ing set (line 4), the following steps occur: First,
candidate excerpts for each topic are ranked (lines
5-6). Next, decoding through ILP optimization is
performed over all ranked lists of candidate ex-
cerpts, selecting one excerpt for each topic (line
7). Finally, the parameters are updated in a joint
fashion. For each topic (line 8), if the selected
excerpt is not similar enough to the gold excerpt
(line 9), the parameters for that topic are updated
using a standard perceptron update rule (line 10).
When convergence is reached or the maximum it-
eration count is exceeded, the learned parameter
values are returned (line 12).
The use of ILP during each step of training
sets this algorithm apart from previous work. In
prior research, ILP was used as a postprocess-
ing step to remove redundancy and make other
global decisions about parameters (McDonald,
2007; Marciniak and Strube, 2005; Clarke and La-
pata, 2007). However, in our training, we inter-
twine the complete decoding procedure with the
parameter updates. Our joint learning approach
finds per-topic parameter values that are maxi-
mally suited for the global decoding procedure for
content selection.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9445405">
We evaluate our method by observing the quality
of automatically created articles in different do-
mains. We compute the similarity of a large num-
ber of articles produced by our system and sev-
eral baselines to the original human-authored arti-
cles using ROUGE, a standard metric for summary
quality. In addition, we perform an analysis of edi-
Input:
</bodyText>
<equation confidence="0.901981666666667">
d1 ... dam: A set of n documents, each containing
k sections si1 ... sik
eij1 ... eijr: Sets of candidate excerpts for each topic
</equation>
<bodyText confidence="0.552624">
tj and document di
</bodyText>
<figure confidence="0.839678814814815">
Define:
Rank(eij1 ... eijr, wj):
As described in Section 3.2.1:
Calculates scorej(eijl) for all excerpts for
document di and topic tj, using parameters wj.
Orders the list of excerpts by scorej(eijl)
from highest to lowest.
Optimize(ei11 ... eikr):
As described in Section 3.2.1:
Finds the optimal selection of excerpts to form a
final article, given ranked lists of excerpts
for each topic t1 ... tk.
,/,/Returns a list of k excerpts, one for each topic.
O(eijl):
Returns the feature vector representing excerpt eijl
Initialization:
1 Forj = 1 ... k
2 Set parameters wj = 0
Training:
3 Repeat until convergence or while iter &lt; itermax:
4 For i = 1 ... n
5 For j = 1 ... k
6 Rank(eij1 ... eijr, wj)
7 x1 ... xk = Optimize(ei11 ... eikr)
8 For j = 1 ... k
9 If sim(xj, sij) &lt; 0.8
10
</figure>
<figureCaption confidence="0.998042">
Figure 2: An algorithm for learning several rank-
</figureCaption>
<bodyText confidence="0.971538904761905">
ing problems with a joint decoding mechanism.
tor reaction to system-produced articles submitted
to Wikipedia.
Data For evaluation, we consider two domains:
American Film Actors and Diseases. These do-
mains have been commonly used in prior work
on summarization (Weischedel et al., 2004; Zhou
et al., 2004; Filatova and Prager, 2005; Demner-
Fushman and Lin, 2007; Biadsy et al., 2008). Our
text corpus consists of articles drawn from the cor-
responding categories in Wikipedia. There are
2,150 articles in American Film Actors and 523
articles in Diseases. For each domain, we ran-
domly select 90% of articles for training and test
on the remaining 10%. Human-authored articles
in both domains contain an average of four top-
ics, and each topic contains an average of 193
words. In order to model the real-world scenario
where Wikipedia articles are not always available
(as for new or specialized topics), we specifically
exclude Wikipedia sources during our search pro-
</bodyText>
<figure confidence="0.84051">
wj = wj + 0(sij) − O(xi)
11 iter = iter + 1
12 Return parameters w1 ... wk
</figure>
<page confidence="0.979658">
213
</page>
<table confidence="0.999874461538461">
Avg. Excerpts Avg. Sources
Amer. Film Actors
Search 2.3 1
No Template 4 4.0
Disjoint 4 2.1
Full Model 4 3.4
Oracle 4.3 4.3
Diseases
Search 3.1 1
No Template 4 2.5
Disjoint 4 3.0
Full Model 4 3.2
Oracle 5.8 3.9
</table>
<tableCaption confidence="0.993679">
Table 2: Average number of excerpts selected and
sources used in article creation for test articles.
</tableCaption>
<bodyText confidence="0.999039409638555">
cedure (Section 3.1) for evaluation.
Baselines Our first baseline, Search, relies
solely on search engine ranking for content selec-
tion. Using the article title as a query – e.g., Bacil-
lary Angiomatosis, this method selects the web
page that is ranked first by the search engine. From
this page we select the first k paragraphs where k
is defined in the same way as in our full model. If
there are less than k paragraphs on the page, all
paragraphs are selected, but no other sources are
used. This yields a document of comparable size
with the output of our system. Despite its sim-
plicity, this baseline is not naive: extracting ma-
terial from a single document guarantees that the
output is coherent, and a page highly ranked by a
search engine may readily contain a comprehen-
sive overview of the subject.
Our second baseline, No Template, does not
use a template to specify desired topics; there-
fore, there are no constraints on content selection.
Instead, we follow a simplified form of previous
work on biography creation, where a classifier is
trained to distinguish biographical text (Zhou et
al., 2004; Biadsy et al., 2008).
In this case, we train a classifier to distinguish
domain-specific text. Positive training data is
drawn from all topics in the given domain cor-
pus. To find negative training data, we perform
the search procedure as in our full model (see
Section 3.1) using only the article titles as search
queries. Any excerpts which have very low sim-
ilarity to the original articles are used as negative
examples. During the decoding procedure, we use
the same search procedure. We then classify each
excerpt as relevant or irrelevant and select the k
non-redundant excerpts with the highest relevance
confidence scores.
Our third baseline, Disjoint, uses the ranking
perceptron framework as in our full system; how-
ever, rather than perform an optimization step
during training and decoding, we simply select
the highest-ranked excerpt for each topic. This
equates to standard linear classification for each
section individually.
In addition to these baselines, we compare
against an Oracle system. For each topic present
in the human-authored article, the Oracle selects
the excerpt from our full model’s candidate ex-
cerpts with the highest cosine similarity to the
human-authored text. This excerpt is the optimal
automatic selection from the results available, and
therefore represents an upper bound on our excerpt
selection task. Some articles contain additional
topics beyond those in the template; in these cases,
the Oracle system produces a longer article than
our algorithm.
Table 2 shows the average number of excerpts
selected and sources used in articles created by our
full model and each baseline.
Automatic Evaluation To assess the quality of
the resulting overview articles, we compare them
with the original human-authored articles. We
use ROUGE, an evaluation metric employed at the
Document Understanding Conferences (DUC),
which assumes that proximity to human-authored
text is an indicator of summary quality. We
use the publicly available ROUGE toolkit (Lin,
2004) to compute recall, precision, and F-score for
ROUGE-1. We use the Wilcoxon Signed Rank Test
to determine statistical significance.
Analysis of Human Edits In addition to our auto-
matic evaluation, we perform a study of reactions
to system-produced articles by the general pub-
lic. To achieve this goal, we insert automatically
created articles4 into Wikipedia itself and exam-
ine the feedback of Wikipedia editors. Selection
of specific articles is constrained by the need to
find topics which are currently of “stub” status that
have enough information available on the Internet
to construct a valid article. After a period of time,
we analyzed the edits made to the articles to deter-
mine the overall editor reaction. We report results
on 15 articles in the Diseases category5.
</bodyText>
<footnote confidence="0.9950716">
4In addition to the summary itself, we also include proper
citations to the sources from which the material is extracted.
5We are continually submitting new articles; however, we
report results on those that have at least a 6 month history at
time of writing.
</footnote>
<page confidence="0.989229">
214
</page>
<table confidence="0.999802538461539">
Recall Precision F-score
Amer. Film Actors
Search 0.09 0.37 0.13 ∗
No Template 0.33 0.50 0.39 ∗
Disjoint 0.45 0.32 0.36 ∗
Full Model 0.46 0.40 0.41
Oracle 0.48 0.64 0.54 ∗
Diseases
Search 0.31 0.37 0.32 †
No Template 0.32 0.27 0.28 ∗
Disjoint 0.33 0.40 0.35 ∗
Full Model 0.36 0.39 0.37
Oracle 0.59 0.37 0.44 ∗
</table>
<tableCaption confidence="0.99336">
Table 3: Results of ROUGE-1 evaluation.
</tableCaption>
<table confidence="0.8083605">
∗ Significant with respect to our full model for P &lt; 0.05.
† Significant with respect to our full model for P &lt; 0.10.
</table>
<bodyText confidence="0.997723428571428">
Since Wikipedia is a live resource, we do not
repeat this procedure for our baseline systems.
Adding articles from systems which have previ-
ously demonstrated poor quality would be im-
proper, especially in Diseases. Therefore, we
present this analysis as an additional observation
rather than a rigorous technical study.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999617857142857">
Automatic Evaluation The results of this evalu-
ation are shown in Table 3. Our full model outper-
forms all of the baselines. By surpassing the Dis-
joint baseline, we demonstrate the benefits of joint
classification. Furthermore, the high performance
of both our full model and the Disjoint baseline
relative to the other baselines shows the impor-
tance of structure-aware content selection. The
Oracle system, which represents an upper bound
on our system’s capabilities, performs well.
The remaining baselines have different flaws:
Articles produced by the No Template baseline
tend to focus on a single topic extensively at the
expense of breadth, because there are no con-
straints to ensure diverse topic selection. On the
other hand, performance of the Search baseline
varies dramatically. This is expected; this base-
line relies heavily on both the search engine and
individual web pages. The search engine must cor-
rectly rank relevant pages, and the web pages must
provide the important material first.
Analysis of Human Edits The results of our ob-
servation of editing patterns are shown in Table
4. These articles have resided on Wikipedia for
a period of time ranging from 5-11 months. All
of them have been edited, and no articles were re-
moved due to lack of quality. Moreover, ten au-
tomatically created articles have been promoted
</bodyText>
<figure confidence="0.8373774">
Type Count
Total articles 15
Promoted articles 10
Edit types
Intra-wiki links 36
Formatting 25
Grammar 20
Minor topic edits 2
Major topic changes 1
Total edits 85
</figure>
<tableCaption confidence="0.897826">
Table 4: Distribution of edits on Wikipedia.
</tableCaption>
<bodyText confidence="0.999814285714286">
by human editors from stubs to regular Wikipedia
entries based on the quality and coverage of the
material. Information was removed in three cases
for being irrelevant, one entire section and two
smaller pieces. The most common changes were
small edits to formatting and introduction of links
to other Wikipedia articles in the body text.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978565217391">
In this paper, we investigated an approach for cre-
ating a multi-paragraph overview article by select-
ing relevant material from the web and organiz-
ing it into a single coherent text. Our algorithm
yields significant gains over a structure-agnostic
approach. Moreover, our results demonstrate the
benefits of structured classification, which out-
performs independently trained topical classifiers.
Overall, the results of our evaluation combined
with our analysis of human edits confirm that the
proposed method can effectively produce compre-
hensive overview articles.
This work opens several directions for future re-
search. Diseases and American Film Actors ex-
hibit fairly consistent article structures, which are
successfully captured by a simple template cre-
ation process. However, with categories that ex-
hibit structural variability, more sophisticated sta-
tistical approaches may be required to produce ac-
curate templates. Moreover, a promising direction
is to consider hierarchical discourse formalisms
such as RST (Mann and Thompson, 1988) to sup-
plement our template-based approach.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.901456125">
The authors acknowledge the support of the NSF (CA-
REER grant IIS-0448168, grant IIS-0835445, and grant IIS-
0835652) and NIH (grant V54LM008748). Thanks to Mike
Collins, Julia Hirschberg, and members of the MIT NLP
group for their helpful suggestions and comments. Any opin-
ions, findings, conclusions, or recommendations expressed in
this paper are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
</bodyText>
<page confidence="0.997496">
215
</page>
<figure confidence="0.564243571428571">
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano. 2001.
Learning search engine specific query transformations for
question answering. In Proceedings of WWW, pages 169–
178.
Inderjeet Mani and Mark T. Maybury. 1999. Advances in
Automatic Text Summarization. The MIT Press.
</figure>
<figureCaption confidence="0.413475333333333">
William C. Mann and Sandra A. Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of text
organization. Text, 8(3):243–281.
</figureCaption>
<reference confidence="0.999641576470588">
Regina Barzilay and Noemie Elhadad. 2003. Sentence align-
ment for monolingual comparable corpora. In Proceed-
ings of EMNLP, pages 25–32.
Regina Barzilay and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to genera-
tion and summarization. In Proceedings of HLT-NAACL,
pages 113–120.
Regina Barzilay, Kathleen R. McKeown, and Michael El-
hadad. 1999. Information fusion in the context of multi-
document summarization. In Proceedings of ACL, pages
550–557.
Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production using
wikipedia. In Proceedings of ACL/HLT, pages 807–815.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings of
EMNLP-CoNLL, pages 1–11.
William W. Cohen, Robert E. Schapire, and Yoram Singer.
1998. Learning to order things. In Proceedings of NIPS,
pages 451–457.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Pro-
ceedings of ACL, pages 489–496.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1992. Intoduction to Algorithms. The MIT Press.
Hal Daum´e III and Daniel Marcu. 2005. A large-scale explo-
ration of effective global features for a joint entity detec-
tion and tracking model. In Proceedings of HLT/EMNLP,
pages 97–104.
Dina Demner-Fushman and Jimmy Lin. 2007. Answer-
ing clinical questions with knowledge-based and statisti-
cal techniques. Computational Linguistics, 33(1):63–103.
Elena Filatova and John M. Prager. 2005. Tell me what you
do and I’ll tell you what you are: Learning occupation-
related activities for biographies. In Proceedings of
HLT/EMNLP, pages 113–120.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of ACL, pages 207–214.
Atsushi Fujii and Tetsuya Ishikawa. 2004. Summarizing en-
cyclopedic term descriptions on the web. In Proceedings
of COLING, page 645.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. In Proceedings of NAACL-ANLP,
pages 40–48.
Marti A. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proceedings of ACL, pages 9–16.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proceedings of ACL, pages
74–81.
Tomasz Marciniak and Michael Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In Proceedings
of CoNLL, pages 136–143.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceedings
of EICR, pages 557–564.
Vivi Nastase and Michael Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI, pages 1219–1224.
Vivi Nastase. 2008. Topic-driven multi-document summa-
rization with encyclopedic knowledge and spreading acti-
vation. In Proceedings of EMNLP, pages 763–772.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based summarization
of multiple documents: sentence extraction, utility-
based evaluation, and user studies. In Proceedings of
ANLP/NAACL, pages 21–29.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press,
Cambridge.
Benjamin Snyder and Regina Barzilay. 2007. Multiple as-
pect ranking using the good grief algorithm. In Proceed-
ings of HLT-NAACL, pages 300–307.
Ralph M. Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions. In
New Directions in Question Answering, pages 59–70.
Fei Wu and Daniel S. Weld. 2007. Autonomously semanti-
fying wikipedia. In Proceedings of CIKM, pages 41–50.
Ying Zhao, George Karypis, and Usama Fayyad. 2005.
Hierarchical clustering algorithms for document datasets.
Data Mining and Knowledge Discovery, 10(2):141–168.
L. Zhou, M. Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proceedings of
EMNLP, pages 434–441.
</reference>
<page confidence="0.999145">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.996758">
<title confidence="0.999605">Automatically Generating Wikipedia Articles: A Structure-Aware Approach</title>
<author confidence="0.999914">Christina Sauper</author>
<author confidence="0.999914">Regina Barzilay</author>
<affiliation confidence="0.999962">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<abstract confidence="0.9998801">In this paper, we investigate an approach for creating a comprehensive textual overview of a subject composed of information drawn from the Internet. We use the high-level structure of human-authored texts to automatically induce a domainspecific template for the topic structure of a new overview. The algorithmic innovation of our work is a method to learn topicspecific extractors for content selection jointly for the entire template. We augment the standard perceptron algorithm with a global integer linear programming formulation to optimize both local fit of information into each topic and global coherence across the entire overview. The results of our evaluation confirm the benefits of incorporating structural information into the content selection process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="12469" citStr="Barzilay and Elhadad, 2003" startWordPosition="1927" endWordPosition="1931">erage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 ... him from all documents di using a repeated bisectioning algorithm (Zhao et al., 2005). As a similarity function, we use cosine similarity weighted with TF*IDF. We eliminate any clusters with low internal similarity (i.e., smaller than 0.5), as we assume these are “miscellaneous” clusters that will not yield unified topics. 210 We determine t</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of EMNLP, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="12183" citStr="Barzilay and Lee, 2004" startWordPosition="1885" endWordPosition="1888">he topical structure of documents in one domain. For instance, the template for articles about actors consists of four topics t1 ... t4: biography, early life, career, and personal life. Using this template to create the biography of a new actor will ensure that its information coverage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 ... him from all documents di using a repeated bisectioning a</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of HLT-NAACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multidocument summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="15015" citStr="Barzilay et al., 1999" startWordPosition="2363" endWordPosition="2366">or each topic tj of each document we wish to create, the total number of excerpts r found on the Internet may differ. We label the excerpts ej1 ... ejr. 3.2 Selection Model Our selection model takes the content template t1 ... tk and the candidate excerpts ej1 ... ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 ... ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise. We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously. This approach enables us</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multidocument summarization. In Proceedings of ACL, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadi Biadsy</author>
<author>Julia Hirschberg</author>
<author>Elena Filatova</author>
</authors>
<title>An unsupervised approach to biography production using wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>807--815</pages>
<contexts>
<context position="7612" citStr="Biadsy et al., 2008" startWordPosition="1137" endWordPosition="1140">ains, output summaries often suffer from coherence and coverage problems. In between these two approaches is work on domain-specific text-to-text generation. Instances of these tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training dat</context>
<context position="24993" citStr="Biadsy et al., 2008" startWordPosition="4052" endWordPosition="4055">t until convergence or while iter &lt; itermax: 4 For i = 1 ... n 5 For j = 1 ... k 6 Rank(eij1 ... eijr, wj) 7 x1 ... xk = Optimize(ei11 ... eikr) 8 For j = 1 ... k 9 If sim(xj, sij) &lt; 0.8 10 Figure 2: An algorithm for learning several ranking problems with a joint decoding mechanism. tor reaction to system-produced articles submitted to Wikipedia. Data For evaluation, we consider two domains: American Film Actors and Diseases. These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008). Our text corpus consists of articles drawn from the corresponding categories in Wikipedia. There are 2,150 articles in American Film Actors and 523 articles in Diseases. For each domain, we randomly select 90% of articles for training and test on the remaining 10%. Human-authored articles in both domains contain an average of four topics, and each topic contains an average of 193 words. In order to model the real-world scenario where Wikipedia articles are not always available (as for new or specialized topics), we specifically exclude Wikipedia sources during our search prowj = wj + 0(sij) </context>
<context position="27096" citStr="Biadsy et al., 2008" startWordPosition="4419" endWordPosition="4422">cument of comparable size with the output of our system. Despite its simplicity, this baseline is not naive: extracting material from a single document guarantees that the output is coherent, and a page highly ranked by a search engine may readily contain a comprehensive overview of the subject. Our second baseline, No Template, does not use a template to specify desired topics; therefore, there are no constraints on content selection. Instead, we follow a simplified form of previous work on biography creation, where a classifier is trained to distinguish biographical text (Zhou et al., 2004; Biadsy et al., 2008). In this case, we train a classifier to distinguish domain-specific text. Positive training data is drawn from all topics in the given domain corpus. To find negative training data, we perform the search procedure as in our full model (see Section 3.1) using only the article titles as search queries. Any excerpts which have very low similarity to the original articles are used as negative examples. During the decoding procedure, we use the same search procedure. We then classify each excerpt as relevant or irrelevant and select the k non-redundant excerpts with the highest relevance confidenc</context>
</contexts>
<marker>Biadsy, Hirschberg, Filatova, 2008</marker>
<rawString>Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008. An unsupervised approach to biography production using wikipedia. In Proceedings of ACL/HLT, pages 807–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="17495" citStr="Clarke and Lapata, 2007" startWordPosition="2777" endWordPosition="2780">e topic-specific candidate vector is the excerpt’s rank. Optimizing the Global Objective To avoid redundancy between topics, we formulate an optimization problem using excerpt rankings to create the final article. Given k topics, we would like to select one excerpt ejl for each topic tj, such that the rank is minimized; that is, scorej(ejl) is high. To select the optimal excerpts, we employ integer linear programming (ILP). This framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints (Marciniak and Strube, 2005; Clarke and Lapata, 2007). We represent excerpts included in the output using a set of indicator variables, xjl. For each excerpt ejl, the corresponding indicator variable xjl = 1 if the excerpt is included in the final document, and xjl = 0 otherwise. Our objective is to minimize the ranks of the excerpts selected for the final document: Feature Value UNI wordi count of word occurrences POS wordi first position of word in excerpt BI wordi wordi+1 count of bigram occurrences SENT count of all sentences EXCL count of exclamations QUES count of questions WORD count of all words NAME count of title mentions DATE count of</context>
<context position="23019" citStr="Clarke and Lapata, 2007" startWordPosition="3710" endWordPosition="3714">n a joint fashion. For each topic (line 8), if the selected excerpt is not similar enough to the gold excerpt (line 9), the parameters for that topic are updated using a standard perceptron update rule (line 10). When convergence is reached or the maximum iteration count is exceeded, the learned parameter values are returned (line 12). The use of ILP during each step of training sets this algorithm apart from previous work. In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters (McDonald, 2007; Marciniak and Strube, 2005; Clarke and Lapata, 2007). However, in our training, we intertwine the complete decoding procedure with the parameter updates. Our joint learning approach finds per-topic parameter values that are maximally suited for the global decoding procedure for content selection. 4 Experimental Setup We evaluate our method by observing the quality of automatically created articles in different domains. We compute the similarity of a large number of articles produced by our system and several baselines to the original human-authored articles using ROUGE, a standard metric for summary quality. In addition, we perform an analysis </context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of EMNLP-CoNLL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Learning to order things.</title>
<date>1998</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>451--457</pages>
<contexts>
<context position="13286" citStr="Cohen et al., 1998" startWordPosition="2066" endWordPosition="2069">emplate induction. We cluster all section headings hi1 ... him from all documents di using a repeated bisectioning algorithm (Zhao et al., 2005). As a similarity function, we use cosine similarity weighted with TF*IDF. We eliminate any clusters with low internal similarity (i.e., smaller than 0.5), as we assume these are “miscellaneous” clusters that will not yield unified topics. 210 We determine the average number of sections k over all documents in our training set, then select the k largest section clusters as topics. We order these topics as t1 ... tk using a majority ordering algorithm (Cohen et al., 1998). This algorithm finds a total order among clusters that is consistent with a maximal number of pairwise relationships observed in our data set. Each topic tj is identified by the most frequent heading found within the cluster – e.g., Causes. This set of topics forms the content template for a domain. Search To retrieve relevant excerpts, we must define appropriate search queries for each topic t1 ... tk. Query reformulation is an active area of research (Agichtein et al., 2001). We have experimented with several of these methods for drawing search queries from representative words in the body</context>
</contexts>
<marker>Cohen, Schapire, Singer, 1998</marker>
<rawString>William W. Cohen, Robert E. Schapire, and Yoram Singer. 1998. Learning to order things. In Proceedings of NIPS, pages 451–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>489--496</pages>
<contexts>
<context position="21014" citStr="Collins, 2002" startWordPosition="3386" endWordPosition="3387">t1 ... tk, and a set of candidate excerpts eij1 ... eijr for each document di and topic tj. For each section of each document, we add the gold excerpt sij to the corresponding vector of candidate excerpts eij1 ... eijr. This excerpt represents the target for our training algorithm. Note that the algorithm does not require annotated ranking data; only knowledge of this “optimal” excerpt is required. However, if the excerpts provided in the training data have low quality, noise is introduced into the system. Training Procedure Our algorithm is a modification of the perceptron ranking algorithm (Collins, 2002), which allows for joint learning across several ranking problems (Daum´e III and Marcu, 2005; Snyder and Barzilay, 2007). Pseudocode for this algorithm is provided in Figure 2. First, we define Rank(eij1 ... eijr, wj), which SIMS count of similar excerpts$ 212 ranks all excerpts from the candidate excerpt vector eij1 ... eijr for document di and topic tj. Excerpts are ordered by scorej(ejl) using the current parameter values. We also define Optimize(eij1 ... eijr), which finds the optimal selection of excerpts (one per topic) given ranked lists of excerpts eij1 ... eijr for each document di a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proceedings of ACL, pages 489–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Intoduction to Algorithms.</title>
<date>1992</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="19136" citStr="Cormen et al., 1992" startWordPosition="3076" endWordPosition="3079">redundancy across topics. We define sim(ejl, ej,l,) as the cosine similarity between excerpts ejl from topic tj and ej,l, from topic tj,. We introduce constraints that ensure no pair of excerpts has similarity above 0.5: (xjl + xj,l,) · sim(ejl, ej,l,) G 1 bj, j&apos; = 1... k bl, l&apos; = 1... r If excerpts ejl and ej,l, have cosine similarity sim(ejl, ej,l,) &gt; 0.5, only one excerpt may be selected for the final document – i.e., either xjl or xj,l, may be 1, but not both. Conversely, if sim(ejl, ej,l,) G 0.5, both excerpts may be selected. Solving the ILP Solving an integer linear program is NP-hard (Cormen et al., 1992); however, in practice there exist several strategies for solving certain ILPs efficiently. In our study, we employed lp solve,3 an efficient mixed integer programming solver which implements the Branch-and-Bound algorithm. On a larger scale, there are several alternatives to approximate the ILP results, such as a dynamic programming approximation to the knapsack problem (McDonald, 2007). 3http://lpsolve.sourceforge.net/5.5/ Table 1: Features employed in the ranking model. * Defined as the first unigram in the excerpt. † Defined as the first bigram in the excerpt. $ Defined as excerpts with co</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1992</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1992. Intoduction to Algorithms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>97--104</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proceedings of HLT/EMNLP, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Jimmy Lin</author>
</authors>
<title>Answering clinical questions with knowledge-based and statistical techniques.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Demner-Fushman, Lin, 2007</marker>
<rawString>Dina Demner-Fushman and Jimmy Lin. 2007. Answering clinical questions with knowledge-based and statistical techniques. Computational Linguistics, 33(1):63–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>John M Prager</author>
</authors>
<title>Tell me what you do and I’ll tell you what you are: Learning occupationrelated activities for biographies.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="7819" citStr="Filatova and Prager, 2005" startWordPosition="1171" endWordPosition="1174">ation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work i</context>
<context position="24942" citStr="Filatova and Prager, 2005" startWordPosition="4043" endWordPosition="4046">Forj = 1 ... k 2 Set parameters wj = 0 Training: 3 Repeat until convergence or while iter &lt; itermax: 4 For i = 1 ... n 5 For j = 1 ... k 6 Rank(eij1 ... eijr, wj) 7 x1 ... xk = Optimize(ei11 ... eikr) 8 For j = 1 ... k 9 If sim(xj, sij) &lt; 0.8 10 Figure 2: An algorithm for learning several ranking problems with a joint decoding mechanism. tor reaction to system-produced articles submitted to Wikipedia. Data For evaluation, we consider two domains: American Film Actors and Diseases. These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008). Our text corpus consists of articles drawn from the corresponding categories in Wikipedia. There are 2,150 articles in American Film Actors and 523 articles in Diseases. For each domain, we randomly select 90% of articles for training and test on the remaining 10%. Human-authored articles in both domains contain an average of four topics, and each topic contains an average of 193 words. In order to model the real-world scenario where Wikipedia articles are not always available (as for new or specialized topics), we specifically exclude Wikip</context>
</contexts>
<marker>Filatova, Prager, 2005</marker>
<rawString>Elena Filatova and John M. Prager. 2005. Tell me what you do and I’ll tell you what you are: Learning occupationrelated activities for biographies. In Proceedings of HLT/EMNLP, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic creation of domain templates.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>207--214</pages>
<contexts>
<context position="7843" citStr="Filatova et al., 2006" startWordPosition="1175" endWordPosition="1178">answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extra</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, McKeown, 2006</marker>
<rawString>Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2006. Automatic creation of domain templates. In Proceedings of ACL, pages 207–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Summarizing encyclopedic term descriptions on the web.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>645</pages>
<contexts>
<context position="7961" citStr="Fujii and Ishikawa, 2004" startWordPosition="1192" endWordPosition="1195">characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and</context>
</contexts>
<marker>Fujii, Ishikawa, 2004</marker>
<rawString>Atsushi Fujii and Tetsuya Ishikawa. 2004. Summarizing encyclopedic term descriptions on the web. In Proceedings of COLING, page 645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-ANLP,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="15039" citStr="Goldstein et al., 2000" startWordPosition="2367" endWordPosition="2370">h document we wish to create, the total number of excerpts r found on the Internet may differ. We label the excerpts ej1 ... ejr. 3.2 Selection Model Our selection model takes the content template t1 ... tk and the candidate excerpts ej1 ... ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 ... ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise. We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously. This approach enables us to maximize both local </context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of NAACL-ANLP, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="12138" citStr="Hearst, 1994" startWordPosition="1880" endWordPosition="1881">tion A content template specifies the topical structure of documents in one domain. For instance, the template for articles about actors consists of four topics t1 ... t4: biography, early life, career, and personal life. Using this template to create the biography of a new actor will ensure that its information coverage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 ... him from all</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="29072" citStr="Lin, 2004" startWordPosition="4726" endWordPosition="4727">yond those in the template; in these cases, the Oracle system produces a longer article than our algorithm. Table 2 shows the average number of excerpts selected and sources used in articles created by our full model and each baseline. Automatic Evaluation To assess the quality of the resulting overview articles, we compare them with the original human-authored articles. We use ROUGE, an evaluation metric employed at the Document Understanding Conferences (DUC), which assumes that proximity to human-authored text is an indicator of summary quality. We use the publicly available ROUGE toolkit (Lin, 2004) to compute recall, precision, and F-score for ROUGE-1. We use the Wilcoxon Signed Rank Test to determine statistical significance. Analysis of Human Edits In addition to our automatic evaluation, we perform a study of reactions to system-produced articles by the general public. To achieve this goal, we insert automatically created articles4 into Wikipedia itself and examine the feedback of Wikipedia editors. Selection of specific articles is constrained by the need to find topics which are currently of “stub” status that have enough information available on the Internet to construct a valid a</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of ACL, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Marciniak</author>
<author>Michael Strube</author>
</authors>
<title>Beyond the pipeline: Discrete optimization in NLP.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="17469" citStr="Marciniak and Strube, 2005" startWordPosition="2773" endWordPosition="2776">n l of excerpt ejl within the topic-specific candidate vector is the excerpt’s rank. Optimizing the Global Objective To avoid redundancy between topics, we formulate an optimization problem using excerpt rankings to create the final article. Given k topics, we would like to select one excerpt ejl for each topic tj, such that the rank is minimized; that is, scorej(ejl) is high. To select the optimal excerpts, we employ integer linear programming (ILP). This framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints (Marciniak and Strube, 2005; Clarke and Lapata, 2007). We represent excerpts included in the output using a set of indicator variables, xjl. For each excerpt ejl, the corresponding indicator variable xjl = 1 if the excerpt is included in the final document, and xjl = 0 otherwise. Our objective is to minimize the ranks of the excerpts selected for the final document: Feature Value UNI wordi count of word occurrences POS wordi first position of word in excerpt BI wordi wordi+1 count of bigram occurrences SENT count of all sentences EXCL count of exclamations QUES count of questions WORD count of all words NAME count of ti</context>
<context position="22993" citStr="Marciniak and Strube, 2005" startWordPosition="3706" endWordPosition="3709">the parameters are updated in a joint fashion. For each topic (line 8), if the selected excerpt is not similar enough to the gold excerpt (line 9), the parameters for that topic are updated using a standard perceptron update rule (line 10). When convergence is reached or the maximum iteration count is exceeded, the learned parameter values are returned (line 12). The use of ILP during each step of training sets this algorithm apart from previous work. In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters (McDonald, 2007; Marciniak and Strube, 2005; Clarke and Lapata, 2007). However, in our training, we intertwine the complete decoding procedure with the parameter updates. Our joint learning approach finds per-topic parameter values that are maximally suited for the global decoding procedure for content selection. 4 Experimental Setup We evaluate our method by observing the quality of automatically created articles in different domains. We compute the similarity of a large number of articles produced by our system and several baselines to the original human-authored articles using ROUGE, a standard metric for summary quality. In additio</context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>Tomasz Marciniak and Michael Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In Proceedings of CoNLL, pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of EICR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="19526" citStr="McDonald, 2007" startWordPosition="3135" endWordPosition="3136">or the final document – i.e., either xjl or xj,l, may be 1, but not both. Conversely, if sim(ejl, ej,l,) G 0.5, both excerpts may be selected. Solving the ILP Solving an integer linear program is NP-hard (Cormen et al., 1992); however, in practice there exist several strategies for solving certain ILPs efficiently. In our study, we employed lp solve,3 an efficient mixed integer programming solver which implements the Branch-and-Bound algorithm. On a larger scale, there are several alternatives to approximate the ILP results, such as a dynamic programming approximation to the knapsack problem (McDonald, 2007). 3http://lpsolve.sourceforge.net/5.5/ Table 1: Features employed in the ranking model. * Defined as the first unigram in the excerpt. † Defined as the first bigram in the excerpt. $ Defined as excerpts with cosine similarity &gt; 0.5 Features As shown in Table 1, most of the features we select in our model have been employed in previous work on summarization (Mani and Maybury, 1999). All features except the SIMS feature are defined for individual excerpts in isolation. For each excerpt ejl, the value of the SIMS feature is the count of excerpts ejl, in the same topic tj for which sim(ejl, ejl,) </context>
<context position="22965" citStr="McDonald, 2007" startWordPosition="3704" endWordPosition="3705">ne 7). Finally, the parameters are updated in a joint fashion. For each topic (line 8), if the selected excerpt is not similar enough to the gold excerpt (line 9), the parameters for that topic are updated using a standard perceptron update rule (line 10). When convergence is reached or the maximum iteration count is exceeded, the learned parameter values are returned (line 12). The use of ILP during each step of training sets this algorithm apart from previous work. In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters (McDonald, 2007; Marciniak and Strube, 2005; Clarke and Lapata, 2007). However, in our training, we intertwine the complete decoding procedure with the parameter updates. Our joint learning approach finds per-topic parameter values that are maximally suited for the global decoding procedure for content selection. 4 Experimental Setup We evaluate our method by observing the quality of automatically created articles in different domains. We compute the similarity of a large number of articles produced by our system and several baselines to the original human-authored articles using ROUGE, a standard metric for</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of EICR, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Decoding wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1219--1224</pages>
<contexts>
<context position="8575" citStr="Nastase and Strube, 2008" startWordPosition="1282" endWordPosition="1285">ikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and Strube, 2008). Our focus is on a different task — generation of new overview articles that follow the structure of Wikipedia articles. 209 3 Method The goal of our system is to produce a comprehensive overview article given a title – e.g., Cancer. We assume that relevant information on the subject is available on the Internet but scattered among several pages interspersed with noise. We are provided with a training corpus consisting of n documents d1 ... do in the same domain – e.g., Diseases. Each document di has a title and a set of delineated sections2 si1 ... sim. The number of sections m varies betwee</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>Vivi Nastase and Michael Strube. 2008. Decoding wikipedia categories for knowledge acquisition. In Proceedings of AAAI, pages 1219–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Topic-driven multi-document summarization with encyclopedic knowledge and spreading activation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>763--772</pages>
<contexts>
<context position="8548" citStr="Nastase, 2008" startWordPosition="1280" endWordPosition="1281"> (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and Strube, 2008). Our focus is on a different task — generation of new overview articles that follow the structure of Wikipedia articles. 209 3 Method The goal of our system is to produce a comprehensive overview article given a title – e.g., Cancer. We assume that relevant information on the subject is available on the Internet but scattered among several pages interspersed with noise. We are provided with a training corpus consisting of n documents d1 ... do in the same domain – e.g., Diseases. Each document di has a title and a set of delineated sections2 si1 ... sim. The number </context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Topic-driven multi-document summarization with encyclopedic knowledge and spreading activation. In Proceedings of EMNLP, pages 763–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP/NAACL,</booktitle>
<pages>21--29</pages>
<contexts>
<context position="15060" citStr="Radev et al., 2000" startWordPosition="2371" endWordPosition="2374">eate, the total number of excerpts r found on the Internet may differ. We label the excerpts ej1 ... ejr. 3.2 Selection Model Our selection model takes the content template t1 ... tk and the candidate excerpts ej1 ... ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 ... ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise. We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously. This approach enables us to maximize both local fit and global cohere</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies. In Proceedings of ANLP/NAACL, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6762" citStr="Reiter and Dale, 2000" startWordPosition="1019" endWordPosition="1023">adth. An advantage of this data set is that Wikipedia articles explicitly delineate topical sections, facilitating structural analysis. The results of our evaluation confirm the benefits of structureaware content selection over approaches that do not explicitly model topical structure. 2 Related Work Concept-to-text generation and text-to-text generation take very different approaches to content selection. In traditional concept-to-text generation, a content planner provides a detailed template for what information should be included in the output and how this information should be organized (Reiter and Dale, 2000). In text-to-text generation, such templates for information organization are not available; sentences are selected based on their salience properties (Mani and Maybury, 1999). While this strategy is robust and portable across domains, output summaries often suffer from coherence and coverage problems. In between these two approaches is work on domain-specific text-to-text generation. Instances of these tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="15875" citStr="Snyder and Barzilay, 2007" startWordPosition="2496" endWordPosition="2499">ancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise. We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously. This approach enables us to maximize both local fit and global coherence. We implement this algorithm using the perceptron framework, as it can be easily modified for structured prediction while preserving convergence guarantees (Daum´e III and Marcu, 2005; Snyder and Barzilay, 2007). In this section, we first describe the structure and decoding procedure of our model. We then present an algorithm to jointly learn the parameters of all topic models. 3.2.1 Model Structure The model inputs are as follows: • The title of the desired document • t1 ... tk — topics from the content template • ej1 ... ejr — candidate excerpts for each topic tj In addition, we define feature and parameter vectors: • 0(ejl) — feature vector for the lth candidate excerpt for topic tj • w1 ... wk — parameter vectors, one for each of the topics t1 ... tk Our model constructs a new article by followin</context>
<context position="21135" citStr="Snyder and Barzilay, 2007" startWordPosition="3402" endWordPosition="3405">of each document, we add the gold excerpt sij to the corresponding vector of candidate excerpts eij1 ... eijr. This excerpt represents the target for our training algorithm. Note that the algorithm does not require annotated ranking data; only knowledge of this “optimal” excerpt is required. However, if the excerpts provided in the training data have low quality, noise is introduced into the system. Training Procedure Our algorithm is a modification of the perceptron ranking algorithm (Collins, 2002), which allows for joint learning across several ranking problems (Daum´e III and Marcu, 2005; Snyder and Barzilay, 2007). Pseudocode for this algorithm is provided in Figure 2. First, we define Rank(eij1 ... eijr, wj), which SIMS count of similar excerpts$ 212 ranks all excerpts from the candidate excerpt vector eij1 ... eijr for document di and topic tj. Excerpts are ordered by scorej(ejl) using the current parameter values. We also define Optimize(eij1 ... eijr), which finds the optimal selection of excerpts (one per topic) given ranked lists of excerpts eij1 ... eijr for each document di and topic tj. These functions follow the ranking and optimization procedures described in Section 3.2.1. The algorithm mai</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In Proceedings of HLT-NAACL, pages 300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph M Weischedel</author>
<author>Jinxi Xu</author>
<author>Ana Licuanan</author>
</authors>
<title>A hybrid approach to answering biographical questions.</title>
<date>2004</date>
<booktitle>In New Directions in Question Answering,</booktitle>
<pages>59--70</pages>
<contexts>
<context position="7792" citStr="Weischedel et al., 2004" startWordPosition="1167" endWordPosition="1170">tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia materia</context>
<context position="24896" citStr="Weischedel et al., 2004" startWordPosition="4035" endWordPosition="4038">representing excerpt eijl Initialization: 1 Forj = 1 ... k 2 Set parameters wj = 0 Training: 3 Repeat until convergence or while iter &lt; itermax: 4 For i = 1 ... n 5 For j = 1 ... k 6 Rank(eij1 ... eijr, wj) 7 x1 ... xk = Optimize(ei11 ... eikr) 8 For j = 1 ... k 9 If sim(xj, sij) &lt; 0.8 10 Figure 2: An algorithm for learning several ranking problems with a joint decoding mechanism. tor reaction to system-produced articles submitted to Wikipedia. Data For evaluation, we consider two domains: American Film Actors and Diseases. These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008). Our text corpus consists of articles drawn from the corresponding categories in Wikipedia. There are 2,150 articles in American Film Actors and 523 articles in Diseases. For each domain, we randomly select 90% of articles for training and test on the remaining 10%. Human-authored articles in both domains contain an average of four topics, and each topic contains an average of 193 words. In order to model the real-world scenario where Wikipedia articles are not always available (as for new or spec</context>
</contexts>
<marker>Weischedel, Xu, Licuanan, 2004</marker>
<rawString>Ralph M. Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A hybrid approach to answering biographical questions. In New Directions in Question Answering, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="8512" citStr="Wu and Weld, 2007" startWordPosition="1272" endWordPosition="1275">d are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text. Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and Strube, 2008). Our focus is on a different task — generation of new overview articles that follow the structure of Wikipedia articles. 209 3 Method The goal of our system is to produce a comprehensive overview article given a title – e.g., Cancer. We assume that relevant information on the subject is available on the Internet but scattered among several pages interspersed with noise. We are provided with a training corpus consisting of n documents d1 ... do in the same domain – e.g., Diseases. Each document di has a title and a set of delineate</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of CIKM, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
<author>Usama Fayyad</author>
</authors>
<title>Hierarchical clustering algorithms for document datasets.</title>
<date>2005</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="12811" citStr="Zhao et al., 2005" startWordPosition="1983" endWordPosition="1986">ore of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 ... him from all documents di using a repeated bisectioning algorithm (Zhao et al., 2005). As a similarity function, we use cosine similarity weighted with TF*IDF. We eliminate any clusters with low internal similarity (i.e., smaller than 0.5), as we assume these are “miscellaneous” clusters that will not yield unified topics. 210 We determine the average number of sections k over all documents in our training set, then select the k largest section clusters as topics. We order these topics as t1 ... tk using a majority ordering algorithm (Cohen et al., 1998). This algorithm finds a total order among clusters that is consistent with a maximal number of pairwise relationships observ</context>
</contexts>
<marker>Zhao, Karypis, Fayyad, 2005</marker>
<rawString>Ying Zhao, George Karypis, and Usama Fayyad. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10(2):141–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>M Ticrea</author>
<author>Eduard Hovy</author>
</authors>
<title>Multidocument biography summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>434--441</pages>
<contexts>
<context position="7590" citStr="Zhou et al., 2004" startWordPosition="1133" endWordPosition="1136">portable across domains, output summaries often suffer from coherence and coverage problems. In between these two approaches is work on domain-specific text-to-text generation. Instances of these tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates. As our experiments demonstrate, patterns observed in domain</context>
<context position="24915" citStr="Zhou et al., 2004" startWordPosition="4039" endWordPosition="4042"> Initialization: 1 Forj = 1 ... k 2 Set parameters wj = 0 Training: 3 Repeat until convergence or while iter &lt; itermax: 4 For i = 1 ... n 5 For j = 1 ... k 6 Rank(eij1 ... eijr, wj) 7 x1 ... xk = Optimize(ei11 ... eikr) 8 For j = 1 ... k 9 If sim(xj, sij) &lt; 0.8 10 Figure 2: An algorithm for learning several ranking problems with a joint decoding mechanism. tor reaction to system-produced articles submitted to Wikipedia. Data For evaluation, we consider two domains: American Film Actors and Diseases. These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008). Our text corpus consists of articles drawn from the corresponding categories in Wikipedia. There are 2,150 articles in American Film Actors and 523 articles in Diseases. For each domain, we randomly select 90% of articles for training and test on the remaining 10%. Human-authored articles in both domains contain an average of four topics, and each topic contains an average of 193 words. In order to model the real-world scenario where Wikipedia articles are not always available (as for new or specialized topics), we</context>
<context position="27074" citStr="Zhou et al., 2004" startWordPosition="4415" endWordPosition="4418">d. This yields a document of comparable size with the output of our system. Despite its simplicity, this baseline is not naive: extracting material from a single document guarantees that the output is coherent, and a page highly ranked by a search engine may readily contain a comprehensive overview of the subject. Our second baseline, No Template, does not use a template to specify desired topics; therefore, there are no constraints on content selection. Instead, we follow a simplified form of previous work on biography creation, where a classifier is trained to distinguish biographical text (Zhou et al., 2004; Biadsy et al., 2008). In this case, we train a classifier to distinguish domain-specific text. Positive training data is drawn from all topics in the given domain corpus. To find negative training data, we perform the search procedure as in our full model (see Section 3.1) using only the article titles as search queries. Any excerpts which have very low similarity to the original articles are used as negative examples. During the decoding procedure, we use the same search procedure. We then classify each excerpt as relevant or irrelevant and select the k non-redundant excerpts with the highe</context>
</contexts>
<marker>Zhou, Ticrea, Hovy, 2004</marker>
<rawString>L. Zhou, M. Ticrea, and Eduard Hovy. 2004. Multidocument biography summarization. In Proceedings of EMNLP, pages 434–441.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>