<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99885">
An Estimate of an Upper Bound for the
Entropy of English
</title>
<author confidence="0.993043">
Peter F. Brown* Stephen A. Della Pietra*
Vincent J. Della Pietra* Jennifer C. Lai*
Robert L. Mercer*
</author>
<affiliation confidence="0.537012">
IBM T.J. Watson Research Center
</affiliation>
<bodyText confidence="0.9971765">
We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed
English, obtained by constructing a word trigram model and then computing the cross-entropy
between this model and a balanced sample of English text. We suggest the well-known and widely
available Brown Corpus of printed English as a standard against which to measure progress in
language modeling and offer our bound as the first of what we hope will be a series of steadily
decreasing bounds.
</bodyText>
<sectionHeader confidence="0.990196" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999864875">
We present an estimate of an upper bound for the entropy of characters in printed
English. The estimate is the cross-entropy of the 5.96 million character Brown Corpus
(Kucera and Francis 1967) as measured by a word trigram language model that we
constructed from 583 million words of training text. We obtain an upper bound of 1.75
bits per character.
Since Shannon&apos;s 1951 paper, there have been a number of estimates of the entropy
of English. Cover and King (1978) list an extensive bibliography. Our approach differs
from previous work in that
</bodyText>
<listItem confidence="0.797829833333333">
1. We use a much larger sample of English text; previous estimates were
based on samples of at most a few hundred letters.
2. We use a language model to approximate the probabilities of character
strings; previous estimates employed human subjects from whom
probabilities were elicited through various clever experiments.
3. We predict all printable ASCII characters.
</listItem>
<sectionHeader confidence="0.800345" genericHeader="method">
2. Method
</sectionHeader>
<bodyText confidence="0.998589333333333">
Our estimate for the entropy bound is based upon the well-known fact that the cross-
entropy of a stochastic process as measured by a model is an upper bound on the
entropy of the process. In this section, we briefly review the relevant notions.
</bodyText>
<subsectionHeader confidence="0.981761">
2.1 Entropy, Cross-Entropy, and Text Compression
</subsectionHeader>
<bodyText confidence="0.933601">
Suppose X = {... X_2, X_1, Xo, X1, X2 . .} is a stationary stochastic process over a finite
alphabet. Let P denote the probability distribution of X and let Ep denote expectations
</bodyText>
<note confidence="0.869499666666667">
* P.O. Box 704, Yorktown Heights, NY 10598
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.95251375">
with respect to P. The entropy of X is defined by
H(X) H(P) -Ep log P(X0 I X-i, X-2, • • •)• (1)
If the base of the logarithm is 2, then the entropy is measured in bits. It can be shown
that H(P) can also be expressed as
</bodyText>
<equation confidence="0.9085896">
H(P) = lim -Ep log P(X0 X-1, X-2, , X_n ) = lim - -1 Ep log P(Xi X2 Xn). (2)
n—■co n-,00 n
If the process is ergodic, then the Shannon-McMillan-Breiman theorem (Algoet and
Cover 1988) states that almost surely
H(P) = lim - log P(Xi X2 . Xn). (3)
</equation>
<bodyText confidence="0.9991438">
Thus, for an ergodic process, an estimate of H(P) can be obtained from a knowledge
of P on a sufficiently long sample drawn randomly according to P.
When P is not known, an upper bound to H(P) can still be obtained from an
approximation to P. Suppose that the stationary stochastic process M is a model for
P. The cross-entropy of P as measured by M is defined by
</bodyText>
<equation confidence="0.940872285714286">
H(P, M) -Ep log M(Xo I X-1, X-2, • •)• (4)
Under suitable regularity conditions, it can be shown that
H(P, M) = nlim -Ep log M(Xo I X-1, X-2, , X_n ) = lirn - IT; Ep log M(X1X2 • • Xn)•
(5)
If P is ergodic, then it can be shown that almost surely for P
H(P, M) = lim — —1 log M(XIX2 • • • Xn).
n—“Do n (6)
</equation>
<bodyText confidence="0.8791795">
The cross-entropy H(P, M) is relevant to us since it is an upper bound on the
entropy H(P). That is, for any model M,
</bodyText>
<equation confidence="0.935775">
H(P) &lt; H(P, M). (7)
</equation>
<bodyText confidence="0.997680666666667">
The difference between H(P, M) and H(P) is a measure of the inaccuracy of the model
M. More accurate models yield better upper bounds on the entropy. Combining Equa-
tions (6) and (7) we see that almost surely for P.
</bodyText>
<equation confidence="0.98002">
1
H(P) &lt; lim - - log M(X1 X2 • • Xn).
co n
</equation>
<bodyText confidence="0.645762">
Entropy and cross-entropy can be understood from the perspective of text com-
pression. It is well known that for any uniquely decodable coding scheme (Cover and
Thomas 1991),
</bodyText>
<equation confidence="0.969035">
Ep 1(XIX2 .X) &gt; -Ep log P (Xi X2 . • • Xn), (9)
</equation>
<bodyText confidence="0.999853333333333">
where 1(Xi X2 • • • Xn) is the number of bits in the encoding of the string Xi X2 • • Xn•
Combining Equations (2) and (9), we see that H(P) is a lower bound on the average
number of bits per symbol required to encode a long string of text drawn from P:
</bodyText>
<equation confidence="0.565504">
H(P) &lt; lim 1Ep 1(Xi X2 • • • Xn)• (10)
00 n
(8)
</equation>
<page confidence="0.991914">
32
</page>
<note confidence="0.821935">
Brown et al. An Estimate of an Upper Bound for the Entropy of English
</note>
<bodyText confidence="0.995564">
On the other hand, an arithmetic coding scheme (Bell, Cleary, and Witten 1990) using
model M will encode the sequence xi x2 ••• xn in
</bodyText>
<equation confidence="0.999209">
1m(x1x2...xn) = r _ log M(xi x2 . xn ) 1 (11)
</equation>
<bodyText confidence="0.996415666666667">
bits, where rr] denotes the smallest integer not less than r. Combining Equations (7)
and (11) we see that H(P,M) is the number of bits per symbol achieved by using
model M to encode a long string of text drawn from P:
</bodyText>
<equation confidence="0.950958">
H(P,M) = lim lm(XiX2 • • • Xn) • (12)
</equation>
<subsectionHeader confidence="0.999799">
2.2 The Entropy Bound
</subsectionHeader>
<bodyText confidence="0.99994775">
We view printed English as a stochastic process over the alphabet of 95 printable ASCII
characters. This alphabet includes, for example, all uppercase and lowercase letters, all
digits, the blank, all punctuation characters, etc. Using Equation (8) we can estimate
an upper bound on the entropy of characters in English as follows:
</bodyText>
<listItem confidence="0.999447666666667">
1. Construct a language model M over finite strings of characters.
2. Collect a reasonably long test sample of English text.
3. Then
</listItem>
<equation confidence="0.976245">
H(English) &lt; —1711 log M(test sample), (13)
</equation>
<bodyText confidence="0.996579363636364">
where n is the number of characters in the sample.
We emphasize that for this paradigm to be reasonable, the language model M
must be constructed without knowledge of the test sample. Without this proscription, one
might, for example, construct a model that assigns probability one to the test sample
and zero to any other character string of the same length. Even quite subtle use of
knowledge of the test sample can have a profound effect on the cross-entropy. For
example, the cross-entropy would be noticeably lower had we restricted ourselves to
characters that appear in the test sample rather than to all printable ASCII characters,
and would be lower still had we used the actual vocabulary of the test sample. But
these values could not be trumpeted as upper bounds to the entropy of English since
Equation (13) would no longer be valid.
</bodyText>
<sectionHeader confidence="0.81455" genericHeader="method">
3. The Language Model
</sectionHeader>
<bodyText confidence="0.999961181818182">
In this section, we describe our language model. The model is very simple: it captures
the structure of English only through token trigram frequencies. Roughly speaking,
the model estimates the probability of a character sequence by dissecting the sequence
into tokens and spaces and computing the probability of the corresponding token
sequence. The situation is slightly more complicated than this since, for a fixed token
vocabulary, some character sequences will not have any such dissection while others
will have several. For example, the sequence abc xyz might not have any dissection
while the sequence bedrock might be dissected as one token or as two tokens without
an intervening space.
We address the difficulty of sequences that cannot be dissected by introducing an
unknown token that can account for any spelling. We address the problem of multiple
</bodyText>
<page confidence="0.986556">
33
</page>
<note confidence="0.283183">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.619543">
dissections by considering the token sequences to be hidden. The model generates a
sequence of characters in four steps:
I. It generates a hidden string of tokens using a token trigram model.
</bodyText>
<listItem confidence="0.996966333333333">
2. It generates a spelling for each token.
3. It generates a case for each spelling.
4. It generates a spacing string to separate cased spellings from one another.
</listItem>
<bodyText confidence="0.99978625">
The final character string consists of the cased spellings separated by the spacing
strings.
The probability of the character string is a sum over all of its dissections of the
joint probability of the string and the dissection:
</bodyText>
<equation confidence="0.83565">
M(character_string) = M(character _string , dissection). (14)
dissections
The joint probability of the string and a dissection is a product of four factors:
M(character _string , dissection) =
Mtoken (tokens) Mspeii(spellings I tokens) Mcase(cased_spellings I spellings, tokens)
Mspace(character _string I cased _spellings , spellings, tokens). (15)
</equation>
<subsectionHeader confidence="0.99956">
3.1 The Token Trigram Model
</subsectionHeader>
<bodyText confidence="0.999954666666667">
The token trigram model is a second-order Markov model that generates a token string
t1t2 • • • tn by generating each token t,, in turn, given the two previous tokens t,_1 and
t,_2. Thus the probability of a string is
</bodyText>
<equation confidence="0.991212">
Mtoken(t1t2 • • • tn) — Mtoken(t1t2) H Mtoken (ti I ti-2t_1) (16)
i=3
</equation>
<bodyText confidence="0.9711665">
The conditional probabilities Mtoken(t3 I ti t) are modeled as a weighted average of
four estimators fi
</bodyText>
<equation confidence="0.995135">
Mtoken(t3 I t2) = A3 (ti t2)f3(t3 I ti t2) + A2(tit2)f2(t3 I t2)+A1(t1t2)f1(t3)+Ao(t1t2)f0, (17)
</equation>
<bodyText confidence="0.9998995">
where the weights A, satisfy E A, = 1 and A, &gt; 0.
The estimators f, and the weights Ai are determined from the training data using a
procedure that is explained in detail by Jelinek and Mercer (1980). Basically, the training
data are divided into a large, primary segment and a smaller, held-out segment. The
estimators f, are chosen to be the conditional frequencies in the primary segment,
while the smoothing weights A, are chosen to fit the combined model to the held-
out segment. In order to decrease the freedom in smoothing, the A, are constrained
to depend on (t1t2) only through the counts c(ti t2) and c(t2) in the primary training
segment. When c(ti t) is large, we expect A3(t1 t2) to be close to 1, since in this case
the trigram frequency in the primary segment should be a reliable estimate of the
</bodyText>
<page confidence="0.994475">
34
</page>
<note confidence="0.906154">
Brown et al. An Estimate of an Upper Bound for the Entropy of English
</note>
<bodyText confidence="0.999105333333333">
frequency in the held-out segment. Similarly, when c(ti t2) is small, but c(t2) is large,
we expect A3(t1 t2) to be close to 0 and )2(t1 t2) to be close to 1.
The token vocabulary consists of
</bodyText>
<listItem confidence="0.99974825">
1. 293,181 spellings, including a separate entry for each punctuation
character;
2. a special unknown_token that accounts for all other spellings;
3. a special sentence_boundary_token that separates sentences.
</listItem>
<subsectionHeader confidence="0.999773">
3.2 The Spelling Model
</subsectionHeader>
<bodyText confidence="0.999983333333333">
The spelling model generates a spelling s1 s2 ... sk given a token. For any token other
than the unknown_token and sentence_boundary_token, the model generates the spelling of
the token. For the sentence_boundary_token, the model generates the null string. Finally,
for the unknown _token, the model generates a character string by first choosing a length
k according to a Poisson distribution, and then choosing k characters independently
and uniformly from the printable ASCII characters. Thus
</bodyText>
<equation confidence="0.995033">
Mspell(5152 . . . Sk I unknown_token) = 1-7-!e-pk,
xk (18)
</equation>
<bodyText confidence="0.9878365">
where A is the average number of characters per token in the training text, 4.1, and
1/p is the number of printable ASCII characters, 95.
</bodyText>
<subsectionHeader confidence="0.997312">
3.3 The Case Model
</subsectionHeader>
<bodyText confidence="0.99999125">
The case model generates a cased spelling given a token, the spelling of the token,
and the previous token. For the unknown _token and sentence_boundary_token, this cased
spelling is the same as the spelling. For all other tokens, the cased spelling is obtained
by modifying the uncased spelling to conform with one of the eight possible patterns
</bodyText>
<equation confidence="0.788302">
L+ ii+ In+ uLuL+ unitL+ um:, uuLIL+ LUL+
</equation>
<bodyText confidence="0.9880345">
Here U denotes an uppercase letter, L a lowercase letter, 11+ a sequence of one or
more uppercase letters, and L+ a sequence of one or more lowercase letters. The case
pattern only affects the 52 uppercase and lowercase letters.
The case pattern C for a token t is generated by a model of the form:
</bodyText>
<equation confidence="0.936056">
Mcase(C I t, b) = A2(t)f (C I t, b) + Xi (t) f (c I b) + Ao(t). (19)
</equation>
<bodyText confidence="0.986146">
Here b is a bit that is 1 if the previous token is the sentence_boundary_token and is 0
otherwise. We use b to model capitalization at the beginning of sentences.
</bodyText>
<subsectionHeader confidence="0.994966">
3.4 The Spacing Model
</subsectionHeader>
<bodyText confidence="0.9993338">
The spacing model generates the spacing string between tokens, which is either null,
a dash, an apostrophe, or one or more blanks. It is generated by an interpolated model
similar to that in Equation (19). The actual spacing that appears between two tokens
should depend on the identity of each token, but in our model we only consider the
dependence on the second token. This simplifies the model, but still allows it to do
</bodyText>
<page confidence="0.990855">
35
</page>
<note confidence="0.495341">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.9377475">
a good job of predicting the null spacing that precedes many punctuation marks. For
strings of blanks, the number of blanks is determined by a Poisson distribution.
</bodyText>
<subsectionHeader confidence="0.96395">
3.5 The Entropy Bound
</subsectionHeader>
<bodyText confidence="0.999656333333333">
According to the paradigm of Section 2.2 (see Equation (13)), we can estimate an
upper bound on the entropy of characters in English by calculating the language
model probability M(character_string) of a long string of English text. For a very long
string it is impractical to calculate this probability exactly, since it involves a sum
over the different hidden dissections of the string. However, for any particular dis-
section M(character _string) &gt; M(character_string, dissection). Moreover, for our model, a
straightforward partition of a character string into tokens usually yields a dissection
for which this inequality is approximately an equality. Thus we settle for the slightly
less sharp bound
</bodyText>
<equation confidence="0.9946695">
1 (20)
H(English) &lt; — —n log M(character_string, dissection)
</equation>
<bodyText confidence="0.999900333333333">
where dissection is provided by a simple finite state tokenizer. By Equation (15), the joint
probability M(characterstring, dissection) is the product of four factors. Consequently,
the upper bound estimate (20) is the sum of four entropies,
</bodyText>
<equation confidence="0.9997295">
H(English) &lt; Htoken(character_string) + Hspell(character _string)
+Hcase(character_string) + Hspaczng(character _string). (21)
</equation>
<sectionHeader confidence="0.994066" genericHeader="method">
4. The Data
</sectionHeader>
<subsectionHeader confidence="0.997622">
4.1 The Test Sample
</subsectionHeader>
<bodyText confidence="0.999989111111111">
We used as a test sample the Brown Corpus of English text (Kucera and Francis 1967).
This well-known corpus was designed to represent a wide range of styles and varieties
of prose. It consists of samples from 500 documents, each of which first appeared in
print in 1961. Each sample is about 2,000 tokens long, yielding a total of 1,014,312
tokens (according to the tokenization scheme used in reference [Kucera and Francis
1967]).
We used the Form C version of the Brown Corpus. Although in this version only
proper names are capitalized, we modified the text by capitalizing the first letter of
every sentence. We also discarded paragraph and segment delimiters.
</bodyText>
<subsectionHeader confidence="0.997597">
4.2 The Training Data
</subsectionHeader>
<bodyText confidence="0.99883225">
We estimated the parameters of our language model from a training text of 583 million
tokens drawn from 18 different sources. We emphasize that this training text does not
include the test sample. The sources of training text are listed in Table 1 and include text
from:
</bodyText>
<listItem confidence="0.9622032">
1. several newspaper and news magazine sources: the Associated Press; the
United Press International (UPI); the Washington Post; and a collection of
magazines published by Time Incorporated;
2. two encyclopedias: Grolier&apos;s Encyclopedia and the McGraw-Hill
Encyclopedia of Science and Technology;
</listItem>
<page confidence="0.998689">
36
</page>
<note confidence="0.97662">
Brown et al. An Estimate of an Upper Bound for the Entropy of English
</note>
<tableCaption confidence="0.999409">
Table 1
</tableCaption>
<table confidence="0.990870142857143">
Training corpora.
Source Millions of words
United Press International 203.768
IBM Depositions 93.210
Canadian Parliament 85.016
Amoco PROFS (OC) 54.853
Washington Post 40.870
APHB 30.194
Associated Press 24.069
IBM Poughkeepsie (OC) 22.140
Time Inc. 10.525
Grolier&apos;s Encyclopedia 8.020
McGraw-Hill Encyclopedia 2.173
IBM Sterling Forest (OC) 1.745
IBM Research (0C) 1.612
Bartlett&apos;s Familiar Quotations 0.489
Congressional Record 0.344
Sherlock Holmes 0.340
Chicago Manual of Style 0.214
World Almanac and Book of Facts 0.173
Total 582.755
</table>
<listItem confidence="0.967242222222222">
3. two literary sources: a collection of novels and magazine articles from
the American Printing House for the Blind (APHB) and a collection of
Sherlock Holmes novels and short stories;
4. several legal and legislative sources: the 1973-1986 proceedings of the
Canadian parliament; a sample issue of the Congressional Record; and
the depositions of a court case involving IBM;
5. office correspondence (OC) from IBM and from Amoco;
6. other miscellaneous sources: Bartlett&apos;s Familiar Quotations, the Chicago
Manual of Style, and The World Almanac and Book of Facts.
</listItem>
<subsectionHeader confidence="0.9991">
4.3 The Token Vocabulary
</subsectionHeader>
<bodyText confidence="0.9983505">
We constructed the token vocabulary by taking the union of a number of lists includ-
ing:
</bodyText>
<listItem confidence="0.996596142857143">
1. two dictionaries;
2. two lists of first and last names: a list derived from the IBM on-line
phone directory, and a list of names we purchased from a marketing
company;
3. a list of place names derived from the 1980 U.S. census;
4. vocabulary lists used in IBM speech recognition and machine translation
experiments.
</listItem>
<page confidence="0.997326">
37
</page>
<note confidence="0.471848">
Computational Linguistics Volume 18, Number 1
</note>
<tableCaption confidence="0.647437666666667">
Table 2
Tokens in the test sample but not in the 293,181-token vocabulary.
Token Occurrences
</tableCaption>
<equation confidence="0.8910698">
1776
*F 1004
Ithrushchey 68
Kohnstamm 35
skywave 31
Prokofieff 28
HeIva 22
patient&apos;s 21
dikkat 21
Podger 21
Katanga 21
ekstrohm 20
Skyros 20
PIP 17
Lalaurie 17
roleplaying 16
Pont&apos;s 15
Fromm&apos;s 15
Hardy&apos;s 15
Helion 14
</equation>
<bodyText confidence="0.9998768">
The resulting vocabulary contains 89.02% of the 44,177 distinct tokens in the Brown
Corpus, and covers 99.09% of 1,014,312-token text. The twenty most frequently occur-
ring tokens in the Brown Corpus not contained in our vocabulary appear in Table 2.
The first two, Jand *F, are codes used in the Brown Corpus to denote formulas and
special symbols.
</bodyText>
<sectionHeader confidence="0.928394" genericHeader="evaluation">
5. Results and Conclusion
</sectionHeader>
<bodyText confidence="0.999983157894737">
The cross-entropy of the Brown Corpus and our model is 1.75 bits per character. Table 3
shows the contributions to this entropy from the token, spelling, case, and spacing
components (see Equation (21)). The main contribution is, of course, from the token
model. The contribution from the spelling model comes entirely from predicting the
spelling of the unknown_token. The model here is especially simple-minded, predicting
each of the 95 printable ASCII characters with equal probability. While we can easily
do better, even if we were able to predict the characters in unknown tokens as well
as we predict those in known tokens, the contribution of the spelling model to the
entropy would decrease by only 0.04 bits. Likewise, we can entertain improvements
to the case and spacing models but any effect on the overall entropy would be small.
Our bound is higher than previous entropy estimates, but it is statistically more
reliable since it is based on a much larger test sample. Previous estimates were nec-
essarily based on very small samples since they relied on human subjects to predict
characters. Quite apart from any issue of statistical significance, however, it is probable
that people predict English text better than the simple model that we have employed
here.
The cross-entropy of a language model and a test sample provides a natural quan-
titative measure of the predictive power of the model. A commonly used measure of
the difficulty of a speech recognition task is the word perplexity of the task (Bahl et
</bodyText>
<page confidence="0.997358">
38
</page>
<note confidence="0.956751">
Brown et al. An Estimate of an Upper Bound for the Entropy of English
</note>
<tableCaption confidence="0.8427">
Table 3
</tableCaption>
<figure confidence="0.847926714285714">
Component contributions to the cross-entropy.
Component Cross-Entropy (bits)
Token 1.61
Spelling 0.08
Case 0.04
Spacing 0.02
Total 1.75
</figure>
<bodyText confidence="0.994304108108108">
al. 1977). The cross-entropy we report here is just the base two logarithm of the char-
acter perplexity of a sample of text with respect to a language model. For a number
of natural language processing tasks, such as speech recognition, machine translation,
handwriting recognition, stenotype transcription, and spelling correction, language
models for which the cross-entropy is lower lead directly to better performance.
We can also think of our cross-entropy as a measure of the compressibility of the
data in the Brown Corpus. The ASCII codd for the characters in the Brown Corpus has 8
bits per character. Because only 95 of the characters are printable, it is a straightforward
matter to reduce this to 7 bits per character. With a simple Huffman code, which allots
bits so that common characters get short bit strings at the expense of rare characters,
we can reach 4.46 bits per character. More exotic compression schemes can reach fewer
bits per character. For example, the standard UNIX command compress, which employs
a Lempel—Ziv scheme, compresses the Brown Corpus to 4.43 bits per character. Miller
and Wegman (1984) have developed an adaptive Lempel—Ziv scheme that achieves
a compression to 4.20 bits per character on the Brown Corpus. Our language model
allows us to reach a compression to 1.75 bits per character.
We do not doubt that one can reduce the cross-entropy below 1.75 bits per charac-
ter. A simple way to do this is to find more reliable estimates of the parameters of the
model by using a larger collection of English text for training. We might also consider
structural changes to the model itself. Our model is static. One can imagine adaptive
models that profit from the text in the early part of the corpus to better predict the
later part. This idea is applicable to the token model and also to the spelling model.
From a loftier perspective, we cannot help but notice that linguistically the trigram
concept, which is the workhorse of our language model, seems almost moronic. It
captures local tactic constraints by sheer force of numbers, but the more well-protected
bastions of semantic, pragmatic, and discourse constraint and even morphological and
global syntactic constraint remain unscathed, in fact unnoticed. Surely the extensive
work on these topics in recent years can be harnessed to predict English better than
we have yet predicted it.
We see this paper as a gauntlet thrown down before the computational linguistics
community. The Brown Corpus is a widely available, standard corpus and the subject
of much linguistic research. By predicting the corpus character by character, we obviate
the need for a common agreement on a vocabulary. Given a model, the computations
required to determine the cross-entropy are within reach for even a modest research
budget. We hope by proposing this standard task to unleash a fury of competitive
energy that will gradually corral the wild and unruly thing that we know the English
language to be.
</bodyText>
<page confidence="0.995289">
39
</page>
<figure confidence="0.468292">
Computational Linguistics Volume 18, Number 1
References
</figure>
<reference confidence="0.999541714285714">
Algoet, P. and Cover, T. (1988). &amp;quot;A sandwich
proof of the Shannon–McMillan–Breiman
theorem.&amp;quot; Annals of Probability
16(2):899-909.
Bahl, L., Baker, J., Jelinek, F., and Mercer, R.
(1977). &amp;quot;Perplexity—a measure of the
difficulty of speech recognition tasks.&amp;quot; In
Program, 94th Meeting of the Acoustical
Society of America 62:S63, Suppl. no. 1.
Bell, T. C., Cleary, J. G., and Witten, I. H.
(1990). Text Compression. Englewood Cliffs,
N.J.: Prentice Hall.
Cover, T., and King, R. (1978). &amp;quot;A
convergent gambling estimate of the
entropy of English.&amp;quot; IEEE Transactions on
Information Theory 24(4):413-421.
Cover, T. M., and Thomas, J. A. (1991).
Elements of Information Theory. New York:
John Wiley.
Jelinek, E, and Mercer, R. L. (1980).
&amp;quot;Interpolated estimation of Markov
source parameters from sparse data.&amp;quot; In
Proceedings, Workshop on Pattern Recognition
in Practice, Amsterdam, The Netherlands.
Kucera, H., and Francis, W. (1967).
Computational Analysis of Present-Day
American English. Providence, R.I.: Brown
University Press.
Miller, V. S., and Wegman, M. N. (1984).
&amp;quot;Variations on a theme by Ziv and
Lempel.&amp;quot; Technical Report RC 10630, IBM
Research Division.
Shannon, C. (1951). &amp;quot;Prediction and entropy
of printed English.&amp;quot; Bell Systems Technical
Journal 30:50-64.
</reference>
<page confidence="0.998633">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932834">
<title confidence="0.9889535">An Estimate of an Upper Bound for the Entropy of English</title>
<author confidence="0.997092">Peter F Brown Stephen A Della Pietra Vincent J Della Pietra Jennifer C Lai Robert L Mercer</author>
<affiliation confidence="0.9951">IBM T.J. Watson Research Center</affiliation>
<abstract confidence="0.993794666666667">We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Algoet</author>
<author>T Cover</author>
</authors>
<title>A sandwich proof of the Shannon–McMillan–Breiman theorem.&amp;quot;</title>
<date>1988</date>
<journal>Annals of Probability</journal>
<pages>16--2</pages>
<contexts>
<context position="2638" citStr="Algoet and Cover 1988" startWordPosition="454" endWordPosition="457"> Let P denote the probability distribution of X and let Ep denote expectations * P.O. Box 704, Yorktown Heights, NY 10598 C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 1 with respect to P. The entropy of X is defined by H(X) H(P) -Ep log P(X0 I X-i, X-2, • • •)• (1) If the base of the logarithm is 2, then the entropy is measured in bits. It can be shown that H(P) can also be expressed as H(P) = lim -Ep log P(X0 X-1, X-2, , X_n ) = lim - -1 Ep log P(Xi X2 Xn). (2) n—■co n-,00 n If the process is ergodic, then the Shannon-McMillan-Breiman theorem (Algoet and Cover 1988) states that almost surely H(P) = lim - log P(Xi X2 . Xn). (3) Thus, for an ergodic process, an estimate of H(P) can be obtained from a knowledge of P on a sufficiently long sample drawn randomly according to P. When P is not known, an upper bound to H(P) can still be obtained from an approximation to P. Suppose that the stationary stochastic process M is a model for P. The cross-entropy of P as measured by M is defined by H(P, M) -Ep log M(Xo I X-1, X-2, • •)• (4) Under suitable regularity conditions, it can be shown that H(P, M) = nlim -Ep log M(Xo I X-1, X-2, , X_n ) = lirn - IT; Ep log M(X</context>
</contexts>
<marker>Algoet, Cover, 1988</marker>
<rawString>Algoet, P. and Cover, T. (1988). &amp;quot;A sandwich proof of the Shannon–McMillan–Breiman theorem.&amp;quot; Annals of Probability 16(2):899-909.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Baker</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Perplexity—a measure of the difficulty of speech recognition tasks.&amp;quot;</title>
<date>1977</date>
<booktitle>In Program, 94th Meeting of the Acoustical Society of America 62:S63, Suppl.</booktitle>
<volume>1</volume>
<marker>Bahl, Baker, Jelinek, Mercer, 1977</marker>
<rawString>Bahl, L., Baker, J., Jelinek, F., and Mercer, R. (1977). &amp;quot;Perplexity—a measure of the difficulty of speech recognition tasks.&amp;quot; In Program, 94th Meeting of the Acoustical Society of America 62:S63, Suppl. no. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression. Englewood Cliffs,</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<location>N.J.:</location>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>Bell, T. C., Cleary, J. G., and Witten, I. H. (1990). Text Compression. Englewood Cliffs, N.J.: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>R King</author>
</authors>
<title>A convergent gambling estimate of the entropy of English.&amp;quot;</title>
<date>1978</date>
<journal>IEEE Transactions on Information Theory</journal>
<pages>24--4</pages>
<contexts>
<context position="1163" citStr="Cover and King (1978)" startWordPosition="194" endWordPosition="197">to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds. 1. Introduction We present an estimate of an upper bound for the entropy of characters in printed English. The estimate is the cross-entropy of the 5.96 million character Brown Corpus (Kucera and Francis 1967) as measured by a word trigram language model that we constructed from 583 million words of training text. We obtain an upper bound of 1.75 bits per character. Since Shannon&apos;s 1951 paper, there have been a number of estimates of the entropy of English. Cover and King (1978) list an extensive bibliography. Our approach differs from previous work in that 1. We use a much larger sample of English text; previous estimates were based on samples of at most a few hundred letters. 2. We use a language model to approximate the probabilities of character strings; previous estimates employed human subjects from whom probabilities were elicited through various clever experiments. 3. We predict all printable ASCII characters. 2. Method Our estimate for the entropy bound is based upon the well-known fact that the crossentropy of a stochastic process as measured by a model is </context>
</contexts>
<marker>Cover, King, 1978</marker>
<rawString>Cover, T., and King, R. (1978). &amp;quot;A convergent gambling estimate of the entropy of English.&amp;quot; IEEE Transactions on Information Theory 24(4):413-421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="3939" citStr="Cover and Thomas 1991" startWordPosition="721" endWordPosition="724"> H(P, M) = lim — —1 log M(XIX2 • • • Xn). n—“Do n (6) The cross-entropy H(P, M) is relevant to us since it is an upper bound on the entropy H(P). That is, for any model M, H(P) &lt; H(P, M). (7) The difference between H(P, M) and H(P) is a measure of the inaccuracy of the model M. More accurate models yield better upper bounds on the entropy. Combining Equations (6) and (7) we see that almost surely for P. 1 H(P) &lt; lim - - log M(X1 X2 • • Xn). co n Entropy and cross-entropy can be understood from the perspective of text compression. It is well known that for any uniquely decodable coding scheme (Cover and Thomas 1991), Ep 1(XIX2 .X) &gt; -Ep log P (Xi X2 . • • Xn), (9) where 1(Xi X2 • • • Xn) is the number of bits in the encoding of the string Xi X2 • • Xn• Combining Equations (2) and (9), we see that H(P) is a lower bound on the average number of bits per symbol required to encode a long string of text drawn from P: H(P) &lt; lim 1Ep 1(Xi X2 • • • Xn)• (10) 00 n (8) 32 Brown et al. An Estimate of an Upper Bound for the Entropy of English On the other hand, an arithmetic coding scheme (Bell, Cleary, and Witten 1990) using model M will encode the sequence xi x2 ••• xn in 1m(x1x2...xn) = r _ log M(xi x2 . xn ) 1 (</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M., and Thomas, J. A. (1991). Elements of Information Theory. New York: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings, Workshop on Pattern Recognition in Practice,</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="8764" citStr="Jelinek and Mercer (1980)" startWordPosition="1561" endWordPosition="1564">erates a token string t1t2 • • • tn by generating each token t,, in turn, given the two previous tokens t,_1 and t,_2. Thus the probability of a string is Mtoken(t1t2 • • • tn) — Mtoken(t1t2) H Mtoken (ti I ti-2t_1) (16) i=3 The conditional probabilities Mtoken(t3 I ti t) are modeled as a weighted average of four estimators fi Mtoken(t3 I t2) = A3 (ti t2)f3(t3 I ti t2) + A2(tit2)f2(t3 I t2)+A1(t1t2)f1(t3)+Ao(t1t2)f0, (17) where the weights A, satisfy E A, = 1 and A, &gt; 0. The estimators f, and the weights Ai are determined from the training data using a procedure that is explained in detail by Jelinek and Mercer (1980). Basically, the training data are divided into a large, primary segment and a smaller, held-out segment. The estimators f, are chosen to be the conditional frequencies in the primary segment, while the smoothing weights A, are chosen to fit the combined model to the heldout segment. In order to decrease the freedom in smoothing, the A, are constrained to depend on (t1t2) only through the counts c(ti t2) and c(t2) in the primary training segment. When c(ti t) is large, we expect A3(t1 t2) to be close to 1, since in this case the trigram frequency in the primary segment should be a reliable est</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, E, and Mercer, R. L. (1980). &amp;quot;Interpolated estimation of Markov source parameters from sparse data.&amp;quot; In Proceedings, Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
<author>W Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<location>Providence, R.I.:</location>
<contexts>
<context position="889" citStr="Kucera and Francis 1967" startWordPosition="146" endWordPosition="149">aracters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds. 1. Introduction We present an estimate of an upper bound for the entropy of characters in printed English. The estimate is the cross-entropy of the 5.96 million character Brown Corpus (Kucera and Francis 1967) as measured by a word trigram language model that we constructed from 583 million words of training text. We obtain an upper bound of 1.75 bits per character. Since Shannon&apos;s 1951 paper, there have been a number of estimates of the entropy of English. Cover and King (1978) list an extensive bibliography. Our approach differs from previous work in that 1. We use a much larger sample of English text; previous estimates were based on samples of at most a few hundred letters. 2. We use a language model to approximate the probabilities of character strings; previous estimates employed human subjec</context>
<context position="13439" citStr="Kucera and Francis 1967" startWordPosition="2342" endWordPosition="2345">is approximately an equality. Thus we settle for the slightly less sharp bound 1 (20) H(English) &lt; — —n log M(character_string, dissection) where dissection is provided by a simple finite state tokenizer. By Equation (15), the joint probability M(characterstring, dissection) is the product of four factors. Consequently, the upper bound estimate (20) is the sum of four entropies, H(English) &lt; Htoken(character_string) + Hspell(character _string) +Hcase(character_string) + Hspaczng(character _string). (21) 4. The Data 4.1 The Test Sample We used as a test sample the Brown Corpus of English text (Kucera and Francis 1967). This well-known corpus was designed to represent a wide range of styles and varieties of prose. It consists of samples from 500 documents, each of which first appeared in print in 1961. Each sample is about 2,000 tokens long, yielding a total of 1,014,312 tokens (according to the tokenization scheme used in reference [Kucera and Francis 1967]). We used the Form C version of the Brown Corpus. Although in this version only proper names are capitalized, we modified the text by capitalizing the first letter of every sentence. We also discarded paragraph and segment delimiters. 4.2 The Training D</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Kucera, H., and Francis, W. (1967). Computational Analysis of Present-Day American English. Providence, R.I.: Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V S Miller</author>
<author>M N Wegman</author>
</authors>
<title>Variations on a theme by Ziv and Lempel.&amp;quot;</title>
<date>1984</date>
<tech>Technical Report RC 10630,</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="19835" citStr="Miller and Wegman (1984)" startWordPosition="3375" endWordPosition="3378">ata in the Brown Corpus. The ASCII codd for the characters in the Brown Corpus has 8 bits per character. Because only 95 of the characters are printable, it is a straightforward matter to reduce this to 7 bits per character. With a simple Huffman code, which allots bits so that common characters get short bit strings at the expense of rare characters, we can reach 4.46 bits per character. More exotic compression schemes can reach fewer bits per character. For example, the standard UNIX command compress, which employs a Lempel—Ziv scheme, compresses the Brown Corpus to 4.43 bits per character. Miller and Wegman (1984) have developed an adaptive Lempel—Ziv scheme that achieves a compression to 4.20 bits per character on the Brown Corpus. Our language model allows us to reach a compression to 1.75 bits per character. We do not doubt that one can reduce the cross-entropy below 1.75 bits per character. A simple way to do this is to find more reliable estimates of the parameters of the model by using a larger collection of English text for training. We might also consider structural changes to the model itself. Our model is static. One can imagine adaptive models that profit from the text in the early part of t</context>
</contexts>
<marker>Miller, Wegman, 1984</marker>
<rawString>Miller, V. S., and Wegman, M. N. (1984). &amp;quot;Variations on a theme by Ziv and Lempel.&amp;quot; Technical Report RC 10630, IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>Prediction and entropy of printed English.&amp;quot;</title>
<date>1951</date>
<journal>Bell Systems Technical Journal</journal>
<pages>30--50</pages>
<marker>Shannon, 1951</marker>
<rawString>Shannon, C. (1951). &amp;quot;Prediction and entropy of printed English.&amp;quot; Bell Systems Technical Journal 30:50-64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>