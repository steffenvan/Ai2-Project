<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000704">
<sectionHeader confidence="0.7642635" genericHeader="method">
USER MODELS AND DISCOURSE MODELS:
UNITED THEY STAND . . .
</sectionHeader>
<author confidence="0.62698">
Alfred Kobsa
</author>
<affiliation confidence="0.606205666666667">
SFB 314: AI-Knowledge-Based Systems
Department of Computer Science
University of Saarbriicken, W. Germany
</affiliation>
<bodyText confidence="0.9863212">
Opinions on the relationship between discourse models
(DMs) and user models (UMs) are obviously influenced
by preassumptions about their respective contents. As
far as DMs are concerned, two divergent views have
been expressed in the discussion published here:
</bodyText>
<listItem confidence="0.503516">
1. The DM contains only representations of the
</listItem>
<bodyText confidence="0.990559">
objects mentioned so far in the discourse (i.e., a
mentioned-object memory—see Schuster, this
issue). The term &amp;quot;object&amp;quot; will be used here in the
broad sense of Schuster, thus also denoting
events, properties, etc.
</bodyText>
<listItem confidence="0.899879833333333">
2. The DM contains in addition
a. a representation of the purpose underlying
the segments of the dialog (i.e. a dialog
purpose—see Grosz Sidner 1986, Chin, this
issue).
b. an attentional structure, which is a subset of
</listItem>
<bodyText confidence="0.999579440677966">
the representations mentioned in (1) contain-
ing the currently focused objects which are
ordered in a focus stack (Cohen, this issue;
Chin, this issue, who requires only that the
user must be familiar with these objects).
Less disagreement seems to exist about the components
of a UM. Generally, it is regarded as containing explicit
representations of the system&apos;s assumptions about all
relevant aspects of the user, i.e., assumptions about his/
her &amp;quot;objective situation&amp;quot; (e.g., marital status, number
of children), as well as about his/her prior knowledge,
goals, plans and false beliefs with respect to the domain
of discourse. In order to meet Wahlster&apos;s personnel-
database counterexample, it must be further required
that the user model be separable by the system from the
rest of the system&apos;s knowledge.
To discuss the relationship between DMs and UMs,
a general belief, goal, and plan maintenance system
(BGP-MS) will be presented here, the purpose of which
is to store and update the beliefs, goals, and plans of
both the system and an arbitrary number of other
agents, including the system&apos;s current user. Specific
subcomponents and subfunctions of this system hope-
fully capture the general consensus on what constitutes
a discourse model and a user model, respectively.
However, we will see that these subcomponents are
strongly interwoven and that—apart from a few rarely
occurring exceptions—the DM is part of the UM at least
at the level of content. The question arises then, of
course, whether it makes sense to separate these no-
tions conceptually.
The belief, goal, and plan maintenance system out-
lined here is being implemented (in a somewhat simpli-
fied form) in XTRA, a natural language access system to
expert systems (Allgayer et al. 1988). A previous imple-
mentation was VIE-DPM (Kobsa 1985a,b). In the
knowledge base of BGP-MS, the representation of the
various types of (nested) beliefs and goals (Kobsa 1988)
is separated into a number of hierarchically ordered
partitions (see Figure 1). If it is shared knowledge
between S and U that U possesses certain beliefs
(knowledge), then this knowledge or these beliefs are
represented in MB(UB).1MB(UW) contains those goals
and plans of the user, MB(SB) those beliefs of the
system, and MR(SW) those goals of the system for
which the same holds true. &amp;quot;Private&amp;quot; beliefs of the
system about the domain of discourse / about the user&apos;s
beliefs / about the user&apos;s beliefs about the system&apos;s
goals are represented in SB, SBUB, and SBUBSW,
respectively. MB contains the mutual beliefs (knowl-
edge) with respect to the domain, and MW the mutual
goals and plans of S and U. The arrows between the
partitions denote inheritance relationships.
In the partitions of BGP-MS, the content of the individ-
ual beliefs, goals, and plans can be expressed through
arbitrary representational structures (e.g., a KL-ONE-
like representation as used in XTRA). Various markers
for non-belief and uncertainty can be added: For in-
stance, in SBUB it can be expressed, among other
</bodyText>
<footnote confidence="0.8475186">
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X!88 /01000-0$03.00
Computational Linguistics, Volume 14, Number 3, September 1988 91
</footnote>
<figure confidence="0.698322">
User Models and Discourse Models: United they stand. . .
Alfred Kobsa
</figure>
<figureCaption confidence="0.9893015">
Figure 1. Hierarchical belief, goal and plan representation
in BGP-MS.
</figureCaption>
<bodyText confidence="0.999623357142857">
things, that S is uncertain whether (or does not believe
that) U knows some fact; and in MB(UB), that S is
uncertain (or does not believe) that a belief of the user
is mutually known.
Where are the user model and the discourse model
located in this architecture? The UM part of BGP-MS
consists of all partitions except SB and SW, plus all
representations in SB (and probably SW) in which an
individual constant occurs denoting the user (the rest of
SB corresponds to Sparck Jones&apos;s world model). The
DM cannot be so easily identified. In the following
sections I will discuss how the different functions of a
discourse model as outlined above can be fulfilled by
the proposed architecture.
</bodyText>
<sectionHeader confidence="0.96597" genericHeader="method">
1. THE MENTIONED OBJECT MEMORY
</sectionHeader>
<bodyText confidence="0.991451861538462">
When an object is mentioned during the discourse, then
mutual knowledge about its existence is usually estab-
lished. Thus a representation of the object can be
entered into MB (implying, for instance, that S can now
use definite NPs to refer to these objects). Finer dis-
tinctions in what is now known can also be expressed:
to take Chin&apos;s example, if S mentions a name unknown
to U (i.e., the relationship between the name and its
bearer is contained in SB only), then the existence of a
person with this name can be entered into MB. In
MR(UB) it can be represented that U does not know,
and in MB(SB) that S does know the bearer of this
name.
All of the mentioned partitions are part of the user
model. One might argue that the above architecture may
completely cover the mentioned object memory func-
tion in an NL dialog system. However, three kinds of
information are lost thereby (this defect also seems to
apply partly to Schuster&apos;s model).
a. The information that objects have been explicitly
mentioned in the discourse:
MB contains not only those objects which were
explicitly mentioned in the discourse (and which
are therefore mutually known by all dialog partic-
ipants), but also representations of those objects
whose existence is mutually known due to stereo-
types (Rich 1988) or to inferences from the dis-
course. Sometimes, however, the system should
possess information about whether or not some
object had been explicitly mentioned, for example
in order to increase coherency in its own dialog
contributions (&amp;quot;As 1/you said before. . . &amp;quot;) or to
point out inconsistencies in dialog contributions of
the user (&amp;quot;But previously you said. . .&amp;quot;).
b. Information about the sequence (and thus recency)
of objects&apos; mention:
This information is very important in NL systems,
since the choice of various forms of anaphora
depends on the degree of recency.
c. Information about the linguistic structure of dialog
contributions:
Sometimes the system should also possess infor-
mation about the wording or the syntactic struc-
ture of the user&apos;s and the system&apos;s previous dialog
contributions (i.e., information on how objects
have been mentioned). This information can be
exploited by the system for reiterating a descrip-
tion in its own dialog contributions or for avoiding
reiteration, for instance.
In XTRA, two additional knowledge bases have been
introduced which serve the above-mentioned functions,
among others: the FSS knowledge base and the so-
called Linguistic Dialog Memory. The FSS (Allgayer
and Reddig 1986) represents the functional semantic
structure of both the user&apos;s and the system&apos;s dialog
contributions. FSS contents can also be linked to the
linguistic surface forms (NPs, PPs, etc.) which caused
their creation (in the case of user input) or became their
linguistic realizations (in the case of system dialog
contributions). The dialog memory, among other things,
records the objects that have been mentioned during the
discourse, and, in its dialog sequence part, the sequence
of the objects&apos; mention.
In general, all system knowledge about what objects
have been mentioned in the on-going discourse, in what
</bodyText>
<figure confidence="0.77837175">
MW
MB (UW)
MB (SW)
SW SBUW SBUBSW
SB
MB
SBUB
MB (UB)
SBUBSB
MB (SB)
92 Computational Linguistics, Volume 14, Number 3, September 1988
Alfred Kobsa User Models and Discourse Models: United they stand . . .
</figure>
<bodyText confidence="0.999915294117647">
order they were mentioned, and how they were de-
scribed (i.e. all parts of the dialog memory) are regarded
as being part of MB, and hence part of the user model.
This is necessarily so, since, by definition, MB contains
all knowledge that is shared by system and user. And
only if knowledge about the previous discourse is
shared between both participants can it be safely em-
ployed in the generation of dialog contributions. (For
example, an anaphor generated by the system will
probably fail to fulfill its referential function if only the
system, but not the user, believes that its intended
referent has been mentioned just recently. Hence the
system should check whether its records in the dialog
sequence memory are shared by the user.)
In cases of communicative failure, however, there
exists system discourse knowledge that is not shared by
the user, and thus not part of MB and the user model. In
such cases, entries are made into SB instead, and
thereby form part of the system&apos;s knowledge only. For
instance, when S assumes that U does not remember
what has been said (see Wahlster&apos;s hastily-presented-
names example), the FSS descriptions and the repre-
sentations of their referents in the dialog memory can be
entered in SB instead of MB, and thus do not form part
of the user model. In addition, however, all sorts of
uncertain assumptions about what the user has or has
not, in fact, kept in mind can be expressed in SBUB or
MB(UB), i.e., in the user model. (For example, the
system can note in the user model that the user probably
remembered the first two but not the subsequent
names.) For simplicity, however, neither of the de-
scribed cases of communicative failure is dealt with in
XTRA, and for implementational reasons the FSS part
of MB forms a separate partition.
</bodyText>
<sectionHeader confidence="0.563995" genericHeader="method">
2A. THE DIALOG PURPOSE
</sectionHeader>
<bodyText confidence="0.9999334">
Here the problem arises in research on NL dialog
systems as to whether or not one should regard the
intentional structure of individual utterances (Cohen,
this issue) or dialog segments (Grosz Sidner 1986) as
being independent of the dialog setting and the dialog
participants. To put it in a more provocative way, do
dialog constituents or dialog participants have an inten-
tional structure? In my view, the essence of problem-
solving dialog lies in the recognition of the dialog
participants&apos; goals and plans, and in the construction of
mutually known goals and plans. Dialog contributions
of the dialog partner serve as a more-or-less helpful aid
in this process (Pollack et al. (1982) present transcripts
in which dialog contributions of clients with misconcep-
tions even impair the recognition of their actual goals).
Apart from that, no intentional character pertains to
dialog constituents that is independent of the dialog and
situational context and of the current (beliefs about)
goals and plans.
In the BGP-MS philosophy, (beliefs about) goals and
plans are contained in those partitions whose labels
include a W. A user&apos;s dialog contribution is first repre-
sented by FSS structures in MB. If user plans or goals
can be inferred by S, they are represented in MB(UW),
or in MW if there is mutual knowledge that they have
been accepted by S. Conversely, when S gradually
communicates its plans or goals to U, the corresponding
representation structures are transferred from SW to
MB(SW), and finally hopefully to MW. Thus any sort of
intentional structure is part of the user model.
</bodyText>
<subsectionHeader confidence="0.451642">
2B. THE ATTENTIONAL STRUCTURE
</subsectionHeader>
<bodyText confidence="0.999823222222222">
I agree with Chin (this issue) that the attentional struc-
ture is also not a context-independent characteristic of
discourse (although Chin&apos;s notion of attentional struc-
ture seems to be broader than mine). Only mutually
known objects can be in focus. In XTRA, focus is
expressed by focus values in the dialog memory which,
logically, can only be applied to representation struc-
tures in MB and MW and therefore form part of the user
model.
</bodyText>
<sectionHeader confidence="0.858658" genericHeader="conclusions">
SUMMARY
</sectionHeader>
<bodyText confidence="0.98647446969697">
The above discussion demonstrates that the function of
a UM and of all mentioned DM components can be
completely fulfilled by the outlined belief, goal, and plan
maintenance system. I cannot deal here in detail with
the question of whether this is also the case for other
components that have been proposed for a DM, for
example, the structuring of the dialog into dialog seg-
ments (Grosz Sidner 1986), a context space grammar
(Reichman 1981), or rhetorical predicates (schemata;
McKeown 1982). With respect to the analyzed compo-
nents, we have seen that the discourse model almost
completely overlaps with the user model at the level of
content. Only if the user does not fully catch the
system&apos;s dialog contributions are entries in the DM
created which do not form part of the UM (see, for
instance, Wahlster&apos;s example, this issue). But at a
procedural level as well, only a few processes can be
found which operate exclusively on that part of the user
model that is identical with the discourse model, or
upon the remaining parts of the user model.
This large degree to which the DM is included in the
UM, however, is not surprising: Discourse models are
ultimately based on linguistic conventions. In order for
the linguistic, intentional, attentional, etc., structure of
the previous discourse to be exploited for future dialog
contributions, conventions about what the structure of a
particular ongoing dialog actually is must exist. Knowl-
edge about convention is mutual knowledge, however,
(Lewis 1969, Schiffer 1972), and thus part of MB. The
same holds true for the above-mentioned additional
components of the DM that could not be dealt with in
this paper. And, by the way, it also holds true for the
grammar the system employs (but see the opposing
views of Monk and Wahlster, this issue). If the system
did not assume that its assumptions about the syntactic
structure of language (as expressed in its grammar) be
Computational Linguistics, Volume 14, Number 3, September 1988 93
Alfred Kobsa &apos;Jser Models and Discourse Models: United they stand. . .
shared by the user, it could not justifiably use it in the
analysis and generation of dialog contributions without
risking miscommunication. And there definitely exists
work in user modeling (e.g., Schuster 1985, Kilbury
1986, Lehman Carbonell 1988), which is concerned with
the recognition of those parts of a user&apos;s idiosyncratic
grammar that deviate from the mutually shared kernel
grammar. Of course, an entry in MB never means that
the system assumes that the user &amp;quot;has the same struc-
ture in his/her mind&amp;quot; (e.g., ATNs, KL-ONE, or LISP),
but only that these structures are functionally equiva-
lent reconstructions of the user&apos;s competence.
Does the large degree of inclusion of discourse mod-
els in user models at the level of content imply that the
notion of discourse model is superflous? As was pointed
out by Monk (this issue), extensionally overlapping
notions may still prove useful if their intension high-
lights different aspects of a system. For example, in the
above architecture, such a concept might characterize
an orthogonal substructure and denote, for instance,
entries in different partitions with specific origin or
function. The above as well as Monk&apos;s and partly
Wahlster&apos;s discussions demonstrate, however, that it is
very hard to find such differential criteria for DMs. I
therefore suspect that a happy fate of that kind will
more probably apply to notions such as mentioned
object memory or discourse sequence memory than to
the vague notion of discourse model.
</bodyText>
<sectionHeader confidence="0.997143" genericHeader="acknowledgments">
ACKNOWLEDGEMENT
</sectionHeader>
<bodyText confidence="0.75875375">
This research was supported by the German Science Foundation in its
Special Collaborative Programme on AI and Knowledge-Based Sys-
tems (SFB 314). I am indebted to Carola Reddig and Norbert
Reithinger for their comments on an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.997963" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997081428571428">
Allgayer, J. and Reddig, C. 1986 Processing Descriptions Containing
Words and Gestures: A System Architecture. In Rollinger, C. R.
and Horn, W. (eds.) GWAI-86 und 2. Osterreichische Artificial-
Intelligence-Tagung. Springer, Verlag, Berlin—New York.
Allgayer, J.; Harbusch, K.; Kobsa, A.; Reddig, C.; Reithinger, N.;
Schmauks, D. 1988 XTRA: A Natural-Language Access System to
Expert Systems. Technical Report, SFB 314: AI-Knowledge-
Based Systems, Department of Computer Science, University of
Saarbriicken, W. Germany.
Kilbury, J. 1986 Language Variation, Parsing, and the Modelling of
User&apos;s Language Variations. In Proceedings of the 7th European
Conference on Artificial Intelligence, Brighton, England: 29-32.
Kobsa, A. I985a Benutzermodellierung in Dialogsystemen. Springer-
Verlag, Berlin—New York.
Kobsa, A. 1985b Using Situation Descriptions and Russellian Atti-
tudes for Representing Beliefs and Wants. In Proceedings of the
International Joint Conference on Artificial Intelligence, Los
Angeles, CA: 513-515.
Kobsa. A. 1988 A Taxonomy of Beliefs and Goals for User Models in
Dialog Systems. In Kobsa, A. and Wahlster, W. (eds.), User
Models in Dialog Systems. Springer-Verlag, Berlin—New York.
Lehman, J. F. and Carbonell, J. G. 1988 Learning the User&apos;s Lan-
guage: A Step Towards Automated Creation of User Models. In
Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog
Systems. Springer-Verlag, Berlin—New York.
Lewis, D. K. 1969 Convention: A Philosophical Study. Harvard
University Press, Cambridge, MA.
McKeown, K. R. 1982 Generating Natural Language Responses to
Questions about Database Structure. TR MS-CIS-82-5, Depart-
ment of Computer and Information Science, University of Penn-
sylvania, Philadelphia, PA.
Pollack, M.E.; Hirschberg, J.; and Webber, B. 1982 User Participa-
tion in the Reasoning Process of Expert Systems. MS CIS-82-9,
Department of Computer and Information Science, University of
Pennsylvania, Philadelphia, PA.
Reichman, R. 1981 Plain Speaking: A Theory and Grammar of
Spontaneous Discourse. Report No. 4681, Bolt, Beranek and
Newman, Cambridge, MA.
Rich, E. 1988 Stereotypes and User Modeling. In Kobsa, A. and
Wahlster, W. (eds.), User Models in Dialog Systems. Springer-
Verlag, Berlin—New York.
Schiffer, S. R. 1972 Meaning. Clarendon Press, Oxford, England.
Schuster, E. 1985 Grammars as User Models. In Proceedings of the
International Joint Conference on Artificial Intelligence, IJCAI-
85, Los Angeles, CA: 20-22.
NOTE
1. The abbreviations are mnemonic: read &amp;quot;system believes&amp;quot; for
&amp;quot;SB&amp;quot;, &amp;quot;system wants&amp;quot; for &amp;quot;SW&amp;quot;, &amp;quot;user believes&amp;quot; for &amp;quot;UB&amp;quot;,
&amp;quot;mutual belief&amp;quot; for &amp;quot;MB&amp;quot;, etc.
</reference>
<page confidence="0.98926">
94 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.8471115">USER MODELS AND DISCOURSE UNITED THEY STAND . . .</title>
<author confidence="0.909097">Alfred Kobsa</author>
<email confidence="0.791029">SFB314:AI-Knowledge-Based</email>
<affiliation confidence="0.992755">Department of Computer Science University of Saarbriicken, W. Germany</affiliation>
<abstract confidence="0.9613790875">Opinions on the relationship between discourse models (DMs) and user models (UMs) are obviously influenced by preassumptions about their respective contents. As far as DMs are concerned, two divergent views have been expressed in the discussion published here: 1. The DM contains only representations of the objects mentioned so far in the discourse (i.e., a memory—see this issue). The term &amp;quot;object&amp;quot; will be used here in the broad sense of Schuster, thus also denoting events, properties, etc. The DM contains addition a. a representation of the purpose underlying segments of the dialog (i.e. a Sidner 1986, Chin, this issue). an structure, is a subset of the representations mentioned in (1) containing the currently focused objects which are ordered in a focus stack (Cohen, this issue; Chin, this issue, who requires only that the user must be familiar with these objects). Less disagreement seems to exist about the components of a UM. Generally, it is regarded as containing explicit representations of the system&apos;s assumptions about all relevant aspects of the user, i.e., assumptions about his/ her &amp;quot;objective situation&amp;quot; (e.g., marital status, number of children), as well as about his/her prior knowledge, goals, plans and false beliefs with respect to the domain of discourse. In order to meet Wahlster&apos;s personneldatabase counterexample, it must be further required that the user model be separable by the system from the rest of the system&apos;s knowledge. To discuss the relationship between DMs and UMs, a general belief, goal, and plan maintenance system (BGP-MS) will be presented here, the purpose of which is to store and update the beliefs, goals, and plans of both the system and an arbitrary number of other agents, including the system&apos;s current user. Specific subcomponents and subfunctions of this system hopefully capture the general consensus on what constitutes a discourse model and a user model, respectively. However, we will see that these subcomponents are strongly interwoven and that—apart from a few rarely occurring exceptions—the DM is part of the UM at least at the level of content. The question arises then, of course, whether it makes sense to separate these notions conceptually. The belief, goal, and plan maintenance system outlined here is being implemented (in a somewhat simplified form) in XTRA, a natural language access system to expert systems (Allgayer et al. 1988). A previous implementation was VIE-DPM (Kobsa 1985a,b). In the knowledge base of BGP-MS, the representation of the various types of (nested) beliefs and goals (Kobsa 1988) is separated into a number of hierarchically ordered partitions (see Figure 1). If it is shared knowledge between S and U that U possesses certain beliefs (knowledge), then this knowledge or these beliefs are in contains those goals and plans of the user, MB(SB) those beliefs of the system, and MR(SW) those goals of the system for which the same holds true. &amp;quot;Private&amp;quot; beliefs of the system about the domain of discourse / about the user&apos;s beliefs / about the user&apos;s beliefs about the system&apos;s goals are represented in SB, SBUB, and SBUBSW, respectively. MB contains the mutual beliefs (knowledge) with respect to the domain, and MW the mutual goals and plans of S and U. The arrows between the partitions denote inheritance relationships. In the partitions of BGP-MS, the content of the individual beliefs, goals, and plans can be expressed through arbitrary representational structures (e.g., a KL-ONElike representation as used in XTRA). Various markers for non-belief and uncertainty can be added: For instance, in SBUB it can be expressed, among other 1988 by Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided the copies are not made for direct commercial advantage and the and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X!88 /01000-0$03.00 Computational Linguistics, Volume 14, Number 3, September 1988 91 User Models and Discourse Models: United they stand. . . Alfred Kobsa 1. belief, goal and plan representation in BGP-MS. things, that S is uncertain whether (or does not believe that) U knows some fact; and in MB(UB), that S is uncertain (or does not believe) that a belief of the user is mutually known. Where are the user model and the discourse model located in this architecture? The UM part of BGP-MS consists of all partitions except SB and SW, plus all representations in SB (and probably SW) in which an individual constant occurs denoting the user (the rest of SB corresponds to Sparck Jones&apos;s world model). The DM cannot be so easily identified. In the following sections I will discuss how the different functions of a discourse model as outlined above can be fulfilled by the proposed architecture. MENTIONED OBJECT MEMORY When an object is mentioned during the discourse, then mutual knowledge about its existence is usually established. Thus a representation of the object can be entered into MB (implying, for instance, that S can now use definite NPs to refer to these objects). Finer distinctions in what is now known can also be expressed: to take Chin&apos;s example, if S mentions a name unknown to U (i.e., the relationship between the name and its bearer is contained in SB only), then the existence of a person with this name can be entered into MB. In MR(UB) it can be represented that U does not know, and in MB(SB) that S does know the bearer of this name. All of the mentioned partitions are part of the user model. One might argue that the above architecture may completely cover the mentioned object memory function in an NL dialog system. However, three kinds of information are lost thereby (this defect also seems to apply partly to Schuster&apos;s model). a. The information that objects have been explicitly mentioned in the discourse: MB contains not only those objects which were explicitly mentioned in the discourse (and which are therefore mutually known by all dialog participants), but also representations of those objects existence is mutually known due to stereo- 1988) or to the discourse. Sometimes, however, the system should possess information about whether or not some object had been explicitly mentioned, for example in order to increase coherency in its own dialog contributions (&amp;quot;As 1/you said before. . . &amp;quot;) or to point out inconsistencies in dialog contributions of the user (&amp;quot;But previously you said. . .&amp;quot;). b. Information about the sequence (and thus recency) of objects&apos; mention: This information is very important in NL systems, since the choice of various forms of anaphora depends on the degree of recency. c. Information about the linguistic structure of dialog contributions: Sometimes the system should also possess information about the wording or the syntactic structure of the user&apos;s and the system&apos;s previous dialog (i.e., information on have been mentioned). This information can be exploited by the system for reiterating a description in its own dialog contributions or for avoiding reiteration, for instance. In XTRA, two additional knowledge bases have been introduced which serve the above-mentioned functions, among others: the FSS knowledge base and the so- Dialog Memory. FSS (Allgayer and Reddig 1986) represents the functional semantic structure of both the user&apos;s and the system&apos;s dialog contributions. FSS contents can also be linked to the linguistic surface forms (NPs, PPs, etc.) which caused their creation (in the case of user input) or became their linguistic realizations (in the case of system dialog contributions). The dialog memory, among other things, records the objects that have been mentioned during the discourse, and, in its dialog sequence part, the sequence of the objects&apos; mention. In general, all system knowledge about what objects have been mentioned in the on-going discourse, in what MW MB (UW) MB (SW) SW SBUW SBUBSW SB MB SBUB MB (UB) SBUBSB MB (SB) Linguistics, Volume 14, Number 3, September 1988 Kobsa User Models and Discourse Models: United they . . . order they were mentioned, and how they were described (i.e. all parts of the dialog memory) are regarded as being part of MB, and hence part of the user model. This is necessarily so, since, by definition, MB contains all knowledge that is shared by system and user. And only if knowledge about the previous discourse is shared between both participants can it be safely employed in the generation of dialog contributions. (For example, an anaphor generated by the system will probably fail to fulfill its referential function if only the system, but not the user, believes that its intended referent has been mentioned just recently. Hence the system should check whether its records in the dialog sequence memory are shared by the user.) In cases of communicative failure, however, there exists system discourse knowledge that is not shared by the user, and thus not part of MB and the user model. In such cases, entries are made into SB instead, and thereby form part of the system&apos;s knowledge only. For instance, when S assumes that U does not remember what has been said (see Wahlster&apos;s hastily-presentednames example), the FSS descriptions and the representations of their referents in the dialog memory can be entered in SB instead of MB, and thus do not form part of the user model. In addition, however, all sorts of uncertain assumptions about what the user has or has not, in fact, kept in mind can be expressed in SBUB or MB(UB), i.e., in the user model. (For example, the system can note in the user model that the user probably remembered the first two but not the subsequent names.) For simplicity, however, neither of the described cases of communicative failure is dealt with in XTRA, and for implementational reasons the FSS part of MB forms a separate partition. 2A. THE DIALOG PURPOSE Here the problem arises in research on NL dialog systems as to whether or not one should regard the intentional structure of individual utterances (Cohen, this issue) or dialog segments (Grosz Sidner 1986) as being independent of the dialog setting and the dialog participants. To put it in a more provocative way, do dialog constituents or dialog participants have an intentional structure? In my view, the essence of problemsolving dialog lies in the recognition of the dialog participants&apos; goals and plans, and in the construction of mutually known goals and plans. Dialog contributions of the dialog partner serve as a more-or-less helpful aid in this process (Pollack et al. (1982) present transcripts in which dialog contributions of clients with misconceptions even impair the recognition of their actual goals). Apart from that, no intentional character pertains to dialog constituents that is independent of the dialog and situational context and of the current (beliefs about) goals and plans. In the BGP-MS philosophy, (beliefs about) goals and plans are contained in those partitions whose labels include a W. A user&apos;s dialog contribution is first represented by FSS structures in MB. If user plans or goals can be inferred by S, they are represented in MB(UW), or in MW if there is mutual knowledge that they have been accepted by S. Conversely, when S gradually communicates its plans or goals to U, the corresponding representation structures are transferred from SW to MB(SW), and finally hopefully to MW. Thus any sort of intentional structure is part of the user model. 2B. THE ATTENTIONAL STRUCTURE I agree with Chin (this issue) that the attentional structure is also not a context-independent characteristic of discourse (although Chin&apos;s notion of attentional structure seems to be broader than mine). Only mutually known objects can be in focus. In XTRA, focus is expressed by focus values in the dialog memory which, logically, can only be applied to representation structures in MB and MW and therefore form part of the user model. SUMMARY The above discussion demonstrates that the function of a UM and of all mentioned DM components can be completely fulfilled by the outlined belief, goal, and plan maintenance system. I cannot deal here in detail with the question of whether this is also the case for other components that have been proposed for a DM, for example, the structuring of the dialog into dialog segments (Grosz Sidner 1986), a context space grammar (Reichman 1981), or rhetorical predicates (schemata; McKeown 1982). With respect to the analyzed components, we have seen that the discourse model almost completely overlaps with the user model at the level of content. Only if the user does not fully catch the system&apos;s dialog contributions are entries in the DM created which do not form part of the UM (see, for instance, Wahlster&apos;s example, this issue). But at a procedural level as well, only a few processes can be found which operate exclusively on that part of the user model that is identical with the discourse model, or upon the remaining parts of the user model. This large degree to which the DM is included in the UM, however, is not surprising: Discourse models are ultimately based on linguistic conventions. In order for the linguistic, intentional, attentional, etc., structure of the previous discourse to be exploited for future dialog contributions, conventions about what the structure of a particular ongoing dialog actually is must exist. Knowledge about convention is mutual knowledge, however, (Lewis 1969, Schiffer 1972), and thus part of MB. The same holds true for the above-mentioned additional components of the DM that could not be dealt with in this paper. And, by the way, it also holds true for the grammar the system employs (but see the opposing views of Monk and Wahlster, this issue). If the system did not assume that its assumptions about the syntactic structure of language (as expressed in its grammar) be Computational Linguistics, Volume 14, Number 3, September 1988 93 Alfred Kobsa &apos;Jser Models and Discourse Models: United they stand. . . shared by the user, it could not justifiably use it in the analysis and generation of dialog contributions without risking miscommunication. And there definitely exists work in user modeling (e.g., Schuster 1985, Kilbury 1986, Lehman Carbonell 1988), which is concerned with the recognition of those parts of a user&apos;s idiosyncratic grammar that deviate from the mutually shared kernel grammar. Of course, an entry in MB never means that the system assumes that the user &amp;quot;has the same structure in his/her mind&amp;quot; (e.g., ATNs, KL-ONE, or LISP), but only that these structures are functionally equivalent reconstructions of the user&apos;s competence. Does the large degree of inclusion of discourse models in user models at the level of content imply that the notion of discourse model is superflous? As was pointed out by Monk (this issue), extensionally overlapping notions may still prove useful if their intension highlights different aspects of a system. For example, in the above architecture, such a concept might characterize an orthogonal substructure and denote, for instance, entries in different partitions with specific origin or function. The above as well as Monk&apos;s and partly Wahlster&apos;s discussions demonstrate, however, that it is very hard to find such differential criteria for DMs. I therefore suspect that a happy fate of that kind will more probably apply to notions such as mentioned object memory or discourse sequence memory than to the vague notion of discourse model. ACKNOWLEDGEMENT</abstract>
<note confidence="0.941177333333333">This research was supported by the German Science Foundation in its Collaborative Programme on Knowledge-Based Systems (SFB 314). I am indebted to Carola Reddig and Norbert Reithinger for their comments on an earlier version of this paper. REFERENCES Allgayer, J. and Reddig, C. 1986 Processing Descriptions Containing Words and Gestures: A System Architecture. In Rollinger, C. R. Horn, W. (eds.) und 2. Osterreichische Artificial- Verlag, Berlin—New York. Allgayer, J.; Harbusch, K.; Kobsa, A.; Reddig, C.; Reithinger, N.; D. 1988 A Natural-Language Access System to Systems. Report, SFB 314: AI-Knowledge- Based Systems, Department of Computer Science, University of Saarbriicken, W. Germany. Kilbury, J. 1986 Language Variation, Parsing, and the Modelling of Language Variations. In of the 7th European on Artificial Intelligence, England: 29-32. A. I985a in Dialogsystemen. Springer- Verlag, Berlin—New York. Kobsa, A. 1985b Using Situation Descriptions and Russellian Attitudes for Representing Beliefs and Wants. In Proceedings of the International Joint Conference on Artificial Intelligence, Los Angeles, CA: 513-515. Kobsa. A. 1988 A Taxonomy of Beliefs and Goals for User Models in</note>
<author confidence="0.721241">In Kobsa</author>
<author confidence="0.721241">A</author>
<author confidence="0.721241">W Wahlster</author>
<affiliation confidence="0.887457">in Dialog Systems. Berlin—New York.</affiliation>
<address confidence="0.868612">Lehman, J. F. and Carbonell, J. G. 1988 Learning the User&apos;s Lan-</address>
<title confidence="0.6900495">guage: A Step Towards Automated Creation of User Models. In A. and Wahlster, W. (eds.), Models in Dialog</title>
<address confidence="0.513577">Berlin—New York.</address>
<note confidence="0.91949525">D. K. 1969 A Philosophical Study. University Press, Cambridge, MA. K. R. 1982 Natural Language Responses to about Database Structure. MS-CIS-82-5, Depart-</note>
<affiliation confidence="0.949847">ment of Computer and Information Science, University of Penn-</affiliation>
<address confidence="0.636686666666667">sylvania, Philadelphia, PA. Pollack, M.E.; Hirschberg, J.; and Webber, B. 1982 User Participation in the Reasoning Process of Expert Systems. MS CIS-82-9,</address>
<affiliation confidence="0.913884">Department of Computer and Information Science, University of</affiliation>
<address confidence="0.904672">Pennsylvania, Philadelphia, PA.</address>
<note confidence="0.800364">R. 1981 Speaking: A Theory and Grammar of Discourse. No. 4681, Bolt, Beranek and Newman, Cambridge, MA. Rich, E. 1988 Stereotypes and User Modeling. In Kobsa, A. and W. (eds.), Models in Dialog Systems. Springer- Verlag, Berlin—New York. S. R. 1972 Press, Oxford, England. Schuster, E. 1985 Grammars as User Models. In Proceedings of the Joint Conference on Artificial Intelligence, IJCAI- 85, Los Angeles, CA: 20-22. NOTE 1. The abbreviations are mnemonic: read &amp;quot;system believes&amp;quot; for &amp;quot;SB&amp;quot;, &amp;quot;system wants&amp;quot; for &amp;quot;SW&amp;quot;, &amp;quot;user believes&amp;quot; for &amp;quot;UB&amp;quot;, &amp;quot;mutual belief&amp;quot; for &amp;quot;MB&amp;quot;, etc. 94 Computational Linguistics, Volume 14, Number 3, September 1988</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allgayer</author>
<author>C Reddig</author>
</authors>
<title>Processing Descriptions Containing Words and Gestures: A System Architecture.</title>
<date>1986</date>
<booktitle>GWAI-86 und 2. Osterreichische ArtificialIntelligence-Tagung.</booktitle>
<editor>In Rollinger, C. R. and Horn, W. (eds.)</editor>
<publisher>Springer, Verlag,</publisher>
<location>Berlin—New York.</location>
<contexts>
<context position="7765" citStr="Allgayer and Reddig 1986" startWordPosition="1255" endWordPosition="1258">tic structure of dialog contributions: Sometimes the system should also possess information about the wording or the syntactic structure of the user&apos;s and the system&apos;s previous dialog contributions (i.e., information on how objects have been mentioned). This information can be exploited by the system for reiterating a description in its own dialog contributions or for avoiding reiteration, for instance. In XTRA, two additional knowledge bases have been introduced which serve the above-mentioned functions, among others: the FSS knowledge base and the socalled Linguistic Dialog Memory. The FSS (Allgayer and Reddig 1986) represents the functional semantic structure of both the user&apos;s and the system&apos;s dialog contributions. FSS contents can also be linked to the linguistic surface forms (NPs, PPs, etc.) which caused their creation (in the case of user input) or became their linguistic realizations (in the case of system dialog contributions). The dialog memory, among other things, records the objects that have been mentioned during the discourse, and, in its dialog sequence part, the sequence of the objects&apos; mention. In general, all system knowledge about what objects have been mentioned in the on-going discour</context>
</contexts>
<marker>Allgayer, Reddig, 1986</marker>
<rawString>Allgayer, J. and Reddig, C. 1986 Processing Descriptions Containing Words and Gestures: A System Architecture. In Rollinger, C. R. and Horn, W. (eds.) GWAI-86 und 2. Osterreichische ArtificialIntelligence-Tagung. Springer, Verlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allgayer</author>
<author>K Harbusch</author>
<author>A Kobsa</author>
<author>C Reddig</author>
<author>N Reithinger</author>
<author>D Schmauks</author>
</authors>
<title>XTRA: A Natural-Language Access System to Expert Systems.</title>
<date>1988</date>
<tech>Technical Report, SFB 314: AI-KnowledgeBased Systems,</tech>
<institution>Department of Computer Science, University of Saarbriicken, W.</institution>
<contexts>
<context position="2674" citStr="Allgayer et al. 1988" startWordPosition="420" endWordPosition="423"> and subfunctions of this system hopefully capture the general consensus on what constitutes a discourse model and a user model, respectively. However, we will see that these subcomponents are strongly interwoven and that—apart from a few rarely occurring exceptions—the DM is part of the UM at least at the level of content. The question arises then, of course, whether it makes sense to separate these notions conceptually. The belief, goal, and plan maintenance system outlined here is being implemented (in a somewhat simplified form) in XTRA, a natural language access system to expert systems (Allgayer et al. 1988). A previous implementation was VIE-DPM (Kobsa 1985a,b). In the knowledge base of BGP-MS, the representation of the various types of (nested) beliefs and goals (Kobsa 1988) is separated into a number of hierarchically ordered partitions (see Figure 1). If it is shared knowledge between S and U that U possesses certain beliefs (knowledge), then this knowledge or these beliefs are represented in MB(UB).1MB(UW) contains those goals and plans of the user, MB(SB) those beliefs of the system, and MR(SW) those goals of the system for which the same holds true. &amp;quot;Private&amp;quot; beliefs of the system about th</context>
</contexts>
<marker>Allgayer, Harbusch, Kobsa, Reddig, Reithinger, Schmauks, 1988</marker>
<rawString>Allgayer, J.; Harbusch, K.; Kobsa, A.; Reddig, C.; Reithinger, N.; Schmauks, D. 1988 XTRA: A Natural-Language Access System to Expert Systems. Technical Report, SFB 314: AI-KnowledgeBased Systems, Department of Computer Science, University of Saarbriicken, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kilbury</author>
</authors>
<title>Language Variation, Parsing, and the Modelling of User&apos;s Language Variations.</title>
<date>1986</date>
<booktitle>In Proceedings of the 7th European Conference on Artificial Intelligence,</booktitle>
<pages>29--32</pages>
<location>Brighton, England:</location>
<contexts>
<context position="14738" citStr="Kilbury 1986" startWordPosition="2418" endWordPosition="2419">he way, it also holds true for the grammar the system employs (but see the opposing views of Monk and Wahlster, this issue). If the system did not assume that its assumptions about the syntactic structure of language (as expressed in its grammar) be Computational Linguistics, Volume 14, Number 3, September 1988 93 Alfred Kobsa &apos;Jser Models and Discourse Models: United they stand. . . shared by the user, it could not justifiably use it in the analysis and generation of dialog contributions without risking miscommunication. And there definitely exists work in user modeling (e.g., Schuster 1985, Kilbury 1986, Lehman Carbonell 1988), which is concerned with the recognition of those parts of a user&apos;s idiosyncratic grammar that deviate from the mutually shared kernel grammar. Of course, an entry in MB never means that the system assumes that the user &amp;quot;has the same structure in his/her mind&amp;quot; (e.g., ATNs, KL-ONE, or LISP), but only that these structures are functionally equivalent reconstructions of the user&apos;s competence. Does the large degree of inclusion of discourse models in user models at the level of content imply that the notion of discourse model is superflous? As was pointed out by Monk (this</context>
</contexts>
<marker>Kilbury, 1986</marker>
<rawString>Kilbury, J. 1986 Language Variation, Parsing, and the Modelling of User&apos;s Language Variations. In Proceedings of the 7th European Conference on Artificial Intelligence, Brighton, England: 29-32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Kobsa</author>
</authors>
<title>I985a Benutzermodellierung in Dialogsystemen.</title>
<publisher>SpringerVerlag,</publisher>
<location>Berlin—New York.</location>
<marker>Kobsa, </marker>
<rawString>Kobsa, A. I985a Benutzermodellierung in Dialogsystemen. SpringerVerlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kobsa</author>
</authors>
<title>Using Situation Descriptions and Russellian Attitudes for Representing Beliefs and Wants.</title>
<date>1985</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>513--515</pages>
<location>Los Angeles, CA:</location>
<contexts>
<context position="2725" citStr="Kobsa 1985" startWordPosition="430" endWordPosition="431"> consensus on what constitutes a discourse model and a user model, respectively. However, we will see that these subcomponents are strongly interwoven and that—apart from a few rarely occurring exceptions—the DM is part of the UM at least at the level of content. The question arises then, of course, whether it makes sense to separate these notions conceptually. The belief, goal, and plan maintenance system outlined here is being implemented (in a somewhat simplified form) in XTRA, a natural language access system to expert systems (Allgayer et al. 1988). A previous implementation was VIE-DPM (Kobsa 1985a,b). In the knowledge base of BGP-MS, the representation of the various types of (nested) beliefs and goals (Kobsa 1988) is separated into a number of hierarchically ordered partitions (see Figure 1). If it is shared knowledge between S and U that U possesses certain beliefs (knowledge), then this knowledge or these beliefs are represented in MB(UB).1MB(UW) contains those goals and plans of the user, MB(SB) those beliefs of the system, and MR(SW) those goals of the system for which the same holds true. &amp;quot;Private&amp;quot; beliefs of the system about the domain of discourse / about the user&apos;s beliefs / </context>
</contexts>
<marker>Kobsa, 1985</marker>
<rawString>Kobsa, A. 1985b Using Situation Descriptions and Russellian Attitudes for Representing Beliefs and Wants. In Proceedings of the International Joint Conference on Artificial Intelligence, Los Angeles, CA: 513-515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A</author>
</authors>
<title>A Taxonomy of Beliefs and Goals for User Models in Dialog Systems.</title>
<date>1988</date>
<booktitle>User Models in Dialog Systems.</booktitle>
<editor>In Kobsa, A. and Wahlster, W. (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin—New York.</location>
<marker>A, 1988</marker>
<rawString>Kobsa. A. 1988 A Taxonomy of Beliefs and Goals for User Models in Dialog Systems. In Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog Systems. Springer-Verlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Lehman</author>
<author>J G Carbonell</author>
</authors>
<title>Learning the User&apos;s Language: A Step Towards Automated Creation of User Models.</title>
<date>1988</date>
<editor>In Kobsa, A. and Wahlster, W. (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin—New York.</location>
<marker>Lehman, Carbonell, 1988</marker>
<rawString>Lehman, J. F. and Carbonell, J. G. 1988 Learning the User&apos;s Language: A Step Towards Automated Creation of User Models. In Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog Systems. Springer-Verlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Lewis</author>
</authors>
<title>Convention: A Philosophical Study.</title>
<date>1969</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13958" citStr="Lewis 1969" startWordPosition="2289" endWordPosition="2290">found which operate exclusively on that part of the user model that is identical with the discourse model, or upon the remaining parts of the user model. This large degree to which the DM is included in the UM, however, is not surprising: Discourse models are ultimately based on linguistic conventions. In order for the linguistic, intentional, attentional, etc., structure of the previous discourse to be exploited for future dialog contributions, conventions about what the structure of a particular ongoing dialog actually is must exist. Knowledge about convention is mutual knowledge, however, (Lewis 1969, Schiffer 1972), and thus part of MB. The same holds true for the above-mentioned additional components of the DM that could not be dealt with in this paper. And, by the way, it also holds true for the grammar the system employs (but see the opposing views of Monk and Wahlster, this issue). If the system did not assume that its assumptions about the syntactic structure of language (as expressed in its grammar) be Computational Linguistics, Volume 14, Number 3, September 1988 93 Alfred Kobsa &apos;Jser Models and Discourse Models: United they stand. . . shared by the user, it could not justifiably </context>
</contexts>
<marker>Lewis, 1969</marker>
<rawString>Lewis, D. K. 1969 Convention: A Philosophical Study. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Generating Natural Language Responses to Questions about Database Structure.</title>
<date>1982</date>
<tech>TR MS-CIS-82-5,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="12944" citStr="McKeown 1982" startWordPosition="2122" endWordPosition="2123">ically, can only be applied to representation structures in MB and MW and therefore form part of the user model. SUMMARY The above discussion demonstrates that the function of a UM and of all mentioned DM components can be completely fulfilled by the outlined belief, goal, and plan maintenance system. I cannot deal here in detail with the question of whether this is also the case for other components that have been proposed for a DM, for example, the structuring of the dialog into dialog segments (Grosz Sidner 1986), a context space grammar (Reichman 1981), or rhetorical predicates (schemata; McKeown 1982). With respect to the analyzed components, we have seen that the discourse model almost completely overlaps with the user model at the level of content. Only if the user does not fully catch the system&apos;s dialog contributions are entries in the DM created which do not form part of the UM (see, for instance, Wahlster&apos;s example, this issue). But at a procedural level as well, only a few processes can be found which operate exclusively on that part of the user model that is identical with the discourse model, or upon the remaining parts of the user model. This large degree to which the DM is inclu</context>
</contexts>
<marker>McKeown, 1982</marker>
<rawString>McKeown, K. R. 1982 Generating Natural Language Responses to Questions about Database Structure. TR MS-CIS-82-5, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
<author>J Hirschberg</author>
<author>B Webber</author>
</authors>
<title>User Participation in the Reasoning Process of Expert Systems.</title>
<date>1982</date>
<tech>MS CIS-82-9,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11074" citStr="Pollack et al. (1982)" startWordPosition="1813" endWordPosition="1816">ether or not one should regard the intentional structure of individual utterances (Cohen, this issue) or dialog segments (Grosz Sidner 1986) as being independent of the dialog setting and the dialog participants. To put it in a more provocative way, do dialog constituents or dialog participants have an intentional structure? In my view, the essence of problemsolving dialog lies in the recognition of the dialog participants&apos; goals and plans, and in the construction of mutually known goals and plans. Dialog contributions of the dialog partner serve as a more-or-less helpful aid in this process (Pollack et al. (1982) present transcripts in which dialog contributions of clients with misconceptions even impair the recognition of their actual goals). Apart from that, no intentional character pertains to dialog constituents that is independent of the dialog and situational context and of the current (beliefs about) goals and plans. In the BGP-MS philosophy, (beliefs about) goals and plans are contained in those partitions whose labels include a W. A user&apos;s dialog contribution is first represented by FSS structures in MB. If user plans or goals can be inferred by S, they are represented in MB(UW), or in MW if </context>
</contexts>
<marker>Pollack, Hirschberg, Webber, 1982</marker>
<rawString>Pollack, M.E.; Hirschberg, J.; and Webber, B. 1982 User Participation in the Reasoning Process of Expert Systems. MS CIS-82-9, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman</author>
</authors>
<title>Plain Speaking: A Theory and Grammar of Spontaneous Discourse.</title>
<date>1981</date>
<tech>Report No. 4681,</tech>
<location>Bolt, Beranek and Newman, Cambridge, MA.</location>
<contexts>
<context position="12893" citStr="Reichman 1981" startWordPosition="2116" endWordPosition="2117">ssed by focus values in the dialog memory which, logically, can only be applied to representation structures in MB and MW and therefore form part of the user model. SUMMARY The above discussion demonstrates that the function of a UM and of all mentioned DM components can be completely fulfilled by the outlined belief, goal, and plan maintenance system. I cannot deal here in detail with the question of whether this is also the case for other components that have been proposed for a DM, for example, the structuring of the dialog into dialog segments (Grosz Sidner 1986), a context space grammar (Reichman 1981), or rhetorical predicates (schemata; McKeown 1982). With respect to the analyzed components, we have seen that the discourse model almost completely overlaps with the user model at the level of content. Only if the user does not fully catch the system&apos;s dialog contributions are entries in the DM created which do not form part of the UM (see, for instance, Wahlster&apos;s example, this issue). But at a procedural level as well, only a few processes can be found which operate exclusively on that part of the user model that is identical with the discourse model, or upon the remaining parts of the use</context>
</contexts>
<marker>Reichman, 1981</marker>
<rawString>Reichman, R. 1981 Plain Speaking: A Theory and Grammar of Spontaneous Discourse. Report No. 4681, Bolt, Beranek and Newman, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rich</author>
</authors>
<title>Stereotypes and User Modeling.</title>
<date>1988</date>
<booktitle>User Models in Dialog Systems.</booktitle>
<editor>In Kobsa, A. and Wahlster, W. (eds.),</editor>
<publisher>SpringerVerlag,</publisher>
<location>Berlin—New York.</location>
<contexts>
<context position="6536" citStr="Rich 1988" startWordPosition="1065" endWordPosition="1066"> are part of the user model. One might argue that the above architecture may completely cover the mentioned object memory function in an NL dialog system. However, three kinds of information are lost thereby (this defect also seems to apply partly to Schuster&apos;s model). a. The information that objects have been explicitly mentioned in the discourse: MB contains not only those objects which were explicitly mentioned in the discourse (and which are therefore mutually known by all dialog participants), but also representations of those objects whose existence is mutually known due to stereotypes (Rich 1988) or to inferences from the discourse. Sometimes, however, the system should possess information about whether or not some object had been explicitly mentioned, for example in order to increase coherency in its own dialog contributions (&amp;quot;As 1/you said before. . . &amp;quot;) or to point out inconsistencies in dialog contributions of the user (&amp;quot;But previously you said. . .&amp;quot;). b. Information about the sequence (and thus recency) of objects&apos; mention: This information is very important in NL systems, since the choice of various forms of anaphora depends on the degree of recency. c. Information about the lin</context>
</contexts>
<marker>Rich, 1988</marker>
<rawString>Rich, E. 1988 Stereotypes and User Modeling. In Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog Systems. SpringerVerlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Schiffer</author>
</authors>
<title>Meaning.</title>
<date>1972</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI85,</booktitle>
<publisher>Clarendon Press,</publisher>
<location>Oxford, England. Schuster, E.</location>
<contexts>
<context position="13974" citStr="Schiffer 1972" startWordPosition="2291" endWordPosition="2292">operate exclusively on that part of the user model that is identical with the discourse model, or upon the remaining parts of the user model. This large degree to which the DM is included in the UM, however, is not surprising: Discourse models are ultimately based on linguistic conventions. In order for the linguistic, intentional, attentional, etc., structure of the previous discourse to be exploited for future dialog contributions, conventions about what the structure of a particular ongoing dialog actually is must exist. Knowledge about convention is mutual knowledge, however, (Lewis 1969, Schiffer 1972), and thus part of MB. The same holds true for the above-mentioned additional components of the DM that could not be dealt with in this paper. And, by the way, it also holds true for the grammar the system employs (but see the opposing views of Monk and Wahlster, this issue). If the system did not assume that its assumptions about the syntactic structure of language (as expressed in its grammar) be Computational Linguistics, Volume 14, Number 3, September 1988 93 Alfred Kobsa &apos;Jser Models and Discourse Models: United they stand. . . shared by the user, it could not justifiably use it in the an</context>
</contexts>
<marker>Schiffer, 1972</marker>
<rawString>Schiffer, S. R. 1972 Meaning. Clarendon Press, Oxford, England. Schuster, E. 1985 Grammars as User Models. In Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI85, Los Angeles, CA: 20-22.</rawString>
</citation>
<citation valid="false">
<title>The abbreviations are mnemonic: read &amp;quot;system believes&amp;quot; for &amp;quot;SB&amp;quot;, &amp;quot;system wants&amp;quot; for &amp;quot;SW&amp;quot;, &amp;quot;user believes&amp;quot; for &amp;quot;UB&amp;quot;, &amp;quot;mutual belief&amp;quot; for &amp;quot;MB&amp;quot;,</title>
<pages>etc.</pages>
<marker></marker>
<rawString>1. The abbreviations are mnemonic: read &amp;quot;system believes&amp;quot; for &amp;quot;SB&amp;quot;, &amp;quot;system wants&amp;quot; for &amp;quot;SW&amp;quot;, &amp;quot;user believes&amp;quot; for &amp;quot;UB&amp;quot;, &amp;quot;mutual belief&amp;quot; for &amp;quot;MB&amp;quot;, etc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>