<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.971475">
FudanNLP: A Toolkit for Chinese Natural Language Processing
</title>
<author confidence="0.903061">
Xipeng Qiu, Qi Zhang, Xuanjing Huang
</author>
<affiliation confidence="0.687105">
Fudan University, 825 Zhangheng Road, Shanghai, China
</affiliation>
<email confidence="0.991173">
xpqiu@fudan.edu.cn, qz@fudan.edu.cn, xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.995596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761">
The growing need for Chinese natural
language processing (NLP) is largely
in a range of research and commer-
cial applications. However, most of
the currently Chinese NLP tools or
components still have a wide range
of issues need to be further improved
and developed. FudanNLP is an open
source toolkit for Chinese natural lan-
guage processing (NLP), which uses
statistics-based and rule-based meth-
ods to deal with Chinese NLP tasks,
such as word segmentation, part-of-
speech tagging, named entity recogni-
tion, dependency parsing, time phrase
recognition, anaphora resolution and so
on.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960857142858">
Chinese is one of the most widely used lan-
guages in this world, and the proportion that
Chinese language holds on the Internet is also
quite high. Under the current circumstances,
there are greater and greater demands for in-
telligent processing and analyzing of the Chi-
nese texts.
Similar to English, the main tasks in Chi-
nese NLP include word segmentation (CWS),
part-of-speech (POS) tagging, named en-
tity recognition (NER), syntactic parsing,
anaphora resolution (AR), and so on. Al-
though the general ways are essentially the
same for English and Chinese, the implemen-
tation details are different. It is also non-
trivial to optimize these methods for Chinese
NLP tasks.
There are also some toolkits to be used
for NLP, such as Stanford CoreNLP1, Apache
OpenNLP2, Curator3 and NLTK4. But these
toolkits are developed mainly for English and
not optimized for Chinese.
In order to customize an optimized system
for Chinese language process, we implement
an open source toolkit, FudanNLPS, which is
written in Java. Since most of the state-of-the-
art methods for NLP are based on statistical
learning, the whole framework of our toolkit
is established around statistics-based meth-
ods, supplemented by some rule-based meth-
ods. Therefore, the quality of training data
is crucial for our toolkit. However, we find
that there are some drawbacks in currently
most commonly used corpora, such as CTB
(Xia, 2000) and CoNLL (Hajič et al., 2009)
corpora. For example, in CTB corpus, the set
of POS tags is relative small and some cate-
gories are derived from the perspective of En-
glish grammar. And in CoNLL corpus, the
head words are often interrogative particles
and punctuations, which are unidiomatic in
Chinese. These drawbacks bring more chal-
lenges to further analyses, such as informa-
tion extraction and semantic understanding.
Therefore, we first construct a corpus with
a modified guideline, which is more in ac-
cordance with the common understanding for
Chinese grammar.
In addition to the basic Chinese NLP tasks
</bodyText>
<footnote confidence="0.999838142857143">
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://incubator.apache.org/opennlp/
3http://cogcomp.cs.illinois.edu/page/
software_view/Curator
4http://www.nltk.org/
5http://fudannlp.googlecode.com
</footnote>
<page confidence="0.988303">
49
</page>
<note confidence="0.627757">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 49–54,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999868">
Figure 1: System Structure of FudanNLP
</figureCaption>
<bodyText confidence="0.999940388888889">
mentioned above, the toolkit also provides
many minor functions, such as text classifi-
cation, dependency tree kernel, tree pattern-
based information extraction, keywords ex-
traction, translation between simplified and
traditional Chinese, and so on.
Currently, our toolkit has been used by
many universities and companies for various
applications, such as the dialogue system, so-
cial computing, recommendation system and
vertical search.
The rest of the demonstration is organized
as follows. We first briefly describe our system
and its main components in section 2. Then we
show system performances in section 3. Sec-
tion 4 introduces three ways to use our toolkit.
In section 5, we summarize the paper and give
some directions for our future efforts.
</bodyText>
<sectionHeader confidence="0.922728" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999899333333333">
The components of our system have three
layers of structure: data preprocessing, ma-
chine learning and natural language process-
ing, which is shown in Figure 1. We will in-
troduce these components in detail in the fol-
lowing subsections.
</bodyText>
<subsectionHeader confidence="0.961646">
2.1 Data Preprocessing Component
</subsectionHeader>
<bodyText confidence="0.995890055555555">
In the natural language processing system,
the original input is always text. However,
the statistical machine learning methods often
deal with data with vector-based representa-
tion. So we firstly need to preprocess the input
texts and transform them to the required for-
mat. Due to the fact that text data is usually
discrete and sparse, the sparse vector struc-
ture is largely used. Similar to Mallet (Mc-
Callum, 2002), we use the pipeline structure
for a flexible transformation of various data.
The pipeline consists of several serial or par-
allel modules. Each module, called “pipe”, is
aimed at a single and simple function.
For example, when we transform a sentence
into a vector with “bag-of-words”, the trans-
formation process would involve the following
serial pipes:
</bodyText>
<listItem confidence="0.899416285714286">
1. String2Token Pipe: to transform a string
into word tokens.
2. Token2Index Pipe: to look up the word
alphabet to get the indices of the words.
3. WeightByFrequency Pipe: to calculate
the vector weight for each word accord-
ing to its frequency of occurrence.
</listItem>
<bodyText confidence="0.978792333333333">
With the pipeline structure, the data pre-
processing component has good flexibility, ex-
tensibility and reusability.
</bodyText>
<subsectionHeader confidence="0.98617">
2.2 Machine Learning Component
</subsectionHeader>
<bodyText confidence="0.999895777777778">
The outputs of NLP are often structured,
so the structured learning is our core module.
Structured learning is the task of assigning a
structured label y to an input x. The label y
can be a discrete variable, a sequence, a tree
or a more complex structure.
To illustrate by a sample x, we define the
feature as 41)(x, y). Thus, we can label x with
a score function,
</bodyText>
<equation confidence="0.9914775">
y� = arg max F(w, -1�(x, y)), (1)
Y
</equation>
<bodyText confidence="0.9998078">
where w is the parameter of function F(·).
The feature vector -1�(x, y) consists of lots of
overlapping features, which is the chief benefit
of a discriminative model.
For example, in sequence labeling, both x =
xi, ... , xL and y = yi, ... , yL are sequences.
For first-order Markov sequence labeling, the
feature can be denoted as ϕk(yz_1, yz, x, i),
where i is the position in the sequence. Then
the score function can be rewritten as
</bodyText>
<equation confidence="0.897903">
wkOk(yi−1, yi, X, i��, (2)
</equation>
<bodyText confidence="0.990948666666667">
where L is the length of x.
Different algorithms vary in the definition of
F(·) and the corresponding objective function.
</bodyText>
<equation confidence="0.958919428571429">
y = arg max
Y
F(
∑L
i=1
∑
k
</equation>
<page confidence="0.899871">
50
</page>
<bodyText confidence="0.993032">
F(·) is usually defined as a linear or exponen-
tial family function. For example, in condi-
tional random fields (CRFs) (Lafferty et al.,
2001), F(·) is defined as:
</bodyText>
<equation confidence="0.994981333333333">
1
Pw(y|x) = exp(w&apos;-1b(x, y)), (3)
Zw
</equation>
<bodyText confidence="0.9973874">
where Zw is the normalization constant such
that it makes the sum of all the terms one.
In FudanNLP, the linear function is univer-
sally used as the objective function. Eq. (1) is
written as:
</bodyText>
<equation confidence="0.842778333333333">
y� = arg max &lt; w, 41b(x, y) &gt; . (4)
Y
2.2.1 Training
</equation>
<bodyText confidence="0.9797035">
In the training stage, we use the passive-
aggressive algorithm to learn the model pa-
rameters. Passive-aggressive (PA) algorithm
(Crammer et al., 2006) was proposed for nor-
mal multi-class classification and can be easily
extended to structure learning (Crammer et
al., 2005). Like Perceptron, PA is an online
learning algorithm.
</bodyText>
<sectionHeader confidence="0.491496" genericHeader="method">
2.2.2 Inference
</sectionHeader>
<bodyText confidence="0.999928619047619">
For consistency with statistical machine
learning, we call the process to calculate the
Eq.(1) as “inference”. In structured learning,
the number of possible solutions is very huge,
so dynamic programming or approximate ap-
proaches are often used for efficiency. For NLP
tasks, the most popular structure is sequence.
To label the sequence, we use Viterbi dynamic
programming to solve the inference problem in
Eq. (4).
Our system can support any order of Viterbi
decoding. In addition, we also implement a
constrained Viterbi algorithm to reduce the
number of possible solutions by pre-defined
rules. For example, when we know the prob-
able labels, we delete the unreachable states
from state transition matrix. It is very useful
for CWS and POS tagging with sequence la-
beling. When we have a word dictionary or
know the POS for some words, we can get
more accurate results.
</bodyText>
<subsubsectionHeader confidence="0.907055">
2.2.3 Other Algorithms
</subsubsectionHeader>
<bodyText confidence="0.951581125">
Apart from the core modules of structured
learning, our system also includes several tra-
ditional machine learning algorithms, such as
Perceptron, Adaboost, kNN, k-means, and so
on.
2.3 Natural Language Processing
Components
Our toolkit provides the basic NLP func-
tions, such as word segmentation, part-of-
speech tagging, named entity recognition, syn-
tactic parsing, temporal phrase recognition,
anaphora resolution, and so on. These func-
tions are trained on our developed corpus. We
also develop a visualization module to display-
ing the output. Table 1 shows the output rep-
resentation of our toolkit.
</bodyText>
<subsubsectionHeader confidence="0.478852">
2.3.1 Chinese Word Segmentation
</subsubsectionHeader>
<bodyText confidence="0.999712421052632">
Different from English, Chinese sentences
are written in a continuous sequence of char-
acters without explicit delimiters such as the
blank space. Since the meanings of most Chi-
nese characters are not complete, words are
the basic syntactic and semantic units. There-
fore, it is indispensable step to segment the
sentence into words in Chinese language pro-
cessing.
We use character-based sequence labeling
(Peng et al., 2004) to find the boundaries of
words. Besides the carefully chosen features,
we also use the meaning of character drawn
from HowNet(Dong and Dong, 2006), which
improves the performance greatly. Since un-
known words detection is still one of main chal-
lenges of Chinese word segmentation. We im-
plement a constrained Viterbi algorithm to al-
low users to add their own word dictionary.
</bodyText>
<subsubsectionHeader confidence="0.908779">
2.3.2 POS tagging
</subsubsectionHeader>
<bodyText confidence="0.999840611111111">
Chinese POS tagging is very different from
that in English. There are no morphological
changes for a word among its different POS
tags. Therefore, most of Chinese words may
have multiple POS tags. For example, there
are different morphologies in English for the
word “毁灭 (destroy)”, such as “destroyed”,
“destroying” and “destruction”. But in Chi-
nese, there is just one same form(Xia, 2000).
There are two popular guidelines to tag the
word’s POS: CTB (Xia, 2000) and PKU (Yu
et al., 2001). We take into account both
the weaknesses and the strengths of these two
guidelines, and propose our guideline for bet-
ter subsequent analyses, such as parser and
named entity recognition. For example, the
proper name is labeled as “NR” in CTB, while
we label it with one of four categories: person,
</bodyText>
<page confidence="0.987156">
51
</page>
<table confidence="0.5158714">
Input:
约翰来自华盛顿,他生于 1980 年。
John is from Washington, and he was born in 1980.
Output:
1 CS:COO means the coordinate complex sentence.
</table>
<tableCaption confidence="0.981125">
Table 1: Example of the output representation of our toolkit
</tableCaption>
<figure confidence="0.996485842105263">
Root
CS:COO1
PUN
SUB
OBJ
SUB
PUN
OBJ
约翰 来自 华盛顿 , 他 生于 1980 年 。
John is from Washington ,he was born in 1980 .
PER VV LOC PU PRN NN PU
1 2 3 4 5 6 7 8
NER:
1 PER
3 LOC
AR:
5 1
TIME:
7 1980
</figure>
<bodyText confidence="0.999638764705883">
location, organization and other proper name.
Conversely, we merge the “VC” and “VE” into
“VV” since there is no link verb in Chinese.
Finally, we use a tag set with 39 categories in
total.
Since a POS tag is assigned to each word,
not to each character, Chinese POS tag-
ging has two ways: pipeline method or joint
method. Currently, the joint method is more
popular and effective because it uses more flex-
ible features and can reduce the error propa-
gation (Ng and Low, 2004). In our system,
we implement both methods for POS tagging.
Besides, we also use some knowledge to im-
prove the performance, such as Chinese sur-
name and the common suffixes of the names
of locations and organizations.
</bodyText>
<subsubsectionHeader confidence="0.770968">
2.3.3 Named Entity Recognition
</subsubsectionHeader>
<bodyText confidence="0.999969318181818">
In Chinese named entity recognition (NER),
there are usually three kinds of named enti-
ties (NEs) to be dealt with: names of per-
sons (PER) , locations (LOC) and organiza-
tions (ORG). Unlike English, there is no obvi-
ous identification for NEs, such as initial capi-
tals. The internal structures are also different
for different kinds of NEs, so it is difficult to
build a unified model for named entity recog-
nition.
Our NER is based on the results of POS
tagging and uses some customize features to
detect NEs. First, the number of NEs is very
large and the new NEs are endlessly emerg-
ing, so it is impossible to store them in dic-
tionary. Since the internal structures are rela-
tively more important, we use language mod-
els to capture the internal structures. Second,
we merge the continuous NEs with some rule-
based strategies. For example, we combine the
continuous words “人民/NN 大会堂/NN” into
“ 人民大会堂/LOC”.
</bodyText>
<subsubsectionHeader confidence="0.728663">
2.3.4 Dependency parsing
</subsubsectionHeader>
<bodyText confidence="0.999967148148148">
Our syntactic parser is currently a depen-
dency parser, which is implemented with the
shift-reduce deterministic algorithm based on
the work in (Yamada and Matsumoto, 2003).
The syntactic structure of Chinese is more
complex than that of English, and semantic
meaning is more dominant than syntax in Chi-
nese sentences. So we select the dependency
parser to avoid the minutiae in syntactic con-
stituents and wish to pay more attention to
the subsequent semantic analysis. Since the
structure of the Chinese language is quite dif-
ferent from that of English, we use more effec-
tive features according to the characteristics of
Chinese sentences.
The common used corpus for Chinese de-
pendency parsing is CoNLL corpus (Hajič et
al., 2009). However, there are some illogical
cases in CoNLL corpus. For example, the
head words are often interrogative particles
and punctuations. Our guideline is based on
common understanding for Chinese grammar.
The Chinese syntactic components usually in-
clude subject, predicate, object, attribute, ad-
verbial modifier and complement. Figure 2
and 3 show the differences between the trees of
CoNLL and our Corpus. Table 2 shows some
</bodyText>
<page confidence="0.995484">
52
</page>
<bodyText confidence="0.872745">
primary dependency relations in our guideline.
</bodyText>
<equation confidence="0.589278666666667">
想 去 合欢山 赏 雪 吗 ?
want to go to Hehuanshan to see the snow ?
VV VV NR VV NN SP PU
</equation>
<figureCaption confidence="0.960213">
Figure 2: Dependency Tree in CoNLL Corpus
</figureCaption>
<figure confidence="0.519172">
Root
PUN
</figure>
<bodyText confidence="0.9985265">
For a phrase indicating a relative time , such
as “一年后” and “ 一小时后”, we first find the
base time in the context. If no base time is
found, or there is also no temporal phrase to
indicate the base time (such as “明天”), we
set the base time to the current system time.
Table 3 gives examples for our temporal phrase
recognition module.
</bodyText>
<figure confidence="0.8636136">
Input:
08 年北京举行奥运会,8 月 8 号开幕。四年后的七月
二十七日,伦敦奥运开幕。
The Beijing Olympic Games took place from Au-
gust 8, 2008. Four years later, the London Olympic
Games took place from July 21.
COMP
COMP
UNK
COMP
ADV
COMP
Root
VOC
OBJ
ADV
OBJ
想 去 合欢山 赏 雪 吗 ?
want to go to Hehuanshan to see the snow ?
MD VV LOC VV NN SP PU
</figure>
<figureCaption confidence="0.997421">
Figure 3: Dependency Tree in Our Corpus
</figureCaption>
<table confidence="0.999259583333333">
Relations Chinese Definitions
SUB 主语 Subject
PRED 谓语 Predicate
OBJ 宾语 Object
ATT 定语 Attribute
ADV 状语 Adverbial Modifier
COMP 补语 Complement
SVP 连动 Serial Verb Phrases
SUB-OBJ 兼语 Pivotal Construction
VOC 语态 Voice
TEN 时态 Tense
PUN 标点 Punctuation
</table>
<tableCaption confidence="0.651461333333333">
Table 2: Some primary dependency relations
2.3.5 Temporal Phrase Recognition
and Normalization
</tableCaption>
<bodyText confidence="0.999788466666667">
Chinese temporal phrases is more flexible
than English. Firstly, there are two calendars:
Gregorian and lunar calendars. Both of them
are frequently used. Secondly, the forms of
same temporal phrase are various, which often
consists of Chinese characters, Arabic numer-
als and English letters, such as “早上 10 点”
and “10:00 PM”.
Different from the general process based
on machine learning, we implement the time
phrase recognizer with a rule-based method.
These rules include 376 regular expressions
and nearly a hundred logical judgments.
After recognizing the temporal phrases, we
normalize them with a standard time format.
</bodyText>
<table confidence="0.925379636363636">
今天我很忙,晚上 9 点才能下班。周日也要加班。
I’m busy today, and have to come off duty after 9:00
PM. And I also have to work this Sunday.
Output:
08 年 (2008) 2008
8 月 8 号 (August 8) 2008-8-8
七月二十七日 (July 21) 2012-7-27
今天 (today) 2012-2-221
晚上 9 点 (9:00 PM) 2012-2-22 21:00
周日 (this Sunday) 2012-2-26
1 The base time is 2012-02-22 10:00AM.
</table>
<tableCaption confidence="0.780211">
Table 3: Examples for Temporal Phrase
Recognition
</tableCaption>
<subsubsectionHeader confidence="0.387198">
2.3.6 Anaphora Resolution
</subsubsectionHeader>
<bodyText confidence="0.999800714285714">
Anaphora resolution is to detect the pro-
nouns and find what they are referring to.
We first find all pronouns and entity names,
then use a classifier to predict whether there
is a relation between each pair of pronoun and
entity name. Table 4 gives examples for our
anaphora resolution module.
</bodyText>
<sectionHeader confidence="0.428712" genericHeader="method">
Input:
</sectionHeader>
<reference confidence="0.452084">
牛津大学创建于 1167 年。它位于英国牛津城。这个
大学培育了好多优秀的学生。
Oxford University is founded in 1167. It is located
in Oxford, UK. The university has nurtured a lot
of good students.
</reference>
<tableCaption confidence="0.859998">
Table 4: Examples for Anaphora Resolution
</tableCaption>
<sectionHeader confidence="0.99332" genericHeader="method">
3 System Performances
</sectionHeader>
<bodyText confidence="0.990407666666667">
In this section, we investigate the per-
formances for the six tasks: Chinese word
segmentation (CWS), POS tagging (POS),
</bodyText>
<figure confidence="0.994537142857143">
Output:
它 (It)
这 个 大 学 (The
university)
牛津大学
牛津大学 (Oxford University)
OBJ
</figure>
<page confidence="0.996485">
53
</page>
<bodyText confidence="0.998800428571428">
named entity recognition (NER) and de-
pendency parser(DePar), Temporal Phrase
Recognition (TPR) and Anaphora Resolution
(AR). We use 5-fold cross validation on our
developed corpus. The corpus includes 65,745
sentences and 959,846 words. The perfor-
mances are shown in Table 5.
</bodyText>
<table confidence="0.944308666666667">
Task Accuracy Speed1 Memory
CWS 97.5% 98.9K 66M
POS 93.4% 44.5K 110M
NER 98.40% 38K 30M
DePar 85.3% 21.1 80M
TPR 95.16% 22.9k 237K
AR 70.3% 35.7K 52K
1 characters per second. Test environment:
CPU 2.67GHz, JRE 7.
</table>
<tableCaption confidence="0.992113">
Table 5: System Performances
</tableCaption>
<sectionHeader confidence="0.993727" genericHeader="method">
4 Usages
</sectionHeader>
<bodyText confidence="0.999957615384615">
We provide three ways to use our toolkit.
Firstly, our toolkit can be used as library.
Users can call application programming inter-
faces (API) in their own applications.
Secondly, users can also invoke the main
NLP modules to process the inputs (strings
or files) from the command line directly.
Thirdly, the web services are provided
for platform-independent and language-
independent use. We use a REST (Represen-
tational State Transfer) architecture, in which
the web services are viewed as resources and
can be identified by their URLs.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999968428571429">
In this demonstration, we have described
the system, FudanNLP, which is a Java-based
open source toolkit for Chinese natural lan-
guage processing. In the future, we will add
more functions, such as semantic parsing. Be-
sides, we will also optimize the algorithms and
codes to improve the system performances.
</bodyText>
<sectionHeader confidence="0.999315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99989">
We would like to thank all the people6
involved with our FudanNLP project. This
work was funded by NSFC (No.61003091
</bodyText>
<footnote confidence="0.971905">
6https://code.google.com/p/fudannlp/wiki/
People
</footnote>
<bodyText confidence="0.9833935">
and No.61073069) and 973 Program
(No.2010CB327900).
</bodyText>
<sectionHeader confidence="0.995113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897469387755">
K. Crammer, R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for struc-
tured classification. In NIPS Workshop on
Learning With Structured Outputs. Citeseer.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of
Machine Learning Research, 7:551–585.
Z. Dong and Q. Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Pub-
lishing Co., Inc. River Edge, NJ, USA.
J. Hajič, M. Ciaramita, R. Johansson, D. Kawa-
hara, M.A. Martí, L. Màrquez, A. Meyers,
J. Nivre, S. Padó, J. Štěpánek, et al. 2009. The
CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, pages 1–18. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
H.T. Ng and J.K. Low. 2004. Chinese part-
of-speech tagging: one-at-a-time or all-at-once?
word-based or character-based. In Proceedings
of EMNLP, volume 4.
F. Peng, F. Feng, and A. McCallum. 2004. Chi-
nese segmentation and new word detection us-
ing conditional random fields. Proceedings of the
20th international conference on Computational
Linguistics.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
H. Yamada and Y. Matsumoto. 2003. Statis-
tical dependency analysis with support vector
machines. In Proceedings of the International
Workshop on Parsing Technologies (IWPT),
volume 3.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern chinese corpus. Technical
report, Technical report.
</reference>
<page confidence="0.999026">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479882">
<title confidence="0.787973">FudanNLP: A Toolkit for Chinese Natural Language Processing Xipeng Qiu, Qi Zhang, Xuanjing</title>
<address confidence="0.825845">Fudan University, 825 Zhangheng Road, Shanghai, China</address>
<email confidence="0.979158">xpqiu@fudan.edu.cn,qz@fudan.edu.cn,xjhuang@fudan.edu.cn</email>
<abstract confidence="0.997484722222222">The growing need for Chinese natural language processing (NLP) is largely in a range of research and commercial applications. However, most of the currently Chinese NLP tools or components still have a wide range of issues need to be further improved and developed. FudanNLP is an open source toolkit for Chinese natural language processing (NLP), which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-ofspeech tagging, named entity recognition, dependency parsing, time phrase recognition, anaphora resolution and so on.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>1167 年。它位于英国牛津城。这个 大学培育了好多优秀的学生。</note>
<marker></marker>
<rawString>牛津大学创建于 1167 年。它位于英国牛津城。这个 大学培育了好多优秀的学生。</rawString>
</citation>
<citation valid="false">
<title>is founded in 1167. It is located in Oxford, UK. The university has nurtured a lot of good students.</title>
<institution>Oxford University</institution>
<marker></marker>
<rawString>Oxford University is founded in 1167. It is located in Oxford, UK. The university has nurtured a lot of good students.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Scalable large-margin online learning for structured classification.</title>
<date>2005</date>
<booktitle>In NIPS Workshop on Learning With Structured Outputs. Citeseer.</booktitle>
<contexts>
<context position="7202" citStr="Crammer et al., 2005" startWordPosition="1149" endWordPosition="1152">l random fields (CRFs) (Lafferty et al., 2001), F(·) is defined as: 1 Pw(y|x) = exp(w&apos;-1b(x, y)), (3) Zw where Zw is the normalization constant such that it makes the sum of all the terms one. In FudanNLP, the linear function is universally used as the objective function. Eq. (1) is written as: y� = arg max &lt; w, 41b(x, y) &gt; . (4) Y 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classification and can be easily extended to structure learning (Crammer et al., 2005). Like Perceptron, PA is an online learning algorithm. 2.2.2 Inference For consistency with statistical machine learning, we call the process to calculate the Eq.(1) as “inference”. In structured learning, the number of possible solutions is very huge, so dynamic programming or approximate approaches are often used for efficiency. For NLP tasks, the most popular structure is sequence. To label the sequence, we use Viterbi dynamic programming to solve the inference problem in Eq. (4). Our system can support any order of Viterbi decoding. In addition, we also implement a constrained Viterbi algo</context>
</contexts>
<marker>Crammer, McDonald, Pereira, 2005</marker>
<rawString>K. Crammer, R. McDonald, and F. Pereira. 2005. Scalable large-margin online learning for structured classification. In NIPS Workshop on Learning With Structured Outputs. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="7079" citStr="Crammer et al., 2006" startWordPosition="1130" endWordPosition="1133">= arg max Y F( ∑L i=1 ∑ k 50 F(·) is usually defined as a linear or exponential family function. For example, in conditional random fields (CRFs) (Lafferty et al., 2001), F(·) is defined as: 1 Pw(y|x) = exp(w&apos;-1b(x, y)), (3) Zw where Zw is the normalization constant such that it makes the sum of all the terms one. In FudanNLP, the linear function is universally used as the objective function. Eq. (1) is written as: y� = arg max &lt; w, 41b(x, y) &gt; . (4) Y 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classification and can be easily extended to structure learning (Crammer et al., 2005). Like Perceptron, PA is an online learning algorithm. 2.2.2 Inference For consistency with statistical machine learning, we call the process to calculate the Eq.(1) as “inference”. In structured learning, the number of possible solutions is very huge, so dynamic programming or approximate approaches are often used for efficiency. For NLP tasks, the most popular structure is sequence. To label the sequence, we use Viterbi dynamic programming to solve the inference problem </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Dong</author>
<author>Q Dong</author>
</authors>
<title>Hownet And the Computation of Meaning.</title>
<date>2006</date>
<publisher>World Scientific Publishing Co., Inc. River</publisher>
<location>Edge, NJ, USA.</location>
<contexts>
<context position="9372" citStr="Dong and Dong, 2006" startWordPosition="1491" endWordPosition="1494"> of our toolkit. 2.3.1 Chinese Word Segmentation Different from English, Chinese sentences are written in a continuous sequence of characters without explicit delimiters such as the blank space. Since the meanings of most Chinese characters are not complete, words are the basic syntactic and semantic units. Therefore, it is indispensable step to segment the sentence into words in Chinese language processing. We use character-based sequence labeling (Peng et al., 2004) to find the boundaries of words. Besides the carefully chosen features, we also use the meaning of character drawn from HowNet(Dong and Dong, 2006), which improves the performance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.3.2 POS tagging Chinese POS tagging is very different from that in English. There are no morphological changes for a word among its different POS tags. Therefore, most of Chinese words may have multiple POS tags. For example, there are different morphologies in English for the word “毁灭 (destroy)”, such as “destroyed”, “destroying” and “destruction”. But in Chinese, th</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Z. Dong and Q. Dong. 2006. Hownet And the Computation of Meaning. World Scientific Publishing Co., Inc. River Edge, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajič</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Martí</author>
<author>L Màrquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Padó</author>
<author>J Štěpánek</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2248" citStr="Hajič et al., 2009" startWordPosition="347" endWordPosition="350">ped mainly for English and not optimized for Chinese. In order to customize an optimized system for Chinese language process, we implement an open source toolkit, FudanNLPS, which is written in Java. Since most of the state-of-theart methods for NLP are based on statistical learning, the whole framework of our toolkit is established around statistics-based methods, supplemented by some rule-based methods. Therefore, the quality of training data is crucial for our toolkit. However, we find that there are some drawbacks in currently most commonly used corpora, such as CTB (Xia, 2000) and CoNLL (Hajič et al., 2009) corpora. For example, in CTB corpus, the set of POS tags is relative small and some categories are derived from the perspective of English grammar. And in CoNLL corpus, the head words are often interrogative particles and punctuations, which are unidiomatic in Chinese. These drawbacks bring more challenges to further analyses, such as information extraction and semantic understanding. Therefore, we first construct a corpus with a modified guideline, which is more in accordance with the common understanding for Chinese grammar. In addition to the basic Chinese NLP tasks 1http://nlp.stanford.ed</context>
<context position="13171" citStr="Hajič et al., 2009" startWordPosition="2158" endWordPosition="2161"> algorithm based on the work in (Yamada and Matsumoto, 2003). The syntactic structure of Chinese is more complex than that of English, and semantic meaning is more dominant than syntax in Chinese sentences. So we select the dependency parser to avoid the minutiae in syntactic constituents and wish to pay more attention to the subsequent semantic analysis. Since the structure of the Chinese language is quite different from that of English, we use more effective features according to the characteristics of Chinese sentences. The common used corpus for Chinese dependency parsing is CoNLL corpus (Hajič et al., 2009). However, there are some illogical cases in CoNLL corpus. For example, the head words are often interrogative particles and punctuations. Our guideline is based on common understanding for Chinese grammar. The Chinese syntactic components usually include subject, predicate, object, attribute, adverbial modifier and complement. Figure 2 and 3 show the differences between the trees of CoNLL and our Corpus. Table 2 shows some 52 primary dependency relations in our guideline. 想 去 合欢山 赏 雪 吗 ? want to go to Hehuanshan to see the snow ? VV VV NR VV NN SP PU Figure 2: Dependency Tree in CoNLL Corpus </context>
</contexts>
<marker>Hajič, Ciaramita, Johansson, Kawahara, Martí, Màrquez, Meyers, Nivre, Padó, Štěpánek, 2009</marker>
<rawString>J. Hajič, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Martí, L. Màrquez, A. Meyers, J. Nivre, S. Padó, J. Štěpánek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6627" citStr="Lafferty et al., 2001" startWordPosition="1047" endWordPosition="1050">efit of a discriminative model. For example, in sequence labeling, both x = xi, ... , xL and y = yi, ... , yL are sequences. For first-order Markov sequence labeling, the feature can be denoted as ϕk(yz_1, yz, x, i), where i is the position in the sequence. Then the score function can be rewritten as wkOk(yi−1, yi, X, i��, (2) where L is the length of x. Different algorithms vary in the definition of F(·) and the corresponding objective function. y = arg max Y F( ∑L i=1 ∑ k 50 F(·) is usually defined as a linear or exponential family function. For example, in conditional random fields (CRFs) (Lafferty et al., 2001), F(·) is defined as: 1 Pw(y|x) = exp(w&apos;-1b(x, y)), (3) Zw where Zw is the normalization constant such that it makes the sum of all the terms one. In FudanNLP, the linear function is universally used as the objective function. Eq. (1) is written as: y� = arg max &lt; w, 41b(x, y) &gt; . (4) Y 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classification and can be easily extended to structure learning (Crammer et al., 2005). Like Perceptron, PA is </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="4706" citStr="McCallum, 2002" startWordPosition="710" endWordPosition="712">processing, machine learning and natural language processing, which is shown in Figure 1. We will introduce these components in detail in the following subsections. 2.1 Data Preprocessing Component In the natural language processing system, the original input is always text. However, the statistical machine learning methods often deal with data with vector-based representation. So we firstly need to preprocess the input texts and transform them to the required format. Due to the fact that text data is usually discrete and sparse, the sparse vector structure is largely used. Similar to Mallet (McCallum, 2002), we use the pipeline structure for a flexible transformation of various data. The pipeline consists of several serial or parallel modules. Each module, called “pipe”, is aimed at a single and simple function. For example, when we transform a sentence into a vector with “bag-of-words”, the transformation process would involve the following serial pipes: 1. String2Token Pipe: to transform a string into word tokens. 2. Token2Index Pipe: to look up the word alphabet to get the indices of the words. 3. WeightByFrequency Pipe: to calculate the vector weight for each word according to its frequency </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J K Low</author>
</authors>
<title>Chinese partof-speech tagging: one-at-a-time or all-at-once? word-based or character-based.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<contexts>
<context position="11266" citStr="Ng and Low, 2004" startWordPosition="1837" endWordPosition="1840">顿 , 他 生于 1980 年 。 John is from Washington ,he was born in 1980 . PER VV LOC PU PRN NN PU 1 2 3 4 5 6 7 8 NER: 1 PER 3 LOC AR: 5 1 TIME: 7 1980 location, organization and other proper name. Conversely, we merge the “VC” and “VE” into “VV” since there is no link verb in Chinese. Finally, we use a tag set with 39 categories in total. Since a POS tag is assigned to each word, not to each character, Chinese POS tagging has two ways: pipeline method or joint method. Currently, the joint method is more popular and effective because it uses more flexible features and can reduce the error propagation (Ng and Low, 2004). In our system, we implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suffixes of the names of locations and organizations. 2.3.3 Named Entity Recognition In Chinese named entity recognition (NER), there are usually three kinds of named entities (NEs) to be dealt with: names of persons (PER) , locations (LOC) and organizations (ORG). Unlike English, there is no obvious identification for NEs, such as initial capitals. The internal structures are also different for different kinds of NEs, so it is diffi</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>H.T. Ng and J.K. Low. 2004. Chinese partof-speech tagging: one-at-a-time or all-at-once? word-based or character-based. In Proceedings of EMNLP, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>F Feng</author>
<author>A McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="9224" citStr="Peng et al., 2004" startWordPosition="1467" endWordPosition="1470">ions are trained on our developed corpus. We also develop a visualization module to displaying the output. Table 1 shows the output representation of our toolkit. 2.3.1 Chinese Word Segmentation Different from English, Chinese sentences are written in a continuous sequence of characters without explicit delimiters such as the blank space. Since the meanings of most Chinese characters are not complete, words are the basic syntactic and semantic units. Therefore, it is indispensable step to segment the sentence into words in Chinese language processing. We use character-based sequence labeling (Peng et al., 2004) to find the boundaries of words. Besides the carefully chosen features, we also use the meaning of character drawn from HowNet(Dong and Dong, 2006), which improves the performance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.3.2 POS tagging Chinese POS tagging is very different from that in English. There are no morphological changes for a word among its different POS tags. Therefore, most of Chinese words may have multiple POS tags. For examp</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the penn chinese treebank (3.0).</title>
<date>2000</date>
<contexts>
<context position="2217" citStr="Xia, 2000" startWordPosition="343" endWordPosition="344">se toolkits are developed mainly for English and not optimized for Chinese. In order to customize an optimized system for Chinese language process, we implement an open source toolkit, FudanNLPS, which is written in Java. Since most of the state-of-theart methods for NLP are based on statistical learning, the whole framework of our toolkit is established around statistics-based methods, supplemented by some rule-based methods. Therefore, the quality of training data is crucial for our toolkit. However, we find that there are some drawbacks in currently most commonly used corpora, such as CTB (Xia, 2000) and CoNLL (Hajič et al., 2009) corpora. For example, in CTB corpus, the set of POS tags is relative small and some categories are derived from the perspective of English grammar. And in CoNLL corpus, the head words are often interrogative particles and punctuations, which are unidiomatic in Chinese. These drawbacks bring more challenges to further analyses, such as information extraction and semantic understanding. Therefore, we first construct a corpus with a modified guideline, which is more in accordance with the common understanding for Chinese grammar. In addition to the basic Chinese NL</context>
<context position="10008" citStr="Xia, 2000" startWordPosition="1597" endWordPosition="1598">rmance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.3.2 POS tagging Chinese POS tagging is very different from that in English. There are no morphological changes for a word among its different POS tags. Therefore, most of Chinese words may have multiple POS tags. For example, there are different morphologies in English for the word “毁灭 (destroy)”, such as “destroyed”, “destroying” and “destruction”. But in Chinese, there is just one same form(Xia, 2000). There are two popular guidelines to tag the word’s POS: CTB (Xia, 2000) and PKU (Yu et al., 2001). We take into account both the weaknesses and the strengths of these two guidelines, and propose our guideline for better subsequent analyses, such as parser and named entity recognition. For example, the proper name is labeled as “NR” in CTB, while we label it with one of four categories: person, 51 Input: 约翰来自华盛顿,他生于 1980 年。 John is from Washington, and he was born in 1980. Output: 1 CS:COO means the coordinate complex sentence. Table 1: Example of the output representation of our toolkit Root</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>F. Xia, 2000. The part-of-speech tagging guidelines for the penn chinese treebank (3.0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies (IWPT),</booktitle>
<volume>3</volume>
<contexts>
<context position="12612" citStr="Yamada and Matsumoto, 2003" startWordPosition="2065" endWordPosition="2068">e customize features to detect NEs. First, the number of NEs is very large and the new NEs are endlessly emerging, so it is impossible to store them in dictionary. Since the internal structures are relatively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm based on the work in (Yamada and Matsumoto, 2003). The syntactic structure of Chinese is more complex than that of English, and semantic meaning is more dominant than syntax in Chinese sentences. So we select the dependency parser to avoid the minutiae in syntactic constituents and wish to pay more attention to the subsequent semantic analysis. Since the structure of the Chinese language is quite different from that of English, we use more effective features according to the characteristics of Chinese sentences. The common used corpus for Chinese dependency parsing is CoNLL corpus (Hajič et al., 2009). However, there are some illogical cases</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the International Workshop on Parsing Technologies (IWPT), volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yu</author>
<author>J Lu</author>
<author>X Zhu</author>
<author>H Duan</author>
<author>S Kang</author>
<author>H Sun</author>
<author>H Wang</author>
<author>Q Zhao</author>
<author>W Zhan</author>
</authors>
<title>Processing norms of modern chinese corpus.</title>
<date>2001</date>
<tech>Technical report, Technical report.</tech>
<contexts>
<context position="10107" citStr="Yu et al., 2001" startWordPosition="1614" endWordPosition="1617"> segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.3.2 POS tagging Chinese POS tagging is very different from that in English. There are no morphological changes for a word among its different POS tags. Therefore, most of Chinese words may have multiple POS tags. For example, there are different morphologies in English for the word “毁灭 (destroy)”, such as “destroyed”, “destroying” and “destruction”. But in Chinese, there is just one same form(Xia, 2000). There are two popular guidelines to tag the word’s POS: CTB (Xia, 2000) and PKU (Yu et al., 2001). We take into account both the weaknesses and the strengths of these two guidelines, and propose our guideline for better subsequent analyses, such as parser and named entity recognition. For example, the proper name is labeled as “NR” in CTB, while we label it with one of four categories: person, 51 Input: 约翰来自华盛顿,他生于 1980 年。 John is from Washington, and he was born in 1980. Output: 1 CS:COO means the coordinate complex sentence. Table 1: Example of the output representation of our toolkit Root CS:COO1 PUN SUB OBJ SUB PUN OBJ 约翰 来自 华盛顿 , 他 生于 1980 年 。 John is from Washington ,he was born in </context>
</contexts>
<marker>Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, Zhan, 2001</marker>
<rawString>S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun, H. Wang, Q. Zhao, and W. Zhan. 2001. Processing norms of modern chinese corpus. Technical report, Technical report.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>