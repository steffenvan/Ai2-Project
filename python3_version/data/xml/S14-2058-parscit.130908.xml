<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000576">
<title confidence="0.975067">
INSIGHT Galway: Syntactic and Lexical Features for Aspect Based
Sentiment Analysis
</title>
<author confidence="0.978186">
Sapna Negi
</author>
<affiliation confidence="0.888011666666667">
Insight Centre for Data Analytics
National University of Ireland
Galway
</affiliation>
<author confidence="0.979945">
Paul Buitelaar
</author>
<affiliation confidence="0.835633">
Insight Centre for Data Analytics
National University of Ireland
Galway
</affiliation>
<email confidence="0.860136">
{sapna.negi, paul.buitelaar}@insight-centre.org
</email>
<sectionHeader confidence="0.990929" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999504923076923">
This work analyses various syntactic and
lexical features for sentence level aspect
based sentiment analysis. The task fo-
cuses on detection of a writer’s sentiment
towards an aspect which is explicitly men-
tioned in a sentence. The target sentiment
polarities are positive, negative, conflict
and neutral. We use a supervised learning
approach, evaluate various features and
report accuracies which are much higher
than the provided baselines. Best features
include unigrams, clauses, dependency re-
lations and SentiWordNet polarity scores.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999086">
The term aspect refers to the features or aspects
of a product, service or topic being discussed in a
text. The task of detection of sentiment towards
these aspects involves two major processing steps,
identifying the aspects in the text and identifying
the sentiments towards these aspects. Our work
describes a submitted system in the Aspect Based
Sentiment Analysis task of SemEval 2014 (Pontiki
et al., 2014). The task was further divided into 4
subtasks; our work corresponds to the subtask 2,
called Aspect Term Polarity Detection. We pre-
dict the polarity of sentiments expressed towards
the aspect terms which are already annotated in a
sentence. The target polarity types are positive,
negative, neutral and conflict.
We employ a statistical classifier and experiment
with various syntactic and lexical features. Se-
lected features for the submitted system include
words which hold certain dependency relations
with the aspect terms, clause in which the aspect
</bodyText>
<footnote confidence="0.4770415">
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.9781985">
term appears, unigrams, and sum of lexicon based
sentiment polarities of the words in the clause.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999975727272727">
Pang et al. (2002) proved that unigrams and
bigrams, adjectives and part of speech tags are
important features for a machine learning based
sentiment classifier. Later, verbs and adjectives
were also identified as important features (Ches-
ley, 2006). Meena and Prabhakar (2007) per-
formed sentence level sentiment analysis using
rules based on clauses of a sentence. However,
in our case we cannot simply consider the adjec-
tives and verbs as features, since they might re-
late to different aspects. For example, in the sen-
tence ‘The pizza is the best if you like thin crusted
pizza.’, sentiment towards ‘pizza’ is positive be-
cause of the adjective ‘best’; however for the term
‘thin crusted pizza’, ‘like’ would be the sentiment
verb. Therefore, only those adjectives and verbs
which relate to the target aspect, can be consid-
ered as the indicator of their polarity. Wilson et
al. (2009) showed that the words which share cer-
tain dependency relations with aspect terms, tend
to indicate the sentiments expressed towards those
terms.
Saif et al. (2012) showed the co-relation between
topic and sentiment polarity in tweets, and as-
serted that majority of people tend to express sim-
ilar sentiments towards same topics, especially in
the case of positive sentiments. The baseline ap-
proach for this task (Pontiki et al., 2014) also as-
sociates polarity with aspect terms. Therefore, we
also consider aspect term as a potential feature.
Our approach for this task is based on our obser-
vation of the data, with a provenance of the above
mentioned findings.
</bodyText>
<sectionHeader confidence="0.995864" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.986323">
We employ a statistical classifier which trains on
the provided training datasets.
</bodyText>
<page confidence="0.988714">
346
</page>
<note confidence="0.730576">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 346–350,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9994296">
Datasets: Training datasets comprise of 3000 sen-
tences from laptop and restaurant reviews. Train-
ing sentences were tagged with the target aspect
term and the corresponding polarity, where more
than one aspect term can be tagged in a sentence.
</bodyText>
<subsectionHeader confidence="0.99937">
3.1 Feature Sets
</subsectionHeader>
<bodyText confidence="0.9975725">
We divide the candidate features into four feature
sets.
</bodyText>
<listItem confidence="0.78101325">
1. Non-contextual: These features comprise of
training vocabulary. They do not target as-
pect based sentiments, but the overall senti-
ment of the sentence. There might be cases
</listItem>
<bodyText confidence="0.9908216">
where the aspect based sentiment is same as
the overall sentiment of the sentence. The
feature set comprises of three feature types,
unigrams, bigrams, adjectives and verbs of
the sentence.
</bodyText>
<sectionHeader confidence="0.608417" genericHeader="method">
2. Lexicon Non-Contextual: These features
</sectionHeader>
<bodyText confidence="0.953860533333333">
are the Sentiwordnet v3.0 polarity scores
(Andrea Baccianella and Sebastiani, 2010)
of the words obtained from the best non-
contextual feature type. This feature set
would include two numerical features, posi-
tive polarity score and negative polarity score
of the best non-contextual feature types. Best
non-contextual feature type is decided by
comparing the classification accuracies of in-
dividual feature types, with cross validation
on the training data (Table 1). We evaluated
two algorithms to obtain the positive and neg-
ative polarities of words using SentiWordNet.
Later, we would provide details of these algo-
rithms.
</bodyText>
<listItem confidence="0.6687">
3. Contextual: These features target aspect
based sentiments. Feature types comprise of
the clause in which an aspect term appears,
the adjective and verbs of this clause, aspect
term itself, and the words which hold certain
dependency relations with aspect term. We
only considered the Stanford parser depen-
dencies ‘nn’, ‘amod’, and ‘nsubj’. The de-
pendency relations were chosen on the basis
of best classification accuracy in a cross vali-
</listItem>
<bodyText confidence="0.973242571428571">
dation trial, where the only features were the
words holding different dependency relations
with the aspect term. However, we only list
the accuracy from the best performing depen-
dency relations in the Tables 1, 3. By the fea-
ture type clause, we mean the unigrams con-
tained in a clause.
</bodyText>
<listItem confidence="0.9834742">
4. Lexicon Contextual: Similar to Lexicon
Non-Contextual features, these are the nu-
meric values obtained from SentiWordNet
polarity scores of the best performing contex-
tual feature type.
</listItem>
<subsectionHeader confidence="0.606746">
Polarity Calculation using SentiWordNet:
</subsectionHeader>
<bodyText confidence="0.9998998">
WordNet (Fellbaum, 1998) is a lexical database
for the English language. It assigns each listed
word the senses to which it may belong, where
each unique sense is represented by a synset id.
SentiWordNet is built on the top of WordNet,
where a pair of positive and negative polarity
score is assigned to each sense of a word. Senti-
Wordnet entry for each word comprises of all the
possible parts of speech in which the word could
appear, all the senses corresponding to each part
of speech, and a pair of polarity scores associated
with each sense 1. The magnitude of positive and
negative polarity scores for each sense ranges
from 0 to 1.
In order to automatically obtain the polarity
scores corresponding to the desired sense of a
word, word sense disambiguation is required
to be performed. We did not perform sense
disambiguation, and picked the polarity scores
simply on the basis of word and part of speech
matching. This gives more than one candidate
senses, and thus more than one pair of polarity
scores for each word. We evaluated the following
2 methods to assign single values of sentiment
polarity scores to each word.
</bodyText>
<listItem confidence="0.959993">
1. Default: The SentiWordnet website 2 pro-
</listItem>
<bodyText confidence="0.963126461538462">
vides a basic algorithm to assign sentiword-
net based polarities to a word. SentiWordnet
assigns a rank to each sense of a word, where
most commonly appearing sense is ranked
as 1. The default algorithm first calculates
an overall polarity (Positive score - Negative
score), for each sense of a word. It then cal-
culates a weighted sum of the overall polarity
scores of all the senses of a word, where the
weights are the ranks of senses. This sum is
considered as a single value polarity score of
a word, which can be a positive or negative
number.
</bodyText>
<footnote confidence="0.999968">
1http://sentiwordnet.isti.cnr.it/search.php?q=good
2http://sentiwordnet.isti.cnr.it
</footnote>
<page confidence="0.991985">
347
</page>
<bodyText confidence="0.961012647058823">
2. Our algorithm: We do not obtain an overall
polarity score for each word, but we obtain a
pair of aggregated negative and positive score
for each word. Aggregate positive score is
obtained by taking the average of the positive
scores of each sense of the word, and same
goes for the aggregate negative score.
One reason for keeping the positive and negative
scores separate in our algorithm is that the task
also involves sentiment classes conflict and neu-
tral. Using only the overall polarity score results
in a loss of information in the case of very low neg-
ativity and positivity (neutral sentiments), or high
but comparable negativity and positivity (conflict-
ing sentiments). Also, our algorithm produced
better results when used with an SVM classifier,
with features as unigrams and their polarity scores.
</bodyText>
<subsectionHeader confidence="0.932257">
3.2 Classifier Model
</subsectionHeader>
<bodyText confidence="0.999934541666667">
Our system is built on the state of the art LibSVM
classifier (EL-Manzalawy and Honavar, 2005).
We used Weka 3.7.10 toolkit (Hall et al., 2009) for
our experiments. The parameters 3 of the SVM
classifier are tuned to the values which give best
results with unigrams. Table 2 provides the tuned
parameters, rest of the parameters are set at default
values.
Pre-processing: We perform stemming using
Weka’s implementation of Snowball stemmer,
convert strings to lower case and filter out stop-
words. We use a customised list of stopwords,
based on our observations of the data. The cus-
tomised list is created using the stopword list of
Weka, with certain words removed. For example,
negators like ’not’, ’didn’t’ etc. are important for
negative sentiments, for example ‘I can barely use
any usb devices because they will not stay prop-
erly connected’. Words like ‘but’, ‘however’ are
prominent in conflicting sentiments, for example
‘No backlit keyboard, but not an issue for me’. Ta-
bles 1, 3 show the difference in results on using fil-
tered stopword list, compared against no stopword
removal, and original stopword list.
</bodyText>
<table confidence="0.8593655">
G R C E Z
0.10 1.0 2 1.0 normalise
</table>
<tableCaption confidence="0.981636">
Table 2: Parameter Settings for SVM Classifier.
</tableCaption>
<footnote confidence="0.9053595">
3http://weka.sourceforge.net/doc.stable/weka/classifiers/
functions/LibSVM.html
</footnote>
<subsectionHeader confidence="0.993991">
3.3 Feature Evaluation
</subsectionHeader>
<bodyText confidence="0.999398142857143">
We evaluated our features using 8-fold cross val-
idation on the training data. We evaluated each
feature by using it as the only feature for the clas-
sifier (Tables 1, 3). We performed experiments
on different combinations of features, but we only
present the best performing combination of fea-
tures in the last row of the tables. The baseline
approach (Pontiki et al., 2014) provided by the or-
ganisers, produced an accuracy of 47% for laptop
and 57% for restaurant, by splitting the training
data.
Metrics include, F score for each class, and overall
classification accuracy. F score ranges from 0-1,
and overall accuracy range from 0-100.
</bodyText>
<sectionHeader confidence="0.992317" genericHeader="method">
4 Submission and Results
</sectionHeader>
<bodyText confidence="0.999983416666667">
Submission involved the prediction of sentiment
polarity towards the already tagged aspect terms
in two test datasets. There were 800 sentences
in each test dataset. The laptop test dataset was
obtained by dividing the original laptop data into
training and test. However, restaurant test dataset
and training dataset come from different sources.
We trained our classifier using the provided train-
ing dataset and the highlighted features (last row)
in the Tables 1, 3. In order to evaluate the submis-
sion, gold standard datasets corresponding to each
test dataset were later released, and submission’s
accuracy was compared against it.
Results: The system performance was evaluated
and ranked on the basis of overall accuracy of sen-
timent prediction. We were ranked as 20/32 for
the laptop domain, and 16/34 for the restaurant
domain. The task organisers reported that 8 polar-
ity predictions for laptop data, and 34 for restau-
rant data were missing from our submission. We
later debugged our system, and obtained the actual
accuracy which our system is capable of produc-
ing with the given test data. The results are sum-
marised in Table 4.
</bodyText>
<sectionHeader confidence="0.960144" genericHeader="evaluation">
5 Observations and Analysis
</sectionHeader>
<bodyText confidence="0.999902375">
We hypothesize that aspect terms should serve as
features when training data and test data come
from same source, which means that they relate
to the same brand, product, service etc. This is
because aspect terms change with data, for exam-
ple names of dishes would change with different
restaurants even if the domain is same. In our
case, the laptop test data was obtained from the
</bodyText>
<page confidence="0.994465">
348
</page>
<table confidence="0.9999594375">
Feature Set Features Positive Negative Conflict Neutral Accuracy
unigrams,bigrams 0.827 0.590 0.210 0.422 70.699
unigrams 0.830 0.584 0.154 0.413 70.962
non-contextual adjectives,verbs 0.704 0.412 0.000 0.257 63.465
adjectives 0.623 0.430 0.000 0.000 56.410
non-contextual + lexicon unigrams, unigram polarity scores 0.833 0.596 0.154 0.414 71.300
clause 0.823 0.571 0.117 0.0.456 71.170
contextual adjective, verbs within clause 0.784 0.472 0.000 0.257 66. 465
aspects 0.734 0.154 0.000 0.264 59.442
dependencies 0.751 0.235 0.000 0.061 61.257
contextual + lexicon clause, clause polarity score 0.735 0.000 0.000 0.000 58.101
combined unigrams, clause, dependencies, 0.837 0.610 0.162 0.418 71.960
clause polarity score, filtered
stopword list
used original stopword 0.825 0.587 0.078 0.371 70.830
no stopwords used 0.830 0.610 0.151 0.435 72.000
</table>
<tableCaption confidence="0.988366">
Table 1: Feature Analysis for Restaurant Reviews.
</tableCaption>
<table confidence="0.999960625">
Feature Set Features Positive Negative Conflict Neutral Accuracy
unigrams,bigrams 0.827 0.590 0.210 0.422 70.699
unigrams 0.781 0.747 0.110 0.484 71.202
Non-Contextual adjectives,verbs 0.569 0.620 0.000 0.164 54.516
adjectives 0.521 0.613 0.000 0.090 51.230
non-contextual + lexicon unigrams, unigram polarity scores 0.783 0.754 0.179 0.529 71.850
Clause 0.823 0.571 0.117 0.0.456 71.170
Contextual adjective, verbs within clause 0.569 0.620 0.000 0.164 54.510
aspects 0.602 0.259 0.000 0.050 45.240
dependencies 0.590 0.078 0.000 0.000 42.480
contextual + lexicon clause, clause polarity score 0.750 0.705 0.000 0.407 67.230
combined unigrams, clause, dependencies, 0.786 0.752 0.100 0.498 71.600
clause polarity score, filtered
stopword list
weka stopword list 0.780 0.744 0.113 0.442 70.590
no stopwords 0.782 0.758 0.154 0.530 72.170
</table>
<tableCaption confidence="0.999536">
Table 3: Feature Analysis for Laptop Reviews.
</tableCaption>
<bodyText confidence="0.99982515">
same dataset which was used to prepare training
data, while restaurant was from a different source.
We observed that, although aspect terms produced
better results with cross validation, it did not hap-
pen in the case of test data. The restaurant test data
produced better accuracy without aspect term fea-
tures, while laptop test data produced better accu-
racy with aspect term features. We submitted our
systems without using aspect terms as features. If
aspect terms were used as features, the laptop test
data would have been classified with an accuracy
of 60.8 %. Another interesting observation is, uni-
grams produce better results on their own, as com-
pared to adjectives and verbs. Dependency and
clauses also seem to be very important features,
since they produce an accuracy of above 60% on
their own. We also observed that some stopwords
are important features for this task, and complete
removal of stopwords lowers the classification ac-
curacy.
</bodyText>
<table confidence="0.998616">
Domain Baseline Best Submitted Debugged
System System System
laptop 51.07 70.48 57.03 59.15
restaurant 64.28 80.95 70.70 71.44
</table>
<tableCaption confidence="0.999762">
Table 4: Results on Gold Standard Data.
</tableCaption>
<sectionHeader confidence="0.997457" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994454545455">
We presented an analysis and evaluation of syn-
tactic and lexical features for performing sentence
level aspect based sentiment analysis. Our fea-
tures depend on part of speech tagging and depen-
dency parsing, and therefore the accuracy might
vary with different parsers. Although our system
did not produce the highest accuracy for the task, it
is capable of achieving accuracies much above the
baselines. Therefore, the proposed features can be
worth testing on different datasets and can be used
in combination with other features.
</bodyText>
<sectionHeader confidence="0.953025" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99961925">
This work has been funded by the Euro-
pean project EUROSENTIVIENT under grant no.
296277, and the Science Foundation Ireland under
Grant Number SFI/12/RC/2289 (Insight Center).
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9911458">
Stefano Esuli Andrea Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
</reference>
<page confidence="0.991035">
349
</page>
<reference confidence="0.999662951219512">
(LREC’10). European Language Resources Associ-
ation (ELRA).
Paula Chesley. 2006. Using verbs and adjectives to
automatically classify blog sentiment. In In Pro-
ceedings of AAAI-CAAW-06, the Spring Symposia
on Computational Approaches, pages 27–29.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environ-
ment.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software, an update.
SIGKDD Explorations, 11:10–18.
Arun Meena and Prabhakar T.V. 2007. Sentence level
sentiment analysis in the presence of conjuncts us-
ing linguistic analysis. In ECIR, volume 4425 of
Lecture Notes in Computer Science. Springer.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ’02,
pages 79–86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval 2014, Dublin, Ireland.
Hassan Saif, Yulan He, and Harith Alani. 2012. Se-
mantic sentiment analysis of twitter. In Interna-
tional Semantic Web Conference (1), volume 7649
of Lecture Notes in Computer Science, pages 508–
524. Springer.
Wilson Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, pages 399–433.
</reference>
<page confidence="0.997681">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277543">
<title confidence="0.994228">INSIGHT Galway: Syntactic and Lexical Features for Aspect Sentiment Analysis</title>
<author confidence="0.764497">Sapna</author>
<affiliation confidence="0.919918">Insight Centre for Data National University of</affiliation>
<address confidence="0.564893">Galway</address>
<author confidence="0.761154">Paul</author>
<affiliation confidence="0.994746">Insight Centre for Data National University of</affiliation>
<address confidence="0.789942">Galway</address>
<abstract confidence="0.998860642857143">This work analyses various syntactic and lexical features for sentence level aspect based sentiment analysis. The task focuses on detection of a writer’s sentiment towards an aspect which is explicitly mentioned in a sentence. The target sentiment polarities are positive, negative, conflict and neutral. We use a supervised learning approach, evaluate various features and report accuracies which are much higher than the provided baselines. Best features include unigrams, clauses, dependency relations and SentiWordNet polarity scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Esuli Andrea Baccianella</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="4745" citStr="Baccianella and Sebastiani, 2010" startWordPosition="725" endWordPosition="728">larity, where more than one aspect term can be tagged in a sentence. 3.1 Feature Sets We divide the candidate features into four feature sets. 1. Non-contextual: These features comprise of training vocabulary. They do not target aspect based sentiments, but the overall sentiment of the sentence. There might be cases where the aspect based sentiment is same as the overall sentiment of the sentence. The feature set comprises of three feature types, unigrams, bigrams, adjectives and verbs of the sentence. 2. Lexicon Non-Contextual: These features are the Sentiwordnet v3.0 polarity scores (Andrea Baccianella and Sebastiani, 2010) of the words obtained from the best noncontextual feature type. This feature set would include two numerical features, positive polarity score and negative polarity score of the best non-contextual feature types. Best non-contextual feature type is decided by comparing the classification accuracies of individual feature types, with cross validation on the training data (Table 1). We evaluated two algorithms to obtain the positive and negative polarities of words using SentiWordNet. Later, we would provide details of these algorithms. 3. Contextual: These features target aspect based sentiment</context>
</contexts>
<marker>Baccianella, Sebastiani, 2010</marker>
<rawString>Stefano Esuli Andrea Baccianella and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
</authors>
<title>Using verbs and adjectives to automatically classify blog sentiment. In</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-CAAW-06, the Spring Symposia on Computational Approaches,</booktitle>
<pages>27--29</pages>
<contexts>
<context position="2405" citStr="Chesley, 2006" startWordPosition="351" endWordPosition="353">aspect terms, clause in which the aspect This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ term appears, unigrams, and sum of lexicon based sentiment polarities of the words in the clause. 2 Related Work Pang et al. (2002) proved that unigrams and bigrams, adjectives and part of speech tags are important features for a machine learning based sentiment classifier. Later, verbs and adjectives were also identified as important features (Chesley, 2006). Meena and Prabhakar (2007) performed sentence level sentiment analysis using rules based on clauses of a sentence. However, in our case we cannot simply consider the adjectives and verbs as features, since they might relate to different aspects. For example, in the sentence ‘The pizza is the best if you like thin crusted pizza.’, sentiment towards ‘pizza’ is positive because of the adjective ‘best’; however for the term ‘thin crusted pizza’, ‘like’ would be the sentiment verb. Therefore, only those adjectives and verbs which relate to the target aspect, can be considered as the indicator of </context>
</contexts>
<marker>Chesley, 2006</marker>
<rawString>Paula Chesley. 2006. Using verbs and adjectives to automatically classify blog sentiment. In In Proceedings of AAAI-CAAW-06, the Spring Symposia on Computational Approaches, pages 27–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasser EL-Manzalawy</author>
<author>Vasant Honavar</author>
</authors>
<title>WLSVM: Integrating LibSVM into Weka Environment.</title>
<date>2005</date>
<contexts>
<context position="8979" citStr="EL-Manzalawy and Honavar, 2005" startWordPosition="1417" endWordPosition="1420">gative score. One reason for keeping the positive and negative scores separate in our algorithm is that the task also involves sentiment classes conflict and neutral. Using only the overall polarity score results in a loss of information in the case of very low negativity and positivity (neutral sentiments), or high but comparable negativity and positivity (conflicting sentiments). Also, our algorithm produced better results when used with an SVM classifier, with features as unigrams and their polarity scores. 3.2 Classifier Model Our system is built on the state of the art LibSVM classifier (EL-Manzalawy and Honavar, 2005). We used Weka 3.7.10 toolkit (Hall et al., 2009) for our experiments. The parameters 3 of the SVM classifier are tuned to the values which give best results with unigrams. Table 2 provides the tuned parameters, rest of the parameters are set at default values. Pre-processing: We perform stemming using Weka’s implementation of Snowball stemmer, convert strings to lower case and filter out stopwords. We use a customised list of stopwords, based on our observations of the data. The customised list is created using the stopword list of Weka, with certain words removed. For example, negators like </context>
</contexts>
<marker>EL-Manzalawy, Honavar, 2005</marker>
<rawString>Yasser EL-Manzalawy and Vasant Honavar, 2005. WLSVM: Integrating LibSVM into Weka Environment.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software, an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<pages>11--10</pages>
<contexts>
<context position="9028" citStr="Hall et al., 2009" startWordPosition="1426" endWordPosition="1429"> scores separate in our algorithm is that the task also involves sentiment classes conflict and neutral. Using only the overall polarity score results in a loss of information in the case of very low negativity and positivity (neutral sentiments), or high but comparable negativity and positivity (conflicting sentiments). Also, our algorithm produced better results when used with an SVM classifier, with features as unigrams and their polarity scores. 3.2 Classifier Model Our system is built on the state of the art LibSVM classifier (EL-Manzalawy and Honavar, 2005). We used Weka 3.7.10 toolkit (Hall et al., 2009) for our experiments. The parameters 3 of the SVM classifier are tuned to the values which give best results with unigrams. Table 2 provides the tuned parameters, rest of the parameters are set at default values. Pre-processing: We perform stemming using Weka’s implementation of Snowball stemmer, convert strings to lower case and filter out stopwords. We use a customised list of stopwords, based on our observations of the data. The customised list is created using the stopword list of Weka, with certain words removed. For example, negators like ’not’, ’didn’t’ etc. are important for negative s</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software, an update. SIGKDD Explorations, 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arun Meena</author>
<author>T V Prabhakar</author>
</authors>
<title>Sentence level sentiment analysis in the presence of conjuncts using linguistic analysis.</title>
<date>2007</date>
<booktitle>In ECIR,</booktitle>
<volume>4425</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="2433" citStr="Meena and Prabhakar (2007)" startWordPosition="354" endWordPosition="357">ause in which the aspect This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ term appears, unigrams, and sum of lexicon based sentiment polarities of the words in the clause. 2 Related Work Pang et al. (2002) proved that unigrams and bigrams, adjectives and part of speech tags are important features for a machine learning based sentiment classifier. Later, verbs and adjectives were also identified as important features (Chesley, 2006). Meena and Prabhakar (2007) performed sentence level sentiment analysis using rules based on clauses of a sentence. However, in our case we cannot simply consider the adjectives and verbs as features, since they might relate to different aspects. For example, in the sentence ‘The pizza is the best if you like thin crusted pizza.’, sentiment towards ‘pizza’ is positive because of the adjective ‘best’; however for the term ‘thin crusted pizza’, ‘like’ would be the sentiment verb. Therefore, only those adjectives and verbs which relate to the target aspect, can be considered as the indicator of their polarity. Wilson et al</context>
</contexts>
<marker>Meena, Prabhakar, 2007</marker>
<rawString>Arun Meena and Prabhakar T.V. 2007. Sentence level sentiment analysis in the presence of conjuncts using linguistic analysis. In ECIR, volume 4425 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2175" citStr="Pang et al. (2002)" startWordPosition="316" endWordPosition="319"> negative, neutral and conflict. We employ a statistical classifier and experiment with various syntactic and lexical features. Selected features for the submitted system include words which hold certain dependency relations with the aspect terms, clause in which the aspect This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ term appears, unigrams, and sum of lexicon based sentiment polarities of the words in the clause. 2 Related Work Pang et al. (2002) proved that unigrams and bigrams, adjectives and part of speech tags are important features for a machine learning based sentiment classifier. Later, verbs and adjectives were also identified as important features (Chesley, 2006). Meena and Prabhakar (2007) performed sentence level sentiment analysis using rules based on clauses of a sentence. However, in our case we cannot simply consider the adjectives and verbs as features, since they might relate to different aspects. For example, in the sentence ‘The pizza is the best if you like thin crusted pizza.’, sentiment towards ‘pizza’ is positiv</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 79–86, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval 2014,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1276" citStr="Pontiki et al., 2014" startWordPosition="181" endWordPosition="184">valuate various features and report accuracies which are much higher than the provided baselines. Best features include unigrams, clauses, dependency relations and SentiWordNet polarity scores. 1 Introduction The term aspect refers to the features or aspects of a product, service or topic being discussed in a text. The task of detection of sentiment towards these aspects involves two major processing steps, identifying the aspects in the text and identifying the sentiments towards these aspects. Our work describes a submitted system in the Aspect Based Sentiment Analysis task of SemEval 2014 (Pontiki et al., 2014). The task was further divided into 4 subtasks; our work corresponds to the subtask 2, called Aspect Term Polarity Detection. We predict the polarity of sentiments expressed towards the aspect terms which are already annotated in a sentence. The target polarity types are positive, negative, neutral and conflict. We employ a statistical classifier and experiment with various syntactic and lexical features. Selected features for the submitted system include words which hold certain dependency relations with the aspect terms, clause in which the aspect This work is licensed under a Creative Commo</context>
<context position="3475" citStr="Pontiki et al., 2014" startWordPosition="528" endWordPosition="531">like’ would be the sentiment verb. Therefore, only those adjectives and verbs which relate to the target aspect, can be considered as the indicator of their polarity. Wilson et al. (2009) showed that the words which share certain dependency relations with aspect terms, tend to indicate the sentiments expressed towards those terms. Saif et al. (2012) showed the co-relation between topic and sentiment polarity in tweets, and asserted that majority of people tend to express similar sentiments towards same topics, especially in the case of positive sentiments. The baseline approach for this task (Pontiki et al., 2014) also associates polarity with aspect terms. Therefore, we also consider aspect term as a potential feature. Our approach for this task is based on our observation of the data, with a provenance of the above mentioned findings. 3 Approach We employ a statistical classifier which trains on the provided training datasets. 346 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 346–350, Dublin, Ireland, August 23-24, 2014. Datasets: Training datasets comprise of 3000 sentences from laptop and restaurant reviews. Training sentences were tagged with the target</context>
<context position="10563" citStr="Pontiki et al., 2014" startWordPosition="1671" endWordPosition="1674">ared against no stopword removal, and original stopword list. G R C E Z 0.10 1.0 2 1.0 normalise Table 2: Parameter Settings for SVM Classifier. 3http://weka.sourceforge.net/doc.stable/weka/classifiers/ functions/LibSVM.html 3.3 Feature Evaluation We evaluated our features using 8-fold cross validation on the training data. We evaluated each feature by using it as the only feature for the classifier (Tables 1, 3). We performed experiments on different combinations of features, but we only present the best performing combination of features in the last row of the tables. The baseline approach (Pontiki et al., 2014) provided by the organisers, produced an accuracy of 47% for laptop and 57% for restaurant, by splitting the training data. Metrics include, F score for each class, and overall classification accuracy. F score ranges from 0-1, and overall accuracy range from 0-100. 4 Submission and Results Submission involved the prediction of sentiment polarity towards the already tagged aspect terms in two test datasets. There were 800 sentences in each test dataset. The laptop test dataset was obtained by dividing the original laptop data into training and test. However, restaurant test dataset and training</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation, SemEval 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Semantic sentiment analysis of twitter.</title>
<date>2012</date>
<booktitle>In International Semantic Web Conference</booktitle>
<volume>1</volume>
<pages>508--524</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3205" citStr="Saif et al. (2012)" startWordPosition="484" endWordPosition="487"> verbs as features, since they might relate to different aspects. For example, in the sentence ‘The pizza is the best if you like thin crusted pizza.’, sentiment towards ‘pizza’ is positive because of the adjective ‘best’; however for the term ‘thin crusted pizza’, ‘like’ would be the sentiment verb. Therefore, only those adjectives and verbs which relate to the target aspect, can be considered as the indicator of their polarity. Wilson et al. (2009) showed that the words which share certain dependency relations with aspect terms, tend to indicate the sentiments expressed towards those terms. Saif et al. (2012) showed the co-relation between topic and sentiment polarity in tweets, and asserted that majority of people tend to express similar sentiments towards same topics, especially in the case of positive sentiments. The baseline approach for this task (Pontiki et al., 2014) also associates polarity with aspect terms. Therefore, we also consider aspect term as a potential feature. Our approach for this task is based on our observation of the data, with a provenance of the above mentioned findings. 3 Approach We employ a statistical classifier which trains on the provided training datasets. 346 Proc</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Semantic sentiment analysis of twitter. In International Semantic Web Conference (1), volume 7649 of Lecture Notes in Computer Science, pages 508– 524. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Theresa</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics,</title>
<date>2009</date>
<pages>399--433</pages>
<marker>Theresa, Wiebe, Hoffmann, 2009</marker>
<rawString>Wilson Theresa, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, pages 399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>