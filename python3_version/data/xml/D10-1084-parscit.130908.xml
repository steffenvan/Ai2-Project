<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.985507">
Classifying Dialogue Acts in One-on-one Live Chats
</title>
<author confidence="0.992548">
Su Nam Kim,&apos; Lawrence Cavedon° and Timothy Baldwin&apos;
</author>
<affiliation confidence="0.9255675">
4 Dept of Computer Science and Software Engineering, University of Melbourne
C7 School of Computer Science and IT, RMIT University
</affiliation>
<email confidence="0.986981">
sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999579933333334">
We explore the task of automatically classify-
ing dialogue acts in 1-on-1 online chat forums,
an increasingly popular means of providing
customer service. In particular, we investi-
gate the effectiveness of various features and
machine learners for this task. While a sim-
ple bag-of-words approach provides a solid
baseline, we find that adding information from
dialogue structure and inter-utterance depen-
dency provides some increase in performance;
learners that account for sequential dependen-
cies (CRFs) show the best performance. We
report our results from testing using a corpus
of chat dialogues derived from online shop-
ping customer-feedback data.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997967945454546">
Recently, live chats have received attention due to
the growing popularity of chat services and the in-
creasing body of applications. For example, large
organizations are increasingly providing support or
information services through live chat. One advan-
tage of chat-based customer service over conven-
tional telephone-based customer service is that it
becomes possible to semi-automate aspects of the
interaction (e.g. conventional openings or canned
responses to standard questions) without the cus-
tomer being aware of it taking place, something that
is not possible with speech-based dialogue systems
(as synthesised speech is still easily distinguishable
from natural speech). Potentially huge savings can
be made by organisations providing customer help
services if we can increase the degree of automation
of live chat.
Given the increasing impact of live chat services,
there is surprisingly little published computational
linguistic research on the topic. There has been sub-
stantially more work done on dialogue and dialogue
corpora, mostly in spoken dialogue (e.g. Stolcke et
al. (2000)) but also multimodal dialogue systems in
application areas such as telephone support service
(Bangalore et al., 2006) and tutoring systems (Lit-
man and Silliman, 2004). Spoken dialogue analysis
introduces many complications related to the error
inherent in current speech recognition technologies.
As an instance of written dialogue, an advantage of
live chats is that recognition errors are not such an is-
sue, although the nature of language used in chat is
typically ill-formed and turn-taking is complicated
by the semi-asynchronous nature of the interaction
(e.g. Werry (1996)).
In this paper, we investigate the task of automatic
classification of dialogue acts in 1-on-1 live chats,
focusing on “information delivery” chats since these
are proving increasingly popular as part of enter-
prise customer-service solutions. Our main chal-
lenge is to develop effective features and classifiers
for classifying aspects of 1-on-1 live chat. Much of
the work on analysing dialogue acts in spoken di-
alogues has relied on non-lexical features, such as
prosody and acoustic features (Stolcke et al., 2000;
Julia and Iftekharuddin, 2008; Sridhar et al., 2009),
which are not available for written dialogues. Pre-
vious dialogue-act detection for chat systems has
used bags-of-words (hereafter, BoW) as features
for dialogue-act detection; this simple approach
has shown some promise (e.g. Bangalore et al.
(2006), Louwerse and Crossley (2006) and Ivanovic
(2008)). Other features such as keywords/ontologies
(Purver et al., 2005; Forsyth, 2007) and lexical cues
(Ang et al., 2005) have also been used for dialogue
act classification.
</bodyText>
<page confidence="0.95702">
862
</page>
<note confidence="0.817678">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999586">
In this paper, we first re-examine BoW features
for dialogue act classification. As a baseline, we
use the work of Ivanovic (2008), which explored 1-
grams and 2-grams with Boolean values in 1-on-1
live chats in the MSN Online Shopping domain (this
dataset is described in Section 5). Although this
work achieved reasonably high performance (up to
a micro-averaged F-score of around 80%), we be-
lieve that there is still room for improvement using
BoW only. We extend this work by using ideas from
related research such as text categorization (Debole
and Sebastiani, 2003), and explore variants of BoW
based on analysis of live chats, along with feature
weighting. Finally, our main aim is to explore new
features based on dialogue structure and dependen-
cies between utterances&apos; that can enhance the use of
BoW for dialogue act classification. Our hypothesis
is that, for task-oriented 1-on-1 live chats, the struc-
ture and interactions among utterances are useful in
predicting future dialogue acts: for example, conver-
sations typically start with a greeting, and questions
and answers typically appear as adjacency pairs in
a conversation. Therefore, we propose new features
based on structural and dependency information de-
rived from utterances (Sections 4.2 and 4.3).
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991655851851852">
While there has been significant work on classify-
ing dialogue acts, the bulk of this has been for spo-
ken dialogue. Most such work has considered: (1)
defining taxonomies of dialogue acts; (2) discover-
ing useful features for the classification task; and (3)
experimenting with different machine learning tech-
niques. We focus here on (2) and (3); we return to
(1) in Section 3.
For classifying dialogue acts in spoken dialogue,
various features such as dialogue cues, speech char-
acteristics, and n-grams have been proposed. For
example, Samuel et al. (1998) utilized the charac-
teristics of spoken dialogues and examined speaker
direction, punctuation marks, cue phrases and n-
grams for classifying spoken dialogues. Jurafsky et
al. (1998) used prosodic, lexical and syntactic fea-
tures for spoken dialogue classification. More re-
cently, Julia and Iftekharuddin (2008) and Sridhar et
&apos;An utterance is the smallest unit to deliver a participant’s
message(s) in a turn.
al. (2009) achieved high performance using acous-
tic and prosodic features. Louwerse and Cross-
ley (2006), on the other hand, used various n-gram
features—which could be adapted to both spoken
and written dialogue—and tested them using the
Map Task Corpus (Anderson et al., 1991). Extend-
ing the discourse model used in previous work, Ban-
galore et al. (2006) used n-grams from the previous
1–3 utterances in order to classify dialogue acts for
the target utterance.
There has been substantially less effort on clas-
sifying dialogue acts in written dialogue: Wu et al.
(2002) and Forsyth (2007) have used keyword-based
approaches for classifying online chats; Ivanovic
(2008) tested the use of n-gram features for 1-on-1
live chats with MSN Online Shopping assistants.
Various machine learning techniques have been
investigated for the dialogue classification task.
Samuel et al. (1998) used transformation-based
learning to classify spoken dialogues, incorporat-
ing Monte Carlo sampling for training efficiency.
Stolcke et al. (2000) used Hidden Markov Mod-
els (HMMs) to account for the structure of spo-
ken dialogues, while Wu et al. (2002) also used
transformation- and rule-based approaches plus
HMMs for written dialogues. Other researchers
have used Bayesian based approaches, such as
naive Bayes (e.g. (Grau et al., 2004; Forsyth,
2007; Ivanovic, 2008)) and Bayesian networks (e.g.
(Keizer, 2001; Forsyth, 2007)). Maximum entropy
(e.g. (Ivanovic, 2008)), support vector machines
(e.g. (Ivanovic, 2008)), and hidden Markov models
(e.g. (Bui, 2003)) have also all been applied to auto-
matic dialogue act classification.
</bodyText>
<sectionHeader confidence="0.993388" genericHeader="method">
3 Dialogue Acts
</sectionHeader>
<bodyText confidence="0.999849818181818">
A number of dialogue act taxonomies have been pro-
posed, designed mainly for spoken dialogue. Many
of these use the Dialogue Act Markup in Several
Layers (DAMSL) scheme (Allen and Core, 1997).
DAMSL was originally applied to the TRAINS cor-
pus of (transcribed) spoken task-oriented dialogues,
but various adaptations of it have since been pro-
posed for specific types of dialogue. The Switch-
board corpus (Godfrey et al., 1992) defines 42 types
of dialogue acts from human-to-human telephone
conversations. The HCRC Map Task corpus (Ander-
</bodyText>
<page confidence="0.997774">
863
</page>
<bodyText confidence="0.961351227272727">
son et al., 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online
model task-based spoken conversations. Shopping live chats, where a user requests assis-
For casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the
define 15 dialogue act tags based on previously- commercial agent asks the customer questions and
defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved solid
Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke performance over this data (around 80% F-score).
et al., 2000). Forsyth (2007) defines 15 dialogue acts While 1-grams performed well (as live chat utter-
for casual online conversations, based on 16 conver- ances are generally shorter than, e.g., sentences in
sations with 10,567 utterances. Ivanovic (2008) pro- news articles), we expect 2- and 3-grams are needed
poses 12 dialogue acts based on DAMSL for 1-on-1 to detect formulaic expressions, such as No problem
online customer service chats. and You are welcome. We would also expect a pos-
Ivanovic’s set of dialogue acts for chat dia- itive effect from combining n-grams due to increas-
logues has significant overlap with the dialogue act ing the coverage of feature words. We thus test 1-,
sets of Wu et al. (2002) and Forsyth (2007) (e.g. 2- and 3-grams individually, as well as the combi-
GREETING, EMOTION/EXPRESSION, STATEMENT, nation of 1- and 2-grams together (i.e. 1+2-grams)
QUESTION). In our work, we re-use the set of dia- and 1-, 2- and 3-grams (i.e. 1+2+3-grams); this re-
logue acts proposed in Ivanovic (2008), due to our sults in five BoW sets. Also, unlike Ivanovic (2008),
targeting the same task of 1-on-1 IM chats, and in- we test both raw words and lemmas; we expect the
deed experimenting over the same dataset. The def- use of lemmas to perform better than raw words as
initions of the dialogue acts are provided in Table 1, our data is less noisy. As the feature weight, in addi-
along with examples. tion to simple Boolean, we also experiment with TF,
TF·IDF and Information Gain (IG).
</bodyText>
<sectionHeader confidence="0.995716" genericHeader="method">
4 Feature Selection
</sectionHeader>
<subsectionHeader confidence="0.998774">
4.2 Structural Information
</subsectionHeader>
<bodyText confidence="0.9994732">
In this section, we describe our initial dialogue-act
classification experiments using simple BoW fea-
tures, and then introduce two groups of new fea-
tures based on structural information and dependen-
cies between utterances.
</bodyText>
<subsectionHeader confidence="0.999353">
4.1 Bag-of-Words
</subsectionHeader>
<bodyText confidence="0.999969682926829">
n-gram-based BoW features are simple yet effec-
tive for identifying similarities between two utter-
ances, and have been used widely in previous work
on dialogue act classification for online chat di-
alogues (Louwerse and Crossley, 2006; Ivanovic,
2008). However, chats containing large amounts of
noise such as typos and emoticons pose a greater
challenge for simple BoW approaches. On the other
hand, keyword-based features (Forsyth, 2007) have
achieved high performance; however, keyword-
based approaches are more domain-dependent. In
this work, we chose to start with a BoW approach
based on our observation that commercial live chat
services contain relatively less noise; in particular,
the commercial agent tends to use well-formed, for-
mulaic prose.
Previously, Ivanovic (2008) explored Boolean 1-
Our motivation for using structural information as
a feature is that the location of an utterance can be
a strong predictor of the dialogue act. That is, dia-
logues are sequenced, comprising turns (i.e. a given
user is sending text), each of which is made up of
one or more messages (i.e. strings sent by the user).
Structured classification methods which make use of
this sequential information have been applied to re-
lated tasks such as tagging semantic labels of key
sentences in biomedical domains (Chung, 2009) and
post labels in web forums (Kim et al., 2010).
Based on the nature of live chats, we observed that
the utterance position in the chat, as well as in a turn,
plays an important role when identifying its dialogue
act. For example, an utterance such as Hello will oc-
cur at the beginning of a chat while an utterance such
as Have a nice day will typically appear at the end.
The position of utterances in a turn can also help
identify the dialogue act; i.e. when there are several
utterances in a turn, utterances are related to each
other, and thus examining the previous utterances in
the same turn can help correctly predict the target
utterance. For example, the greeting (Welcome to ..)
and question (How may I help you?) could occur in
</bodyText>
<page confidence="0.99513">
864
</page>
<table confidence="0.57347655">
Dialogue Act, Definition and Examples
CONVENTIONAL CLOSING: Various ways of ending a conversation e.g. Bye Bye
CONVENTIONAL OPENING: Greeting and other ways of starting a conversation e.g. Hello Customer
DOWNPLAYER: A backwards-linking label often used after THANKS to down play the contribution
e.g. You are welcome, my pleasure
EXPRESSIVE: An acknowledgement of a previous utterance or an indication of the speaker’s mood.
e.g. haha, : −) wow
NO ANSWER: A backward-linking label in the form of a negative response to a YESNO-QUESTION e.g. no, nope
OPEN QUESTION: A question that cannot be answered with only a yes or no. The answer is usually
some form of explanation or statement. e.g. how do I use the international version?
REQUEST: Used to express a speaker’s desire that the learner do something – either performing some action
or simply waiting. e.g. Please let me know how I can assist you on MSN Shopping today.
RESPONSE ACK: A backward-linking acknowledgement of the previous utterance. Used to confirm
that the previous utterance was received/accepted. e.g. Sure
STATEMENT: Used for assertions that may state a belief or commit the speaker to doing something
e.g. I am sending you the page which will pop up in a new window on your screen.
THANKS: Conventional thanks e.g. Thank you for contacting us.
YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g. yes, yeah
YESNO QUESTION: A closed question which can be answered in the affirmative or negative.
e.g. Did you receive the page, Customer?
</table>
<tableCaption confidence="0.997627">
Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008)
</tableCaption>
<listItem confidence="0.620216666666667">
the same turn. We also noticed that identifying the
utterance author can help classify the dialogue act
(previously used in Ivanovic (2008)).
</listItem>
<bodyText confidence="0.9769825">
Based on these observations, we tested the follow-
ing four structural features:
</bodyText>
<listItem confidence="0.997883">
• Author information,
• Relative position in the chat,
• Author + Relative position,
• Author + Turn-relative position among utter-
ances in a given turn.
</listItem>
<bodyText confidence="0.999971692307692">
We illustrate our structural features in Table 2,
which shows an example of a 1-on-1 live chat. The
participants are the agent (A) and customer (C); Uxx
indicates an utterance (U) with ID number xx. This
conversation has 42 utterances in total. The relative
position is calculated by dividing the utterance num-
ber by the total number of utterances in the dialogue;
the turn-relative position is calculated by dividing
the utterance position by the number of utterances
in that turn. For example, for utterance 4 (U4), the
relative position is 442, while its turn-relative position
is 23 since U4 is the second utterance among U3,4,5
that the customer makes in a single turn.
</bodyText>
<subsectionHeader confidence="0.999745">
4.3 Utterance Dependency
</subsectionHeader>
<bodyText confidence="0.99996732">
In recent work, Kim et al. (2010) demonstrated the
importance of dependencies between post labels in
web forums. The authors introduced series of fea-
tures based on structural dependencies among posts.
They used relative position, author information and
automatically predicted labels from previous post(s)
as dependency features for assigning a semantic la-
bel to the current target post.
Similarly, by examining our chat corpus, we ob-
served significant dependencies between utterances.
First, 1-on-1 (i.e. agent-to-user) dialogues often con-
tain dependencies between adjacent utterances by
different authors. For example, in Table 2, when the
agent asks Is that correct?, the expected response
from the user is a Yes or No. Another example is
that when the agent makes a greeting, such as Have
a nice day, then the customer will typically respond
with a greeting or closing remark, and not a Yes or
No. Second, the flow of dialogues is in general co-
hesive, unless the topic of utterances changes dra-
matically (e.g. U5: Are you still there?, U22: brb
in 1 min in Table 2). Third, we observed that be-
tween utterances made by the same author (either
agent or user), the target utterance relies on previous
utterances made by the same author, especially when
</bodyText>
<page confidence="0.993884">
865
</page>
<table confidence="0.967987192307692">
ID Utterance
A:U1 Hello Customer, welcome to MSN Shopping.
A:U2 My name is Krishna and I am your
online Shopping assistant today.
C:U3 Hello!
C:U4 I’m trying to find a sports watch.
C:U5 are you still there?
A:U6 I understand that you are looking for sports
watch.
A:U7 Is that correct?
C:U8 yes, that is correct.
..
C:U22 brb in 1 min
C:U23 Thank you for waiting
..
A:U37 Thank you for allowing us to assist
you regarding wrist watch.
A:U38 I hope you found our session today helpful.
A:U39 If you have any additional questions or
you need additional information,
please log in again to chat with us.
We are available 24 hours a day, 7 days a
week for your help.
A:U40 Thank you for contacting MSN Shopping.
A:U41 Have a nice day! Good Bye and Take Care.
C:U42 You too.
</table>
<tableCaption confidence="0.98851">
Table 2: An example of a 1-on-1 live chat, with turn and
utterance structure
</tableCaption>
<bodyText confidence="0.985937142857143">
the agent and user repeatedly question and answer.
With these observations, we checked the likelihood
of dialogue act pairings between two adjacent utter-
ances, as well as between two adjacent utterances
made by the same author. Overall, we found strong
co-occurrence (as measured by number of occur-
rences of labels across adjacency pairs) between cer-
tain pairs of dialogue acts (e.g. (YESNO QUESTION
—*YES ANSWER/NO ANSWER) and (REQUEST
—*YES ANSWER)). STATEMENT, on the other
hand, can associate with most other dialogue acts.
Based on this, we designed the following five ut-
terance dependency features; by combining these,
we obtain 31 feature sets.
</bodyText>
<listItem confidence="0.997943333333334">
1. Dependency of utterances regardless of author
(a) Dialogue act of previous utterance
(b) Accumulated dialogue act(s) of previous
utterances
(c) Accumulated dialogue acts of previous ut-
terances in a given turn
2. Dependency of utterances made by a single au-
thor
(a) Dialogue act of previous utterance
by same author; a dialogue act can be in
the same turn or in the previous turn
(b) Accumulated dialogue acts of previous
utterances by same author; dialogue acts
can be in the same turn or in the previous
turn
</listItem>
<bodyText confidence="0.9988206">
To capture utterance dependency, Bangalore et al.
(2006) previously used n-gram BoW features from
the previous 1–3 utterances. In contrast, instead of
using utterances which indirectly encode dialogue
acts, we directly use the dialogue act classifications,
as done in Stolcke et al. (2000). The motivation is
that, due to the high performance of simple BoW
features, using dialogue acts directly would cap-
ture the dependency better than indirect information
from utterances, despite introducing some noise. We
do not build a probabilistic model of dialogue tran-
sitions the way Stolcke et al. (2000) does, but follow
an approach similar to that used in Kim et al. (2010)
in using predicted dialogue act(s) labels learned in
previous step(s) as a feature.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="method">
5 Experiment Setup
</sectionHeader>
<bodyText confidence="0.999743529411765">
As stated earlier, we use the data set from Ivanovic
(2008) for our experiments; it contains 1-on-1 live
chats from an information delivery task. This dataset
contains 8 live chats, including 542 manually-
segmented utterances. The maximum and minimum
number of utterances in a dialogue are 84 and 42,
respectively; the maximum number of utterances in
a turn is 14. The live chats were manually tagged
with the 12 dialogue acts described in Section 3.
The utterance distribution over the dialogue acts is
described in Table 3.
For our experiments, we calculated TF, TF·IDF
and IG (Information Gain) over the utterances,
which were optionally lemmatized with the morph
tool (Minnen et al., 2000). We then built a dialogue
act classifier using three different machine learn-
ers: SVM-HMM (Joachims, 1998),2 naive Bayes
</bodyText>
<footnote confidence="0.996569">
2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
</footnote>
<page confidence="0.997842">
866
</page>
<figure confidence="0.959505142857143">
Utterance number
15
12
15
5
12
17
</figure>
<table confidence="0.8255145">
n-gram Boolean TF TF·IDF IG
1 .731 .511 .517 .766
2 .603 .530 .601 .614
3 .474 .463 .472 .482
1+2 .756 .511 .522 .777
1+2+3 .773 .511 .528 .777
</table>
<figure confidence="0.9188289375">
Dialogue Act
CONVENTIONAL CLOSING
CONVENTIONAL OPENING
DOWNPLAYER
EXPRESSIVE
NO ANSWER
OPEN QUESTION
REQUEST
28
27 Table 5: Accuracy of different feature representations and
198 weighting methods for SVM-HMM
79
RESPONSE ACK
STATEMENT
THANKS
YES ANSWER
</figure>
<table confidence="0.963776">
Index Learner Ours Ivanovic
Feature Acc. Feature Acc.
Word SVM 1+2+3/B .790 1/B .751
NB 1/B .673 1/B .673
CRF 1/IG .839 1/B .825
Lemma SVM 1+2+3/IG .777 N/A N/A
NB 1/B .672 N/A N/A
CRF 1/B .862 N/A N/A
YESNO QUESTION
</table>
<tableCaption confidence="0.9008042">
Table 3: Dialogue act distribution in the corpus
Table 4: Best accuracy achieved by the different learn-
ers over different feature sets and weighting methods (1
= 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = in-
formation gain)
</tableCaption>
<bodyText confidence="0.999824769230769">
from the WEKA machine learning toolkit (Wit-
ten and Frank, 2005), and Conditional Random
Fields (CRF) using CRF++.3 Note that we chose
to test CRF and SVM-HMM as previous work (e.g.
(Samuel et al., 1998; Stolcke et al., 2000; Chung,
2009)) has shown the effectiveness of structured
classification models on sequential dependencies.
Thus, we expect similar effects with CRF and SVM-
HMM. Finally, we ran 8-fold cross-validation using
the feature sets described above (partitioning across
the 8 sessions). All results are presented in terms
of classification accuracy. The accuracy of a zero-R
(i.e. majority vote) baseline is 0.36.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="method">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.9999">
6.1 Testing Bag-of-Words Features
</subsectionHeader>
<bodyText confidence="0.994295">
Table 4 shows the best accuracy achieved by the dif-
ferent learners, in combination with BoW represen-
</bodyText>
<footnote confidence="0.767833">
3http://crfpp.sourceforge.net/
</footnote>
<bodyText confidence="0.999546583333333">
tations and feature weighting methods. Note that the
CRF learner ran using 1-grams only, as CRF++ does
not accept large numbers of features. As a bench-
mark, we also tested the method in Ivanovic (2008)
and present the best performance over words (rather
than lemmas). Overall, we found using just 1-grams
produced the best performance for all learners, al-
though SVM achieved the best performance when
using all three n-gram orders (i.e. 1+2+3). Since the
utterances are very short, 2-grams or 3-grams alone
are too sparse to be effective. Among the feature
weighting methods, Boolean and IG achieved higher
accuracy than TF and TF·IDF. Likewise, due to the
short utterances, simple Boolean values were often
the most effective. However, as IG was computed
using the training data, it also achieved high perfor-
mance. When comparing the learners, we found that
CRF produced the best performance, due to its abil-
ity to capture inter-utterance dependencies. Finally,
we confirmed that using lemmas results in higher ac-
curacy.
Table 5 shows the accuracy over all feature sets;
for brevity, we show this for SVM only since the
pattern is similar across all learners.
</bodyText>
<subsectionHeader confidence="0.999975">
6.2 Using Structural Information
</subsectionHeader>
<bodyText confidence="0.999672090909091">
In this section, we describe experiments using struc-
tural information—i.e. author and/or position—with
BoWs. As with the base BoW technique, we used
1-gram lemmas with Boolean values, based on the
results from Section 6.1. Table 6 shows the results:
Pos indicates the relative position of an utterance in
the whole dialogue, Author means author informa-
tion, and Postum indicates the relative position of
the utterance in a turn. All methods outperformed
the baseline; methods that surpassed the results for
the simple BoW method (for the given learner) at a
</bodyText>
<page confidence="0.959211333333333">
35
99
867
</page>
<table confidence="0.999808625">
Feature Learners
CRF SVM NB
BoW .862 .731 .672
BoW+Author .860 .655 .649
BoW+Pos .862 .721 .655
BoW+Posabsolute .863 .631 .524
BoW+Author+Pos .875 .700 .642
BoW+Author+Posture, .871 .651 .631
</table>
<tableCaption confidence="0.999922">
Table 6: Accuracy with structural information
</tableCaption>
<bodyText confidence="0.999874333333333">
level of statistical significance (based on randomised
estimation, p &lt; 0.05) are boldfaced.
Overall, using CRFs with Author and Position in-
formation produced better performance than using
BoW alone. Clearly, the ability of CRFs to natively
optimise over structural dependencies provides an
advantage over other learners.
Relative position cannot of course be measured
directly in an actual online application; hence Ta-
ble 6 also includes the use of “absolute position” as
a feature. We see that, for CRF, the absolute posi-
tion feature shows an insignificant drop in accuracy
as compared to the use of relative position. (How-
ever, we do see a significant drop in performance
when using this feature with SVM and NB.)
</bodyText>
<subsectionHeader confidence="0.999832">
6.3 Using Utterance Dependency
</subsectionHeader>
<bodyText confidence="0.998189421052631">
We next combined the inter-utterance dependency
features with the BoW features. Since we use the
dialogue acts directly in utterance dependency, we
first experimented using gold-standard dialogue act
labels. We also tested using the dialogue acts which
were automatically learned in previous steps.
Table 7 shows performance using both the gold-
standard and learned dialogue acts. The differ-
ent features listed are as follows: LabelList/L in-
dicates those corresponding to all utterances in
a dialogue preceding the target utterance; Label-
Prev/P indicates a dialogue act from a previous
utterance; LabelAuthor/A indicates a dialogue act
from a previous utterance by the same author;
and LabelPrevt/LabelAuthort indicates the previ-
ous utterance(s) and previously same-authored ut-
terance(s) in a turn, respectively. Since the accuracy
for SVM and NB using learned labels is similar to
that using gold standard labels, for brevity we report
</bodyText>
<tableCaption confidence="0.9861205">
Table 7: Accuracy for the different learners with depen-
dency features
</tableCaption>
<bodyText confidence="0.999607769230769">
the performance for CRF using learned labels only.
Results that exceed the BoW accuracy at a level of
statistical significance (p &lt; 0.05) are boldfaced.
Utterance dependency features worked well in
combination with CRF only. Individually, Prev and
Prevt (i.e. BoW+P+Pt) helped to achieve higher ac-
curacies, and the Author feature was also benefi-
cial. However, List decreased the performance, as
the flow of dialogues can change, and when a larger
history of dialogue acts is included, it tends to in-
troduce noise. Comparing use of gold-standard and
learned dialogue acts, the reduction in accuracy was
not statistically significant, indicating that we can
</bodyText>
<table confidence="0.9943115">
Features Dialogue Acts
Goldstandard Learned
CRF HMM NB CRF
BoW .862 .731 .672 .862
BoW+LabelList(L) .795 .435 .225 .803
BoW+LabelPrev(P) .875 .661 .364 .876
BoW+LabelAuthor(A) .865 .633 .559 .865
BoW+LabelPrevt(Pt) .873 .603 .557 .873
BoW+LabelAuthort(At) .862 .587 .535 .851
BoW+L+P .804 .428 .227 .808
BoW+L+A .799 .404 .225 .804
BoW+L+Pt .803 .413 .229 .804
BoW+L+At .808 .408 .216 .801
BoW+P+A .873 .631 .517 .869
BoW+P+Pt .878 .579 .539 .875
BoW+P+At .871 .603 .519 .867
BoW+A+Pt .847 .594 .519 .849
BoW+A+At .869 .594 .530 .871
BoW+Pt+At .871 .592 .519 .867
BoW+L+P+A .812 .419 .231 .804
BoW+L+P+Pt .816 .423 .229 .812
BoW+L+P+At .808 .397 .225 .806
BoW+L+A+Pt .810 .388 .225 .810
BoW+L+A+At .812 .415 .216 .801
BoW+L+Pt+At .810 .375 .205 .816
BoW+P+A+Pt .875 .602 .522 .876
BoW+P+A+At .862 .609 .511 .864
BoW+P+Pt+At .873 .594 .515 .867
BoW+A+Pt+At .865 .594 .517 .864
BoW+L+P+A+Pt .817 .410 .231 .810
BoW+L+P+A+At .814 .411 .223 .810
BoW+L+P+Pt+At .816 .382 .205 .806
BoW+L+A+Pt+At .812 .406 .203 .808
BoW+P+A+Pt+At .865 .583 .513 .865
BoW+L+P+A+Pt+At .816 .399 .205 .803
868
Feature CRF SVM NB
C+LabelList .9557 .4613 .2565
C+LabelPrev .9649 .6365 .5720
C+LabelAuthor .9686 .6310 .5424
C+LabelPrevt .9686 .5738 .5738
C+LabelAuthort .9561 .6125 .5332
</table>
<tableCaption confidence="0.9966095">
Table 8: Accuracy with Structural and Dependency Infor-
mation: C means lemmatized Unigram+Position+Author
</tableCaption>
<bodyText confidence="0.999940764705882">
achieve high performance on dialogue act classifi-
cation even with interactively-learned dialogue acts.
We believe this demonstrates the robustness of the
proposed techniques.
Finally, we tested the combination of features
from structural and dependency information. That
is, we used a base feature (unigrams with Boolean
value), relative position, author information, com-
bined with each of the different dependency features
– LabelList, LabelPrev, LabelAuthor, LabelPrevt
and LabelAuthort.
Table 8 shows the performance when using these
combinations, for each dependency feature. As we
would expect, CRFs performed well with the com-
bined features since CRFs can incorporate the struc-
tural and dependency information; the achieved the
highest accuracy of 96.86%.
</bodyText>
<subsectionHeader confidence="0.985123">
6.4 Error Analysis and Future Work
</subsectionHeader>
<bodyText confidence="0.9921618">
Finally, we analyzed the errors of
the best-performing feature set (i.e.
BoW+Position+Author+LabelAuthor). In Ta-
ble 9, we present a confusion matrix of errors,
for CONVENTIONAL CLOSING (Cl), CON-
</bodyText>
<sectionHeader confidence="0.77698175" genericHeader="method">
VENTIONAL OPENING (Op), DOWNPLAYER
(Dp), EXPRESSIVE (Ex), NO ANSWER (No),
OPEN QUESTION (Qu), REQUEST (Rq), RE-
SPONSE ACK (Ack), STATEMENT (St), THANKS
</sectionHeader>
<bodyText confidence="0.994166875">
(Ta), YES ANSWER (Yes), and YESNO QUESTION
(YN). Rows indicate the correct dialogue acts and
columns indicate misclassified dialogue acts.
Looking over the data, STATEMENT is a common
source of misclassification, as it is the majority class
in the data. In particularly, a large number of RE-
QUEST and RESPONSE ACK utterances were tagged
as STATEMENT. We did not include punctuation
such as question marks in our feature sets; includ-
ing this would likely improve results further.
In future work, we plan to investigate methods for
automatically cleansing the data to remove typos,
and taking account of temporal gaps that can some-
times arise in online chats (e.g. in Table 2, there is
a time gap between C:U22 brb in 1 min and C:U23
Thank you for waiting).
</bodyText>
<sectionHeader confidence="0.998355" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995909090909">
We have explored an automated approach for classi-
fying dialogue acts in 1-on-1 live chats in the shop-
ping domain, using bag-of-words (BoW), structural
information and utterance dependency features. We
found that the BoW features perform remarkably
well, with slight improvements when using lemmas
rather than words. Including structural and inter-
utterance dependency information further improved
performance. Of the learners we experimented with,
CRFs performed best, due to their ability to natively
capture sequential dialogue act dependencies.
</bodyText>
<sectionHeader confidence="0.995255" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<copyright confidence="0.421265">
This research was supported in part by funding from
Microsoft Research Asia.
</copyright>
<sectionHeader confidence="0.99525" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991464">
J.Allen and M.Core. Draft of DAMSL: Dialog Act
Markup in Several Layers. The Multiparty Dis-
course Group. University of Rochester, Rochester,
USA. 1997.
A. Anderson, M. Bader, E. Bard, E. Boyle G.M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H.S. Thompson, R. and Weinert.
The HCRC Map Task Corpus. Language and Speech.
1991, 34, pp. 351–366.
J. Ang, Y. Liu and E. Shriberg. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. IEEE International Conference on Acoustics,
Speech, and Signal Processing. 2005, pp, 1061–1064.
S. Bangalore, G. Di Fabbrizio and A. Stent. Learning
the Structure of Task-Driven Human-Human Dialogs.
Proceedings of the 21st COLING and 44th ACL. 2006,
pp. 201–208.
H. H. Bui. A general model for online probabilistic plan
recognition. IJCAI. 2003, pp. 1309–1318.
G.Y Chung. Sentence retrieval for abstracts of random-
ized controlled trials. BMC Medical Informatics and
Decision Making. 2009, 9(10), pp. 1–13.
F. Debole and F. Sebastiani. Supervised term weighting
for automated text categorization. 18th ACM Sympo-
sium on Applied Computing. 2003, pp. 784–788.
</reference>
<page confidence="0.996081">
869
</page>
<table confidence="0.985644615384616">
Cl Op Dp Ex No Qu Rq Ack St Ta Yes YN
Op 0 0 0 0 0 0 0 0 0 0 0 2
Cl 0 0 0 0 0 0 0 0 1 1 0 0
Dp 0 0 0 0 0 0 0 0 0 0 0 0
Ex 0 0 0 0 0 0 0 0 0 0 0 0
No 0 0 0 0 0 0 0 0 0 0 0 0
Qu 0 0 0 0 0 0 0 0 0 0 0 0
Rq 0 0 0 0 0 0 0 0 3 0 0 0
Ack 0 0 1 0 1 0 0 0 5 0 0 0
St 0 0 0 0 0 0 1 0 0 0 0 0
Ta 1 0 0 0 0 0 0 0 0 0 0 0
Yes 0 0 0 0 0 0 0 0 0 0 0 0
YN 0 1 0 0 0 0 0 0 0 0 0 0
</table>
<tableCaption confidence="0.992952">
Table 9: Confusion matrix for errors from the CRF with BoW+Position+Author+LabelAuthor (rows = correct clas-
</tableCaption>
<reference confidence="0.991162149253731">
sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWN-
PLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK
= Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)
E. N. Forsyth. Improving Automated Lexical and Dis-
course Analysis of Online Chat Dialog. Master’s the-
sis. Naval Postgraduate School, 2007.
J. Godfrey and E. Holliman and J. McDaniel. SWITCH-
BOARD: Telephone speech corpus for research and
development. Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing. 1992, pp. 517–520.
S. Grau, E. Sanchis, M. Jose and D. Vilar. Dialogue act
classification using a Bayesian approach. Proceedings
of the 9th Conference on Speech and Computer. 2004.
P. A. Heeman and J. Allen. The Trains 93 Dialogues.
Trains Technical Note 94-2. Computer Science Dept.,
University of Rochester, March 1995.
T. Joachims. Text categorization with support vector ma-
chines: Learning with many relevant features. Pro-
ceedings of European Conference on Machine Learn-
ing. 1998, pp. 137–142.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker and P. Maloor.
MATCH: An Architecture for Multimodal Dialogue
Systems. Proceedings of 40th ACL. 2002, pp. 376–
383.
F. N. Julia and K. M. Iftekharuddin. Dialog Act clas-
sification using acoustic and discourse information of
MapTask Data. Proceedings of the International Joint
Conference on Neural Networks. 2008, pp. 1472–
1479.
D. Jurafsky, E. Shriberg, B Fox and T. Curl. Lexical,
Prosodic, and Syntactic Cues for Dialog Acts. Pro-
ceedings of ACL/COLING-98 Workshop on Discourse
Relations and Discourse Markers. 1998, pp. 114–120.
E. Ivanovic. Automatic instant messaging dialogue us-
ing statistical models and dialogue acts. Master’s The-
sis. The University of Melbourne. 2008.
S. Keizer. A Bayesian Approach to Dialogue Act Clas-
sification. 5th Workshop on Formal Semantics and
Pragmatics of Dialogue. 2001, pp. 210–218.
S.N. Kim and L. Wang and T. Baldwin. Tagging and
Linking Web Forum Posts. Fourteenth Conference on
Computational Natural Language Learning. 2010.
J. Lafferty, A. McCallum and F. Pereira. Conditional
random fields: Probabilistic models for segmenting
and labeling sequence data. Proceedings of ICML.
2001, pp. 282–289.
D. J. Litman and S. Silliman. ITSPOKE: An Intelligent
Tutoring Spoken Dialogue SYstem. Proceedings of
the HLT/NAACL. 2004.
M. M. Louwerse and S. Crossley. Dialog Act Classifica-
tion Using N-Gram Algorithms. FLAIRS Conference,
2006, pp. 758–763.
G. Minnen, J. Carroll and D. Pearce. Applied morpho-
logical processing of English Natural Language Engi-
neering 2000, 7(3), pp. 77–80.
M. Purver, J. Niekrasz and S. Peters. Ontology-Based
Discourse Understanding for a Persistent Meeting As-
sistant. Proc. CHI 2005 Workshop on The Virtuality
Continuum Revisited. 2005.
K. Samuel, Sandra Carberry and K. Vijay-Shanker. Dia-
logue Act Tagging with Transformation-Based Learn-
ing. Proceedings of COLING/ACL 1998. 1998, pp.
1150-1156.
E. Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,
K. Ries, N. Coccaro, R. Martin, M. Meteer and C. Van
</reference>
<page confidence="0.972461">
870
</page>
<reference confidence="0.999875107142857">
Ess-Dykema. Can Prosody Aid the Automatic Clas-
sification of Dialog Acts in Conversational Speech?.
Language and Speech. 1998, 41(3-4), pp. 439–487.
V. R. Sridhar, S. Bangalore and S. Narayanan. Combin-
ing lexical, syntactic and prosodic cues for improved
online dialog act tagging. Computer Speech and Lan-
guage. 2009, 23(4), pp. 407–422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema
and M. Meteer. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics. 2000, 26(3), pp. 339–373.
A. Stolcke and E. Shriberg. Markovian Combination of
Language and Prosodic Models for better Speech Un-
derstanding and Recognition. Invited talk at the IEEE
Workshop on Speech Recognition and Understanding,
Madonna di Campiglio, Italy, December 2001 2001,
C. C. Werry. Linguistic and interactional features of In-
ternet Relay Chat. In S. C. Herring (ed.). Computer-
Mediated Communication. Benjamins, 1996.
I. Witten and E. Frank. Data Mining: Practical Machine
Learning Tools and Techniques. Morgan Kaufmann,
2005.
T. Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M.
Pottenger. Posting act tagging using transformation-
based learning. Proceedings of the Workshop on Foun-
dations of Data Mining and Discovery, IEEE Interna-
tional Conference on Data Mining. 2002.
</reference>
<page confidence="0.998449">
871
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262193">
<title confidence="0.999457">Classifying Dialogue Acts in One-on-one Live Chats</title>
<author confidence="0.606937">Nam</author>
<author confidence="0.606937">Timothy</author>
<affiliation confidence="0.461348">of Computer Science and Software Engineering, University of of Computer Science and IT, RMIT</affiliation>
<abstract confidence="0.998078125">We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>M.Core. Draft of DAMSL: Dialog Act Markup in Several Layers. The Multiparty Discourse Group.</title>
<date>1997</date>
<institution>University of Rochester,</institution>
<location>Rochester, USA.</location>
<marker>Allen, 1997</marker>
<rawString>J.Allen and M.Core. Draft of DAMSL: Dialog Act Markup in Several Layers. The Multiparty Discourse Group. University of Rochester, Rochester, USA. 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Anderson</author>
<author>M Bader</author>
<author>E Bard</author>
<author>E Boyle G M Doherty</author>
<author>S Garrod</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>J McAllister</author>
<author>J Miller</author>
<author>C Sotillo</author>
<author>H S Thompson</author>
<author>R</author>
<author>Weinert</author>
</authors>
<title>The HCRC Map Task Corpus. Language and Speech.</title>
<date>1991</date>
<pages>34--351</pages>
<contexts>
<context position="6406" citStr="Anderson et al., 1991" startWordPosition="965" endWordPosition="968">speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et &apos;An utterance is the smallest unit to deliver a participant’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1</context>
</contexts>
<marker>Anderson, Bader, Bard, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, R, Weinert, 1991</marker>
<rawString>A. Anderson, M. Bader, E. Bard, E. Boyle G.M. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo, H.S. Thompson, R. and Weinert. The HCRC Map Task Corpus. Language and Speech. 1991, 34, pp. 351–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>Y Liu</author>
<author>E Shriberg</author>
</authors>
<title>Automatic Dialog Act Segmentation and Classification in Multiparty Meetings.</title>
<date>2005</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>1061--1064</pages>
<contexts>
<context position="3627" citStr="Ang et al., 2005" startWordPosition="531" endWordPosition="534">k on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats in the MSN Online Shopping domain (this dataset is described in Section 5). Although this work achieved reasonably high performance (up t</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>J. Ang, Y. Liu and E. Shriberg. Automatic Dialog Act Segmentation and Classification in Multiparty Meetings. IEEE International Conference on Acoustics, Speech, and Signal Processing. 2005, pp, 1061–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Di Fabbrizio</author>
<author>A Stent</author>
</authors>
<title>Learning the Structure of Task-Driven Human-Human Dialogs.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st COLING and 44th ACL.</booktitle>
<pages>201--208</pages>
<marker>Bangalore, Di Fabbrizio, Stent, 2006</marker>
<rawString>S. Bangalore, G. Di Fabbrizio and A. Stent. Learning the Structure of Task-Driven Human-Human Dialogs. Proceedings of the 21st COLING and 44th ACL. 2006, pp. 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Bui</author>
</authors>
<title>A general model for online probabilistic plan recognition. IJCAI.</title>
<date>2003</date>
<pages>1309--1318</pages>
<contexts>
<context position="7674" citStr="Bui, 2003" startWordPosition="1156" endWordPosition="1157"> dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches plus HMMs for written dialogues. Other researchers have used Bayesian based approaches, such as naive Bayes (e.g. (Grau et al., 2004; Forsyth, 2007; Ivanovic, 2008)) and Bayesian networks (e.g. (Keizer, 2001; Forsyth, 2007)). Maximum entropy (e.g. (Ivanovic, 2008)), support vector machines (e.g. (Ivanovic, 2008)), and hidden Markov models (e.g. (Bui, 2003)) have also all been applied to automatic dialogue act classification. 3 Dialogue Acts A number of dialogue act taxonomies have been proposed, designed mainly for spoken dialogue. Many of these use the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). DAMSL was originally applied to the TRAINS corpus of (transcribed) spoken task-oriented dialogues, but various adaptations of it have since been proposed for specific types of dialogue. The Switchboard corpus (Godfrey et al., 1992) defines 42 types of dialogue acts from human-to-human telephone conversations. The HCRC M</context>
</contexts>
<marker>Bui, 2003</marker>
<rawString>H. H. Bui. A general model for online probabilistic plan recognition. IJCAI. 2003, pp. 1309–1318.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Y Chung</author>
</authors>
<title>Sentence retrieval for abstracts of randomized controlled trials.</title>
<journal>BMC Medical Informatics and Decision Making.</journal>
<volume>9</volume>
<issue>10</issue>
<pages>1--13</pages>
<marker>Chung, </marker>
<rawString>G.Y Chung. Sentence retrieval for abstracts of randomized controlled trials. BMC Medical Informatics and Decision Making. 2009, 9(10), pp. 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debole</author>
<author>F Sebastiani</author>
</authors>
<title>Supervised term weighting for automated text categorization.</title>
<date>2003</date>
<booktitle>18th ACM Symposium on Applied Computing.</booktitle>
<pages>784--788</pages>
<contexts>
<context position="4452" citStr="Debole and Sebastiani, 2003" startWordPosition="659" endWordPosition="662">r 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats in the MSN Online Shopping domain (this dataset is described in Section 5). Although this work achieved reasonably high performance (up to a micro-averaged F-score of around 80%), we believe that there is still room for improvement using BoW only. We extend this work by using ideas from related research such as text categorization (Debole and Sebastiani, 2003), and explore variants of BoW based on analysis of live chats, along with feature weighting. Finally, our main aim is to explore new features based on dialogue structure and dependencies between utterances&apos; that can enhance the use of BoW for dialogue act classification. Our hypothesis is that, for task-oriented 1-on-1 live chats, the structure and interactions among utterances are useful in predicting future dialogue acts: for example, conversations typically start with a greeting, and questions and answers typically appear as adjacency pairs in a conversation. Therefore, we propose new featu</context>
</contexts>
<marker>Debole, Sebastiani, 2003</marker>
<rawString>F. Debole and F. Sebastiani. Supervised term weighting for automated text categorization. 18th ACM Symposium on Applied Computing. 2003, pp. 784–788.</rawString>
</citation>
<citation valid="false">
<authors>
<author>columns misclassification sification</author>
</authors>
<journal>CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWNPLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK = Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN</journal>
<marker>sification, </marker>
<rawString>sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWNPLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK = Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)</rawString>
</citation>
<citation valid="true">
<authors>
<author>E N Forsyth</author>
</authors>
<title>Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master’s thesis. Naval Postgraduate School,</title>
<date>2007</date>
<contexts>
<context position="3591" citStr="Forsyth, 2007" startWordPosition="526" endWordPosition="527">1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats in the MSN Online Shopping domain (this dataset is described in Section 5). Although this work achiev</context>
<context position="6718" citStr="Forsyth (2007)" startWordPosition="1019" endWordPosition="1020">’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches pl</context>
<context position="8923" citStr="Forsyth (2007)" startWordPosition="1357" endWordPosition="1358">, 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online model task-based spoken conversations. Shopping live chats, where a user requests assisFor casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the define 15 dialogue act tags based on previously- commercial agent asks the customer questions and defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved solid Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke performance over this data (around 80% F-score). et al., 2000). Forsyth (2007) defines 15 dialogue acts While 1-grams performed well (as live chat utterfor casual online conversations, based on 16 conver- ances are generally shorter than, e.g., sentences in sations with 10,567 utterances. Ivanovic (2008) pro- news articles), we expect 2- and 3-grams are needed poses 12 dialogue acts based on DAMSL for 1-on-1 to detect formulaic expressions, such as No problem online customer service chats. and You are welcome. We would also expect a posIvanovic’s set of dialogue acts for chat dia- itive effect from combining n-grams due to increaslogues has significant overlap with the </context>
<context position="11126" citStr="Forsyth, 2007" startWordPosition="1716" endWordPosition="1717">ation experiments using simple BoW features, and then introduce two groups of new features based on structural information and dependencies between utterances. 4.1 Bag-of-Words n-gram-based BoW features are simple yet effective for identifying similarities between two utterances, and have been used widely in previous work on dialogue act classification for online chat dialogues (Louwerse and Crossley, 2006; Ivanovic, 2008). However, chats containing large amounts of noise such as typos and emoticons pose a greater challenge for simple BoW approaches. On the other hand, keyword-based features (Forsyth, 2007) have achieved high performance; however, keywordbased approaches are more domain-dependent. In this work, we chose to start with a BoW approach based on our observation that commercial live chat services contain relatively less noise; in particular, the commercial agent tends to use well-formed, formulaic prose. Previously, Ivanovic (2008) explored Boolean 1- Our motivation for using structural information as a feature is that the location of an utterance can be a strong predictor of the dialogue act. That is, dialogues are sequenced, comprising turns (i.e. a given user is sending text), each</context>
</contexts>
<marker>Forsyth, 2007</marker>
<rawString>E. N. Forsyth. Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master’s thesis. Naval Postgraduate School, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>517--520</pages>
<contexts>
<context position="8184" citStr="Godfrey et al., 1992" startWordPosition="1237" endWordPosition="1240"> (Ivanovic, 2008)), support vector machines (e.g. (Ivanovic, 2008)), and hidden Markov models (e.g. (Bui, 2003)) have also all been applied to automatic dialogue act classification. 3 Dialogue Acts A number of dialogue act taxonomies have been proposed, designed mainly for spoken dialogue. Many of these use the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). DAMSL was originally applied to the TRAINS corpus of (transcribed) spoken task-oriented dialogues, but various adaptations of it have since been proposed for specific types of dialogue. The Switchboard corpus (Godfrey et al., 1992) defines 42 types of dialogue acts from human-to-human telephone conversations. The HCRC Map Task corpus (Ander863 son et al., 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online model task-based spoken conversations. Shopping live chats, where a user requests assisFor casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the define 15 dialogue act tags based on previously- commercial agent asks the customer questions and defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey and E. Holliman and J. McDaniel. SWITCHBOARD: Telephone speech corpus for research and development. Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing. 1992, pp. 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Grau</author>
<author>E Sanchis</author>
<author>M Jose</author>
<author>D Vilar</author>
</authors>
<title>Dialogue act classification using a Bayesian approach.</title>
<date>2004</date>
<booktitle>Proceedings of the 9th Conference on Speech and Computer.</booktitle>
<contexts>
<context position="7448" citStr="Grau et al., 2004" startWordPosition="1124" endWordPosition="1127">es for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches plus HMMs for written dialogues. Other researchers have used Bayesian based approaches, such as naive Bayes (e.g. (Grau et al., 2004; Forsyth, 2007; Ivanovic, 2008)) and Bayesian networks (e.g. (Keizer, 2001; Forsyth, 2007)). Maximum entropy (e.g. (Ivanovic, 2008)), support vector machines (e.g. (Ivanovic, 2008)), and hidden Markov models (e.g. (Bui, 2003)) have also all been applied to automatic dialogue act classification. 3 Dialogue Acts A number of dialogue act taxonomies have been proposed, designed mainly for spoken dialogue. Many of these use the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). DAMSL was originally applied to the TRAINS corpus of (transcribed) spoken task-oriented dialogu</context>
</contexts>
<marker>Grau, Sanchis, Jose, Vilar, 2004</marker>
<rawString>S. Grau, E. Sanchis, M. Jose and D. Vilar. Dialogue act classification using a Bayesian approach. Proceedings of the 9th Conference on Speech and Computer. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J Allen</author>
</authors>
<title>The Trains 93 Dialogues.</title>
<date>1995</date>
<tech>Trains Technical Note 94-2.</tech>
<institution>Computer Science Dept., University of Rochester,</institution>
<marker>Heeman, Allen, 1995</marker>
<rawString>P. A. Heeman and J. Allen. The Trains 93 Dialogues. Trains Technical Note 94-2. Computer Science Dept., University of Rochester, March 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>Proceedings of European Conference on Machine Learning.</booktitle>
<pages>137--142</pages>
<contexts>
<context position="20303" citStr="Joachims, 1998" startWordPosition="3240" endWordPosition="3241">ncluding 542 manuallysegmented utterances. The maximum and minimum number of utterances in a dialogue are 84 and 42, respectively; the maximum number of utterances in a turn is 14. The live chats were manually tagged with the 12 dialogue acts described in Section 3. The utterance distribution over the dialogue acts is described in Table 3. For our experiments, we calculated TF, TF·IDF and IG (Information Gain) over the utterances, which were optionally lemmatized with the morph tool (Minnen et al., 2000). We then built a dialogue act classifier using three different machine learners: SVM-HMM (Joachims, 1998),2 naive Bayes 2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html 866 Utterance number 15 12 15 5 12 17 n-gram Boolean TF TF·IDF IG 1 .731 .511 .517 .766 2 .603 .530 .601 .614 3 .474 .463 .472 .482 1+2 .756 .511 .522 .777 1+2+3 .773 .511 .528 .777 Dialogue Act CONVENTIONAL CLOSING CONVENTIONAL OPENING DOWNPLAYER EXPRESSIVE NO ANSWER OPEN QUESTION REQUEST 28 27 Table 5: Accuracy of different feature representations and 198 weighting methods for SVM-HMM 79 RESPONSE ACK STATEMENT THANKS YES ANSWER Index Learner Ours Ivanovic Feature Acc. Feature Acc. Word SVM 1+2+3/B .790 1/B .751 NB 1/B</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. Text categorization with support vector machines: Learning with many relevant features. Proceedings of European Conference on Machine Learning. 1998, pp. 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An Architecture for Multimodal Dialogue Systems.</title>
<date>2002</date>
<booktitle>Proceedings of 40th ACL.</booktitle>
<pages>376--383</pages>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker and P. Maloor. MATCH: An Architecture for Multimodal Dialogue Systems. Proceedings of 40th ACL. 2002, pp. 376– 383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F N Julia</author>
<author>K M Iftekharuddin</author>
</authors>
<title>Dialog Act classification using acoustic and discourse information of MapTask Data.</title>
<date>2008</date>
<booktitle>Proceedings of the International Joint Conference on Neural Networks.</booktitle>
<pages>1472--1479</pages>
<contexts>
<context position="3185" citStr="Julia and Iftekharuddin, 2008" startWordPosition="466" endWordPosition="469">complicated by the semi-asynchronous nature of the interaction (e.g. Werry (1996)). In this paper, we investigate the task of automatic classification of dialogue acts in 1-on-1 live chats, focusing on “information delivery” chats since these are proving increasingly popular as part of enterprise customer-service solutions. Our main challenge is to develop effective features and classifiers for classifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–87</context>
<context position="6029" citStr="Julia and Iftekharuddin (2008)" startWordPosition="904" endWordPosition="907">ification task; and (3) experimenting with different machine learning techniques. We focus here on (2) and (3); we return to (1) in Section 3. For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et &apos;An utterance is the smallest unit to deliver a participant’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less eff</context>
</contexts>
<marker>Julia, Iftekharuddin, 2008</marker>
<rawString>F. N. Julia and K. M. Iftekharuddin. Dialog Act classification using acoustic and discourse information of MapTask Data. Proceedings of the International Joint Conference on Neural Networks. 2008, pp. 1472– 1479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lexical</author>
</authors>
<title>Prosodic, and Syntactic Cues for Dialog Acts.</title>
<date>1998</date>
<booktitle>Proceedings of ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers.</booktitle>
<pages>114--120</pages>
<marker>Lexical, 1998</marker>
<rawString>D. Jurafsky, E. Shriberg, B Fox and T. Curl. Lexical, Prosodic, and Syntactic Cues for Dialog Acts. Proceedings of ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers. 1998, pp. 114–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ivanovic</author>
</authors>
<title>Automatic instant messaging dialogue using statistical models and dialogue acts. Master’s Thesis. The University of Melbourne.</title>
<date>2008</date>
<contexts>
<context position="3509" citStr="Ivanovic (2008)" startWordPosition="515" endWordPosition="516">llenge is to develop effective features and classifiers for classifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats in the MSN Online S</context>
<context position="6799" citStr="Ivanovic (2008)" startWordPosition="1029" endWordPosition="1030"> prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches plus HMMs for written dialogues. Other researchers have used Bayesian based approac</context>
<context position="8775" citStr="Ivanovic (2008)" startWordPosition="1334" endWordPosition="1335">s (Godfrey et al., 1992) defines 42 types of dialogue acts from human-to-human telephone conversations. The HCRC Map Task corpus (Ander863 son et al., 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online model task-based spoken conversations. Shopping live chats, where a user requests assisFor casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the define 15 dialogue act tags based on previously- commercial agent asks the customer questions and defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved solid Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke performance over this data (around 80% F-score). et al., 2000). Forsyth (2007) defines 15 dialogue acts While 1-grams performed well (as live chat utterfor casual online conversations, based on 16 conver- ances are generally shorter than, e.g., sentences in sations with 10,567 utterances. Ivanovic (2008) pro- news articles), we expect 2- and 3-grams are needed poses 12 dialogue acts based on DAMSL for 1-on-1 to detect formulaic expressions, such as No problem online customer service chats. and You are welcome. We would also </context>
<context position="10938" citStr="Ivanovic, 2008" startWordPosition="1688" endWordPosition="1689"> simple Boolean, we also experiment with TF, TF·IDF and Information Gain (IG). 4 Feature Selection 4.2 Structural Information In this section, we describe our initial dialogue-act classification experiments using simple BoW features, and then introduce two groups of new features based on structural information and dependencies between utterances. 4.1 Bag-of-Words n-gram-based BoW features are simple yet effective for identifying similarities between two utterances, and have been used widely in previous work on dialogue act classification for online chat dialogues (Louwerse and Crossley, 2006; Ivanovic, 2008). However, chats containing large amounts of noise such as typos and emoticons pose a greater challenge for simple BoW approaches. On the other hand, keyword-based features (Forsyth, 2007) have achieved high performance; however, keywordbased approaches are more domain-dependent. In this work, we chose to start with a BoW approach based on our observation that commercial live chat services contain relatively less noise; in particular, the commercial agent tends to use well-formed, formulaic prose. Previously, Ivanovic (2008) explored Boolean 1- Our motivation for using structural information a</context>
<context position="14386" citStr="Ivanovic (2008)" startWordPosition="2260" endWordPosition="2261"> previous utterance was received/accepted. e.g. Sure STATEMENT: Used for assertions that may state a belief or commit the speaker to doing something e.g. I am sending you the page which will pop up in a new window on your screen. THANKS: Conventional thanks e.g. Thank you for contacting us. YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g. yes, yeah YESNO QUESTION: A closed question which can be answered in the affirmative or negative. e.g. Did you receive the page, Customer? Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008) the same turn. We also noticed that identifying the utterance author can help classify the dialogue act (previously used in Ivanovic (2008)). Based on these observations, we tested the following four structural features: • Author information, • Relative position in the chat, • Author + Relative position, • Author + Turn-relative position among utterances in a given turn. We illustrate our structural features in Table 2, which shows an example of a 1-on-1 live chat. The participants are the agent (A) and customer (C); Uxx indicates an utterance (U) with ID number xx. This conversation has 42 u</context>
<context position="19564" citStr="Ivanovic (2008)" startWordPosition="3122" endWordPosition="3123">y use the dialogue act classifications, as done in Stolcke et al. (2000). The motivation is that, due to the high performance of simple BoW features, using dialogue acts directly would capture the dependency better than indirect information from utterances, despite introducing some noise. We do not build a probabilistic model of dialogue transitions the way Stolcke et al. (2000) does, but follow an approach similar to that used in Kim et al. (2010) in using predicted dialogue act(s) labels learned in previous step(s) as a feature. 5 Experiment Setup As stated earlier, we use the data set from Ivanovic (2008) for our experiments; it contains 1-on-1 live chats from an information delivery task. This dataset contains 8 live chats, including 542 manuallysegmented utterances. The maximum and minimum number of utterances in a dialogue are 84 and 42, respectively; the maximum number of utterances in a turn is 14. The live chats were manually tagged with the 12 dialogue acts described in Section 3. The utterance distribution over the dialogue acts is described in Table 3. For our experiments, we calculated TF, TF·IDF and IG (Information Gain) over the utterances, which were optionally lemmatized with the</context>
<context position="22263" citStr="Ivanovic (2008)" startWordPosition="3557" endWordPosition="3558"> we ran 8-fold cross-validation using the feature sets described above (partitioning across the 8 sessions). All results are presented in terms of classification accuracy. The accuracy of a zero-R (i.e. majority vote) baseline is 0.36. 6 Evaluation 6.1 Testing Bag-of-Words Features Table 4 shows the best accuracy achieved by the different learners, in combination with BoW represen3http://crfpp.sourceforge.net/ tations and feature weighting methods. Note that the CRF learner ran using 1-grams only, as CRF++ does not accept large numbers of features. As a benchmark, we also tested the method in Ivanovic (2008) and present the best performance over words (rather than lemmas). Overall, we found using just 1-grams produced the best performance for all learners, although SVM achieved the best performance when using all three n-gram orders (i.e. 1+2+3). Since the utterances are very short, 2-grams or 3-grams alone are too sparse to be effective. Among the feature weighting methods, Boolean and IG achieved higher accuracy than TF and TF·IDF. Likewise, due to the short utterances, simple Boolean values were often the most effective. However, as IG was computed using the training data, it also achieved hig</context>
</contexts>
<marker>Ivanovic, 2008</marker>
<rawString>E. Ivanovic. Automatic instant messaging dialogue using statistical models and dialogue acts. Master’s Thesis. The University of Melbourne. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keizer</author>
</authors>
<title>A Bayesian Approach to Dialogue Act Classification.</title>
<date>2001</date>
<booktitle>5th Workshop on Formal Semantics and Pragmatics of Dialogue.</booktitle>
<pages>210--218</pages>
<contexts>
<context position="7523" citStr="Keizer, 2001" startWordPosition="1136" endWordPosition="1137">arning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches plus HMMs for written dialogues. Other researchers have used Bayesian based approaches, such as naive Bayes (e.g. (Grau et al., 2004; Forsyth, 2007; Ivanovic, 2008)) and Bayesian networks (e.g. (Keizer, 2001; Forsyth, 2007)). Maximum entropy (e.g. (Ivanovic, 2008)), support vector machines (e.g. (Ivanovic, 2008)), and hidden Markov models (e.g. (Bui, 2003)) have also all been applied to automatic dialogue act classification. 3 Dialogue Acts A number of dialogue act taxonomies have been proposed, designed mainly for spoken dialogue. Many of these use the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). DAMSL was originally applied to the TRAINS corpus of (transcribed) spoken task-oriented dialogues, but various adaptations of it have since been proposed for specific typ</context>
</contexts>
<marker>Keizer, 2001</marker>
<rawString>S. Keizer. A Bayesian Approach to Dialogue Act Classification. 5th Workshop on Formal Semantics and Pragmatics of Dialogue. 2001, pp. 210–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Kim</author>
<author>L Wang</author>
<author>T Baldwin</author>
</authors>
<title>Tagging and Linking Web Forum Posts.</title>
<date>2010</date>
<booktitle>Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="12052" citStr="Kim et al., 2010" startWordPosition="1864" endWordPosition="1867">rose. Previously, Ivanovic (2008) explored Boolean 1- Our motivation for using structural information as a feature is that the location of an utterance can be a strong predictor of the dialogue act. That is, dialogues are sequenced, comprising turns (i.e. a given user is sending text), each of which is made up of one or more messages (i.e. strings sent by the user). Structured classification methods which make use of this sequential information have been applied to related tasks such as tagging semantic labels of key sentences in biomedical domains (Chung, 2009) and post labels in web forums (Kim et al., 2010). Based on the nature of live chats, we observed that the utterance position in the chat, as well as in a turn, plays an important role when identifying its dialogue act. For example, an utterance such as Hello will occur at the beginning of a chat while an utterance such as Have a nice day will typically appear at the end. The position of utterances in a turn can also help identify the dialogue act; i.e. when there are several utterances in a turn, utterances are related to each other, and thus examining the previous utterances in the same turn can help correctly predict the target utterance.</context>
<context position="15494" citStr="Kim et al. (2010)" startWordPosition="2441" endWordPosition="2444"> the agent (A) and customer (C); Uxx indicates an utterance (U) with ID number xx. This conversation has 42 utterances in total. The relative position is calculated by dividing the utterance number by the total number of utterances in the dialogue; the turn-relative position is calculated by dividing the utterance position by the number of utterances in that turn. For example, for utterance 4 (U4), the relative position is 442, while its turn-relative position is 23 since U4 is the second utterance among U3,4,5 that the customer makes in a single turn. 4.3 Utterance Dependency In recent work, Kim et al. (2010) demonstrated the importance of dependencies between post labels in web forums. The authors introduced series of features based on structural dependencies among posts. They used relative position, author information and automatically predicted labels from previous post(s) as dependency features for assigning a semantic label to the current target post. Similarly, by examining our chat corpus, we observed significant dependencies between utterances. First, 1-on-1 (i.e. agent-to-user) dialogues often contain dependencies between adjacent utterances by different authors. For example, in Table 2, </context>
<context position="19401" citStr="Kim et al. (2010)" startWordPosition="3093" endWordPosition="3096">006) previously used n-gram BoW features from the previous 1–3 utterances. In contrast, instead of using utterances which indirectly encode dialogue acts, we directly use the dialogue act classifications, as done in Stolcke et al. (2000). The motivation is that, due to the high performance of simple BoW features, using dialogue acts directly would capture the dependency better than indirect information from utterances, despite introducing some noise. We do not build a probabilistic model of dialogue transitions the way Stolcke et al. (2000) does, but follow an approach similar to that used in Kim et al. (2010) in using predicted dialogue act(s) labels learned in previous step(s) as a feature. 5 Experiment Setup As stated earlier, we use the data set from Ivanovic (2008) for our experiments; it contains 1-on-1 live chats from an information delivery task. This dataset contains 8 live chats, including 542 manuallysegmented utterances. The maximum and minimum number of utterances in a dialogue are 84 and 42, respectively; the maximum number of utterances in a turn is 14. The live chats were manually tagged with the 12 dialogue acts described in Section 3. The utterance distribution over the dialogue a</context>
</contexts>
<marker>Kim, Wang, Baldwin, 2010</marker>
<rawString>S.N. Kim and L. Wang and T. Baldwin. Tagging and Linking Web Forum Posts. Fourteenth Conference on Computational Natural Language Learning. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>Proceedings of ICML.</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proceedings of ICML. 2001, pp. 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>S Silliman</author>
</authors>
<title>ITSPOKE: An Intelligent Tutoring Spoken Dialogue SYstem.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT/NAACL.</booktitle>
<contexts>
<context position="2224" citStr="Litman and Silliman, 2004" startWordPosition="319" endWordPosition="323">easily distinguishable from natural speech). Potentially huge savings can be made by organisations providing customer help services if we can increase the degree of automation of live chat. Given the increasing impact of live chat services, there is surprisingly little published computational linguistic research on the topic. There has been substantially more work done on dialogue and dialogue corpora, mostly in spoken dialogue (e.g. Stolcke et al. (2000)) but also multimodal dialogue systems in application areas such as telephone support service (Bangalore et al., 2006) and tutoring systems (Litman and Silliman, 2004). Spoken dialogue analysis introduces many complications related to the error inherent in current speech recognition technologies. As an instance of written dialogue, an advantage of live chats is that recognition errors are not such an issue, although the nature of language used in chat is typically ill-formed and turn-taking is complicated by the semi-asynchronous nature of the interaction (e.g. Werry (1996)). In this paper, we investigate the task of automatic classification of dialogue acts in 1-on-1 live chats, focusing on “information delivery” chats since these are proving increasingly </context>
</contexts>
<marker>Litman, Silliman, 2004</marker>
<rawString>D. J. Litman and S. Silliman. ITSPOKE: An Intelligent Tutoring Spoken Dialogue SYstem. Proceedings of the HLT/NAACL. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Louwerse</author>
<author>S Crossley</author>
</authors>
<title>Dialog Act Classification Using N-Gram Algorithms.</title>
<date>2006</date>
<booktitle>FLAIRS Conference,</booktitle>
<pages>758--763</pages>
<contexts>
<context position="3489" citStr="Louwerse and Crossley (2006)" startWordPosition="510" endWordPosition="513">r-service solutions. Our main challenge is to develop effective features and classifiers for classifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats</context>
<context position="6232" citStr="Louwerse and Crossley (2006)" startWordPosition="936" endWordPosition="940">ures such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et &apos;An utterance is the smallest unit to deliver a participant’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram feature</context>
<context position="10921" citStr="Louwerse and Crossley, 2006" startWordPosition="1684" endWordPosition="1687">ialong with examples. tion to simple Boolean, we also experiment with TF, TF·IDF and Information Gain (IG). 4 Feature Selection 4.2 Structural Information In this section, we describe our initial dialogue-act classification experiments using simple BoW features, and then introduce two groups of new features based on structural information and dependencies between utterances. 4.1 Bag-of-Words n-gram-based BoW features are simple yet effective for identifying similarities between two utterances, and have been used widely in previous work on dialogue act classification for online chat dialogues (Louwerse and Crossley, 2006; Ivanovic, 2008). However, chats containing large amounts of noise such as typos and emoticons pose a greater challenge for simple BoW approaches. On the other hand, keyword-based features (Forsyth, 2007) have achieved high performance; however, keywordbased approaches are more domain-dependent. In this work, we chose to start with a BoW approach based on our observation that commercial live chat services contain relatively less noise; in particular, the commercial agent tends to use well-formed, formulaic prose. Previously, Ivanovic (2008) explored Boolean 1- Our motivation for using structu</context>
</contexts>
<marker>Louwerse, Crossley, 2006</marker>
<rawString>M. M. Louwerse and S. Crossley. Dialog Act Classification Using N-Gram Algorithms. FLAIRS Conference, 2006, pp. 758–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing</title>
<date>2000</date>
<journal>of English Natural Language Engineering</journal>
<volume>7</volume>
<issue>3</issue>
<pages>77--80</pages>
<contexts>
<context position="20197" citStr="Minnen et al., 2000" startWordPosition="3222" endWordPosition="3225">riments; it contains 1-on-1 live chats from an information delivery task. This dataset contains 8 live chats, including 542 manuallysegmented utterances. The maximum and minimum number of utterances in a dialogue are 84 and 42, respectively; the maximum number of utterances in a turn is 14. The live chats were manually tagged with the 12 dialogue acts described in Section 3. The utterance distribution over the dialogue acts is described in Table 3. For our experiments, we calculated TF, TF·IDF and IG (Information Gain) over the utterances, which were optionally lemmatized with the morph tool (Minnen et al., 2000). We then built a dialogue act classifier using three different machine learners: SVM-HMM (Joachims, 1998),2 naive Bayes 2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html 866 Utterance number 15 12 15 5 12 17 n-gram Boolean TF TF·IDF IG 1 .731 .511 .517 .766 2 .603 .530 .601 .614 3 .474 .463 .472 .482 1+2 .756 .511 .522 .777 1+2+3 .773 .511 .528 .777 Dialogue Act CONVENTIONAL CLOSING CONVENTIONAL OPENING DOWNPLAYER EXPRESSIVE NO ANSWER OPEN QUESTION REQUEST 28 27 Table 5: Accuracy of different feature representations and 198 weighting methods for SVM-HMM 79 RESPONSE ACK STATEMENT THA</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>G. Minnen, J. Carroll and D. Pearce. Applied morphological processing of English Natural Language Engineering 2000, 7(3), pp. 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>J Niekrasz</author>
<author>S Peters</author>
</authors>
<title>Ontology-Based Discourse Understanding for a Persistent Meeting Assistant.</title>
<date>2005</date>
<booktitle>Proc. CHI 2005 Workshop on The Virtuality Continuum Revisited.</booktitle>
<contexts>
<context position="3575" citStr="Purver et al., 2005" startWordPosition="522" endWordPosition="525">assifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics In this paper, we first re-examine BoW features for dialogue act classification. As a baseline, we use the work of Ivanovic (2008), which explored 1- grams and 2-grams with Boolean values in 1-on-1 live chats in the MSN Online Shopping domain (this dataset is described in Section 5). Although </context>
</contexts>
<marker>Purver, Niekrasz, Peters, 2005</marker>
<rawString>M. Purver, J. Niekrasz and S. Peters. Ontology-Based Discourse Understanding for a Persistent Meeting Assistant. Proc. CHI 2005 Workshop on The Virtuality Continuum Revisited. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Samuel</author>
<author>Sandra Carberry</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Dialogue Act Tagging with Transformation-Based Learning.</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL</booktitle>
<pages>1150--1156</pages>
<contexts>
<context position="5721" citStr="Samuel et al. (1998)" startWordPosition="860" endWordPosition="863">n derived from utterances (Sections 4.2 and 4.3). 2 Related Work While there has been significant work on classifying dialogue acts, the bulk of this has been for spoken dialogue. Most such work has considered: (1) defining taxonomies of dialogue acts; (2) discovering useful features for the classification task; and (3) experimenting with different machine learning techniques. We focus here on (2) and (3); we return to (1) in Section 3. For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et &apos;An utterance is the smallest unit to deliver a participant’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken a</context>
<context position="7010" citStr="Samuel et al. (1998)" startWordPosition="1058" endWordPosition="1061">on et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-based approaches plus HMMs for written dialogues. Other researchers have used Bayesian based approaches, such as naive Bayes (e.g. (Grau et al., 2004; Forsyth, 2007; Ivanovic, 2008)) and Bayesian networks (e.g. (Keizer, 2001; Forsyth, 2007)). Maximum entropy (e.g. (Ivanovic, 2008)), support vector machines (e.</context>
<context position="8739" citStr="Samuel et al., 1998" startWordPosition="1328" endWordPosition="1331">types of dialogue. The Switchboard corpus (Godfrey et al., 1992) defines 42 types of dialogue acts from human-to-human telephone conversations. The HCRC Map Task corpus (Ander863 son et al., 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online model task-based spoken conversations. Shopping live chats, where a user requests assisFor casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the define 15 dialogue act tags based on previously- commercial agent asks the customer questions and defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved solid Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke performance over this data (around 80% F-score). et al., 2000). Forsyth (2007) defines 15 dialogue acts While 1-grams performed well (as live chat utterfor casual online conversations, based on 16 conver- ances are generally shorter than, e.g., sentences in sations with 10,567 utterances. Ivanovic (2008) pro- news articles), we expect 2- and 3-grams are needed poses 12 dialogue acts based on DAMSL for 1-on-1 to detect formulaic expressions, such as No problem online customer service chats.</context>
<context position="21457" citStr="Samuel et al., 1998" startWordPosition="3433" endWordPosition="3436">c Feature Acc. Feature Acc. Word SVM 1+2+3/B .790 1/B .751 NB 1/B .673 1/B .673 CRF 1/IG .839 1/B .825 Lemma SVM 1+2+3/IG .777 N/A N/A NB 1/B .672 N/A N/A CRF 1/B .862 N/A N/A YESNO QUESTION Table 3: Dialogue act distribution in the corpus Table 4: Best accuracy achieved by the different learners over different feature sets and weighting methods (1 = 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = information gain) from the WEKA machine learning toolkit (Witten and Frank, 2005), and Conditional Random Fields (CRF) using CRF++.3 Note that we chose to test CRF and SVM-HMM as previous work (e.g. (Samuel et al., 1998; Stolcke et al., 2000; Chung, 2009)) has shown the effectiveness of structured classification models on sequential dependencies. Thus, we expect similar effects with CRF and SVMHMM. Finally, we ran 8-fold cross-validation using the feature sets described above (partitioning across the 8 sessions). All results are presented in terms of classification accuracy. The accuracy of a zero-R (i.e. majority vote) baseline is 0.36. 6 Evaluation 6.1 Testing Bag-of-Words Features Table 4 shows the best accuracy achieved by the different learners, in combination with BoW represen3http://crfpp.sourceforge.</context>
</contexts>
<marker>Samuel, Carberry, Vijay-Shanker, 1998</marker>
<rawString>K. Samuel, Sandra Carberry and K. Vijay-Shanker. Dialogue Act Tagging with Transformation-Based Learning. Proceedings of COLING/ACL 1998. 1998, pp. 1150-1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Bates</author>
<author>P Taylor</author>
<author>A Stolcke</author>
<author>D Jurafsky</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>R Martin</author>
<author>M Meteer</author>
<author>C Van Ess-Dykema</author>
</authors>
<date>1998</date>
<booktitle>Can Prosody Aid the Automatic Classification of Dialog Acts in Conversational Speech?. Language and Speech.</booktitle>
<pages>41--3</pages>
<marker>Shriberg, Bates, Taylor, Stolcke, Jurafsky, Ries, Coccaro, Martin, Meteer, Van Ess-Dykema, 1998</marker>
<rawString>E. Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky, K. Ries, N. Coccaro, R. Martin, M. Meteer and C. Van Ess-Dykema. Can Prosody Aid the Automatic Classification of Dialog Acts in Conversational Speech?. Language and Speech. 1998, 41(3-4), pp. 439–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Sridhar</author>
<author>S Bangalore</author>
<author>S Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language.</title>
<date>2009</date>
<volume>23</volume>
<issue>4</issue>
<pages>407--422</pages>
<contexts>
<context position="3208" citStr="Sridhar et al., 2009" startWordPosition="470" endWordPosition="473">onous nature of the interaction (e.g. Werry (1996)). In this paper, we investigate the task of automatic classification of dialogue acts in 1-on-1 live chats, focusing on “information delivery” chats since these are proving increasingly popular as part of enterprise customer-service solutions. Our main challenge is to develop effective features and classifiers for classifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for written dialogues. Previous dialogue-act detection for chat systems has used bags-of-words (hereafter, BoW) as features for dialogue-act detection; this simple approach has shown some promise (e.g. Bangalore et al. (2006), Louwerse and Crossley (2006) and Ivanovic (2008)). Other features such as keywords/ontologies (Purver et al., 2005; Forsyth, 2007) and lexical cues (Ang et al., 2005) have also been used for dialogue act classification. 862 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871, MIT, Massachusetts, </context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2009</marker>
<rawString>V. R. Sridhar, S. Bangalore and S. Narayanan. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language. 2009, 23(4), pp. 407–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics.</title>
<date>2000</date>
<volume>26</volume>
<issue>3</issue>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema and M. Meteer. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics. 2000, 26(3), pp. 339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Markovian Combination of Language and Prosodic Models for better Speech Understanding and Recognition.</title>
<date>2001</date>
<booktitle>Invited talk at the IEEE Workshop on Speech Recognition and Understanding, Madonna di Campiglio,</booktitle>
<location>Italy,</location>
<marker>Stolcke, Shriberg, 2001</marker>
<rawString>A. Stolcke and E. Shriberg. Markovian Combination of Language and Prosodic Models for better Speech Understanding and Recognition. Invited talk at the IEEE Workshop on Speech Recognition and Understanding, Madonna di Campiglio, Italy, December 2001 2001,</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Werry</author>
</authors>
<title>Linguistic and interactional features of Internet Relay Chat.</title>
<date>1996</date>
<booktitle>ComputerMediated Communication. Benjamins,</booktitle>
<editor>In S. C. Herring (ed.).</editor>
<contexts>
<context position="2637" citStr="Werry (1996)" startWordPosition="384" endWordPosition="385">ogue (e.g. Stolcke et al. (2000)) but also multimodal dialogue systems in application areas such as telephone support service (Bangalore et al., 2006) and tutoring systems (Litman and Silliman, 2004). Spoken dialogue analysis introduces many complications related to the error inherent in current speech recognition technologies. As an instance of written dialogue, an advantage of live chats is that recognition errors are not such an issue, although the nature of language used in chat is typically ill-formed and turn-taking is complicated by the semi-asynchronous nature of the interaction (e.g. Werry (1996)). In this paper, we investigate the task of automatic classification of dialogue acts in 1-on-1 live chats, focusing on “information delivery” chats since these are proving increasingly popular as part of enterprise customer-service solutions. Our main challenge is to develop effective features and classifiers for classifying aspects of 1-on-1 live chat. Much of the work on analysing dialogue acts in spoken dialogues has relied on non-lexical features, such as prosody and acoustic features (Stolcke et al., 2000; Julia and Iftekharuddin, 2008; Sridhar et al., 2009), which are not available for</context>
</contexts>
<marker>Werry, 1996</marker>
<rawString>C. C. Werry. Linguistic and interactional features of Internet Relay Chat. In S. C. Herring (ed.). ComputerMediated Communication. Benjamins, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="21319" citStr="Witten and Frank, 2005" startWordPosition="3408" endWordPosition="3412">different feature representations and 198 weighting methods for SVM-HMM 79 RESPONSE ACK STATEMENT THANKS YES ANSWER Index Learner Ours Ivanovic Feature Acc. Feature Acc. Word SVM 1+2+3/B .790 1/B .751 NB 1/B .673 1/B .673 CRF 1/IG .839 1/B .825 Lemma SVM 1+2+3/IG .777 N/A N/A NB 1/B .672 N/A N/A CRF 1/B .862 N/A N/A YESNO QUESTION Table 3: Dialogue act distribution in the corpus Table 4: Best accuracy achieved by the different learners over different feature sets and weighting methods (1 = 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = information gain) from the WEKA machine learning toolkit (Witten and Frank, 2005), and Conditional Random Fields (CRF) using CRF++.3 Note that we chose to test CRF and SVM-HMM as previous work (e.g. (Samuel et al., 1998; Stolcke et al., 2000; Chung, 2009)) has shown the effectiveness of structured classification models on sequential dependencies. Thus, we expect similar effects with CRF and SVMHMM. Finally, we ran 8-fold cross-validation using the feature sets described above (partitioning across the 8 sessions). All results are presented in terms of classification accuracy. The accuracy of a zero-R (i.e. majority vote) baseline is 0.36. 6 Evaluation 6.1 Testing Bag-of-Wor</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wu</author>
<author>F M Khan</author>
<author>T A Fisher</author>
<author>L A Shuler</author>
<author>W M Pottenger</author>
</authors>
<title>Posting act tagging using transformationbased learning.</title>
<date>2002</date>
<booktitle>Proceedings of the Workshop on Foundations of Data Mining and Discovery, IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="6699" citStr="Wu et al. (2002)" startWordPosition="1014" endWordPosition="1017">deliver a participant’s message(s) in a turn. al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-</context>
<context position="8540" citStr="Wu et al. (2002)" startWordPosition="1295" endWordPosition="1298">heme (Allen and Core, 1997). DAMSL was originally applied to the TRAINS corpus of (transcribed) spoken task-oriented dialogues, but various adaptations of it have since been proposed for specific types of dialogue. The Switchboard corpus (Godfrey et al., 1992) defines 42 types of dialogue acts from human-to-human telephone conversations. The HCRC Map Task corpus (Ander863 son et al., 1991) defines a set of 128 dialogue acts to gram and 2-gram features to classify MSN Online model task-based spoken conversations. Shopping live chats, where a user requests assisFor casual online chat dialogues, Wu et al. (2002) tance in purchasing an item, in response to which the define 15 dialogue act tags based on previously- commercial agent asks the customer questions and defined dialogue act sets (Samuel et al., 1998; makes suggestions. Ivanovic (2008) achieved solid Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke performance over this data (around 80% F-score). et al., 2000). Forsyth (2007) defines 15 dialogue acts While 1-grams performed well (as live chat utterfor casual online conversations, based on 16 conver- ances are generally shorter than, e.g., sentences in sations with 10,567 utterances. Ivano</context>
</contexts>
<marker>Wu, Khan, Fisher, Shuler, Pottenger, 2002</marker>
<rawString>T. Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M. Pottenger. Posting act tagging using transformationbased learning. Proceedings of the Workshop on Foundations of Data Mining and Discovery, IEEE International Conference on Data Mining. 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>