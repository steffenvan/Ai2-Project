<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008838">
<title confidence="0.970089">
Surprising parser actions and reading difficulty
</title>
<author confidence="0.991278">
Marisa Ferrara Boston, John Hale Reinhold Kliegl, Shravan Vasishth
</author>
<affiliation confidence="0.999223">
Michigan State University Potsdam University
</affiliation>
<address confidence="0.614">
USA Germany
</address>
<email confidence="0.999446">
{mferrara,jthale}@msu.edu {kliegl,vasishth}@uni-potsdam.de
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999613461538461">
An incremental dependency parser’s proba-
bility model is entered as a predictor in a
linear mixed-effects model of German read-
ers’ eye-fixation durations. This dependency-
based predictor improves a baseline that takes
into account word length, n-gram probabil-
ity, and Cloze predictability that are typically
applied in models of human reading. This
improvement obtains even when the depen-
dency parser explores a tiny fraction of its
search space, as suggested by narrow-beam
accounts of human sentence processing such
as Garden Path theory.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964555555556">
A growing body of work in cognitive science char-
acterizes human readers as some kind of probabilis-
tic parser (Jurafsky, 1996; Crocker and Brants, 2000;
Chater and Manning, 2006). This view gains sup-
port when specific aspects of these programs match
up well with measurable properties of humans en-
gaged in sentence comprehension.
One way to connect theory to data in this man-
ner uses a parser’s probability model to work out
the surprisal or log-probability of the next word.
Hale (2001) suggests this quantity as an index
of psycholinguistic difficulty. When the transi-
tion from previous word to current word is low-
probability, from the parser’s perspective, the sur-
prisal is high and the psycholinguistic claim is that
behavioral measures should register increased cog-
nitive difficulty. In other words, rare parser ac-
tions are cognitively costly. This basic notion has
</bodyText>
<page confidence="0.868521">
5
</page>
<bodyText confidence="0.964678272727273">
proved remarkably applicable across sentence types
and languages (Park and Brew, 2006; Demberg and
Keller, 2007; Levy, 2008).
The present work uses the time spent looking at
a word during reading as an empirical measure of
sentence processing difficulty. From the theoretical
side, we calculate word-by-word surprisal pre-
dictions from a family of incremental depen-
dency parsers for German based on Nivre (2004);
these parsers differ only in the size k of the beam
used in the search for analyses of longer and longer
sentence-initial substrings. We find that predictions
derived even from very narrow-beamed parsers im-
prove a baseline eye-fixation duration model. The
fact that any member of this parser family derives
a useful predictor shows that at least some syn-
tactic properties are reflected in readers’ eye fixa-
tion durations. From a cognitive perspective, the
utility of small k parsers for modeling comprehen-
sion difficulty lends credence to the view that the
human processor is a single-path analyzer (Frazier
and Fodor, 1978).
</bodyText>
<sectionHeader confidence="0.8323885" genericHeader="method">
2 Parsing costs and theories of reading
difficulty
</sectionHeader>
<bodyText confidence="0.996118666666667">
The length of time that a reader’s eyes spend fix-
ated on a particular word in a sentence is known
to be affected by a variety of word-level factors
such as length in characters, n-gram frequency and
empirical predictability (Ehrlich and Rayner, 1981;
Kliegl et al., 2004). This last factor is the one mea-
sured when human readers are asked to guess the
next word given a left-context string.
Any role for parser-derived syntactic factors
</bodyText>
<subsubsectionHeader confidence="0.312726">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5–8,
</subsubsectionHeader>
<page confidence="0.462263">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figureCaption confidence="0.996003">
Figure 1: Dependency structure of a PSC sentence.
</figureCaption>
<table confidence="0.998016571428571">
Error type Amount
Noun attachment 4.2%
Prepositional Phrase attachment 3.0%
Conjunction 1.9%
Adverb ambiguity 1.8%
Other 1.1%
Total error 12.1%
</table>
<bodyText confidence="0.999771875">
would have to go beyond these word-level influ-
ences. Our methodology imposes this requirement
by fitting a kind of regression known as a lin-
ear mixed-effects model to the total reading times
associated with each sentence-medial word in the
Potsdam Sentence Corpus (PSC) (Kliegl et al.,
2006). The PSC records the eye-movements of 272
native speakers as they read 144 German sentences.
</bodyText>
<tableCaption confidence="0.975774">
Table 2: Parser errors by category.
</tableCaption>
<sectionHeader confidence="0.996199" genericHeader="method">
3 The Parsing Model
</sectionHeader>
<bodyText confidence="0.999979833333333">
The parser’s outputs define a relation on
word pairs (Tesni`ere, 1959; Hays, 1964). The
structural description in Figure 1 is an example
output that depicts this dependency relation using
arcs. The word near the arrowhead is the dependent,
the other word its head (or governor).
These outputs are built up by monotonically
adding to an initially-empty set of dependency re-
lations as analysis proceeds from left to right. To
arrive at Figure 1 the Nivre parser passes through
a number of intermediate states that aggregate four
data structures, detailed below in Table 1.
</bodyText>
<listItem confidence="0.878741">
σ A stack of already-parsed unreduced words.
τ An ordered input list of words.
h A function from dependent words to heads.
d A function from dependent words to arc types.
</listItem>
<tableCaption confidence="0.998738">
Table 1: Parser configuration.
</tableCaption>
<bodyText confidence="0.99992808">
The stack σ holds words that could eventually be
connected by new arcs, while τ lists unparsed words.
h and d are where the current set of dependency arcs
reside. There are only four possible transitions
from configuration to configuration. Left-Arc
and Right-Arc transitions create dependency re-
lations between the top elements in σ and τ, while
Shift and Reduce transitions manipulate σ.
When more than one transition is applicable, the
parser decides between them by consulting a proba-
bility model derived from the Negra and Tiger news-
paper corpora (Skut et al., 1997; K¨onig and Lezius,
2003). This model is called Stack3 because it con-
siders only the parts-of-speech of the top three el-
ements of σ along with the top element of τ. On
the PSC this model achieves 87.9% precision and
79.5% recall for unlabeled dependencies. Most of
the attachments it gets wrong (Table 2) represent al-
ternative readings that would require semantic guid-
ance to rule out.
To compare “serial” human sentence processing
models against “parallel” models, our implemen-
tation does beam search in the space of Nivre-
configurations. The number of configurations main-
tained at any point is a changeable parameter k.
</bodyText>
<subsectionHeader confidence="0.992678">
3.1 Surprisal
</subsectionHeader>
<bodyText confidence="0.9999662">
In Figure 1 the thermometer beneath the Ger-
man preposition “in” graphically indicates a
high surprisal prediction derived from the depen-
dency parser. Greater cognitive effort, reflected in
reading time, should be observed on “in” as com-
</bodyText>
<page confidence="0.995878">
6
</page>
<bodyText confidence="0.999928">
pared to “alte.” The difficulty prediction at “in” ul-
timately follows from the frequency of verbs tak-
ing prepositional complements that follow nominal
complements in the training data. Equation 1 ex-
presses the general theory: the surprisal of a word,
on a language model, is the logarithm of the pre-
fix probability eliminated in the transition from one
word to the next.
</bodyText>
<equation confidence="0.934708333333333">
�αn−1 �
surprisal(n) = log2 (1)
αn
</equation>
<bodyText confidence="0.999782666666667">
The prefix-probability αn of an initial substring is
the total probability of all grammatical analyses that
derive w = w1...wn as a left-prefix (Equation 2).
</bodyText>
<equation confidence="0.998218">
αn = E Prob(d) (2)
dED(G,wv)
</equation>
<bodyText confidence="0.9998976">
In a complete parser, every member of D is in cor-
respondence with a state transition sequence. In the
beam-search approximation, only the top k config-
urations are retained from prefix to prefix, which
amounts to choosing a subset of D.
</bodyText>
<sectionHeader confidence="0.997921" genericHeader="evaluation">
4 Study
</sectionHeader>
<bodyText confidence="0.999700363636364">
The study addresses whether surprisal is a signif-
icant predictor of reading difficulty and, if it is,
whether the beam-size parameter k affects the use-
fulness of the calculated surprisal values in account-
ing for reading difficulty.
Using total reading time as a dependent measure,
we fit a baseline linear mixed-effects model (Equa-
tion 3) that takes into account word-level predictors
log frequency (lf), log bigram frequency (bi), word
length (len), and human predictability given the left
context (pr).
</bodyText>
<equation confidence="0.9506375">
log (TRT) = (3)
5.4 − 0.02lf − 0.01bi − 0.59len−1 − 0.02pr
</equation>
<bodyText confidence="0.999909555555556">
All of the word-level predictors were statistically
significant at the α level 0.05.
Beyond this baseline, we fitted ten other lin-
ear mixed-effects models. To the inventory of word-
level predictors, each of the ten regressions uniquely
added the surprisal predictions calculated from a
parser that retains at most k=1...9,100 analyses at
each prefix. We evaluated the change in relative
quality of fit due to surprisal with the Deviance In-
formation Criterion (DIC) discussed in Spiegelhal-
ter et al. (2002). Whereas the more commonly ap-
plied Akaike Information Criterion (1973) requires
the number of estimated parameters to be deter-
mined exactly, the DIC facilitates the evaluation of
mixed-effects models by relaxing this requirement.
When comparing two models, if one of the models
has a lower DIC value, this means that the model fit
has improved.
</bodyText>
<subsectionHeader confidence="0.821751">
4.1 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999803545454545">
Table 3 shows that the linear mixed-effects model of
German reading difficulty improves when surprisal
values from the dependency parser are used as pre-
dictors in addition to the word-level predictors. The
coefficients on the baseline predictors remained un-
changed (Equation 3) when any of the parser-based
predictors was added.
Table 3 also suggests the returns to be had in
accounting for reading time are greatest when the
beam is limited to a handful of parses. Indeed,
a parser that handles a few analyses at a time
(k=1,2,3) is just as valuable as one that spends far
greater memory resources (k=100). This observa-
tion is consistent with Brants and Crocker’s (2000)
observation that accuracy can be maintained even
when restricted to 1% of the memory required for
exhaustive parsing. The role of small k depen-
dency parsers in determining the quality of statisti-
cal fit challenges the assumption that cognitive func-
tions are global optima. Perhaps human parsing is
boundedly rational in the sense of the bound im-
posed by Stack3 (Simon, 1955).
</bodyText>
<sectionHeader confidence="0.99519" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9997141">
This study demonstrates that surprisal calculated
with a dependency parser is a significant predictor of
reading times, an empirical measure of cognitive dif-
ficulty. Surprisal is a significant predictor even when
examined alongside the more commonly used pre-
dictors, word length, predictability, and n-gram fre-
quency. The viability of parsers that consider just a
small number of analyses at each increment is con-
sistent with conceptions of the human comprehender
that incorporate that restriction.
</bodyText>
<page confidence="0.999577">
7
</page>
<table confidence="0.999709333333333">
Model Coefficient Std. Error t value DIC
Baseline - - - 144511.1
k=1 0.033691 0.002285 15 143964.9
k=2 0.038573 0.002510 15 143946.2
k=3 0.037320 0.002693 14 143990.4
k=4 0.041035 0.002853 14 143975.7
k=5 0.048692 0.002953 16 143910.9
k=6 0.046580 0.003063 15 143951.6
k=7 0.045008 0.003118 14 143974.4
k=8 0.042039 0.003165 13 144006.4
k=9 0.040657 0.003225 13 144023.9
k=100 0.029467 0.003878 8 144125.4
</table>
<tableCaption confidence="0.999232666666667">
Table 3: Coefficients and standard errors from the multiple regressions using different versions of surprisal (baseline
predictors’ coefficients are not shown for space reasons). t values &gt; 2 are statistically significant at α = 0.05. The
table also shows DIC values for the baseline model (Equation 3) and the models with baseline predictors plus surprisal.
</tableCaption>
<sectionHeader confidence="0.991435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.941949367647059">
H. Akaike. 1973. Information theory and an extension
of the maximum likelihood principle. In B. N. Petrov
and F. Caski, editors, 2nd International Symposium on
Information Theory, pages 267–281, Budapest, Hun-
gary.
T. Brants and M. Crocker. 2000. Probabilistic pars-
ing and psychological plausibility. In Proceedings of
COLING 2000: The 18th International Conference on
Computational Linguistics.
N. Chater and C. Manning. 2006. Probabilistic mod-
els of language processing and acquisition. Trends in
Cognitive Sciences, 10:287–291.
M. W. Crocker and T. Brants. 2000. Wide-coverage
probabilistic sentence processing. Journal of Psy-
cholinguistic Research, 29(6):647–669.
V. Demberg and F. Keller. 2007. Data from eye-tracking
corpora as evidence for theories of syntactic process-
ing complexity. Manuscript, University of Edinburgh.
S. F. Ehrlich and K. Rayner. 1981. Contextual effects
on word perception and eye movements during read-
ing. Journal of Verbal Learning and Verbal Behavior,
20:641–655.
L. Frazier and J. D. Fodor. 1978. The sausage machine: a
new two-stage parsing model. Cognition, 6:291–325.
J. Hale. 2001. A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of 2nd NAACL,
pages 1–8. Carnegie Mellon University.
D.G. Hays. 1964. Dependency theory: A formalism and
some observations. Language, 40:511–525.
D. Jurafsky. 1996. A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive Sci-
ence, 20:137–194.
R. Kliegl, E. Grabner, M. Rolfs, and R. Engbert. 2004.
Length, frequency, and predictability effects of words
on eye movements in reading. European Journal of
Cognitive Psychology, 16:262–284.
R. Kliegl, A. Nuthmann, and R. Engbert. 2006. Track-
ing the mind during reading: The influence of past,
present, and future words on fixation durations. Jour-
nal of Experimental Psychology: General, 135:12–35.
E. K¨onig and W. Lezius. 2003. The TIGER language -
a description language for syntax graphs, Formal def-
inition. Technical report, IMS, Universit¨at Stuttgart,
Germany.
R. Levy. 2008. Expectation-based syntactic comprehen-
sion. Cognition, 106(3):1126–1177.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together, Barcelona, Spain.
Association for Computational Linguistics.
J. Park and C. Brew. 2006. A finite-state model of hu-
man sentence processing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, pages
49–56, Sydney, Australia.
H. Simon. 1955. A behavioral model of rational choice.
The Quarterly Journal of Economics, 69(1):99–118.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Proceedings of the Fifth Conference on Applied
Natural Language Processing ANLP-97, Washington,
DC.
D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and
A. van der Linde. 2002. Bayesian measures of model
complexity and fit (with discussion). Journal of the
Royal Statistical Society, 64(B):583–639.
L. Tesni`ere. 1959. El´ements de syntaxe structurale. Edi-
tions Klincksiek, Paris.
</reference>
<page confidence="0.998494">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.887985">
<title confidence="0.968962">Surprising parser actions and reading difficulty</title>
<author confidence="0.998751">Marisa Ferrara Boston</author>
<author confidence="0.998751">John Hale Reinhold Kliegl</author>
<author confidence="0.998751">Shravan Vasishth</author>
<affiliation confidence="0.998852">Michigan State University Potsdam University</affiliation>
<address confidence="0.952047">USA Germany</address>
<abstract confidence="0.997458642857143">An incremental dependency parser’s probability model is entered as a predictor in a linear mixed-effects model of German readers’ eye-fixation durations. This dependencybased predictor improves a baseline that takes account word length, probability, and Cloze predictability that are typically applied in models of human reading. This improvement obtains even when the dependency parser explores a tiny fraction of its search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Akaike</author>
</authors>
<title>Information theory and an extension of the maximum likelihood principle.</title>
<date>1973</date>
<booktitle>2nd International Symposium on Information Theory,</booktitle>
<pages>267--281</pages>
<editor>In B. N. Petrov and F. Caski, editors,</editor>
<location>Budapest, Hungary.</location>
<marker>Akaike, 1973</marker>
<rawString>H. Akaike. 1973. Information theory and an extension of the maximum likelihood principle. In B. N. Petrov and F. Caski, editors, 2nd International Symposium on Information Theory, pages 267–281, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>M Crocker</author>
</authors>
<title>Probabilistic parsing and psychological plausibility.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000: The 18th International Conference on Computational Linguistics.</booktitle>
<marker>Brants, Crocker, 2000</marker>
<rawString>T. Brants and M. Crocker. 2000. Probabilistic parsing and psychological plausibility. In Proceedings of COLING 2000: The 18th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chater</author>
<author>C Manning</author>
</authors>
<title>Probabilistic models of language processing and acquisition.</title>
<date>2006</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<pages>10--287</pages>
<contexts>
<context position="972" citStr="Chater and Manning, 2006" startWordPosition="134" endWordPosition="137">odel of German readers’ eye-fixation durations. This dependencybased predictor improves a baseline that takes into account word length, n-gram probability, and Cloze predictability that are typically applied in models of human reading. This improvement obtains even when the dependency parser explores a tiny fraction of its search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory. 1 Introduction A growing body of work in cognitive science characterizes human readers as some kind of probabilistic parser (Jurafsky, 1996; Crocker and Brants, 2000; Chater and Manning, 2006). This view gains support when specific aspects of these programs match up well with measurable properties of humans engaged in sentence comprehension. One way to connect theory to data in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive d</context>
</contexts>
<marker>Chater, Manning, 2006</marker>
<rawString>N. Chater and C. Manning. 2006. Probabilistic models of language processing and acquisition. Trends in Cognitive Sciences, 10:287–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Crocker</author>
<author>T Brants</author>
</authors>
<title>Wide-coverage probabilistic sentence processing.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="945" citStr="Crocker and Brants, 2000" startWordPosition="130" endWordPosition="133">n a linear mixed-effects model of German readers’ eye-fixation durations. This dependencybased predictor improves a baseline that takes into account word length, n-gram probability, and Cloze predictability that are typically applied in models of human reading. This improvement obtains even when the dependency parser explores a tiny fraction of its search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory. 1 Introduction A growing body of work in cognitive science characterizes human readers as some kind of probabilistic parser (Jurafsky, 1996; Crocker and Brants, 2000; Chater and Manning, 2006). This view gains support when specific aspects of these programs match up well with measurable properties of humans engaged in sentence comprehension. One way to connect theory to data in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should reg</context>
</contexts>
<marker>Crocker, Brants, 2000</marker>
<rawString>M. W. Crocker and T. Brants. 2000. Wide-coverage probabilistic sentence processing. Journal of Psycholinguistic Research, 29(6):647–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2007</date>
<tech>Manuscript,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1778" citStr="Demberg and Keller, 2007" startWordPosition="263" endWordPosition="266">ata in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty. In other words, rare parser actions are cognitively costly. This basic notion has 5 proved remarkably applicable across sentence types and languages (Park and Brew, 2006; Demberg and Keller, 2007; Levy, 2008). The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004); these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any member of this parser family </context>
</contexts>
<marker>Demberg, Keller, 2007</marker>
<rawString>V. Demberg and F. Keller. 2007. Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Manuscript, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Ehrlich</author>
<author>K Rayner</author>
</authors>
<title>Contextual effects on word perception and eye movements during reading.</title>
<date>1981</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>20--641</pages>
<contexts>
<context position="3005" citStr="Ehrlich and Rayner, 1981" startWordPosition="462" endWordPosition="465">ives a useful predictor shows that at least some syntactic properties are reflected in readers’ eye fixation durations. From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer (Frazier and Fodor, 1978). 2 Parsing costs and theories of reading difficulty The length of time that a reader’s eyes spend fixated on a particular word in a sentence is known to be affected by a variety of word-level factors such as length in characters, n-gram frequency and empirical predictability (Ehrlich and Rayner, 1981; Kliegl et al., 2004). This last factor is the one measured when human readers are asked to guess the next word given a left-context string. Any role for parser-derived syntactic factors Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5–8, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Dependency structure of a PSC sentence. Error type Amount Noun attachment 4.2% Prepositional Phrase attachment 3.0% Conjunction 1.9% Adverb ambiguity 1.8% Other 1.1% Total error 12.1% would have to go beyond these word-level influences. Our methodol</context>
</contexts>
<marker>Ehrlich, Rayner, 1981</marker>
<rawString>S. F. Ehrlich and K. Rayner. 1981. Contextual effects on word perception and eye movements during reading. Journal of Verbal Learning and Verbal Behavior, 20:641–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>J D Fodor</author>
</authors>
<title>The sausage machine: a new two-stage parsing model.</title>
<date>1978</date>
<journal>Cognition,</journal>
<pages>6--291</pages>
<contexts>
<context position="2703" citStr="Frazier and Fodor, 1978" startWordPosition="411" endWordPosition="414">these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any member of this parser family derives a useful predictor shows that at least some syntactic properties are reflected in readers’ eye fixation durations. From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer (Frazier and Fodor, 1978). 2 Parsing costs and theories of reading difficulty The length of time that a reader’s eyes spend fixated on a particular word in a sentence is known to be affected by a variety of word-level factors such as length in characters, n-gram frequency and empirical predictability (Ehrlich and Rayner, 1981; Kliegl et al., 2004). This last factor is the one measured when human readers are asked to guess the next word given a left-context string. Any role for parser-derived syntactic factors Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5–8, Columbus, Ohio, USA, June 2008. c�2008</context>
</contexts>
<marker>Frazier, Fodor, 1978</marker>
<rawString>L. Frazier and J. D. Fodor. 1978. The sausage machine: a new two-stage parsing model. Cognition, 6:291–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of 2nd NAACL,</booktitle>
<pages>1--8</pages>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="1281" citStr="Hale (2001)" startWordPosition="189" endWordPosition="190">s search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory. 1 Introduction A growing body of work in cognitive science characterizes human readers as some kind of probabilistic parser (Jurafsky, 1996; Crocker and Brants, 2000; Chater and Manning, 2006). This view gains support when specific aspects of these programs match up well with measurable properties of humans engaged in sentence comprehension. One way to connect theory to data in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty. In other words, rare parser actions are cognitively costly. This basic notion has 5 proved remarkably applicable across sentence types and languages (Park and Brew, 2006; Demberg and Keller, 2007; Levy, 2008). The present work uses the time spent looking at a word during reading as an empirical me</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>J. Hale. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of 2nd NAACL, pages 1–8. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511</pages>
<contexts>
<context position="4063" citStr="Hays, 1964" startWordPosition="629" endWordPosition="630">ase attachment 3.0% Conjunction 1.9% Adverb ambiguity 1.8% Other 1.1% Total error 12.1% would have to go beyond these word-level influences. Our methodology imposes this requirement by fitting a kind of regression known as a linear mixed-effects model to the total reading times associated with each sentence-medial word in the Potsdam Sentence Corpus (PSC) (Kliegl et al., 2006). The PSC records the eye-movements of 272 native speakers as they read 144 German sentences. Table 2: Parser errors by category. 3 The Parsing Model The parser’s outputs define a relation on word pairs (Tesni`ere, 1959; Hays, 1964). The structural description in Figure 1 is an example output that depicts this dependency relation using arcs. The word near the arrowhead is the dependent, the other word its head (or governor). These outputs are built up by monotonically adding to an initially-empty set of dependency relations as analysis proceeds from left to right. To arrive at Figure 1 the Nivre parser passes through a number of intermediate states that aggregate four data structures, detailed below in Table 1. σ A stack of already-parsed unreduced words. τ An ordered input list of words. h A function from dependent word</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>D.G. Hays. 1964. Dependency theory: A formalism and some observations. Language, 40:511–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--137</pages>
<contexts>
<context position="919" citStr="Jurafsky, 1996" startWordPosition="128" endWordPosition="129">as a predictor in a linear mixed-effects model of German readers’ eye-fixation durations. This dependencybased predictor improves a baseline that takes into account word length, n-gram probability, and Cloze predictability that are typically applied in models of human reading. This improvement obtains even when the dependency parser explores a tiny fraction of its search space, as suggested by narrow-beam accounts of human sentence processing such as Garden Path theory. 1 Introduction A growing body of work in cognitive science characterizes human readers as some kind of probabilistic parser (Jurafsky, 1996; Crocker and Brants, 2000; Chater and Manning, 2006). This view gains support when specific aspects of these programs match up well with measurable properties of humans engaged in sentence comprehension. One way to connect theory to data in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that beha</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>D. Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20:137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kliegl</author>
<author>E Grabner</author>
<author>M Rolfs</author>
<author>R Engbert</author>
</authors>
<title>Length, frequency, and predictability effects of words on eye movements in reading.</title>
<date>2004</date>
<journal>European Journal of Cognitive Psychology,</journal>
<pages>16--262</pages>
<contexts>
<context position="3027" citStr="Kliegl et al., 2004" startWordPosition="466" endWordPosition="469">ows that at least some syntactic properties are reflected in readers’ eye fixation durations. From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer (Frazier and Fodor, 1978). 2 Parsing costs and theories of reading difficulty The length of time that a reader’s eyes spend fixated on a particular word in a sentence is known to be affected by a variety of word-level factors such as length in characters, n-gram frequency and empirical predictability (Ehrlich and Rayner, 1981; Kliegl et al., 2004). This last factor is the one measured when human readers are asked to guess the next word given a left-context string. Any role for parser-derived syntactic factors Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5–8, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Dependency structure of a PSC sentence. Error type Amount Noun attachment 4.2% Prepositional Phrase attachment 3.0% Conjunction 1.9% Adverb ambiguity 1.8% Other 1.1% Total error 12.1% would have to go beyond these word-level influences. Our methodology imposes this requi</context>
</contexts>
<marker>Kliegl, Grabner, Rolfs, Engbert, 2004</marker>
<rawString>R. Kliegl, E. Grabner, M. Rolfs, and R. Engbert. 2004. Length, frequency, and predictability effects of words on eye movements in reading. European Journal of Cognitive Psychology, 16:262–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kliegl</author>
<author>A Nuthmann</author>
<author>R Engbert</author>
</authors>
<title>Tracking the mind during reading: The influence of past, present, and future words on fixation durations.</title>
<date>2006</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>135--12</pages>
<contexts>
<context position="3831" citStr="Kliegl et al., 2006" startWordPosition="589" endWordPosition="592">-08: HLT, Short Papers (Companion Volume), pages 5–8, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Dependency structure of a PSC sentence. Error type Amount Noun attachment 4.2% Prepositional Phrase attachment 3.0% Conjunction 1.9% Adverb ambiguity 1.8% Other 1.1% Total error 12.1% would have to go beyond these word-level influences. Our methodology imposes this requirement by fitting a kind of regression known as a linear mixed-effects model to the total reading times associated with each sentence-medial word in the Potsdam Sentence Corpus (PSC) (Kliegl et al., 2006). The PSC records the eye-movements of 272 native speakers as they read 144 German sentences. Table 2: Parser errors by category. 3 The Parsing Model The parser’s outputs define a relation on word pairs (Tesni`ere, 1959; Hays, 1964). The structural description in Figure 1 is an example output that depicts this dependency relation using arcs. The word near the arrowhead is the dependent, the other word its head (or governor). These outputs are built up by monotonically adding to an initially-empty set of dependency relations as analysis proceeds from left to right. To arrive at Figure 1 the Niv</context>
</contexts>
<marker>Kliegl, Nuthmann, Engbert, 2006</marker>
<rawString>R. Kliegl, A. Nuthmann, and R. Engbert. 2006. Tracking the mind during reading: The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General, 135:12–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E K¨onig</author>
<author>W Lezius</author>
</authors>
<title>The TIGER language -a description language for syntax graphs, Formal definition.</title>
<date>2003</date>
<tech>Technical report, IMS,</tech>
<institution>Universit¨at</institution>
<location>Stuttgart, Germany.</location>
<marker>K¨onig, Lezius, 2003</marker>
<rawString>E. K¨onig and W. Lezius. 2003. The TIGER language -a description language for syntax graphs, Formal definition. Technical report, IMS, Universit¨at Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="1791" citStr="Levy, 2008" startWordPosition="267" endWordPosition="268">parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty. In other words, rare parser actions are cognitively costly. This basic notion has 5 proved remarkably applicable across sentence types and languages (Park and Brew, 2006; Demberg and Keller, 2007; Levy, 2008). The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004); these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any member of this parser family derives a use</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>R. Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together,</title>
<date>2004</date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona,</location>
<contexts>
<context position="2077" citStr="Nivre (2004)" startWordPosition="312" endWordPosition="313">al is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty. In other words, rare parser actions are cognitively costly. This basic notion has 5 proved remarkably applicable across sentence types and languages (Park and Brew, 2006; Demberg and Keller, 2007; Levy, 2008). The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004); these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any member of this parser family derives a useful predictor shows that at least some syntactic properties are reflected in readers’ eye fixation durations. From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>J. Nivre. 2004. Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Park</author>
<author>C Brew</author>
</authors>
<title>A finite-state model of human sentence processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL,</booktitle>
<pages>49--56</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1752" citStr="Park and Brew, 2006" startWordPosition="259" endWordPosition="262">o connect theory to data in this manner uses a parser’s probability model to work out the surprisal or log-probability of the next word. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. When the transition from previous word to current word is lowprobability, from the parser’s perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty. In other words, rare parser actions are cognitively costly. This basic notion has 5 proved remarkably applicable across sentence types and languages (Park and Brew, 2006; Demberg and Keller, 2007; Levy, 2008). The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004); these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any mem</context>
</contexts>
<marker>Park, Brew, 2006</marker>
<rawString>J. Park and C. Brew. 2006. A finite-state model of human sentence processing. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL, pages 49–56, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Simon</author>
</authors>
<title>A behavioral model of rational choice.</title>
<date>1955</date>
<journal>The Quarterly Journal of Economics,</journal>
<volume>69</volume>
<issue>1</issue>
<contexts>
<context position="9522" citStr="Simon, 1955" startWordPosition="1528" endWordPosition="1529">limited to a handful of parses. Indeed, a parser that handles a few analyses at a time (k=1,2,3) is just as valuable as one that spends far greater memory resources (k=100). This observation is consistent with Brants and Crocker’s (2000) observation that accuracy can be maintained even when restricted to 1% of the memory required for exhaustive parsing. The role of small k dependency parsers in determining the quality of statistical fit challenges the assumption that cognitive functions are global optima. Perhaps human parsing is boundedly rational in the sense of the bound imposed by Stack3 (Simon, 1955). 5 Conclusion This study demonstrates that surprisal calculated with a dependency parser is a significant predictor of reading times, an empirical measure of cognitive difficulty. Surprisal is a significant predictor even when examined alongside the more commonly used predictors, word length, predictability, and n-gram frequency. The viability of parsers that consider just a small number of analyses at each increment is consistent with conceptions of the human comprehender that incorporate that restriction. 7 Model Coefficient Std. Error t value DIC Baseline - - - 144511.1 k=1 0.033691 0.0022</context>
</contexts>
<marker>Simon, 1955</marker>
<rawString>H. Simon. 1955. A behavioral model of rational choice. The Quarterly Journal of Economics, 69(1):99–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>B Krenn</author>
<author>T Brants</author>
<author>H Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing ANLP-97,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="5324" citStr="Skut et al., 1997" startWordPosition="836" endWordPosition="839">s to arc types. Table 1: Parser configuration. The stack σ holds words that could eventually be connected by new arcs, while τ lists unparsed words. h and d are where the current set of dependency arcs reside. There are only four possible transitions from configuration to configuration. Left-Arc and Right-Arc transitions create dependency relations between the top elements in σ and τ, while Shift and Reduce transitions manipulate σ. When more than one transition is applicable, the parser decides between them by consulting a probability model derived from the Negra and Tiger newspaper corpora (Skut et al., 1997; K¨onig and Lezius, 2003). This model is called Stack3 because it considers only the parts-of-speech of the top three elements of σ along with the top element of τ. On the PSC this model achieves 87.9% precision and 79.5% recall for unlabeled dependencies. Most of the attachments it gets wrong (Table 2) represent alternative readings that would require semantic guidance to rule out. To compare “serial” human sentence processing models against “parallel” models, our implementation does beam search in the space of Nivreconfigurations. The number of configurations maintained at any point is a ch</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings of the Fifth Conference on Applied Natural Language Processing ANLP-97, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Spiegelhalter</author>
<author>N G Best</author>
<author>B P Carlin</author>
<author>A van der Linde</author>
</authors>
<title>Bayesian measures of model complexity and fit (with discussion).</title>
<date>2002</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>64--583</pages>
<marker>Spiegelhalter, Best, Carlin, van der Linde, 2002</marker>
<rawString>D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and A. van der Linde. 2002. Bayesian measures of model complexity and fit (with discussion). Journal of the Royal Statistical Society, 64(B):583–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale. Editions Klincksiek,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. El´ements de syntaxe structurale. Editions Klincksiek, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>