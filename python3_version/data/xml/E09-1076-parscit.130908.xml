<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005792">
<title confidence="0.995145">
Flexible Answer Typing with Discriminative Preference Ranking
</title>
<author confidence="0.997255">
Christopher Pinchak† Dekang Lint Davood Rafiei†
</author>
<affiliation confidence="0.991363">
†Department of Computing Science $Google Inc.
University of Alberta 1600 Amphitheatre Parkway
</affiliation>
<address confidence="0.758834">
Edmonton, Alberta, Canada Mountain View, CA, USA
</address>
<email confidence="0.998163">
{pinchak,drafiei}@cs.ualberta.ca lindek@google.com
</email>
<sectionHeader confidence="0.997379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940777777778">
An important part of question answering
is ensuring a candidate answer is plausi-
ble as a response. We present a flexible
approach based on discriminative prefer-
ence ranking to determine which of a set
of candidate answers are appropriate. Dis-
criminative methods provide superior per-
formance while at the same time allow the
flexibility of adding new and diverse fea-
tures. Experimental results on a set of fo-
cused What ...? and Which ...? questions
show that our learned preference ranking
methods perform better than alternative
solutions to the task of answer typing. A
gain of almost 0.2 in MRR for both the
first appropriate and first correct answers
is observed along with an increase in pre-
cision over the entire range of recall.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848393442623">
Question answering (QA) systems have received a
great deal of attention because they provide both
a natural means of querying via questions and be-
cause they return short, concise answers. These
two advantages simplify the task of finding in-
formation relevant to a topic of interest. Ques-
tions convey more than simply a natural language
query; an implicit expectation of answer type is
provided along with the question words. The dis-
covery and exploitation of this implicit expected
type is called answer typing.
We introduce an answer typing method that is
sufficiently flexible to use a wide variety of fea-
tures while at the same time providing a high level
of performance. Our answer typing method avoids
the use of pre-determined classes that are often
lacking for unanticipated answer types. Because
answer typing is only part of the QA task, a flexi-
ble answer typing model ensures that answer typ-
ing can be easily and usefully incorporated into a
complete QA system. A discriminative preference
ranking model with a preference for appropriate
answers is trained and applied to unseen ques-
tions. In terms of Mean Reciprocal Rank (MRR),
we observe improvements over existing systems of
around 0.2 both in terms of the correct answer and
in terms of appropriate responses. This increase
in MRR brings the performance of our model to
near the level of a full QA system on a subset of
questions, despite the fact that we rely on answer
typing features alone.
The amount of information given about the ex-
pected answer can vary by question. If the ques-
tion contains a question focus, which we define
to be the head noun following the wh-word such
as city in “What city hosted the 1988 Winter
Olympics?”, some of the typing information is ex-
plicitly stated. In this instance, the answer is re-
quired to be a city. However, there is often addi-
tional information available about the type. In our
example, the answer must plausibly host a Winter
Olympic Games. The focus, along with the ad-
ditional information, give strong clues about what
are appropriate as responses.
We define an appropriate candidate answer as
one that a user, who does not necessarily know
the correct answer, would identify as a plausible
answer to a given question. For most questions,
there exist plausible responses that are not correct
answers to the question. For our above question,
the city of Vancouver is plausible even though it
is not correct. For the purposes of this paper, we
assume correct answers are a subset of appropri-
ate candidates. Because answer typing is only in-
tended to be a component of a full QA system, we
rely on other components to help establish the true
correctness of a candidate answer.
The remainder of the paper is organized as fol-
lows. Section 2 presents the application of dis-
criminative preference rank learning to answer
typing. Section 3 introduces the models we use
</bodyText>
<note confidence="0.9229975">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 666–674,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99771">
666
</page>
<bodyText confidence="0.999960166666667">
for learning appropriate answer preferences. Sec-
tions 4 and 5 discuss our experiments and their re-
sults, respectively. Section 6 presents prior work
on answer typing and the use of discriminative
methods in QA. Finally, concluding remarks and
ideas for future work are presented in Section 7.
</bodyText>
<sectionHeader confidence="0.967143" genericHeader="introduction">
2 Preference Ranking
</sectionHeader>
<bodyText confidence="0.999737966666667">
Preference ranking naturally lends itself to any
problem in which the relative ordering between
examples is more important than labels or values
assigned to those examples. The classic exam-
ple application of preference ranking (Joachims,
2002) is that of information retrieval results rank-
ing. Generally, information retrieval results are
presented in some ordering such that those higher
on the list are either more relevant to the query or
would be of greater interest to the user.
In a preference ranking task we have a set of
candidates c1, c2, ..., c,,,, and a ranking r such that
the relation ci &lt;r cj holds if and only if can-
didate ci should be ranked higher than cj, for
1 ≤ i, j ≤ n and i =6 j. The ranking r can form
a total ordering, as in information retrieval, or a
partial ordering in which we have both ci �4r cj
and cj �4r ci. Partial orderings are useful for our
task of answer typing because they can be used to
specify candidates that are of an equivalent rank.
Given some ci &lt;r cj, preference ranking only
considers the difference between the feature rep-
resentations of ci and cj (Φ(ci) and Φ(cj), respec-
tively) as evidence. We want to learn some weight
vector w such that w · Φ(ci) &gt; w · Φ(cj) holds for
all pairs ci and cj that have the relation ci &lt;r cj. In
other words, we want w · (Φ(ci) − Φ(cj)) &gt; 0 and
we can use some margin in the place of 0. In the
context of Support Vector Machines (Joachims,
2002), we are trying to minimize the function:
</bodyText>
<equation confidence="0.9599008">
�
V(��, ��) = 2 1 �� ·�� +C �i�j (1)
subject to the constraints:
∀(ci &lt;r cj) : w · (Φ(ci) − Φ(cj)) ≥ 1 − �ij (2)
∀i, j : �ij ≥ 0 (3)
</equation>
<bodyText confidence="0.999936888888889">
The margin incorporates slack variables �ij for
problems that are not linearly separable. This
ranking task is analogous to the SVM classi-
fication task on the pairwise difference vectors
(Φ(ci) − Φ(cj)), known as rank constraints. Un-
like classification, no explicit negative evidence is
required as w·(Φ(ci)−Φ(cj)) =(−1)��·(Φ(cj)−
Φ(ci)). It is also important to note that no rank
constraints are generated for candidates for which
no order relation exists under the ranking r.
Support Vector Machines (SVMs) have previ-
ously been used for preference ranking in the
context of information retrieval (Joachims, 2002).
We adopt the same framework for answer typing
by preference ranking. The SVM&amp;quot;ght package
(Joachims, 1999) implements the preference rank-
ing of Joachims (2002) and is used here for learn-
ing answer types.
</bodyText>
<subsectionHeader confidence="0.998883">
2.1 Application to Answer Typing
</subsectionHeader>
<bodyText confidence="0.999952864864865">
Assigning meaningful scores for answer typing is
a difficult task. For example, given the question
“What city hosted the 1988 Winter Olympics?”
and the candidates New York, Calgary, and the
word blue, how can we identify New York and
Calgary as appropriate and the word blue as inap-
propriate? Scoring answer candidates is compli-
cated by the fact that a gold standard for appropri-
ateness scores does not exist. Therefore, we have
no a priori notion that New York is better than the
word blue by some amount v. Because of this, we
approach the problem of answer typing as one of
preference ranking in which the relative appropri-
ateness is more important than the absolute scores.
Preference ranking stands in contrast to classifi-
cation, in which a candidate is classified as appro-
priate or inappropriate depending on the values in
its feature representation. Unfortunately, simple
classification does not work well in the face of a
large imbalance in positive and negative examples.
In answer typing we typically have far more inap-
propriate candidates than appropriate candidates,
and this is especially true for the experiments de-
scribed in Section 4. This is indeed a problem for
our system, as neither re-weighting nor attempt-
ing to balance the set of examples with the use
of random negative examples were shown to give
better performance on development data. This is
not to say that some means of balancing the data
would not provide comparable or superior perfor-
mance, but rather that such a weighting or sam-
pling scheme is not obvious.
An additional benefit of preference ranking over
classification is that preference ranking models the
better-than relationship between candidates. Typ-
ically a set of candidate answers are all related to a
question in some way, and we wish to know which
</bodyText>
<page confidence="0.996367">
667
</page>
<bodyText confidence="0.998918333333333">
of the candidates are better than others. In con-
trast, binary classification simply deals with the
is/is-not relationship and will have difficulty when
two responses with similar feature values are clas-
sified differently. With preference ranking, viola-
tions of some rank constraints will affect the re-
sulting order of candidates, but sufficient ordering
information may still be present to correctly iden-
tify appropriate candidates.
To apply preference ranking to answer typing,
we learn a model over a set of questions qi, ..., qry,,.
Each question qz has a list of appropriate candidate
answers a(z,i), ..., a(z,.) and a list of inappropriate
candidate answers b(z,i), ..., b(z,„). The partial or-
dering r is simply the set
</bodyText>
<equation confidence="0.783178">
Vi, j, k : {a(z,�) &lt;r b(z,k)} (4)
</equation>
<bodyText confidence="0.999945235294118">
This means that rank constraints are only gen-
erated for candidate answers a(z,�) and b(z,k) for
question qz and not between candidates a(z j) and
b(l k) where i 7� l. For example, the candidate an-
swers for the question “What city hosted the 1988
Winter Olympics?” are not compared with those
for “What colour is the sky?” because our partial
ordering r does not attempt to rank candidates for
one question in relation to candidates for another.
Moreover, no rank constraints are generated be-
tween a(z j) and a(z k) nor b(z j) and b(z k) because
the partial ordering does not include orderings be-
tween two candidates of the same class. Given two
appropriate candidates to the question “What city
hosted the 1988 Winter Olympics?”, New York
and Calgary, rank constraints will not be created
for the pair (New York, Calgary).
</bodyText>
<sectionHeader confidence="0.998153" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.9999258">
We begin with the work of Pinchak and Lin (2006)
in which question contexts (dependency tree paths
involving the wh-word) are extracted from the
question and matched against those found in a cor-
pus of text. The basic idea is that words that are
appropriate as answers will appear in place of the
wh-word in these contexts when found in the cor-
pus. For example, the question “What city hosted
the 1988 Winter Olympics?” will have as one of
the question contexts “X hosted Olympics.” We
then consult a corpus to discover what replace-
ments for X were actually mentioned and smooth
the resulting distribution.
We use the model of Pinchak and Lin (2006)
to produce features for our discriminative model.
</bodyText>
<tableCaption confidence="0.994211">
Table 1: Feature templates
</tableCaption>
<table confidence="0.663156181818182">
Pattern Description
E(t, c) Estimated count of term t
in context c
C(t, c) Observed count of term t in
context c
Et, C(t&apos;, c) Count of all terms appearing
in context c
Ec, C(t, c&apos;) Count of term t in all
contexts
�(t) Count of the times t occurs
in the candidate list
</table>
<bodyText confidence="0.999596777777778">
These features are mostly based on question con-
texts, and are briefly summarized in Table 1. Fol-
lowing Pinchak and Lin (2006), all of our features
are derived from a limited corpus (AQUAINT);
large-scale text resources are not required for our
model to perform well. By restricting ourselves
to relatively small corpora, we believe that our ap-
proach will easily transfer to other domains or lan-
guages (provided parsing resources are available).
To address the sparseness of question contexts,
we remove lexical elements from question context
paths. This removal is performed after feature val-
ues are obtained for the fully lexicalized path; the
removal of lexical elements simply allows many
similar paths to share a single learned weight. For
example, the term Calgary in context X +— sub-
ject +— host --+ object --+ Olympics (X hosted
Olympics) is used to obtain a feature value v that
is assigned to a feature such as C(Calgary, X +—
subject +— * --+ object --+ *) = v. Removal of
lexical elements results in a space of 73 possible
question contexts. To facilitate learning, all counts
are log values and feature vectors are normalized
to unit length.
The estimated count of term t in context c,
E(t, c), is a component of the model of Pinchak
and Lin (2006) and is calculated according to:
</bodyText>
<equation confidence="0.996359">
E(t, c) = � Pr(X|t)C(x, c) (5)
X
</equation>
<bodyText confidence="0.999712333333333">
Essentially, this equation computes an expected
count for term t in question c by observing how
likely t is to be part of a cluster X (Pr(X|t)) and
then observing how often terms of cluster X oc-
cur in context c (C(X, c)). Although the model
of Pinchak and Lin (2006) is significantly more
</bodyText>
<page confidence="0.992172">
668
</page>
<bodyText confidence="0.999429696969697">
complex, we use their core idea of cluster-based
smoothing to decide how often a term t will oc-
cur in a context c, regardless of whether or not t
was actually observed in c within our corpus. The
Pinchak and Lin (2006) system is unable to as-
sign individual weights to different question con-
texts, even though not all question contexts are
equally important. For example, the Pinchak and
Lin (2006) model is forced to consider a question
focus context (such as “X is a city”) to be of equal
importance to non-focus contexts (such as “X host
Olympics”). However, we have observed that it is
more important that candidate X is a city than it
hosted an Olympics in this instance. Appropriate
answers are required to be cities even though not
all cities have hosted Olympics. We wish to ad-
dress this problem with the use of discriminative
methods.
The observed count features of term t in con-
text c, Qt, c), are included to allow for combina-
tion with the estimated values from the model of
Pinchak and Lin (2006). Because Pinchak and Lin
(2006) make use of cluster-based smoothing, er-
rors may occur. By including the observed counts
of term t in context c, we hope to allow for the
use of more accurate statistics whenever they are
available, and for the smoothed counts in cases for
which they are not.
Finally, we include the frequency of a term t in
the list of candidates, 5(t). The idea here is that
the correct and/or appropriate answers are likely
to be repeated many times in a list of candidate
answers. Terms that are strongly associated with
the question and appear often in results are likely
to be what the question is looking for.
Both the Qt, c) and 5(t) features are exten-
sions to the Pinchak and Lin (2006) model and can
be incorporated into the Pinchak and Lin (2006)
model with varying degrees of difficulty. The
value of 5(t) in particular is highly dependent on
the means used to obtain the candidate list, and the
distribution of words over the candidate list is of-
ten very different from the distribution of words in
the corpus. Because this feature value comes from
a different source than our other features, it would
be difficult to use in a non-discriminative model.
Correct answers to our set of questions are
obtained from the TREC 2002-2006 results
(Voorhees, 2002). For appropriateness labels we
turn to human annotators. Two annotators were in-
structed to label a candidate as appropriate if that
candidate was believable as an answer, even if that
candidate was not correct. For a question such as
“What city hosted the 1988 Winter Olympics?”,
all cities should be labeled as appropriate even
though only Calgary is correct. This task comes
with a moderate degree of difficulty, especially
when dealing with questions for which appropriate
answers are less obvious (such as “What kind of a
community is a Kibbutz?”). We observed an inter-
annotator (kappa) agreement of 0.73, which indi-
cates substantial agreement. This value of kappa
conveys the difficulty that even human annotators
have when trying to decide which candidates are
appropriate for a given question. Because of this
value of kappa, we adopt strict gold standard ap-
propriateness labels that are the intersection of the
two annotators’ labels (i.e., a candidate is only ap-
propriate if both annotators label it as such, other-
wise it is inappropriate).
We introduce four different models for the rank-
ing of appropriate answers, each of which makes
use of appropriateness labels in different ways:
Correctness Model: Although appropriateness
and correctness are not equivalent, this model
deals with distinguishing correct from incorrect
candidates in the hopes that the resulting model
will be able to perform well on finding both cor-
rect and appropriate answers. For learning, cor-
rect answers are placed at a rank above that of
incorrect candidates, regardless of whether or not
those candidates are appropriate. This represents
the strictest definition of appropriateness and re-
quires no human annotation.
Appropriateness Model: The correctness model
assumes only correct answers are appropriate. In
reality, this is seldom the case. For example,
documents or snipppets returned for the question
“What country did Catherine the Great rule?” will
contain not only Russia (the correct answer), but
also Germany (the nationality of her parents) and
Poland (her modern-day birthplace). To better ad-
dress this overly strict definition of appropriate-
ness, we rank all candidates labeled as appropri-
ate above those labeled as inappropriate, without
regards to correctness. Because we want to learn
a model for appropriateness, training on appropri-
ateness rather than correctness information should
produce a model closer to what we desire.
</bodyText>
<footnote confidence="0.538998333333333">
Combined Model: Discriminative preference
ranking is not limited to only two ranks. We
combine the ideas of correctness and appropri-
</footnote>
<page confidence="0.995719">
669
</page>
<bodyText confidence="0.999417470588235">
ateness together to form a three-rank combined
model. This model places correct answers above
appropriate-but-incorrect candidates, which are
in turn placed above inappropriate-and-incorrect
candidates.
Reduced Model: Both the appropriateness model
and the combined model incorporate a large num-
ber of rank constraints. We can reduce the number
of rank constraints generated by simply remov-
ing all appropriate, but incorrect, candidates from
consideration and otherwise following the correct-
ness model. The main difference is that some ap-
propriate candidates are no longer assigned a low
rank. By removing appropriate, but incorrect, can-
didates from the generation of rank constraints, we
no longer rank correct answers above appropriate
candidates.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999983981818182">
To compare with the prior approach of Pinchak
and Lin (2006), we use a set of what and which
questions with question focus (questions with a
noun phrase following the wh-word). These are
a subset of the more general what, which, and who
questions dealt with by Pinchak and Lin (2006).
Although our model can accommodate a wide
range of what, which, when, and who questions,
the focused what and which questions are an easily
identifiable subclass that are rarely definitional or
otherwise complex in terms of the desired answer.
We take the set of focused what and which ques-
tions from TREC 2002-2006 (Voorhees, 2002)
comprising a total of 385 questions and performed
9-fold cross-validation, with one dedicated devel-
opment partition (the tenth partition). The devel-
opment partition was used to tune the regulariza-
tion parameter of the SVM used for testing.
Candidates are obtained by submitting the ques-
tion as-is to the Google search engine and chunk-
ing the top 20 snippets returned, resulting in an
average of 140 candidates per question. Google
snippets create a better confusion set than simply
random words for appropriate and inappropriate
candidates; many of the terms found in Google
snippets are related in some way to the question.
To ensure a correct answer is present (where pos-
sible), we append the list of correct answers to the
list of candidates.
As a measure of performance, we adopt Mean
Reciprocal Rank (MRR) for both correct and ap-
propriate answers, as well as precision-recall for
appropriate answers. MRR is useful as a mea-
sure of overall QA system performance (Voorhees,
2002), but is based only on the top correct or
appropriate answer encountered in a ranked list.
For this reason, we also show the precision-recall
curve to better understand how our models per-
form.
We compare our models with three alternative
approaches, the simplest of which is random. For
random, the candidate answers are randomly shuf-
fled and performance is averaged over a number
of runs (100). The snippet frequency approach
orders candidates based on their frequency of oc-
currence in the Google snippets, and is simply the
5(t) feature of our discriminative models in isola-
tion. We remove terms comprised solely of ques-
tion words from all approaches to prevent question
words (which tend to be very frequent in the snip-
pets) from being selected as answers. The last of
our alternative systems is an implementation of the
work of Pinchak and Lin (2006) in which the out-
put probabilities of their model are used to rank
candidates.
</bodyText>
<sectionHeader confidence="0.536001" genericHeader="method">
4.1 Results
</sectionHeader>
<bodyText confidence="0.999829625">
Figures 1 and 2 show the MRR results and
precision-recall curve of our correctness model
against the alternative approaches. In comparison
to these alternative systems, we show two versions
of our correctness model. The first uses a linear
kernel and is able to outperform the alternative ap-
proaches. The second uses a radial basis function
(RBF) kernel and exhibits performance superior to
that of the linear kernel. This suggests a degree
of non-linearity present in the data that cannot be
captured by the linear kernel alone. Both the train-
ing and running times of the RBF kernel are con-
siderably larger than that of the linear kernel. The
accuracy gain of the RBF kernel must therefore be
weighed against the increased time required to use
the model.
Figures 3 and 4 give the MRR results and
precision-recall curves for our additional mod-
els in comparison with that of the correctness
model. Although losses in MRR and precision
are observed for both the appropriate and com-
bined model using the RBF kernel, the linear ker-
nel versions of these models show slight perfor-
mance gains.
</bodyText>
<page confidence="0.997457">
670
</page>
<figureCaption confidence="0.999455">
Figure 2: Precision-recall of appropriate candi-
dates under the correctness model
Figure 1: MRR results for the correctness model
</figureCaption>
<figure confidence="0.995850472222222">
First Correct Answer First Appropriate Candidate
Random
Snippet Frequency
Pinchak and Lin (2006)
Linear Kernel
RBF Kernel
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
Mean Reciprocal Rank (MRR)
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Precision 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
RBF Kernel
Linear Kernel
Pinchak &amp; Lin (2006)
Snippet Frequency
Random
</figure>
<sectionHeader confidence="0.954563" genericHeader="method">
5 Discussion of Results
</sectionHeader>
<bodyText confidence="0.99997286440678">
The results of our correctness model, found in Fig-
ures 1 and 2 show considerable gains over our al-
ternative systems, including that of Pinchak and
Lin (2006). The Pinchak and Lin (2006) system
was specifically designed with answer typing in
mind, although it makes use of a brittle generative
model that does not account for ranking of answer
candidates nor for the variable strength of various
question contexts. These results show that our dis-
criminative preference ranking approach creates a
better model of both correctness and appropriate-
ness via weighting of contexts, preference rank
learning, and with the incorporation of additional
related features (Table 1). The last feature, snippet
frequency, is not particularly strong on its own, but
can be easily incorporated into our discriminative
model. The ability to add a wide variety of po-
tentially helpful features is one of the strengths of
discriminative techniques in general.
By moving away from simply correct answers
in the correctness model and incorporating labeled
appropriate examples in various ways, we are able
to further improve upon the performance of our
approach. Training on appropriateness labels in-
stead of correct answers results in a loss in MRR
for the first correct answer, but a gain in MRR for
the first appropriate candidate. Unfortunately, this
does not carry over to the entire range of precision
over recall. For the linear kernel, our three ad-
ditional models (appropriateness, combined, and
reduced) show consistent improvements over the
correctness model, but with the RBF kernel only
the reduced model produces a meaningful change.
The precision-recall curves of Figures 2 and 4
show remarkable consistency across the full range
of recall, despite the fact that candidates exist for
which feature values cannot easily be obtained.
Due to tagging and chunking errors, ill-formed
candidates may exist that are judged appropriate
by the annotators. For example, “explorer Her-
nando Soto” is a candidate marked appropriate
by both annotators to the question “What Span-
ish explorer discovered the Mississippi River?”
However, our context database does not include
the phrase “explorer Hernando Soto” meaning that
only a few features will have non-zero values. De-
spite these occasional problems, our models are
able to rank most correct and appropriate candi-
dates high in a ranked list.
Finally, we examine the effects of training set
size on MRR. The learning curve for a single par-
titioning under the correctness model is presented
in Figure 5. Although the model trained with
the RBF kernel exhibits some degree of instabil-
ity below 100 training questions, both the linear
and RBF models gain little benefit from additional
training questions beyond 100. This may be due
to the fact that the most common unlexicalized
question contexts have been observed in the first
</bodyText>
<page confidence="0.997976">
671
</page>
<figureCaption confidence="0.999512">
Figure 3: MRR results (RBF kernel)
Figure 4: Precision-recall of appropriate (RBF
</figureCaption>
<figure confidence="0.986461514285714">
kernel)
First Correct Answer First Appropriate Candidate
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
Mean Reciprocal Rank (MRR)
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Precision 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
</figure>
<bodyText confidence="0.994863166666667">
100 training examples and so therefore additional
questions simply repeat the same information. Re-
quiring only a relatively small number of training
examples means that an effective model can be
learned with relatively little input in the form of
question-answer pairs or annotated candidate lists.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="method">
6 Prior Work
</sectionHeader>
<bodyText confidence="0.999856692307692">
The expected answer type can be captured in a
number of possible ways. By far the most com-
mon is the assignment of one or more prede-
fined types to a question during a question anal-
ysis phase. Although the vast majority of the ap-
proaches to answer type detection make use of
rules (either partly or wholly) (Harabagiu et al.,
2005; Sun et al., 2005; Wu et al., 2005; Moll´a and
Gardiner, 2004), a few notable learned methods
for answer type detection exist.
One of the first attempts at learning a model for
answer type detection was made by Ittycheriah et
al. (2000; 2001) who learn a maximum entropy
classifier over the Message Understanding Confer-
ence (MUC) types. Those same MUC types are
then assigned by a named-entity tagger to iden-
tify appropriate candidate answers. Because of the
potential for unanticipated types, Ittycheriah et al.
(2000; 2001) include a Phrase type as a catch-all
class that is used when no other class is appropri-
ate. Although the classifier and named-entity tag-
ger are shown to be among the components with
the lowest error rate in their QA system, it is not
clear how much benefit is obtained from using a
relatively coarse-grained set of classes.
The approach of Li and Roth (2002) is sim-
ilar in that it uses learning for answer type de-
tection. They make use of multi-class learning
with a Sparse Network of Winnows (SNoW) and
a two-layer class hierarchy comprising a total of
fifty possible answer types. These finer-grained
classes are of more use when computing a notion
of appropriateness, although one major drawback
is that no entity tagger is discussed that can iden-
tify these types in text. Li and Roth (2002) also
rely on a rigid set of classes and so run the risk of
encountering a new question of an unseen type.
Pinchak and Lin (2006) present an alternative in
which the probability of a term being appropriate
to a question is computed directly. Instead of as-
signing an answer type to a question, the question
is broken down into a number of possibly overlap-
ping contexts. A candidate is then evaluated as to
how likely it is to appear in these contexts. Un-
fortunately, Pinchak and Lin (2006) use a brittle
generative model when combining question con-
texts that assumes all contexts are equally impor-
tant. This assumption was dealt with by Pinchak
and Lin (2006) by discarding all non-focus con-
texts with a focus context is present, but this is not
an ideal solution.
Learning methods are abundant in QA research
</bodyText>
<page confidence="0.995543">
672
</page>
<figureCaption confidence="0.899925">
Figure 5: Learning curve for MRR of the first cor-
rect answer under the correctness model
</figureCaption>
<bodyText confidence="0.9999262">
and have been applied in a number of different
ways. Ittycheriah et al. (2000) created an en-
tire QA system based on maximum entropy com-
ponents in addition to the question classifier dis-
cussed above. Ittycheriah et al. (2000) were able
to obtain reasonable performance from learned
components alone, although future versions of the
system use non-learned components in addition to
learned components (Prager et al., 2003). The
JAVELIN I system (Nyberg et al., 2005) uses
a SVM during the answer/information extraction
phase. Although learning is applied in many QA
tasks, very few QA systems rely solely on learn-
ing. Compositional approaches, in which multiple
distinct QA techniques are combined, also show
promise for improving QA performance. Echihabi
et al. (2003) use three separate answer extraction
agents and combine the output scores with a max-
imum entropy re-ranker.
Surdeanu et al. (2008) explore preference rank-
ing for advice or “how to” questions in which a
unique correct answer is preferred over all other
candidates. Their focus is on complex-answer
questions in addition to the use of a collection of
user-generated answers rather than answer typing.
However, their use of preference ranking mirrors
the techniques we describe here in which the rela-
tive difference between two candidates at different
ranks is more important than the individual candi-
dates.
</bodyText>
<sectionHeader confidence="0.995848" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987756097561">
We have introduced a means of flexible answer
typing with discriminative preference rank learn-
ing. Although answer typing does not represent a
complete QA system, it is an important component
to ensure that those candidates selected as answers
are indeed appropriate to the question being asked.
By casting the problem of evaluating appropriate-
ness as one of preference ranking, we allow for
the learning of what differentiates an appropriate
candidate from an inappropriate one.
Experimental results on focused what and
which questions show that a discriminatively
trained preference rank model is able to outper-
form alternative approaches designed for the same
task. This increase in performance comes from
both the flexibility to easily combine a number of
weighted features and because comparisons only
need to be made between appropriate and inappro-
priate candidates. A preference ranking model can
be trained from a relatively small set of example
questions, meaning that only a small number of
question/answer pairs or annotated candidate lists
are required.
The power of an answer typing system lies
in its ability to identify, in terms of some given
query, appropriate candidates. Applying the flexi-
ble model described here to a domain other than
question answering could allow for a more fo-
cused set of results. One straight-forward appli-
cation is to apply our model to the process of in-
formation or document retrieval itself. Ensuring
that there are terms present in the document ap-
propriate to the query could allow for the intel-
ligent expansion of the query. In a related vein,
queries are occasionally comprised of natural lan-
guage text fragments that can be treated similarly
to questions. Rarely are users searching for sim-
ple mentions of the query in pages; we wish to
provide them with something more useful. Our
model achieves the goal of finding those appropri-
ate related concepts.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996178428571428">
We would like to thank Debra Shiau for her as-
sistance annotating training and test data and the
anonymous reviewers for their insightful com-
ments. We would also like to thank the Alberta
Informatics Circle of Research Excellence and the
Alberta Ingenuity Fund for their support in devel-
oping this work.
</bodyText>
<figure confidence="0.999365333333333">
Mean Reciprocal Rank (MRR)
0.9
0.6
0.4
0.2
0.8
0.7
0.5
0.3
0.1
010 25 50 100 150 200 310
Training Set Size
1
RBF Kernel
Linear Kernel
Snippet Frequency
Pinchak &amp; Lin (2006)
Random
</figure>
<page confidence="0.996865">
673
</page>
<sectionHeader confidence="0.998176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883013888889">
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu,
E. Melz, and D. Ravichandran. 2003. Multiple-
Engine Question Answering in TextMap. In Pro-
ceedings of the Twelfth Text REtrieval Conference
(TREC-2003), Gaithersburg, Maryland.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing Two
Question Answering Systems in TREC-2005. In
Proceedings of the Fourteenth Text REtrieval Con-
ference (TREC-2005), Gaithersburg, Maryland.
A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi,
and R. Mammone. 2000. IBM’s Statistical Ques-
tion Answering System. In Proceedings of the 9th
Text REtrieval Conference (TREC-9), Gaithersburg,
Maryland.
A. Ittycheriah, M. Franz, and S. Roukos. 2001. IBM’s
Statistical Question Answering System – TREC-10.
In Proceedings of the 10th Text REtrieval Confer-
ence (TREC-10), Gaithersburg, Maryland.
T. Joachims. 1999. Making Large-Scale SVM Learn-
ing Practical. In B. Sch¨olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT-Press.
T. Joachims. 2002. Optimizing Search Engines Us-
ing Clickthrough Data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD). ACM.
X. Li and D. Roth. 2002. Learning Question Clas-
sifiers. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING 2002),
pages 556–562.
D. Moll´a and M. Gardiner. 2004. AnswerFinder -
Question Answering by Combining Lexical, Syntac-
tic and Semantic Information. In Proceedings of the
Australian Language Technology Workshop (ALTW
2004, pages 9–16, Sydney, December.
E. Nyberg, R. Frederking, T. Mitamura, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,
V. Pedro, and A. Schlaikjer. 2005. JAVELIN I and
II Systems at TREC 2005. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
C. Pinchak and D. Lin. 2006. A Probabilistic Answer
Type Model. In Proceedings of the Eleventh Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2006), Trento,
Italy, April.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2003. IBM’s PIQUANT
in TREC2003. In Proceedings of the Twelfth Text
REtrieval Conference (TREC-2003), Gaithersburg,
Maryland.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T-S. Chua, and M-Y.
Kan. 2005. Using Syntactic and Semantic Relation
Analysis in Question Answering. In Proceedings
of the Fourteenth Text REtrieval Conference (TREC-
2005), Gaithersburg, Maryland.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of the 46th Annual Meeting for
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-08: HLT), pages
719–727, Columbus, Ohio, June. Association for
Computational Linguistics.
E.M. Voorhees. 2002. Overview of the TREC 2002
Question Answering Track. In Proceedings of
TREC 2002, Gaithersburg, Maryland.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. ILQUA – An IE-Driven Ques-
tion Answering System. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
</reference>
<page confidence="0.998832">
674
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696535">
<title confidence="0.999327">Flexible Answer Typing with Discriminative Preference Ranking</title>
<author confidence="0.933932">Dekang Davood</author>
<affiliation confidence="0.9117905">of Computing Science Inc. University of Alberta 1600 Amphitheatre Parkway</affiliation>
<address confidence="0.998177">Edmonton, Alberta, Canada Mountain View, CA, USA</address>
<email confidence="0.999921">lindek@google.com</email>
<abstract confidence="0.995089315789474">An important part of question answering is ensuring a candidate answer is plausible as a response. We present a flexible approach based on discriminative preference ranking to determine which of a set of candidate answers are appropriate. Discriminative methods provide superior performance while at the same time allow the flexibility of adding new and diverse features. Experimental results on a set of focused What ...? and Which ...? questions show that our learned preference ranking methods perform better than alternative solutions to the task of answer typing. A gain of almost 0.2 in MRR for both the first appropriate and first correct answers is observed along with an increase in precision over the entire range of recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>U Hermjakob</author>
<author>E Hovy</author>
<author>D Marcu</author>
<author>E Melz</author>
<author>D Ravichandran</author>
</authors>
<title>MultipleEngine Question Answering in TextMap.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC-2003),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="29592" citStr="Echihabi et al. (2003)" startWordPosition="4963" endWordPosition="4966"> addition to the question classifier discussed above. Ittycheriah et al. (2000) were able to obtain reasonable performance from learned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techniques are combined, also show promise for improving QA performance. Echihabi et al. (2003) use three separate answer extraction agents and combine the output scores with a maximum entropy re-ranker. Surdeanu et al. (2008) explore preference ranking for advice or “how to” questions in which a unique correct answer is preferred over all other candidates. Their focus is on complex-answer questions in addition to the use of a collection of user-generated answers rather than answer typing. However, their use of preference ranking mirrors the techniques we describe here in which the relative difference between two candidates at different ranks is more important than the individual candid</context>
</contexts>
<marker>Echihabi, Hermjakob, Hovy, Marcu, Melz, Ravichandran, 2003</marker>
<rawString>A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran. 2003. MultipleEngine Question Answering in TextMap. In Proceedings of the Twelfth Text REtrieval Conference (TREC-2003), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing Two Question Answering Systems in TREC-2005.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="26602" citStr="Harabagiu et al., 2005" startWordPosition="4456" endWordPosition="4459">es and so therefore additional questions simply repeat the same information. Requiring only a relatively small number of training examples means that an effective model can be learned with relatively little input in the form of question-answer pairs or annotated candidate lists. 6 Prior Work The expected answer type can be captured in a number of possible ways. By far the most common is the assignment of one or more predefined types to a question during a question analysis phase. Although the vast majority of the approaches to answer type detection make use of rules (either partly or wholly) (Harabagiu et al., 2005; Sun et al., 2005; Wu et al., 2005; Moll´a and Gardiner, 2004), a few notable learned methods for answer type detection exist. One of the first attempts at learning a model for answer type detection was made by Ittycheriah et al. (2000; 2001) who learn a maximum entropy classifier over the Message Understanding Conference (MUC) types. Those same MUC types are then assigned by a named-entity tagger to identify appropriate candidate answers. Because of the potential for unanticipated types, Ittycheriah et al. (2000; 2001) include a Phrase type as a catch-all class that is used when no other cla</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005. Employing Two Question Answering Systems in TREC-2005. In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W-J Zhu</author>
<author>A Ratnaparkhi</author>
<author>R Mammone</author>
</authors>
<title>IBM’s Statistical Question Answering System.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th Text REtrieval Conference (TREC-9),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="26838" citStr="Ittycheriah et al. (2000" startWordPosition="4498" endWordPosition="4501">on-answer pairs or annotated candidate lists. 6 Prior Work The expected answer type can be captured in a number of possible ways. By far the most common is the assignment of one or more predefined types to a question during a question analysis phase. Although the vast majority of the approaches to answer type detection make use of rules (either partly or wholly) (Harabagiu et al., 2005; Sun et al., 2005; Wu et al., 2005; Moll´a and Gardiner, 2004), a few notable learned methods for answer type detection exist. One of the first attempts at learning a model for answer type detection was made by Ittycheriah et al. (2000; 2001) who learn a maximum entropy classifier over the Message Understanding Conference (MUC) types. Those same MUC types are then assigned by a named-entity tagger to identify appropriate candidate answers. Because of the potential for unanticipated types, Ittycheriah et al. (2000; 2001) include a Phrase type as a catch-all class that is used when no other class is appropriate. Although the classifier and named-entity tagger are shown to be among the components with the lowest error rate in their QA system, it is not clear how much benefit is obtained from using a relatively coarse-grained s</context>
<context position="28903" citStr="Ittycheriah et al. (2000)" startWordPosition="4856" endWordPosition="4859">ing contexts. A candidate is then evaluated as to how likely it is to appear in these contexts. Unfortunately, Pinchak and Lin (2006) use a brittle generative model when combining question contexts that assumes all contexts are equally important. This assumption was dealt with by Pinchak and Lin (2006) by discarding all non-focus contexts with a focus context is present, but this is not an ideal solution. Learning methods are abundant in QA research 672 Figure 5: Learning curve for MRR of the first correct answer under the correctness model and have been applied in a number of different ways. Ittycheriah et al. (2000) created an entire QA system based on maximum entropy components in addition to the question classifier discussed above. Ittycheriah et al. (2000) were able to obtain reasonable performance from learned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techni</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, Mammone, 2000</marker>
<rawString>A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi, and R. Mammone. 2000. IBM’s Statistical Question Answering System. In Proceedings of the 9th Text REtrieval Conference (TREC-9), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>S Roukos</author>
</authors>
<title>IBM’s Statistical Question Answering System – TREC-10.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th Text REtrieval Conference (TREC-10),</booktitle>
<location>Gaithersburg, Maryland.</location>
<marker>Ittycheriah, Franz, Roukos, 2001</marker>
<rawString>A. Ittycheriah, M. Franz, and S. Roukos. 2001. IBM’s Statistical Question Answering System – TREC-10. In Proceedings of the 10th Text REtrieval Conference (TREC-10), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning. MIT-Press.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="6757" citStr="Joachims, 1999" startWordPosition="1147" endWordPosition="1148">k is analogous to the SVM classification task on the pairwise difference vectors (Φ(ci) − Φ(cj)), known as rank constraints. Unlike classification, no explicit negative evidence is required as w·(Φ(ci)−Φ(cj)) =(−1)��·(Φ(cj)− Φ(ci)). It is also important to note that no rank constraints are generated for candidates for which no order relation exists under the ranking r. Support Vector Machines (SVMs) have previously been used for preference ranking in the context of information retrieval (Joachims, 2002). We adopt the same framework for answer typing by preference ranking. The SVM&amp;quot;ght package (Joachims, 1999) implements the preference ranking of Joachims (2002) and is used here for learning answer types. 2.1 Application to Answer Typing Assigning meaningful scores for answer typing is a difficult task. For example, given the question “What city hosted the 1988 Winter Olympics?” and the candidates New York, Calgary, and the word blue, how can we identify New York and Calgary as appropriate and the word blue as inappropriate? Scoring answer candidates is complicated by the fact that a gold standard for appropriateness scores does not exist. Therefore, we have no a priori notion that New York is bett</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing Search Engines Using Clickthrough Data.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="4674" citStr="Joachims, 2002" startWordPosition="762" endWordPosition="763">009. c�2009 Association for Computational Linguistics 666 for learning appropriate answer preferences. Sections 4 and 5 discuss our experiments and their results, respectively. Section 6 presents prior work on answer typing and the use of discriminative methods in QA. Finally, concluding remarks and ideas for future work are presented in Section 7. 2 Preference Ranking Preference ranking naturally lends itself to any problem in which the relative ordering between examples is more important than labels or values assigned to those examples. The classic example application of preference ranking (Joachims, 2002) is that of information retrieval results ranking. Generally, information retrieval results are presented in some ordering such that those higher on the list are either more relevant to the query or would be of greater interest to the user. In a preference ranking task we have a set of candidates c1, c2, ..., c,,,, and a ranking r such that the relation ci &lt;r cj holds if and only if candidate ci should be ranked higher than cj, for 1 ≤ i, j ≤ n and i =6 j. The ranking r can form a total ordering, as in information retrieval, or a partial ordering in which we have both ci �4r cj and cj �4r ci. </context>
<context position="6650" citStr="Joachims, 2002" startWordPosition="1131" endWordPosition="1132"> The margin incorporates slack variables �ij for problems that are not linearly separable. This ranking task is analogous to the SVM classification task on the pairwise difference vectors (Φ(ci) − Φ(cj)), known as rank constraints. Unlike classification, no explicit negative evidence is required as w·(Φ(ci)−Φ(cj)) =(−1)��·(Φ(cj)− Φ(ci)). It is also important to note that no rank constraints are generated for candidates for which no order relation exists under the ranking r. Support Vector Machines (SVMs) have previously been used for preference ranking in the context of information retrieval (Joachims, 2002). We adopt the same framework for answer typing by preference ranking. The SVM&amp;quot;ght package (Joachims, 1999) implements the preference ranking of Joachims (2002) and is used here for learning answer types. 2.1 Application to Answer Typing Assigning meaningful scores for answer typing is a difficult task. For example, given the question “What city hosted the 1988 Winter Olympics?” and the candidates New York, Calgary, and the word blue, how can we identify New York and Calgary as appropriate and the word blue as inappropriate? Scoring answer candidates is complicated by the fact that a gold stan</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING</booktitle>
<pages>556--562</pages>
<contexts>
<context position="27487" citStr="Li and Roth (2002)" startWordPosition="4607" endWordPosition="4610">ropy classifier over the Message Understanding Conference (MUC) types. Those same MUC types are then assigned by a named-entity tagger to identify appropriate candidate answers. Because of the potential for unanticipated types, Ittycheriah et al. (2000; 2001) include a Phrase type as a catch-all class that is used when no other class is appropriate. Although the classifier and named-entity tagger are shown to be among the components with the lowest error rate in their QA system, it is not clear how much benefit is obtained from using a relatively coarse-grained set of classes. The approach of Li and Roth (2002) is similar in that it uses learning for answer type detection. They make use of multi-class learning with a Sparse Network of Winnows (SNoW) and a two-layer class hierarchy comprising a total of fifty possible answer types. These finer-grained classes are of more use when computing a notion of appropriateness, although one major drawback is that no entity tagger is discussed that can identify these types in text. Li and Roth (2002) also rely on a rigid set of classes and so run the risk of encountering a new question of an unseen type. Pinchak and Lin (2006) present an alternative in which th</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning Question Classifiers. In Proceedings of the International Conference on Computational Linguistics (COLING 2002), pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moll´a</author>
<author>M Gardiner</author>
</authors>
<title>AnswerFinder -Question Answering by Combining Lexical, Syntactic and Semantic Information.</title>
<date>2004</date>
<booktitle>In Proceedings of the Australian Language Technology Workshop (ALTW 2004,</booktitle>
<pages>9--16</pages>
<location>Sydney,</location>
<marker>Moll´a, Gardiner, 2004</marker>
<rawString>D. Moll´a and M. Gardiner. 2004. AnswerFinder -Question Answering by Combining Lexical, Syntactic and Semantic Information. In Proceedings of the Australian Language Technology Workshop (ALTW 2004, pages 9–16, Sydney, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>R Frederking</author>
<author>T Mitamura</author>
<author>M Bilotti</author>
<author>K Hannan</author>
<author>L Hiyakumoto</author>
<author>J Ko</author>
<author>F Lin</author>
<author>L Lita</author>
<author>V Pedro</author>
<author>A Schlaikjer</author>
</authors>
<date>2005</date>
<journal>JAVELIN I and II Systems at TREC</journal>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="29289" citStr="Nyberg et al., 2005" startWordPosition="4917" endWordPosition="4920">n ideal solution. Learning methods are abundant in QA research 672 Figure 5: Learning curve for MRR of the first correct answer under the correctness model and have been applied in a number of different ways. Ittycheriah et al. (2000) created an entire QA system based on maximum entropy components in addition to the question classifier discussed above. Ittycheriah et al. (2000) were able to obtain reasonable performance from learned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techniques are combined, also show promise for improving QA performance. Echihabi et al. (2003) use three separate answer extraction agents and combine the output scores with a maximum entropy re-ranker. Surdeanu et al. (2008) explore preference ranking for advice or “how to” questions in which a unique correct answer is preferred over all other candidates. Their focus is on complex-answer</context>
</contexts>
<marker>Nyberg, Frederking, Mitamura, Bilotti, Hannan, Hiyakumoto, Ko, Lin, Lita, Pedro, Schlaikjer, 2005</marker>
<rawString>E. Nyberg, R. Frederking, T. Mitamura, M. Bilotti, K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita, V. Pedro, and A. Schlaikjer. 2005. JAVELIN I and II Systems at TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pinchak</author>
<author>D Lin</author>
</authors>
<title>A Probabilistic Answer Type Model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="10326" citStr="Pinchak and Lin (2006)" startWordPosition="1745" endWordPosition="1748">cs?” are not compared with those for “What colour is the sky?” because our partial ordering r does not attempt to rank candidates for one question in relation to candidates for another. Moreover, no rank constraints are generated between a(z j) and a(z k) nor b(z j) and b(z k) because the partial ordering does not include orderings between two candidates of the same class. Given two appropriate candidates to the question “What city hosted the 1988 Winter Olympics?”, New York and Calgary, rank constraints will not be created for the pair (New York, Calgary). 3 Methods We begin with the work of Pinchak and Lin (2006) in which question contexts (dependency tree paths involving the wh-word) are extracted from the question and matched against those found in a corpus of text. The basic idea is that words that are appropriate as answers will appear in place of the wh-word in these contexts when found in the corpus. For example, the question “What city hosted the 1988 Winter Olympics?” will have as one of the question contexts “X hosted Olympics.” We then consult a corpus to discover what replacements for X were actually mentioned and smooth the resulting distribution. We use the model of Pinchak and Lin (2006)</context>
<context position="12532" citStr="Pinchak and Lin (2006)" startWordPosition="2131" endWordPosition="2134">h; the removal of lexical elements simply allows many similar paths to share a single learned weight. For example, the term Calgary in context X +— subject +— host --+ object --+ Olympics (X hosted Olympics) is used to obtain a feature value v that is assigned to a feature such as C(Calgary, X +— subject +— * --+ object --+ *) = v. Removal of lexical elements results in a space of 73 possible question contexts. To facilitate learning, all counts are log values and feature vectors are normalized to unit length. The estimated count of term t in context c, E(t, c), is a component of the model of Pinchak and Lin (2006) and is calculated according to: E(t, c) = � Pr(X|t)C(x, c) (5) X Essentially, this equation computes an expected count for term t in question c by observing how likely t is to be part of a cluster X (Pr(X|t)) and then observing how often terms of cluster X occur in context c (C(X, c)). Although the model of Pinchak and Lin (2006) is significantly more 668 complex, we use their core idea of cluster-based smoothing to decide how often a term t will occur in a context c, regardless of whether or not t was actually observed in c within our corpus. The Pinchak and Lin (2006) system is unable to as</context>
<context position="13898" citStr="Pinchak and Lin (2006)" startWordPosition="2381" endWordPosition="2384">d Lin (2006) model is forced to consider a question focus context (such as “X is a city”) to be of equal importance to non-focus contexts (such as “X host Olympics”). However, we have observed that it is more important that candidate X is a city than it hosted an Olympics in this instance. Appropriate answers are required to be cities even though not all cities have hosted Olympics. We wish to address this problem with the use of discriminative methods. The observed count features of term t in context c, Qt, c), are included to allow for combination with the estimated values from the model of Pinchak and Lin (2006). Because Pinchak and Lin (2006) make use of cluster-based smoothing, errors may occur. By including the observed counts of term t in context c, we hope to allow for the use of more accurate statistics whenever they are available, and for the smoothed counts in cases for which they are not. Finally, we include the frequency of a term t in the list of candidates, 5(t). The idea here is that the correct and/or appropriate answers are likely to be repeated many times in a list of candidate answers. Terms that are strongly associated with the question and appear often in results are likely to be w</context>
<context position="18574" citStr="Pinchak and Lin (2006)" startWordPosition="3125" endWordPosition="3128"> Reduced Model: Both the appropriateness model and the combined model incorporate a large number of rank constraints. We can reduce the number of rank constraints generated by simply removing all appropriate, but incorrect, candidates from consideration and otherwise following the correctness model. The main difference is that some appropriate candidates are no longer assigned a low rank. By removing appropriate, but incorrect, candidates from the generation of rank constraints, we no longer rank correct answers above appropriate candidates. 4 Experiments To compare with the prior approach of Pinchak and Lin (2006), we use a set of what and which questions with question focus (questions with a noun phrase following the wh-word). These are a subset of the more general what, which, and who questions dealt with by Pinchak and Lin (2006). Although our model can accommodate a wide range of what, which, when, and who questions, the focused what and which questions are an easily identifiable subclass that are rarely definitional or otherwise complex in terms of the desired answer. We take the set of focused what and which questions from TREC 2002-2006 (Voorhees, 2002) comprising a total of 385 questions and pe</context>
<context position="20968" citStr="Pinchak and Lin (2006)" startWordPosition="3525" endWordPosition="3528">ative approaches, the simplest of which is random. For random, the candidate answers are randomly shuffled and performance is averaged over a number of runs (100). The snippet frequency approach orders candidates based on their frequency of occurrence in the Google snippets, and is simply the 5(t) feature of our discriminative models in isolation. We remove terms comprised solely of question words from all approaches to prevent question words (which tend to be very frequent in the snippets) from being selected as answers. The last of our alternative systems is an implementation of the work of Pinchak and Lin (2006) in which the output probabilities of their model are used to rank candidates. 4.1 Results Figures 1 and 2 show the MRR results and precision-recall curve of our correctness model against the alternative approaches. In comparison to these alternative systems, we show two versions of our correctness model. The first uses a linear kernel and is able to outperform the alternative approaches. The second uses a radial basis function (RBF) kernel and exhibits performance superior to that of the linear kernel. This suggests a degree of non-linearity present in the data that cannot be captured by the </context>
<context position="22375" citStr="Pinchak and Lin (2006)" startWordPosition="3758" endWordPosition="3761">weighed against the increased time required to use the model. Figures 3 and 4 give the MRR results and precision-recall curves for our additional models in comparison with that of the correctness model. Although losses in MRR and precision are observed for both the appropriate and combined model using the RBF kernel, the linear kernel versions of these models show slight performance gains. 670 Figure 2: Precision-recall of appropriate candidates under the correctness model Figure 1: MRR results for the correctness model First Correct Answer First Appropriate Candidate Random Snippet Frequency Pinchak and Lin (2006) Linear Kernel RBF Kernel 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Mean Reciprocal Rank (MRR) 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 RBF Kernel Linear Kernel Pinchak &amp; Lin (2006) Snippet Frequency Random 5 Discussion of Results The results of our correctness model, found in Figures 1 and 2 show considerable gains over our alternative systems, including that of Pinchak and Lin (2006). The Pinchak and Lin (2006) system was specifically designed with answer typing in mind, although it makes use of a brittle generative model that does not ac</context>
<context position="28052" citStr="Pinchak and Lin (2006)" startWordPosition="4708" endWordPosition="4711">rained set of classes. The approach of Li and Roth (2002) is similar in that it uses learning for answer type detection. They make use of multi-class learning with a Sparse Network of Winnows (SNoW) and a two-layer class hierarchy comprising a total of fifty possible answer types. These finer-grained classes are of more use when computing a notion of appropriateness, although one major drawback is that no entity tagger is discussed that can identify these types in text. Li and Roth (2002) also rely on a rigid set of classes and so run the risk of encountering a new question of an unseen type. Pinchak and Lin (2006) present an alternative in which the probability of a term being appropriate to a question is computed directly. Instead of assigning an answer type to a question, the question is broken down into a number of possibly overlapping contexts. A candidate is then evaluated as to how likely it is to appear in these contexts. Unfortunately, Pinchak and Lin (2006) use a brittle generative model when combining question contexts that assumes all contexts are equally important. This assumption was dealt with by Pinchak and Lin (2006) by discarding all non-focus contexts with a focus context is present, </context>
<context position="22610" citStr="Pinchak &amp; Lin (2006)" startWordPosition="3809" endWordPosition="3812">on are observed for both the appropriate and combined model using the RBF kernel, the linear kernel versions of these models show slight performance gains. 670 Figure 2: Precision-recall of appropriate candidates under the correctness model Figure 1: MRR results for the correctness model First Correct Answer First Appropriate Candidate Random Snippet Frequency Pinchak and Lin (2006) Linear Kernel RBF Kernel 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Mean Reciprocal Rank (MRR) 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 RBF Kernel Linear Kernel Pinchak &amp; Lin (2006) Snippet Frequency Random 5 Discussion of Results The results of our correctness model, found in Figures 1 and 2 show considerable gains over our alternative systems, including that of Pinchak and Lin (2006). The Pinchak and Lin (2006) system was specifically designed with answer typing in mind, although it makes use of a brittle generative model that does not account for ranking of answer candidates nor for the variable strength of various question contexts. These results show that our discriminative preference ranking approach creates a better model of both correctness and appropriateness vi</context>
</contexts>
<marker>Pinchak, Lin, 2006</marker>
<rawString>C. Pinchak and D. Lin. 2006. A Probabilistic Answer Type Model. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006), Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>C Welty</author>
<author>A Ittycheriah</author>
<author>R Mahindru</author>
</authors>
<title>IBM’s PIQUANT in TREC2003.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC-2003),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="29245" citStr="Prager et al., 2003" startWordPosition="4909" endWordPosition="4912"> focus context is present, but this is not an ideal solution. Learning methods are abundant in QA research 672 Figure 5: Learning curve for MRR of the first correct answer under the correctness model and have been applied in a number of different ways. Ittycheriah et al. (2000) created an entire QA system based on maximum entropy components in addition to the question classifier discussed above. Ittycheriah et al. (2000) were able to obtain reasonable performance from learned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techniques are combined, also show promise for improving QA performance. Echihabi et al. (2003) use three separate answer extraction agents and combine the output scores with a maximum entropy re-ranker. Surdeanu et al. (2008) explore preference ranking for advice or “how to” questions in which a unique correct answer is preferred over all other </context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, Welty, Ittycheriah, Mahindru, 2003</marker>
<rawString>J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Ittycheriah, and R. Mahindru. 2003. IBM’s PIQUANT in TREC2003. In Proceedings of the Twelfth Text REtrieval Conference (TREC-2003), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sun</author>
<author>J Jiang</author>
<author>Y F Tan</author>
<author>H Cui</author>
<author>T-S Chua</author>
<author>M-Y Kan</author>
</authors>
<title>Using Syntactic and Semantic Relation Analysis in Question Answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference (TREC2005),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="26620" citStr="Sun et al., 2005" startWordPosition="4460" endWordPosition="4463">tional questions simply repeat the same information. Requiring only a relatively small number of training examples means that an effective model can be learned with relatively little input in the form of question-answer pairs or annotated candidate lists. 6 Prior Work The expected answer type can be captured in a number of possible ways. By far the most common is the assignment of one or more predefined types to a question during a question analysis phase. Although the vast majority of the approaches to answer type detection make use of rules (either partly or wholly) (Harabagiu et al., 2005; Sun et al., 2005; Wu et al., 2005; Moll´a and Gardiner, 2004), a few notable learned methods for answer type detection exist. One of the first attempts at learning a model for answer type detection was made by Ittycheriah et al. (2000; 2001) who learn a maximum entropy classifier over the Message Understanding Conference (MUC) types. Those same MUC types are then assigned by a named-entity tagger to identify appropriate candidate answers. Because of the potential for unanticipated types, Ittycheriah et al. (2000; 2001) include a Phrase type as a catch-all class that is used when no other class is appropriate.</context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2005</marker>
<rawString>R. Sun, J. Jiang, Y.F. Tan, H. Cui, T-S. Chua, and M-Y. Kan. 2005. Using Syntactic and Semantic Relation Analysis in Question Answering. In Proceedings of the Fourteenth Text REtrieval Conference (TREC2005), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>M Ciaramita</author>
<author>H Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>719--727</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="29723" citStr="Surdeanu et al. (2008)" startWordPosition="4984" endWordPosition="4987">rned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techniques are combined, also show promise for improving QA performance. Echihabi et al. (2003) use three separate answer extraction agents and combine the output scores with a maximum entropy re-ranker. Surdeanu et al. (2008) explore preference ranking for advice or “how to” questions in which a unique correct answer is preferred over all other candidates. Their focus is on complex-answer questions in addition to the use of a collection of user-generated answers rather than answer typing. However, their use of preference ranking mirrors the techniques we describe here in which the relative difference between two candidates at different ranks is more important than the individual candidates. 7 Conclusions and Future Work We have introduced a means of flexible answer typing with discriminative preference rank learni</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 719–727, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of TREC 2002,</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="15181" citStr="Voorhees, 2002" startWordPosition="2608" endWordPosition="2609">re extensions to the Pinchak and Lin (2006) model and can be incorporated into the Pinchak and Lin (2006) model with varying degrees of difficulty. The value of 5(t) in particular is highly dependent on the means used to obtain the candidate list, and the distribution of words over the candidate list is often very different from the distribution of words in the corpus. Because this feature value comes from a different source than our other features, it would be difficult to use in a non-discriminative model. Correct answers to our set of questions are obtained from the TREC 2002-2006 results (Voorhees, 2002). For appropriateness labels we turn to human annotators. Two annotators were instructed to label a candidate as appropriate if that candidate was believable as an answer, even if that candidate was not correct. For a question such as “What city hosted the 1988 Winter Olympics?”, all cities should be labeled as appropriate even though only Calgary is correct. This task comes with a moderate degree of difficulty, especially when dealing with questions for which appropriate answers are less obvious (such as “What kind of a community is a Kibbutz?”). We observed an interannotator (kappa) agreemen</context>
<context position="19131" citStr="Voorhees, 2002" startWordPosition="3222" endWordPosition="3223"> compare with the prior approach of Pinchak and Lin (2006), we use a set of what and which questions with question focus (questions with a noun phrase following the wh-word). These are a subset of the more general what, which, and who questions dealt with by Pinchak and Lin (2006). Although our model can accommodate a wide range of what, which, when, and who questions, the focused what and which questions are an easily identifiable subclass that are rarely definitional or otherwise complex in terms of the desired answer. We take the set of focused what and which questions from TREC 2002-2006 (Voorhees, 2002) comprising a total of 385 questions and performed 9-fold cross-validation, with one dedicated development partition (the tenth partition). The development partition was used to tune the regularization parameter of the SVM used for testing. Candidates are obtained by submitting the question as-is to the Google search engine and chunking the top 20 snippets returned, resulting in an average of 140 candidates per question. Google snippets create a better confusion set than simply random words for appropriate and inappropriate candidates; many of the terms found in Google snippets are related in </context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E.M. Voorhees. 2002. Overview of the TREC 2002 Question Answering Track. In Proceedings of TREC 2002, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wu</author>
<author>M Duan</author>
<author>S Shaikh</author>
<author>S Small</author>
<author>T Strzalkowski</author>
</authors>
<title>ILQUA – An IE-Driven Question Answering System.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005),</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="26637" citStr="Wu et al., 2005" startWordPosition="4464" endWordPosition="4467">imply repeat the same information. Requiring only a relatively small number of training examples means that an effective model can be learned with relatively little input in the form of question-answer pairs or annotated candidate lists. 6 Prior Work The expected answer type can be captured in a number of possible ways. By far the most common is the assignment of one or more predefined types to a question during a question analysis phase. Although the vast majority of the approaches to answer type detection make use of rules (either partly or wholly) (Harabagiu et al., 2005; Sun et al., 2005; Wu et al., 2005; Moll´a and Gardiner, 2004), a few notable learned methods for answer type detection exist. One of the first attempts at learning a model for answer type detection was made by Ittycheriah et al. (2000; 2001) who learn a maximum entropy classifier over the Message Understanding Conference (MUC) types. Those same MUC types are then assigned by a named-entity tagger to identify appropriate candidate answers. Because of the potential for unanticipated types, Ittycheriah et al. (2000; 2001) include a Phrase type as a catch-all class that is used when no other class is appropriate. Although the cla</context>
</contexts>
<marker>Wu, Duan, Shaikh, Small, Strzalkowski, 2005</marker>
<rawString>M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strzalkowski. 2005. ILQUA – An IE-Driven Question Answering System. In Proceedings of the Fourteenth Text REtrieval Conference (TREC-2005), Gaithersburg, Maryland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>