<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.911128">
Batch Tuning Strategies for Statistical Machine Translation
</title>
<author confidence="0.954094">
Colin Cherry and George Foster
</author>
<affiliation confidence="0.947004">
National Research Council Canada
</affiliation>
<email confidence="0.429571">
{Colin.Cherry,George.Foster}@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.990194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999703571428571">
There has been a proliferation of recent work
on SMT tuning algorithms capable of han-
dling larger feature sets than the traditional
MERT approach. We analyze a number of
these algorithms in terms of their sentence-
level loss functions, which motivates several
new approaches, including a Structured SVM.
We perform empirical comparisons of eight
different tuning strategies, including MERT,
in a variety of settings. Among other results,
we find that a simple and efficient batch ver-
sion of MIRA performs at least as well as
training online, and consistently outperforms
other options.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954411764706">
The availability of linear models and discriminative
tuning algorithms has been a huge boon to statis-
tical machine translation (SMT), allowing the field
to move beyond the constraints of generative noisy
channels (Och and Ney, 2002). The ability to opti-
mize these models according to an error metric has
become a standard assumption in SMT, due to the
wide-spread adoption of Minimum Error Rate Train-
ing or MERT (Och, 2003). However, MERT has
trouble scaling to more than 30 features, which has
led to a surge in research on tuning schemes that can
handle high-dimensional feature spaces.
These methods fall into a number of broad cate-
gories. Minimum risk approaches (Och, 2003; Smith
and Eisner, 2006) have been quietly capable of han-
dling many features for some time, but have yet to
see widespread adoption. Online methods (Liang
et al., 2006; Watanabe et al., 2007), are recognized
to be effective, but require substantial implementa-
tion efforts due to difficulties with parallelization.
Pairwise ranking (Shen et al., 2004; Hopkins and
May, 2011) recasts tuning as classification, and can
be very easy to implement, as it fits nicely into the
established MERT infrastructure.
The MERT algorithm optimizes linear weights
relative to a collection of k-best lists or lattices,
which provide an approximation to the true search
space. This optimization is wrapped in an outer
loop that iterates between optimizing weights and
re-decoding with those weights to enhance the ap-
proximation. Our primary contribution is to empiri-
cally compare eight tuning algorithms and variants,
focusing on methods that work within MERT’s es-
tablished outer loop. This is the first comparison to
include all three categories of optimizer.
Furthermore, we introduce three tuners that have
not been previously tested. In particular, we
test variants of Chiang et al.’s (2008) hope-fear
MIRA that use k-best or lattice-approximated search
spaces, producing a Batch MIRA that outperforms
a popular mechanism for parallelizing online learn-
ers. We also investigate the direct optimization of
hinge loss on k-best lists, through the use of a Struc-
tured SVM (Tsochantaridis et al., 2004). We review
and organize the existing tuning literature, provid-
ing sentence-level loss functions for minimum risk,
online and pairwise training. Finally, since random-
ization plays a different role in each tuner, we also
suggest a new method for testing an optimizer’s sta-
bility (Clark et al., 2011), which sub-samples the
tuning set instead of varying a random seed.
</bodyText>
<sectionHeader confidence="0.996997" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999898">
We begin by establishing some notation. We view
our training set as a list of triples [f, R, £]Z1, where
f is a source-language sentence, R is a set of target-
language reference sentences, and £ is the set of
</bodyText>
<page confidence="0.655401333333333">
427
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999484142857143">
all reachable hypotheses; that is, each e ∈ Ei is a
target-language derivation that can be decoded from
fi. The function ~hi(e) describes e’s relationship to
its source fi using features that decompose into the
decoder. A linear model w~ scores derivations ac-
cording to their features, meaning that the decoder
solves:
</bodyText>
<equation confidence="0.9712">
ei(~w) = arg max w~ · ~hi(e) (1)
eE£i
</equation>
<bodyText confidence="0.999867727272727">
Assuming we wish to optimize our decoder’s BLEU
score (Papineni et al., 2002), the natural objec-
tive of learning would be to find a w~ such that
BLEU([e(~w), R]n1) is maximal. In most machine
learning papers, this would be the point where we
would say, “unfortunately, this objective is unfeasi-
ble.” But in SMT, we have been happily optimizing
exactly this objective for years using MERT.
However, it is now acknowledged that the MERT
approach is not feasible for more than 30 or so fea-
tures. This is due to two main factors:
</bodyText>
<listItem confidence="0.626541625">
1. MERT’s parameter search slows and becomes
less effective as the number of features rises,
stopping it from finding good training scores.
2. BLEU is a scale invariant objective: one can
scale w~ by any positive constant and receive the
same BLEU score.1 This causes MERT to re-
sist standard mechanisms of regularization that
aim to keep ||~w ||small.
</listItem>
<bodyText confidence="0.9385799375">
The problems with MERT can be addressed
through the use of surrogate loss functions. In this
paper, we focus on linear losses that decompose over
training examples. Using Ri and Ei, each loss `i(~w)
indicates how poorly w~ performs on the i�h training
example. This requires a sentence-level approxima-
tion of BLEU, which we re-encode into a cost Δi(e)
on derivations, where a high cost indicates that e re-
ceives a low BLEU score. Unless otherwise stated,
we will assume the use of sentence BLEU with add-
1 smoothing (Lin and Och, 2004). The learners dif-
fer in their definition of ` and Δ, and in how they
employ their loss functions to tune their weights.
1This is true of any evaluation metric that considers only the
ranking of hypotheses and not their model scores; ie, it is true
of all common MT metrics.
</bodyText>
<subsectionHeader confidence="0.988042">
2.1 Margin Infused Relaxed Algorithm
</subsectionHeader>
<bodyText confidence="0.9995435">
First employed in SMT by Watanabe et al. (2007),
and refined by Chiang et al. (2008; 2009), the Mar-
gin Infused Relaxed Algorithm (MIRA) employs a
structured hinge loss:
</bodyText>
<equation confidence="0.910232">
`i(~w) = ma£x IΔi(e) + w~ · (hi(e) − hi(ez )eEi
)J
</equation>
<bodyText confidence="0.9998068125">
where ez is an oracle derivation, and cost is de-
fined as Δi(e) = BLEUi(ez) − BLEUi(e), so that
Δi(ez) = 0. The loss `i(~w) is 0 only if w~ separates
each e ∈ Ei from ez by a margin proportional to their
BLEU differentials.
MIRA is an instance of online learning, repeating
the following steps: visit an example i, decode ac-
cording to ~w, and update w~ to reduce `i(~w). Each
update makes the smallest change to w~ (subject to a
step-size cap C) that will separate the oracle from a
number of negative hypotheses. The work of Cram-
mer et al. (2006) shows that updating away from a
single “fear” hypothesis that maximizes (2) admits
a closed-form update that performs well. Let ez be
the e ∈ Ei that maximizes `i(~w); the update can be
performed in two steps:
</bodyText>
<equation confidence="0.999312">
ηt = min �C ~ `i(~wt) J i
)�~hi(ed)��2 i)�
~wt+1 = ~wt + ηt �~hi(e� i ) − ~hi(e�
</equation>
<bodyText confidence="0.999817315789474">
To improve generalization, the average of all
weights seen during learning is used on unseen data.
Chiang et al. (2008) take advantage of MIRA’s
online nature to modify each update to better suit
SMT. The cost Δi is defined using a pseudo-
corpus BLEU that tracks the n-gram statistics of
the model-best derivations from the last few up-
dates. This modified cost matches corpus BLEU
better than add-1 smoothing, but it also makes Δi
time-dependent: each update for an example i will
be in the context of a different pseudo-corpus. The
oracle ez also shifts with each update to ~w, as it
is defined as a “hope” derivation, which maximizes
w~ · ~hi(e) + BLEUi(e). Hope updating ensures that
MIRA aims for ambitious, reachable derivations.
In our implementation, we make a number of
small, empirically verified deviations from Chiang
et al. (2008). These include the above-mentioned
use of a single hope and fear hypothesis, and the use
</bodyText>
<page confidence="0.998131">
428
</page>
<bodyText confidence="0.9999654">
of hope hypotheses (as opposed to model-best hy-
potheses) to build the pseudo-corpus for calculating
BLEUi. These changes were observed to be neu-
tral with respect to translation quality, but resulted in
faster running time and simplified implementation.
</bodyText>
<subsectionHeader confidence="0.99826">
2.2 Direct Optimization
</subsectionHeader>
<bodyText confidence="0.999948315789474">
With the exception of MIRA, the tuning approaches
discussed in this paper are direct optimizers. That is,
each solves the following optimization problem:
where the first term provides regularization,
weighted by λ. Throughout this paper, (4) is
optimized with respect to a fixed approximation
of the decoder’s true search space, represented as
a collection of k-best lists. The various methods
differ in their definition of loss and in how they
optimize their objective.
Without the complications added by hope decod-
ing and a time-dependent cost function, unmodified
MIRA can be shown to be carrying out dual coordi-
nate descent for an SVM training objective (Martins
et al., 2010). However, exactly what objective hope-
fear MIRA is optimizing remains an open question.
Gimpel and Smith (2012) discuss these issues in
greater detail, while also providing an interpretable
alternative to MIRA.
</bodyText>
<subsectionHeader confidence="0.999137">
2.3 Pairwise Ranking Optimization
</subsectionHeader>
<bodyText confidence="0.998817642857143">
Introduced by Hopkins and May (2011), Pairwise
Ranking Optimization (PRO) aims to handle large
feature sets inside the traditional MERT architec-
ture. That is, PRO employs a growing approxima-
tion of £i by aggregating the k-best hypotheses from
a series of increasingly refined models. This archi-
tecture is desirable, as most groups have infrastruc-
ture to k-best decode their tuning sets in parallel.
For a given approximate �£i, PRO creates a sam-
ple Si of (eg, eb) pairs, such that BLEUi(eg) &gt;
BLEUi(eb). It then uses a binary classifier to sep-
arate each pair. We describe the resulting loss in
terms of an SVM classifier, to highlight similarities
with MIRA. In terms of (4), PRO defines
</bodyText>
<equation confidence="0.9655745">
`i(~w) = � 2 �1 + w~ · �~hi(eb) − ~hi(eg)��+
(eg,eb)ESi
</equation>
<bodyText confidence="0.99997405882353">
where (x)+ = max(0, x). The hinge loss is multi-
plied by 2 to account for PRO’s use of two examples
(positive and negative) for each sampled pair. This
sum of hinge-losses is 0 only if each pair is separated
by a model score of 1. Given [S]Z1, this convex
objective can be optimized using any binary SVM.2
Unlike MIRA, the margin here is fixed to 1; cost en-
ters into PRO through its sampling routine, which
performs a large uniform sample and then selects a
subset of pairs with large BLEU differentials.
The PRO loss uses a sum over pairs in place of
MIRA’s max, which allows PRO to bypass oracle
selection, and to optimize with off-the-shelf classi-
fiers. This sum is potentially a weakness, as PRO
receives credit for each correctly ordered pair in its
sample, and these pairs are not equally relevant to
the final BLEU score.
</bodyText>
<subsectionHeader confidence="0.982803">
2.4 Minimum Risk Training
</subsectionHeader>
<bodyText confidence="0.9990111">
Minimum risk training (MR) interprets w~ as a prob-
abilistic model, and optimizes expected BLEU. We
focus on expected sentence costs (Och, 2003; Zens
et al., 2007; Li and Eisner, 2009), as this risk is sim-
ple to optimize and fits nicely into our mathemati-
cal framework. Variants that use the expected suffi-
cient statistics of BLEU also exist (Smith and Eisner,
2006; Pauls et al., 2009; Rosti et al., 2011).
We again assume a MERT-like tuning architec-
ture. Let Ai(e) = −BLEUi(e) and let
</bodyText>
<equation confidence="0.9962645">
`i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)]
Ee/E£i exp(~w · ~hi(e&apos;))
</equation>
<bodyText confidence="0.999954888888889">
This expected cost becomes increasingly small as
greater probability mass is placed on derivations
with high BLEU scores. This smooth, non-convex
objective can be solved to a local minimum using
gradient-based optimizers; we have found stochastic
gradient descent to be quite effective (Bottou, 2010).
Like PRO, MR requires no oracle derivation, and
fits nicely into the established MERT architecture.
The expectations needed to calculate the gradient
</bodyText>
<equation confidence="0.960033">
[ ] [ ]
EP~w ~hi(e)Ai(e) − EP~w [Ai(e)] EP~w ~hi(e)
</equation>
<bodyText confidence="0.483452">
2Hopkins and May (2011) advocate a maximum-entropy
version of PRO, which is what we evaluate in our empirical
comparison. It can be obtained using a logit loss fi(97) _
</bodyText>
<equation confidence="0.9925164">
E � �79 · �9�i(eb) − 9�i(eg))//
g,b 2 log 1 + exp .
~w* = arg min λ � `i(~w) (4)
w 2 ||~w||2 +
i
</equation>
<page confidence="0.989296">
429
</page>
<bodyText confidence="0.99990325">
are trivial to extract from a k-best list of derivations.
Each downward step along this gradient moves the
model toward likely derivations, and away from
likely derivations that incur high costs.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="method">
3 Novel Methods
</sectionHeader>
<bodyText confidence="0.9999325">
We have reviewed three tuning methods, all of which
address MERT’s weakness with large features by us-
ing surrogate loss functions. Additionally, MIRA
has the following advantages over PRO and MR:
</bodyText>
<listItem confidence="0.93521625">
1. Loss is optimized using the true £i, as opposed
to an approximate search space �£i.
2. Sentence BLEU is calculated with a fluid
pseudo-corpus, instead of add-1 smoothing.
</listItem>
<bodyText confidence="0.9978886">
Both of these advantages come at a cost: oper-
ating on the true £i sacrifices easy parallelization,
while using a fluid pseudo-corpus creates an unsta-
ble learning objective. We develop two large-margin
tuners that explore these trade-offs.
</bodyText>
<subsectionHeader confidence="0.999056">
3.1 Batch MIRA
</subsectionHeader>
<bodyText confidence="0.99996748">
Online training makes it possible to learn with the
decoder in the loop, forgoing the need to approxi-
mate the search space, but it is not necessarily con-
venient to do so. Online algorithms are notoriously
difficult to parallelize, as they assume each example
is visited in sequence. Parallelization is important
for efficient SMT tuning, as decoding is still rela-
tively expensive.
The parallel online updates suggested by Chi-
ang et al. (2008) involve substantial inter-process
communication, which may not be easily supported
by all clusters. McDonald et al. (2010) suggest
a simpler distributed strategy that is amenable to
map-reduce-like frameworks, which interleaves on-
line training on shards with weight averaging across
shards. This strategy has been adopted by Moses
(Hasler et al., 2011), and it is the one we adopt in
our MIRA implementation.
However, online training using the decoder may
not be necessary for good performance. The success
of MERT, PRO and MR indicates that their shared
search approximation is actually quite reasonable.
Therefore, we propose Batch MIRA, which sits ex-
actly where MERT sits in the standard tuning archi-
tecture, greatly simplifying parallelization:
</bodyText>
<listItem confidence="0.9987385">
1. Parallel Decode: [�£&apos;]n1 = k-best([f, £]n1, w—)
2. Aggregate: [ �£]n1 = [�£]n1 U [�£l]n1
3. Train: w� = BatchMIRA([f, R,
4. Repeat
</listItem>
<bodyText confidence="0.999969807692308">
where BatchMIRA() trains the SMT-adapted MIRA
algorithm to completion on the current approxima-
tion £, without parallelization.3 The only change we
make to MIRA is to replace the hope-fear decoding
of sentences with the hope-fear re-ranking of k-best
lists. Despite its lack of parallelization, each call to
BatchMIRA() is extremely fast, as SMT tuning sets
are small enough to load all of [£]n 1 into memory. We
test two Batch MIRA variants, which differ in their
representation of £. Pseudo-code that covers both is
provided in Algorithm 1. Note that if we set £ = £,
Algorithm 1 also describes online MIRA.
Batch k-best MIRA inherits all of the MERT archi-
tecture. It is very easy to implement; the hope-fear
decoding steps can by carried out by simply evaluat-
ing BLEU score and model score for each hypothe-
sis in the k-best list.
Batch Lattice MIRA replaces k-best decoding in
step 1 with decoding to lattices. To enable loading
all of the lattices into memory at once, we prune to
a density of 50 edges per reference word. The hope-
fear decoding step requires the same oracle lattice
decoding algorithms as online MIRA (Chiang et al.,
2008). The lattice aggregation in the outer loop can
be kept reasonable by aggregating only those paths
corresponding to hope or fear derivations.
</bodyText>
<subsectionHeader confidence="0.996614">
3.2 Structured SVM
</subsectionHeader>
<bodyText confidence="0.99974975">
While MIRA takes a series of local hinge-loss re-
ducing steps, it is also possible to directly minimize
the sum of hinge-losses using a batch algorithm, cre-
ating a structured SVM (Tsochantaridis et al., 2004).
To avoid fixing an oracle before optimization begins,
we adapt Yu and Joachim’s (2009) latent SVM to
our task, which allows the oracle derivation for each
sentence to vary during training. Again we assume a
MERT-like architecture, which approximates £ with
an £ constructed from aggregated k-best lists.
Inspired by the local oracle of Liang et al. (2006),
we define �£i* to be an oracle set:
</bodyText>
<equation confidence="0.9697752">
�£i* = {e|BLEUi(e) is maximal}.
3In our implementation, BatchMIRA() trains for J = 30
passes over � ���� .
£]n
1,�w)
</equation>
<page confidence="0.913009">
430
</page>
<bodyText confidence="0.698107454545455">
Algorithm 1 BatchMIRA
input [f, R, �£]n1, ~w, max epochs J, step cap C,
and pseudo-corpus decay γ.
init Pseudo-corpus BG to small positive counts.
init t = 1; ~wt = w~
for j from 1 to J do
for i from 1 to n in random order do
// Hope-fear decode in �£i
et = arg maxeE £i [~wt ~hi(e) + BLEUi(e)]
et = arg maxeE £i [~wt ~hi(e) − BLEUi(e)]
// Update weights
</bodyText>
<equation confidence="0.885353826086956">
At = BLEUi(et) − BLEUi(et)
min I
C At+~wt·(~hi(e0t)−~hi(et)&amp;quot; J
ηt = , ||~hi(e∗t )−~hi(e0t)||2
i)~
~wt+1 = ~wt + ηt �~hi(e� t ) − ~hi(e�
// Update statistics
BG = γBG+ BLEU stats for et and Ri
t = t + 1
end for Pnj
~wavg 1 t0=1 ~wt0
j = nj
end for
return ~wavg
j that maximizes training BLEU
Cost is also defined in terms of the maximal BLEU,
Ai(e) = max [BLEUi(e&apos;)] − BLEUi(e).
e0E�£i
Finally, loss is defined as:
�Ai(e) + w~ ~hi(e)
`i(~w) = maxeE �£i �~w ~hi(e� i )��
− maxe∗E�£
i i
</equation>
<bodyText confidence="0.998975974358974">
This loss is 0 only if some hypothesis in the oracle
set is separated from all others by a margin propor-
tional to their BLEUi differentials.
With loss defined in this manner, we can mini-
mize (4) to local minimum by using an alternating
training procedure. For each example i, we select
a fixed ez E �£i* that maximizes model score; that
is, w~ is used to break ties in BLEU for oracle selec-
tion. With the oracle fixed, the objective becomes
a standard structured SVM objective, which can be
minimized using a cutting-plane algorithm, as de-
scribed by Tsochantaridis et al. (2004). After doing
so, we can drive the loss lower still by iterating this
process: re-select each oracle (breaking ties with the
new ~w), then re-optimize ~w. We do so 10 times. We
were surprised by the impact of these additional iter-
ations on the final loss; for some sentences, �£i* can
be quite large.
Despite the fact that both algorithms use a struc-
tured hinge loss, there are several differences be-
tween our SVM and MIRA. The SVM has an ex-
plicit regularization term λ that is factored into its
global objective, while MIRA regularizes implicitly
by taking small steps. The SVM requires a stable
objective to optimize, meaning that it must forgo the
pseudo-corpus used by MIRA to calculate Ai; in-
stead, the SVM uses an interpolated sentence-level
BLEU (Liang et al., 2006).4 Finally, MIRA’s oracle
is selected with hope decoding. With a sufficiently
large ~w, any e E £ can potentially become the ora-
cle. In contrast, the SVM’s local oracle is selected
from a small set £*, which was done to more closely
match the assumptions of the Latent SVM.
To solve the necessary quadratic programming
sub-problems, we use a multiclass SVM similar to
LIBLINEAR (Hsieh et al., 2008). Like Batch MIRA
and PRO, the actual optimization is very fast, as the
cutting plane converges quickly and all of [�£]n1 can
be loaded into memory at once.
</bodyText>
<subsectionHeader confidence="0.998201">
3.3 Qualitative Summary
</subsectionHeader>
<bodyText confidence="0.999981866666667">
We have reviewed three tuning methods and intro-
duced three tuning methods. All six methods em-
ploy sentence-level loss functions, which in turn em-
ploy sentence-level BLEU approximations. Except
for online MIRA, all methods plug nicely into the
existing MERT architecture. These methods can be
split into two groups: MIRA variants (online, batch
k-best, batch lattice), and direct optimizers (PRO,
MR and SVM). The MIRA variants use pseudo-
corpus BLEU in place of smoothed BLEU, and
provide access to richer hypothesis spaces through
the use of online training or lattices.5 The direct
optimizers have access to a tunable regularization
parameter λ, and do not require special purpose
code for hope and fear lattice decoding. Batch
</bodyText>
<footnote confidence="0.984494571428571">
4SVM training with interpolated BLEU outperformed add-1
BLEU in preliminary testing. A comparison of different BLEU
approximations under different tuning objectives would be an
interesting path for future work.
5MR approaches that use lattices (Li and Eisner, 2009;
Pauls et al., 2009; Rosti et al., 2011) or the complete search
space (Arun et al., 2010) exist, but are not tested here.
</footnote>
<page confidence="0.998827">
431
</page>
<bodyText confidence="0.999931333333333">
k-best MIRA straddles the two groups, benefiting
from pseudo-corpus BLEU and easy implementa-
tion, while being restricted to a k-best list.
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.999965">
We evaluated the six tuning strategies described
in this paper, along with two MERT baselines,
on three language pairs (French-English (Fr-En),
English-French (En-Fr) and Chinese-English (Zh-
En)), across three different feature-set sizes. Each
setting was run five times over randomized variants
to improve reliability. To cope with the resulting
large number of configurations, we ran all experi-
ments using an efficient phrase-based decoder simi-
lar to Moses (Koehn et al., 2007).
All tuning methods that use an approximate £ per-
form 15 iterations of the outer loop and return the
weights that achieve the best development BLEU
score. When present, A was coarsely tuned (trying 3
values differing by magnitudes of 10) in our large-
feature Chinese-English setting.
</bodyText>
<listItem confidence="0.986808571428571">
• kb-mert : k-best MERT with 20 random
restarts. All k-best methods use k = 100.
• lb-mert : Lattice MERT (Machery et al., 2008)
using unpruned lattices and aggregating only
those paths on the line search’s upper envelope.
• mira : Online MIRA (§2.1). All MIRA vari-
ants use a pseudo-corpus decay -y = 0.999 and
</listItem>
<bodyText confidence="0.908277666666667">
C = 0.01. Online parallelization follows Mc-
Donald et al. (2010), using 8 shards. We tested
20, 15, 10, 8 and 5 shards during development.
</bodyText>
<listItem confidence="0.99411825">
• lb-mira : Batch Lattice MIRA (§3.1).
• kb-mira : Batch k-best MIRA (§3.1).
• pro : PRO (§2.3) follows Hopkins and May
(2011); however, we were unable to find set-
</listItem>
<bodyText confidence="0.964070625">
tings that performed well in general. Reported
results use MegaM6 with a maximum of 30 it-
erations (as is done in Moses; the early stop-
ping provides a form of regularization) for our
six English/French tests, and MegaM with 100
iterations and a reduced initial uniform sam-
ple (50 pairs instead of 5000) for our three En-
glish/Chinese tests.
</bodyText>
<listItem confidence="0.931916">
• mr : MR as described in §2.4. We employ a
learning rate of �0/(1 + goAt) for stochastic
</listItem>
<footnote confidence="0.982871">
6Available at www.cs.utah.edu/∼hal/megam/
</footnote>
<table confidence="0.9990088">
corpus sentences words (en) words (fr)
train 2,928,759 60,482,232 68,578,459
dev 2,002 40,094 44,603
test1 2,148 42,503 48,064
test2 2,166 44,701 49,986
</table>
<tableCaption confidence="0.983834">
Table 1: Hansard Corpus (English/French)
</tableCaption>
<table confidence="0.999162285714286">
corpus sentences words (zh) words (en)
train1 6,677,729 200,706,469 213,175,586
train2 3,378,230 69,232,098 66,510,420
dev 1,506 38,233 40,260
nist04 1,788 53,439 59,944
nist06 1,664 41,782 46,140
nist08 1,357 35,369 42,039
</table>
<tableCaption confidence="0.938917333333333">
Table 2: NIST09 Corpus (Chinese-English). Train1 cor-
responds to the UN and Hong Kong sub-corpora; train2
to all others.
</tableCaption>
<bodyText confidence="0.99956">
gradient descent, with qo tuned to optimize the
training loss achieved after one epoch (Bottou,
2010). Upon reaching a local optimum, we re-
shuffle our data, re-tune our learning rate, and
re-start from the optimum, repeating this pro-
cess 5 times. We do not sharpen our distribu-
tion with a temperature or otherwise control for
entropy; instead, we trust A = 50 to maintain a
reasonable distribution.
</bodyText>
<listItem confidence="0.988259">
• svm : Structured SVM (§3.2) with A = 1000.
</listItem>
<subsectionHeader confidence="0.985857">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999948545454545">
Systems for English/French were trained on Cana-
dian Hansard data (years 2001–2009) summarized
in table 1.7 The dev and test sets were chosen
randomly from among the most recent 5 days of
Hansard transcripts.
The system for Zh-En was trained on data from
the NIST 2009 Chinese MT evaluation, summarized
in table 2. The dev set was taken from the NIST
05 evaluation set, augmented with some material re-
served from other NIST corpora. The NIST 04, 06,
and 08 evaluation sets were used for testing.
</bodyText>
<subsectionHeader confidence="0.987799">
4.2 SMT Features
</subsectionHeader>
<bodyText confidence="0.991734">
For all language pairs, phrases were extracted with
a length limit of 7 from separate word alignments
</bodyText>
<footnote confidence="0.859913">
7This corpus will be distributed on request.
</footnote>
<page confidence="0.993266">
432
</page>
<table confidence="0.997476166666666">
template max fren enfr zhen
tgt unal 50 50 50 31
count bin 11 11 11 11
word pair 6724 1298 1291 1664
length bin 63 63 63 63
total 6848 1422 1415 1769
</table>
<tableCaption confidence="0.999922">
Table 3: Sparse feature templates used in Big.
</tableCaption>
<bodyText confidence="0.999892244897959">
performed by IBM2 and HMM models and sym-
metrized using diag-and (Koehn et al., 2003). Con-
ditional phrase probabilities in both directions were
estimated from relative frequencies, and from lexical
probabilities (Zens and Ney, 2004). Language mod-
els were estimated with Kneser-Ney smoothing us-
ing SRILM. Six-feature lexicalized distortion mod-
els were estimated and applied as in Moses.
For each language pair, we defined roughly equiv-
alent systems (exactly equivalent for En-Fr and Fr-
En, which are mirror images) for each of three
nested feature sets: Small, Medium, and Big.
The Small set defines a minimal 7-feature sys-
tem intended to be within easy reach of all tuning
strategies. It comprises 4 TM features, one LM, and
length and distortion features. For the Chinese sys-
tem, the LM is a 5-gram trained on the NIST09 Gi-
gaword corpus; for English/French, it is a 4-gram
trained on the target half of the parallel Hansard.
The Medium set is a more competitive 18-feature
system. It adds 4 TM features, one LM, and 6 lex-
icalized distortion features. For Zh-En, Small’s TM
(trained on both train1 and train2 in table 2) is re-
placed by 2 separate TMs from these sub-corpora;
for En/Fr, the extra TM (4 features) comes from a
forced-decoding alignment of the training corpus, as
proposed by Wuebker et al. (2010). For Zh-En, the
extra LM is a 4-gram trained on the target half of the
parallel corpus; for En/Fr, it is a 4-gram trained on
5m sentences of similar parliamentary data.
The Big set adds sparse Boolean features to
Medium, for a maximum of 6,848 features. We used
sparse feature templates that are equivalent to the
PBMT set described in (Hopkins and May, 2011):
tgt unal picks out each of the 50 most frequent tar-
get words to appear unaligned in the phrase table;
count bin uniquely bins joint phrase pair counts with
upper bounds 1,2,4,8,16,32,64,128,1k,10k,00; word
pair fires when each of the 80 most frequent words
in each language appear aligned 1-1 to each other, to
some other word, or not 1-1; and length bin captures
each possible phrase length and length pair. Table 3
summarizes the feature templates, showing the max-
imum number of features each can generate, and the
number of features that received non-zero weights in
the final model tuned by MR for each language pair.
Feature weights are initialized to 1.0 for each of
the TM, LM and distortion penalty features. All
other weights are initialized to 0.0.
</bodyText>
<subsectionHeader confidence="0.999846">
4.3 Stability Testing
</subsectionHeader>
<bodyText confidence="0.999985727272727">
We follow Clark et al (2011), and perform multiple
randomized replications of each experiment. How-
ever, their method of using different random seeds
is not applicable in our context, since randomization
does not play the same role for all tuning methods.
Our solution was to randomly draw and fix four dif-
ferent sub-samples of each dev set, retaining each
sentence with a probability of 0.9. For each tuning
method and setting, we then optimize on the origi-
nal dev and all sub-samples. The resulting standard
deviations provide an indication of stability.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999975684210526">
The results of our survey of tuning methods can be
seen in Tables 4, 5 and 6. Results are averaged over
test sets (2 for Fr/En, 3 for Zh/En), and over 5 sub-
sampled runs per test set. The SD column reports the
standard deviation of the average test score across
the 5 sub-samples.
It may be dismaying to see only small score
improvements when transitioning from Medium to
Big. This is partially due to the fact that our Big fea-
ture set affects only phrase-table scores. Our phrase
tables are already strong, through our use of large
data or leave-one-out forced decoding. The impor-
tant baseline when assessing the utility of a method
is Medium k-best MERT. In all language pairs, our
Big systems generally outperform this baseline by
0.4 BLEU points. It is interesting to note that most
methods achieve the bulk of this improvement on the
Medium feature set.8 This indicates that MERT be-
gins to show some problems even in an 18-feature
</bodyText>
<footnote confidence="0.96353">
8One can see the same phenomenon in the results of Hop-
kins and May (2011) as well.
</footnote>
<page confidence="0.999294">
433
</page>
<tableCaption confidence="0.999396">
Table 4: French to English Translation (Fr-En)
</tableCaption>
<table confidence="0.9999366">
Tune Small SD Medium SD Tune Big SD
Test Tune Test Test
kb-mert 40.50 39.94 0.04 40.75 40.29 0.13 n/a n/a n/a
lb-mert 40.52 39.93 0.11 40.93 40.39 0.08 n/a n/a n/a
mira 40.38 39.94 0.04 40.64 40.59 0.06 41.02 40.74 0.05
kb-mira 40.46 39.97 0.05 40.92 40.64 0.12 41.46 40.75 0.08
lb-mira 40.44 39.98 0.06 40.94 40.65 0.06 41.59 40.78 0.09
pro 40.11 40.05 0.05 40.16 40.07 0.08 40.55 40.21 0.24
mr 40.24 39.88 0.05 40.70 40.57 0.14 41.18 40.60 0.08
svm 40.05 40.20 0.03 40.60 40.56 0.08 41.32 40.52 0.07
</table>
<tableCaption confidence="0.995706">
Table 5: English to French Translation (En-Fr)
</tableCaption>
<table confidence="0.9999173">
Tune Small SD Medium SD Tune Big SD
Test Tune Test Test
kb-mert 40.47 39.72 0.06 40.70 40.02 0.11 n/a n/a n/a
lb-mert 40.45 39.76 0.08 40.90 40.13 0.10 n/a n/a n/a
mira 40.36 39.83 0.03 40.78 40.44 0.02 40.89 40.45 0.05
kb-mira 40.44 39.83 0.02 40.94 40.35 0.06 41.48 40.52 0.06
lb-mira 40.45 39.83 0.02 41.05 40.45 0.04 41.65 40.59 0.07
pro 40.17 39.57 0.15 40.30 40.01 0.04 40.75 40.22 0.17
mr 40.31 39.65 0.04 40.94 40.30 0.13 41.45 40.47 0.10
svm 39.99 39.55 0.03 40.40 39.96 0.05 41.00 40.21 0.03
</table>
<tableCaption confidence="0.981588">
Table 6: Chinese to English Translation (Zh-En)
</tableCaption>
<table confidence="0.9998675">
Tune Small SD Medium SD Tune Big SD
Test Tune Test Test
kb-mert 23.97 29.65 0.06 25.74 31.58 0.42 n/a n/a n/a
lb-mert 24.18 29.48 0.15 26.42 32.39 0.22 n/a n/a n/a
mira 23.98 29.54 0.01 26.23 32.58 0.08 25.99 32.52 0.08
kb-mira 24.10 29.51 0.06 26.28 32.50 0.12 26.18 32.61 0.14
lb-mira 24.13 29.59 0.05 26.43 32.77 0.06 26.40 32.82 0.18
pro 23.25 28.74 0.24 25.80 32.42 0.20 26.49 32.18 0.40
mr 23.87 29.55 0.09 26.26 32.52 0.12 26.42 32.79 0.15
svm 23.59 28.91 0.05 26.26 32.70 0.05 27.23 33.04 0.12
</table>
<page confidence="0.8241065">
45
44
40.6
43
</page>
<figure confidence="0.957397916666667">
svm
40.4
42
40.2
41
40
40.8
40
lb-mira
svm
mr
Tune Test
</figure>
<figureCaption confidence="0.9767345">
Figure 1: French-English test of regularization with an over-fitting feature set. lb-mira varies C ={1, 1e-1, 1e-2, 1e-31, its default
C is 1e-2; svm varies A ={1e2, 1e3, 1e4, 1e51, its default A is 1e3; mr varies A ={5, 5e1, 5e2, 5e31, its default A is 5e1.
</figureCaption>
<page confidence="0.996578">
434
</page>
<bodyText confidence="0.999931378378378">
setting, which can be mitigated through the use of
Lattice MERT.
When examining score differentials, recall that
the reported scores average over multiple test sets
and sub-sampled tuning runs. Using Small features,
all of the tested methods are mostly indistinguish-
able, but as we move to Medium and Big, Batch
Lattice MIRA emerges as our method of choice. It
is the top scoring system in all Medium settings,
and in two of three Big settings (in Big Zh-En, the
SVM comes first, with batch lattice MIRA placing
second). However, all of the MIRA variants per-
form similarly, though our implementation of on-
line MIRA is an order of magnitude slower, mostly
due to its small number of shards. It is interest-
ing that our batch lattice variant consistently outper-
forms online MIRA. We attribute this to our paral-
lelization strategy, Chiang et al.’s (2008) more com-
plex solution may perform better.
There may be settings where an explicit regular-
ization parameter is desirable, thus we also make a
recommendation among the direct optimizers (PRO,
MR and SVM). Though these systems all tend to
show a fair amount of variance across language and
feature sets (likely due to their use sentence-level
BLEU), MR performs the most consistently, and is
always within 0.2 of batch lattice MIRA.
The SVM’s performance on Big Zh-En is an in-
triguing outlier in our results. Note that it not only
performs best on the test set, but also achieves the
best tuning score by a large margin. We suspect
we have simply found a setting where interpolated
BLEU and our choice of A work particularly well.
We intend to investigate this case to see if this level
of success can be replicated consistently, perhaps
through improved sentence BLEU approximation or
improved oracle selection.
</bodyText>
<subsectionHeader confidence="0.997894">
5.1 Impact of Regularization
</subsectionHeader>
<bodyText confidence="0.99997955">
One main difference between MIRA and the direct
optimizers is the availability of an explicit regular-
ization term A. To measure the impact of this param-
eter, we designed a feature set explicitly for over-
fitting. This set uses our Big Fr-En features, with the
count bin template modified to distinguish each joint
count observed in the tuning set. These new fea-
tures, which expand the set to 20k+ features, should
generalize poorly.
We tested MR and SVM on our Fr-En data us-
ing this feature set, varying their respective regular-
ization parameters by factors of 10. We compared
this to Batch Lattice MIRA’s step-size cap C, which
controls its regularization (Martins et al., 2010). The
results are shown in Figure 1. Looking at the tuning
scores, one can see that A affords much greater con-
trol over tuning performance than MIRA’s C. Look-
ing at test scores, MIRA’s narrow band of regular-
ization appears to be just about right; however, there
is no reason to expect this to always be the case.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999574">
We have presented three new, large-margin tuning
methods for SMT that can handle thousands of fea-
tures. Batch lattice and k-best MIRA carry out their
online training within approximated search spaces,
reducing costs in terms of both implementation and
training time. The Structured SVM optimizes a sum
of hinge losses directly, exposing an explicit reg-
ularization term. We have organized the literature
on tuning, and carried out an extensive comparison
of linear-loss SMT tuners. Our experiments show
Batch Lattice MIRA to be the most consistent of the
tested methods. In the future, we intend to inves-
tigate improved sentence-BLEU approximations to
help narrow the gap between MIRA and the direct
optimizers.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999911">
Thanks to Mark Hopkins, Zhifei Li and Jonathan
May for their advice while implementing the meth-
ods in this review, and to Kevin Gimpel, Roland
Kuhn and the anonymous reviewers for their valu-
able comments on an earlier draft.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019272727273">
Abhishek Arun, Barry Haddow, and Philipp Koehn.
2010. A unified approach to minimum risk training
and decoding. In Proceedings of the Joint Workshop
on Statistical Machine Translation and MetricsMATR,
pages 365–374.
Leon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In International Confer-
ence on Computational Statistics, pages 177–187.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224–233.
</reference>
<page confidence="0.98921">
435
</page>
<reference confidence="0.999887764705883">
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In HLT-NAACL, pages 218–226.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176–181.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
HLT-NAACL, Montreal, Canada, June.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011.
Margin infused relaxed algorithm for moses. The
Prague Bulletin of Mathematical Linguistics, 96:69–
78.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In EMNLP, pages 1352–1362.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177–180, Prague, Czech Republic, June.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40–51.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In COLING-ACL,
pages 761–768.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING, pages 501–507.
Wolfgang Machery, Franz Josef Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725–734.
Andr´e F. T. Martins, Kevin Gimpel, Noah A. Smith,
Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T.
Figueiredo. 2010. Learning structured classifiers with
dual coordinate descent. Technical Report CMU-ML-
10-109, Carnegie Mellon University.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In ACL, pages 456–464.
Franz Joseph Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295–302,
Philadelphia, PA, July.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In ACL, pages 160–
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311–318.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In EMNLP, pages 1418–1427.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected bleu training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 159–
165.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177–184, Boston, Massachusetts,
May 2 - May 7.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In COLING-
ACL, pages 787–794.
Ioannis Tsochantaridis, Thomas Hofman, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML, pages 823–830.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764–773.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In HLT-
NAACL, pages 257–264, Boston, USA, May.
Richard Zens, Sˇasa Hasan, and Hermann Ney. 2007. A
systematic comparison of training criteria for statisti-
cal machine translation. In EMNLP, pages 524–532.
</reference>
<page confidence="0.999222">
436
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908500">
<title confidence="0.996954">Batch Tuning Strategies for Statistical Machine Translation</title>
<author confidence="0.961548">Cherry</author>
<affiliation confidence="0.934746">National Research Council Canada</affiliation>
<abstract confidence="0.999322666666667">There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>A unified approach to minimum risk training and decoding.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>365--374</pages>
<contexts>
<context position="20005" citStr="Arun et al., 2010" startWordPosition="3369" endWordPosition="3372">thed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. 431 k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs (French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn)), across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments u</context>
</contexts>
<marker>Arun, Haddow, Koehn, 2010</marker>
<rawString>Abhishek Arun, Barry Haddow, and Philipp Koehn. 2010. A unified approach to minimum risk training and decoding. In Proceedings of the Joint Workshop on Statistical Machine Translation and MetricsMATR, pages 365–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Bottou</author>
</authors>
<title>Large-scale machine learning with stochastic gradient descent.</title>
<date>2010</date>
<booktitle>In International Conference on Computational Statistics,</booktitle>
<pages>177--187</pages>
<contexts>
<context position="11491" citStr="Bottou, 2010" startWordPosition="1911" endWordPosition="1912">atical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations needed to calculate the gradient [ ] [ ] EP~w ~hi(e)Ai(e) − EP~w [Ai(e)] EP~w ~hi(e) 2Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. It can be obtained using a logit loss fi(97) _ E � �79 · �9�i(eb) − 9�i(eg))// g,b 2 log 1 + exp . ~w* = arg min λ � `i(~w) (4) w 2 ||~w||2 + i 429 are trivial to extract from a k-best list of derivations. Each downward step along this gradient moves the model toward likely</context>
<context position="22687" citStr="Bottou, 2010" startWordPosition="3806" endWordPosition="3807">) words (fr) train 2,928,759 60,482,232 68,578,459 dev 2,002 40,094 44,603 test1 2,148 42,503 48,064 test2 2,166 44,701 49,986 Table 1: Hansard Corpus (English/French) corpus sentences words (zh) words (en) train1 6,677,729 200,706,469 213,175,586 train2 3,378,230 69,232,098 66,510,420 dev 1,506 38,233 40,260 nist04 1,788 53,439 59,944 nist06 1,664 41,782 46,140 nist08 1,357 35,369 42,039 Table 2: NIST09 Corpus (Chinese-English). Train1 corresponds to the UN and Hong Kong sub-corpora; train2 to all others. gradient descent, with qo tuned to optimize the training loss achieved after one epoch (Bottou, 2010). Upon reaching a local optimum, we reshuffle our data, re-tune our learning rate, and re-start from the optimum, repeating this process 5 times. We do not sharpen our distribution with a temperature or otherwise control for entropy; instead, we trust A = 50 to maintain a reasonable distribution. • svm : Structured SVM (§3.2) with A = 1000. 4.1 Data Systems for English/French were trained on Canadian Hansard data (years 2001–2009) summarized in table 1.7 The dev and test sets were chosen randomly from among the most recent 5 days of Hansard transcripts. The system for Zh-En was trained on data</context>
</contexts>
<marker>Bottou, 2010</marker>
<rawString>Leon Bottou. 2010. Large-scale machine learning with stochastic gradient descent. In International Conference on Computational Statistics, pages 177–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>224--233</pages>
<contexts>
<context position="5917" citStr="Chiang et al. (2008" startWordPosition="957" endWordPosition="960">LEU, which we re-encode into a cost Δi(e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and Δ, and in how they employ their loss functions to tune their weights. 1This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: `i(~w) = ma£x IΔi(e) + w~ · (hi(e) − hi(ez )eEi )J where ez is an oracle derivation, and cost is defined as Δi(e) = BLEUi(ez) − BLEUi(e), so that Δi(ez) = 0. The loss `i(~w) is 0 only if w~ separates each e ∈ Ei from ez by a margin proportional to their BLEU differentials. MIRA is an instance of online learning, repeating the following steps: visit an example i, decode according to ~w, and update w~ to reduce `i(~w). Each update makes the smallest change to w~ (subject to a step-size cap C) that will separate</context>
<context position="7733" citStr="Chiang et al. (2008)" startWordPosition="1285" endWordPosition="1288">ed using a pseudocorpus BLEU that tracks the n-gram statistics of the model-best derivations from the last few updates. This modified cost matches corpus BLEU better than add-1 smoothing, but it also makes Δi time-dependent: each update for an example i will be in the context of a different pseudo-corpus. The oracle ez also shifts with each update to ~w, as it is defined as a “hope” derivation, which maximizes w~ · ~hi(e) + BLEUi(e). Hope updating ensures that MIRA aims for ambitious, reachable derivations. In our implementation, we make a number of small, empirically verified deviations from Chiang et al. (2008). These include the above-mentioned use of a single hope and fear hypothesis, and the use 428 of hope hypotheses (as opposed to model-best hypotheses) to build the pseudo-corpus for calculating BLEUi. These changes were observed to be neutral with respect to translation quality, but resulted in faster running time and simplified implementation. 2.2 Direct Optimization With the exception of MIRA, the tuning approaches discussed in this paper are direct optimizers. That is, each solves the following optimization problem: where the first term provides regularization, weighted by λ. Throughout thi</context>
<context position="13243" citStr="Chiang et al. (2008)" startWordPosition="2205" endWordPosition="2209"> easy parallelization, while using a fluid pseudo-corpus creates an unstable learning objective. We develop two large-margin tuners that explore these trade-offs. 3.1 Batch MIRA Online training makes it possible to learn with the decoder in the loop, forgoing the need to approximate the search space, but it is not necessarily convenient to do so. Online algorithms are notoriously difficult to parallelize, as they assume each example is visited in sequence. Parallelization is important for efficient SMT tuning, as decoding is still relatively expensive. The parallel online updates suggested by Chiang et al. (2008) involve substantial inter-process communication, which may not be easily supported by all clusters. McDonald et al. (2010) suggest a simpler distributed strategy that is amenable to map-reduce-like frameworks, which interleaves online training on shards with weight averaging across shards. This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation. However, online training using the decoder may not be necessary for good performance. The success of MERT, PRO and MR indicates that their shared search approximation is actually quite reaso</context>
<context position="15269" citStr="Chiang et al., 2008" startWordPosition="2540" endWordPosition="2543">ded in Algorithm 1. Note that if we set £ = £, Algorithm 1 also describes online MIRA. Batch k-best MIRA inherits all of the MERT architecture. It is very easy to implement; the hope-fear decoding steps can by carried out by simply evaluating BLEU score and model score for each hypothesis in the k-best list. Batch Lattice MIRA replaces k-best decoding in step 1 with decoding to lattices. To enable loading all of the lattices into memory at once, we prune to a density of 50 edges per reference word. The hopefear decoding step requires the same oracle lattice decoding algorithms as online MIRA (Chiang et al., 2008). The lattice aggregation in the outer loop can be kept reasonable by aggregating only those paths corresponding to hope or fear derivations. 3.2 Structured SVM While MIRA takes a series of local hinge-loss reducing steps, it is also possible to directly minimize the sum of hinge-losses using a batch algorithm, creating a structured SVM (Tsochantaridis et al., 2004). To avoid fixing an oracle before optimization begins, we adapt Yu and Joachim’s (2009) latent SVM to our task, which allows the oracle derivation for each sentence to vary during training. Again we assume a MERT-like architecture,</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>218--226</pages>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>176--181</pages>
<contexts>
<context position="3235" citStr="Clark et al., 2011" startWordPosition="497" endWordPosition="500">et al.’s (2008) hope-fear MIRA that use k-best or lattice-approximated search spaces, producing a Batch MIRA that outperforms a popular mechanism for parallelizing online learners. We also investigate the direct optimization of hinge loss on k-best lists, through the use of a Structured SVM (Tsochantaridis et al., 2004). We review and organize the existing tuning literature, providing sentence-level loss functions for minimum risk, online and pairwise training. Finally, since randomization plays a different role in each tuner, we also suggest a new method for testing an optimizer’s stability (Clark et al., 2011), which sub-samples the tuning set instead of varying a random seed. 2 Background We begin by establishing some notation. We view our training set as a list of triples [f, R, £]Z1, where f is a source-language sentence, R is a set of targetlanguage reference sentences, and £ is the set of 427 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation tha</context>
<context position="26379" citStr="Clark et al (2011)" startWordPosition="4446" endWordPosition="4449">k,10k,00; word pair fires when each of the 80 most frequent words in each language appear aligned 1-1 to each other, to some other word, or not 1-1; and length bin captures each possible phrase length and length pair. Table 3 summarizes the feature templates, showing the maximum number of features each can generate, and the number of features that received non-zero weights in the final model tuned by MR for each language pair. Feature weights are initialized to 1.0 for each of the TM, LM and distortion penalty features. All other weights are initialized to 0.0. 4.3 Stability Testing We follow Clark et al (2011), and perform multiple randomized replications of each experiment. However, their method of using different random seeds is not applicable in our context, since randomization does not play the same role for all tuning methods. Our solution was to randomly draw and fix four different sub-samples of each dev set, retaining each sentence with a probability of 0.9. For each tuning method and setting, we then optimize on the original dev and all sub-samples. The resulting standard deviations provide an indication of stability. 5 Results The results of our survey of tuning methods can be seen in Tab</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In ACL, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="6600" citStr="Crammer et al. (2006)" startWordPosition="1085" endWordPosition="1089"> structured hinge loss: `i(~w) = ma£x IΔi(e) + w~ · (hi(e) − hi(ez )eEi )J where ez is an oracle derivation, and cost is defined as Δi(e) = BLEUi(ez) − BLEUi(e), so that Δi(ez) = 0. The loss `i(~w) is 0 only if w~ separates each e ∈ Ei from ez by a margin proportional to their BLEU differentials. MIRA is an instance of online learning, repeating the following steps: visit an example i, decode according to ~w, and update w~ to reduce `i(~w). Each update makes the smallest change to w~ (subject to a step-size cap C) that will separate the oracle from a number of negative hypotheses. The work of Crammer et al. (2006) shows that updating away from a single “fear” hypothesis that maximizes (2) admits a closed-form update that performs well. Let ez be the e ∈ Ei that maximizes `i(~w); the update can be performed in two steps: ηt = min �C ~ `i(~wt) J i )�~hi(ed)��2 i)� ~wt+1 = ~wt + ηt �~hi(e� i ) − ~hi(e� To improve generalization, the average of all weights seen during learning is used on unseen data. Chiang et al. (2008) take advantage of MIRA’s online nature to modify each update to better suit SMT. The cost Δi is defined using a pseudocorpus BLEU that tracks the n-gram statistics of the model-best deriva</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="8895" citStr="Gimpel and Smith (2012)" startWordPosition="1467" endWordPosition="1470">t term provides regularization, weighted by λ. Throughout this paper, (4) is optimized with respect to a fixed approximation of the decoder’s true search space, represented as a collection of k-best lists. The various methods differ in their definition of loss and in how they optimize their objective. Without the complications added by hope decoding and a time-dependent cost function, unmodified MIRA can be shown to be carrying out dual coordinate descent for an SVM training objective (Martins et al., 2010). However, exactly what objective hopefear MIRA is optimizing remains an open question. Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA. 2.3 Pairwise Ranking Optimization Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. That is, PRO employs a growing approximation of £i by aggregating the k-best hypotheses from a series of increasingly refined models. This architecture is desirable, as most groups have infrastructure to k-best decode their tuning sets in parallel. For a given approximate �£i, PRO creates a sample Si of (eg, eb) pa</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In HLT-NAACL, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Margin infused relaxed algorithm for moses. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2011</date>
<pages>96--69</pages>
<contexts>
<context position="13596" citStr="Hasler et al., 2011" startWordPosition="2257" endWordPosition="2260">ne algorithms are notoriously difficult to parallelize, as they assume each example is visited in sequence. Parallelization is important for efficient SMT tuning, as decoding is still relatively expensive. The parallel online updates suggested by Chiang et al. (2008) involve substantial inter-process communication, which may not be easily supported by all clusters. McDonald et al. (2010) suggest a simpler distributed strategy that is amenable to map-reduce-like frameworks, which interleaves online training on shards with weight averaging across shards. This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation. However, online training using the decoder may not be necessary for good performance. The success of MERT, PRO and MR indicates that their shared search approximation is actually quite reasonable. Therefore, we propose Batch MIRA, which sits exactly where MERT sits in the standard tuning architecture, greatly simplifying parallelization: 1. Parallel Decode: [�£&apos;]n1 = k-best([f, £]n1, w—) 2. Aggregate: [ �£]n1 = [�£]n1 U [�£l]n1 3. Train: w� = BatchMIRA([f, R, 4. Repeat where BatchMIRA() trains the SMT-adapted MIRA algorithm to completion</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2011</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2011. Margin infused relaxed algorithm for moses. The Prague Bulletin of Mathematical Linguistics, 96:69– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1352--1362</pages>
<contexts>
<context position="1828" citStr="Hopkins and May, 2011" startWordPosition="279" endWordPosition="282">ver, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop. This i</context>
<context position="9065" citStr="Hopkins and May (2011)" startWordPosition="1491" endWordPosition="1494">d as a collection of k-best lists. The various methods differ in their definition of loss and in how they optimize their objective. Without the complications added by hope decoding and a time-dependent cost function, unmodified MIRA can be shown to be carrying out dual coordinate descent for an SVM training objective (Martins et al., 2010). However, exactly what objective hopefear MIRA is optimizing remains an open question. Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA. 2.3 Pairwise Ranking Optimization Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. That is, PRO employs a growing approximation of £i by aggregating the k-best hypotheses from a series of increasingly refined models. This architecture is desirable, as most groups have infrastructure to k-best decode their tuning sets in parallel. For a given approximate �£i, PRO creates a sample Si of (eg, eb) pairs, such that BLEUi(eg) &gt; BLEUi(eb). It then uses a binary classifier to separate each pair. We describe the resulting loss in terms of an SVM classifier, to highlight s</context>
<context position="11718" citStr="Hopkins and May (2011)" startWordPosition="1946" endWordPosition="1949">BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations needed to calculate the gradient [ ] [ ] EP~w ~hi(e)Ai(e) − EP~w [Ai(e)] EP~w ~hi(e) 2Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. It can be obtained using a logit loss fi(97) _ E � �79 · �9�i(eb) − 9�i(eg))// g,b 2 log 1 + exp . ~w* = arg min λ � `i(~w) (4) w 2 ||~w||2 + i 429 are trivial to extract from a k-best list of derivations. Each downward step along this gradient moves the model toward likely derivations, and away from likely derivations that incur high costs. 3 Novel Methods We have reviewed three tuning methods, all of which address MERT’s weakness with large features by using surrogate loss functions. Additional</context>
<context position="21540" citStr="Hopkins and May (2011)" startWordPosition="3624" endWordPosition="3627">) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay -y = 0.999 and C = 0.01. Online parallelization follows McDonald et al. (2010), using 8 shards. We tested 20, 15, 10, 8 and 5 shards during development. • lb-mira : Batch Lattice MIRA (§3.1). • kb-mira : Batch k-best MIRA (§3.1). • pro : PRO (§2.3) follows Hopkins and May (2011); however, we were unable to find settings that performed well in general. Reported results use MegaM6 with a maximum of 30 iterations (as is done in Moses; the early stopping provides a form of regularization) for our six English/French tests, and MegaM with 100 iterations and a reduced initial uniform sample (50 pairs instead of 5000) for our three English/Chinese tests. • mr : MR as described in §2.4. We employ a learning rate of �0/(1 + goAt) for stochastic 6Available at www.cs.utah.edu/∼hal/megam/ corpus sentences words (en) words (fr) train 2,928,759 60,482,232 68,578,459 dev 2,002 40,09</context>
<context position="25568" citStr="Hopkins and May, 2011" startWordPosition="4308" endWordPosition="4311">s. For Zh-En, Small’s TM (trained on both train1 and train2 in table 2) is replaced by 2 separate TMs from these sub-corpora; for En/Fr, the extra TM (4 features) comes from a forced-decoding alignment of the training corpus, as proposed by Wuebker et al. (2010). For Zh-En, the extra LM is a 4-gram trained on the target half of the parallel corpus; for En/Fr, it is a 4-gram trained on 5m sentences of similar parliamentary data. The Big set adds sparse Boolean features to Medium, for a maximum of 6,848 features. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011): tgt unal picks out each of the 50 most frequent target words to appear unaligned in the phrase table; count bin uniquely bins joint phrase pair counts with upper bounds 1,2,4,8,16,32,64,128,1k,10k,00; word pair fires when each of the 80 most frequent words in each language appear aligned 1-1 to each other, to some other word, or not 1-1; and length bin captures each possible phrase length and length pair. Table 3 summarizes the feature templates, showing the maximum number of features each can generate, and the number of features that received non-zero weights in the final model tuned by MR </context>
<context position="27925" citStr="Hopkins and May (2011)" startWordPosition="4713" endWordPosition="4717">rtially due to the fact that our Big feature set affects only phrase-table scores. Our phrase tables are already strong, through our use of large data or leave-one-out forced decoding. The important baseline when assessing the utility of a method is Medium k-best MERT. In all language pairs, our Big systems generally outperform this baseline by 0.4 BLEU points. It is interesting to note that most methods achieve the bulk of this improvement on the Medium feature set.8 This indicates that MERT begins to show some problems even in an 18-feature 8One can see the same phenomenon in the results of Hopkins and May (2011) as well. 433 Table 4: French to English Translation (Fr-En) Tune Small SD Medium SD Tune Big SD Test Tune Test Test kb-mert 40.50 39.94 0.04 40.75 40.29 0.13 n/a n/a n/a lb-mert 40.52 39.93 0.11 40.93 40.39 0.08 n/a n/a n/a mira 40.38 39.94 0.04 40.64 40.59 0.06 41.02 40.74 0.05 kb-mira 40.46 39.97 0.05 40.92 40.64 0.12 41.46 40.75 0.08 lb-mira 40.44 39.98 0.06 40.94 40.65 0.06 41.59 40.78 0.09 pro 40.11 40.05 0.05 40.16 40.07 0.08 40.55 40.21 0.24 mr 40.24 39.88 0.05 40.70 40.57 0.14 41.18 40.60 0.08 svm 40.05 40.20 0.03 40.60 40.56 0.08 41.32 40.52 0.07 Table 5: English to French Translatio</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In EMNLP, pages 1352–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Kai-Wei Chang</author>
<author>Chih-Jen Lin</author>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18741" citStr="Hsieh et al., 2008" startWordPosition="3168" endWordPosition="3171">aking small steps. The SVM requires a stable objective to optimize, meaning that it must forgo the pseudo-corpus used by MIRA to calculate Ai; instead, the SVM uses an interpolated sentence-level BLEU (Liang et al., 2006).4 Finally, MIRA’s oracle is selected with hope decoding. With a sufficiently large ~w, any e E £ can potentially become the oracle. In contrast, the SVM’s local oracle is selected from a small set £*, which was done to more closely match the assumptions of the Latent SVM. To solve the necessary quadratic programming sub-problems, we use a multiclass SVM similar to LIBLINEAR (Hsieh et al., 2008). Like Batch MIRA and PRO, the actual optimization is very fast, as the cutting plane converges quickly and all of [�£]n1 can be loaded into memory at once. 3.3 Qualitative Summary We have reviewed three tuning methods and introduced three tuning methods. All six methods employ sentence-level loss functions, which in turn employ sentence-level BLEU approximations. Except for online MIRA, all methods plug nicely into the existing MERT architecture. These methods can be split into two groups: MIRA variants (online, batch k-best, batch lattice), and direct optimizers (PRO, MR and SVM). The MIRA v</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="23983" citStr="Koehn et al., 2003" startWordPosition="4038" endWordPosition="4041"> was taken from the NIST 05 evaluation set, augmented with some material reserved from other NIST corpora. The NIST 04, 06, and 08 evaluation sets were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments 7This corpus will be distributed on request. 432 template max fren enfr zhen tgt unal 50 50 50 31 count bin 11 11 11 11 word pair 6724 1298 1291 1664 length bin 63 63 63 63 total 6848 1422 1415 1769 Table 3: Sparse feature templates used in Big. performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent systems (exactly equivalent for En-Fr and FrEn, which are mirror images) for each of three nested feature sets: Small, Medium, and Big. The Small set defines a minimal 7-feature system intended to be within easy reach of all tuning strategies. I</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="20681" citStr="Koehn et al., 2007" startWordPosition="3471" endWordPosition="3474">s the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs (French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn)), across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate £ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, A was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay -y = 0.999 and C = 0</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>40--51</pages>
<contexts>
<context position="10810" citStr="Li and Eisner, 2009" startWordPosition="1798" endWordPosition="1801">large uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizer</context>
<context position="19915" citStr="Li and Eisner, 2009" startWordPosition="3352" endWordPosition="3355">rect optimizers (PRO, MR and SVM). The MIRA variants use pseudocorpus BLEU in place of smoothed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. 431 k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs (French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn)), across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliab</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In EMNLP, pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>761--768</pages>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLING-ACL, pages 761–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation. In</title>
<date>2004</date>
<booktitle>COLING,</booktitle>
<pages>501--507</pages>
<contexts>
<context position="5522" citStr="Lin and Och, 2004" startWordPosition="885" endWordPosition="888">MERT to resist standard mechanisms of regularization that aim to keep ||~w ||small. The problems with MERT can be addressed through the use of surrogate loss functions. In this paper, we focus on linear losses that decompose over training examples. Using Ri and Ei, each loss `i(~w) indicates how poorly w~ performs on the i�h training example. This requires a sentence-level approximation of BLEU, which we re-encode into a cost Δi(e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and Δ, and in how they employ their loss functions to tune their weights. 1This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: `i(~w) = ma£x IΔi(e) + w~ · (hi(e) − hi(ez )eEi )J where ez is an oracle derivation, and cost is defined as Δi(e) = BLE</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In COLING, pages 501–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Machery</author>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>725--734</pages>
<contexts>
<context position="21093" citStr="Machery et al., 2008" startWordPosition="3543" endWordPosition="3546"> over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate £ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, A was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay -y = 0.999 and C = 0.01. Online parallelization follows McDonald et al. (2010), using 8 shards. We tested 20, 15, 10, 8 and 5 shards during development. • lb-mira : Batch Lattice MIRA (§3.1). • kb-mira : Batch k-best MIRA (§3.1). • pro : PRO (§2.3) follows Hopkins and May (2011); however, we were unable to find settings that performed well in general. Reported results use MegaM6 with a maximum of 30 iterations (as is done in Mos</context>
</contexts>
<marker>Machery, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Machery, Franz Josef Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In EMNLP, pages 725–734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Learning structured classifiers with dual coordinate descent.</title>
<date>2010</date>
<tech>Technical Report CMU-ML10-109,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="8784" citStr="Martins et al., 2010" startWordPosition="1450" endWordPosition="1453"> in this paper are direct optimizers. That is, each solves the following optimization problem: where the first term provides regularization, weighted by λ. Throughout this paper, (4) is optimized with respect to a fixed approximation of the decoder’s true search space, represented as a collection of k-best lists. The various methods differ in their definition of loss and in how they optimize their objective. Without the complications added by hope decoding and a time-dependent cost function, unmodified MIRA can be shown to be carrying out dual coordinate descent for an SVM training objective (Martins et al., 2010). However, exactly what objective hopefear MIRA is optimizing remains an open question. Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA. 2.3 Pairwise Ranking Optimization Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. That is, PRO employs a growing approximation of £i by aggregating the k-best hypotheses from a series of increasingly refined models. This architecture is desirable, as most groups have infrastructure to k</context>
<context position="32388" citStr="Martins et al., 2010" startWordPosition="5495" endWordPosition="5498">s is the availability of an explicit regularization term A. To measure the impact of this parameter, we designed a feature set explicitly for overfitting. This set uses our Big Fr-En features, with the count bin template modified to distinguish each joint count observed in the tuning set. These new features, which expand the set to 20k+ features, should generalize poorly. We tested MR and SVM on our Fr-En data using this feature set, varying their respective regularization parameters by factors of 10. We compared this to Batch Lattice MIRA’s step-size cap C, which controls its regularization (Martins et al., 2010). The results are shown in Figure 1. Looking at the tuning scores, one can see that A affords much greater control over tuning performance than MIRA’s C. Looking at test scores, MIRA’s narrow band of regularization appears to be just about right; however, there is no reason to expect this to always be the case. 6 Conclusion We have presented three new, large-margin tuning methods for SMT that can handle thousands of features. Batch lattice and k-best MIRA carry out their online training within approximated search spaces, reducing costs in terms of both implementation and training time. The Str</context>
</contexts>
<marker>Martins, Gimpel, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e F. T. Martins, Kevin Gimpel, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010. Learning structured classifiers with dual coordinate descent. Technical Report CMU-ML10-109, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>456--464</pages>
<contexts>
<context position="13366" citStr="McDonald et al. (2010)" startWordPosition="2223" endWordPosition="2226">gin tuners that explore these trade-offs. 3.1 Batch MIRA Online training makes it possible to learn with the decoder in the loop, forgoing the need to approximate the search space, but it is not necessarily convenient to do so. Online algorithms are notoriously difficult to parallelize, as they assume each example is visited in sequence. Parallelization is important for efficient SMT tuning, as decoding is still relatively expensive. The parallel online updates suggested by Chiang et al. (2008) involve substantial inter-process communication, which may not be easily supported by all clusters. McDonald et al. (2010) suggest a simpler distributed strategy that is amenable to map-reduce-like frameworks, which interleaves online training on shards with weight averaging across shards. This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation. However, online training using the decoder may not be necessary for good performance. The success of MERT, PRO and MR indicates that their shared search approximation is actually quite reasonable. Therefore, we propose Batch MIRA, which sits exactly where MERT sits in the standard tuning architecture, greatly si</context>
<context position="21339" citStr="McDonald et al. (2010)" startWordPosition="3585" endWordPosition="3589">roximate £ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, A was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay -y = 0.999 and C = 0.01. Online parallelization follows McDonald et al. (2010), using 8 shards. We tested 20, 15, 10, 8 and 5 shards during development. • lb-mira : Batch Lattice MIRA (§3.1). • kb-mira : Batch k-best MIRA (§3.1). • pro : PRO (§2.3) follows Hopkins and May (2011); however, we were unable to find settings that performed well in general. Reported results use MegaM6 with a maximum of 30 iterations (as is done in Moses; the early stopping provides a form of regularization) for our six English/French tests, and MegaM with 100 iterations and a reduced initial uniform sample (50 pairs instead of 5000) for our three English/Chinese tests. • mr : MR as described </context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In ACL, pages 456–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1009" citStr="Och and Ney, 2002" startWordPosition="144" endWordPosition="147">evel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1200" citStr="Och, 2003" startWordPosition="180" endWordPosition="181">s. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., </context>
<context position="10769" citStr="Och, 2003" startWordPosition="1792" endWordPosition="1793">ing routine, which performs a large uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a loc</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training for statistical machine translation. In ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4187" citStr="Papineni et al., 2002" startWordPosition="653" endWordPosition="656">n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation that can be decoded from fi. The function ~hi(e) describes e’s relationship to its source fi using features that decompose into the decoder. A linear model w~ scores derivations according to their features, meaning that the decoder solves: ei(~w) = arg max w~ · ~hi(e) (1) eE£i Assuming we wish to optimize our decoder’s BLEU score (Papineni et al., 2002), the natural objective of learning would be to find a w~ such that BLEU([e(~w), R]n1) is maximal. In most machine learning papers, this would be the point where we would say, “unfortunately, this objective is unfeasible.” But in SMT, we have been happily optimizing exactly this objective for years using MERT. However, it is now acknowledged that the MERT approach is not feasible for more than 30 or so features. This is due to two main factors: 1. MERT’s parameter search slows and becomes less effective as the number of features rises, stopping it from finding good training scores. 2. BLEU is </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>John Denero</author>
<author>Dan Klein</author>
</authors>
<title>Consensus training for consensus decoding in machine translation.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1418--1427</pages>
<contexts>
<context position="11011" citStr="Pauls et al., 2009" startWordPosition="1834" endWordPosition="1837">ize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations n</context>
<context position="19935" citStr="Pauls et al., 2009" startWordPosition="3356" endWordPosition="3359"> MR and SVM). The MIRA variants use pseudocorpus BLEU in place of smoothed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. 431 k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs (French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn)), across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with </context>
</contexts>
<marker>Pauls, Denero, Klein, 2009</marker>
<rawString>Adam Pauls, John Denero, and Dan Klein. 2009. Consensus training for consensus decoding in machine translation. In EMNLP, pages 1418–1427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Expected bleu training for graphs: BBN system description for WMT11 system combination task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>159--165</pages>
<contexts>
<context position="11032" citStr="Rosti et al., 2011" startWordPosition="1838" endWordPosition="1841">lf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations needed to calculate th</context>
<context position="19956" citStr="Rosti et al., 2011" startWordPosition="3360" endWordPosition="3363">RA variants use pseudocorpus BLEU in place of smoothed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. 431 k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs (French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn)), across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large n</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2011</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2011. Expected bleu training for graphs: BBN system description for WMT11 system combination task. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 159– 165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative reranking for machine translation. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<volume>2</volume>
<pages>177--184</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="1804" citStr="Shen et al., 2004" startWordPosition="275" endWordPosition="278">T (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s establ</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In HLT-NAACL, pages 177–184, Boston, Massachusetts, May 2 - May 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In COLINGACL,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="1479" citStr="Smith and Eisner, 2006" startWordPosition="224" endWordPosition="227"> a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approx</context>
<context position="10991" citStr="Smith and Eisner, 2006" startWordPosition="1830" endWordPosition="1833"> selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In COLINGACL, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofman</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In ICML,</booktitle>
<pages>823--830</pages>
<contexts>
<context position="2937" citStr="Tsochantaridis et al., 2004" startWordPosition="450" endWordPosition="453">ompare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop. This is the first comparison to include all three categories of optimizer. Furthermore, we introduce three tuners that have not been previously tested. In particular, we test variants of Chiang et al.’s (2008) hope-fear MIRA that use k-best or lattice-approximated search spaces, producing a Batch MIRA that outperforms a popular mechanism for parallelizing online learners. We also investigate the direct optimization of hinge loss on k-best lists, through the use of a Structured SVM (Tsochantaridis et al., 2004). We review and organize the existing tuning literature, providing sentence-level loss functions for minimum risk, online and pairwise training. Finally, since randomization plays a different role in each tuner, we also suggest a new method for testing an optimizer’s stability (Clark et al., 2011), which sub-samples the tuning set instead of varying a random seed. 2 Background We begin by establishing some notation. We view our training set as a list of triples [f, R, £]Z1, where f is a source-language sentence, R is a set of targetlanguage reference sentences, and £ is the set of 427 2012 Con</context>
<context position="15637" citStr="Tsochantaridis et al., 2004" startWordPosition="2600" endWordPosition="2603">p 1 with decoding to lattices. To enable loading all of the lattices into memory at once, we prune to a density of 50 edges per reference word. The hopefear decoding step requires the same oracle lattice decoding algorithms as online MIRA (Chiang et al., 2008). The lattice aggregation in the outer loop can be kept reasonable by aggregating only those paths corresponding to hope or fear derivations. 3.2 Structured SVM While MIRA takes a series of local hinge-loss reducing steps, it is also possible to directly minimize the sum of hinge-losses using a batch algorithm, creating a structured SVM (Tsochantaridis et al., 2004). To avoid fixing an oracle before optimization begins, we adapt Yu and Joachim’s (2009) latent SVM to our task, which allows the oracle derivation for each sentence to vary during training. Again we assume a MERT-like architecture, which approximates £ with an £ constructed from aggregated k-best lists. Inspired by the local oracle of Liang et al. (2006), we define �£i* to be an oracle set: �£i* = {e|BLEUi(e) is maximal}. 3In our implementation, BatchMIRA() trains for J = 30 passes over � ���� . £]n 1,�w) 430 Algorithm 1 BatchMIRA input [f, R, �£]n1, ~w, max epochs J, step cap C, and pseudo-c</context>
<context position="17570" citStr="Tsochantaridis et al. (2004)" startWordPosition="2966" endWordPosition="2969">i(~w) = maxeE �£i �~w ~hi(e� i )�� − maxe∗E�£ i i This loss is 0 only if some hypothesis in the oracle set is separated from all others by a margin proportional to their BLEUi differentials. With loss defined in this manner, we can minimize (4) to local minimum by using an alternating training procedure. For each example i, we select a fixed ez E �£i* that maximizes model score; that is, w~ is used to break ties in BLEU for oracle selection. With the oracle fixed, the objective becomes a standard structured SVM objective, which can be minimized using a cutting-plane algorithm, as described by Tsochantaridis et al. (2004). After doing so, we can drive the loss lower still by iterating this process: re-select each oracle (breaking ties with the new ~w), then re-optimize ~w. We do so 10 times. We were surprised by the impact of these additional iterations on the final loss; for some sentences, �£i* can be quite large. Despite the fact that both algorithms use a structured hinge loss, there are several differences between our SVM and MIRA. The SVM has an explicit regularization term λ that is factored into its global objective, while MIRA regularizes implicitly by taking small steps. The SVM requires a stable obj</context>
</contexts>
<marker>Tsochantaridis, Hofman, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofman, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In ICML, pages 823–830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>764--773</pages>
<contexts>
<context position="1646" citStr="Watanabe et al., 2007" startWordPosition="253" endWordPosition="256">o optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance </context>
<context position="5881" citStr="Watanabe et al. (2007)" startWordPosition="950" endWordPosition="953">res a sentence-level approximation of BLEU, which we re-encode into a cost Δi(e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and Δ, and in how they employ their loss functions to tune their weights. 1This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: `i(~w) = ma£x IΔi(e) + w~ · (hi(e) − hi(ez )eEi )J where ez is an oracle derivation, and cost is defined as Δi(e) = BLEUi(ez) − BLEUi(e), so that Δi(ez) = 0. The loss `i(~w) is 0 only if w~ separates each e ∈ Ei from ez by a margin proportional to their BLEU differentials. MIRA is an instance of online learning, repeating the following steps: visit an example i, decode according to ~w, and update w~ to reduce `i(~w). Each update makes the smallest change to w~ (subject to a</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL, pages 764–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-oneout.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25208" citStr="Wuebker et al. (2010)" startWordPosition="4244" endWordPosition="4247">mprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword corpus; for English/French, it is a 4-gram trained on the target half of the parallel Hansard. The Medium set is a more competitive 18-feature system. It adds 4 TM features, one LM, and 6 lexicalized distortion features. For Zh-En, Small’s TM (trained on both train1 and train2 in table 2) is replaced by 2 separate TMs from these sub-corpora; for En/Fr, the extra TM (4 features) comes from a forced-decoding alignment of the training corpus, as proposed by Wuebker et al. (2010). For Zh-En, the extra LM is a 4-gram trained on the target half of the parallel corpus; for En/Fr, it is a 4-gram trained on 5m sentences of similar parliamentary data. The Big set adds sparse Boolean features to Medium, for a maximum of 6,848 features. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011): tgt unal picks out each of the 50 most frequent target words to appear unaligned in the phrase table; count bin uniquely bins joint phrase pair counts with upper bounds 1,2,4,8,16,32,64,128,1k,10k,00; word pair fires when each of the 80 m</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-oneout. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>257--264</pages>
<location>Boston, USA,</location>
<contexts>
<context position="24130" citStr="Zens and Ney, 2004" startWordPosition="4058" endWordPosition="4061">were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments 7This corpus will be distributed on request. 432 template max fren enfr zhen tgt unal 50 50 50 31 count bin 11 11 11 11 word pair 6724 1298 1291 1664 length bin 63 63 63 63 total 6848 1422 1415 1769 Table 3: Sparse feature templates used in Big. performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent systems (exactly equivalent for En-Fr and FrEn, which are mirror images) for each of three nested feature sets: Small, Medium, and Big. The Small set defines a minimal 7-feature system intended to be within easy reach of all tuning strategies. It comprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword co</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In HLTNAACL, pages 257–264, Boston, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Sˇasa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of training criteria for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP,</booktitle>
<pages>524--532</pages>
<contexts>
<context position="10788" citStr="Zens et al., 2007" startWordPosition="1794" endWordPosition="1797">, which performs a large uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let Ai(e) = −BLEUi(e) and let `i(~w) = EP~w[Ai(e)] = EeE 4 [exp(~w · ~hi(e))Ai(e)] Ee/E£i exp(~w · ~hi(e&apos;)) This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gr</context>
</contexts>
<marker>Zens, Hasan, Ney, 2007</marker>
<rawString>Richard Zens, Sˇasa Hasan, and Hermann Ney. 2007. A systematic comparison of training criteria for statistical machine translation. In EMNLP, pages 524–532.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>