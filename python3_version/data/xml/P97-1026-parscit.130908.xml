<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001803">
<title confidence="0.985756">
Sentence Planning as Description Using Tree Adjoining Grammar *
</title>
<author confidence="0.99742">
Matthew Stone Christine Doran
</author>
<affiliation confidence="0.999764">
Department of Computer and Information Science Department of Linguistics
University of Pennsylvania University of Pennsylvania
</affiliation>
<bodyText confidence="0.4699015">
Philadelphia, PA 19014 Philadelphia, PA 19014
mat thew@linc . cis . upenn • edu cdoran@linc . cis .upenn. edu
</bodyText>
<sectionHeader confidence="0.964807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883076923077">
We present an algorithm for simultaneously
constructing both the syntax and semantics of
a sentence using a Lexicalized Tree Adjoin-
ing Grammar (LTAG). This approach captures
naturally and elegantly the interaction between
pragmatic and syntactic constraints on descrip-
tions in a sentence, and the inferential interac-
tions between multiple descriptions in a sen-
tence. At the same time, it exploits linguis-
tically motivated, declarative specifications of
the discourse functions of syntactic construc-
tions to make contextually appropriate syntac-
tic choices.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999740306122449">
Since (Meteer, 1991), researchers in natural language
generation have recognized the need to refine and re-
organize content after the rhetorical organization of ar-
guments and before the syntactic realization of phrases.
This process has been named sentence planning (Ram-
bow and Korelsky, 1992). Broadly speaking, it involves
aggregating content into sentence-sized units, and then
selecting the lexical and syntactic elements that are used
in realizing each sentence. Here, we consider this second
process.
The challenge lies in integrating constraints from syn-
tax, semantics and pragmatics. Although most generation
systems pipeline decisions (Reiter, 1994), we believe the
most efficient and flexible way to integrate constraints
in sentence planning is to synchronize the decisions. In
this paper, we provide a natural framework for dealing
with interactions and ensuring contextually appropriate
output in a single pass. As in (Yang et al., 1991), Lex-
icalized Tree Adjoining Grammar (LTAG) provides an
The authors thank Aravind Joshi, Mark Steedman, Martha
Palmer, Ellen Prince, Owen Rambow, Mike White, Betty
Birner, and the participants of INLG96 for their helpful com-
ments on various incarnations of this work. This work has been
supported by NSF and IRCS graduate fellowships, NSF grant
NSF-STC SBR 8920230, ARPA grant N00014-94 and ARO
grant DAAH04-94-G0426.
abstraction of the combinatorial properties of words. We
combine LTAG syntax with declarative specifications of
semantics and pragmatics of words and constructions, so
that we can build the syntax and semantics of sentences
simultaneously. To drive this process, we take descrip-
tion as the paradigm for sentence planning. Our planner,
SPUD (Sentence Planner Using Descriptions), takes in a
collection of goals to achieve in describing an event or
state in the world; SPUD incrementally and recursively
applies lexical specifications to determine which entities
to describe and what information to include about them.
Our system is unique in the streamlined organization of
the grammar, and in its evaluation both of contextual ap-
propriateness of pragmatics and of descriptive adequacy
of semantics.
The organization of the paper is as follows. In sec-
tion 2, we review research on generating referring ex-
pressions and motivate our treatment of sentences as re-
ferring expressions. Then, in section 3, we present the
linguistic underpinnings of our work. In section 4, we
describe our algorithm and its operation on an example.
Finally, in section 5 we compare our system with related
approaches.
</bodyText>
<sectionHeader confidence="0.655141" genericHeader="method">
2 Sentences as referring expressions
</sectionHeader>
<bodyText confidence="0.993215">
Our proposal is to treat the realization of sentences as
parallel to the construction of referring expressions, and
thereby bring to bear modern discourse-oriented theories
of semantics and the idea that language use is INTEN-
TIONAL ACTION.
Semantically, a DESCRIPTION D is j ust an open formula.
D applies to a sequence of entities when substituting them
for the variables in D yields a true formula. D REFERS to
C just in case it distinguishes c from its DISTRACTORS—
that is D applies to c but to no other salient alternatives.
Given a sufficiently rich logical language, the meaning
of a natural language sentence can be represented as a
description in this sense, by assuming sentences refer to
entities in a DISCOURSE MODEL, cf. alternative semantics
(Karttunen and Peters, 1979; Rooth, 1985).
Pragmatic analyses of referring expressions model
speakers as PLANNING those expressions to achieve sev-
eral different kinds of intentions (Donellan, 1966; Appelt,
</bodyText>
<page confidence="0.997664">
198
</page>
<bodyText confidence="0.999703617647059">
1985; ICronfeld, 1986). Given a set of entities to describe
and a set of intentions to achieve in describing them, a
plan is constructed by applying operators that enrich the
content of the description until all intentions are satisfied.
Recent work on generating definite referring NPs (Reiter,
1991; Dale and Haddock, 1991; Reiter and Dale, 1992;
Horacek, 1995) has emphasized how circumscribed in-
stantiations of this procedure can exploit linguistic con-
text and convention to arrive quickly at short, unambigu-
ous descriptions. For example, (Reiter and Dale, 1992)
apply generalizations about the salience of properties of
objects and conventions about what words make base-
level attributions to incrementally select words for inclu-
sion in a description. (Dale and Haddock, 1991) use a
constraint network to represent the distractors described
by a complex referring NP, and incrementally select a
property or relation that rules out as many alternatives as
possible. Our approach is to extend such NP planning
procedures to apply to sentences, using TAG syntax and
a rich semantics.
Treating sentences as referring expressions allows us
to encompass the strengths of many disparate proposals.
Incorporating material into descriptions of a variety of
entities until the addressee can infer desired conclusions
allows the sentence planner to enrich input content, so
that descriptions refer successfully (Dale and Haddock,
1991) or reduce it, to eliminate redundancy (McDonald,
1992). Moreover, selecting alternatives on the basis of
their syntactic, semantic, and pragmatic contributions to
the sentence using TAG allows the sentence planner to
choose words in tandem with appropriate syntax (Yang et
al., 1991), in a flexible order (Elhadad and Robin, 1992),
and, if necessary, in conventional combinations (Smadja
and McKeown, 1991; Wanner, 1994).
</bodyText>
<sectionHeader confidence="0.970654" genericHeader="method">
3 Linguistic Specifications
</sectionHeader>
<bodyText confidence="0.999826047619047">
Realizing this procedure requires a declarative specifica-
tion of three kinds of information: first, what operators
are available and how they may combine; second, how
operators specify the content of a description; and third,
how operators achieve pragmatic effects. We represent
operators as elementary trees in LTAG, and use TAG op-
erations to combine them; we give the meaning of each
tree as a formula in an ontologically promiscuous rep-
resentation language; and, we model the pragmatics of
operators by associating with each tree a set of discourse
constraints describing when that operator can and should
be used.
Other frameworks have the capability to make com-
parable specifications; for example, HPSG (Pollard and
Sag, 1994) feature structures describe syntax (suscAT),
semantics (CONTENT) and pragmatics (CONTEXT). We
choose TAG because it enables local specification of syn-
tactic dependencies in explicit constructions and flexibil-
ity in incorporating modifiers; further, it is a constrained
grammar formalism with tractable computational proper-
ties.
</bodyText>
<subsectionHeader confidence="0.999522">
3.1 Syntactic specification
</subsectionHeader>
<bodyText confidence="0.999983545454545">
TAG (Joshi et al., 1975) is a grammar formalism built
around two operations that combine pairs of trees, SUB-
STITUTION and ADJOINING. A TAG grammar consists of
a finite set of ELEMENTARY trees, which can be combined
by these substitution and adjoining operations to produce
derived trees recognized by the grammar. In substitu-
tion, the root of the first tree is identified with a leaf of the
second tree, called the substitution site. Adjoining is a
more complicated splicing operation, where the first tree
replaces the subtree of the second tree rooted at a node
called the adjunction site; that subtree is then substituted
back into the first tree at a distinguished leaf called the
FOOT node. Elementary trees without foot nodes are
called INITIAL trees and can only substitute; trees with
foot nodes are called AUXILIARY trees, and must adjoin.
(The symbol marks substitution sites, and the symbol
* marks the foot node.) Figure 1(a) shows an initial tree
representing the book. Figure 1(b) shows an auxiliary
tree representing the modifier syntax, which could adjoin
into the tree for the book to give the syntax book.
Our grammar incorporates two additional principles.
First, the grammar is LEXICALIZED (Schabes, 1990): each
elementary structure in the grammar contains at least
one lexical item. Second, our trees include FEATURES,
following (Vijay-Shanker, 1987).
LTAG elementary trees abstract the combinatorial
properties of words in a linguistically appealing way. All
predicate-argument structures are localized within a sin-
gle elementary tree, even in long-distance relationships,
so elementary trees give a natural domain of locality
over which to state semantic and pragmatic constraints.
The LTAG formalism does not dictate particular syntactic
analyses; ours follow basic GB conventions.
</bodyText>
<subsectionHeader confidence="0.9989">
3.2 Semantics
</subsectionHeader>
<bodyText confidence="0.999985681818182">
We specify the semantics of trees by applying two prin-
ciples to the LTAG formalism. First, we adopt an ONTO-
LOGICALLY PROMISCUOUS representation (Hobbs, 1985)
that includes a wide variety of types of entities. On-
tological promiscuity offers a simple syntax-semantics
interface. The meaning of a tree is just the CONJUNCTION
of the meanings of the elementary trees used to derive it,
once appropriate parameters are recovered. Such flat se-
mantics is enjoying a resurgence in NLP; see (Copestake
et al., 1997) for an overview and formalism. Second,
we constrain these parameters syntactically, by labeling
each syntactic node as supplying information about a par-
ticular entity or collection of entities, as in Jackendoff&apos;s
X-bar semantics (Jackendoff, 1990). A node X:x (about
X) can only substitute or adjoin into another node with the
same label. These semantic parameters are instantiated
using a knowledge base (cf. figure 7).
For Jackendoff, noun phrases describe ordinary indi-
viduals, while PPs describe PLACES or PATHS and VPs
describe ACTIONS and EVENTUALITIES (in terms of a Re-
ichenbachian reference point). Under these assumptions,
the trees of figure 1. are elaborated for semantics as in
</bodyText>
<page confidence="0.978994">
199
</page>
<figure confidence="0.999693857142857">
NP
DetP N
D book
the
(a)
NPI S
Z/N
NP1. VP
/\
V NP
syntax
(b)
have e
(c)
</figure>
<figureCaption confidence="0.99829">
Figure 1: Sample LTAG trees: (a) NP, (b) Noun-Noun Compound, (c) Topicalized Transitive
</figureCaption>
<figure confidence="0.999248111111111">
NP &lt;1&gt;x S &lt;1&gt;&lt;r,having&gt;
N &lt;1&gt;x
N: syntax N&amp;quot; : &lt;1&gt;
syntax
concerns(x, syntax)
(b)
NP 1 : &lt;2&gt; havee S &lt;1&gt;
NP1 : haver VP: &lt;1&gt;
V NP : &lt;2&gt;
I
/have/
during(r, having) A have(having, haver, havee)
(c)
DetP N:&lt;1&gt;
Det book
the
book(X)
(a)
</figure>
<figureCaption confidence="0.999866">
Figure 2: LTAG trees with semantic specifications
</figureCaption>
<bodyText confidence="0.989248">
figure 2. Ontological promiscuity makes it possible to
explore more complicated analyses in this general frame-
work. For example, in (Stone and Doran, 1996), we use
reference to properties, actions and belief contexts (Bal-
lim etal., 1991) to describe semantic collocations (Puste-
jovsky, 1991) and idiomatic composition (Nunberg etal.,
1994).
</bodyText>
<subsectionHeader confidence="0.99907">
3.3 Pragmatics
</subsectionHeader>
<bodyText confidence="0.999959351351351">
Different constructions make different assumptions about
the status of entities and propositions in the discourse,
which we model by including in each tree a specification
of the contextual conditions under which use of the tree
is pragmatically licensed. We have selected four repre-
sentative pragmatic distinctions for our implementation;
however, the framework does not commit one to the use
of particular theories.
We use the following distinctions. First, entities differ
in NEWNESS (Prince, 1981). At any point, an entity is ei-
ther new or old to the HEARER and either new or old to the
DISCOURSE. Second, entities differ in SALIENCE (Grosz
and Sidner, 1986; Grosz et al., 1995). Salience assigns
each entity a position in a partial order that indicates
how accessible it is for reference in the current con-
text. Third, entities are related by salient PARTIALLY-
ORDERED SET (POSET) RELATIONS to other entities in the
context (Hirschberg, 1985). These relations include part
and whole, subset and superset, and membership in a
common class. Finally, the discourse may distinguish
some OPEN PROPOSITIONS (propositions containing free
variables) as being under discussion (Prince, 1986). We
assume that information of these four kinds is available
in a model of the current discourse state.
The applicability conditions of constructions can freely
make reference to this information. In particular, NP trees
include the determiner (the determiner does not have a
separate tree), the head noun, and pragmatic conditions
that match the determiner with the status of the entity
in context, as in 3(a). Following (Gundel et al., 1993),
the definite article the may be used when the entity is
UNIQUELY IDENTIFIABLE in the discourse model, i.e. the
hearer knows or can infer the existence of this entity and
can distinguish it from any other hearer-old entity of equal
or greater salience. (Note that this test only determines
the status of the entity in context; we ensure separately
that the sentence includes enough content to distinguish
</bodyText>
<page confidence="0.949764">
200
</page>
<figure confidence="0.998097857142857">
N : &lt;1&gt;
DetP
Det book
NP : &lt;1&gt;x
the
book(x)
(unique-id (x))
(a)
N &lt;1&gt;x
syntax
concerns(x, syntax)
[always applicable]
(b)
S &lt;1&gt;&lt;r,having&gt;
NP 1 &lt;2&gt;havee S : &lt;1&gt;
NP 1 : haver VP: &lt;1&gt;
V NP :&lt;2
/have/
during(r, having) A have(having, haver, havee)
(in-poset(havee), in-op(have(having, haver, havee)))
(c)
</figure>
<figureCaption confidence="0.999924">
Figure 3: LTAG trees with semantic and pragmatic specifications
</figureCaption>
<bodyText confidence="0.999380266666667">
the entity from all its alternatives.) In contrast, the indef-
inite articles, a, an, and 0, are used for entities that are
NOT uniquely identifiable.
S trees specify the main verb and the number and po-
sition of its arguments. Our S trees specify the unmarked
SVO order or one of a number of fancy variants: topical-
ization (TOP), left-dislocation (LD), and locative inversion
(INV). We follow the analysis of TOP in (Ward, 1985).
For Ward, TOP is not a topic-marking construction at all.
Rather, TOP is felicitous as long as (1) the fronted NP is in
a salient poset relation to the previous discourse and (2)
the utterance conveys a salient open proposition which
is formed by replacing the tonically stressed constituent
with a variable (3(c)). Likewise, we follow (Prince, 1993)
and (Birner, 1992) for LD and INV respectively.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="method">
4 SPUD
</sectionHeader>
<subsectionHeader confidence="0.998973">
4.1 The algorithm
</subsectionHeader>
<bodyText confidence="0.999975877192982">
Our system takes two types of goals. First, goals of
the form distinguish x as cat instruct the algorithm to
construct a description of entity x using the syntactic
category cat. If x is uniquely identifiable in the discourse
model, then this goal is only satisfied when the meaning
planned so far distinguishes x for the hearer. If x is hearer
new, this goal is satisfied by including any constituent
of type cat. Second, goals of the form communicate p
instruct the algorithm to include the proposition p. This
goal is satisfied as long as the sentence IMPLIES p given
shared common-sense knowledge.
In each iteration, our algorithm must determine the ap-
propriate elementary tree to incorporate into the current
description. It performs this task in two steps to take
advantage of the regular associations between words and
trees in the lexicon. Sample lexical entries are shown
in figure 4. They associate a word with the semantics
of the word, special pragmatic restrictions on the use of
the word, and a set of trees that describe the combina-
tory possibilities for realizing the word and may impose
additional pragmatic restrictions. Tree types are shared
between lexical items (figure 5). This allows us to spec-
ify the pragmatic constraints associated with the tree type
once, regardless of which verb selects it. Moreover, we
can determine which tree to use by looking at each tree
ONCE per instantiation of its arguments, even when the
same tree is associated with multiple lexical items.
Hence, the first step is to identify applicable lexical
entries by meaning: these items must truly and appropri-
ately describe some entity; they must anchor trees that can
substitute or adjoin into a node that describes the entity;
and they must distinguish entities from their distractors
or entail required information. Then, the second step
identifies which of the associated trees are applicable,
by testing their pragmatic conditions against the current
representation of discourse. The algorithm identifies the
combinations of words and trees that satisfy the most
communicate goals and eliminate the most distractors.
From these, it selects the entry with the most specific se-
mantic and pragmatic licensing conditions. This means
that the algorithm generates the most marked licensed
form. In (Stone and Doran, 1996) we explore the use
of additional factors, such as attentional state and lexical
preferences, in this step.
The new tree is then substituted or adjoined into the
existing tree at the appropriate node. The entry may
specify additional goals, because it describes one entity
in terms of a new one. These new goals are added to the
current goals, and then the algorithm repeats.
Note that this algorithm performs greedy search. To
avoid backtracking, we choose uninflected forms. Mor-
phological features are set wherever possible as a result
of the general unification processes in the grammar; the
inflected form is determined from the lemma and its as-
sociated features in a post-processing step.
The specification of this algorithm is summarized in
the following pseudocode:
</bodyText>
<page confidence="0.977108">
201
</page>
<figure confidence="0.803494">
STEM SEMANTICS SYNTAX PRAGMATICS
/buy/ S buyer/buy/bought /from/seller, etc. register(informal)
/sell/ S seller /selU bought /to/ buyer, etc. register(informal)
/purchase/ S buyer/purchase/bought/from/seller, etc. register(formal)
/book/ book(k) /a/ /book/, etc. [always possible]
S = buy(buying,buyer,seller,bought)
</figure>
<figureCaption confidence="0.865153">
Figure 4: Sample entries from the lexicon
</figureCaption>
<table confidence="0.995165">
SUBCAT FRAME TREES PRAGMATICS
Intransitive Active [always possible]
Transitive Active [always possible]
Topicalized Object in-poset(obj), in-op(event)
Left-Dislocated Object in-poset(obj)
Ditransitive Active [always possible]
Topicalized Dir Object in-poset(dir obj), in-op(event)
Left-Dislocated Dir Object in-poset(dir obj)
PP Predicative Active [always possible]
Locative Inversion newer-than(subj,loc)
etc.
</table>
<figureCaption confidence="0.994015">
Figure 5: Sample entries from the tree database
</figureCaption>
<bodyText confidence="0.963885625">
until goals are satisfied:
determine which uninflected forms apply;
determine which associated trees apply;
evaluate progress towards goals;
incorporate most specific, best ( form, tree):
perform adjunction or substitution;
conjoin new semantics;
add any additional goals;
</bodyText>
<subsectionHeader confidence="0.98467">
4.2 The system
</subsectionHeader>
<bodyText confidence="0.999908857142857">
SPUD&apos;S grammar currently includes a range of syntactic
constructions, including adjective and PP modification,
relative clauses, idioms and various verbal alternations.
Each is associated with a semantic and pragmatic specifi-
cation as discussed above and illustrated in figures 4 and
5. These linguistic specifications can apply across many
domains.
In each domain, an extensive set of inferences, pre-
sumed known in common with the user, are required to
ensure appropriate behavior. We use logic programming
to capture these inferences. In our domain, the system
has the role of a librarian answering patrons&apos; queries.
Our specifications define: the properties and identities of
objects (e.g., attributes of books, parts of the library); the
taxonomic relationships among terms (e.g., that a service
desk is an area but not a room); and the typical span and
course of events in the domain (e.g., rules about how
to check out books). This information is complete and
available for each lexical entry. Of course, SPUD also
represents its private knowledge about the domain. This
includes facts like the status of books in the library.
</bodyText>
<subsectionHeader confidence="0.99947">
4.3 An example
</subsectionHeader>
<bodyText confidence="0.970497">
Suppose our system is given the task of answering the
following question:
</bodyText>
<figure confidence="0.5078365">
(1) Do you have the books for Syntax 551 and
Pragmatics 590?
</figure>
<figureCaption confidence="0.56676725">
Figure 6 shows part of the discourse model after process-
ing the question. The two books, the set they comprise
(introducing a poset relation), and the library are men-
tioned in (1). Hence, these entities must be both hearer-
old and discourse-old. As in centering (Grosz et al.,
1995), the subject is taken to be the most salient entity.
Finally, the meaning of the question becomes a salient
open proposition.
</figureCaption>
<bodyText confidence="0.999355684210526">
On the basis of the knowledge in figure 7, a rhetor-
ical planner might decide to answer by describing state
have27 as an Sand lose5 likewise. To construct its refer-
ence to have27, SPUD first determines which lexical and
syntactic options are available. Using the lexicon and in-
formation about have27 available from figure 7(b), SPUD
determines that, of lemmas that truthfully and appropri-
ately describe have27 as an S. /have/ has the most spe-
cific licensing conditions. The tree set for/have/includes
unmarked, LD and TOP trees. All are licensed, because
of the poset relation R between book19 and books and
the salient open proposition 0. We choose TOP, the tree
with the most specific condition—TOP requires R and 0,
while LD requires only R and the unmarked form has no
requirements.
Thus, a topicalized /have/ tree, appropriately instan-
tiated as shown in figure 8, is added to the description.
The tree refers to three new entities, the object book19,
the subject library and the reference point r of the tense.
</bodyText>
<page confidence="0.997468">
202
</page>
<note confidence="0.985662666666667">
STATUS ENTITIES
DISCOURSE OLD: book] 9, book2, books, library, patron
HEARER OLD: bookI9, book2, books, library, patron
SALIENCE: {library, patron} &gt; {book/9, book2, books}
POSET RELATIONS: book] 9 MEMBER-OF books; book2 MEMBER-OF books
OPEN PROPOSITION: library X have Y: X={ does/doesn&apos;t}; YE books.
</note>
<figureCaption confidence="0.997881">
Figure 6: Discourse model for the example
</figureCaption>
<figure confidence="0.9952637">
book(book19)
book(book2)
concerns(book19, syntax)
concerns(book2, pragmatics)
(a) Common Knowledge
have(have27, library, book19)
lost(lose5, book2)
during (have27, now)
during(lose5, now)
(b) Speaker&apos;s Knowledge
</figure>
<figureCaption confidence="0.996049">
Figure 7: Knowledge bases for the example
</figureCaption>
<equation confidence="0.633540166666667">
S : &lt;1&gt; &lt;r,have27&gt;
NP1 : &lt;2&gt; booki 9 : &lt;1&gt;
NPJ. : library VP : &lt;1&gt;
V NP : &lt;2&gt;
/have/
during(r, have27) A have(have27, library, book19)
</equation>
<figureCaption confidence="0.988074">
Figure 8: The first tree incorporated into the description
of have27.
</figureCaption>
<bodyText confidence="0.999810772727273">
Subgoals of distinguishing these entities are introduced.
Other constructions that describe one entity in terms of
another, such as complex NPs, relative clauses and se-
mantic collocations, are also handled this way by SPUD.
The algorithm now selects the goal of describing
book19 as an NP. Again, the knowledge base is con-
sulted to select NP lemmas that truly describe bookl 9.
The best is /book/. (Note that SPUD would use it if either
the verb or the discourse context ruled out all distrac-
tors.) The tree set for /book/ includes trees with definite
and indefinite determiners; since the hearer can uniquely
identify book19, the definite tree is selected and substi-
tuted as the leftmost NP, as in figure 9.
The goal of distinguishing book19 is still not satis-
fied, however; as far as the hearer knows, both book19
and book2 could match the current description. We con-
sult the knowledge base for further information about
book19. The modifier entry for the lexical item /syntax/
can apply to the new N node; its tree adjoins there, giving
the tree in figure 10. Note that because trees are lexi-
calized and instantiated, and must unify with the existing
derivation, SPUD can enforce collocations and idiomatic
</bodyText>
<figure confidence="0.838439714285714">
S : &lt;1&gt;&lt;r,have27&gt;
NP &lt;2&gt;book19 S :&lt;1&gt;.
DetP N : &lt;2&gt; NP/. : library VP : &lt;1&gt;
Det book V NP : &lt;2&gt;
the /have/
during(r, have27)
have(have27, library, book19) A book(book19)
</figure>
<figureCaption confidence="0.9205885">
Figure 9: The description of have27 after substituting
the book.
</figureCaption>
<figure confidence="0.988899545454546">
S : &lt;1&gt;&lt;r.hava27&gt;
NP : 4&gt;book19
Vet? N ; &lt;2&gt; NI&apos; 1 : library
Del N : syntax N :d&gt; VP &lt;1&gt;
I_______.-/\............
V NP :.c2&gt;
:
I I I I I
the syntax book /have/ c
during(r, have27) A have(have27, library, book19) A
book(book19) A concerns(book19, syntax)
</figure>
<figureCaption confidence="0.919333">
Figure 10: The tree with the complete description of
book19.
</figureCaption>
<bodyText confidence="0.99874725">
composition in steps like this one.
Now we describe the library. Since we ARE the library,
the lexical item /we/ is chosen. Finally, we describe
r. Consulting the knowledge base, we determine that
r is now, and that the present tense morpheme applies.
For uniformity with auxiliary verbs, we represent it as a
separate tree, in this case with a null head, which assigns
a morphological feature to the main verb. This gives the
</bodyText>
<equation confidence="0.814954">
S : &lt;1&gt;
</equation>
<page confidence="0.99625">
203
</page>
<table confidence="0.9220623">
S aNsve27.
PIP : 4. bookl 9 S
DaP N 4, NP 4&gt; IIINWY VP .1.
N syntax N:4, N 4. VP
the syntax book V NP
.
/have/ e
during(r, have27) A have(have27, library, book19)
book(book19) A concerns(book19, syntax) A
we(library) A pres(r, have27)
</table>
<figureCaption confidence="0.997921">
Figure 11: The final tree.
</figureCaption>
<bodyText confidence="0.999538380952381">
tree in figure 11, representing the sentence:
(2) The syntax book, we have.
All goals are now satisfied. Note that the semantics has
been accumulated incrementally and straightforwardly in
parallel with the syntax.
To illustrate the role of inclusion goals, let us suppose
that the system also knows that book19 is on reserve
in the state have27. Given the additional input goal of
communicating this fact, the algorithm would proceed as
before, deriving The syntax book we have. However, the
new goal would still be unsatisfied; in the next iteration,
the PP on reserve would be adjoined into the tree to satisfy
it: The syntax book, we have on reserve. Because TAG
allows adjunction to apply at anytime, flexible realization
of content is facilitated without need for sophisticated
back-tracking (Elhadad and Robin, 1992).
The processing of this example may seem simple, but
it illustrates the way in which SPUD integrates syntac-
tic, semantic and pragmatic knowledge in realizing sen-
tences. We tackle additional examples in (Stone and
Doran, 1996).
</bodyText>
<sectionHeader confidence="0.957939" genericHeader="method">
5 Comparison with related work
</sectionHeader>
<bodyText confidence="0.999422594594594">
The strength of the present work is that it captures a num-
ber of phenomena discussed elsewhere separately, and
does so within a unified framework. With its incremental
choices and its emphasis on the consequences of func-
tional choices in the grammar, our algorithm resembles
the networks of systemic grammar (Mathiessen, 1983;
Yang et al., 1991). However, unlike systemic networks,
our system derives its functional choices dynamically us-
ing a simple declarative specification of function. Like
many sentence planners, we assume that there is a flexible
• association between the content input to a sentence plan-
ner and the meaning that comes out. Other researchers
(Nicolov et al., 1995; Rubinoff, 1992) have assumed that
this flexibility comes from a mismatch between input
content and grammatical options. In our system, such
differences arise from the referential requirements and
inferential opportunities that are encountered.
Previous authors (McDonald and Pustejovsky, 1985;
Joshi, 1987) have noted that TAG has many advantages
for generation as a syntactic formalism, because of its
localization of argument structure. (Joshi, 1987) states
that adjunction is a powerful tool for elaborating descrip-
tions. These aspects of TAGs are crucial to SPUD, as they
are to (McDonald and Pustejovsky, 1985; Joshi, 1987;
Yang et al., 1991; Nicolov et al., 1995; Wahlster et al.,
1991; Danlos, 1996). What sets SPUD apart is its simul-
taneous construction of syntax and semantics, and the
tripartite, lexicalized, declarative grammatical specifica-
tions for constructions it uses. Two contrasts should be
emphasized in this regard. (Shieber et al., 1990; Shieber
and Schabes, 1991) construct a simultaneous derivation
of syntax and semantics but they do not construct the
semantics—it is an input to their system. (Prevost and
Steedman, 1993; Hoffman, 1994) represent syntax, se-
mantics and pragmatics in a lexicalized framework, but
concentrate on information structure rather than the prag-
matics of particular constructions.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999872090909091">
Most generation systems pipeline pragmatic, semantic,
lexical and syntactic decisions (Reiter, 1994). With the
right formalism, constructing pragmatics, semantics and
syntax simultaneously is easier and better. The approach
elegantly captures the interaction between pragmatic and
syntactic constraints on descriptions in a sentence, and the
inferential interactions between multiple descriptions in a
sentence. At the same time, it exploits linguistically mo-
tivated, declarative specifications of the discourse func-
tions of syntactic constructions to make contextually ap-
propriate syntactic choices.
</bodyText>
<sectionHeader confidence="0.997842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999162857142857">
D. Appelt. 1985. Planning English Sentences. Cambridge
University Press.
A. Ballim, Y. Wilks, and J. Barnden. 1991. Belief ascription,
metaphor, and intensional identification. Cognitive Science,
15:133-171.
B. Birner. 1992. The Discourse Function of Inversion in En-
glish. Ph.D. thesis, Northwestern University.
A. Copestake, D. Flickinger, and I. A. Sag. 1997. Min-
imal Recursion Semantics An Introduction. MS, CSLI,
http://hpsg.stanford.edu/hpsg/sag.html.
R. Dale and N. Haddock. 1991. Content determination in the
generation of referring expressions. Computational Intelli-
gence, 7(4):252-265.
L. Danlos. 1996. G-TAG: A formalism for Text Generation
inspired from Tree Adjoining Grammar: TAG issues. Un-
published manuscript, TALANA, Universite Paris 7.
K. Donellan. 1966. Reference and definite description. Philo-
sophical Review, 75:281-304.
M. Elhadad and J. Robin. 1992. Controlling content realiza-
tion with functional unification grammars. In Dale, Hovy,
Rosner, and Stock, editors, Aspects of Automated Natural
</reference>
<page confidence="0.993597">
204
</page>
<reference confidence="0.998971300884956">
Language Generation: 6th International Workshop on Nat-
ural Language Generation, pages 89-104. Springer Verlag.
B. Grosz and C. Sidner. 1986. Attention, intentions, and the
structure of discourse. Computational Linguistics, 12:175-
204.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering:
A framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203-225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive
status and the form of referring expressions in discourse.
Language, 69(2):274-307.
J. Hirschberg. 1985. A Theory of Scalar lmplicature. Ph.D.
thesis, University of Pennsylvania.
J. R. Hobbs. 1985. Ontological promiscuity. In ACL, pages
61-69.
B. Hoffman. 1994. Generating context-appropriate word or-
ders in Turkish. In Seventh International Generation Work-
shop.
H. Horacek. 1995. More on generating referring expressions.
In Fifth European Workshop on Natural Language Genera-
tion, pages 43-58, Leiden.
R. S. Jackendoff. 1990. Semantic Structures. MIT Press.
A. K. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct
grammars. Journal of the Computer and System Sciences,
10:136-163.
A. K. Joshi. 1987. The relevance of tree adjoining grammar to
generation. In Kempen, editor, Natural Language Genera-
tion, pages 233-252. Martinus Nijhoff Press, Dordrect, The
Netherlands.
L. Karttunen and S. Peters. 1979. Conventional implicature.
In Oh and Dineen, editors, Syntax and Semantics 11: Pre-
supposition. Academic Press.
A. Kronfeld. 1986. Donellan&apos;s distinction and a computational
model of reference. In ACL, pages 186-191.
C. M. I. M. Mathiessen. 1983. Systemic grammar in computa-
tion: the Nigel case. In EACL, pages 155-164.
D. D. McDonald and J.. Pustejovsky. 1985. TAG&apos;s as a gram-
matical formalism for generation. In ACL, pages 94-103.
D. McDonald. 1992. Type-driven suppression of redundancy
in the generation of inference-rich reports. In Dale, Hovy,
Rosner, and Stock, editors, Aspects of Automated Natural
Language Generation: 6th International Workshop on Nat-
ural Language Generation, pages 73-88. Springer Verlag.
M. W. Meteer. 1991. Bridging the generation gap between text
planning and linguistic realization. Computational Intelli-
gence, 7(4):296-304.
N. Nicolov, C. Mellish, and G. Ritchie. 1995. Sentence genera-
tion from conceptual graphs. In W. Rich G. Ellis, R. Levinson
and F. Sowa, editors, Conceptual Structures: Applications,
Implementation and Theory, pages 74-88. Springer.
G. Nunberg, 1. A. Sag, and T. Wasow. 1994. Idioms. Language,
70(3):491-538.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Structure
Grammar. CSLI.
S. Prevost and M. Steedman. 1993. Generating contextually
appropriate intonation. In EACL.
Prince. 1981. Toward a taxonomy of given-new information.
In P. Cole, editor, Radical Pragmatics. Academic Press.
Prince. 1986. On the syntactic marking of presupposed open
propositions. In CLS, pages 208-222, Chicago. CLS.
Prince. 1993. On the functions of left dislocation.
Manuscript, University of Pennsylvania.
Pustejovsky. 1991. The generative lexicon. Computational
Linguistics, l7(3):409-441.
Rambow and T. Korelsky. 1992. Applied text generation.
In ANLP, pages 40-47.
Reiter and R. Dale. 1992. A fast algorithm for the generation
of referring expressions. In Proceedings of COLING, pages
232-238.
Reiter. 1991. A new model of lexical choice for nouns.
Computational Intelligence, 7(4):240-251.
Reiter. 1994. Has a consensus NL generation architecture
appeared, and is it psycholinguistically plausible? In Pro-
ceedings of the Seventh International Workshop on Natural
Language Generation, pages 163-170.
Rooth. 1985. Association with focus. Ph.D. thesis, Univer-
sity of Massachusetts.
Rubinoff. 1992. Integrating text planning and linguistic
choice by annotating linguistic structures. In Dale, Hovy,
Rosner, and Stock, editors, Aspects of Automated Natural
Language Generation: 6th International Workshop on Nat-
ural Language Generation, pages 45-56. Springer Verlag.
Schabes. 1990. Mathematical and Computational Aspects
of Lexicalized Grammars. Ph.D. thesis, University of Penn-
sylvania.
Shieber and Y. Schabes. 1991. Generation and syn-
chronous tree adjoining grammars. Computational Intelli-
gence, 4(7):220-228.
Shieber, G. van Noord, F. Pereira, and R. Moore. 1990.
Semantic-head-driven generation. Computational Linguis-
tics, 16:30-42.
Smadja and K. McKeown. 1991. Using collocations for
language generation. Computational Intelligence, 7(4):229-
239.
Vijay-Shanker. 1987. A Study of Tree Adjoining Grammars.
Ph.D. thesis, University of Pennsylvania.
. Stone and C. Doran. 1996. Paying heed to collocations. In
Eighth International Workshop on Natural Language Gen-
eration 96, pages 91-100.
. Wahlster, E. André, S. Bandyopadhyay, W. Graf, and T. Rist.
1991. WIP: The coordinated generation of multimodal pre-
sentations from a common representation. In Stock, Slack,
and Ortony, editors, Computational Theories of Communi-
cation and their Applications. Springer Verlag.
Wanner. 1994. Building another bridge over the gener-
ation gap. In Seventh International Workshop on Natural
Language Generation, pages 137-144, June.
Ward. 1985. The Semantics and Pragmatics of Preposing.
Ph.D. thesis, University of Pennsylvania.
Yang, K. F. McCoy, and K. Vijay-Shanker. 1991. From
functional specification to syntactic structures: systemic
grammar and tree-adjoining grammar. Computational In-
telligence, 7(4):207-219.
</reference>
<page confidence="0.998863">
205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684426">
<title confidence="0.999782">Sentence Planning as Description Using Tree Adjoining Grammar *</title>
<author confidence="0.999848">Matthew Stone Christine Doran</author>
<affiliation confidence="0.9998345">Department of Computer and Information Science Department of Linguistics University of Pennsylvania University of Pennsylvania</affiliation>
<address confidence="0.999582">Philadelphia, PA 19014 Philadelphia, PA 19014</address>
<email confidence="0.727098">matthew@linc.cis.upenn•educdoran@linc.cis.upenn.edu</email>
<abstract confidence="0.995830285714286">We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<marker>Appelt, 1985</marker>
<rawString>D. Appelt. 1985. Planning English Sentences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ballim</author>
<author>Y Wilks</author>
<author>J Barnden</author>
</authors>
<title>Belief ascription, metaphor, and intensional identification.</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<pages>15--133</pages>
<marker>Ballim, Wilks, Barnden, 1991</marker>
<rawString>A. Ballim, Y. Wilks, and J. Barnden. 1991. Belief ascription, metaphor, and intensional identification. Cognitive Science, 15:133-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Birner</author>
</authors>
<title>The Discourse Function of Inversion in English.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Northwestern University.</institution>
<contexts>
<context position="14464" citStr="Birner, 1992" startWordPosition="2249" endWordPosition="2250">he number and position of its arguments. Our S trees specify the unmarked SVO order or one of a number of fancy variants: topicalization (TOP), left-dislocation (LD), and locative inversion (INV). We follow the analysis of TOP in (Ward, 1985). For Ward, TOP is not a topic-marking construction at all. Rather, TOP is felicitous as long as (1) the fronted NP is in a salient poset relation to the previous discourse and (2) the utterance conveys a salient open proposition which is formed by replacing the tonically stressed constituent with a variable (3(c)). Likewise, we follow (Prince, 1993) and (Birner, 1992) for LD and INV respectively. 4 SPUD 4.1 The algorithm Our system takes two types of goals. First, goals of the form distinguish x as cat instruct the algorithm to construct a description of entity x using the syntactic category cat. If x is uniquely identifiable in the discourse model, then this goal is only satisfied when the meaning planned so far distinguishes x for the hearer. If x is hearer new, this goal is satisfied by including any constituent of type cat. Second, goals of the form communicate p instruct the algorithm to include the proposition p. This goal is satisfied as long as the</context>
</contexts>
<marker>Birner, 1992</marker>
<rawString>B. Birner. 1992. The Discourse Function of Inversion in English. Ph.D. thesis, Northwestern University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>I A Sag</author>
</authors>
<title>Minimal Recursion Semantics An Introduction.</title>
<date>1997</date>
<location>MS, CSLI, http://hpsg.stanford.edu/hpsg/sag.html.</location>
<contexts>
<context position="9769" citStr="Copestake et al., 1997" startWordPosition="1483" endWordPosition="1486">. The LTAG formalism does not dictate particular syntactic analyses; ours follow basic GB conventions. 3.2 Semantics We specify the semantics of trees by applying two principles to the LTAG formalism. First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs, 1985) that includes a wide variety of types of entities. Ontological promiscuity offers a simple syntax-semantics interface. The meaning of a tree is just the CONJUNCTION of the meanings of the elementary trees used to derive it, once appropriate parameters are recovered. Such flat semantics is enjoying a resurgence in NLP; see (Copestake et al., 1997) for an overview and formalism. Second, we constrain these parameters syntactically, by labeling each syntactic node as supplying information about a particular entity or collection of entities, as in Jackendoff&apos;s X-bar semantics (Jackendoff, 1990). A node X:x (about X) can only substitute or adjoin into another node with the same label. These semantic parameters are instantiated using a knowledge base (cf. figure 7). For Jackendoff, noun phrases describe ordinary individuals, while PPs describe PLACES or PATHS and VPs describe ACTIONS and EVENTUALITIES (in terms of a Reichenbachian reference </context>
</contexts>
<marker>Copestake, Flickinger, Sag, 1997</marker>
<rawString>A. Copestake, D. Flickinger, and I. A. Sag. 1997. Minimal Recursion Semantics An Introduction. MS, CSLI, http://hpsg.stanford.edu/hpsg/sag.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>N Haddock</author>
</authors>
<title>Content determination in the generation of referring expressions.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="4799" citStr="Dale and Haddock, 1991" startWordPosition="728" endWordPosition="731">by assuming sentences refer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure can exploit linguistic context and convention to arrive quickly at short, unambiguous descriptions. For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make baselevel attributions to incrementally select words for inclusion in a description. (Dale and Haddock, 1991) use a constraint network to represent the distractors described by a complex referring NP, and incrementally select a property or relation that</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>R. Dale and N. Haddock. 1991. Content determination in the generation of referring expressions. Computational Intelligence, 7(4):252-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>G-TAG: A formalism for Text Generation inspired from Tree Adjoining Grammar: TAG issues.</title>
<date>1996</date>
<institution>Universite Paris</institution>
<note>Unpublished manuscript, TALANA,</note>
<contexts>
<context position="27172" citStr="Danlos, 1996" startWordPosition="4276" endWordPosition="4277">t and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the pragmatics of p</context>
</contexts>
<marker>Danlos, 1996</marker>
<rawString>L. Danlos. 1996. G-TAG: A formalism for Text Generation inspired from Tree Adjoining Grammar: TAG issues. Unpublished manuscript, TALANA, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Donellan</author>
</authors>
<title>Reference and definite description.</title>
<date>1966</date>
<journal>Philosophical Review,</journal>
<pages>75--281</pages>
<contexts>
<context position="4461" citStr="Donellan, 1966" startWordPosition="676" endWordPosition="677">ubstituting them for the variables in D yields a true formula. D REFERS to C just in case it distinguishes c from its DISTRACTORS— that is D applies to c but to no other salient alternatives. Given a sufficiently rich logical language, the meaning of a natural language sentence can be represented as a description in this sense, by assuming sentences refer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure can exploit linguistic context and convention to arrive quickly at short, unambiguous descriptions. For example, (Reiter and Dale, 1992) apply generalizatio</context>
</contexts>
<marker>Donellan, 1966</marker>
<rawString>K. Donellan. 1966. Reference and definite description. Philosophical Review, 75:281-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>Controlling content realization with functional unification grammars.</title>
<date>1992</date>
<booktitle>Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation,</booktitle>
<pages>89--104</pages>
<editor>In Dale, Hovy, Rosner, and Stock, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="6234" citStr="Elhadad and Robin, 1992" startWordPosition="943" endWordPosition="946"> us to encompass the strengths of many disparate proposals. Incorporating material into descriptions of a variety of entities until the addressee can infer desired conclusions allows the sentence planner to enrich input content, so that descriptions refer successfully (Dale and Haddock, 1991) or reduce it, to eliminate redundancy (McDonald, 1992). Moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic contributions to the sentence using TAG allows the sentence planner to choose words in tandem with appropriate syntax (Yang et al., 1991), in a flexible order (Elhadad and Robin, 1992), and, if necessary, in conventional combinations (Smadja and McKeown, 1991; Wanner, 1994). 3 Linguistic Specifications Realizing this procedure requires a declarative specification of three kinds of information: first, what operators are available and how they may combine; second, how operators specify the content of a description; and third, how operators achieve pragmatic effects. We represent operators as elementary trees in LTAG, and use TAG operations to combine them; we give the meaning of each tree as a formula in an ontologically promiscuous representation language; and, we model the </context>
<context position="25518" citStr="Elhadad and Robin, 1992" startWordPosition="4018" endWordPosition="4021">ith the syntax. To illustrate the role of inclusion goals, let us suppose that the system also knows that book19 is on reserve in the state have27. Given the additional input goal of communicating this fact, the algorithm would proceed as before, deriving The syntax book we have. However, the new goal would still be unsatisfied; in the next iteration, the PP on reserve would be adjoined into the tree to satisfy it: The syntax book, we have on reserve. Because TAG allows adjunction to apply at anytime, flexible realization of content is facilitated without need for sophisticated back-tracking (Elhadad and Robin, 1992). The processing of this example may seem simple, but it illustrates the way in which SPUD integrates syntactic, semantic and pragmatic knowledge in realizing sentences. We tackle additional examples in (Stone and Doran, 1996). 5 Comparison with related work The strength of the present work is that it captures a number of phenomena discussed elsewhere separately, and does so within a unified framework. With its incremental choices and its emphasis on the consequences of functional choices in the grammar, our algorithm resembles the networks of systemic grammar (Mathiessen, 1983; Yang et al., 1</context>
</contexts>
<marker>Elhadad, Robin, 1992</marker>
<rawString>M. Elhadad and J. Robin. 1992. Controlling content realization with functional unification grammars. In Dale, Hovy, Rosner, and Stock, editors, Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation, pages 89-104. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--175</pages>
<contexts>
<context position="11923" citStr="Grosz and Sidner, 1986" startWordPosition="1834" endWordPosition="1837">assumptions about the status of entities and propositions in the discourse, which we model by including in each tree a specification of the contextual conditions under which use of the tree is pragmatically licensed. We have selected four representative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of particular theories. We use the following distinctions. First, entities differ in NEWNESS (Prince, 1981). At any point, an entity is either new or old to the HEARER and either new or old to the DISCOURSE. Second, entities differ in SALIENCE (Grosz and Sidner, 1986; Grosz et al., 1995). Salience assigns each entity a position in a partial order that indicates how accessible it is for reference in the current context. Third, entities are related by salient PARTIALLYORDERED SET (POSET) RELATIONS to other entities in the context (Hirschberg, 1985). These relations include part and whole, subset and superset, and membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITIONS (propositions containing free variables) as being under discussion (Prince, 1986). We assume that information of these four kinds is available in a model of</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. Grosz and C. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12:175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="11944" citStr="Grosz et al., 1995" startWordPosition="1838" endWordPosition="1841">atus of entities and propositions in the discourse, which we model by including in each tree a specification of the contextual conditions under which use of the tree is pragmatically licensed. We have selected four representative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of particular theories. We use the following distinctions. First, entities differ in NEWNESS (Prince, 1981). At any point, an entity is either new or old to the HEARER and either new or old to the DISCOURSE. Second, entities differ in SALIENCE (Grosz and Sidner, 1986; Grosz et al., 1995). Salience assigns each entity a position in a partial order that indicates how accessible it is for reference in the current context. Third, entities are related by salient PARTIALLYORDERED SET (POSET) RELATIONS to other entities in the context (Hirschberg, 1985). These relations include part and whole, subset and superset, and membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITIONS (propositions containing free variables) as being under discussion (Prince, 1986). We assume that information of these four kinds is available in a model of the current discours</context>
<context position="20285" citStr="Grosz et al., 1995" startWordPosition="3142" endWordPosition="3145">tion is complete and available for each lexical entry. Of course, SPUD also represents its private knowledge about the domain. This includes facts like the status of books in the library. 4.3 An example Suppose our system is given the task of answering the following question: (1) Do you have the books for Syntax 551 and Pragmatics 590? Figure 6 shows part of the discourse model after processing the question. The two books, the set they comprise (introducing a poset relation), and the library are mentioned in (1). Hence, these entities must be both hearerold and discourse-old. As in centering (Grosz et al., 1995), the subject is taken to be the most salient entity. Finally, the meaning of the question becomes a salient open proposition. On the basis of the knowledge in figure 7, a rhetorical planner might decide to answer by describing state have27 as an Sand lose5 likewise. To construct its reference to have27, SPUD first determines which lexical and syntactic options are available. Using the lexicon and information about have27 available from figure 7(b), SPUD determines that, of lemmas that truthfully and appropriately describe have27 as an S. /have/ has the most specific licensing conditions. The </context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Gundel</author>
<author>N Hedberg</author>
<author>R Zacharski</author>
</authors>
<title>Cognitive status and the form of referring expressions in discourse.</title>
<date>1993</date>
<journal>Language,</journal>
<pages>69--2</pages>
<contexts>
<context position="12892" citStr="Gundel et al., 1993" startWordPosition="1984" endWordPosition="1987">nd membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITIONS (propositions containing free variables) as being under discussion (Prince, 1986). We assume that information of these four kinds is available in a model of the current discourse state. The applicability conditions of constructions can freely make reference to this information. In particular, NP trees include the determiner (the determiner does not have a separate tree), the head noun, and pragmatic conditions that match the determiner with the status of the entity in context, as in 3(a). Following (Gundel et al., 1993), the definite article the may be used when the entity is UNIQUELY IDENTIFIABLE in the discourse model, i.e. the hearer knows or can infer the existence of this entity and can distinguish it from any other hearer-old entity of equal or greater salience. (Note that this test only determines the status of the entity in context; we ensure separately that the sentence includes enough content to distinguish 200 N : &lt;1&gt; DetP Det book NP : &lt;1&gt;x the book(x) (unique-id (x)) (a) N &lt;1&gt;x syntax concerns(x, syntax) [always applicable] (b) S &lt;1&gt;&lt;r,having&gt; NP 1 &lt;2&gt;havee S : &lt;1&gt; NP 1 : haver VP: &lt;1&gt; V NP :&lt;2 </context>
</contexts>
<marker>Gundel, Hedberg, Zacharski, 1993</marker>
<rawString>J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive status and the form of referring expressions in discourse. Language, 69(2):274-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>A Theory of Scalar lmplicature.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12208" citStr="Hirschberg, 1985" startWordPosition="1882" endWordPosition="1883">mentation; however, the framework does not commit one to the use of particular theories. We use the following distinctions. First, entities differ in NEWNESS (Prince, 1981). At any point, an entity is either new or old to the HEARER and either new or old to the DISCOURSE. Second, entities differ in SALIENCE (Grosz and Sidner, 1986; Grosz et al., 1995). Salience assigns each entity a position in a partial order that indicates how accessible it is for reference in the current context. Third, entities are related by salient PARTIALLYORDERED SET (POSET) RELATIONS to other entities in the context (Hirschberg, 1985). These relations include part and whole, subset and superset, and membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITIONS (propositions containing free variables) as being under discussion (Prince, 1986). We assume that information of these four kinds is available in a model of the current discourse state. The applicability conditions of constructions can freely make reference to this information. In particular, NP trees include the determiner (the determiner does not have a separate tree), the head noun, and pragmatic conditions that match the determiner w</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>J. Hirschberg. 1985. A Theory of Scalar lmplicature. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Ontological promiscuity.</title>
<date>1985</date>
<booktitle>In ACL,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="9420" citStr="Hobbs, 1985" startWordPosition="1428" endWordPosition="1429"> 1987). LTAG elementary trees abstract the combinatorial properties of words in a linguistically appealing way. All predicate-argument structures are localized within a single elementary tree, even in long-distance relationships, so elementary trees give a natural domain of locality over which to state semantic and pragmatic constraints. The LTAG formalism does not dictate particular syntactic analyses; ours follow basic GB conventions. 3.2 Semantics We specify the semantics of trees by applying two principles to the LTAG formalism. First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs, 1985) that includes a wide variety of types of entities. Ontological promiscuity offers a simple syntax-semantics interface. The meaning of a tree is just the CONJUNCTION of the meanings of the elementary trees used to derive it, once appropriate parameters are recovered. Such flat semantics is enjoying a resurgence in NLP; see (Copestake et al., 1997) for an overview and formalism. Second, we constrain these parameters syntactically, by labeling each syntactic node as supplying information about a particular entity or collection of entities, as in Jackendoff&apos;s X-bar semantics (Jackendoff, 1990). A</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>J. R. Hobbs. 1985. Ontological promiscuity. In ACL, pages 61-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hoffman</author>
</authors>
<title>Generating context-appropriate word orders in Turkish.</title>
<date>1994</date>
<booktitle>In Seventh International Generation Workshop.</booktitle>
<contexts>
<context position="27628" citStr="Hoffman, 1994" startWordPosition="4344" endWordPosition="4345">re crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the pragmatics of particular constructions. 6 Conclusion Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). With the right formalism, constructing pragmatics, semantics and syntax simultaneously is easier and better. The approach elegantly captures the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the</context>
</contexts>
<marker>Hoffman, 1994</marker>
<rawString>B. Hoffman. 1994. Generating context-appropriate word orders in Turkish. In Seventh International Generation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Horacek</author>
</authors>
<title>More on generating referring expressions. In</title>
<date>1995</date>
<booktitle>Fifth European Workshop on Natural Language Generation,</booktitle>
<pages>43--58</pages>
<location>Leiden.</location>
<contexts>
<context position="4838" citStr="Horacek, 1995" startWordPosition="736" endWordPosition="737">SCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure can exploit linguistic context and convention to arrive quickly at short, unambiguous descriptions. For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make baselevel attributions to incrementally select words for inclusion in a description. (Dale and Haddock, 1991) use a constraint network to represent the distractors described by a complex referring NP, and incrementally select a property or relation that rules out as many alternatives as poss</context>
</contexts>
<marker>Horacek, 1995</marker>
<rawString>H. Horacek. 1995. More on generating referring expressions. In Fifth European Workshop on Natural Language Generation, pages 43-58, Leiden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10017" citStr="Jackendoff, 1990" startWordPosition="1520" endWordPosition="1521">tation (Hobbs, 1985) that includes a wide variety of types of entities. Ontological promiscuity offers a simple syntax-semantics interface. The meaning of a tree is just the CONJUNCTION of the meanings of the elementary trees used to derive it, once appropriate parameters are recovered. Such flat semantics is enjoying a resurgence in NLP; see (Copestake et al., 1997) for an overview and formalism. Second, we constrain these parameters syntactically, by labeling each syntactic node as supplying information about a particular entity or collection of entities, as in Jackendoff&apos;s X-bar semantics (Jackendoff, 1990). A node X:x (about X) can only substitute or adjoin into another node with the same label. These semantic parameters are instantiated using a knowledge base (cf. figure 7). For Jackendoff, noun phrases describe ordinary individuals, while PPs describe PLACES or PATHS and VPs describe ACTIONS and EVENTUALITIES (in terms of a Reichenbachian reference point). Under these assumptions, the trees of figure 1. are elaborated for semantics as in 199 NP DetP N D book the (a) NPI S Z/N NP1. VP /\ V NP syntax (b) have e (c) Figure 1: Sample LTAG trees: (a) NP, (b) Noun-Noun Compound, (c) Topicalized Tra</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>R. S. Jackendoff. 1990. Semantic Structures. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of the Computer and System Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="7467" citStr="Joshi et al., 1975" startWordPosition="1124" endWordPosition="1127">operators by associating with each tree a set of discourse constraints describing when that operator can and should be used. Other frameworks have the capability to make comparable specifications; for example, HPSG (Pollard and Sag, 1994) feature structures describe syntax (suscAT), semantics (CONTENT) and pragmatics (CONTEXT). We choose TAG because it enables local specification of syntactic dependencies in explicit constructions and flexibility in incorporating modifiers; further, it is a constrained grammar formalism with tractable computational properties. 3.1 Syntactic specification TAG (Joshi et al., 1975) is a grammar formalism built around two operations that combine pairs of trees, SUBSTITUTION and ADJOINING. A TAG grammar consists of a finite set of ELEMENTARY trees, which can be combined by these substitution and adjoining operations to produce derived trees recognized by the grammar. In substitution, the root of the first tree is identified with a leaf of the second tree, called the substitution site. Adjoining is a more complicated splicing operation, where the first tree replaces the subtree of the second tree rooted at a node called the adjunction site; that subtree is then substituted</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A. K. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal of the Computer and System Sciences, 10:136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>The relevance of tree adjoining grammar to generation.</title>
<date>1987</date>
<booktitle>Natural Language Generation,</booktitle>
<pages>233--252</pages>
<editor>In Kempen, editor,</editor>
<publisher>Martinus Nijhoff Press,</publisher>
<location>Dordrect, The Netherlands.</location>
<contexts>
<context position="26772" citStr="Joshi, 1987" startWordPosition="4211" endWordPosition="4212">ur system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association between the content input to a sentence planner and the meaning that comes out. Other researchers (Nicolov et al., 1995; Rubinoff, 1992) have assumed that this flexibility comes from a mismatch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>A. K. Joshi. 1987. The relevance of tree adjoining grammar to generation. In Kempen, editor, Natural Language Generation, pages 233-252. Martinus Nijhoff Press, Dordrect, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>S Peters</author>
</authors>
<title>Conventional implicature.</title>
<date>1979</date>
<booktitle>In Oh and Dineen, editors, Syntax and Semantics 11:</booktitle>
<publisher>Presupposition. Academic Press.</publisher>
<contexts>
<context position="4292" citStr="Karttunen and Peters, 1979" startWordPosition="651" endWordPosition="654">iented theories of semantics and the idea that language use is INTENTIONAL ACTION. Semantically, a DESCRIPTION D is j ust an open formula. D applies to a sequence of entities when substituting them for the variables in D yields a true formula. D REFERS to C just in case it distinguishes c from its DISTRACTORS— that is D applies to c but to no other salient alternatives. Given a sufficiently rich logical language, the meaning of a natural language sentence can be represented as a description in this sense, by assuming sentences refer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of th</context>
</contexts>
<marker>Karttunen, Peters, 1979</marker>
<rawString>L. Karttunen and S. Peters. 1979. Conventional implicature. In Oh and Dineen, editors, Syntax and Semantics 11: Presupposition. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kronfeld</author>
</authors>
<title>Donellan&apos;s distinction and a computational model of reference.</title>
<date>1986</date>
<booktitle>In ACL,</booktitle>
<pages>186--191</pages>
<marker>Kronfeld, 1986</marker>
<rawString>A. Kronfeld. 1986. Donellan&apos;s distinction and a computational model of reference. In ACL, pages 186-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M I M Mathiessen</author>
</authors>
<title>Systemic grammar in computation: the Nigel case.</title>
<date>1983</date>
<booktitle>In EACL,</booktitle>
<pages>155--164</pages>
<contexts>
<context position="26102" citStr="Mathiessen, 1983" startWordPosition="4113" endWordPosition="4114">acking (Elhadad and Robin, 1992). The processing of this example may seem simple, but it illustrates the way in which SPUD integrates syntactic, semantic and pragmatic knowledge in realizing sentences. We tackle additional examples in (Stone and Doran, 1996). 5 Comparison with related work The strength of the present work is that it captures a number of phenomena discussed elsewhere separately, and does so within a unified framework. With its incremental choices and its emphasis on the consequences of functional choices in the grammar, our algorithm resembles the networks of systemic grammar (Mathiessen, 1983; Yang et al., 1991). However, unlike systemic networks, our system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association between the content input to a sentence planner and the meaning that comes out. Other researchers (Nicolov et al., 1995; Rubinoff, 1992) have assumed that this flexibility comes from a mismatch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encou</context>
</contexts>
<marker>Mathiessen, 1983</marker>
<rawString>C. M. I. M. Mathiessen. 1983. Systemic grammar in computation: the Nigel case. In EACL, pages 155-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
<author>J Pustejovsky</author>
</authors>
<title>TAG&apos;s as a grammatical formalism for generation.</title>
<date>1985</date>
<booktitle>In ACL,</booktitle>
<pages>94--103</pages>
<contexts>
<context position="26758" citStr="McDonald and Pustejovsky, 1985" startWordPosition="4207" endWordPosition="4210">ver, unlike systemic networks, our system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association between the content input to a sentence planner and the meaning that comes out. Other researchers (Nicolov et al., 1995; Rubinoff, 1992) have assumed that this flexibility comes from a mismatch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two co</context>
</contexts>
<marker>McDonald, Pustejovsky, 1985</marker>
<rawString>D. D. McDonald and J.. Pustejovsky. 1985. TAG&apos;s as a grammatical formalism for generation. In ACL, pages 94-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Type-driven suppression of redundancy in the generation of inference-rich reports.</title>
<date>1992</date>
<booktitle>Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation,</booktitle>
<pages>73--88</pages>
<editor>In Dale, Hovy, Rosner, and Stock, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="5958" citStr="McDonald, 1992" startWordPosition="903" endWordPosition="904">NP, and incrementally select a property or relation that rules out as many alternatives as possible. Our approach is to extend such NP planning procedures to apply to sentences, using TAG syntax and a rich semantics. Treating sentences as referring expressions allows us to encompass the strengths of many disparate proposals. Incorporating material into descriptions of a variety of entities until the addressee can infer desired conclusions allows the sentence planner to enrich input content, so that descriptions refer successfully (Dale and Haddock, 1991) or reduce it, to eliminate redundancy (McDonald, 1992). Moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic contributions to the sentence using TAG allows the sentence planner to choose words in tandem with appropriate syntax (Yang et al., 1991), in a flexible order (Elhadad and Robin, 1992), and, if necessary, in conventional combinations (Smadja and McKeown, 1991; Wanner, 1994). 3 Linguistic Specifications Realizing this procedure requires a declarative specification of three kinds of information: first, what operators are available and how they may combine; second, how operators specify the content of a de</context>
</contexts>
<marker>McDonald, 1992</marker>
<rawString>D. McDonald. 1992. Type-driven suppression of redundancy in the generation of inference-rich reports. In Dale, Hovy, Rosner, and Stock, editors, Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation, pages 73-88. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Meteer</author>
</authors>
<title>Bridging the generation gap between text planning and linguistic realization.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="931" citStr="Meteer, 1991" startWordPosition="131" endWordPosition="132"> .upenn. edu Abstract We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices. 1 Introduction Since (Meteer, 1991), researchers in natural language generation have recognized the need to refine and reorganize content after the rhetorical organization of arguments and before the syntactic realization of phrases. This process has been named sentence planning (Rambow and Korelsky, 1992). Broadly speaking, it involves aggregating content into sentence-sized units, and then selecting the lexical and syntactic elements that are used in realizing each sentence. Here, we consider this second process. The challenge lies in integrating constraints from syntax, semantics and pragmatics. Although most generation syst</context>
</contexts>
<marker>Meteer, 1991</marker>
<rawString>M. W. Meteer. 1991. Bridging the generation gap between text planning and linguistic realization. Computational Intelligence, 7(4):296-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nicolov</author>
<author>C Mellish</author>
<author>G Ritchie</author>
</authors>
<title>Sentence generation from conceptual graphs.</title>
<date>1995</date>
<booktitle>Conceptual Structures: Applications, Implementation and Theory,</booktitle>
<pages>74--88</pages>
<editor>In W. Rich G. Ellis, R. Levinson and F. Sowa, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="26464" citStr="Nicolov et al., 1995" startWordPosition="4168" endWordPosition="4171">enomena discussed elsewhere separately, and does so within a unified framework. With its incremental choices and its emphasis on the consequences of functional choices in the grammar, our algorithm resembles the networks of systemic grammar (Mathiessen, 1983; Yang et al., 1991). However, unlike systemic networks, our system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association between the content input to a sentence planner and the meaning that comes out. Other researchers (Nicolov et al., 1995; Rubinoff, 1992) have assumed that this flexibility comes from a mismatch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and P</context>
</contexts>
<marker>Nicolov, Mellish, Ritchie, 1995</marker>
<rawString>N. Nicolov, C. Mellish, and G. Ritchie. 1995. Sentence generation from conceptual graphs. In W. Rich G. Ellis, R. Levinson and F. Sowa, editors, Conceptual Structures: Applications, Implementation and Theory, pages 74-88. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sag</author>
<author>T Wasow</author>
</authors>
<date>1994</date>
<journal>Idioms. Language,</journal>
<pages>70--3</pages>
<marker>Sag, Wasow, 1994</marker>
<rawString>G. Nunberg, 1. A. Sag, and T. Wasow. 1994. Idioms. Language, 70(3):491-538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="7086" citStr="Pollard and Sag, 1994" startWordPosition="1073" endWordPosition="1076">tors are available and how they may combine; second, how operators specify the content of a description; and third, how operators achieve pragmatic effects. We represent operators as elementary trees in LTAG, and use TAG operations to combine them; we give the meaning of each tree as a formula in an ontologically promiscuous representation language; and, we model the pragmatics of operators by associating with each tree a set of discourse constraints describing when that operator can and should be used. Other frameworks have the capability to make comparable specifications; for example, HPSG (Pollard and Sag, 1994) feature structures describe syntax (suscAT), semantics (CONTENT) and pragmatics (CONTEXT). We choose TAG because it enables local specification of syntactic dependencies in explicit constructions and flexibility in incorporating modifiers; further, it is a constrained grammar formalism with tractable computational properties. 3.1 Syntactic specification TAG (Joshi et al., 1975) is a grammar formalism built around two operations that combine pairs of trees, SUBSTITUTION and ADJOINING. A TAG grammar consists of a finite set of ELEMENTARY trees, which can be combined by these substitution and ad</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prevost</author>
<author>M Steedman</author>
</authors>
<title>Generating contextually appropriate intonation.</title>
<date>1993</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="27612" citStr="Prevost and Steedman, 1993" startWordPosition="4340" endWordPosition="4343">ons. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the pragmatics of particular constructions. 6 Conclusion Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). With the right formalism, constructing pragmatics, semantics and syntax simultaneously is easier and better. The approach elegantly captures the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a </context>
</contexts>
<marker>Prevost, Steedman, 1993</marker>
<rawString>S. Prevost and M. Steedman. 1993. Generating contextually appropriate intonation. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<editor>In P. Cole, editor, Radical Pragmatics.</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="11763" citStr="Prince, 1981" startWordPosition="1805" endWordPosition="1806">cribe semantic collocations (Pustejovsky, 1991) and idiomatic composition (Nunberg etal., 1994). 3.3 Pragmatics Different constructions make different assumptions about the status of entities and propositions in the discourse, which we model by including in each tree a specification of the contextual conditions under which use of the tree is pragmatically licensed. We have selected four representative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of particular theories. We use the following distinctions. First, entities differ in NEWNESS (Prince, 1981). At any point, an entity is either new or old to the HEARER and either new or old to the DISCOURSE. Second, entities differ in SALIENCE (Grosz and Sidner, 1986; Grosz et al., 1995). Salience assigns each entity a position in a partial order that indicates how accessible it is for reference in the current context. Third, entities are related by salient PARTIALLYORDERED SET (POSET) RELATIONS to other entities in the context (Hirschberg, 1985). These relations include part and whole, subset and superset, and membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITI</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Prince. 1981. Toward a taxonomy of given-new information. In P. Cole, editor, Radical Pragmatics. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prince</author>
</authors>
<title>On the syntactic marking of presupposed open propositions.</title>
<date>1986</date>
<booktitle>In CLS,</booktitle>
<pages>208--222</pages>
<location>Chicago. CLS.</location>
<contexts>
<context position="12448" citStr="Prince, 1986" startWordPosition="1915" endWordPosition="1916">new or old to the DISCOURSE. Second, entities differ in SALIENCE (Grosz and Sidner, 1986; Grosz et al., 1995). Salience assigns each entity a position in a partial order that indicates how accessible it is for reference in the current context. Third, entities are related by salient PARTIALLYORDERED SET (POSET) RELATIONS to other entities in the context (Hirschberg, 1985). These relations include part and whole, subset and superset, and membership in a common class. Finally, the discourse may distinguish some OPEN PROPOSITIONS (propositions containing free variables) as being under discussion (Prince, 1986). We assume that information of these four kinds is available in a model of the current discourse state. The applicability conditions of constructions can freely make reference to this information. In particular, NP trees include the determiner (the determiner does not have a separate tree), the head noun, and pragmatic conditions that match the determiner with the status of the entity in context, as in 3(a). Following (Gundel et al., 1993), the definite article the may be used when the entity is UNIQUELY IDENTIFIABLE in the discourse model, i.e. the hearer knows or can infer the existence of </context>
</contexts>
<marker>Prince, 1986</marker>
<rawString>Prince. 1986. On the syntactic marking of presupposed open propositions. In CLS, pages 208-222, Chicago. CLS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prince</author>
</authors>
<title>On the functions of left dislocation.</title>
<date>1993</date>
<booktitle>The generative lexicon. Computational Linguistics,</booktitle>
<tech>Manuscript,</tech>
<pages>7--3</pages>
<institution>University of Pennsylvania. Pustejovsky.</institution>
<contexts>
<context position="14445" citStr="Prince, 1993" startWordPosition="2246" endWordPosition="2247">the main verb and the number and position of its arguments. Our S trees specify the unmarked SVO order or one of a number of fancy variants: topicalization (TOP), left-dislocation (LD), and locative inversion (INV). We follow the analysis of TOP in (Ward, 1985). For Ward, TOP is not a topic-marking construction at all. Rather, TOP is felicitous as long as (1) the fronted NP is in a salient poset relation to the previous discourse and (2) the utterance conveys a salient open proposition which is formed by replacing the tonically stressed constituent with a variable (3(c)). Likewise, we follow (Prince, 1993) and (Birner, 1992) for LD and INV respectively. 4 SPUD 4.1 The algorithm Our system takes two types of goals. First, goals of the form distinguish x as cat instruct the algorithm to construct a description of entity x using the syntactic category cat. If x is uniquely identifiable in the discourse model, then this goal is only satisfied when the meaning planned so far distinguishes x for the hearer. If x is hearer new, this goal is satisfied by including any constituent of type cat. Second, goals of the form communicate p instruct the algorithm to include the proposition p. This goal is satis</context>
</contexts>
<marker>Prince, 1993</marker>
<rawString>Prince. 1993. On the functions of left dislocation. Manuscript, University of Pennsylvania. Pustejovsky. 1991. The generative lexicon. Computational Linguistics, l7(3):409-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rambow</author>
<author>T Korelsky</author>
</authors>
<title>Applied text generation.</title>
<date>1992</date>
<booktitle>In ANLP,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1203" citStr="Rambow and Korelsky, 1992" startWordPosition="169" endWordPosition="173">yntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices. 1 Introduction Since (Meteer, 1991), researchers in natural language generation have recognized the need to refine and reorganize content after the rhetorical organization of arguments and before the syntactic realization of phrases. This process has been named sentence planning (Rambow and Korelsky, 1992). Broadly speaking, it involves aggregating content into sentence-sized units, and then selecting the lexical and syntactic elements that are used in realizing each sentence. Here, we consider this second process. The challenge lies in integrating constraints from syntax, semantics and pragmatics. Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. In this paper, we provide a natural framework for dealing with interactions and ensuring contextually appro</context>
</contexts>
<marker>Rambow, Korelsky, 1992</marker>
<rawString>Rambow and T. Korelsky. 1992. Applied text generation. In ANLP, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiter</author>
<author>R Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>232--238</pages>
<contexts>
<context position="4822" citStr="Reiter and Dale, 1992" startWordPosition="732" endWordPosition="735">fer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure can exploit linguistic context and convention to arrive quickly at short, unambiguous descriptions. For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make baselevel attributions to incrementally select words for inclusion in a description. (Dale and Haddock, 1991) use a constraint network to represent the distractors described by a complex referring NP, and incrementally select a property or relation that rules out as many alte</context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>Reiter and R. Dale. 1992. A fast algorithm for the generation of referring expressions. In Proceedings of COLING, pages 232-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiter</author>
</authors>
<title>A new model of lexical choice for nouns.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="4775" citStr="Reiter, 1991" startWordPosition="726" endWordPosition="727">n this sense, by assuming sentences refer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure can exploit linguistic context and convention to arrive quickly at short, unambiguous descriptions. For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make baselevel attributions to incrementally select words for inclusion in a description. (Dale and Haddock, 1991) use a constraint network to represent the distractors described by a complex referring NP, and incrementally select a p</context>
</contexts>
<marker>Reiter, 1991</marker>
<rawString>Reiter. 1991. A new model of lexical choice for nouns. Computational Intelligence, 7(4):240-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiter</author>
</authors>
<title>Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible?</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventh International Workshop on Natural Language Generation,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="1568" citStr="Reiter, 1994" startWordPosition="222" endWordPosition="223"> language generation have recognized the need to refine and reorganize content after the rhetorical organization of arguments and before the syntactic realization of phrases. This process has been named sentence planning (Rambow and Korelsky, 1992). Broadly speaking, it involves aggregating content into sentence-sized units, and then selecting the lexical and syntactic elements that are used in realizing each sentence. Here, we consider this second process. The challenge lies in integrating constraints from syntax, semantics and pragmatics. Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. In this paper, we provide a natural framework for dealing with interactions and ensuring contextually appropriate output in a single pass. As in (Yang et al., 1991), Lexicalized Tree Adjoining Grammar (LTAG) provides an The authors thank Aravind Joshi, Mark Steedman, Martha Palmer, Ellen Prince, Owen Rambow, Mike White, Betty Birner, and the participants of INLG96 for their helpful comments on various incarnations of this work. This work has been supported by NSF and </context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Reiter. 1994. Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible? In Proceedings of the Seventh International Workshop on Natural Language Generation, pages 163-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rooth</author>
</authors>
<title>Association with focus.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="4306" citStr="Rooth, 1985" startWordPosition="655" endWordPosition="656"> and the idea that language use is INTENTIONAL ACTION. Semantically, a DESCRIPTION D is j ust an open formula. D applies to a sequence of entities when substituting them for the variables in D yields a true formula. D REFERS to C just in case it distinguishes c from its DISTRACTORS— that is D applies to c but to no other salient alternatives. Given a sufficiently rich logical language, the meaning of a natural language sentence can be represented as a description in this sense, by assuming sentences refer to entities in a DISCOURSE MODEL, cf. alternative semantics (Karttunen and Peters, 1979; Rooth, 1985). Pragmatic analyses of referring expressions model speakers as PLANNING those expressions to achieve several different kinds of intentions (Donellan, 1966; Appelt, 198 1985; ICronfeld, 1986). Given a set of entities to describe and a set of intentions to achieve in describing them, a plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied. Recent work on generating definite referring NPs (Reiter, 1991; Dale and Haddock, 1991; Reiter and Dale, 1992; Horacek, 1995) has emphasized how circumscribed instantiations of this procedure c</context>
</contexts>
<marker>Rooth, 1985</marker>
<rawString>Rooth. 1985. Association with focus. Ph.D. thesis, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rubinoff</author>
</authors>
<title>Integrating text planning and linguistic choice by annotating linguistic structures.</title>
<date>1992</date>
<booktitle>Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation,</booktitle>
<pages>45--56</pages>
<editor>In Dale, Hovy, Rosner, and Stock, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="26481" citStr="Rubinoff, 1992" startWordPosition="4172" endWordPosition="4173">where separately, and does so within a unified framework. With its incremental choices and its emphasis on the consequences of functional choices in the grammar, our algorithm resembles the networks of systemic grammar (Mathiessen, 1983; Yang et al., 1991). However, unlike systemic networks, our system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association between the content input to a sentence planner and the meaning that comes out. Other researchers (Nicolov et al., 1995; Rubinoff, 1992) have assumed that this flexibility comes from a mismatch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985;</context>
</contexts>
<marker>Rubinoff, 1992</marker>
<rawString>Rubinoff. 1992. Integrating text planning and linguistic choice by annotating linguistic structures. In Dale, Hovy, Rosner, and Stock, editors, Aspects of Automated Natural Language Generation: 6th International Workshop on Natural Language Generation, pages 45-56. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8668" citStr="Schabes, 1990" startWordPosition="1321" endWordPosition="1322">n substituted back into the first tree at a distinguished leaf called the FOOT node. Elementary trees without foot nodes are called INITIAL trees and can only substitute; trees with foot nodes are called AUXILIARY trees, and must adjoin. (The symbol marks substitution sites, and the symbol * marks the foot node.) Figure 1(a) shows an initial tree representing the book. Figure 1(b) shows an auxiliary tree representing the modifier syntax, which could adjoin into the tree for the book to give the syntax book. Our grammar incorporates two additional principles. First, the grammar is LEXICALIZED (Schabes, 1990): each elementary structure in the grammar contains at least one lexical item. Second, our trees include FEATURES, following (Vijay-Shanker, 1987). LTAG elementary trees abstract the combinatorial properties of words in a linguistically appealing way. All predicate-argument structures are localized within a single elementary tree, even in long-distance relationships, so elementary trees give a natural domain of locality over which to state semantic and pragmatic constraints. The LTAG formalism does not dictate particular syntactic analyses; ours follow basic GB conventions. 3.2 Semantics We sp</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shieber</author>
<author>Y Schabes</author>
</authors>
<title>Generation and synchronous tree adjoining grammars.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>4--7</pages>
<contexts>
<context position="27452" citStr="Shieber and Schabes, 1991" startWordPosition="4315" endWordPosition="4318">n as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the pragmatics of particular constructions. 6 Conclusion Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). With the right formalism, constructing pragmatics, semantics and syntax simultaneously is easier and better. The approach elegantly captures</context>
</contexts>
<marker>Shieber, Schabes, 1991</marker>
<rawString>Shieber and Y. Schabes. 1991. Generation and synchronous tree adjoining grammars. Computational Intelligence, 4(7):220-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord Shieber</author>
<author>F Pereira</author>
<author>R Moore</author>
</authors>
<date>1990</date>
<booktitle>Semantic-head-driven generation. Computational Linguistics,</booktitle>
<pages>16--30</pages>
<contexts>
<context position="27424" citStr="Shieber et al., 1990" startWordPosition="4311" endWordPosition="4314">vantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the pragmatics of particular constructions. 6 Conclusion Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). With the right formalism, constructing pragmatics, semantics and syntax simultaneously is easier and better. The</context>
</contexts>
<marker>Shieber, Pereira, Moore, 1990</marker>
<rawString>Shieber, G. van Noord, F. Pereira, and R. Moore. 1990. Semantic-head-driven generation. Computational Linguistics, 16:30-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smadja</author>
<author>K McKeown</author>
</authors>
<title>Using collocations for language generation.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="6309" citStr="Smadja and McKeown, 1991" startWordPosition="953" endWordPosition="956">aterial into descriptions of a variety of entities until the addressee can infer desired conclusions allows the sentence planner to enrich input content, so that descriptions refer successfully (Dale and Haddock, 1991) or reduce it, to eliminate redundancy (McDonald, 1992). Moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic contributions to the sentence using TAG allows the sentence planner to choose words in tandem with appropriate syntax (Yang et al., 1991), in a flexible order (Elhadad and Robin, 1992), and, if necessary, in conventional combinations (Smadja and McKeown, 1991; Wanner, 1994). 3 Linguistic Specifications Realizing this procedure requires a declarative specification of three kinds of information: first, what operators are available and how they may combine; second, how operators specify the content of a description; and third, how operators achieve pragmatic effects. We represent operators as elementary trees in LTAG, and use TAG operations to combine them; we give the meaning of each tree as a formula in an ontologically promiscuous representation language; and, we model the pragmatics of operators by associating with each tree a set of discourse co</context>
</contexts>
<marker>Smadja, McKeown, 1991</marker>
<rawString>Smadja and K. McKeown. 1991. Using collocations for language generation. Computational Intelligence, 7(4):229-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8814" citStr="Vijay-Shanker, 1987" startWordPosition="1341" endWordPosition="1342">trees and can only substitute; trees with foot nodes are called AUXILIARY trees, and must adjoin. (The symbol marks substitution sites, and the symbol * marks the foot node.) Figure 1(a) shows an initial tree representing the book. Figure 1(b) shows an auxiliary tree representing the modifier syntax, which could adjoin into the tree for the book to give the syntax book. Our grammar incorporates two additional principles. First, the grammar is LEXICALIZED (Schabes, 1990): each elementary structure in the grammar contains at least one lexical item. Second, our trees include FEATURES, following (Vijay-Shanker, 1987). LTAG elementary trees abstract the combinatorial properties of words in a linguistically appealing way. All predicate-argument structures are localized within a single elementary tree, even in long-distance relationships, so elementary trees give a natural domain of locality over which to state semantic and pragmatic constraints. The LTAG formalism does not dictate particular syntactic analyses; ours follow basic GB conventions. 3.2 Semantics We specify the semantics of trees by applying two principles to the LTAG formalism. First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs,</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stone</author>
<author>C Doran</author>
</authors>
<title>Paying heed to collocations.</title>
<date>1996</date>
<booktitle>In Eighth International Workshop on Natural Language Generation 96,</booktitle>
<pages>91--100</pages>
<contexts>
<context position="11061" citStr="Stone and Doran, 1996" startWordPosition="1701" endWordPosition="1704"> for semantics as in 199 NP DetP N D book the (a) NPI S Z/N NP1. VP /\ V NP syntax (b) have e (c) Figure 1: Sample LTAG trees: (a) NP, (b) Noun-Noun Compound, (c) Topicalized Transitive NP &lt;1&gt;x S &lt;1&gt;&lt;r,having&gt; N &lt;1&gt;x N: syntax N&amp;quot; : &lt;1&gt; syntax concerns(x, syntax) (b) NP 1 : &lt;2&gt; havee S &lt;1&gt; NP1 : haver VP: &lt;1&gt; V NP : &lt;2&gt; I /have/ during(r, having) A have(having, haver, havee) (c) DetP N:&lt;1&gt; Det book the book(X) (a) Figure 2: LTAG trees with semantic specifications figure 2. Ontological promiscuity makes it possible to explore more complicated analyses in this general framework. For example, in (Stone and Doran, 1996), we use reference to properties, actions and belief contexts (Ballim etal., 1991) to describe semantic collocations (Pustejovsky, 1991) and idiomatic composition (Nunberg etal., 1994). 3.3 Pragmatics Different constructions make different assumptions about the status of entities and propositions in the discourse, which we model by including in each tree a specification of the contextual conditions under which use of the tree is pragmatically licensed. We have selected four representative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of pa</context>
<context position="16838" citStr="Stone and Doran, 1996" startWordPosition="2632" endWordPosition="2635">ode that describes the entity; and they must distinguish entities from their distractors or entail required information. Then, the second step identifies which of the associated trees are applicable, by testing their pragmatic conditions against the current representation of discourse. The algorithm identifies the combinations of words and trees that satisfy the most communicate goals and eliminate the most distractors. From these, it selects the entry with the most specific semantic and pragmatic licensing conditions. This means that the algorithm generates the most marked licensed form. In (Stone and Doran, 1996) we explore the use of additional factors, such as attentional state and lexical preferences, in this step. The new tree is then substituted or adjoined into the existing tree at the appropriate node. The entry may specify additional goals, because it describes one entity in terms of a new one. These new goals are added to the current goals, and then the algorithm repeats. Note that this algorithm performs greedy search. To avoid backtracking, we choose uninflected forms. Morphological features are set wherever possible as a result of the general unification processes in the grammar; the infle</context>
<context position="25744" citStr="Stone and Doran, 1996" startWordPosition="4054" endWordPosition="4057">proceed as before, deriving The syntax book we have. However, the new goal would still be unsatisfied; in the next iteration, the PP on reserve would be adjoined into the tree to satisfy it: The syntax book, we have on reserve. Because TAG allows adjunction to apply at anytime, flexible realization of content is facilitated without need for sophisticated back-tracking (Elhadad and Robin, 1992). The processing of this example may seem simple, but it illustrates the way in which SPUD integrates syntactic, semantic and pragmatic knowledge in realizing sentences. We tackle additional examples in (Stone and Doran, 1996). 5 Comparison with related work The strength of the present work is that it captures a number of phenomena discussed elsewhere separately, and does so within a unified framework. With its incremental choices and its emphasis on the consequences of functional choices in the grammar, our algorithm resembles the networks of systemic grammar (Mathiessen, 1983; Yang et al., 1991). However, unlike systemic networks, our system derives its functional choices dynamically using a simple declarative specification of function. Like many sentence planners, we assume that there is a flexible • association</context>
</contexts>
<marker>Stone, Doran, 1996</marker>
<rawString>. Stone and C. Doran. 1996. Paying heed to collocations. In Eighth International Workshop on Natural Language Generation 96, pages 91-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E André Wahlster</author>
<author>S Bandyopadhyay</author>
<author>W Graf</author>
<author>T Rist</author>
</authors>
<title>WIP: The coordinated generation of multimodal presentations from a common representation.</title>
<date>1991</date>
<booktitle>Computational Theories of Communication and their Applications.</booktitle>
<editor>In Stock, Slack, and Ortony, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="27157" citStr="Wahlster et al., 1991" startWordPosition="4272" endWordPosition="4275">ch between input content and grammatical options. In our system, such differences arise from the referential requirements and inferential opportunities that are encountered. Previous authors (McDonald and Pustejovsky, 1985; Joshi, 1987) have noted that TAG has many advantages for generation as a syntactic formalism, because of its localization of argument structure. (Joshi, 1987) states that adjunction is a powerful tool for elaborating descriptions. These aspects of TAGs are crucial to SPUD, as they are to (McDonald and Pustejovsky, 1985; Joshi, 1987; Yang et al., 1991; Nicolov et al., 1995; Wahlster et al., 1991; Danlos, 1996). What sets SPUD apart is its simultaneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifications for constructions it uses. Two contrasts should be emphasized in this regard. (Shieber et al., 1990; Shieber and Schabes, 1991) construct a simultaneous derivation of syntax and semantics but they do not construct the semantics—it is an input to their system. (Prevost and Steedman, 1993; Hoffman, 1994) represent syntax, semantics and pragmatics in a lexicalized framework, but concentrate on information structure rather than the </context>
</contexts>
<marker>Wahlster, Bandyopadhyay, Graf, Rist, 1991</marker>
<rawString>. Wahlster, E. André, S. Bandyopadhyay, W. Graf, and T. Rist. 1991. WIP: The coordinated generation of multimodal presentations from a common representation. In Stock, Slack, and Ortony, editors, Computational Theories of Communication and their Applications. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanner</author>
</authors>
<title>Building another bridge over the generation gap.</title>
<date>1994</date>
<booktitle>In Seventh International Workshop on Natural Language Generation,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="6324" citStr="Wanner, 1994" startWordPosition="957" endWordPosition="958">of a variety of entities until the addressee can infer desired conclusions allows the sentence planner to enrich input content, so that descriptions refer successfully (Dale and Haddock, 1991) or reduce it, to eliminate redundancy (McDonald, 1992). Moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic contributions to the sentence using TAG allows the sentence planner to choose words in tandem with appropriate syntax (Yang et al., 1991), in a flexible order (Elhadad and Robin, 1992), and, if necessary, in conventional combinations (Smadja and McKeown, 1991; Wanner, 1994). 3 Linguistic Specifications Realizing this procedure requires a declarative specification of three kinds of information: first, what operators are available and how they may combine; second, how operators specify the content of a description; and third, how operators achieve pragmatic effects. We represent operators as elementary trees in LTAG, and use TAG operations to combine them; we give the meaning of each tree as a formula in an ontologically promiscuous representation language; and, we model the pragmatics of operators by associating with each tree a set of discourse constraints descr</context>
</contexts>
<marker>Wanner, 1994</marker>
<rawString>Wanner. 1994. Building another bridge over the generation gap. In Seventh International Workshop on Natural Language Generation, pages 137-144, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ward</author>
</authors>
<title>The Semantics and Pragmatics of Preposing.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14093" citStr="Ward, 1985" startWordPosition="2188" endWordPosition="2189"> V NP :&lt;2 /have/ during(r, having) A have(having, haver, havee) (in-poset(havee), in-op(have(having, haver, havee))) (c) Figure 3: LTAG trees with semantic and pragmatic specifications the entity from all its alternatives.) In contrast, the indefinite articles, a, an, and 0, are used for entities that are NOT uniquely identifiable. S trees specify the main verb and the number and position of its arguments. Our S trees specify the unmarked SVO order or one of a number of fancy variants: topicalization (TOP), left-dislocation (LD), and locative inversion (INV). We follow the analysis of TOP in (Ward, 1985). For Ward, TOP is not a topic-marking construction at all. Rather, TOP is felicitous as long as (1) the fronted NP is in a salient poset relation to the previous discourse and (2) the utterance conveys a salient open proposition which is formed by replacing the tonically stressed constituent with a variable (3(c)). Likewise, we follow (Prince, 1993) and (Birner, 1992) for LD and INV respectively. 4 SPUD 4.1 The algorithm Our system takes two types of goals. First, goals of the form distinguish x as cat instruct the algorithm to construct a description of entity x using the syntactic category </context>
</contexts>
<marker>Ward, 1985</marker>
<rawString>Ward. 1985. The Semantics and Pragmatics of Preposing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K F McCoy Yang</author>
<author>K Vijay-Shanker</author>
</authors>
<title>From functional specification to syntactic structures: systemic grammar and tree-adjoining grammar.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<marker>Yang, Vijay-Shanker, 1991</marker>
<rawString>Yang, K. F. McCoy, and K. Vijay-Shanker. 1991. From functional specification to syntactic structures: systemic grammar and tree-adjoining grammar. Computational Intelligence, 7(4):207-219.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>