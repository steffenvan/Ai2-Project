<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001052">
<title confidence="0.992929">
An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT
</title>
<author confidence="0.982205">
Ashish Venugopal and Andreas Zollmann and Stephan Vogel
</author>
<affiliation confidence="0.978023">
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
</affiliation>
<email confidence="0.995958">
{ashishv,zollmann,vogel+}@cs.cmu.edu
</email>
<sectionHeader confidence="0.984795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948285714286">
We present an efficient, novel two-pass
approach to mitigate the computational
impact resulting from online intersection
of an n-gram language model (LM) and
a probabilistic synchronous context-free
grammar (PSCFG) for statistical machine
translation. In first pass CYK-style decod-
ing, we consider first-best chart item ap-
proximations, generating a hypergraph of
sentence spanning target language deriva-
tions. In the second stage, we instantiate
specific alternative derivations from this
hypergraph, using the LM to drive this
search process, recovering from search er-
rors made in the first pass. Model search
errors in our approach are comparable to
those made by the state-of-the-art “Cube
Pruning” approach in (Chiang, 2007) un-
der comparable pruning conditions evalu-
ated on both hierarchical and syntax-based
grammars.
</bodyText>
<sectionHeader confidence="0.992362" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985449534883721">
Syntax-driven (Galley et al., 2006) and hierarchi-
cal translation models (Chiang, 2005) take advan-
tage of probabilistic synchronous context free gram-
mars (PSCFGs) to represent structured, lexical re-
ordering constraints during the decoding process.
These models extend the domain of locality (over
phrase-based models) during decoding, represent-
ing a significantly larger search space of possible
translation derivations. While PSCFG models are
often induced with the goal of producing grammati-
cally correct target translations as an implicit syntax-
structured language model, we acknowledge the
value of n-gram language models (LM) in phrase-
based approaches.
Integrating n-gram LMs into PSCFGs based de-
coding can be viewed as online intersection of the
PSCFG grammar with the finite state machine rep-
resented by the n-gram LM, dramatically increasing
the effective number of nonterminals in the decoding
grammar, rendering the decoding process essentially
infeasible without severe, beam-based lossy prun-
ing. The alternative, simply decoding without the
n-gram LM and rescoring N-best alternative transla-
tions, results in substantially more search errors, as
shown in (Zollmann and Venugopal, 2006).
Our two-pass approach involves fast, approximate
synchronous parsing in a first stage, followed by a
second, detailed exploration through the resulting
hypergraph of sentence spanning derivations, using
the n-gram LM to drive that search. This achieves
search errors comparable to a strong “Cube Pruning”
(Chiang, 2007), single-pass baseline. The first pass
corresponds to a severe parameterization of Cube
Pruning considering only the first-best (LM inte-
grated) chart item in each cell while maintaining un-
explored alternatives for second-pass consideration.
Our second stage allows the integration of long dis-
tance and flexible history n-gram LMs to drive the
search process, rather than simply using such mod-
els for hypothesis rescoring.
We begin by discussing the PSCFG model for
statistical machine translation, motivating the need
500
</bodyText>
<note confidence="0.9486225">
Proceedings of NAACL HLT 2007, pages 500–507,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9992855">
for effective n-gram LM integration during decod-
ing. We then present our two-pass approach and
discuss Cube Pruning as a state-of-the-art baseline.
We present results in the form of search error analy-
sis and translation quality as measured by the BLEU
score (Papineni et al., 2002) on the IWSLT 06 text
translation task (Eck and Hori, 2005)1, comparing
Cube Pruning with our two-pass approach.
</bodyText>
<sectionHeader confidence="0.80177" genericHeader="method">
2 Synchronous Parsing for SMT
</sectionHeader>
<bodyText confidence="0.990340166666667">
Probabilistic Synchronous Context Free Grammar
(PSCFG) approaches to statistical machine transla-
tion use a source terminal set (source vocabulary)
TS, a target terminal set (target vocabulary) TT and
a shared nonterminal set N and induce rules of the
form
</bodyText>
<equation confidence="0.608729">
X → h-y, α,∼,wi
</equation>
<bodyText confidence="0.980473548387097">
where (i) X ∈ N is a nonterminal, (ii) -y ∈ (N ∪
TS)* is a sequence of nonterminals and source ter-
minals, (iii) α ∈ (N ∪ TT)* is a sequence of nonter-
minals and target terminals, (iv) the number cnt(-y)
of nonterminal occurrences in -y is equal to the num-
ber cnt(α) of nonterminal occurrences in α, (v)
∼: {1, ... , cnt(-y)} → {1, ... , cnt(α)} is a one-to-
one mapping from nonterminal occurrences in -y to
nonterminal occurrences in α, and (vi) w ∈ [0, ∞)
is a non-negative real-valued weight assigned to the
rule. We will assume ∼ to be implicitly defined by
indexing the NT occurrences in -y from left to right
starting with 1, and by indexing the NT occurrences
in α by the indices of their corresponding counter-
parts in -y. Syntax-oriented PSCFG approaches typ-
ically ignore source structure, instead focussing on
generating syntactically well formed target deriva-
tions. (Galley et al., 2006) use syntactic constituents
for the PSCFG nonterminal set and (Zollmann and
Venugopal, 2006) take advantage of CCG (Steed-
man, 1999) categories, while (Chiang, 2005) uses
a single generic nonterminal. PSCFG derivations
function analogously to CFG derivations. Given
a source sentence f, the translation task under a
PSCFG grammar can be expressed as
1While IWSLT represents a limited resource translation task
(120K sentences of training data for Chinese-English), the prob-
lem of efficient n-gram LM integration is still critically impor-
tant to efficient decoding, and our contributions can be expected
to have an even more significant impact when decoding with
grammars induced from larger corpora.
</bodyText>
<equation confidence="0.9991015">
e� = arg max P(D)
{e  |�]D. src(D)=f,tgt(D)=e}
</equation>
<bodyText confidence="0.997555857142857">
where tgt(D) refers to the target terminal symbols
generated by the derivation D and src(D) refers to
the source terminal symbols spanned by D. The
score (also laxly called probability, since we never
need to compute the partition function) of a deriva-
tion D under a log-linear model, referring to the
rules r used in D, ism}
</bodyText>
<equation confidence="0.996132">
P(D) = 1 PLM(tgt(D))ALM ×
i
</equation>
<bodyText confidence="0.999996285714286">
where Oi refers to features defined on each rule,
and PLM is a g-gram LM probability applied to the
target terminal symbols generated by the derivation
D. Introducing the LM feature defines dependen-
cies across adjacent rules used in each derivation,
and requires modifications to the decoding strategy.
Viewing the LM as a finite-state machine, the de-
coding process involves performing an intersection
between the PSCFG grammar and the g-gram LM
(Bar-Hillel et al., 1964). We present our work under
the construction in (Wu, 1996), following notation
from (Chiang, 2007), extending the formal descrip-
tion to reflect grammars with an arbitrary number of
nonterminals in each rule.
</bodyText>
<subsectionHeader confidence="0.99889">
2.1 Decoding Strategies
</subsectionHeader>
<bodyText confidence="0.999054">
In Figure 1, we reproduce the decoding algorithm
from (Chiang, 2007) that applies a PSCFG to
translate a source sentence in the same notation (as
a deductive proof system (Shieber et al., 1995)),
generalized to handle more than two non-terminal
pairs. Chart items [X, i, j, e] : w span j − i words
in the source sentence f1 · · · f, starting at position
i + 1, and have weight w (equivalent to P(D)), and
e ∈ (TT ∪ {*})* is a sequence of target terminals,
with possible elided parts, marked by *. Functions
p, q whose domain is TT ∪ {*} are defined in
(Chiang, 2007) and are repeated here for clarity.
</bodyText>
<equation confidence="0.9997498">
PLM(ai|ai−g+1 ··· ai−1)
g≤i≤m,*%ai−g}1···ai−1
�
a1 ··· ag−1 * am−g+2 ··· am if m &gt; g
�(a1 · · · am) =a1 · · · am else
</equation>
<bodyText confidence="0.993945666666667">
The function q elides elements from a target lan-
guage terminal sequence, leaving the leftmost and
rightmost g − 1 words, replacing the elided words
</bodyText>
<equation confidence="0.9909115">
H Oi(r)-li
rED
P(a1 ··· am) =
501
X → h-y, αi : w (X → h7, α, wi) ∈ G
j
X → hfi+1, αi: w
[X, i, j; q(α)] : wp(α)
Z → hfi1
i+1(X1)1fi�
j1+1 ··· (Xm−1)m−1fim
jm−1+1(Xm)mfj jm+1, αi : w [X1, i1, j1; e1] : w1 ··· [Xm, im, jm; em] : wm
[Z, i, j, q(α0)] : ww1 ··· wmp(α0) (where α0 = α [e1/(X1)1, ... , em/(Xm)m])
Goal item: [S, 0, n; hsig−1 * h\sig−1]
</equation>
<figureCaption confidence="0.657818">
Figure 1. CYK parsing with integrated g-gram LM. The inference rules are explored in ascending order of j − i. Here
α [e/Y] is the string α where the NT occurrence Y is replaced by e. Functions q and p are explained in the text.
</figureCaption>
<bodyText confidence="0.998752333333333">
with a single * symbol. The function p returns g-
gram LM probabilities for target terminal sequences,
where the * delineates context boundaries, prevent-
ing the calculation from spanning this boundary. We
add a distinguished start nonterminal S to gener-
ate sentences spanning target translations beginning
with g — 1 (s) symbols and ending with g — 1 (\s)
symbols. This can e.g. be achieved by adding for
each nonterminal X a PSCFG rule
</bodyText>
<equation confidence="0.977013">
S — (X, (s)9−1X(\s)9−1, 1)
</equation>
<bodyText confidence="0.999831684210526">
We are only searching for the derivation of highest
probability, so we can discard identical chart items
that have lower weight. Since chart items are de-
fined by their left-hand side nonterminal production,
span, and the LM contexts e, we can safely discard
these identical items since q has retained all context
that could possibly impact the LM calculation. This
process is commonly referred to as item recombina-
tion. Backpointers to antecedent cells are typically
retained to allow N-Best extraction using an algo-
rithm such as (Huang and Chiang, 2005).
The impact of g-gram LM intersection during de-
coding is apparent in the final deduction step. Gen-
erating the set of consequent Z chart items involves
combining m previously produced chart cells. Since
each of these chart cells with given source span [i, A
is identified by nonterminal symbol X and LM con-
text e, we have at worst |N |* |TT|2(9−1) such chart
cells in a span. The runtime of this algorithm is thus
</bodyText>
<equation confidence="0.999194">
I n3 [
O |N ||TT|2(9−1)
</equation>
<bodyText confidence="0.999968947368421">
where K is the maximum number of NT pairs per
rule and n the source sentence length. Without se-
vere pruning, this runtime is prohibitive for even the
smallest induced grammars. Traditional pruning ap-
proaches that limit the number of consequents after
they are produced are not effective since they first re-
quire that the cost of each consequent be computed
(which requires calls to the g-gram LM).
Restrictions to the grammar afford alternative de-
coding strategies to reduce the runtime cost of syn-
chronous parsing. (Zhang et al., 2006) “binarize”
grammars into CNF normal form, while (Watan-
abe et al., 2006) allow only Griebach-Normal form
grammars. (Wellington et al., 2006) argue that these
restrictions reduce our ability to model translation
equivalence effectively. We take an agnostic view
on the issue; directly addressing the question of effi-
cient LM intersection rather than grammar construc-
tion.
</bodyText>
<sectionHeader confidence="0.932859" genericHeader="method">
3 Two-pass LM Intersection
</sectionHeader>
<bodyText confidence="0.999496733333333">
We propose a two-pass solution to the problem of
online g-gram LM intersection. A naive two-pass
approach would simply ignore the LM interactions
during parsing, extract a set of N derivations from
the sentence spanning hypergraph and rescore these
derivations with the g-gram LM. In practice, this ap-
proach performs poorly (Chiang, 2007; Zollmann
and Venugopal, 2006). While parsing time is dra-
matically reduced (and N-best extraction time is
negligible), N is typically significantly less than the
complete number of possible derivations and sub-
stantial search errors remain. We propose an ap-
proach that builds upon the concept of a second pass
but uses the g-gram LM to search for alternative,
better translations.
</bodyText>
<figure confidence="0.8350405">
1&amp;quot;)
502
</figure>
<subsectionHeader confidence="0.997263">
3.1 First pass: parsing
</subsectionHeader>
<bodyText confidence="0.999988034482759">
We begin by relaxing the criterion that determines
when two chart items are equivalent during parsing.
We consider two chart items to be equivalent (and
therefore candidates for recombination) if they have
matching left-hand side nonterminals, and span. We
no longer require them to have the same LM con-
text e. We do however propagate the e, w for the
chart item with highest score, causing the algorithm
to still compute LM probabilities during parsing. As
a point of notation, we refer to such a chart item by
annotating its e, w as e1, w1, and we refer to them
as approximate items (since they have made a first-
best approximation for the purposes of LM calcula-
tion). These approximate items labeled with e1, w1
are used in consequent parse calculations.
The parsing algorithm under this approximation
stays unchanged, but parsing time is dramatically re-
duced. The runtime complexity of this algorithm is
now O (n3|N|K) at the cost of significant search
errors (since we ignored most LM contexts that we
encountered).
This relaxation is different from approaches that
do not use the LM during parsing. The sentence
spanning item does have LM probabilities associ-
ated with it (but potentially valuable chart items
were not considered during parsing). Like in tra-
ditional parsing, we retain the recombined items in
the cell to allow us to explore new derivations in a
second stage.
</bodyText>
<subsectionHeader confidence="0.997102">
3.2 Second pass: hypergraph search
</subsectionHeader>
<bodyText confidence="0.999065564516129">
The goal item of the parsing step represents a sen-
tence spanning hypergraph of alternative deriva-
tions. Exploring alternatives from this hyper-
graph and updating LM probabilities can now reveal
derivations with higher scores that were not consid-
ered in the first pass. Exploring the whole space of
alternative derivations in this hypergraph is clearly
infeasible. We propose a g-gram LM driven heuris-
tic search “H.Search” of this space that allows the g-
gram LM to decide which section of the hypergraph
to explore. By construction, traversing a particular
derivation item from the parse chart in target-side
left-to-right, depth-first order yields the correctly or-
dered sequence of target terminals that is the transla-
tion represented by this item. Now consider a partial
traversal of the item in that order, where we gener-
ate only the first M target terminals, leaving the rest
of the item in its original backpointer form. We in-
formally define our second pass algorithm based on
these partial derivation items.
Consider a simple example, where we have parsed
a source sentence, and arrived at a sentence spanning
item obtained from a rule with the following target
side:
NP2 VP3 PP1
and that the item’s best-score estimate is w. A par-
tial traversal of this item would replace NP2 with
one of the translations available in the chart cell un-
derlying NP2 (called “unwinding”), and recalculate
the weights associated with this item, taking into
account the alternative target terminals. Assuming
“the nice man” was the target side of the best scoring
item in NP2, the respective traversal would main-
tain the same weight. An alternative item at NP2
might yield “a nice man”. This partial traversal rep-
resents a possible item that we did not consider dur-
ing parsing, and recalculating LM probabilities for
this new item (based on approximate items VP3 and
PP1) yields weight w2:
the nice man VP3 PP1 : w1 = w
a nice man VP3 PP1 : w2
Alternative derivation items that obtain a higher
score than the best-score estimates represent recov-
ery from search errors. Our algorithm is based on
the premise that these items should be traversed fur-
ther, with the LM continuing to score newly gener-
ated target words. These partially traversed items
are placed on an agenda (sorted by score). At each
step of the second pass search, we select those items
from the agenda that are within a search beam of Z
from the best item, and perform the unwind opera-
tion on each of these items. Since we unwind partial
items from left-to-right the g-gram LM is able to in-
fluence the search through the space of alternative
derivations.
Applying the g-gram LM on partial items with
leading only-terminal symbols allows the integra-
tion of high- / flexible-order LMs during this sec-
ond stage process, and has the advantage of explor-
ing only those alternatives that participate in sen-
tence spanning, high scoring (considering both LM
and translation model scores) derivations. While
</bodyText>
<page confidence="0.698167">
503
</page>
<bodyText confidence="0.999964104166667">
we do not evaluate such models here, we note that
H.Search was developed specifically for the integra-
tion of such models during search.
We further note that partial items that have gen-
erated translations that differ only in the word po-
sitions up to g − 1 words before the first nonter-
minal site can be recombined (for the same rea-
sons as during LM intersected parsing). For exam-
ple, when considering a 3-gram LM, the two par-
tial items above can be recombined into one equiv-
alence class, since partial item LM costs resulting
from these items would only depend on ‘nice man’,
but not on ‘a’ vs. ‘the’. Even if two partial items
are candidates for recombination due to their termi-
nal words, they must also have identical backpoint-
ers (representing a set of approximate parse deci-
sions for the rest of the sentence, in our example
VP3PP1 ). Items that are filed into existing equiv-
alence classes with a lower score are not put onto
the agenda, while those that are better, or have cre-
ated new equivalence classes are scheduled onto the
agenda. For each newly created partial derivation,
we also add a backpointer to the “parent” partial
derivation that was unwound to create it.
This equivalence classing operation transforms
the original left-hand side NT based hypergraph into
an (ordinary) graph of partial items. Each equiva-
lence class is a node in this new graph, and recom-
bined items are the edges. Thus, N-best extraction
can now be performed on this graph. We use the
extraction method from (Huang and Chiang, 2005).
The expensive portion of our algorithm lies in the
unwinding step, in which we generate a new par-
tial item for each alternative at the non-terminal site
that we are “unwinding”. For each new partial item,
we factor out LM estimates and rule weights that
were used to score the parent item, and factor in
the LM probabilities and rule weights of the alter-
native choice that we are considering. In addition,
we must also update the new item’s LM estimates
for the remaining non-terminal and terminal sym-
bols that depend on this new left context of termi-
nals. Fortunately, the number of LM calculations
per new item is constant, i.e., does not dependent on
the length of the partial derivation, or how unwound
it is. Only (g − 1) * 2 LM probabilities have to be
re-evaluated per partial item. We now define this
“unwind-recombine” algorithm formally.
</bodyText>
<subsectionHeader confidence="0.702446">
3.2.1 The unwind-recombine algorithm
</subsectionHeader>
<bodyText confidence="0.986671195652174">
Going back to the first-pass parsing algorithm
(Figure 1), remember that each application of a
grammar rule containing nonterminals corresponds
to an application of the third inference rule of the
algorithm. We can assign chart items C created by
the third inference rule a back-pointer (BP) target
side as follows: When applying the third inference
rule, each nonterminal occurrence (Xk)k in the cor-
responding Z —* (A, α) grammar rule corresponds
to a chart cell [Xk, ik, ik] used as an antecedent for
the inference rule. We assign a BP target side for C
by replacing NT occurrences in α (from the rule that
created C) with backpointers to their corresponding
antecedent chart cells. Further we define the distin-
guished backpointer PS as the pointer to the goal
cell [5, 0, n] : w*.
The deductive program for our second-pass al-
gorithm is presented in Figure 2. It makes use of
two kind of items. The first, {P —* α; e1} : w,
links a backpointer P to a BP target side, storing
current-item vs. best-item correction terms in form
of an LM context e1 and a relative score w. The
second item form [[e; α]] in this algorithm corre-
sponds to partial left-to-right traversal states as de-
scribed above, where e is the LM context of the tra-
versed and unwound translation part, and α the part
that is yet to be traversed and whose backpointers
are still to be unwound. The first deduction rule
presents the logical axioms, creating BP items for
each backpointer used in a NT inference rule appli-
cation during the first-pass parsing step. The sec-
ond deduction rule represents the unwinding step
as discussed in the example above. These deduc-
tions govern a search for derivations through the hy-
pergraph that is driven by updates of rule weights
and LM probabilities when unwinding non-first-best
hypotheses. The functions p and q are as defined
in Section 2, except that the domain of q is ex-
tended to BP target sides by first replacing each
back-pointer with its corresponding chart cell’s LM
context and then applying the original q on the re-
sulting sequence of target-terminals and * symbols.2
Note that w&apos;, which was computed by the first de-
duction rule, adjusts the current hypothesis’ weight
2Note also that p((s)9−1 *(\s)9−1) = 1 as the product over
the empty set is one.
</bodyText>
<equation confidence="0.925488">
504
�P � α; e1� : w&apos;/w(P back-points to 1st-pass cell [X, i, j; e1] : w; α and w&apos; are BP-target-side and weight of one of that cell’s items)
[[e; Pαend]] : w {P --+ αlexαmid; e1} : w&apos; αlex contains no BPs and
[[q(eαlex); αmidαend]] : ww&apos;p[eq(αlexαmid)]p[q(eαlexαmid)q(αend)]/p(ee1)/p[q(ee1)q(αend)] (αmid = P&apos;α&apos; or αmid = ε
</equation>
<figureCaption confidence="0.9429625">
Figure 2. Left-to-right LM driven hypergraph search of the sentence spanning hypergraph; ε denotes the empty word.
Non-logical (Start) axiom: [[ε; PS]] : w*; Goal item: [[(s)9−1 ? (\s)9−1; ε]] : w
</figureCaption>
<bodyText confidence="0.999115714285714">
that is based on the first-best instance of P to the
actually chosen instance’s weight. Further, the ra-
tio p(eq(αlexαmid))/p(ee1) adjusts the LM prob-
abilities of P’s instantiation given its left context,
and p[q(eαlexαmid)q(αend)]/p[q(ee1)q(αend)] ad-
justs the LM probabilities of the g − 1 words right
of P.
</bodyText>
<sectionHeader confidence="0.992375" genericHeader="method">
4 Alternative Approaches
</sectionHeader>
<bodyText confidence="0.99998824137931">
We evaluate our two pass hypergraph search
“H.Search” against the strong single pass Cube
Pruning (CP) baseline as mentioned in (Chiang,
2005) and detailed in (Chiang, 2007). In the latter
work, the author shows that CP clearly outperforms
both the naive single pass solution of severe prun-
ing as well as the naive two-pass rescoring approach.
Thus, we focus on comparing our approach to CP.
CP is an optimization to the intersected LM pars-
ing algorithm presented in Figure 1. It addresses
the creation of the HKk�1 �[Xk, ik, A, *] chart items
when generating consequent items. CP amounts to
an early termination condition when generating the
set of possible consequents. Instead of generating
all consequents, and then pruning away the poor per-
formers, CP uses the K-Best extraction approach of
(Huang and Chiang, 2005) to select the best K con-
sequents only, at the cost of potential search errors.
CP’s termination condition can be defined in terms
of an absolute number of consequents to generate, or
by terminating the generation process when a newly
generated item is worse (by Q) than the current best
item for the same left-hand side and span. To sim-
ulate comparable pruning criteria we parameterize
each method with soft-threshold based criteria only
(Q for CP and Z for H.Search) since counter based
limits like K have different effects in CP (selecting
e labeled items) vs H.Search (selecting rules since
items are not labeled with e).
</bodyText>
<sectionHeader confidence="0.995786" genericHeader="evaluation">
5 Experimental Framework
</sectionHeader>
<bodyText confidence="0.999924291666667">
We present results on the IWSLT 2006 Chinese to
English translation task, based on the Full BTEC
corpus of travel expressions with 120K parallel sen-
tences (906K source words and 1.2m target words).
The evaluation test set contains 500 sentences with
an average length of 10.3 source words.
Grammar rules were induced with the syntax-
based SMT system “SAMT” described in (Zoll-
mann and Venugopal, 2006), which requires ini-
tial phrase alignments that we generated with
“GIZA++” (Koehn et al., 2003), and syntactic parse
trees of the target training sentences, generated by
the Stanford Parser (D. Klein, 2003) pre-trained on
the Penn Treebank. All these systems are freely
available on the web.
We experiment with 2 grammars, one syntax-
based (3688 nonterminals, 0.3m rules), and one
purely hierarchical (1 generic nonterminal, 0.05m
rules) as in (Chiang, 2005). The large number of
nonterminals in the syntax based systems is due to
the CCG extension over the original 75 Penn Tree-
bank nonterminals. Parameters A used to calculate
P(D) are trained using MER training (Och, 2003)
on development data.
</bodyText>
<subsectionHeader confidence="0.479598">
6 Comparison of Approaches
</subsectionHeader>
<bodyText confidence="0.99986425">
We evaluate each approach by considering both
search errors made on the development data for a
fixed set of model parameters, and the BLEU metric
to judge translation quality.
</bodyText>
<subsectionHeader confidence="0.999755">
6.1 Search Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999508">
While it is common to evaluate MT quality using the
BLEU score, we would like to evaluate search errors
made as a function of “effort” made by each algo-
rithm to produce a first-best translation. We con-
sider two metrics of effort made by each algorithm.
</bodyText>
<page confidence="0.787275">
505
</page>
<bodyText confidence="0.99999165625">
We first evaluate search errors as a function of novel
queries made to the g-gram LM (since LM calls tend
to be the dominant component of runtime in large
MT systems). We consider novel queries as those
that have not already been queried for a particular
sentence, since the repeated calls are typically effi-
ciently cached in memory and do not affect runtime
significantly. Our goal is to develop techniques that
can achieve low search error with the fewest novel
queries to the g-gram LM.
To appreciate the practical impact of each algo-
rithm, we also measure search errors as a function of
the number of seconds required to translate a fixed
unseen test set. This second metric is more sensitive
to implementation and, as it turned out, even com-
piler memory management decisions.
We define search errors based on the weight of
the best sentence spanning item. Treating weights as
negative log probabilities (costs), we accumulate the
value of the lowest cost derivation for each sentence
in the testing data as we vary pruning settings ap-
propriate to each method. Search errors are reduced
when we are able to lower this accumulated model
cost. We prefer approaches that yield low model cost
with the least number of LM calls or number of sec-
onds spent decoding.
It is important to note that model cost
(− log(P(D))) is a function of the parameters
A which have been trained using MER training. The
parameters used for these experiments were trained
with the CP approach; in practice we find that either
approach is effective for MER training.
</bodyText>
<subsectionHeader confidence="0.952626">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999980872340426">
Figure 3 and Figure 4 plot model cost as a function
of LM cache misses for the IWSLT Hierarchical and
Syntax based systems, while Figure 5 plots decod-
ing time. The plots are based on accumulated model
cost, decoding time and LM cache misses over the
IWSLT Test 06 set. For H.Search, we vary the beam
parameter Z for a fixed value of Q = 5 during pars-
ing while for CP, we vary Q. We also limit the total
number of items on the agenda at any time to 1000
for H.Search as a memory consideration. We plot
each method until we see no change in BLEU score
for that method. BLEU scores for each parameter
setting are also noted on the plots.
For both the hierarchical and syntax based gram-
mars we see that the H.Search method achieves a
given model cost ‘earlier’ in terms of novel LM
calls for most of the plotted region, but ultimately
fails to achieve the same lowest model cost as the
CP method.3 While the search beam of Z mit-
igates the impact of the estimated scores during
H.Search’s second pass, the score is still not an ad-
missible heuristic for error-free search. We suspect
that simple methods to “underestimate” the score of
a partial derivation’s remaining nonterminals could
bridge this gap in search error. BLEU scores in
the regions of lowest model cost tend to be reason-
ably stable and reflect comparable translation per-
formance for both methods.
Under both H.Search and CP, the hierarchical
grammar ultimately achieves a BLEU score of
19.1%, while the syntactic grammar’s score is ap-
proximately 1.5 points higher at 20.7%. The hierar-
chical grammar demonstrates a greater variance of
BLEU score for both CP and H.Search compared
to the syntax-based grammar. The use of syntac-
tic structure serves as an additional model of target
language fluency, and can explain the fact that syn-
tax based translation quality is more robust to differ-
ences in the number of g-gram LM options explored.
Decoding time plots shows a similar result,
but with diminished relative improvement for the
H.Search method. Profiling analysis of the H.Search
method shows that significant time is spent simply
on allocating and deallocating memory for partial
derivations on top of the scoring times for these
items. We expect to be able to reduce this overhead
significantly in future work.
</bodyText>
<sectionHeader confidence="0.996434" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.992223538461538">
We presented an novel two-pass decoding approach
for PSCFG-based machine translation that achieves
search errors comparable to the state of the art Cube
Pruning method. By maintaining comparable, sen-
tence spanning derivations we allow easy integration
of high or flexible order LMs as well as sentence
level syntactic features during the search process.
We plan to evaluate the impact of these more power-
ful models in future work. We also hope to address
the question of how much search error is tolerable to
3Analysis of total LM calls made by each method (not pre-
sented here) shows the H.Search makes significantly fewer (1/2)
total LM calls than CP to achieve each model cost.
</bodyText>
<figure confidence="0.995836723076923">
506
-2500
-2600
-2700
Model Cost
-2800
-2900
-3000
-3100
-3200
IWSLT - LM Cache Misses
Hierarchical
0.0E+00 2.5E+06 5.0E+06 7.5E+06
Number of LM Misses
0.174
CP
H.Seardh
0.175
0.177
0.178
0.180
0.181
0.182
0.191
0.191 0.188
0.191
0.186
37500
37475
Model Cost
37450
37425
37400
IWSLT - LM Cache Misses Syntax
2.0E+05 7.0E+05 1.2E+06
Number of LM Misses
0.205
0.2
CP
H.Search
0.207
0.2
0.206
0.206
0.207 0.206 0.207
37500
37480
Model Cost
37460
37440
37420
37400
IWSLT - Syntax Decoding Time
9.0E+02 9.8E+02 1.1E+03 1E+03 1.2E+03
Decoding Time (s)
0.205
0.206
CP
H.Search
0.206
0.207 0.207
0.205
0.206
0.207
0.206
</figure>
<figureCaption confidence="0.870362111111111">
Figure 3. LM caches misses for
IWSLT hierarchical grammar and
BLEU scores for varied pruning pa-
rameters
Figure 4. LM caches misses
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
Figure 5. Decoding time (s)
</figureCaption>
<bodyText confidence="0.996268166666667">
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
run MER training and still generate parameters that
generalize well to test data. This point is particularly
relevant to evaluate the use of search error analysis.
</bodyText>
<sectionHeader confidence="0.994328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999690137254902">
Bar-Hillel, M.Perles, and E.Shamir. 1964. An efficient
context-free parsing algorithm. Communications of
the Assocation for Computing Machinery.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. To Appear in the Journal of Computational Lin-
guistics.
C. Manning D. Klein. 2003. Accurate unlexicalized
parsing. In Proc. of ACL.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of Inter-
national Workshop on Spoken Language Translation,
pages 11–17.
Michael Galley, M. Hopkins, Kevin Knight, and Daniel
Marcu. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
NAACL-HLT.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of the 9th International Workshop on
Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3–15.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase based translation. In ACL.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In ACL.
Dekai Wu. 1996. A polynomial time algorithm for statis-
tical machine translation. In Proc. of the Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
</reference>
<page confidence="0.869226">
507
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421700">
<title confidence="0.99599">An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT</title>
<author confidence="0.976867">Venugopal Zollmann</author>
<affiliation confidence="0.775783">School of Computer Science, Carnegie Mellon University, interACT Lab, University of</affiliation>
<abstract confidence="0.989325863636364">We present an efficient, novel two-pass approach to mitigate the computational impact resulting from online intersection of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation. In first pass CYK-style decoding, we consider first-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this using the LM to search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Perles Bar-Hillel</author>
<author>E Shamir</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1964</date>
<journal>Communications of the Assocation for Computing Machinery.</journal>
<marker>Bar-Hillel, Shamir, 1964</marker>
<rawString>Bar-Hillel, M.Perles, and E.Shamir. 1964. An efficient context-free parsing algorithm. Communications of the Assocation for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1203" citStr="Chiang, 2005" startWordPosition="163" endWordPosition="164">art item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 1 Introduction Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing grammatically correct target translations as an implicit syntaxstructured language model, we acknowledge the value of n-gram language models (LM) in phrasebased approaches. Integrating n-gram LMs into P</context>
<context position="5032" citStr="Chiang, 2005" startWordPosition="761" endWordPosition="762"> ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule. We will assume ∼ to be implicitly defined by indexing the NT occurrences in -y from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in -y. Syntax-oriented PSCFG approaches typically ignore source structure, instead focussing on generating syntactically well formed target derivations. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f, the translation task under a PSCFG grammar can be expressed as 1While IWSLT represents a limited resource translation task (120K sentences of training data for Chinese-English), the problem of efficient n-gram LM integration is still critically important to efficient decoding, and our contributions can be expected to have an even more significant impact when decoding with grammars induced from larger corpora. e� = arg max P(D) {e |�]D. src(D)=f,tgt(D)=e} where tgt(D) refers</context>
<context position="9130" citStr="Chiang, 2005" startWordPosition="1485" endWordPosition="1486">each nonterminal X a PSCFG rule S — (X, (s)9−1X(\s)9−1, 1) We are only searching for the derivation of highest probability, so we can discard identical chart items that have lower weight. Since chart items are defined by their left-hand side nonterminal production, span, and the LM contexts e, we can safely discard these identical items since q has retained all context that could possibly impact the LM calculation. This process is commonly referred to as item recombination. Backpointers to antecedent cells are typically retained to allow N-Best extraction using an algorithm such as (Huang and Chiang, 2005). The impact of g-gram LM intersection during decoding is apparent in the final deduction step. Generating the set of consequent Z chart items involves combining m previously produced chart cells. Since each of these chart cells with given source span [i, A is identified by nonterminal symbol X and LM context e, we have at worst |N |* |TT|2(9−1) such chart cells in a span. The runtime of this algorithm is thus I n3 [ O |N ||TT|2(9−1) where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the smallest ind</context>
<context position="17141" citStr="Chiang, 2005" startWordPosition="2834" endWordPosition="2835">core are not put onto the agenda, while those that are better, or have created new equivalence classes are scheduled onto the agenda. For each newly created partial derivation, we also add a backpointer to the “parent” partial derivation that was unwound to create it. This equivalence classing operation transforms the original left-hand side NT based hypergraph into an (ordinary) graph of partial items. Each equivalence class is a node in this new graph, and recombined items are the edges. Thus, N-best extraction can now be performed on this graph. We use the extraction method from (Huang and Chiang, 2005). The expensive portion of our algorithm lies in the unwinding step, in which we generate a new partial item for each alternative at the non-terminal site that we are “unwinding”. For each new partial item, we factor out LM estimates and rule weights that were used to score the parent item, and factor in the LM probabilities and rule weights of the alternative choice that we are considering. In addition, we must also update the new item’s LM estimates for the remaining non-terminal and terminal symbols that depend on this new left context of terminals. Fortunately, the number of LM calculation</context>
<context position="21291" citStr="Chiang, 2005" startWordPosition="3543" endWordPosition="3544">sentence spanning hypergraph; ε denotes the empty word. Non-logical (Start) axiom: [[ε; PS]] : w*; Goal item: [[(s)9−1 ? (\s)9−1; ε]] : w that is based on the first-best instance of P to the actually chosen instance’s weight. Further, the ratio p(eq(αlexαmid))/p(ee1) adjusts the LM probabilities of P’s instantiation given its left context, and p[q(eαlexαmid)q(αend)]/p[q(ee1)q(αend)] adjusts the LM probabilities of the g − 1 words right of P. 4 Alternative Approaches We evaluate our two pass hypergraph search “H.Search” against the strong single pass Cube Pruning (CP) baseline as mentioned in (Chiang, 2005) and detailed in (Chiang, 2007). In the latter work, the author shows that CP clearly outperforms both the naive single pass solution of severe pruning as well as the naive two-pass rescoring approach. Thus, we focus on comparing our approach to CP. CP is an optimization to the intersected LM parsing algorithm presented in Figure 1. It addresses the creation of the HKk�1 �[Xk, ik, A, *] chart items when generating consequent items. CP amounts to an early termination condition when generating the set of possible consequents. Instead of generating all consequents, and then pruning away the poor </context>
<context position="23476" citStr="Chiang, 2005" startWordPosition="3902" endWordPosition="3903">s with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters A used to calculate P(D) are trained using MER training (Och, 2003) on development data. 6 Comparison of Approaches We evaluate each approach by considering both search errors made on the development data for a fixed set of model parameters, and the BLEU metric to judge translation quality. 6.1 Search Error Analysis While it is common to evaluate MT quality using the BLEU score, we would like to evaluate search errors made as a function of “effo</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase based translation.</title>
<date>2007</date>
<journal>the Journal of Computational Linguistics.</journal>
<note>To Appear in</note>
<contexts>
<context position="1007" citStr="Chiang, 2007" startWordPosition="136" endWordPosition="137">ction of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation. In first pass CYK-style decoding, we consider first-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 1 Introduction Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing gramma</context>
<context position="2634" citStr="Chiang, 2007" startWordPosition="369" endWordPosition="370">ammar, rendering the decoding process essentially infeasible without severe, beam-based lossy pruning. The alternative, simply decoding without the n-gram LM and rescoring N-best alternative translations, results in substantially more search errors, as shown in (Zollmann and Venugopal, 2006). Our two-pass approach involves fast, approximate synchronous parsing in a first stage, followed by a second, detailed exploration through the resulting hypergraph of sentence spanning derivations, using the n-gram LM to drive that search. This achieves search errors comparable to a strong “Cube Pruning” (Chiang, 2007), single-pass baseline. The first pass corresponds to a severe parameterization of Cube Pruning considering only the first-best (LM integrated) chart item in each cell while maintaining unexplored alternatives for second-pass consideration. Our second stage allows the integration of long distance and flexible history n-gram LMs to drive the search process, rather than simply using such models for hypothesis rescoring. We begin by discussing the PSCFG model for statistical machine translation, motivating the need 500 Proceedings of NAACL HLT 2007, pages 500–507, Rochester, NY, April 2007. c�200</context>
<context position="6536" citStr="Chiang, 2007" startWordPosition="1002" endWordPosition="1003">used in D, ism} P(D) = 1 PLM(tgt(D))ALM × i where Oi refers to features defined on each rule, and PLM is a g-gram LM probability applied to the target terminal symbols generated by the derivation D. Introducing the LM feature defines dependencies across adjacent rules used in each derivation, and requires modifications to the decoding strategy. Viewing the LM as a finite-state machine, the decoding process involves performing an intersection between the PSCFG grammar and the g-gram LM (Bar-Hillel et al., 1964). We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule. 2.1 Decoding Strategies In Figure 1, we reproduce the decoding algorithm from (Chiang, 2007) that applies a PSCFG to translate a source sentence in the same notation (as a deductive proof system (Shieber et al., 1995)), generalized to handle more than two non-terminal pairs. Chart items [X, i, j, e] : w span j − i words in the source sentence f1 · · · f, starting at position i + 1, and have weight w (equivalent to P(D)), and e ∈ (TT ∪ {*})* is a sequence of target terminals, with possi</context>
<context position="10839" citStr="Chiang, 2007" startWordPosition="1768" endWordPosition="1769">ton et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gram LM. In practice, this approach performs poorly (Chiang, 2007; Zollmann and Venugopal, 2006). While parsing time is dramatically reduced (and N-best extraction time is negligible), N is typically significantly less than the complete number of possible derivations and substantial search errors remain. We propose an approach that builds upon the concept of a second pass but uses the g-gram LM to search for alternative, better translations. 1&amp;quot;) 502 3.1 First pass: parsing We begin by relaxing the criterion that determines when two chart items are equivalent during parsing. We consider two chart items to be equivalent (and therefore candidates for recombina</context>
<context position="21322" citStr="Chiang, 2007" startWordPosition="3548" endWordPosition="3549"> denotes the empty word. Non-logical (Start) axiom: [[ε; PS]] : w*; Goal item: [[(s)9−1 ? (\s)9−1; ε]] : w that is based on the first-best instance of P to the actually chosen instance’s weight. Further, the ratio p(eq(αlexαmid))/p(ee1) adjusts the LM probabilities of P’s instantiation given its left context, and p[q(eαlexαmid)q(αend)]/p[q(ee1)q(αend)] adjusts the LM probabilities of the g − 1 words right of P. 4 Alternative Approaches We evaluate our two pass hypergraph search “H.Search” against the strong single pass Cube Pruning (CP) baseline as mentioned in (Chiang, 2005) and detailed in (Chiang, 2007). In the latter work, the author shows that CP clearly outperforms both the naive single pass solution of severe pruning as well as the naive two-pass rescoring approach. Thus, we focus on comparing our approach to CP. CP is an optimization to the intersected LM parsing algorithm presented in Figure 1. It addresses the creation of the HKk�1 �[Xk, ik, A, *] chart items when generating consequent items. CP amounts to an early termination condition when generating the set of possible consequents. Instead of generating all consequents, and then pruning away the poor performers, CP uses the K-Best </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase based translation. To Appear in the Journal of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning D Klein</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="23225" citStr="Klein, 2003" startWordPosition="3863" endWordPosition="3864">mework We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters A used to calculate P(D) are trained using MER training (Och, 2003) on development data. 6 Comparison of Approaches We evaluate each approach by considering both search errors made on the developmen</context>
</contexts>
<marker>Klein, 2003</marker>
<rawString>C. Manning D. Klein. 2003. Accurate unlexicalized parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2005</date>
<booktitle>In Proc. of International Workshop on Spoken Language Translation,</booktitle>
<pages>11--17</pages>
<contexts>
<context position="3618" citStr="Eck and Hori, 2005" startWordPosition="518" endWordPosition="521"> than simply using such models for hypothesis rescoring. We begin by discussing the PSCFG model for statistical machine translation, motivating the need 500 Proceedings of NAACL HLT 2007, pages 500–507, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics for effective n-gram LM integration during decoding. We then present our two-pass approach and discuss Cube Pruning as a state-of-the-art baseline. We present results in the form of search error analysis and translation quality as measured by the BLEU score (Papineni et al., 2002) on the IWSLT 06 text translation task (Eck and Hori, 2005)1, comparing Cube Pruning with our two-pass approach. 2 Synchronous Parsing for SMT Probabilistic Synchronous Context Free Grammar (PSCFG) approaches to statistical machine translation use a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT and a shared nonterminal set N and induce rules of the form X → h-y, α,∼,wi where (i) X ∈ N is a nonterminal, (ii) -y ∈ (N ∪ TS)* is a sequence of nonterminals and source terminals, (iii) α ∈ (N ∪ TT)* is a sequence of nonterminals and target terminals, (iv) the number cnt(-y) of nonterminal occurrences in -y is equal </context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>Matthias Eck and Chiori Hori. 2005. Overview of the IWSLT 2005 evaluation campaign. In Proc. of International Workshop on Spoken Language Translation, pages 11–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>M Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="1152" citStr="Galley et al., 2006" startWordPosition="154" endWordPosition="157">n first pass CYK-style decoding, we consider first-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 1 Introduction Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing grammatically correct target translations as an implicit syntaxstructured language model, we acknowledge the value of n-gram language models (LM) in ph</context>
<context position="4868" citStr="Galley et al., 2006" startWordPosition="735" endWordPosition="738">minal occurrences in α, (v) ∼: {1, ... , cnt(-y)} → {1, ... , cnt(α)} is a one-toone mapping from nonterminal occurrences in -y to nonterminal occurrences in α, and (vi) w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule. We will assume ∼ to be implicitly defined by indexing the NT occurrences in -y from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in -y. Syntax-oriented PSCFG approaches typically ignore source structure, instead focussing on generating syntactically well formed target derivations. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f, the translation task under a PSCFG grammar can be expressed as 1While IWSLT represents a limited resource translation task (120K sentences of training data for Chinese-English), the problem of efficient n-gram LM integration is still critically important to efficient decoding, and our contributions can be expecte</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2006</marker>
<rawString>Michael Galley, M. Hopkins, Kevin Knight, and Daniel Marcu. 2006. Scalable inferences and training of context-rich syntax translation models. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. of the 9th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="9130" citStr="Huang and Chiang, 2005" startWordPosition="1483" endWordPosition="1486">dding for each nonterminal X a PSCFG rule S — (X, (s)9−1X(\s)9−1, 1) We are only searching for the derivation of highest probability, so we can discard identical chart items that have lower weight. Since chart items are defined by their left-hand side nonterminal production, span, and the LM contexts e, we can safely discard these identical items since q has retained all context that could possibly impact the LM calculation. This process is commonly referred to as item recombination. Backpointers to antecedent cells are typically retained to allow N-Best extraction using an algorithm such as (Huang and Chiang, 2005). The impact of g-gram LM intersection during decoding is apparent in the final deduction step. Generating the set of consequent Z chart items involves combining m previously produced chart cells. Since each of these chart cells with given source span [i, A is identified by nonterminal symbol X and LM context e, we have at worst |N |* |TT|2(9−1) such chart cells in a span. The runtime of this algorithm is thus I n3 [ O |N ||TT|2(9−1) where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the smallest ind</context>
<context position="17141" citStr="Huang and Chiang, 2005" startWordPosition="2832" endWordPosition="2835"> a lower score are not put onto the agenda, while those that are better, or have created new equivalence classes are scheduled onto the agenda. For each newly created partial derivation, we also add a backpointer to the “parent” partial derivation that was unwound to create it. This equivalence classing operation transforms the original left-hand side NT based hypergraph into an (ordinary) graph of partial items. Each equivalence class is a node in this new graph, and recombined items are the edges. Thus, N-best extraction can now be performed on this graph. We use the extraction method from (Huang and Chiang, 2005). The expensive portion of our algorithm lies in the unwinding step, in which we generate a new partial item for each alternative at the non-terminal site that we are “unwinding”. For each new partial item, we factor out LM estimates and rule weights that were used to score the parent item, and factor in the LM probabilities and rule weights of the alternative choice that we are considering. In addition, we must also update the new item’s LM estimates for the remaining non-terminal and terminal symbols that depend on this new left context of terminals. Fortunately, the number of LM calculation</context>
<context position="21969" citStr="Huang and Chiang, 2005" startWordPosition="3654" endWordPosition="3657">e author shows that CP clearly outperforms both the naive single pass solution of severe pruning as well as the naive two-pass rescoring approach. Thus, we focus on comparing our approach to CP. CP is an optimization to the intersected LM parsing algorithm presented in Figure 1. It addresses the creation of the HKk�1 �[Xk, ik, A, *] chart items when generating consequent items. CP amounts to an early termination condition when generating the set of possible consequents. Instead of generating all consequents, and then pruning away the poor performers, CP uses the K-Best extraction approach of (Huang and Chiang, 2005) to select the best K consequents only, at the cost of potential search errors. CP’s termination condition can be defined in terms of an absolute number of consequents to generate, or by terminating the generation process when a newly generated item is worse (by Q) than the current best item for the same left-hand side and span. To simulate comparable pruning criteria we parameterize each method with soft-threshold based criteria only (Q for CP and Z for H.Search) since counter based limits like K have different effects in CP (selecting e labeled items) vs H.Search (selecting rules since items</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of the 9th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="23114" citStr="Koehn et al., 2003" startWordPosition="3844" endWordPosition="3847">in CP (selecting e labeled items) vs H.Search (selecting rules since items are not labeled with e). 5 Experimental Framework We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters A used to calculate P(D) are trained using MER training (Och, 2003) on development data</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>6--7</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="23694" citStr="Och, 2003" startWordPosition="3939" endWordPosition="3940"> “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters A used to calculate P(D) are trained using MER training (Och, 2003) on development data. 6 Comparison of Approaches We evaluate each approach by considering both search errors made on the development data for a fixed set of model parameters, and the BLEU metric to judge translation quality. 6.1 Search Error Analysis While it is common to evaluate MT quality using the BLEU score, we would like to evaluate search errors made as a function of “effort” made by each algorithm to produce a first-best translation. We consider two metrics of effort made by each algorithm. 505 We first evaluate search errors as a function of novel queries made to the g-gram LM (since </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, Sapporo, Japan, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3559" citStr="Papineni et al., 2002" startWordPosition="507" endWordPosition="510">lexible history n-gram LMs to drive the search process, rather than simply using such models for hypothesis rescoring. We begin by discussing the PSCFG model for statistical machine translation, motivating the need 500 Proceedings of NAACL HLT 2007, pages 500–507, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics for effective n-gram LM integration during decoding. We then present our two-pass approach and discuss Cube Pruning as a state-of-the-art baseline. We present results in the form of search error analysis and translation quality as measured by the BLEU score (Papineni et al., 2002) on the IWSLT 06 text translation task (Eck and Hori, 2005)1, comparing Cube Pruning with our two-pass approach. 2 Synchronous Parsing for SMT Probabilistic Synchronous Context Free Grammar (PSCFG) approaches to statistical machine translation use a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT and a shared nonterminal set N and induce rules of the form X → h-y, α,∼,wi where (i) X ∈ N is a nonterminal, (ii) -y ∈ (N ∪ TS)* is a sequence of nonterminals and source terminals, (iii) α ∈ (N ∪ TT)* is a sequence of nonterminals and target terminals, (iv) th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="6863" citStr="Shieber et al., 1995" startWordPosition="1053" endWordPosition="1056">s to the decoding strategy. Viewing the LM as a finite-state machine, the decoding process involves performing an intersection between the PSCFG grammar and the g-gram LM (Bar-Hillel et al., 1964). We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule. 2.1 Decoding Strategies In Figure 1, we reproduce the decoding algorithm from (Chiang, 2007) that applies a PSCFG to translate a source sentence in the same notation (as a deductive proof system (Shieber et al., 1995)), generalized to handle more than two non-terminal pairs. Chart items [X, i, j, e] : w span j − i words in the source sentence f1 · · · f, starting at position i + 1, and have weight w (equivalent to P(D)), and e ∈ (TT ∪ {*})* is a sequence of target terminals, with possible elided parts, marked by *. Functions p, q whose domain is TT ∪ {*} are defined in (Chiang, 2007) and are repeated here for clarity. PLM(ai|ai−g+1 ··· ai−1) g≤i≤m,*%ai−g}1···ai−1 � a1 ··· ag−1 * am−g+2 ··· am if m &gt; g �(a1 · · · am) =a1 · · · am else The function q elides elements from a target language terminal sequence, </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Alternative quantifier scope in CCG.</title>
<date>1999</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4999" citStr="Steedman, 1999" startWordPosition="756" endWordPosition="758">rminal occurrences in α, and (vi) w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule. We will assume ∼ to be implicitly defined by indexing the NT occurrences in -y from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in -y. Syntax-oriented PSCFG approaches typically ignore source structure, instead focussing on generating syntactically well formed target derivations. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f, the translation task under a PSCFG grammar can be expressed as 1While IWSLT represents a limited resource translation task (120K sentences of training data for Chinese-English), the problem of efficient n-gram LM integration is still critically important to efficient decoding, and our contributions can be expected to have an even more significant impact when decoding with grammars induced from larger corpora. e� = arg max P(D) {e |�]D. src(D</context>
</contexts>
<marker>Steedman, 1999</marker>
<rawString>Mark Steedman. 1999. Alternative quantifier scope in CCG. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase based translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10176" citStr="Watanabe et al., 2006" startWordPosition="1665" endWordPosition="1669">N ||TT|2(9−1) where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gr</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase based translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10244" citStr="Wellington et al., 2006" startWordPosition="1675" endWordPosition="1678">nd n the source sentence length. Without severe pruning, this runtime is prohibitive for even the smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gram LM. In practice, this approach performs poorly (Chiang, 2007; Zol</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6496" citStr="Wu, 1996" startWordPosition="997" endWordPosition="998">ear model, referring to the rules r used in D, ism} P(D) = 1 PLM(tgt(D))ALM × i where Oi refers to features defined on each rule, and PLM is a g-gram LM probability applied to the target terminal symbols generated by the derivation D. Introducing the LM feature defines dependencies across adjacent rules used in each derivation, and requires modifications to the decoding strategy. Viewing the LM as a finite-state machine, the decoding process involves performing an intersection between the PSCFG grammar and the g-gram LM (Bar-Hillel et al., 1964). We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule. 2.1 Decoding Strategies In Figure 1, we reproduce the decoding algorithm from (Chiang, 2007) that applies a PSCFG to translate a source sentence in the same notation (as a deductive proof system (Shieber et al., 1995)), generalized to handle more than two non-terminal pairs. Chart items [X, i, j, e] : w span j − i words in the source sentence f1 · · · f, starting at position i + 1, and have weight w (equivalent to P(D)), and e ∈ (TT ∪ {*})* is a </context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial time algorithm for statistical machine translation. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="10104" citStr="Zhang et al., 2006" startWordPosition="1654" endWordPosition="1657">art cells in a span. The runtime of this algorithm is thus I n3 [ O |N ||TT|2(9−1) where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation, HLT/NAACL,</booktitle>
<location>New York,</location>
<contexts>
<context position="2313" citStr="Zollmann and Venugopal, 2006" startWordPosition="321" endWordPosition="324">model, we acknowledge the value of n-gram language models (LM) in phrasebased approaches. Integrating n-gram LMs into PSCFGs based decoding can be viewed as online intersection of the PSCFG grammar with the finite state machine represented by the n-gram LM, dramatically increasing the effective number of nonterminals in the decoding grammar, rendering the decoding process essentially infeasible without severe, beam-based lossy pruning. The alternative, simply decoding without the n-gram LM and rescoring N-best alternative translations, results in substantially more search errors, as shown in (Zollmann and Venugopal, 2006). Our two-pass approach involves fast, approximate synchronous parsing in a first stage, followed by a second, detailed exploration through the resulting hypergraph of sentence spanning derivations, using the n-gram LM to drive that search. This achieves search errors comparable to a strong “Cube Pruning” (Chiang, 2007), single-pass baseline. The first pass corresponds to a severe parameterization of Cube Pruning considering only the first-best (LM integrated) chart item in each cell while maintaining unexplored alternatives for second-pass consideration. Our second stage allows the integratio</context>
<context position="4960" citStr="Zollmann and Venugopal, 2006" startWordPosition="748" endWordPosition="751">e mapping from nonterminal occurrences in -y to nonterminal occurrences in α, and (vi) w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule. We will assume ∼ to be implicitly defined by indexing the NT occurrences in -y from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in -y. Syntax-oriented PSCFG approaches typically ignore source structure, instead focussing on generating syntactically well formed target derivations. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f, the translation task under a PSCFG grammar can be expressed as 1While IWSLT represents a limited resource translation task (120K sentences of training data for Chinese-English), the problem of efficient n-gram LM integration is still critically important to efficient decoding, and our contributions can be expected to have an even more significant impact when decoding with grammars induced from larger co</context>
<context position="10870" citStr="Zollmann and Venugopal, 2006" startWordPosition="1770" endWordPosition="1773">06) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gram LM. In practice, this approach performs poorly (Chiang, 2007; Zollmann and Venugopal, 2006). While parsing time is dramatically reduced (and N-best extraction time is negligible), N is typically significantly less than the complete number of possible derivations and substantial search errors remain. We propose an approach that builds upon the concept of a second pass but uses the g-gram LM to search for alternative, better translations. 1&amp;quot;) 502 3.1 First pass: parsing We begin by relaxing the criterion that determines when two chart items are equivalent during parsing. We consider two chart items to be equivalent (and therefore candidates for recombination) if they have matching lef</context>
<context position="23019" citStr="Zollmann and Venugopal, 2006" startWordPosition="3828" endWordPosition="3832">sed criteria only (Q for CP and Z for H.Search) since counter based limits like K have different effects in CP (selecting e labeled items) vs H.Search (selecting rules since items are not labeled with e). 5 Experimental Framework We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Par</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of the Workshop on Statistical Machine Translation, HLT/NAACL, New York, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>