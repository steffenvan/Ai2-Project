<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006121">
<title confidence="0.995712">
A Learning-based Sampling Approach to Extractive Summarization
</title>
<author confidence="0.824957">
Vishal Juneja and Sebastian Germesin and Thomas Kleinbauer
</author>
<affiliation confidence="0.776698">
German Research Center for Artificial Intelligence
</affiliation>
<address confidence="0.933867">
Campus D3.2
66123 Saarb¨ucken, Germany
</address>
<email confidence="0.99932">
{firstname.lastname}@dfki.de
</email>
<sectionHeader confidence="0.998673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9931475">
In this paper we present a novel resampling
model for extractive meeting summarization.
With resampling based on the output of a base-
line classifier, our method outperforms previ-
ous research in the field. Further, we com-
pare an existing resampling technique with
our model. We report on an extensive se-
ries of experiments on a large meeting corpus
which leads to classification improvement in
weighted precision and f-score.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989697847826087">
Feature-based machine learning approaches have
become a standard technique in the field of extrac-
tive summarization wherein the most important sec-
tions within a meeting transcripts need to be iden-
tified. We perceive the problem as recognizing the
most extract-worthy meeting dialog acts (DAs) in a
binary classification framework.
In this paper, firstly, in section 4 we create a gold
standard to train the classifier, by improvising upon
the existing annotations in our meeting corpus. Then
in section 5 we present actual numbers which dis-
play a very skewed class distribution to learn for
the binary classifier. This skewness is attributed to
the less number of actual extract-worthy and im-
portant DAs (positive examples) compared to ordi-
nary chit-chat, backchannel noises etc (negative ex-
amples) spoken during the course of the meeting.
We tackle this data skewness with a novel resam-
pling approach which reselects the data set to create
a more comparable class distribution between these
postive and negative instances.
34
Resampling methods have been found effective in
catering to the data imbalance problem mentioned
above. (Corbett and Copestake, 2008) used a re-
sampling module for chemical named entity recog-
nition. The pre-classifier, based on n-gram character
features, assigned a probability of being a chemical
word, to each token. Only tokens having probability
greater than a predefined threshold were preserved
and the output of the first stage classification along
with word suffix were used as features in further
classification steps. (Hinrichs et al., 2005) used a
hybrid approach for Computational Anaphora Res-
olution (CAR) combining rule based filtering with
Memory based learning to reduce the huge popu-
lation of anaphora/candidate-antecedent pairs. (Xie
et al., 2008), in their experimentation on the ICSI
meeting corpus, employ the salience scores gener-
ated by a TFIDF classifier in the resampling task.
We discuss the actual technique and our resampling
module further in section 6.
We compare its performance with the TFIDF
model of (Xie et al., 2008) in section 8.2 and observe
a general improvement in summary scores through
resampling.
</bodyText>
<sectionHeader confidence="0.994482" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999946714285714">
We use the scenario meetings of the AMI corpus
for our experiments in this paper which comprise
about two thirds of around 100 hours of recorded
and annotated meetings. The scenario meetings each
have four participants who play different roles in a
fictitious company for designing a remote control.
The AMI corpus has a standard training set of 94
</bodyText>
<subsectionHeader confidence="0.496663">
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 34–39,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999564923076923">
meetings1 and 20 meetings each for development
and testing.
Annotators wrote abstractive summaries for each
meeting and then linked summary sentences to
those DA segments from the meeting transcripts
which best conveyed the information in the ab-
stracts. There was no limit on the number of links an
annotator could create and a many-to-many mapping
exists between the meeting DA segments and human
abstracts. Here, DA segments are used in analogy
to sentences in document summarization because
the spontaneously spoken material in meeting tran-
scripts rarely contains actual grammatical sentences.
</bodyText>
<sectionHeader confidence="0.981797" genericHeader="method">
3 Pre-processing and Feature Extraction
</sectionHeader>
<bodyText confidence="0.99959525">
To the feature set of (Murray, 2008) listed in table
1 we add some high level features. Since the main
focus of this paper is to deal with the data imbal-
anace issue hence for the sake of completeness and
reproducibility of our work we briefly mention the
basic features used. In section 8.3 we explicitly re-
port the performance rise over the baseline due to
the added features.
</bodyText>
<subsectionHeader confidence="0.999377">
3.1 Lexical and Structural features
</subsectionHeader>
<bodyText confidence="0.999986090909091">
The list of added features include the number of
content words (nouns and adjectives) in a DA. (Ed-
mundson, 1969) looked at cue-phrases, keywords
title and location of a sentence as features indica-
tive of important sections in a document. We use
a handpicked list of cue words like ”for example”,
”gonna have” etc as binary features. We also add
several keywords like ”remote”,”plastic” etc based
upon manual scrutiny, as binary features into the
classifier. Further we use DA labels of current and
four adjacent DAs as features.
</bodyText>
<subsectionHeader confidence="0.997305">
3.2 Disfluency
</subsectionHeader>
<bodyText confidence="0.999972333333333">
The role of disfluencies in summarization has been
investigated by (Zhu and Penn, 2006) before. They
found that disfluencies improve summarization per-
formance when used as an additional feature. We
count the number of disfluent words in a DA using
an automatic disfluency detector.
</bodyText>
<footnote confidence="0.470897">
1Three of the meetings were missing some required features.
</footnote>
<subsectionHeader confidence="0.997826">
3.3 Prosodic
</subsectionHeader>
<bodyText confidence="0.98756232">
We employ all the signal level features described by
(Murray, 2008) which include mean, max and stan-
dard deviation of energy and pitch values normal-
ized by both speaker and meeting. The duration of
the DA in terms of time and number of words spo-
ken. The subsequent, precedent pauses and rate of
speech feature.
DA Features
mean energy
mean pitch
maximum energy value
maximum pitch value
standard deviation of pitch
precedent pause
subsequent pause
uninterrupted length
number of words
position in the meeting
position in the speaker turn
DA time duration
speaker dominance in DA
speaker dominance in time
rate of speech
SUIDF score
TFIDF score
</bodyText>
<tableCaption confidence="0.998055">
Table 1: Features used in baseline classifier
</tableCaption>
<sectionHeader confidence="0.97563" genericHeader="method">
4 Gold Standard
</sectionHeader>
<bodyText confidence="0.9999728125">
In supervised frameworks, the creation of gold-
standard annotations for training (and testing) is
known to be a difficult task, since (a) what should
go into a summary can be a matter of opinion and
(b) multiple sentences from the original document
may express similar content, making each of them
equally good candidates for selection. The hypoth-
esis is well supported by the low kappa value (Co-
hen, 1960) of 0.48 reported by (Murray, 2008) on
the AMI corpus.
We describe the procedure for creating the gold
standard for our experimentation in this paper.
Firstly we join all annotations and rank the DAs
from most number of links to least number of links
to create a sorted list of DAs. Depending on a pre-
defined variable percentage as gold standard cut-off
</bodyText>
<page confidence="0.995942">
35
</page>
<bodyText confidence="0.999931428571429">
or threshold we preserve the corresponding number
of highest ranked DAs in the above list. For evalu-
ation, (Murray, 2008) uses gold standard summaries
obtained using similar procedure. For training, how-
ever, he uses all DA segments with at least one link
as positive examples.
As the term gold standard for the data set, cre-
ated above, is misleading. We call the set of DAs
so obtained by using this ranking and resampling
procedure as Weighted-Resampled Gold Standard
(WRGS). Henceforth in this paper, for a resampling
rate of say 35% we will name the set of DAs so ob-
tained as WRGS(35%) or simply WRGS for some
undefined, arbitrary threshold.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="method">
5 Data Skewness
</sectionHeader>
<bodyText confidence="0.9998705">
In this section we focus on the skewed data set
which arises because of creating WRGS for training
our classifiers. Consider the set of DAs with at least
one link to the abstractive or human summaries. Let
us call it DAl&apos;—1. This set accounts for 20.9% of all
DAs in the training set.
</bodyText>
<table confidence="0.987026">
set size%
WRGS(25%) 5.22%
DAl&apos;—1 20.9%
</table>
<tableCaption confidence="0.999347">
Table 2: Set sizes in % of all training DAs
</tableCaption>
<bodyText confidence="0.999353529411765">
Again consider set of DAs for WRGS(25%). This
set, by definition, contains 25% of all DAs in the
set DAl&apos;—1. Hence the set WRGS(25%) constitute
5.22% of all DAs in the training set. Note that this is
a skewed class distribution as also visible in table 2.
Our system employs resampling architecture
shown in figure 1. The first classifier is similar in
spirit to the one developed in (Murray, 2008) with
the additional features listed in section 3. The out-
put we use is not the discrete classification result but
rather the probability for each DA segment to be ex-
tracted.
These probabilities are used in two ways for train-
ing the second classifier: firstly, to create the resam-
pled training set and secondly, as an additional fea-
ture for the second classifier. The procedure for re-
sampling is explained in the section 6.
</bodyText>
<figureCaption confidence="0.992062">
Figure 1: A two-step classification architecture for ex-
tractive meeting summarization.
</figureCaption>
<sectionHeader confidence="0.998074" genericHeader="method">
6 Resampling
</sectionHeader>
<bodyText confidence="0.999723757575758">
As explained in previous section our model obtains
resampled data for second stage classification using
the probabilistic outcomes of a first stage classifier.
The resampling is done similar to (Xie et al., 2008)
to cater to the data skewness problem. To do the
resampling, firstly, the DAs are ranked on decreasing
probabilities. In the next step, depending on some
resampling rate, a percentage of highest ranked DAs
is used in further classification steps, while rest of
DA segments are neglected.
(Xie et al., 2008) obtained the resampled set by
ranking the DAs on TFIDF weights. Data resam-
pling benefits the model in two ways a) by improv-
ing the positive/negative example ratio during the
training phase b) by discarding noisy utterances in
the test phase as they usually attain low scores from
the first classifier.
In testing, the first classifier is run on the test data,
its output is used, as in training, to create the resam-
pled test set and the probability features. Finally,
the summary is created from the probabilities pro-
duced by the second classifier by selecting the high-
est ranked DA segments for the specified summary
length.
As the data for resampling is derived by a
learning-based classifier, we call our approach
Learning-Based Sampling (LBS).
In this paper, we compare our LBS model with
the TFIDF sampling approach adopted by (Xie et
al., 2008) and present the results of resampling on
both models in section 8.2.
For comparison, we use Murray’s (2008) state of
art extractive summarization model.
</bodyText>
<figure confidence="0.998037375">
Resampled
Training Set
Training Set
First Classifier /
Resampler
Second
Classifier
probabilties
</figure>
<page confidence="0.930801">
36
</page>
<bodyText confidence="0.990139259259259">
7 Evaluation Metric tained by any summary corresponding to this com-
The main metric we use for evaluating the sum- pression rate. Weighted Recall on the other hand
maries is the extension of the weighted precision signifies total information content of the meeting.
evaluation scheme introduced by (Murray, 2008). For intelligent systems in general the recall rate in-
The measure relies on having multiple annotations creases with increasing summary compression rates
for a meeting and a many-to-many mapping dis- while weighted precision decreases2.
cussed in section 2. To calculate weighted precision, Since we experiment with short summaries that
the number of times that each extractive summary have at most 700 words, we do most of the com-
DA was linked by each annotator is counted and av- parisons in terms of weighted precision values. In
eraged to get a single DA score. The DA scores are the final system evaluation in section 8.3, we include
then averaged over all DAs in the summary to get weighted recall and f-score values.
the weighted precision score for the entire summary. 8 Experimental Results and Discussion
The total number of links in an extractive summary 8.1 Training on gold standard
divided by the total number of links to the abstract as Figure 2 shows the weighted precision results on
a whole gives the weighted recall score. By this def- training an SVM classifier with different gold stan-
inition, weighted recall can have a maximum score dard thresholds. For example, at a threshold of 60%,
of 1 since it is a fraction of the total links for the en- the top 60% of the linked DA segments are defined
tire summary. Also, there is no theoretical maximum as the gold standard positive examples, all other DA
for weighted precision as annotators were allowed to segments of the meeting are defined as negative,
create any number of links for a single DA. non-extraction worthy. The tests are performed on
Both weighted precision and recall share the same a single stage classifier similar to (Murray, 2008).
numerator: num = Ed Ld/N where Ld is the num- In addition, the curves show the behavior of the
ber of links for a DA d in the extractive summary, system at three different summary compression rates
and N is the number of annotators. Weighted pre- (i.e., number of words in the summary). A gen-
cision is equal to wp = num/D3 where D3 is the eral tendency that can be observed is the increase
number of DAs in the extractive summary. Weighted in summary scores with decreasing threshold. For
recall is given by recall = num/(Lt/N) where Lt 700 word summaries the peak weighted precision
is the total number of links made between DAs and score is observed at 35% threshold. The recall rate
abstract sentences by all annotators, and N is the remains constant as seen by comparing the first two
number of annotators. The f-score is calculated as: rows of table 5.
(2 x wp x recall)/(wp + recall). We believe that low inter annotator agreement is
In simple terms a DA which might be discussing the major factor responsible for these results. This
an important meeting topic e.g. selling price of the shows that a reduced subset classification approach
remote control etc is more likely to be linked by will generally improve results when multiple anno-
more than one annotator and possibly more than tations are available.
once by an annotator. Therefore the high scoring 8.2 Resampling
DAs are in a way indicative of quintessential topics In this section we compare two resampling models.
and agenda points of the meeting. Hence, weighted The TFIDF model explained in section 6 selects best
precision which is number of links per annotator DAs based on their TFIDF scores. As discussed
averaged over all the meeting DAs is a figure that
aligns itself with average information content per
DA in the summary. Low scoring meeting chit-chats
will tend to bring the precision score down. We re-
port a weighted precision of 1.33 for 700 word sum-
mary extracted using the procedure described in 2
for obtaining gold standard. This is hence a ceil-
ing to the weighted precision score that can be ob-
37
2An important point to notice is that, a high recall rate does
not ensure a good content coverage by the summary. As an
example, the summary might pick up DAs pertaining to only a
few very important points discussed during the meeting which
will lead to a high recall rate although lesser important concepts
may still be exclusive.
</bodyText>
<figureCaption confidence="0.985294">
Figure 2: SVM at different compression rates.
</figureCaption>
<table confidence="0.997853818181818">
LBS outperforms maximum f-scores of 0.233 and
0.305 (table 3) for TFIDF.
# words: 700 1000
resampl. % wp f-score wp f-score
15 .684 .236 .662 .309
25 .706 .244 .664 .317
35 .710 .248 .664 .319
55 .707 .245 .652 .313
75 .702 .239 .650 .310
85 .702 .239 .642 .307
100 .692 .236 .639 .306
</table>
<tableCaption confidence="0.999925">
Table 4: weighted precision, f-scores on LBS model
</tableCaption>
<bodyText confidence="0.999795">
previously all sentences above a resampling thresh-
old are preserved while rest are discarded. In 8.2.2
resampling is done from the probabilities of a first
stage classifier. SVM model is used for both first
and second stage classification.
</bodyText>
<subsectionHeader confidence="0.733892">
8.2.1 TFIDF Resampling
</subsectionHeader>
<bodyText confidence="0.95767">
Table 3 reports weighted precision and f-scores at
two compression rates. The highest f-scores for 700,
1000 word summaries are obtained at 85% and 55%
respectively. Plots of figure 3 compare weighted
precision scores for LBS and TFIDF models.
</bodyText>
<table confidence="0.999831666666667">
# words: 700 1000
resampl. % wp f-score wp f-score
15 .631 .217 .600 .274
25 .670 .227 .610 .282
35 .673 .227 .630 .296
55 .685 .231 .641 .305
75 .689 .232 .632 .302
85 .692 .233 .631 .299
100 .686 .231 .637 .302
</table>
<tableCaption confidence="0.983066">
Table 3: TFIDF weighted Precision, f-score for 700 and
1000 word summaries
</tableCaption>
<sectionHeader confidence="0.469152" genericHeader="method">
8.2.2 LBS
</sectionHeader>
<bodyText confidence="0.99807">
The peak performance of the LBS model is ob-
served at resampling rate of 35% for both 700 and
1000 word summaries as seen in table 4. The maxi-
mum f-scores, 0.248 and 0.319 (table 4) obtained for
</bodyText>
<figureCaption confidence="0.9989385">
Figure 3: LBS and TFIDF wp values at different com-
pression rates.
</figureCaption>
<bodyText confidence="0.999890333333333">
From figure 4 which shows positive example re-
tention against sampling rate for TFIDF and LBS it
is clear that for all sampling rates, LBS provides a
higher rate of positive examples.
Also as discussed above, using a learning-based
first classifier produces probability values that can
be leveraged as features for the second classifier. We
speculate that this also contributes to the differences
in overall performance.
</bodyText>
<subsectionHeader confidence="0.998259">
8.3 Overall System Performance
</subsectionHeader>
<bodyText confidence="0.9994562">
In this section we report weighted precision, recall
and f-score for 700-word summaries, comparing re-
sults of the new model with the initial baseline sys-
tem.
As shown in table 5, training the system on
</bodyText>
<page confidence="0.998137">
38
</page>
<figureCaption confidence="0.999787">
Figure 4: LBS and TFIDF retention rates.
</figureCaption>
<bodyText confidence="0.995610090909091">
WRGS, with a threshold of 35% increases the pre-
cision score from 0.61 to 0.64 while maintaining the
recall rate. This is corresponding to the weighted
precision score for 35% data point in figure 2.
The last row in table 5 correspond to results ob-
tained with using the LBS proposed in this paper.
The scores at 35% resampling are same as the bold
faced observations in table 4 for 700 word sum-
maries. We observe that the LBS architecture alone
brings about an absolute improvement of 4.41% and
8.69% in weighted precision and f-score.
</bodyText>
<table confidence="0.9968288">
System wp recall f-score
baseline 0.61 0.13 0.20
+ gold standard 0.64 0.13 0.20
+ new features 0.68 0.15 0.23
+ resampling(LBS 35)% 0.71 0.16 0.25
</table>
<tableCaption confidence="0.999757">
Table 5: Results on the AMI corpus.
</tableCaption>
<sectionHeader confidence="0.995157" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.987136444444445">
Through our experimental results in this pa-
per, we firstly observed that training the classifier
on WRGS (weighted-resampled gold standard) in-
stances, rather than all the annotated DAs improved
the weighted precision scores of our summarizer.
We further addressed the problem of skewed class
distribution in our data set and introduced a learning-
based resampling approach where we resample from
the probabilistic outcomes of a first stage classifier.
We noted that resampling the data set increased per-
formance, peaking at around 35% sampling rate. We
compared the LBS model with the TFIDF resampler
obtaining better f-scores from our proposed machine
learning based architecture. We conclude in general
that resampling techniques for resolving data imbal-
ance problem in extractive meeting summarization
domain, results in enhanced system performance.
We are currently working on multiple extensions
of this work, including investigating how the results
can be applied to other corpora, adding additional
features, and finally methods for post-processing ex-
tractive summaries.
Acknowledgments This work is supported by the Eu-
ropean IST Programme Project AMIDA [FP6-0033812].
This paper only reflects the authors views and funding
agencies are not liable for any use that may be made of
the information contained herein.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999722166666667">
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement.
Peter Corbett and Ann Copestake. 2008. Cascaded
classifiers for confidence-based chemical named entity
recognition. In Current Trends in Biomedical Natural
Language Processing.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. In J. ACM, 16(2).
Erhard W. Hinrichs, Katja Filippova, and HolgerWunsch.
2005. A data-driven approach to pronominal anaphora
resolution for german. In In Proceedings of Recent
Advances in Natural Language Processing.
Gabriel Murray. 2008. Using Speech-Specific Charac-
teristics for Automatic Speech Summarization. Ph.D.
thesis, University of Edinburgh.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In IEEE Spoken Language
Technology Workshop (SLT), pages 157–160.
Xiaodan Zhu and Gerald Penn. 2006. Summarization
of spontaneous conversations. In Proceedings of the
2006 ACM Conference on Computer Supported Coop-
erative Work (CSCW 2006),.
</reference>
<page confidence="0.999531">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.186718">
<title confidence="0.924675666666667">A Learning-based Sampling Approach to Extractive Summarization Vishal Juneja and Sebastian Germesin and Thomas German Research Center for Artificial</title>
<address confidence="0.412514">Campus 66123 Saarb¨ucken,</address>
<abstract confidence="0.995095181818182">In this paper we present a novel resampling model for extractive meeting summarization. With resampling based on the output of a baseline classifier, our method outperforms previous research in the field. Further, we compare an existing resampling technique with our model. We report on an extensive series of experiments on a large meeting corpus which leads to classification improvement in weighted precision and f-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>In Educational and Psychological Measurement.</booktitle>
<contexts>
<context position="6432" citStr="Cohen, 1960" startWordPosition="1014" endWordPosition="1016">e meeting position in the speaker turn DA time duration speaker dominance in DA speaker dominance in time rate of speech SUIDF score TFIDF score Table 1: Features used in baseline classifier 4 Gold Standard In supervised frameworks, the creation of goldstandard annotations for training (and testing) is known to be a difficult task, since (a) what should go into a summary can be a matter of opinion and (b) multiple sentences from the original document may express similar content, making each of them equally good candidates for selection. The hypothesis is well supported by the low kappa value (Cohen, 1960) of 0.48 reported by (Murray, 2008) on the AMI corpus. We describe the procedure for creating the gold standard for our experimentation in this paper. Firstly we join all annotations and rank the DAs from most number of links to least number of links to create a sorted list of DAs. Depending on a predefined variable percentage as gold standard cut-off 35 or threshold we preserve the corresponding number of highest ranked DAs in the above list. For evaluation, (Murray, 2008) uses gold standard summaries obtained using similar procedure. For training, however, he uses all DA segments with at lea</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. In Educational and Psychological Measurement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Corbett</author>
<author>Ann Copestake</author>
</authors>
<title>Cascaded classifiers for confidence-based chemical named entity recognition.</title>
<date>2008</date>
<booktitle>In Current Trends in Biomedical Natural Language Processing.</booktitle>
<contexts>
<context position="1850" citStr="Corbett and Copestake, 2008" startWordPosition="275" endWordPosition="278">hich display a very skewed class distribution to learn for the binary classifier. This skewness is attributed to the less number of actual extract-worthy and important DAs (positive examples) compared to ordinary chit-chat, backchannel noises etc (negative examples) spoken during the course of the meeting. We tackle this data skewness with a novel resampling approach which reselects the data set to create a more comparable class distribution between these postive and negative instances. 34 Resampling methods have been found effective in catering to the data imbalance problem mentioned above. (Corbett and Copestake, 2008) used a resampling module for chemical named entity recognition. The pre-classifier, based on n-gram character features, assigned a probability of being a chemical word, to each token. Only tokens having probability greater than a predefined threshold were preserved and the output of the first stage classification along with word suffix were used as features in further classification steps. (Hinrichs et al., 2005) used a hybrid approach for Computational Anaphora Resolution (CAR) combining rule based filtering with Memory based learning to reduce the huge population of anaphora/candidate-antec</context>
</contexts>
<marker>Corbett, Copestake, 2008</marker>
<rawString>Peter Corbett and Ann Copestake. 2008. Cascaded classifiers for confidence-based chemical named entity recognition. In Current Trends in Biomedical Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>In J. ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="4534" citStr="Edmundson, 1969" startWordPosition="703" endWordPosition="705">pts rarely contains actual grammatical sentences. 3 Pre-processing and Feature Extraction To the feature set of (Murray, 2008) listed in table 1 we add some high level features. Since the main focus of this paper is to deal with the data imbalanace issue hence for the sake of completeness and reproducibility of our work we briefly mention the basic features used. In section 8.3 we explicitly report the performance rise over the baseline due to the added features. 3.1 Lexical and Structural features The list of added features include the number of content words (nouns and adjectives) in a DA. (Edmundson, 1969) looked at cue-phrases, keywords title and location of a sentence as features indicative of important sections in a document. We use a handpicked list of cue words like ”for example”, ”gonna have” etc as binary features. We also add several keywords like ”remote”,”plastic” etc based upon manual scrutiny, as binary features into the classifier. Further we use DA labels of current and four adjacent DAs as features. 3.2 Disfluency The role of disfluencies in summarization has been investigated by (Zhu and Penn, 2006) before. They found that disfluencies improve summarization performance when used</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. In J. ACM, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
<author>Katja Filippova</author>
<author>HolgerWunsch</author>
</authors>
<title>A data-driven approach to pronominal anaphora resolution for german. In</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="2267" citStr="Hinrichs et al., 2005" startWordPosition="339" endWordPosition="342">ble class distribution between these postive and negative instances. 34 Resampling methods have been found effective in catering to the data imbalance problem mentioned above. (Corbett and Copestake, 2008) used a resampling module for chemical named entity recognition. The pre-classifier, based on n-gram character features, assigned a probability of being a chemical word, to each token. Only tokens having probability greater than a predefined threshold were preserved and the output of the first stage classification along with word suffix were used as features in further classification steps. (Hinrichs et al., 2005) used a hybrid approach for Computational Anaphora Resolution (CAR) combining rule based filtering with Memory based learning to reduce the huge population of anaphora/candidate-antecedent pairs. (Xie et al., 2008), in their experimentation on the ICSI meeting corpus, employ the salience scores generated by a TFIDF classifier in the resampling task. We discuss the actual technique and our resampling module further in section 6. We compare its performance with the TFIDF model of (Xie et al., 2008) in section 8.2 and observe a general improvement in summary scores through resampling. 2 Data We u</context>
</contexts>
<marker>Hinrichs, Filippova, HolgerWunsch, 2005</marker>
<rawString>Erhard W. Hinrichs, Katja Filippova, and HolgerWunsch. 2005. A data-driven approach to pronominal anaphora resolution for german. In In Proceedings of Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
</authors>
<title>Using Speech-Specific Characteristics for Automatic Speech Summarization.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4044" citStr="Murray, 2008" startWordPosition="617" endWordPosition="618">Annotators wrote abstractive summaries for each meeting and then linked summary sentences to those DA segments from the meeting transcripts which best conveyed the information in the abstracts. There was no limit on the number of links an annotator could create and a many-to-many mapping exists between the meeting DA segments and human abstracts. Here, DA segments are used in analogy to sentences in document summarization because the spontaneously spoken material in meeting transcripts rarely contains actual grammatical sentences. 3 Pre-processing and Feature Extraction To the feature set of (Murray, 2008) listed in table 1 we add some high level features. Since the main focus of this paper is to deal with the data imbalanace issue hence for the sake of completeness and reproducibility of our work we briefly mention the basic features used. In section 8.3 we explicitly report the performance rise over the baseline due to the added features. 3.1 Lexical and Structural features The list of added features include the number of content words (nouns and adjectives) in a DA. (Edmundson, 1969) looked at cue-phrases, keywords title and location of a sentence as features indicative of important sections</context>
<context position="5388" citStr="Murray, 2008" startWordPosition="840" endWordPosition="841">keywords like ”remote”,”plastic” etc based upon manual scrutiny, as binary features into the classifier. Further we use DA labels of current and four adjacent DAs as features. 3.2 Disfluency The role of disfluencies in summarization has been investigated by (Zhu and Penn, 2006) before. They found that disfluencies improve summarization performance when used as an additional feature. We count the number of disfluent words in a DA using an automatic disfluency detector. 1Three of the meetings were missing some required features. 3.3 Prosodic We employ all the signal level features described by (Murray, 2008) which include mean, max and standard deviation of energy and pitch values normalized by both speaker and meeting. The duration of the DA in terms of time and number of words spoken. The subsequent, precedent pauses and rate of speech feature. DA Features mean energy mean pitch maximum energy value maximum pitch value standard deviation of pitch precedent pause subsequent pause uninterrupted length number of words position in the meeting position in the speaker turn DA time duration speaker dominance in DA speaker dominance in time rate of speech SUIDF score TFIDF score Table 1: Features used </context>
<context position="6910" citStr="Murray, 2008" startWordPosition="1098" endWordPosition="1099">ntent, making each of them equally good candidates for selection. The hypothesis is well supported by the low kappa value (Cohen, 1960) of 0.48 reported by (Murray, 2008) on the AMI corpus. We describe the procedure for creating the gold standard for our experimentation in this paper. Firstly we join all annotations and rank the DAs from most number of links to least number of links to create a sorted list of DAs. Depending on a predefined variable percentage as gold standard cut-off 35 or threshold we preserve the corresponding number of highest ranked DAs in the above list. For evaluation, (Murray, 2008) uses gold standard summaries obtained using similar procedure. For training, however, he uses all DA segments with at least one link as positive examples. As the term gold standard for the data set, created above, is misleading. We call the set of DAs so obtained by using this ranking and resampling procedure as Weighted-Resampled Gold Standard (WRGS). Henceforth in this paper, for a resampling rate of say 35% we will name the set of DAs so obtained as WRGS(35%) or simply WRGS for some undefined, arbitrary threshold. 5 Data Skewness In this section we focus on the skewed data set which arises</context>
<context position="8216" citStr="Murray, 2008" startWordPosition="1330" endWordPosition="1331"> one link to the abstractive or human summaries. Let us call it DAl&apos;—1. This set accounts for 20.9% of all DAs in the training set. set size% WRGS(25%) 5.22% DAl&apos;—1 20.9% Table 2: Set sizes in % of all training DAs Again consider set of DAs for WRGS(25%). This set, by definition, contains 25% of all DAs in the set DAl&apos;—1. Hence the set WRGS(25%) constitute 5.22% of all DAs in the training set. Note that this is a skewed class distribution as also visible in table 2. Our system employs resampling architecture shown in figure 1. The first classifier is similar in spirit to the one developed in (Murray, 2008) with the additional features listed in section 3. The output we use is not the discrete classification result but rather the probability for each DA segment to be extracted. These probabilities are used in two ways for training the second classifier: firstly, to create the resampled training set and secondly, as an additional feature for the second classifier. The procedure for resampling is explained in the section 6. Figure 1: A two-step classification architecture for extractive meeting summarization. 6 Resampling As explained in previous section our model obtains resampled data for second</context>
<context position="10674" citStr="Murray, 2008" startWordPosition="1731" endWordPosition="1732">F sampling approach adopted by (Xie et al., 2008) and present the results of resampling on both models in section 8.2. For comparison, we use Murray’s (2008) state of art extractive summarization model. Resampled Training Set Training Set First Classifier / Resampler Second Classifier probabilties 36 7 Evaluation Metric tained by any summary corresponding to this comThe main metric we use for evaluating the sum- pression rate. Weighted Recall on the other hand maries is the extension of the weighted precision signifies total information content of the meeting. evaluation scheme introduced by (Murray, 2008). For intelligent systems in general the recall rate inThe measure relies on having multiple annotations creases with increasing summary compression rates for a meeting and a many-to-many mapping dis- while weighted precision decreases2. cussed in section 2. To calculate weighted precision, Since we experiment with short summaries that the number of times that each extractive summary have at most 700 words, we do most of the comDA was linked by each annotator is counted and av- parisons in terms of weighted precision values. In eraged to get a single DA score. The DA scores are the final syste</context>
<context position="12395" citStr="Murray, 2008" startWordPosition="2022" endWordPosition="2023">nition, weighted recall can have a maximum score dard thresholds. For example, at a threshold of 60%, of 1 since it is a fraction of the total links for the en- the top 60% of the linked DA segments are defined tire summary. Also, there is no theoretical maximum as the gold standard positive examples, all other DA for weighted precision as annotators were allowed to segments of the meeting are defined as negative, create any number of links for a single DA. non-extraction worthy. The tests are performed on Both weighted precision and recall share the same a single stage classifier similar to (Murray, 2008). numerator: num = Ed Ld/N where Ld is the num- In addition, the curves show the behavior of the ber of links for a DA d in the extractive summary, system at three different summary compression rates and N is the number of annotators. Weighted pre- (i.e., number of words in the summary). A gencision is equal to wp = num/D3 where D3 is the eral tendency that can be observed is the increase number of DAs in the extractive summary. Weighted in summary scores with decreasing threshold. For recall is given by recall = num/(Lt/N) where Lt 700 word summaries the peak weighted precision is the total n</context>
</contexts>
<marker>Murray, 2008</marker>
<rawString>Gabriel Murray. 2008. Using Speech-Specific Characteristics for Automatic Speech Summarization. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Yang Liu</author>
<author>Hui Lin</author>
</authors>
<title>Evaluating the effectiveness of features and sampling in extractive meeting summarization.</title>
<date>2008</date>
<booktitle>In IEEE Spoken Language Technology Workshop (SLT),</booktitle>
<pages>157--160</pages>
<contexts>
<context position="2481" citStr="Xie et al., 2008" startWordPosition="370" endWordPosition="373">ling module for chemical named entity recognition. The pre-classifier, based on n-gram character features, assigned a probability of being a chemical word, to each token. Only tokens having probability greater than a predefined threshold were preserved and the output of the first stage classification along with word suffix were used as features in further classification steps. (Hinrichs et al., 2005) used a hybrid approach for Computational Anaphora Resolution (CAR) combining rule based filtering with Memory based learning to reduce the huge population of anaphora/candidate-antecedent pairs. (Xie et al., 2008), in their experimentation on the ICSI meeting corpus, employ the salience scores generated by a TFIDF classifier in the resampling task. We discuss the actual technique and our resampling module further in section 6. We compare its performance with the TFIDF model of (Xie et al., 2008) in section 8.2 and observe a general improvement in summary scores through resampling. 2 Data We use the scenario meetings of the AMI corpus for our experiments in this paper which comprise about two thirds of around 100 hours of recorded and annotated meetings. The scenario meetings each have four participants</context>
<context position="8952" citStr="Xie et al., 2008" startWordPosition="1448" endWordPosition="1451">er the probability for each DA segment to be extracted. These probabilities are used in two ways for training the second classifier: firstly, to create the resampled training set and secondly, as an additional feature for the second classifier. The procedure for resampling is explained in the section 6. Figure 1: A two-step classification architecture for extractive meeting summarization. 6 Resampling As explained in previous section our model obtains resampled data for second stage classification using the probabilistic outcomes of a first stage classifier. The resampling is done similar to (Xie et al., 2008) to cater to the data skewness problem. To do the resampling, firstly, the DAs are ranked on decreasing probabilities. In the next step, depending on some resampling rate, a percentage of highest ranked DAs is used in further classification steps, while rest of DA segments are neglected. (Xie et al., 2008) obtained the resampled set by ranking the DAs on TFIDF weights. Data resampling benefits the model in two ways a) by improving the positive/negative example ratio during the training phase b) by discarding noisy utterances in the test phase as they usually attain low scores from the first cl</context>
</contexts>
<marker>Xie, Liu, Lin, 2008</marker>
<rawString>Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating the effectiveness of features and sampling in extractive meeting summarization. In IEEE Spoken Language Technology Workshop (SLT), pages 157–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Gerald Penn</author>
</authors>
<title>Summarization of spontaneous conversations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 ACM Conference on Computer Supported Cooperative Work (CSCW</booktitle>
<contexts>
<context position="5053" citStr="Zhu and Penn, 2006" startWordPosition="786" endWordPosition="789">added features include the number of content words (nouns and adjectives) in a DA. (Edmundson, 1969) looked at cue-phrases, keywords title and location of a sentence as features indicative of important sections in a document. We use a handpicked list of cue words like ”for example”, ”gonna have” etc as binary features. We also add several keywords like ”remote”,”plastic” etc based upon manual scrutiny, as binary features into the classifier. Further we use DA labels of current and four adjacent DAs as features. 3.2 Disfluency The role of disfluencies in summarization has been investigated by (Zhu and Penn, 2006) before. They found that disfluencies improve summarization performance when used as an additional feature. We count the number of disfluent words in a DA using an automatic disfluency detector. 1Three of the meetings were missing some required features. 3.3 Prosodic We employ all the signal level features described by (Murray, 2008) which include mean, max and standard deviation of energy and pitch values normalized by both speaker and meeting. The duration of the DA in terms of time and number of words spoken. The subsequent, precedent pauses and rate of speech feature. DA Features mean ener</context>
</contexts>
<marker>Zhu, Penn, 2006</marker>
<rawString>Xiaodan Zhu and Gerald Penn. 2006. Summarization of spontaneous conversations. In Proceedings of the 2006 ACM Conference on Computer Supported Cooperative Work (CSCW 2006),.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>