<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.819886">
Sparse Overcomplete Word Vector Representations
</title>
<author confidence="0.90054">
Manaal Faruqui Yulia Tsvetkov Dani Yogatama Chris Dyer Noah A. Smith
</author>
<affiliation confidence="0.9600025">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.530656">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.999276">
{mfaruqui,ytsvetko,dyogatama,cdyer,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829277777778">
Current distributed representations of
words show little resemblance to theo-
ries of lexical semantics. The former
are dense and uninterpretable, the lat-
ter largely based on familiar, discrete
classes (e.g., supersenses) and relations
(e.g., synonymy and hypernymy). We pro-
pose methods that transform word vec-
tors into sparse (and optionally binary)
vectors. The resulting representations are
more similar to the interpretable features
typically used in NLP, though they are dis-
covered automatically from raw corpora.
Because the vectors are highly sparse, they
are computationally easy to work with.
Most importantly, we find that they out-
perform the original vectors on benchmark
tasks.
</bodyText>
<sectionHeader confidence="0.998759" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999827083333333">
Distributed representations of words have been
shown to benefit NLP tasks like parsing (Lazari-
dou et al., 2013; Bansal et al., 2014), named en-
tity recognition (Guo et al., 2014), and sentiment
analysis (Socher et al., 2013). The attraction of
word vectors is that they can be derived directly
from raw, unannotated corpora. Intrinsic evalua-
tions on various tasks are guiding methods toward
discovery of a representation that captures many
facts about lexical semantics (Turney, 2001; Tur-
ney and Pantel, 2010).
Yet word vectors do not look anything like the
representations described in most lexical seman-
tic theories, which focus on identifying classes of
words (Levin, 1993; Baker et al., 1998; Schuler,
2005) and relationships among word meanings
(Miller, 1995). Though expensive to construct,
conceptualizing word meanings symbolically is
important for theoretical understanding and also
when we incorporate lexical semantics into com-
putational models where interpretability is de-
sired. On the surface, discrete theories seem in-
commensurate with the distributed approach, a
problem now receiving much attention in compu-
tational linguistics (Lewis and Steedman, 2013;
Kiela and Clark, 2013; Vecchi et al., 2013; Grefen-
stette, 2013; Lewis and Steedman, 2014; Paperno
et al., 2014).
Our contribution to this discussion is a new,
principled sparse coding method that transforms
any distributed representation of words into sparse
vectors, which can then be transformed into binary
vectors (§2). Unlike recent approaches of incorpo-
rating semantics in distributional word vectors (Yu
and Dredze, 2014; Xu et al., 2014; Faruqui et al.,
2015), the method does not rely on any external
information source. The transformation results in
longer, sparser vectors, sometimes called an “over-
complete” representation (Olshausen and Field,
1997). Sparse, overcomplete representations have
been motivated in other domains as a way to in-
crease separability and interpretability, with each
instance (here, a word) having a small number
of active dimensions (Olshausen and Field, 1997;
Lewicki and Sejnowski, 2000), and to increase
stability in the presence of noise (Donoho et al.,
2006).
Our work builds on recent explorations of spar-
sity as a useful form of inductive bias in NLP and
machine learning more broadly (Kazama and Tsu-
jii, 2003; Goodman, 2004; Friedman et al., 2008;
Glorot et al., 2011; Yogatama and Smith, 2014,
inter alia). Introducing sparsity in word vector di-
mensions has been shown to improve dimension
interpretability (Murphy et al., 2012; Fyshe et al.,
2014) and usability of word vectors as features in
downstream tasks (Guo et al., 2014). The word
vectors we produce are more than 90% sparse; we
also consider binarizing transformations that bring
them closer to the categories and relations of lex-
</bodyText>
<page confidence="0.93131">
1491
</page>
<note confidence="0.976055333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999768266666667">
ical semantic theories. Using a number of state-
of-the-art word vectors as input, we find consis-
tent benefits of our method on a suite of standard
benchmark evaluation tasks (§3). We also evalu-
ate our word vectors in a word intrusion experi-
ment with humans (Chang et al., 2009) and find
that our sparse vectors are more interpretable than
the original vectors (§4).
We anticipate that sparse, binary vectors can
play an important role as features in statistical
NLP models, which still rely predominantly on
discrete, sparse features whose interpretability en-
ables error analysis and continued development.
We have made an implementation of our method
publicly available.1
</bodyText>
<sectionHeader confidence="0.983016" genericHeader="method">
2 Sparse Overcomplete Word Vectors
</sectionHeader>
<bodyText confidence="0.999859533333333">
We consider methods for transforming dense word
vectors to sparse, binary overcomplete word vec-
tors. Fig. 1 shows two approaches. The one on the
top, method A, converts dense vectors to sparse
overcomplete vectors (§2.1). The one beneath,
method B, converts dense vectors to sparse and bi-
nary overcomplete vectors (§2.2 and §2.4).
Let V be the vocabulary size. In the following,
X E RLxV is the matrix constructed by stack-
ing V non-sparse “input” word vectors of length
L (produced by an arbitrary word vector estima-
tor). We will refer to these as initializing vectors.
A E RKxV contains V sparse overcomplete word
vectors of length K. “Overcomplete” representa-
tion learning implies that K &gt; L.
</bodyText>
<subsectionHeader confidence="0.994746">
2.1 Sparse Coding
</subsectionHeader>
<bodyText confidence="0.998954666666667">
In sparse coding (Lee et al., 2006), the goal is to
represent each input vector xi as a sparse linear
combination of basis vectors, ai. Our experiments
consider four initializing methods for these vec-
tors, discussed in Appendix A. Given X, we seek
to solve
</bodyText>
<equation confidence="0.6151255">
arg min 11X − DA112 2 + λQ(A) + τ11D1122, (1)
D,A
</equation>
<bodyText confidence="0.997509833333333">
where D E RLxK is the dictionary of basis vec-
tors. λ is a regularization hyperparameter, and Q is
the regularizer. Here, we use the squared loss for
the reconstruction error, but other loss functions
could also be used (Lee et al., 2009). To obtain
sparse word representations we will impose an `1
</bodyText>
<footnote confidence="0.909756">
1https://github.com/mfaruqui/
sparse-coding
</footnote>
<bodyText confidence="0.996757">
penalty on A. Eq. 1 can be broken down into loss
for each word vector which can be optimized sep-
arately in parallel (§2.3):
</bodyText>
<equation confidence="0.995558">
V
arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2)
D,A i=1
</equation>
<bodyText confidence="0.999852333333333">
where mi denotes the ith column vector of matrix
M. Note that this problem is not convex. We refer
to this approach as method A.
</bodyText>
<subsectionHeader confidence="0.999245">
2.2 Sparse Nonnegative Vectors
</subsectionHeader>
<bodyText confidence="0.999192666666667">
Nonnegativity in the feature space has often been
shown to correspond to interpretability (Lee and
Seung, 1999; Cichocki et al., 2009; Murphy et al.,
2012; Fyshe et al., 2014; Fyshe et al., 2015). To
obtain nonnegative sparse word vectors, we use a
variation of the nonnegative sparse coding method
(Hoyer, 2002). Nonnegative sparse coding further
constrains the problem in Eq. 2 so that D and ai
are nonnegative. Here, we apply this constraint
only to the representation vectors {ail. Thus, the
new objective for nonnegative sparse vectors be-
comes:
</bodyText>
<equation confidence="0.91310625">
V
arg min 11xi−Dai1122+λ11ai111+τ11D1122
D∈R&gt;0 K,A∈R&gt;O V i=1
(3)
</equation>
<bodyText confidence="0.999733">
This problem will play a role in our second ap-
proach, method B, to which we will return shortly.
This nonnegativity constraint can be easily incor-
porated during optimization, as explained next.
</bodyText>
<subsectionHeader confidence="0.995576">
2.3 Optimization
</subsectionHeader>
<bodyText confidence="0.999964066666667">
We use online adaptive gradient descent (Ada-
Grad; Duchi et al., 2010) for solving the optimiza-
tion problems in Eqs. 2–3 by updating A and D.
In order to speed up training we use asynchronous
updates to the parameters of the model in parallel
for every word vector (Duchi et al., 2012; Heigold
et al., 2014).
However, directly applying stochastic subgradi-
ent descent to an `1-regularized objective fails to
produce sparse solutions in bounded time, which
has motivated several specialized algorithms that
target such objectives. We use the AdaGrad vari-
ant of one such learning algorithm, the regular-
ized dual averaging algorithm (Xiao, 2009), which
keeps track of the online average gradient at time
</bodyText>
<equation confidence="0.7200245">
t: ¯gt = 1 �t t,=1 gt, Here, the subgradients do not
t
</equation>
<bodyText confidence="0.9777995">
include terms for the regularizer; they are deriva-
tives of the unregularized objective (λ = 0, τ = 0)
</bodyText>
<page confidence="0.997238">
1492
</page>
<figureCaption confidence="0.987239">
Figure 1: Methods for obtaining sparse overcomplete vectors (top, method A, §2.1) and sparse, binary
</figureCaption>
<bodyText confidence="0.95222">
overcomplete word vectors (bottom, method B, §2.2 and §2.4). Observed dense vectors of length L (left)
are converted to sparse non-negative vectors (center) of length K which are then projected into the binary
vector space (right), where L « K. X is dense, A is sparse, and B is the binary word vector matrix.
Strength of colors signify the magnitude of values; negative is red, positive is blue, and zero is white.
</bodyText>
<figure confidence="0.921202833333334">
K
V
Sparse overcomplete vectors
Sparse, binary overcomplete
vectors
D
x
K
A
Sparse coding
V
X
L
x
Initial dense vectors
D
K
K
Projection
B
V
Non-negative sparse coding
V
&apos;y = −sign(¯gt,i,j) /Gt,i,j (|¯gt,i,j |− A),
</figure>
<bodyText confidence="0.999454">
where Gt,i,j = Ett,=1 g2t,,i,j. Now, using the av-
erage gradient, the `1-regularized objective is op-
timized as follows:
</bodyText>
<equation confidence="0.61952">
�
0, if |¯gt,i,j |≤ A at+1,i,j =(4)
&apos;y, otherwise
</equation>
<bodyText confidence="0.999860857142857">
where, at+1,i,j is the jth element of sparse vector
ai at the tth update and ¯gt,i,j is the correspond-
ing average gradient. For obtaining nonnegative
sparse vectors we take projection of the updated ai
onto RK ≥0 by choosing the closest point in RK≥0 ac-
cording to Euclidean distance (which corresponds
to zeroing out the negative elements):
</bodyText>
<table confidence="0.995826">
X L A Ir K % Sparse
Glove 300 1.0 10−5 3000 91
SG 300 0.5 10−5 3000 92
GC 50 1.0 10−5 500 98
Multi 48 0.1 10−5 960 93
</table>
<tableCaption confidence="0.99108275">
Table 1: Hyperparameters for learning sparse
overcomplete vectors tuned on the WS-353 task.
Tasks are explained in §B. The four initial vector
representations X are explained in §A.
</tableCaption>
<bodyText confidence="0.935311">
hot, fresh, fish, 1/2, wine, salt
series, tv, appearances, episodes
1975, 1976, 1968, 1970, 1977, 1969
dress, shirt, ivory, shirts, pants
upscale, affluent, catering, clientele
with respect to ai. We define
ηt
at+1,i,j = { 0, if |¯gt,i,j |≤ A (5) Table 2: Highest frequency words in randomly
0, if &apos;y &lt; 0 picked word clusters of binary sparse overcom-
&apos;y, otherwise plete Glove vectors.
</bodyText>
<subsectionHeader confidence="0.998712">
2.4 Binarizing Transformation
</subsectionHeader>
<bodyText confidence="0.972274333333333">
Our aim with method B is to obtain word rep-
resentations that can emulate the binary-feature
space designed for various NLP tasks. We could
</bodyText>
<page confidence="0.929076">
1493
</page>
<bodyText confidence="0.991583">
state this as an optimization problem:
</bodyText>
<equation confidence="0.616409">
kxi − Dbik22 + Akbik1 1 + TkDk22
(6)
</equation>
<bodyText confidence="0.999986">
where B denotes the binary (and also sparse) rep-
resentation. This is an mixed integer bilinear pro-
gram, which is NP-hard (Al-Khayyal and Falk,
1983). Unfortunately, the number of variables in
the problem is ≈ KV which reaches 100 million
when V = 100, 000 and K = 1, 000, which is
intractable to solve using standard techniques.
A more tractable relaxation to this hard prob-
lem is to first constrain the continuous represen-
tation A to be nonnegative (i.e, ai ∈ RK ≥0; §2.2).
Then, in order to avoid an expensive computation,
we take the nonnegative word vectors obtained us-
ing Eq. 3 and project nonzero values to 1, preserv-
ing the 0 values. Table 2 shows a random set of
word clusters obtained by (i) applying our method
to Glove initial vectors and (ii) applying k-means
clustering (k = 100). In §3 we will find that these
vectors perform well quantitatively.
</bodyText>
<subsectionHeader confidence="0.992508">
2.5 Hyperparameter Tuning
</subsectionHeader>
<bodyText confidence="0.99996075">
Methods A and B have three hyperparameters: the
E1-regularization penalty A, the E2-regularization
penalty T, and the length of the overcomplete word
vector representation K. We perform a grid search
on A ∈ {0.1, 0.5,1.0} and K ∈ {10L, 20L}, se-
lecting values that maximizes performance on one
“development” word similarity task (WS-353, dis-
cussed in §B) while achieving at least 90% sparsity
in overcomplete vectors. T was tuned on one col-
lection of initializing vectors (Glove, discussed in
§A) so that the vectors in D are near unit norm.
The four vector representations and their corre-
sponding hyperparameters selected by this proce-
dure are summarized in Table 1. There hyperpa-
rameters were chosen for method A and retained
for method B.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998554785714286">
Using methods A and B, we constructed sparse
overcomplete vector representations A, starting
from four initial vector representations X; these
are explained in Appendix A. We used one bench-
mark evaluation (WS-353) to tune hyperparame-
ters, resulting in the settings shown in Table 1;
seven other tasks were used to evaluate the quality
of the sparse overcomplete representations. The
first of these is a word similarity task, where the
score is correlation with human judgments, and
the others are classification accuracies of an E2-
regularized logistic regression model trained using
the word vectors. These tasks are described in de-
tail in Appendix B.
</bodyText>
<subsectionHeader confidence="0.999767">
3.1 Effects of Transforming Vectors
</subsectionHeader>
<bodyText confidence="0.999765166666666">
First, we quantify the effects of our transforma-
tions by comparing their output to the initial (X)
vectors. Table 3 shows consistent improvements
of sparsifying vectors (method A). The exceptions
are on the SimLex task, where our sparse vectors
are worse than the skip-gram initializer and on par
with the multilingual initializer. Sparsification is
beneficial across all of the text classification tasks,
for all initial vector representations. On average
across all vector types and all tasks, sparse over-
complete vectors outperform their corresponding
initializers by 4.2 points.2
Binarized vectors (from method B) are also usu-
ally better than the initial vectors (also shown in
Table 3), and tend to outperform the sparsified
variants, except when initializing with Glove. On
average across all vector types and all tasks, bina-
rized overcomplete vectors outperform their cor-
responding initializers by 4.8 points and the con-
tinuous, sparse intermediate vectors by 0.6 points.
From here on, we explore more deeply the
sparse overcomplete vectors from method A (de-
noted by A), leaving binarization and method B
aside.
</bodyText>
<subsectionHeader confidence="0.999787">
3.2 Effect of Vector Length
</subsectionHeader>
<bodyText confidence="0.999392888888889">
How does the length of the overcomplete vector
(K) affect performance? We focus here on the
Glove vectors, where L = 300, and report av-
erage performance across all tasks. We consider
K = αL where α ∈ {2, 3, 5,10,15, 20}. Figure 2
plots the average performance across tasks against
α. The earlier selection of K = 3, 000 (α = 10)
gives the best result; gains are monotonic in α to
that point and then begin to diminish.
</bodyText>
<subsectionHeader confidence="0.99629">
3.3 Alternative Transformations
</subsectionHeader>
<bodyText confidence="0.999947">
We consider two alternative transformations. The
first preserves the original vector length but
</bodyText>
<footnote confidence="0.756197333333333">
2We report correlation on a 100 point scale, so that the
average which includes accuracuies and correlation is equally
representatitve of both.
</footnote>
<figure confidence="0.5618334">
arg min
D∈RL×K
BE{0,1}K×V
V
i=1
</figure>
<page confidence="0.864223">
1494
</page>
<table confidence="0.999482428571429">
Vectors SimLex Senti. TREC Sports Comp. Relig. NP Average
Corr. Acc. Acc. Acc. Acc. Acc. Acc.
X 36.9 77.7 76.2 95.9 79.7 86.7 77.9 76.2
Glove A 38.9 81.4 81.5 96.3 87.0 88.8 82.3 79.4
B 39.7 81.0 81.2 95.7 84.6 87.4 81.6 78.7
X 43.6 81.5 77.8 97.1 80.2 85.9 80.1 78.0
SG A 41.7 82.7 81.2 98.2 84.5 86.5 81.6 79.4
B 42.8 81.6 81.6 95.2 86.5 88.0 82.9 79.8
X 9.7 68.3 64.6 75.1 60.5 76.0 79.4 61.9
GC A 12.0 73.3 77.6 77.0 68.3 81.0 81.2 67.2
B 18.7 73.6 79.2 79.7 70.5 79.6 79.4 68.6
X 28.7 75.5 63.8 83.6 64.3 81.8 79.2 68.1
Multi A 28.1 78.6 79.2 93.9 78.2 84.5 81.1 74.8
B 28.7 77.6 82.0 94.7 81.4 85.6 81.9 75.9
</table>
<tableCaption confidence="0.789923333333333">
Table 3: Performance comparison of transformed vectors to initial vectors X. We show sparse over-
complete representations A and also binarized representations B. Initial vectors are discussed in §A and
tasks in §B.
</tableCaption>
<figureCaption confidence="0.87864775">
Figure 2: Average performace across all tasks
for sparse overcomplete vectors (A) produced by
Glove initial vectors, as a function of the ratio of
K to L.
</figureCaption>
<bodyText confidence="0.93119175">
achieves a binary, sparse vector (B) by applying:
The second transformation was proposed by
Guo et al. (2014). Here, the original vector length
is also preserved, but sparsity is achieved through:
</bodyText>
<equation confidence="0.979600666666667">
1 if xi,j ≥ M+
−1 if xi,j ≤ M− (8)
0 otherwise
</equation>
<bodyText confidence="0.999830714285714">
where M+ (M−) is the mean of positive-valued
(negative-valued) elements of X. These vectors
are, obviously, not binary.
We find that on average, across initializing vec-
tors and across all tasks that our sparse overcom-
plete (A) vectors lead to better performance than
either of the alternative transformations.
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="method">
4 Interpretability
</sectionHeader>
<bodyText confidence="0.999961428571429">
Our hypothesis is that the dimensions of sparse
overcomplete vectors are more interpretable than
those of dense word vectors. Following Murphy
et al. (2012), we use a word intrusion experiment
(Chang et al., 2009) to corroborate this hypothesis.
In addition, we conduct qualitative analysis of in-
terpretability, focusing on individual dimensions.
</bodyText>
<subsectionHeader confidence="0.985923">
4.1 Word Intrusion
</subsectionHeader>
<bodyText confidence="0.992535117647059">
Word intrusion experiments seek to quantify the
extent to which dimensions of a learned word rep-
resentation are coherent to humans. In one in-
stance of the experiment, a human judge is pre-
sented with five words in random order and asked
to select the “intruder.” The words are selected by
the experimenter by choosing one dimension j of
the learned representation, then ranking the words
on that dimension alone. The dimensions are cho-
sen in decreasing order of the variance of their
values across the vocabulary. Four of the words
are the top-ranked words according to j, and the
“true” intruder is a word from the bottom half of
the list, chosen to be a word that appears in the top
10% of some other dimension. An example of an
instance is:
naval, industrial, technological, marine, identity
</bodyText>
<equation confidence="0.694894714285714">
�1 if xi,j &gt; 0
0 otherwise
bi,j = (7)
⎧
⎨
⎩
ai,j =
</equation>
<page confidence="0.904988">
1495
</page>
<table confidence="0.9997254">
X: Glove SG GC Multi Average
X 76.2 78.0 61.9 68.1 71.0
Eq. 7 75.7 75.8 60.5 64.1 69.0
Eq. 8 (Guo et al., 2014) 75.8 76.9 60.5 66.2 69.8
A 79.4 79.4 67.2 74.8 75.2
</table>
<tableCaption confidence="0.992362">
Table 4: Average performance across all tasks and vector models using different transformations.
</tableCaption>
<table confidence="0.999868666666667">
Vectors A1 A2 A3 Avg. IAA K
X 61 53 56 57 70 0.40
A 71 70 72 71 77 0.45
</table>
<tableCaption confidence="0.997939">
Table 5: Accuracy of three human annotators on
</tableCaption>
<bodyText confidence="0.930142055555555">
the word intrusion task, along with the average
inter-annotator agreement (Artstein and Poesio,
2008) and Fleiss’ K (Davies and Fleiss, 1982).
(The last word is the intruder.)
We formed instances from initializing vectors
and from our sparse overcomplete vectors (A).
Each of these two combines the four different ini-
tializers X. We selected the 25 dimensions d in
each case. Each of the 100 instances per condition
(initial vs. sparse overcomplete) was given to three
judges.
Results in Table 5 confirm that the sparse over-
complete vectors are more interpretable than the
dense vectors. The inter-annotator agreement on
the sparse vectors increases substantially, from
57% to 71%, and the Fleiss’ K increases from
“fair” to “moderate” agreement (Landis and Koch,
1977).
</bodyText>
<subsectionHeader confidence="0.992576">
4.2 Qualitative Evaluation of Interpretability
</subsectionHeader>
<bodyText confidence="0.934000064516129">
If a vector dimension is interpretable, the top-
ranking words for that dimension should display
semantic or syntactic groupings. To verify this
qualitatively, we select five dimensions with the
highest variance of values in initial and sparsi-
fied GC vectors. We compare top-ranked words in
the dimensions extracted from the two representa-
tions. The words are listed in Table 6, a dimension
per row. Subjectively, we find the semantic group-
ings better in the sparse vectors than in the initial
vectors.
Figure 3 visualizes the sparsified GC vectors for
six words. The dimensions are sorted by the aver-
age value across the three “animal” vectors. The
animal-related words use many of the same di-
mensions (102 common active dimensions out of
500 total); in constrast, the three city names use
combat, guard, honor, bow, trim, naval
’ll, could, faced, lacking, seriously, scored
X see, n’t, recommended, depending, part
due, positive, equal, focus, respect, better
sergeant, comments, critics, she, videos
fracture, breathing, wound, tissue, relief
relationships, connections, identity, relations
A files, bills, titles, collections, poems, songs
naval, industrial, technological, marine
stadium, belt, championship, toll, ride, coach
Table 6: Top-ranked words per dimension for ini-
tial and sparsified GC representations. Each line
shows words from a different dimension.
mostly distinct vectors.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999883043478261">
To the best of our knowledge, there has been no
prior work on obtaining overcomplete word vec-
tor representations that are sparse and categorical.
However, overcomplete features have been widely
used in image processing, computer vision (Ol-
shausen and Field, 1997; Lewicki and Sejnowski,
2000) and signal processing (Donoho et al., 2006).
Nonnegative matrix factorization is often used for
interpretable coding of information (Lee and Se-
ung, 1999; Liu et al., 2003; Cichocki et al., 2009).
Sparsity constraints are in general useful in NLP
problems (Kazama and Tsujii, 2003; Friedman
et al., 2008; Goodman, 2004), like POS tagging
(Ganchev et al., 2009), dependency parsing (Mar-
tins et al., 2011), text classification (Yogatama and
Smith, 2014), and representation learning (Ben-
gio et al., 2013). Including sparsity constraints
in Bayesian models of lexical semantics like LDA
in the form of sparse Dirichlet priors has been
shown to be useful for downstream tasks like POS-
tagging (Toutanova and Johnson, 2007), and im-
proving interpretation (Paul and Dredze, 2012;
Zhu and Xing, 2012).
</bodyText>
<page confidence="0.978078">
1496
</page>
<figure confidence="0.977525826086957">
fish
horse
dog
chicago
seattle
boston
VV
i
I
i
Y
A
I
I
r
I
I
V
I
I
V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes
are white.
h
</figure>
<sectionHeader confidence="0.856718" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977727272727">
We have presented a method that converts word
vectors obtained using any state-of-the-art word
vector model into sparse and optionally binary
word vectors. These transformed vectors appear to
come closer to features used in NLP tasks and out-
perform the original vectors from which they are
derived on a suite of semantics and syntactic eval-
uation benchmarks. We also find that the sparse
vectors are more interpretable than the dense vec-
tors by humans according to a word intrusion de-
tection test.
</bodyText>
<sectionHeader confidence="0.994273" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998547">
We thank Alona Fyshe for discussions on vec-
tor interpretability and three anonymous review-
ers for their feedback. This research was sup-
ported in part by the National Science Foundation
through grant IIS-1251131 and the Defense Ad-
vanced Research Projects Agency through grant
FA87501420244. This work was supported in part
by the U.S. Army Research Laboratory and the
U.S. Army Research Office under contract/grant
number W911NF-10-1-0533.
</bodyText>
<subsectionHeader confidence="0.437662">
A Initial Vector Representations (X)
</subsectionHeader>
<bodyText confidence="0.9999025">
Our experiments consider four publicly available
collections of pre-trained word vectors. They vary
in the amount of data used and the estimation
method.
Glove. Global vectors for word representations
(Pennington et al., 2014) are trained on aggregated
global word-word co-occurrence statistics from a
corpus. These vectors were trained on 6 billion
words from Wikipedia and English Gigaword and
are of length 300.3
</bodyText>
<footnote confidence="0.9774885">
3http://www-nlp.stanford.edu/projects/
glove/
</footnote>
<bodyText confidence="0.999306714285714">
Skip-Gram (SG). The word2vec tool (Mikolov
et al., 2013) is fast and widely-used. In this model,
each word’s Huffman code is used as an input to
a log-linear classifier with a continuous projection
layer and words within a given context window are
predicted. These vectors were trained on 100 bil-
lion words of Google news data and are of length
300.4
Global Context (GC). These vectors are
learned using a recursive neural network that
incorporates both local and global (document-
level) context features (Huang et al., 2012). These
vectors were trained on the first 1 billion words of
English Wikipedia and are of length 50.5
Multilingual (Multi). Faruqui and Dyer (2014)
learned vectors by first performing SVD on text
in different languages, then applying canonical
correlation analysis on pairs of vectors for words
that align in parallel corpora. These vectors were
trained on WMT-2011 news corpus containing
360 million words and are of length 48.6
</bodyText>
<subsectionHeader confidence="0.922473">
B Evaluation Benchmarks
</subsectionHeader>
<bodyText confidence="0.999639666666667">
Our comparisons of word vector quality consider
five benchmark tasks. We now describe the differ-
ent evaluation benchmarks for word vectors.
Word Similarity. We evaluate our word repre-
sentations on two word similarity tasks. The first
is the WS-353 dataset (Finkelstein et al., 2001),
which contains 353 pairs of English words that
have been assigned similarity ratings by humans.
This dataset is used to tune sparse vector learning
hyperparameters (§2.5), while the remaining of the
tasks discussed in this section are completely held
out.
</bodyText>
<footnote confidence="0.999982">
4https://code.google.com/p/word2vec
5http://nlp.stanford.edu/˜socherr/
ACL2012_wordVectorsTextFile.zip
6http://cs.cmu.edu/˜mfaruqui/soft.html
</footnote>
<page confidence="0.994557">
1497
</page>
<bodyText confidence="0.997944382978723">
A more recent dataset, SimLex-999 (Hill et al.,
2014), has been constructed to specifically focus
on similarity (rather than relatedness). It con-
tains a balanced set of noun, verb, and adjective
pairs. We calculate cosine similarity between the
vectors of two words forming a test item and re-
port Spearman’s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
Sentiment Analysis (Senti). Socher et al.
(2013) created a treebank of sentences anno-
tated with fine-grained sentiment labels on phrases
and sentences from movie review excerpts. The
coarse-grained treebank of positive and negative
classes has been split into training, development,
and test datasets containing 6,920, 872, and 1,821
sentences, respectively. We use average of the
word vectors of a given sentence as feature for
classification. The classifier is tuned on the
dev. set and accuracy is reported on the test set.
Question Classification (TREC). As an aid to
question answering, a question may be classi-
fied as belonging to one of many question types.
The TREC questions dataset involves six differ-
ent question types, e.g., whether the question is
about a location, about a person, or about some nu-
meric information (Li and Roth, 2002). The train-
ing dataset consists of 5,452 labeled questions, and
the test dataset consists of 500 questions. An av-
erage of the word vectors of the input question is
used as features and accuracy is reported on the
test set.
20 Newsgroup Dataset. We consider three bi-
nary categorization tasks from the 20 News-
groups dataset.7 Each task involves categoriz-
ing a document according to two related cate-
gories with training/dev./test split in accordance
with Yogatama and Smith (2014): (1) Sports:
baseball vs. hockey (958/239/796) (2) Comp.:
IBM vs. Mac (929/239/777) (3) Religion: atheism
vs. christian (870/209/717). We use average of the
word vectors of a given sentence as features. The
classifier is tuned on the dev. set and accuracy is
reported on the test set.
NP bracketing (NP). Lazaridou et al. (2013)
constructed a dataset from the Penn Treebank
(Marcus et al., 1993) of noun phrases (NP) of
</bodyText>
<footnote confidence="0.650014">
7http://qwone.com/˜jason/20Newsgroups
</footnote>
<bodyText confidence="0.999881666666667">
length three words, where the first can be an ad-
jective or a noun and the other two are nouns. The
task is to predict the correct bracketing in the parse
tree for a given noun phrase. For example, local
(phone company) and (blood pressure) medicine
exhibit right and left bracketing, respectively. We
append the word vectors of the three words in the
NP in order and use them as features for binary
classification. The dataset contains 2,227 noun
phrases split into 10 folds. The classifier is tuned
on the first fold and cross-validation accuracy is
reported on the remaining nine folds.
</bodyText>
<sectionHeader confidence="0.997795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984325888888889">
Faiz A. Al-Khayyal and James E. Falk. 1983. Jointly
constrained biconvex programming. Mathematics of
Operations Research, pages 273–286.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
ACL.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798–1828.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L.
Boyd-Graber, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
NIPS.
Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and
Shun-ichi Amari. 2009. Nonnegative Matrix and
Tensor Factorizations: Applications to Exploratory
Multi-way Data Analysis and Blind Source Separa-
tion. John Wiley &amp; Sons.
Mark Davies and Joseph L Fleiss. 1982. Measuring
agreement for multinomial data. Biometrics, pages
1047–1051.
David L. Donoho, Michael Elad, and Vladimir N.
Temlyakov. 2006. Stable recovery of sparse over-
complete representations in the presence of noise.
IEEE Transactions on Information Theory, 52(1).
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learn-
ing and stochastic optimization. Technical Report
EECS-2010-24, University of California Berkeley.
</reference>
<page confidence="0.948879">
1498
</page>
<reference confidence="0.998684349514563">
John C. Duchi, Alekh Agarwal, and Martin J. Wain-
wright. 2012. Dual averaging for distributed opti-
mization: Convergence analysis and network scal-
ing. IEEE Transactions on Automatic Control,
57(3):592–606.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL.
Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proc. of NAACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW.
Jerome Friedman, Trevor Hastie, and Robert Tibshi-
rani. 2008. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432–
441.
Alona Fyshe, Partha P. Talukdar, Brian Murphy, and
Tom M. Mitchell. 2014. Interpretable semantic vec-
tors from a joint model of brain- and text- based
meaning. In Proc. of ACL.
Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian
Murphy, and Tom M. Mitchell. 2015. A composi-
tional and interpretable semantic space. In Proc. of
NAACL.
Kuzman Ganchev, Ben Taskar, Fernando Pereira, and
Jo˜ao Gama. 2009. Posterior vs. parameter sparsity
in latent variable models. In NIPS.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.
E. Grefenstette. 2013. Towards a formal distributional
semantics: Simulating logical calculi with tensors.
arXiv:1304.5823.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proc. of EMNLP.
Georg Heigold, Erik McDermott, Vincent Vanhoucke,
Andrew Senior, and Michiel Bacchiani. 2014.
Asynchronous stochastic optimization for sequence
training of deep neural networks. In Proc. of
ICASSP.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.
Patrik O. Hoyer. 2002. Non-negative sparse coding. In
Neural Networks for Signal Processing, 2002. Proc.
of IEEE Workshop on.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP.
Douwe Kiela and Stephen Clark. 2013. Detecting
compositionality of multi-word expressions using
nearest neighbours in vector space models. In Proc.
of EMNLP.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proc. of EMNLP.
Daniel D. Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788–791.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In NIPS.
Honglak Lee, Rajat Raina, Alex Teichman, and An-
drew Y. Ng. 2009. Exponential family sparse cod-
ing with application to self-taught learning. In Proc.
of IJCAI.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Michael Lewicki and Terrence Sejnowski. 2000.
Learning overcomplete representations. Neural
Computation, 12(2):337–365.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the ACL, 1:179–192.
Mike Lewis and Mark Steedman. 2014. Combining
formal and distributional models of temporal and in-
tensional semantics. In Proc. of ACL.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proc. of COLING.
Weixiang Liu, Nanning Zheng, and Xiaofeng Lu.
2003. Non-negative matrix factorization for visual
coding. In Proc. of ICASSP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.
</reference>
<page confidence="0.931634">
1499
</page>
<reference confidence="0.999878890625">
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M´ario A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proc. of
EMNLP.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Learning effective and interpretable seman-
tic models using non-negative sparse embedding. In
Proc. of COLING.
Jerome L. Myers and Arnold D. Well. 1995. Research
Design &amp; Statistical Analysis. Routledge.
Bruno A. Olshausen and David J. Field. 1997. Sparse
coding with an overcomplete basis set: A strategy
employed by v1? Vision Research, 37(23):3311 –
3325.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proc. of ACL.
Michael Paul and Mark Dredze. 2012. Factorial LDA:
Sparse multi-dimensional text models. In NIPS.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proc. of EMNLP.
Karin Kipper Schuler. 2005. Verbnet: A Broad-
coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JAIR, 37(1):141–188.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proc. of ECML.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Lin Xiao. 2009. Dual averaging methods for regular-
ized stochastic learning and online optimization. In
NIPS.
Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proc. of CIKM.
Dani Yogatama and Noah A Smith. 2014. Linguistic
structured sparsity in text categorization. In Proc. of
ACL.
Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In Proc. of
ACL.
Jun Zhu and Eric P Xing. 2012. Sparse topical coding.
arXiv:1202.3778.
</reference>
<page confidence="0.982676">
1500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.609985">
<title confidence="0.998569">Sparse Overcomplete Word Vector Representations</title>
<author confidence="0.812893">Manaal Faruqui Yulia Tsvetkov Dani Yogatama Chris Dyer Noah A</author>
<affiliation confidence="0.8442915">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.998311">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.996630842105263">Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Faiz A Al-Khayyal</author>
<author>James E Falk</author>
</authors>
<title>Jointly constrained biconvex programming.</title>
<date>1983</date>
<journal>Mathematics of Operations Research,</journal>
<pages>273--286</pages>
<contexts>
<context position="10465" citStr="Al-Khayyal and Falk, 1983" startWordPosition="1685" endWordPosition="1688">catering, clientele with respect to ai. We define ηt at+1,i,j = { 0, if |¯gt,i,j |≤ A (5) Table 2: Highest frequency words in randomly 0, if &apos;y &lt; 0 picked word clusters of binary sparse overcom&apos;y, otherwise plete Glove vectors. 2.4 Binarizing Transformation Our aim with method B is to obtain word representations that can emulate the binary-feature space designed for various NLP tasks. We could 1493 state this as an optimization problem: kxi − Dbik22 + Akbik1 1 + TkDk22 (6) where B denotes the binary (and also sparse) representation. This is an mixed integer bilinear program, which is NP-hard (Al-Khayyal and Falk, 1983). Unfortunately, the number of variables in the problem is ≈ KV which reaches 100 million when V = 100, 000 and K = 1, 000, which is intractable to solve using standard techniques. A more tractable relaxation to this hard problem is to first constrain the continuous representation A to be nonnegative (i.e, ai ∈ RK ≥0; §2.2). Then, in order to avoid an expensive computation, we take the nonnegative word vectors obtained using Eq. 3 and project nonzero values to 1, preserving the 0 values. Table 2 shows a random set of word clusters obtained by (i) applying our method to Glove initial vectors an</context>
</contexts>
<marker>Al-Khayyal, Falk, 1983</marker>
<rawString>Faiz A. Al-Khayyal and James E. Falk. 1983. Jointly constrained biconvex programming. Mathematics of Operations Research, pages 273–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="17778" citStr="Artstein and Poesio, 2008" startWordPosition="2933" endWordPosition="2936">ension. An example of an instance is: naval, industrial, technological, marine, identity �1 if xi,j &gt; 0 0 otherwise bi,j = (7) ⎧ ⎨ ⎩ ai,j = 1495 X: Glove SG GC Multi Average X 76.2 78.0 61.9 68.1 71.0 Eq. 7 75.7 75.8 60.5 64.1 69.0 Eq. 8 (Guo et al., 2014) 75.8 76.9 60.5 66.2 69.8 A 79.4 79.4 67.2 74.8 75.2 Table 4: Average performance across all tasks and vector models using different transformations. Vectors A1 A2 A3 Avg. IAA K X 61 53 56 57 70 0.40 A 71 70 72 71 77 0.45 Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss’ K (Davies and Fleiss, 1982). (The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges. Results in Table 5 confirm that the sparse overcomplete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss’ K i</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1664" citStr="Baker et al., 1998" startWordPosition="239" endWordPosition="242">ke parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contrib</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1102" citStr="Bansal et al., 2014" startWordPosition="150" endWordPosition="153">ses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships amo</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pascal Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context position="20696" citStr="Bengio et al., 2013" startWordPosition="3379" endWordPosition="3383">es have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We have presented a method that converts word vectors obtained using any state-of-the-art word vector</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4321" citStr="Chang et al., 2009" startWordPosition="645" endWordPosition="648">ansformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ical semantic theories. Using a number of stateof-the-art word vectors as input, we find consistent benefits of our method on a suite of standard benchmark evaluation tasks (§3). We also evaluate our word vectors in a word intrusion experiment with humans (Chang et al., 2009) and find that our sparse vectors are more interpretable than the original vectors (§4). We anticipate that sparse, binary vectors can play an important role as features in statistical NLP models, which still rely predominantly on discrete, sparse features whose interpretability enables error analysis and continued development. We have made an implementation of our method publicly available.1 2 Sparse Overcomplete Word Vectors We consider methods for transforming dense word vectors to sparse, binary overcomplete word vectors. Fig. 1 shows two approaches. The one on the top, method A, converts </context>
<context position="16294" citStr="Chang et al., 2009" startWordPosition="2667" endWordPosition="2670">sparsity is achieved through: 1 if xi,j ≥ M+ −1 if xi,j ≤ M− (8) 0 otherwise where M+ (M−) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability, focusing on individual dimensions. 4.1 Word Intrusion Word intrusion experiments seek to quantify the extent to which dimensions of a learned word representation are coherent to humans. In one instance of the experiment, a human judge is presented with five words in random order and asked to select the “intruder.” The words are selected by the experimenter by choosing one dimension j of the learned representation, then ranking the words on that dimension alone. The dimensions are chosen in decrea</context>
</contexts>
<marker>Chang, Gerrish, Wang, Boyd-Graber, Blei, 2009</marker>
<rawString>Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L. Boyd-Graber, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrzej Cichocki</author>
<author>Rafal Zdunek</author>
<author>Anh Huy Phan</author>
<author>Shun-ichi Amari</author>
</authors>
<title>Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation.</title>
<date>2009</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="6586" citStr="Cichocki et al., 2009" startWordPosition="1023" endWordPosition="1026">ctions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity co</context>
<context position="20389" citStr="Cichocki et al., 2009" startWordPosition="3333" endWordPosition="3336">r initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 14</context>
</contexts>
<marker>Cichocki, Zdunek, Phan, Amari, 2009</marker>
<rawString>Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. 2009. Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring agreement for multinomial data.</title>
<date>1982</date>
<journal>Biometrics,</journal>
<pages>1047--1051</pages>
<contexts>
<context position="17818" citStr="Davies and Fleiss, 1982" startWordPosition="2940" endWordPosition="2943">l, industrial, technological, marine, identity �1 if xi,j &gt; 0 0 otherwise bi,j = (7) ⎧ ⎨ ⎩ ai,j = 1495 X: Glove SG GC Multi Average X 76.2 78.0 61.9 68.1 71.0 Eq. 7 75.7 75.8 60.5 64.1 69.0 Eq. 8 (Guo et al., 2014) 75.8 76.9 60.5 66.2 69.8 A 79.4 79.4 67.2 74.8 75.2 Table 4: Average performance across all tasks and vector models using different transformations. Vectors A1 A2 A3 Avg. IAA K X 61 53 56 57 70 0.40 A 71 70 72 71 77 0.45 Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss’ K (Davies and Fleiss, 1982). (The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges. Results in Table 5 confirm that the sparse overcomplete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss’ K increases from “fair” to “moderate” agree</context>
</contexts>
<marker>Davies, Fleiss, 1982</marker>
<rawString>Mark Davies and Joseph L Fleiss. 1982. Measuring agreement for multinomial data. Biometrics, pages 1047–1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Donoho</author>
<author>Michael Elad</author>
<author>Vladimir N Temlyakov</author>
</authors>
<title>Stable recovery of sparse overcomplete representations in the presence of noise.</title>
<date>2006</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>52</volume>
<issue>1</issue>
<contexts>
<context position="3141" citStr="Donoho et al., 2006" startWordPosition="455" endWordPosition="458">stributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to</context>
<context position="20238" citStr="Donoho et al., 2006" startWordPosition="3309" endWordPosition="3312">ions, poems, songs naval, industrial, technological, marine stadium, belt, championship, toll, ride, coach Table 6: Top-ranked words per dimension for initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be </context>
</contexts>
<marker>Donoho, Elad, Temlyakov, 2006</marker>
<rawString>David L. Donoho, Michael Elad, and Vladimir N. Temlyakov. 2006. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<tech>Technical Report EECS-2010-24,</tech>
<institution>University of California Berkeley.</institution>
<contexts>
<context position="7348" citStr="Duchi et al., 2010" startWordPosition="1146" endWordPosition="1149">e sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next. 2.3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2–3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014). However, directly applying stochastic subgradient descent to an `1-regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the on</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. Technical Report EECS-2010-24, University of California Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Duchi</author>
<author>Alekh Agarwal</author>
<author>Martin J Wainwright</author>
</authors>
<title>Dual averaging for distributed optimization: Convergence analysis and network scaling.</title>
<date>2012</date>
<journal>IEEE Transactions on Automatic Control,</journal>
<volume>57</volume>
<issue>3</issue>
<contexts>
<context position="7562" citStr="Duchi et al., 2012" startWordPosition="1186" endWordPosition="1189"> the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next. 2.3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2–3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014). However, directly applying stochastic subgradient descent to an `1-regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: ¯gt = 1 �t t,=1 gt, Here, the subgradients do not t include terms for the regularizer; they are derivatives of the unregularized objective (λ = 0, τ = 0) 1492 Figure 1: Methods for </context>
</contexts>
<marker>Duchi, Agarwal, Wainwright, 2012</marker>
<rawString>John C. Duchi, Alekh Agarwal, and Martin J. Wainwright. 2012. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57(3):592–606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="23318" citStr="Faruqui and Dyer (2014)" startWordPosition="3798" endWordPosition="3801">., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which con</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay K Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
<author>Noah A Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="2607" citStr="Faruqui et al., 2015" startWordPosition="376" endWordPosition="379">heories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful f</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retrofitting word vectors to semantic lexicons. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="23907" citStr="Finkelstein et al., 2001" startWordPosition="3889" endWordPosition="3892">ual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. This dataset is used to tune sparse vector learning hyperparameters (§2.5), while the remaining of the tasks discussed in this section are completely held out. 4https://code.google.com/p/word2vec 5http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6http://cs.cmu.edu/˜mfaruqui/soft.html 1497 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
</authors>
<title>Sparse inverse covariance estimation with the graphical lasso.</title>
<date>2008</date>
<journal>Biostatistics,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>441</pages>
<contexts>
<context position="3332" citStr="Friedman et al., 2008" startWordPosition="489" endWordPosition="492">rser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language</context>
<context position="20497" citStr="Friedman et al., 2008" startWordPosition="3350" endWordPosition="3353">t vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsifie</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2008</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2008. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432– 441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alona Fyshe</author>
<author>Partha P Talukdar</author>
<author>Brian Murphy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Interpretable semantic vectors from a joint model of brain- and text- based meaning.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3535" citStr="Fyshe et al., 2014" startWordPosition="521" endWordPosition="524">interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ical semantic theories. Using a number of stateof-the-art word vectors as input, we find c</context>
<context position="6627" citStr="Fyshe et al., 2014" startWordPosition="1031" endWordPosition="1034">. To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated durin</context>
</contexts>
<marker>Fyshe, Talukdar, Murphy, Mitchell, 2014</marker>
<rawString>Alona Fyshe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. 2014. Interpretable semantic vectors from a joint model of brain- and text- based meaning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alona Fyshe</author>
<author>Leila Wehbe</author>
<author>Partha P Talukdar</author>
<author>Brian Murphy</author>
<author>Tom M Mitchell</author>
</authors>
<title>A compositional and interpretable semantic space.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="6648" citStr="Fyshe et al., 2015" startWordPosition="1035" endWordPosition="1038">ord representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as ex</context>
</contexts>
<marker>Fyshe, Wehbe, Talukdar, Murphy, Mitchell, 2015</marker>
<rawString>Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. 2015. A compositional and interpretable semantic space. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
<author>Jo˜ao Gama</author>
</authors>
<title>Posterior vs. parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="20554" citStr="Ganchev et al., 2009" startWordPosition="3359" endWordPosition="3362">here has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values ar</context>
</contexts>
<marker>Ganchev, Taskar, Pereira, Gama, 2009</marker>
<rawString>Kuzman Ganchev, Ben Taskar, Fernando Pereira, and Jo˜ao Gama. 2009. Posterior vs. parameter sparsity in latent variable models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3353" citStr="Glorot et al., 2011" startWordPosition="493" endWordPosition="496"> called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 14</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3309" citStr="Goodman, 2004" startWordPosition="487" endWordPosition="488"> in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confere</context>
<context position="20513" citStr="Goodman, 2004" startWordPosition="3354" endWordPosition="3355">rk To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Ne</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
</authors>
<title>Towards a formal distributional semantics: Simulating logical calculi with tensors.</title>
<date>2013</date>
<pages>1304--5823</pages>
<contexts>
<context position="2202" citStr="Grefenstette, 2013" startWordPosition="315" endWordPosition="317">es, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997)</context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>E. Grefenstette. 2013. Towards a formal distributional semantics: Simulating logical calculi with tensors. arXiv:1304.5823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1147" citStr="Guo et al., 2014" startWordPosition="158" endWordPosition="161">). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expen</context>
<context position="3616" citStr="Guo et al., 2014" startWordPosition="535" endWordPosition="538"> dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ical semantic theories. Using a number of stateof-the-art word vectors as input, we find consistent benefits of our method on a suite of standard benchmark evaluation task</context>
<context position="15617" citStr="Guo et al. (2014)" startWordPosition="2557" endWordPosition="2560">4 68.6 X 28.7 75.5 63.8 83.6 64.3 81.8 79.2 68.1 Multi A 28.1 78.6 79.2 93.9 78.2 84.5 81.1 74.8 B 28.7 77.6 82.0 94.7 81.4 85.6 81.9 75.9 Table 3: Performance comparison of transformed vectors to initial vectors X. We show sparse overcomplete representations A and also binarized representations B. Initial vectors are discussed in §A and tasks in §B. Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L. achieves a binary, sparse vector (B) by applying: The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through: 1 if xi,j ≥ M+ −1 if xi,j ≤ M− (8) 0 otherwise where M+ (M−) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following </context>
<context position="17408" citStr="Guo et al., 2014" startWordPosition="2866" endWordPosition="2869">ned representation, then ranking the words on that dimension alone. The dimensions are chosen in decreasing order of the variance of their values across the vocabulary. Four of the words are the top-ranked words according to j, and the “true” intruder is a word from the bottom half of the list, chosen to be a word that appears in the top 10% of some other dimension. An example of an instance is: naval, industrial, technological, marine, identity �1 if xi,j &gt; 0 0 otherwise bi,j = (7) ⎧ ⎨ ⎩ ai,j = 1495 X: Glove SG GC Multi Average X 76.2 78.0 61.9 68.1 71.0 Eq. 7 75.7 75.8 60.5 64.1 69.0 Eq. 8 (Guo et al., 2014) 75.8 76.9 60.5 66.2 69.8 A 79.4 79.4 67.2 74.8 75.2 Table 4: Average performance across all tasks and vector models using different transformations. Vectors A1 A2 A3 Avg. IAA K X 61 53 56 57 70 0.40 A 71 70 72 71 77 0.45 Table 5: Accuracy of three human annotators on the word intrusion task, along with the average inter-annotator agreement (Artstein and Poesio, 2008) and Fleiss’ K (Davies and Fleiss, 1982). (The last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. W</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg Heigold</author>
<author>Erik McDermott</author>
<author>Vincent Vanhoucke</author>
<author>Andrew Senior</author>
<author>Michiel Bacchiani</author>
</authors>
<title>Asynchronous stochastic optimization for sequence training of deep neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="7585" citStr="Heigold et al., 2014" startWordPosition="1190" endWordPosition="1193">or nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next. 2.3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2–3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014). However, directly applying stochastic subgradient descent to an `1-regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: ¯gt = 1 �t t,=1 gt, Here, the subgradients do not t include terms for the regularizer; they are derivatives of the unregularized objective (λ = 0, τ = 0) 1492 Figure 1: Methods for obtaining sparse overco</context>
</contexts>
<marker>Heigold, McDermott, Vanhoucke, Senior, Bacchiani, 2014</marker>
<rawString>Georg Heigold, Erik McDermott, Vincent Vanhoucke, Andrew Senior, and Michiel Bacchiani. 2014. Asynchronous stochastic optimization for sequence training of deep neural networks. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="24365" citStr="Hill et al., 2014" startWordPosition="3943" endWordPosition="3946">for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. This dataset is used to tune sparse vector learning hyperparameters (§2.5), while the remaining of the tasks discussed in this section are completely held out. 4https://code.google.com/p/word2vec 5http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6http://cs.cmu.edu/˜mfaruqui/soft.html 1497 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik O Hoyer</author>
</authors>
<title>Non-negative sparse coding.</title>
<date>2002</date>
<booktitle>In Neural Networks for Signal Processing,</booktitle>
<contexts>
<context position="6765" citStr="Hoyer, 2002" startWordPosition="1055" endWordPosition="1056">into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimization, as explained next. 2.3 Optimization We use online adaptive gradient descent (AdaGrad; Duchi et al., 2010) for solving the </context>
</contexts>
<marker>Hoyer, 2002</marker>
<rawString>Patrik O. Hoyer. 2002. Non-negative sparse coding. In Neural Networks for Signal Processing, 2002. Proc. of IEEE Workshop on.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="23171" citStr="Huang et al., 2012" startWordPosition="3774" endWordPosition="3777">dia and English Gigaword and are of length 300.3 3http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word S</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3294" citStr="Kazama and Tsujii, 2003" startWordPosition="482" endWordPosition="486">he transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Internationa</context>
<context position="20474" citStr="Kazama and Tsujii, 2003" startWordPosition="3346" endWordPosition="3349">dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Vis</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>Detecting compositionality of multi-word expressions using nearest neighbours in vector space models.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2161" citStr="Kiela and Clark, 2013" startWordPosition="307" endWordPosition="310">ns described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” r</context>
</contexts>
<marker>Kiela, Clark, 2013</marker>
<rawString>Douwe Kiela and Stephen Clark. 2013. Detecting compositionality of multi-word expressions using nearest neighbours in vector space models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="18446" citStr="Landis and Koch, 1977" startWordPosition="3040" endWordPosition="3043"> last word is the intruder.) We formed instances from initializing vectors and from our sparse overcomplete vectors (A). Each of these two combines the four different initializers X. We selected the 25 dimensions d in each case. Each of the 100 instances per condition (initial vs. sparse overcomplete) was given to three judges. Results in Table 5 confirm that the sparse overcomplete vectors are more interpretable than the dense vectors. The inter-annotator agreement on the sparse vectors increases substantially, from 57% to 71%, and the Fleiss’ K increases from “fair” to “moderate” agreement (Landis and Koch, 1977). 4.2 Qualitative Evaluation of Interpretability If a vector dimension is interpretable, the topranking words for that dimension should display semantic or syntactic groupings. To verify this qualitatively, we select five dimensions with the highest variance of values in initial and sparsified GC vectors. We compare top-ranked words in the dimensions extracted from the two representations. The words are listed in Table 6, a dimension per row. Subjectively, we find the semantic groupings better in the sparse vectors than in the initial vectors. Figure 3 visualizes the sparsified GC vectors for </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
</authors>
<title>Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1080" citStr="Lazaridou et al., 2013" startWordPosition="145" endWordPosition="149"> classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005)</context>
<context position="26397" citStr="Lazaridou et al. (2013)" startWordPosition="4272" endWordPosition="4275">d accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 f</context>
</contexts>
<marker>Lazaridou, Vecchi, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni. 2013. Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="6563" citStr="Lee and Seung, 1999" startWordPosition="1019" endWordPosition="1022">r, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly</context>
<context position="20347" citStr="Lee and Seung, 1999" startWordPosition="3324" endWordPosition="3328">le 6: Top-ranked words per dimension for initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglak Lee</author>
<author>Alexis Battle</author>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
</authors>
<title>Efficient sparse coding algorithms.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5500" citStr="Lee et al., 2006" startWordPosition="835" endWordPosition="838">he one on the top, method A, converts dense vectors to sparse overcomplete vectors (§2.1). The one beneath, method B, converts dense vectors to sparse and binary overcomplete vectors (§2.2 and §2.4). Let V be the vocabulary size. In the following, X E RLxV is the matrix constructed by stacking V non-sparse “input” word vectors of length L (produced by an arbitrary word vector estimator). We will refer to these as initializing vectors. A E RKxV contains V sparse overcomplete word vectors of length K. “Overcomplete” representation learning implies that K &gt; L. 2.1 Sparse Coding In sparse coding (Lee et al., 2006), the goal is to represent each input vector xi as a sparse linear combination of basis vectors, ai. Our experiments consider four initializing methods for these vectors, discussed in Appendix A. Given X, we seek to solve arg min 11X − DA112 2 + λQ(A) + τ11D1122, (1) D,A where D E RLxK is the dictionary of basis vectors. λ is a regularization hyperparameter, and Q is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ </context>
</contexts>
<marker>Lee, Battle, Raina, Ng, 2006</marker>
<rawString>Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. 2006. Efficient sparse coding algorithms. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglak Lee</author>
<author>Rajat Raina</author>
<author>Alex Teichman</author>
<author>Andrew Y Ng</author>
</authors>
<title>Exponential family sparse coding with application to self-taught learning.</title>
<date>2009</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="6009" citStr="Lee et al., 2009" startWordPosition="928" endWordPosition="931">rcomplete” representation learning implies that K &gt; L. 2.1 Sparse Coding In sparse coding (Lee et al., 2006), the goal is to represent each input vector xi as a sparse linear combination of basis vectors, ai. Our experiments consider four initializing methods for these vectors, discussed in Appendix A. Given X, we seek to solve arg min 11X − DA112 2 + λQ(A) + τ11D1122, (1) D,A where D E RLxK is the dictionary of basis vectors. λ is a regularization hyperparameter, and Q is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; </context>
</contexts>
<marker>Lee, Raina, Teichman, Ng, 2009</marker>
<rawString>Honglak Lee, Rajat Raina, Alex Teichman, and Andrew Y. Ng. 2009. Exponential family sparse coding with application to self-taught learning. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1644" citStr="Levin, 1993" startWordPosition="237" endWordPosition="238"> NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al.</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lewicki</author>
<author>Terrence Sejnowski</author>
</authors>
<title>Learning overcomplete representations.</title>
<date>2000</date>
<journal>Neural Computation,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="3067" citStr="Lewicki and Sejnowski, 2000" startWordPosition="442" endWordPosition="445">nto binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% spa</context>
<context position="20194" citStr="Lewicki and Sejnowski, 2000" startWordPosition="3302" endWordPosition="3305"> identity, relations A files, bills, titles, collections, poems, songs naval, industrial, technological, marine stadium, belt, championship, toll, ride, coach Table 6: Top-ranked words per dimension for initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of s</context>
</contexts>
<marker>Lewicki, Sejnowski, 2000</marker>
<rawString>Michael Lewicki and Terrence Sejnowski. 2000. Learning overcomplete representations. Neural Computation, 12(2):337–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>Transactions of the ACL,</journal>
<pages>1--179</pages>
<contexts>
<context position="2138" citStr="Lewis and Steedman, 2013" startWordPosition="303" endWordPosition="306">ing like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes cal</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined distributional and logical semantics. Transactions of the ACL, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combining formal and distributional models of temporal and intensional semantics.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2228" citStr="Lewis and Steedman, 2014" startWordPosition="318" endWordPosition="321">dentifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete rep</context>
</contexts>
<marker>Lewis, Steedman, 2014</marker>
<rawString>Mike Lewis and Mark Steedman. 2014. Combining formal and distributional models of temporal and intensional semantics. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="25591" citStr="Li and Roth, 2002" startWordPosition="4139" endWordPosition="4142">e classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (8</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weixiang Liu</author>
<author>Nanning Zheng</author>
<author>Xiaofeng Lu</author>
</authors>
<title>Non-negative matrix factorization for visual coding.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="20365" citStr="Liu et al., 2003" startWordPosition="3329" endWordPosition="3332">s per dimension for initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012;</context>
</contexts>
<marker>Liu, Zheng, Lu, 2003</marker>
<rawString>Weixiang Liu, Nanning Zheng, and Xiaofeng Lu. 2003. Non-negative matrix factorization for visual coding. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="26464" citStr="Marcus et al., 1993" startWordPosition="4283" endWordPosition="4286">der three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validatio</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="20597" citStr="Martins et al., 2011" startWordPosition="3365" endWordPosition="3369">ercomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<contexts>
<context position="22703" citStr="Mikolov et al., 2013" startWordPosition="3696" endWordPosition="3699">rmy Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1733" citStr="Miller, 1995" startWordPosition="250" endWordPosition="251">ognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method th</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning effective and interpretable semantic models using non-negative sparse embedding.</title>
<date>2012</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="3514" citStr="Murphy et al., 2012" startWordPosition="517" endWordPosition="520">ase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ical semantic theories. Using a number of stateof-the-art word vector</context>
<context position="6607" citStr="Murphy et al., 2012" startWordPosition="1027" endWordPosition="1030">ed (Lee et al., 2009). To obtain sparse word representations we will impose an `1 1https://github.com/mfaruqui/ sparse-coding penalty on A. Eq. 1 can be broken down into loss for each word vector which can be optimized separately in parallel (§2.3): V arg min 11xi−Dai112 2+λ11ai111+τ11D112 2 (2) D,A i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ail. Thus, the new objective for nonnegative sparse vectors becomes: V arg min 11xi−Dai1122+λ11ai111+τ11D1122 D∈R&gt;0 K,A∈R&gt;O V i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easil</context>
<context position="16237" citStr="Murphy et al. (2012)" startWordPosition="2657" endWordPosition="2660">. Here, the original vector length is also preserved, but sparsity is achieved through: 1 if xi,j ≥ M+ −1 if xi,j ≤ M− (8) 0 otherwise where M+ (M−) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability, focusing on individual dimensions. 4.1 Word Intrusion Word intrusion experiments seek to quantify the extent to which dimensions of a learned word representation are coherent to humans. In one instance of the experiment, a human judge is presented with five words in random order and asked to select the “intruder.” The words are selected by the experimenter by choosing one dimension j of the learned representation, then ranking the words on </context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable semantic models using non-negative sparse embedding. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome L Myers</author>
<author>Arnold D Well</author>
</authors>
<date>1995</date>
<journal>Research Design &amp; Statistical Analysis. Routledge.</journal>
<contexts>
<context position="24671" citStr="Myers and Well, 1995" startWordPosition="3991" endWordPosition="3995">arning hyperparameters (§2.5), while the remaining of the tasks discussed in this section are completely held out. 4https://code.google.com/p/word2vec 5http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6http://cs.cmu.edu/˜mfaruqui/soft.html 1497 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Q</context>
</contexts>
<marker>Myers, Well, 1995</marker>
<rawString>Jerome L. Myers and Arnold D. Well. 1995. Research Design &amp; Statistical Analysis. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno A Olshausen</author>
<author>David J Field</author>
</authors>
<title>Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research,</title>
<date>1997</date>
<volume>37</volume>
<issue>23</issue>
<pages>3325</pages>
<contexts>
<context position="2802" citStr="Olshausen and Field, 1997" startWordPosition="403" endWordPosition="406">, 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introduc</context>
<context position="20164" citStr="Olshausen and Field, 1997" startWordPosition="3297" endWordPosition="3301">relationships, connections, identity, relations A files, bills, titles, collections, poems, songs naval, industrial, technological, marine stadium, belt, championship, toll, ride, coach Table 6: Top-ranked words per dimension for initial and sparsified GC representations. Each line shows words from a different dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical seman</context>
</contexts>
<marker>Olshausen, Field, 1997</marker>
<rawString>Bruno A. Olshausen and David J. Field. 1997. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research, 37(23):3311 – 3325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
</authors>
<title>Nghia The Pham, and</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Paperno, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Factorial LDA: Sparse multi-dimensional text models.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="20964" citStr="Paul and Dredze, 2012" startWordPosition="3422" endWordPosition="3425">1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and outperform the original vectors from which they are derived on a suite of semantics and syntactic evaluation benchmarks. We also fi</context>
</contexts>
<marker>Paul, Dredze, 2012</marker>
<rawString>Michael Paul and Mark Dredze. 2012. Factorial LDA: Sparse multi-dimensional text models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="22411" citStr="Pennington et al., 2014" startWordPosition="3656" endWordPosition="3659">ymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Contex</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>Verbnet: A Broadcoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1680" citStr="Schuler, 2005" startWordPosition="243" endWordPosition="244">u et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this di</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin Kipper Schuler. 2005. Verbnet: A Broadcoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1193" citStr="Socher et al., 2013" startWordPosition="165" endWordPosition="168">ectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanin</context>
<context position="24791" citStr="Socher et al. (2013)" startWordPosition="4011" endWordPosition="4014">://code.google.com/p/word2vec 5http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6http://cs.cmu.edu/˜mfaruqui/soft.html 1497 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian lda-based model for semi-supervised partof-speech tagging.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="20911" citStr="Toutanova and Johnson, 2007" startWordPosition="3414" endWordPosition="3417">sed for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and outperform the original vectors from which they are derived on a suite of sema</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2007. A bayesian lda-based model for semi-supervised partof-speech tagging. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>JAIR,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1478" citStr="Turney and Pantel, 2010" startWordPosition="208" endWordPosition="212">o work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much atte</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. JAIR, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proc. of ECML.</booktitle>
<contexts>
<context position="1452" citStr="Turney, 2001" startWordPosition="206" endWordPosition="207">ionally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. 1 Introduction Distributed representations of words have been shown to benefit NLP tasks like parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013). The attraction of word vectors is that they can be derived directly from raw, unannotated corpora. Intrinsic evaluations on various tasks are guiding methods toward discovery of a representation that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a probl</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proc. of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Maria Vecchi</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Studying the recursive behaviour of adjectival modification with compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2182" citStr="Vecchi et al., 2013" startWordPosition="311" endWordPosition="314">xical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005) and relationships among word meanings (Miller, 1995). Though expensive to construct, conceptualizing word meanings symbolically is important for theoretical understanding and also when we incorporate lexical semantics into computational models where interpretability is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshau</context>
</contexts>
<marker>Vecchi, Zamparelli, Baroni, 2013</marker>
<rawString>Eva Maria Vecchi, Roberto Zamparelli, and Marco Baroni. 2013. Studying the recursive behaviour of adjectival modification with compositional distributional semantics. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Xiao</author>
</authors>
<title>Dual averaging methods for regularized stochastic learning and online optimization.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7919" citStr="Xiao, 2009" startWordPosition="1240" endWordPosition="1241">nt descent (AdaGrad; Duchi et al., 2010) for solving the optimization problems in Eqs. 2–3 by updating A and D. In order to speed up training we use asynchronous updates to the parameters of the model in parallel for every word vector (Duchi et al., 2012; Heigold et al., 2014). However, directly applying stochastic subgradient descent to an `1-regularized objective fails to produce sparse solutions in bounded time, which has motivated several specialized algorithms that target such objectives. We use the AdaGrad variant of one such learning algorithm, the regularized dual averaging algorithm (Xiao, 2009), which keeps track of the online average gradient at time t: ¯gt = 1 �t t,=1 gt, Here, the subgradients do not t include terms for the regularizer; they are derivatives of the unregularized objective (λ = 0, τ = 0) 1492 Figure 1: Methods for obtaining sparse overcomplete vectors (top, method A, §2.1) and sparse, binary overcomplete word vectors (bottom, method B, §2.2 and §2.4). Observed dense vectors of length L (left) are converted to sparse non-negative vectors (center) of length K which are then projected into the binary vector space (right), where L « K. X is dense, A is sparse, and B is</context>
</contexts>
<marker>Xiao, 2009</marker>
<rawString>Lin Xiao. 2009. Dual averaging methods for regularized stochastic learning and online optimization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Xu</author>
<author>Yalong Bai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Gang Wang</author>
<author>Xiaoguang Liu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Rcnet: A general framework for incorporating knowledge into word representations.</title>
<date>2014</date>
<booktitle>In Proc. of CIKM.</booktitle>
<contexts>
<context position="2584" citStr="Xu et al., 2014" startWordPosition="372" endWordPosition="375">rface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of</context>
</contexts>
<marker>Xu, Bai, Bian, Gao, Wang, Liu, Liu, 2014</marker>
<rawString>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rcnet: A general framework for incorporating knowledge into word representations. In Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Noah A Smith</author>
</authors>
<title>Linguistic structured sparsity in text categorization.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3379" citStr="Yogatama and Smith, 2014" startWordPosition="497" endWordPosition="500">ete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recent explorations of sparsity as a useful form of inductive bias in NLP and machine learning more broadly (Kazama and Tsujii, 2003; Goodman, 2004; Friedman et al., 2008; Glorot et al., 2011; Yogatama and Smith, 2014, inter alia). Introducing sparsity in word vector dimensions has been shown to improve dimension interpretability (Murphy et al., 2012; Fyshe et al., 2014) and usability of word vectors as features in downstream tasks (Guo et al., 2014). The word vectors we produce are more than 90% sparse; we also consider binarizing transformations that bring them closer to the categories and relations of lex1491 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500, Beijing, China, J</context>
<context position="20645" citStr="Yogatama and Smith, 2014" startWordPosition="3372" endWordPosition="3375">are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We have presented a method that converts word vect</context>
<context position="26068" citStr="Yogatama and Smith (2014)" startWordPosition="4218" endWordPosition="4221"> six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing </context>
</contexts>
<marker>Yogatama, Smith, 2014</marker>
<rawString>Dani Yogatama and Noah A Smith. 2014. Linguistic structured sparsity in text categorization. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2567" citStr="Yu and Dredze, 2014" startWordPosition="368" endWordPosition="371">is desired. On the surface, discrete theories seem incommensurate with the distributed approach, a problem now receiving much attention in computational linguistics (Lewis and Steedman, 2013; Kiela and Clark, 2013; Vecchi et al., 2013; Grefenstette, 2013; Lewis and Steedman, 2014; Paperno et al., 2014). Our contribution to this discussion is a new, principled sparse coding method that transforms any distributed representation of words into sparse vectors, which can then be transformed into binary vectors (§2). Unlike recent approaches of incorporating semantics in distributional word vectors (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015), the method does not rely on any external information source. The transformation results in longer, sparser vectors, sometimes called an “overcomplete” representation (Olshausen and Field, 1997). Sparse, overcomplete representations have been motivated in other domains as a way to increase separability and interpretability, with each instance (here, a word) having a small number of active dimensions (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000), and to increase stability in the presence of noise (Donoho et al., 2006). Our work builds on recen</context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Eric P Xing</author>
</authors>
<date>2012</date>
<note>Sparse topical coding. arXiv:1202.3778.</note>
<contexts>
<context position="20985" citStr="Zhu and Xing, 2012" startWordPosition="3426" endWordPosition="3429"> Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 fish horse dog chicago seattle boston VV i I i Y A I I r I I V I I V Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes are white. h 6 Conclusion We have presented a method that converts word vectors obtained using any state-of-the-art word vector model into sparse and optionally binary word vectors. These transformed vectors appear to come closer to features used in NLP tasks and outperform the original vectors from which they are derived on a suite of semantics and syntactic evaluation benchmarks. We also find that the sparse ve</context>
</contexts>
<marker>Zhu, Xing, 2012</marker>
<rawString>Jun Zhu and Eric P Xing. 2012. Sparse topical coding. arXiv:1202.3778.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>