<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000653">
<title confidence="0.986175">
Computing Backchannel Distributions in Multi-Party Conversations
</title>
<author confidence="0.903312">
Dirk Heylen Rieks op den Akker
</author>
<affiliation confidence="0.9374885">
Human Media Interaction Human Media Interaction
University of Twente University of Twente
</affiliation>
<email confidence="0.99675">
heylen@cs.utwente.nl infrieks@cs.utwente.nl
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975444444444">
In multi-party conversations it may not al-
ways be obvious who is talking to whom.
Backchannels may provide a partial answer
to this question, possibly in combination
with some other events, such as gaze behav-
iors of the interlocutors. We look at some
patterns in multi-party interaction relating
features of backchannel behaviours to as-
pects of the partipation framework.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999656095238096">
In this paper we present a summary of our investiga-
tions into the distribution ofback-channels and some
other forms of feedback and assesments in argumen-
tative multi-party discourse. We are interested in
such expressions for several reasons. First, the sheer
utterance of a backchannel indicates the presence of
an auditor that indicates “I am here, I am attending”.
The fact that it is being uttered by an auditor indi-
cates intrinsically that the auditor felt addressed in
some way or another by the speaker. For the anal-
ysis of multi-party conversations, it is important to
establish who is talking to whom and backchannels,
at least seem to give away the whom part. Second,
the exact form, the kind of vocalisation, the intona-
tion and the context may further invest the utterance
with additional meanings, expressing various atti-
tudes towards what has been said: skepticism, sur-
prise, liking, agreement, and so on. So, when we
look at back-channels in the context of multi-party
dialogues they may tell us something about the par-
ticipation framework on the one hand (who was talk-
</bodyText>
<page confidence="0.98631">
17
</page>
<bodyText confidence="0.994987285714286">
ing to whom) and about the way utterances are being
assessed by their audience.
The qualifier “in some way or another” with re-
spect to feeling or being addressed is particularly
important in the context of multi-party dialogues
(i.e. dialogues with more than two persons present).
Typically, an utterance by a speaker instantiates the
performance of a speech act with a particular il-
locutionary and perlocutionary force. The speech
act involves a request for uptake. However, as has
been pointed out several times (Goffman (Goffman,
1981), Levinson (Levinson, 1988), Clark and Carl-
son (Clark and Carlson, 1992), Schegloff (Schegloff,
1988)), participants in a multi-party conversation
can have a different role or status and they can be
addressed in different ways.
In this paper we report on some of our investiga-
tions into the distribution of backchannels in mul-
tiparty interactions (for instance in relation to other
phenomena such as gaze) and how this information
can help us to uncover certain features of floor and
stance taking automatically.
We will first describe the corpus and the anno-
tations. Next we look at the annotations of utter-
ances consisting of starting with “yeah” and try to
see whether we can classify these utterances as con-
tinuers, i.e. neutral with respect to stance taking
(Schegloff, 1981), or as assessments.
</bodyText>
<sectionHeader confidence="0.993887" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999786">
The argumentative discourses that we are study-
ing are part of the meeting corpus collected during
the AMI project (McCowan et al., 2005). From a
computational, technological perspective, the aims
</bodyText>
<note confidence="0.875781">
Proceedings of the Workshop on Embodied Language Processing, pages 17–24,
Prague, Czech Republic, June 28, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.994175333333334">
of this research is directed at developing automatic
procedures that can help to provide answers to any
query users may have about what goes on in the
meetings. The AMI corpus consists of meetings in
which a group of four people discuss the design of a
new remote control. T
The kinds of queries that we would like our proce-
dures to be able to answer are related to these moves:
what suggestions have been made; what were the ar-
guments given and how much animosity was there
related to the decision. In the AMI corpus, the meet-
ing recordings have been annotated on many levels,
allowing the use of machine learning techniques to
develop appropriate algorithms for answering such
questions. We focus on the dialogue act annotation
scheme. This contains three types of information.
Information on the speech act, the relation between
speech acts and information on addressing.
The dialogue act classes that are distinguished in
our dialogue act annotation schema fall into the fol-
lowing classes:
</bodyText>
<listItem confidence="0.99302095">
• Classes for things that are not really dialogue
acts at all, but are present to account for some-
thing in the transcription that doesn’t really
convey a speaker intention. This includes
backchannels, stalls and fragments
• Classes for acts that are about information ex-
change: inform and elicit inform.
• Classes for acts about some action that an indi-
vidual or group might take: suggest, offer, elicit
suggest or offer.
• Classes for acts that are about commenting on
the previous discussion: assess, comment about
understanding, elicit assessment, elicit com-
ment about understanding
• Classes for acts whose primary purpose is to
smooth the social functioning of the group: be-
positive, be-negative.
• A “bucket” type, OTHER, for acts that do con-
vey a speaker intention, but where the intention
doesn’t fit any of the other classes.
</listItem>
<bodyText confidence="0.99731468">
For our studies into feedback in the AMI cor-
pus, the dialogue acts labelled as backchannesl are
clearly important. They were defined in the annota-
tion manual as follows.
In backchannels, someone who has just been
listening to a speaker says something in the
background, without really stopping that speaker.
[...] Some typical backchannels are “uhhuh”,
“mm-hmm”, “yeah”, “yep”, “ok”, “ah”, “huh”,
“hmm”, “mm” and, for the Scottish speakers in the
data recorded in Edinburgh, “aye”. Backchannels
can also repeat or paraphrase part or all of what
the main speaker has just said.
The labels assess and comment-about-
understanding are closely related. They were
defined as follows.
An ASSESS is any comment that expresses an
evaluation, however tentative or incomplete, of
something that the group is discussing. [...] There
are many different kinds of assessment; they include,
among other things, accepting an offer, express-
ing agreement/disagreement or any opinion about
some information that’s been given, expressing un-
certainty as to whether a suggestion is a good idea
or not, evaluating actions by members of the group,
such as drawings. [...] An ASSESS can be very
short, like “yeah” and “ok”. It is important not to
confuse this type of act with the class BACKCHAN-
NEL, where the speaker is merely expressing, in the
background, that they are following the conversa-
tion.
C-A-U is for the very specific case of comment-
ing on a previous dialogue act where the speaker in-
dicates something about whether they heard or un-
derstood what a previous speaker has said, without
doing anything more substantive. In a C-A-U, the
speaker can indicate either that they did understand
(or simply hear) what a previous speaker said, or
that they didn’t.
The Backchannel class largely conforms to Yn-
gve’s notion of backchannel and is used for the
functions of contact (Yngve, 1970). Assess is used
for the attitudinal reactions, where the speaker ex-
presses his stance towards what is said, either ac-
ceptance or rejection. Comments about understand-
ing are used for explicit signals of understanding or
non-understanding.
In addition to dialogue acts also relation between
dialogue acts are annotated. Relations are anno-
tated between two dialogue acts (a later source act
</bodyText>
<page confidence="0.995199">
18
</page>
<bodyText confidence="0.999890625">
and an earlier target act) or between a dialogue act
(the source of the relation) and some other action, in
which case the target is not specified. Relations are
a more general concept than adjacency pairs, like
question-answer. Relation have one of four types:
positive, negative, partial and uncertain, indicating
that the source expresses a positive, negative, par-
tially positive or uncertain stance of the speaker to-
wards the contents of the target of the related pair.
For example: a “yes”-answer to a question is an in-
form act that is the source of a positive relation with
the question act, which is the target of the relation.
A dialogue act that assesses some action that is not a
dialogue act, will be coded as the source of a relation
that has no (dialogue act as) target.
A part of the scenario-based meetings (14 meet-
ings) were annotated with addressee labels, i.e. an-
notators had to say who the speaker is talking to.
The addressee tag is attached to the dialogue act. If
a speaker changes his addressee (for instance, from
group to a particular participant) during a turn the
utterance should be split into two dialogue act seg-
ments, even if the type of dialogue act is the same
for both segments.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="method">
3 Yeah
</sectionHeader>
<bodyText confidence="0.999954222222222">
In this section we look at the distribution of yeah in
the AMI corpus. “yeah” utterances make up a sub-
stantial part of the dialogue acts in the AMI meeting
conversations (about 8%). If we try to tell group
addressed dialogue acts from individually addressed
acts then “yeah” is the best cue phrase for the class
of single addressed dialogue acts; cf. (Stehouwer,
2006).
In order to get information about the stance that
participants take with respect towards the issue dis-
cussed it is important to be able to tell utterances of
“yeah” as a mere backchannel, or a stall, from yeah-
utterances that express agreement with the opinion
of the speaker. The latter will more often be classi-
fied as assessments. We first look at the way anno-
tators used and confused the labels and then turn to
see in what way we can predict the assignments to
the class.
</bodyText>
<subsectionHeader confidence="0.999949">
3.1 Annotations of yeah utterances
</subsectionHeader>
<bodyText confidence="0.999985744186046">
One important feature of the dialogue act annota-
tion scheme is that the annotators had to decide what
they consider to be the segments that constitute a di-
alogue act. Annotators differ in the way they seg-
ment the transcribed speech of a speaker. Where one
annotator splits “Yeah. Maybe pear yeah or some-
thing like that.” into two segments labeling “yeah.”
as a backchannel and the rest as a suggest, an other
may not split it and consider the whole utterance as
a suggest.
In comparing how different annotators labeled
“yeah” occurrences, we compared the labels they as-
signed to the segment that starts with the occurrence
of “yeah”.
The confusion matrix for 2 annotators of 213
yeah-utterances, i.e. utterances that start with
“yeah”, is given below. It shows that backchan-
nel (38%), assess (37%) and inform (11%) are the
largest categories 1. Each of the annotators has about
80 items in the backchannel class. In about 75% of
the cases, annotators agree on the back-channel la-
bel. In either of the other cases a category deemed
a backchannel is mostly categorized as assessment
by the other and vice versa. For the assessments,
annotators agree on about slightly more than half
of the cases (43 out of 79 and 43 out of 76). The
disagreements are, for both annotators split between
the backchannels, for the larger part, the inform cat-
egory, as second largest, and the other category.
The other category subsumes the following types
of dialogue acts: summing up for both annotators:
be-positive(9), suggest(8), elicit-assess(3), elicit-
inform(2), comment-about-understanding(2). The
dialogue act type of these other labeled utterances
is mostly motivated by the utterances following
“Yeah”. Examples: “Yeah , it’s a bit difficult” is
labeled as Be-positive. “Yeah ? Was it a nice way to
create your remote control ?” is labeled as an Elicit-
Assessment .
Out of the 213 Yeah-utterances a number contains
just “yeah” without a continuation. Below, the con-
fusion matrix for the same two annotators, but now
for only those cases that have text “yeah” only. In
</bodyText>
<footnote confidence="0.975076333333333">
1As the numbers for each of the classes by both annotators
is about the same, we have permitted ourselves the license to
this sloppy way of presenting the percentages.
</footnote>
<page confidence="0.975161">
19
</page>
<table confidence="0.998323428571429">
yeah 0 1 2 3 4 SUM
0 59.0 2.0 17.0 0.0 2.0 80.0
1 0.0 9.0 4.0 2.0 2.0 17.0
2 21.0 3.0 43.0 7.0 5.0 79.0
3 2.0 0.0 7.0 13.0 4.0 26.0
4 1.0 0.0 5.0 0.0 5.0 11.0
SUM 83.0 14.0 76.0 22.0 18.0 213.0
</table>
<figureCaption confidence="0.47810325">
Figure 1: Confusion matrix of two annotations of
all Yeah utterances. labels: 0 = backchannel; 1 =
fragment or stall; 2 = assess; 3 = inform; 4 = other.
p0=0.61 (percentage agreement); kappa=0.44.
</figureCaption>
<table confidence="0.990814">
yeah-only 0 1 2 SUM
0 50.0 12.0 3.0 65.0
1 13.0 5.0 1.0 19.0
2 2.0 0.0 2.0 4.0
SUM 65.0 17.0 6.0 88.0
</table>
<figureCaption confidence="0.997630333333333">
Figure 2: labels: 0 = bc 1 = assess 2 = other
(subsuming: be-positive, fragment, comment-about-
understanding). p0=0.65; kappa=0.14
</figureCaption>
<bodyText confidence="0.999799315789474">
the comparison only those segments were taken into
account that both annotators marked as a segment
i.e. a dialogue act realized by the word “Yeah” only.2
What do these patterns in the interpretation of
“yeah” expressions tell us about its semantics? It
appears that there is a significant collection of occur-
rences that annotators agree on as being backchan-
nels. For the classes of assessments and other there
also seem to be prototypical examples that are clear
for both annotators. The confusions show that there
is a class of expressions that are either interpreted
as backchannel or assess and a class whose expres-
sions are interpreted as either assessments or some
other label. Annotators often disagree in segmenta-
tion. A segment of speech that only consist of the
word “yeah” is considered to be either a backchan-
nel or an assess, with very few exceptions. There is
more confusion between annotators than agreement
about the potential assess acts.
</bodyText>
<footnote confidence="0.870228">
2The text segment covered by the dialogue act then contains
“Yeah”, “Yeah ?”, “Yeah ,” or “Yeah .”.
</footnote>
<subsectionHeader confidence="0.998243">
3.2 Predicting the class of a yeah utterance
</subsectionHeader>
<bodyText confidence="0.999995444444444">
We derived a decision rule model for the assignment
of a dialogue act label to yeah utterances, based
on annotated meeting data. For our exploration we
used decision tree classifiers as they have the advan-
tage over other classifiers that the rules can be inter-
preted.
The data we used consisted of 1122 yeah utter-
ances from 15 meetings. Because of the relative low
inter-annotator agreement, we took meetings that
were all annotated by one and the same annotator,
because we expect that it will find better rules for
classifying the utterances when the data is not too
noisy.
There are 12786 dialogue act segments in the cor-
pus. The number of segments that start with “yeah”
is 1122, of which 861 are short utterances only con-
taining the word “yeah”. Of the 1122 yeahs 493 di-
alogue acts were annotated as related to a previous
dialogue act. 319 out of the 861 short yeah utter-
ances are related to a previous act.
The distribution of the 1122 yeah utterances over
dialogue act classes is: assess (407), stall (224),
backchannel (348), inform (95) and other (48 of
which 25 comment-about-understanding). These are
the class variables we used in the classification. The
model consists of five features. We make use of the
notion of conversational state, being an ensemble
of the speech activities of all participants. Since
we have four participants a state is a 4-tuple &lt;
a, b, c, d &gt; where a is the dialogue act performed by
participant A, etc. A conversation is in a particular
state as long as no participant stops or starts speak-
ing. Thus, a state change occurs every time when
some participants starts speaking or stops speaking,
in the sense that the dialogue act that he performs
has finished. The features that we use are:
</bodyText>
<listItem confidence="0.9981635">
• lex This feature has value 0 if the utterance con-
sists of the word Yeah only. Otherwise 1.
• continue Has value 1 when the producer of the
utterance also speaks in the next conversational
state. Otherwise 0. This feature models incipi-
ent behavior of the backchanneler.
• samespeaker Has value 1 if the conversational
state in which this utterance happens has the
</listItem>
<page confidence="0.835932">
20
</page>
<figure confidence="0.731125454545455">
Null 629.0
Assess 81.0
Inform 162.0
Elicit-Comment-Understanding 2.0
Elicit-Assessment 40.0
Elicit-Inform 73.0
Elicit-Offer-Or-Suggestion 2.0
Suggest 114.0
Comment-About-Understanding 13.0
Offer 5.0
Be-Positive 1.0
</figure>
<figureCaption confidence="0.9978365">
Figure 3: Distribution of the types of dialogue acts
that yeah utterances are responses to.
</figureCaption>
<bodyText confidence="0.99818675">
same speaker, but different from the backchan-
neler, as the next state. Otherwise 0. This fea-
ture indicates that there is another speaker that
continues speaking.
</bodyText>
<listItem confidence="0.986252333333333">
• overlap There is speaker overlap in the state
where the utterance started.
• source This involves the relation labeling of the
</listItem>
<bodyText confidence="0.949724055555555">
annotation scheme. source refers to the dia-
logue act type of the source of the relation of
the dialogue act that is realized by the Yeah ut-
terance. If the yeah dialogue act is not related
to some other act the value of this feature is
null. The possible values for this feature are:
null, assess, inform, suggest, elicitation (which
covers all elicitations), and other.
The distribution of source types of the 1122 yeah
dialogue acts is shown in table 3.2. The table shows
that 629 out of 1122 yeah utterances were not related
to some other act.
We first show the decision tree computed by
the J48-tree classifier as implemented in the weka-
toolkit, if we do not use the source feature looks as
follows. The tree shows that 392 utterances satisfy
the properties: continued = 1 and short = 1. Of these
158 are misclassified as backchannel.
</bodyText>
<figure confidence="0.8059043">
1. Continued &lt; 0
(a) lex &lt; 0: bc(392.0/158.0)
(b) lex &gt; 0: as(56.0/24.0)
2. Continue ) 0
(a) samespkr &lt; 0
i. overlap &lt; 0: st(105.0/27.0)
ii. overlap &gt; 0
A. lex &lt; 0: st(76.0/30.0)
B. lex &gt; 0: bc(16.0/6.0)
(b) samespkr &gt; 0 : ass(477.0/233.0)
</figure>
<bodyText confidence="0.999912585365854">
In this case the J48 decision tree classifier has an
accuracy of 57%. If we decide that every yeah utter-
ance is a Backchannel, the most frequent class in our
data, we would have an accuracy of 31%. If we in-
clude the source feature, so we know the type of dia-
logue act that the yeah utterance is a response to, the
accuracy of the J48 classifier raises at 80%. Figure
3.2 shows the decision tree for this classifier. The re-
sults were obtained using ten-fold cross-validation.
It is clear from these results that there is a strong
relation between the source type of a Yeah dialogue
act and the way this Yeah dialogue act should be
classified: as a backchannel or as an assess. Note
that since backchannels are never marked as target
of a relation, null as source value is a good indicator
for the Yeah act to be a backchannel or a stall.
We also tested the decision tree classifier on a test
set that consists of 4453 dialogue acts of which 539
are yeah-utterances (219 marked as related to some
source act). Of these 219 are short utterances con-
sisting only of the word “Yeah” (139 marked as re-
lated). The utterances in this test set were annotated
by other annotators than the annotator that annotated
the training set. The J48 classifier had an accuracy
on the test set of 64%. The classes which are con-
fused most are those that are also confused most by
the human annotators: backchannels and stall, and
assess and inform. One cause of the performance
drop is that in the test corpus the distribution of class
labels differs substantially from that of the training
set. In the test set yeah utterances were very rarely
labelled as stall, whereas this was a frequent label
(about 20%) in the training set. The distribution of
yeah-utterance labels in the test set is: backchannels
241, stalls 4, assessments 186, inform 66 and other
42.
When we merged the train and test meetings and
trained the J48 decision tree classifier, a 10 fold
cross-validation test showed an accuracy of 75%.
Classes that are confused most are again: backchan-
nel and stall, and assessment and inform.
</bodyText>
<page confidence="0.999319">
21
</page>
<figureCaption confidence="0.9934705">
Figure 4: Decision tree for classification of yeah utterances when information about the source of the related
dialogue act is used.
</figureCaption>
<sectionHeader confidence="0.7647975" genericHeader="method">
4 Measuring Speaker Gaze at
Backchannelors
</sectionHeader>
<bodyText confidence="0.999921">
When thinking about the interaction between
speaker and backchannelor, it seems obvious, as we
said before, that the person backchanneling feels ad-
dressed by the speaker. We were wondering whether
the backchannel was not prompted by an invitation
of a speaker, for example, by gazing at the listener.
Gaze behavior of speaker and backchannelor is
classified by means of the following gaze targets, a
sequence of focus of attention labels that indicates
where the actor is looking at during a period of time:
</bodyText>
<listItem confidence="0.933192833333333">
1. the gaze targets of the speaker in the period
starting some short time (DeltaTime) before
the start time of the backchannel act till the start
of the backchannel act.
2. the gaze targets of the backchannelor in the pe-
riod starting some short time (DeltaTime) be-
fore the start time of the backchannel act till the
start of the backchannel act.
3. the gaze targets of the speaker during the
backchannel act.
4. the gaze targets of the backchannelor during the
backchannel act.
</listItem>
<bodyText confidence="0.815615375">
We set DeltaTime at 1 sec, so we observed the gaze
behavior of the speaker in the period from one sec-
ond before the start of the backchannel act. Using
these gaze target sequences, we classified the gaze
behavior of the actor as follows:
0: the gaze before target sequence of the actor
does not contain any person
1: the before gaze target sequence of the actor
does contain a person but not the other ac-
tor involved: for the speaker this means that
he did not look at backchannelor before the
backchannel act started, for the backchannelor
this means that he did not look at the speaker
before the start of the backchannel.
2: the actor did look at the other person involved
before that backchannel act.
</bodyText>
<page confidence="0.99269">
22
</page>
<bodyText confidence="0.993325648648649">
Figure 4 show a table with counts of these classes
of events. In the 13 meetings we counted 1085
backchannel events. There were 687 events with a
single speaker of a real dialogue act. For this cases
it is clear who the backchannelor was reacting on.
This is the selected speaker. The table shows speaker
data in rows and backchannel data in columns. The
MaxDownTime is 1sec and the MinUpTime is
2 sec. The DeltaTime for the gaze period is 1sec.
From the table we can infer that:
1. The selected speaker looks at the backchan-
nelor in the period before the backchannelor act
starts in 316 out of the 687 cases.
2. The backchannelor looks at the selected
speaker in the period before the backchannelor
act starts in 430 out of the 687 cases.
3. The selected speaker looks at someone else
than the backchannelor in the period before the
backchannelor act starts in 209 out of the 687
cases.
4. The backchannelor looks at someone else than
the selected speaker in the period before the
backchannelor act starts in 54 out of the 687
cases.
5. In 254 out of the 687 cases the speaker looked
at the backchannelor and the backchannelor
looked at the speaker.
We may conclude that the speakers look more at
the backchannelor than at the other two persons to-
gether (316 against 209). The table also shows that
backchannelors look far more at the selected speaker
than at the two others (430 against 54 instances).
In order to compare gaze of speaker in backchan-
nel events, we also computed for each of the
13 meetings for each pair of participants (X, Y ):
dagaze(X, Y ): how long X looks at Y in those time
frames that X is performing a dialogue act.
</bodyText>
<equation confidence="0.9968425">
E OT (gaze(X, Y ), da(X))
dagaze(X, Y ) =
E da(X)
(1)
</equation>
<bodyText confidence="0.738939">
where summation is over all real dialogue acts
performed by X,
</bodyText>
<table confidence="0.895959">
OT (gaze(X, Y ), da(X)) is the overlap time of the
splbc 0 1 2 T
0 103 4 55 162
1 46 42 121 209
2 54 8 254 316
T 203 54 430 687
</table>
<figureCaption confidence="0.72633">
Figure 5: Gaze table of speaker and backchannelor.
</figureCaption>
<bodyText confidence="0.993190545454545">
DeltaTime = 1sec. Total number of backchannel
events is 1085. In the table only those 687 backchan-
nel events with a single speaker are considered (ex-
cluded are those instances where no speaker or more
than one speaker was performing a real dialogue act
in the period with a MinUpTime of 2 sec and a
MaxDownTime of 1 sec.). Speaker data in rows;
backchannelor data in columns. The table shows
for example that in 121 cases the speaker looked
at someone but not the backchannelor, in the period
from 1 sec before the start of the backchannel act till
the start of the backchannel act, while the backchan-
nelor looked in that period at the speaker.
two events: gaze(X, Y ): the time that X gazes at Y ,
and da(X) the time that the dialogue act performed
by X lasts. The numbers are normalized over the to-
tal duration of the dialogue acts during which gaze
behavior was measured.
Next we computed bcgaze(X, Y ): how long X
looks at Y in those time frames that X performs
a real dialogue act and the Y responds with a
backchannel act.
</bodyText>
<equation confidence="0.8933785">
E OT (gaze(X, Y ), dabc(X, Y ))
bcgaze(X, Y ) =
E da(X, Y )
(2)
</equation>
<bodyText confidence="0.99998925">
where dabc(X, Y ) is the time that X performs
the dialogue act that Y reacts on by a backchannel.
Here normalization is with the sum of the lengths
of all dialogue acts performed by X that elicited a
backchannel act by Y .
Analysis of pairs of values gaze(X, Y ) and
bcgaze(X, Y ) shows that in a situation where some-
one performs a backchannel the speaker looks
significantly more at the backchannelor than the
speaker looks at the same person in general when
the speaker is performing a dialogue act (t = 8.66,
df = 101, p &lt; 0.0001). The mean values are 0.33
</bodyText>
<page confidence="0.991416">
23
</page>
<bodyText confidence="0.999713857142857">
and 0.16.3
Perhaps we can use the information on gaze of the
participants in the short period before the backchan-
nel act as features for predicting who the backchan-
nel actor is. For the 687 data points of backchannel
events with a single speaker, we used gaze of partici-
pants, the speaker and the duration of the backchan-
nel act as features. Using a decision tree classifier
we obtained an accuracy of 51% in predicting who
will perform a backchannel act (given that someone
will do that). Note that there are three possible ac-
tors (the speaker is given). This score is 16% above
the a priori likelihood of the most likely participant:
A (36%).
</bodyText>
<sectionHeader confidence="0.951277" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999973705882353">
In this paper, we have explored some questions
about the possible use and function of backchan-
nels in multiparty interactions. On the one hand
backchannels can be informative about functions re-
lated to floor and participation: who is talking to
whom. Obviously, a person producing a backchan-
nel was responding to an utterance of speaker.
For the semantic analysis of meeting data an im-
portant question is whether he was just using the
backchannel as a continuer (a sign of attention) or
as an assessment. We also checked our intuition
that backchannels in the kinds of meetings that we
are looking at might often be invited by speakers
through gaze. Obviously, these investigations just
scratch the service of how backchannels work in
conversations and how we can use them to uncover
information from recorded conversations.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998992256410256">
H. H. Clark and T. B. Carlson. 1992. Hearers and speech
acts. In Herbert H. Clark, editor, Arenas of Language
Use, pages 205–247. University of Chicago Press and
CSLI.
Erving Goffman. 1981. Footing. In Erving Goffman,
editor, Forms of Talk, pages 124–159. University of
Pennsylvania Press, Philadelphia, PA.
Stephen C. Levinson. 1988. Putting linguistics on a
proper footing: explorations in goffman’s concept of
3For 13 meeting and 4 participants we would have 156 pairs
of values. We only used those 102 pairs of which both values
are non-zero.
participation. In Paul Drew and Anthony Wootton, ed-
itors, Erving Goffman. Exploring the Interaction Or-
der, pages 161–227. Polity Press, Cambridge.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M.Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, W. Post, D. Reidsma, and P. Wellner.
2005. The ami meeting corpus. In Measuring Be-
haviour, Proceedings of 5th International Conference
on Methods and Techniques in Behavioral Research.
Emanuel A. Schegloff. 1981. Discourse as an interac-
tional achievement: some uses of ’uh huh’ and other
things that come between sentences. In Deborah Tan-
nen, editor, Analyzing Discourse: Text and Talk, pages
71–93. Georgetown University Press, Washington.
Emanuel A. Schegloff. 1988. Goffman and the analysis
of conversation. In Paul Drew and Anthony Wootton,
editors, Erving Goffman. Exploring the Interaction Or-
der, pages 89–135. Polity Press, Cambridge.
J.H. Stehouwer. 2006. Cue-phrase selection methods
for textual classification problems. Technical report,
M.Sc. Thesis, Twente University, Human Media Inter-
action, Enschede, the Netherlands.
V.H. Yngve. 1970. On getting a word in edgewise. In
Papers from the sixth regional meeting of the Chicago
Linguistic Society, pages 567–77, Chicago: Chicago
Linguistic Society.
</reference>
<page confidence="0.999168">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782376">
<title confidence="0.999932">Computing Backchannel Distributions in Multi-Party Conversations</title>
<author confidence="0.999441">Dirk Heylen Rieks op den_Akker</author>
<affiliation confidence="0.9854375">Human Media Interaction Human Media Interaction University of Twente University of Twente</affiliation>
<email confidence="0.826906">heylen@cs.utwente.nlinfrieks@cs.utwente.nl</email>
<abstract confidence="0.9970884">In multi-party conversations it may not always be obvious who is talking to whom. Backchannels may provide a partial answer to this question, possibly in combination with some other events, such as gaze behaviors of the interlocutors. We look at some patterns in multi-party interaction relating features of backchannel behaviours to aspects of the partipation framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>T B Carlson</author>
</authors>
<title>Hearers and speech acts.</title>
<date>1992</date>
<booktitle>Arenas of Language Use,</booktitle>
<pages>205--247</pages>
<editor>In Herbert H. Clark, editor,</editor>
<publisher>University of Chicago Press and CSLI.</publisher>
<contexts>
<context position="2321" citStr="Clark and Carlson, 1992" startWordPosition="362" endWordPosition="365">ho was talk17 ing to whom) and about the way utterances are being assessed by their audience. The qualifier “in some way or another” with respect to feeling or being addressed is particularly important in the context of multi-party dialogues (i.e. dialogues with more than two persons present). Typically, an utterance by a speaker instantiates the performance of a speech act with a particular illocutionary and perlocutionary force. The speech act involves a request for uptake. However, as has been pointed out several times (Goffman (Goffman, 1981), Levinson (Levinson, 1988), Clark and Carlson (Clark and Carlson, 1992), Schegloff (Schegloff, 1988)), participants in a multi-party conversation can have a different role or status and they can be addressed in different ways. In this paper we report on some of our investigations into the distribution of backchannels in multiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations of utterances consisting of starting with “yeah” and try to see whether </context>
</contexts>
<marker>Clark, Carlson, 1992</marker>
<rawString>H. H. Clark and T. B. Carlson. 1992. Hearers and speech acts. In Herbert H. Clark, editor, Arenas of Language Use, pages 205–247. University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erving Goffman</author>
</authors>
<date>1981</date>
<booktitle>Forms of Talk,</booktitle>
<pages>124--159</pages>
<editor>Footing. In Erving Goffman, editor,</editor>
<publisher>University of Pennsylvania Press,</publisher>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2249" citStr="Goffman, 1981" startWordPosition="353" endWordPosition="354">something about the participation framework on the one hand (who was talk17 ing to whom) and about the way utterances are being assessed by their audience. The qualifier “in some way or another” with respect to feeling or being addressed is particularly important in the context of multi-party dialogues (i.e. dialogues with more than two persons present). Typically, an utterance by a speaker instantiates the performance of a speech act with a particular illocutionary and perlocutionary force. The speech act involves a request for uptake. However, as has been pointed out several times (Goffman (Goffman, 1981), Levinson (Levinson, 1988), Clark and Carlson (Clark and Carlson, 1992), Schegloff (Schegloff, 1988)), participants in a multi-party conversation can have a different role or status and they can be addressed in different ways. In this paper we report on some of our investigations into the distribution of backchannels in multiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations </context>
</contexts>
<marker>Goffman, 1981</marker>
<rawString>Erving Goffman. 1981. Footing. In Erving Goffman, editor, Forms of Talk, pages 124–159. University of Pennsylvania Press, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen C Levinson</author>
</authors>
<title>Putting linguistics on a proper footing: explorations in goffman’s concept of 3For 13 meeting and 4 participants we would have 156 pairs of values. We only used those 102 pairs of which both values are non-zero.</title>
<date>1988</date>
<contexts>
<context position="2276" citStr="Levinson, 1988" startWordPosition="356" endWordPosition="357">ipation framework on the one hand (who was talk17 ing to whom) and about the way utterances are being assessed by their audience. The qualifier “in some way or another” with respect to feeling or being addressed is particularly important in the context of multi-party dialogues (i.e. dialogues with more than two persons present). Typically, an utterance by a speaker instantiates the performance of a speech act with a particular illocutionary and perlocutionary force. The speech act involves a request for uptake. However, as has been pointed out several times (Goffman (Goffman, 1981), Levinson (Levinson, 1988), Clark and Carlson (Clark and Carlson, 1992), Schegloff (Schegloff, 1988)), participants in a multi-party conversation can have a different role or status and they can be addressed in different ways. In this paper we report on some of our investigations into the distribution of backchannels in multiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations of utterances consisting of</context>
</contexts>
<marker>Levinson, 1988</marker>
<rawString>Stephen C. Levinson. 1988. Putting linguistics on a proper footing: explorations in goffman’s concept of 3For 13 meeting and 4 participants we would have 156 pairs of values. We only used those 102 pairs of which both values are non-zero.</rawString>
</citation>
<citation valid="false">
<booktitle>Erving Goffman. Exploring the Interaction Order,</booktitle>
<pages>161--227</pages>
<editor>participation. In Paul Drew and Anthony Wootton, editors,</editor>
<publisher>Polity Press,</publisher>
<location>Cambridge.</location>
<marker></marker>
<rawString>participation. In Paul Drew and Anthony Wootton, editors, Erving Goffman. Exploring the Interaction Order, pages 161–227. Polity Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McCowan</author>
<author>J Carletta</author>
<author>W Kraaij</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>G Lathoud M Kronenthal</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The ami meeting corpus.</title>
<date>2005</date>
<booktitle>In Measuring Behaviour, Proceedings of 5th International Conference on Methods and Techniques in Behavioral Research.</booktitle>
<contexts>
<context position="3195" citStr="McCowan et al., 2005" startWordPosition="506" endWordPosition="509">ltiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations of utterances consisting of starting with “yeah” and try to see whether we can classify these utterances as continuers, i.e. neutral with respect to stance taking (Schegloff, 1981), or as assessments. 2 Corpus The argumentative discourses that we are studying are part of the meeting corpus collected during the AMI project (McCowan et al., 2005). From a computational, technological perspective, the aims Proceedings of the Workshop on Embodied Language Processing, pages 17–24, Prague, Czech Republic, June 28, 2007. c�2007 Association for Computational Linguistics of this research is directed at developing automatic procedures that can help to provide answers to any query users may have about what goes on in the meetings. The AMI corpus consists of meetings in which a group of four people discuss the design of a new remote control. T The kinds of queries that we would like our procedures to be able to answer are related to these moves:</context>
</contexts>
<marker>McCowan, Carletta, Kraaij, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kronenthal, Lincoln, Lisowska, Post, Reidsma, Wellner, 2005</marker>
<rawString>I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M.Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and P. Wellner. 2005. The ami meeting corpus. In Measuring Behaviour, Proceedings of 5th International Conference on Methods and Techniques in Behavioral Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Discourse as an interactional achievement: some uses of ’uh huh’ and other things that come between sentences.</title>
<date>1981</date>
<booktitle>Analyzing Discourse: Text and Talk,</booktitle>
<pages>71--93</pages>
<editor>In Deborah Tannen, editor,</editor>
<publisher>Georgetown University Press,</publisher>
<location>Washington.</location>
<contexts>
<context position="3029" citStr="Schegloff, 1981" startWordPosition="480" endWordPosition="481">ent role or status and they can be addressed in different ways. In this paper we report on some of our investigations into the distribution of backchannels in multiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations of utterances consisting of starting with “yeah” and try to see whether we can classify these utterances as continuers, i.e. neutral with respect to stance taking (Schegloff, 1981), or as assessments. 2 Corpus The argumentative discourses that we are studying are part of the meeting corpus collected during the AMI project (McCowan et al., 2005). From a computational, technological perspective, the aims Proceedings of the Workshop on Embodied Language Processing, pages 17–24, Prague, Czech Republic, June 28, 2007. c�2007 Association for Computational Linguistics of this research is directed at developing automatic procedures that can help to provide answers to any query users may have about what goes on in the meetings. The AMI corpus consists of meetings in which a grou</context>
</contexts>
<marker>Schegloff, 1981</marker>
<rawString>Emanuel A. Schegloff. 1981. Discourse as an interactional achievement: some uses of ’uh huh’ and other things that come between sentences. In Deborah Tannen, editor, Analyzing Discourse: Text and Talk, pages 71–93. Georgetown University Press, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Goffman and the analysis of conversation.</title>
<date>1988</date>
<booktitle>Exploring the Interaction Order,</booktitle>
<pages>89--135</pages>
<editor>In Paul Drew and Anthony Wootton, editors, Erving Goffman.</editor>
<publisher>Polity Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2350" citStr="Schegloff, 1988" startWordPosition="367" endWordPosition="368">the way utterances are being assessed by their audience. The qualifier “in some way or another” with respect to feeling or being addressed is particularly important in the context of multi-party dialogues (i.e. dialogues with more than two persons present). Typically, an utterance by a speaker instantiates the performance of a speech act with a particular illocutionary and perlocutionary force. The speech act involves a request for uptake. However, as has been pointed out several times (Goffman (Goffman, 1981), Levinson (Levinson, 1988), Clark and Carlson (Clark and Carlson, 1992), Schegloff (Schegloff, 1988)), participants in a multi-party conversation can have a different role or status and they can be addressed in different ways. In this paper we report on some of our investigations into the distribution of backchannels in multiparty interactions (for instance in relation to other phenomena such as gaze) and how this information can help us to uncover certain features of floor and stance taking automatically. We will first describe the corpus and the annotations. Next we look at the annotations of utterances consisting of starting with “yeah” and try to see whether we can classify these utteran</context>
</contexts>
<marker>Schegloff, 1988</marker>
<rawString>Emanuel A. Schegloff. 1988. Goffman and the analysis of conversation. In Paul Drew and Anthony Wootton, editors, Erving Goffman. Exploring the Interaction Order, pages 89–135. Polity Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Stehouwer</author>
</authors>
<title>Cue-phrase selection methods for textual classification problems.</title>
<date>2006</date>
<tech>Technical report, M.Sc. Thesis,</tech>
<institution>Twente University, Human Media Interaction,</institution>
<location>Enschede, the Netherlands.</location>
<contexts>
<context position="9092" citStr="Stehouwer, 2006" startWordPosition="1487" endWordPosition="1488">logue act. If a speaker changes his addressee (for instance, from group to a particular participant) during a turn the utterance should be split into two dialogue act segments, even if the type of dialogue act is the same for both segments. 3 Yeah In this section we look at the distribution of yeah in the AMI corpus. “yeah” utterances make up a substantial part of the dialogue acts in the AMI meeting conversations (about 8%). If we try to tell group addressed dialogue acts from individually addressed acts then “yeah” is the best cue phrase for the class of single addressed dialogue acts; cf. (Stehouwer, 2006). In order to get information about the stance that participants take with respect towards the issue discussed it is important to be able to tell utterances of “yeah” as a mere backchannel, or a stall, from yeahutterances that express agreement with the opinion of the speaker. The latter will more often be classified as assessments. We first look at the way annotators used and confused the labels and then turn to see in what way we can predict the assignments to the class. 3.1 Annotations of yeah utterances One important feature of the dialogue act annotation scheme is that the annotators had </context>
</contexts>
<marker>Stehouwer, 2006</marker>
<rawString>J.H. Stehouwer. 2006. Cue-phrase selection methods for textual classification problems. Technical report, M.Sc. Thesis, Twente University, Human Media Interaction, Enschede, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H Yngve</author>
</authors>
<title>On getting a word in edgewise.</title>
<date>1970</date>
<booktitle>In Papers from the sixth regional meeting of the Chicago Linguistic Society,</booktitle>
<pages>567--77</pages>
<publisher>Chicago Linguistic Society.</publisher>
<location>Chicago:</location>
<contexts>
<context position="7112" citStr="Yngve, 1970" startWordPosition="1147" endWordPosition="1148">class BACKCHANNEL, where the speaker is merely expressing, in the background, that they are following the conversation. C-A-U is for the very specific case of commenting on a previous dialogue act where the speaker indicates something about whether they heard or understood what a previous speaker has said, without doing anything more substantive. In a C-A-U, the speaker can indicate either that they did understand (or simply hear) what a previous speaker said, or that they didn’t. The Backchannel class largely conforms to Yngve’s notion of backchannel and is used for the functions of contact (Yngve, 1970). Assess is used for the attitudinal reactions, where the speaker expresses his stance towards what is said, either acceptance or rejection. Comments about understanding are used for explicit signals of understanding or non-understanding. In addition to dialogue acts also relation between dialogue acts are annotated. Relations are annotated between two dialogue acts (a later source act 18 and an earlier target act) or between a dialogue act (the source of the relation) and some other action, in which case the target is not specified. Relations are a more general concept than adjacency pairs, l</context>
</contexts>
<marker>Yngve, 1970</marker>
<rawString>V.H. Yngve. 1970. On getting a word in edgewise. In Papers from the sixth regional meeting of the Chicago Linguistic Society, pages 567–77, Chicago: Chicago Linguistic Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>