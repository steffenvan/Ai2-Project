<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010838">
<title confidence="0.974514">
Extending the Entity Grid with Entity-Specific Features
</title>
<author confidence="0.998506">
Micha Elsner Eugene Charniak
</author>
<affiliation confidence="0.9978505">
School of Informatics Department of Computer Science
University of Edinburgh Brown University, Providence, RI 02912
</affiliation>
<email confidence="0.996006">
melsner0@gmail.com ec@cs.brown.edu
</email>
<sectionHeader confidence="0.983003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999835">
We extend the popular entity grid representa-
tion for local coherence modeling. The grid
abstracts away information about the entities it
models; we add discourse prominence, named
entity type and coreference features to distin-
guish between important and unimportant en-
tities. We improve the best result for WSJ doc-
ument discrimination by 6%.
</bodyText>
<sectionHeader confidence="0.995513" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984907756756757">
A well-written document is coherent (Halliday and
Hasan, 1976)— it structures information so that each
new piece of information is interpretable given the
preceding context. Models that distinguish coherent
from incoherent documents are widely used in gen-
eration, summarization and text evaluation.
Among the most popular models of coherence is
the entity grid (Barzilay and Lapata, 2008), a sta-
tistical model based on Centering Theory (Grosz et
al., 1995). The grid models the way texts focus
on important entities, assigning them repeatedly to
prominent syntactic roles. While the grid has been
successful in a variety of applications, it is still a
surprisingly unsophisticated model, and there have
been few direct improvements to its simple feature
set. We present an extension to the entity grid which
distinguishes between different types of entity, re-
sulting in significant gains in performance1.
At its core, the grid model works by predicting
whether an entity will appear in the next sentence
✶A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
(and what syntactic role it will have) given its his-
tory of occurrences in the previous sentences. For
instance, it estimates the probability that &amp;quot;Clinton&amp;quot;
will be the subject of sentence 2, given that it was
the subject of sentence 1. The standard grid model
uses no information about the entity itself— the prob-
ability is the same whether the entity under discus-
sion is &amp;quot;Hillary Clinton&amp;quot; or &amp;quot;wheat&amp;quot;. Plainly, this
assumption is too strong. Distinguishing important
from unimportant entity types is important in coref-
erence (Haghighi and Klein, 2010) and summariza-
tion (Nenkova et al., 2005); our model applies the
same insight to the entity grid, by adding informa-
tion from syntax, a named-entity tagger and statis-
tics from an external coreference corpus.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999769823529412">
Since its initial appearance (Lapata and Barzilay,
2005; Barzilay and Lapata, 2005), the entity grid
has been used to perform wide variety of tasks. In
addition to its first proposed application, sentence
ordering for multidocument summarization, it has
proven useful for story generation (McIntyre and
Lapata, 2010), readability prediction (Pitler et al.,
2010; Barzilay and Lapata, 2008) and essay scor-
ing (Burstein et al., 2010). It also remains a criti-
cal component in state-of-the-art sentence ordering
models (Soricut and Marcu, 2006; Elsner and Char-
niak, 2008), which typically combine it with other
independently-trained models.
There have been few attempts to improve the en-
tity grid directly by altering its feature representa-
tion. Filippova and Strube (2007) incorporate se-
mantic relatedness, but find no significant improve-
</bodyText>
<page confidence="0.98646">
125
</page>
<note confidence="0.8234335">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.682279875">
1 [Visual meteorological conditions]S prevailed for [the
personal cross country flight for which [a VFR flight
plan]o was filed]X .
2 [The flight]S originated at [Nuevo Laredo, Mexico]X ,
at [approximately 1300]X.
s conditions plan flight laredo
1 S O X -
2 - - S X
</table>
<figureCaption confidence="0.999511333333333">
Figure 1: A short text (using NP-only mention detection),
and its corresponding entity grid. The numeric token
1300 is removed in preprocessing.
</figureCaption>
<bodyText confidence="0.9987276">
ment over the original model. Cheung and Penn
(2010) adapt the grid to German, where focused con-
stituents are indicated by sentence position rather
than syntactic role. The best entity grid for English
text, however, is still the original.
</bodyText>
<sectionHeader confidence="0.997134" genericHeader="method">
3 Entity grids
</sectionHeader>
<bodyText confidence="0.999952384615385">
The entity grid represents a document as a matrix
(Figure 1) with a row for each sentence and a column
for each entity. The entry for (sentence i, entity j),
which we write rij, represents the syntactic role that
entity takes on in that sentence: subject (S), object
(O), or some other role (X)2. In addition, there is a
special marker (-) for entities which do not appear at
all in a given sentence.
To construct a grid, we must first decide which
textual units are to be considered &amp;quot;entities&amp;quot;, and how
the different mentions of an entity are to be linked.
We follow the -COREFERENCE setting from Barzi-
lay and Lapata (2005) and perform heuristic coref-
erence resolution by linking mentions which share a
head noun. Although some versions of the grid use
an automatic coreference resolver, this often fails
to improve results; in Barzilay and Lapata (2005),
coreference improves results in only one of their tar-
get domains, and actually hurts for readability pre-
diction. Their results, moreover, rely on running
coreference on the document in its original order; in
a summarization task, the correct order is not known,
which will cause even more resolver errors.
To build a model based on the grid, we treat the
columns (entities) as independent, and look at lo-
cal transitions between sentences. We model the
</bodyText>
<footnote confidence="0.686587">
2Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005).
</footnote>
<bodyText confidence="0.999969666666667">
transitions using the generative approach given in
Lapata and Barzilay (2005)3, in which the model
estimates the probability of an entity&apos;s role in the
next sentence, rij, given its history in the previ-
ous two sentences, ri_1j, ri_2j. It also uses a sin-
gle entity-specific feature, salience, determined by
counting the total number of times the entity is men-
tioned in the document. We denote this feature vec-
tor Fi;j. For example, the vector for &amp;quot;flight&amp;quot; after the
last sentence of the example would be F3;flight =
❤X, S, sal = 2✐. Using two sentences of context
and capping salience at 4, there are only 64 possi-
ble vectors, so we can learn an independent multino-
mial distribution for each F. However, the number
of vectors grows exponentially as we add features.
</bodyText>
<sectionHeader confidence="0.990373" genericHeader="method">
4 Experimental design
</sectionHeader>
<bodyText confidence="0.999992777777778">
We test our model on two experimental tasks, both
testing its ability to distinguish between correct
and incorrect orderings for wSi articles. In doc-
ument discrimination (Barzilay and Lapata, 2005),
we compare a document to a random permutation of
its sentences, scoring the system correct if it prefers
the original ordering4.
We also evaluate on the more difficult task of sen-
tence insertion (Chen et al., 2007; Elsner and Char-
niak, 2008). In this task, we remove each sentence
from the article and test whether the model prefers to
re-insert it at its original location. We report the av-
erage proportion of correct insertions per document.
As in Elsner and Charniak (2008), we test on sec-
tions 14-24 of the Penn Treebank, for 1004 test doc-
uments. We test significance using the Wilcoxon
Sign-rank test, which detects significant differences
in the medians of two distributions5.
</bodyText>
<sectionHeader confidence="0.948233" genericHeader="method">
5 Mention detection
</sectionHeader>
<bodyText confidence="0.99996025">
Our main contribution is to extend the entity grid
by adding a large number of entity-specific features.
Before doing so, however, we add non-head nouns
to the grid. Doing so gives our feature-based model
</bodyText>
<footnote confidence="0.940863714285714">
3Barzilay and Lapata (2005) give a discriminative model,
which relies on the same feature set as discussed here.
4As in previous work, we use 20 random permutations of
each document. Since the original and permutation might tie,
we report both accuracy and balanced F-score.
5Our reported scores are means, but to test significance of
differences in means, we would need to use a parametric test.
</footnote>
<page confidence="0.988606">
126
</page>
<table confidence="0.9994855">
Disc. Acc Disc. F Ins.
Random 50.0 50.0 12.6
Grid: NPs 74.4 76.2 21.3
Grid: all nouns② 77.8 79.7 23.5
</table>
<tableCaption confidence="0.96980575">
Table 1: Discrimination scores for entity grids with dif-
ferent mention detectors on WSJ development documents.
② indicates performance on both tasks is significantly dif-
ferent from the previous row of the table with p=.05.
</tableCaption>
<bodyText confidence="0.999632166666667">
more information to work with, but is beneficial
even to the standard entity grid.
We alter our mention detector to add all nouns
in the document to the grid6, even those which do
not head NPs. This enables the model to pick up
premodifiers in phrases like &amp;quot;a Bush spokesman&amp;quot;,
which do not head NPs in the Penn Treebank. Find-
ing these is also necessary to maximize coreference
recall (Elsner and Charniak, 2010). We give non-
head mentions the role X. The results of this change
are shown in Table 1; discrimination performance
increases about 4%, from 76% to 80%.
</bodyText>
<sectionHeader confidence="0.998584" genericHeader="method">
6 Entity-specific features
</sectionHeader>
<bodyText confidence="0.974843">
As we mentioned earlier, the standard grid model
does not distinguish between different types of en-
tity. Given the same history and salience, the same
probabilities are assigned to occurrences of &amp;quot;Hillary
Clinton&amp;quot;, &amp;quot;the airlines&amp;quot;, or &amp;quot;May 25th&amp;quot;, even though
we know a priori that a document is more likely to
be about Hillary Clinton than it is to be about May
25th. This problem is exacerbated by our same-head
coreference heuristic, which sometimes creates spu-
rious entities by lumping together mentions headed
by nouns like &amp;quot;miles&amp;quot; or &amp;quot;dollars&amp;quot;. In this section,
we add features that separate important entities from
less important or spurious ones.
Proper Does the entity have a proper mention?
Named entity The majority OPENNLP Morton et
al. (2005) named entity label for the coreferen-
tial chain.
Modifiers The total number of modifiers in all men-
tions in the chain, bucketed by 5s.
Singular Does the entity have a singular mention?
</bodyText>
<footnote confidence="0.634566666666667">
6Barzilay and Lapata (2008) uses NPs as mentions; we are
unsure whether all other implementations do the same, but we
believe we are the first to make the distinction explicit.
</footnote>
<bodyText confidence="0.999938674418605">
News articles are likely to be about people and
organizations, so we expect these named entity tags,
and proper NPs in general, to be more important to
the discourse. Entities with many modifiers through-
out the document are also likely to be important,
since this implies that the writer wishes to point
out more information about them. Finally, singular
nouns are less likely to be generic.
We also add some features to pick out entities
that are likely to be spurious or unimportant. These
features depend on in-domain coreference data, but
they do not require us to run a coreference resolver
on the target document itself. This avoids the prob-
lem that coreference resolvers do not work well for
disordered or automatically produced text such as
multidocument summary sentences, and also avoids
the computational cost associated with coreference
resolution.
Linkable Was the head word of the entity ever
marked as coreferring in MUC6?
Unlinkable Did the head word of the entity occur 5
times in MUC6 and never corefer?
Has pronouns Were there 5 or more pronouns
coreferent with the head word of the entity in
the NANC corpus? (Pronouns in NANC are
automatically resolved using an unsupervised
model (Charniak and Elsner, 2009).)
No pronouns Did the head word of the entity occur
over 50 times in NANC, and have fewer than 5
coreferent pronouns?
To learn probabilities based on these features,
we model the conditional probability p(ri;j❥F) us-
ing multilabel logistic regression. Our model has
a parameter for each combination of syntactic role
r, entity-specific feature h and feature vector F:
r✂h✂F. This allows the old and new features to in-
teract while keeping the parameter space tractable7.
In Table 2, we examine the changes in our esti-
mated probability in one particular context: an entity
with salience 3 which appeared in a non-emphatic
role in the previous sentence. The standard entity
grid estimates that such an entity will be the sub-
ject of the next sentence with a probability of about
</bodyText>
<footnote confidence="0.66754475">
7We train the regressor using OWLQN (Andrew and Gao,
2007), modified and distributed by Mark Johnson as part of
the Charniak-Johnson parse reranker (Charniak and Johnson,
2005).
</footnote>
<page confidence="0.983091">
127
</page>
<figure confidence="0.958235857142857">
P(next role is subj)
.045
.013
.025
.037
.133
.006
</figure>
<table confidence="0.982634866666667">
Random Disc. Acc Disc. F Ins.
Elsner+Charniak 50.00 50.00 12.6
79.6 81.0 23.0
Grid 79.5 80.9 21.4
Extended Grid 84.O② 84.5 24.2
Grid+combo 82.6 84.0 24.3
ExtEGrid+combo 86.O② 86.5 26.7②
Context
Standard egrid
Head coref in MUC6
...and proper noun
...and NE type person
...and 5 modifiers overall
Never coref in MUC6
...and NE type date .001
</table>
<tableCaption confidence="0.993702666666667">
Table 2: Probability of an entity appearing as subject of
the next sentence, given the history - X, salience 3, and
various entity-specific features.
</tableCaption>
<bodyText confidence="0.999838083333333">
.04. For most classes of entity, we can see that this
is an overestimate; for an entity described by a com-
mon noun (such as &amp;quot;the airline&amp;quot;), the probability as-
signed by the extended grid model is .01. If we
suspect (based on MUC6 evidence) that the noun
is not coreferent, the probability drops to .006 (&amp;quot;an
increase&amp;quot;) if itis a date, it falls even further, to .001.
However, given that the entity refers to a person, and
some of its mentions are modified, suggesting the ar-
ticle gives a title or description (&amp;quot;Obama&apos;s Secretary
of State, Hillary Clinton&amp;quot;), the chance that it will be
the subject of the next sentence more than triples.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.934317047619048">
Table 3 gives results for the extended grid model
on the test set. This model is significantly better
than the standard grid on discrimination (84% ver-
sus 80%) and has a higher mean score on insertion
(24% versus 21%)8.
The best WSJ results in previous work are those of
Elsner and Charniak (2008), who combine the entity
grid with models based on pronoun coreference and
discourse-new NP detection. We report their scores
in the table. This comparison is unfair, however,
because the improvements from adding non-head
nouns improve our baseline grid sufficiently to equal
their discrimination result. State-of-the-art results
on a different corpus and task were achieved by Sori-
cut and Marcu (2006) using a log-linear mixture of
an entity grid, IBM translation models, and a word-
correspondence model based on Lapata (2003).
✽For insertion using the model on its own, the median
changes less than the mean, and the change in median score is
not significant. However, using the combined model, the change
is significant.
</bodyText>
<tableCaption confidence="0.9513802">
Table 3: Extended entity grid and combination model
performance on 1004 WSJ test documents. Combination
models incorporate pronoun coreference, discourse-new
NP detection, and IBM model 1. ②indicates an extended
model score better than its baseline counterpart at p=.05.
</tableCaption>
<bodyText confidence="0.99998">
To perform a fair comparison of our extended
grid with these model-combining approaches, we
train our own combined model incorporating an en-
tity grid, pronouns, discourse-newness and the IBM
model. We combine models using a log-linear mix-
ture as in Soricut and Marcu (2006), training the
weights to maximize discrimination accuracy.
The second section of Table 3 shows these model
combination results. Notably, our extended entity
grid on its own is essentially just as good as the com-
bined model, which represents our implementation
of the previous state of the art. When we incorpo-
rate it into a combination, the performance increase
remains, and is significant for both tasks (disc. 86%
versus 83%, ins. 27% versus 24%). Though the im-
provement is not perfectly additive, a good deal of
it is retained, demonstrating that our additions to the
entity grid are mostly orthogonal to previously de-
scribed models. These results are the best reported
for sentence ordering of English news articles.
</bodyText>
<sectionHeader confidence="0.998259" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999911909090909">
We improve a widely used model of local discourse
coherence. Our extensions to the feature set involve
distinguishing simple properties of entities, such as
their named entity type, which are also useful in
coreference and summarization tasks. Although our
method uses coreference information, it does not re-
quire coreference resolution to be run on the target
documents. Given the popularity of entity grid mod-
els for practical applications, we hope our model&apos;s
improvements will transfer to summarization, gen-
eration and readability prediction.
</bodyText>
<page confidence="0.997414">
128
</page>
<sectionHeader confidence="0.997572" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997525">
We are most grateful to Regina Barzilay, Mark John-
son and three anonymous reviewers. This work was
funded by a Google Fellowship for Natural Lan-
guage Processing.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662217391304">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML &apos;07.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL&apos;05).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Computa-
tional Linguistics, 34(1):1-34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 681-684, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173-180.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 186-195, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
08: HLT, Short Papers, pages 41-44, Columbus, Ohio,
June. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2010. The same-
head heuristic for coreference. In Proceedings of ACL
10, Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139-142, Saarbrucken, Germany, June. DFKI
GmbH. Document D-07-01.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203-225.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385-393, Los An-
geles, California, June. Association for Computational
Linguistics.
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman, London.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085-1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Neil McIntyre and Mirella Lapata. 2010. Plot induction
and evolutionary search for story generation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1562-1572,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
Ani Nenkova, Advaith Siddharthan, and Kathleen McK-
eown. 2005. Automatically learning cognitive status
for multi-document summarization of newswire. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 241-248, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544-554, Uppsala, Sweden, July.
Association for Computational Linguistics.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
</reference>
<page confidence="0.998566">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789078">
<title confidence="0.999887">Extending the Entity Grid with Entity-Specific Features</title>
<author confidence="0.999785">Micha Elsner Eugene Charniak</author>
<affiliation confidence="0.969347">School of Informatics Department of Computer Science University of Edinburgh Brown University, Providence, RI 02912</affiliation>
<email confidence="0.997538">melsner0@gmail.comec@cs.brown.edu</email>
<abstract confidence="0.981819444444444">We extend the popular entity grid representation for local coherence modeling. The grid abstracts away information about the entities it models; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant en- We improve the best result for document discrimination by 6%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML &apos;07.</booktitle>
<contexts>
<context position="11977" citStr="Andrew and Gao, 2007" startWordPosition="1939" endWordPosition="1942">) using multilabel logistic regression. Our model has a parameter for each combination of syntactic role r, entity-specific feature h and feature vector F: r✂h✂F. This allows the old and new features to interact while keeping the parameter space tractable7. In Table 2, we examine the changes in our estimated probability in one particular context: an entity with salience 3 which appeared in a non-emphatic role in the previous sentence. The standard entity grid estimates that such an entity will be the subject of the next sentence with a probability of about 7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005). 127 P(next role is subj) .045 .013 .025 .037 .133 .006 Random Disc. Acc Disc. F Ins. Elsner+Charniak 50.00 50.00 12.6 79.6 81.0 23.0 Grid 79.5 80.9 21.4 Extended Grid 84.O 84.5 24.2 Grid+combo 82.6 84.0 24.3 ExtEGrid+combo 86.O 86.5 26.7 Context Standard egrid Head coref in MUC6 ...and proper noun ...and NE type person ...and 5 modifiers overall Never coref in MUC6 ...and NE type date .001 Table 2: Probability of an entity appearing as subject of the next sentence, given t</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In ICML &apos;07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05).</booktitle>
<contexts>
<context position="2552" citStr="Barzilay and Lapata, 2005" startWordPosition="383" endWordPosition="386">ce 1. The standard grid model uses no information about the entity itself— the probability is the same whether the entity under discussion is &amp;quot;Hillary Clinton&amp;quot; or &amp;quot;wheat&amp;quot;. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the en</context>
<context position="4801" citStr="Barzilay and Lapata (2005)" startWordPosition="744" endWordPosition="748">s The entity grid represents a document as a matrix (Figure 1) with a row for each sentence and a column for each entity. The entry for (sentence i, entity j), which we write rij, represents the syntactic role that entity takes on in that sentence: subject (S), object (O), or some other role (X)2. In addition, there is a special marker (-) for entities which do not appear at all in a given sentence. To construct a grid, we must first decide which textual units are to be considered &amp;quot;entities&amp;quot;, and how the different mentions of an entity are to be linked. We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun. Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction. Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors. To build a model based on the grid, we treat the columns (ent</context>
<context position="6574" citStr="Barzilay and Lapata, 2005" startWordPosition="1036" endWordPosition="1039">tioned in the document. We denote this feature vector Fi;j. For example, the vector for &amp;quot;flight&amp;quot; after the last sentence of the example would be F3;flight = ❤X, S, sal = 2✐. Using two sentences of context and capping salience at 4, there are only 64 possible vectors, so we can learn an independent multinomial distribution for each F. However, the number of vectors grows exponentially as we add features. 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4. We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the average proportion of correct insertions per document. As in Elsner and Charniak (2008), we test on sections 14-24 of the Penn Treebank, for 1004 test documents. We test significance using the Wilcoxon Sign-ra</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--1</pages>
<contexts>
<context position="990" citStr="Barzilay and Lapata, 2008" startWordPosition="137" endWordPosition="140">t the entities it models; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant entities. We improve the best result for WSJ document discrimination by 6%. 1 Introduction A well-written document is coherent (Halliday and Hasan, 1976)— it structures information so that each new piece of information is interpretable given the preceding context. Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation. Among the most popular models of coherence is the entity grid (Barzilay and Lapata, 2008), a statistical model based on Centering Theory (Grosz et al., 1995). The grid models the way texts focus on important entities, assigning them repeatedly to prominent syntactic roles. While the grid has been successful in a variety of applications, it is still a surprisingly unsophisticated model, and there have been few direct improvements to its simple feature set. We present an extension to the entity grid which distinguishes between different types of entity, resulting in significant gains in performance1. At its core, the grid model works by predicting whether an entity will appear in th</context>
<context position="2858" citStr="Barzilay and Lapata, 2008" startWordPosition="428" endWordPosition="431">nd Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011</context>
<context position="9765" citStr="Barzilay and Lapata (2008)" startWordPosition="1570" endWordPosition="1573">it is to be about May 25th. This problem is exacerbated by our same-head coreference heuristic, which sometimes creates spurious entities by lumping together mentions headed by nouns like &amp;quot;miles&amp;quot; or &amp;quot;dollars&amp;quot;. In this section, we add features that separate important entities from less important or spurious ones. Proper Does the entity have a proper mention? Named entity The majority OPENNLP Morton et al. (2005) named entity label for the coreferential chain. Modifiers The total number of modifiers in all mentions in the chain, bucketed by 5s. Singular Does the entity have a singular mention? 6Barzilay and Lapata (2008) uses NPs as mentions; we are unsure whether all other implementations do the same, but we believe we are the first to make the distinction explicit. News articles are likely to be about people and organizations, so we expect these named entity tags, and proper NPs in general, to be more important to the discourse. Entities with many modifiers throughout the document are also likely to be important, since this implies that the writer wishes to point out more information about them. Finally, singular nouns are less likely to be generic. We also add some features to pick out entities that are li</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: an entity-based approach. Computational Linguistics, 34(1):1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Joel Tetreault</author>
<author>Slava Andreyev</author>
</authors>
<title>Using entity-based features to model coherence in student essays.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>681--684</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="2900" citStr="Burstein et al., 2010" startWordPosition="436" endWordPosition="439">al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Lin</context>
</contexts>
<marker>Burstein, Tetreault, Andreyev, 2010</marker>
<rawString>Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010. Using entity-based features to model coherence in student essays. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 681-684, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="11144" citStr="Charniak and Elsner, 2009" startWordPosition="1799" endWordPosition="1802">target document itself. This avoids the problem that coreference resolvers do not work well for disordered or automatically produced text such as multidocument summary sentences, and also avoids the computational cost associated with coreference resolution. Linkable Was the head word of the entity ever marked as coreferring in MUC6? Unlinkable Did the head word of the entity occur 5 times in MUC6 and never corefer? Has pronouns Were there 5 or more pronouns coreferent with the head word of the entity in the NANC corpus? (Pronouns in NANC are automatically resolved using an unsupervised model (Charniak and Elsner, 2009).) No pronouns Did the head word of the entity occur over 50 times in NANC, and have fewer than 5 coreferent pronouns? To learn probabilities based on these features, we model the conditional probability p(ri;j❥F) using multilabel logistic regression. Our model has a parameter for each combination of syntactic role r, entity-specific feature h and feature vector F: r✂h✂F. This allows the old and new features to interact while keeping the parameter space tractable7. In Table 2, we examine the changes in our estimated probability in one particular context: an entity with salience 3 which appeare</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="5588" citStr="Charniak and Johnson, 2005" startWordPosition="871" endWordPosition="874">ver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction. Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors. To build a model based on the grid, we treat the columns (entities) as independent, and look at local transitions between sentences. We model the 2Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). transitions using the generative approach given in Lapata and Barzilay (2005)3, in which the model estimates the probability of an entity&apos;s role in the next sentence, rij, given its history in the previous two sentences, ri_1j, ri_2j. It also uses a single entity-specific feature, salience, determined by counting the total number of times the entity is mentioned in the document. We denote this feature vector Fi;j. For example, the vector for &amp;quot;flight&amp;quot; after the last sentence of the example would be F3;flight = ❤X, S, sal = 2✐. Using two sentences of context and capping salience at 4, there ar</context>
<context position="12095" citStr="Charniak and Johnson, 2005" startWordPosition="1956" endWordPosition="1959">ty-specific feature h and feature vector F: r✂h✂F. This allows the old and new features to interact while keeping the parameter space tractable7. In Table 2, we examine the changes in our estimated probability in one particular context: an entity with salience 3 which appeared in a non-emphatic role in the previous sentence. The standard entity grid estimates that such an entity will be the subject of the next sentence with a probability of about 7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005). 127 P(next role is subj) .045 .013 .025 .037 .133 .006 Random Disc. Acc Disc. F Ins. Elsner+Charniak 50.00 50.00 12.6 79.6 81.0 23.0 Grid 79.5 80.9 21.4 Extended Grid 84.O 84.5 24.2 Grid+combo 82.6 84.0 24.3 ExtEGrid+combo 86.O 86.5 26.7 Context Standard egrid Head coref in MUC6 ...and proper noun ...and NE type person ...and 5 modifiers overall Never coref in MUC6 ...and NE type date .001 Table 2: Probability of an entity appearing as subject of the next sentence, given the history - X, salience 3, and various entity-specific features. .04. For most classes of entity, we can see that thi</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL), pages 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erdong Chen</author>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Incremental text structuring with online hierarchical ranking.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6789" citStr="Chen et al., 2007" startWordPosition="1073" endWordPosition="1076">e at 4, there are only 64 possible vectors, so we can learn an independent multinomial distribution for each F. However, the number of vectors grows exponentially as we add features. 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4. We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the average proportion of correct insertions per document. As in Elsner and Charniak (2008), we test on sections 14-24 of the Penn Treebank, for 1004 test documents. We test significance using the Wilcoxon Sign-rank test, which detects significant differences in the medians of two distributions5. 5 Mention detection Our main contribution is to extend the entity grid by adding a large number of entity-specific features. Befor</context>
</contexts>
<marker>Chen, Snyder, Barzilay, 2007</marker>
<rawString>Erdong Chen, Benjamin Snyder, and Regina Barzilay. 2007. Incremental text structuring with online hierarchical ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Entitybased local coherence modelling using topological fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>186--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3974" citStr="Cheung and Penn (2010)" startWordPosition="599" endWordPosition="602">e Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]o was filed]X . 2 [The flight]S originated at [Nuevo Laredo, Mexico]X , at [approximately 1300]X. s conditions plan flight laredo 1 S O X - 2 - - S X Figure 1: A short text (using NP-only mention detection), and its corresponding entity grid. The numeric token 1300 is removed in preprocessing. ment over the original model. Cheung and Penn (2010) adapt the grid to German, where focused constituents are indicated by sentence position rather than syntactic role. The best entity grid for English text, however, is still the original. 3 Entity grids The entity grid represents a document as a matrix (Figure 1) with a row for each sentence and a column for each entity. The entry for (sentence i, entity j), which we write rij, represents the syntactic role that entity takes on in that sentence: subject (S), object (O), or some other role (X)2. In addition, there is a special marker (-) for entities which do not appear at all in a given senten</context>
</contexts>
<marker>Cheung, Penn, 2010</marker>
<rawString>Jackie Chi Kit Cheung and Gerald Penn. 2010. Entitybased local coherence modelling using topological fields. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186-195, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreferenceinspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>41--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3036" citStr="Elsner and Charniak, 2008" startWordPosition="456" endWordPosition="460">istics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]o was filed</context>
<context position="6817" citStr="Elsner and Charniak, 2008" startWordPosition="1077" endWordPosition="1081">nly 64 possible vectors, so we can learn an independent multinomial distribution for each F. However, the number of vectors grows exponentially as we add features. 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4. We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the average proportion of correct insertions per document. As in Elsner and Charniak (2008), we test on sections 14-24 of the Penn Treebank, for 1004 test documents. We test significance using the Wilcoxon Sign-rank test, which detects significant differences in the medians of two distributions5. 5 Mention detection Our main contribution is to extend the entity grid by adding a large number of entity-specific features. Before doing so, however, we add </context>
<context position="13591" citStr="Elsner and Charniak (2008)" startWordPosition="2217" endWordPosition="2220">itis a date, it falls even further, to .001. However, given that the entity refers to a person, and some of its mentions are modified, suggesting the article gives a title or description (&amp;quot;Obama&apos;s Secretary of State, Hillary Clinton&amp;quot;), the chance that it will be the subject of the next sentence more than triples. 7 Experiments Table 3 gives results for the extended grid model on the test set. This model is significantly better than the standard grid on discrimination (84% versus 80%) and has a higher mean score on insertion (24% versus 21%)8. The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection. We report their scores in the table. This comparison is unfair, however, because the improvements from adding non-head nouns improve our baseline grid sufficiently to equal their discrimination result. State-of-the-art results on a different corpus and task were achieved by Soricut and Marcu (2006) using a log-linear mixture of an entity grid, IBM translation models, and a wordcorrespondence model based on Lapata (2003). ✽For insertion using the model on its own, the median changes less than </context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. Coreferenceinspired coherence modeling. In Proceedings of ACL08: HLT, Short Papers, pages 41-44, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>The samehead heuristic for coreference.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 10,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8621" citStr="Elsner and Charniak, 2010" startWordPosition="1382" endWordPosition="1385"> Discrimination scores for entity grids with different mention detectors on WSJ development documents.  indicates performance on both tasks is significantly different from the previous row of the table with p=.05. more information to work with, but is beneficial even to the standard entity grid. We alter our mention detector to add all nouns in the document to the grid6, even those which do not head NPs. This enables the model to pick up premodifiers in phrases like &amp;quot;a Bush spokesman&amp;quot;, which do not head NPs in the Penn Treebank. Finding these is also necessary to maximize coreference recall (Elsner and Charniak, 2010). We give nonhead mentions the role X. The results of this change are shown in Table 1; discrimination performance increases about 4%, from 76% to 80%. 6 Entity-specific features As we mentioned earlier, the standard grid model does not distinguish between different types of entity. Given the same history and salience, the same probabilities are assigned to occurrences of &amp;quot;Hillary Clinton&amp;quot;, &amp;quot;the airlines&amp;quot;, or &amp;quot;May 25th&amp;quot;, even though we know a priori that a document is more likely to be about Hillary Clinton than it is to be about May 25th. This problem is exacerbated by our same-head coreferen</context>
</contexts>
<marker>Elsner, Charniak, 2010</marker>
<rawString>Micha Elsner and Eugene Charniak. 2010. The samehead heuristic for coreference. In Proceedings of ACL 10, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Extending the entity-grid coherence model to semantically related entities.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>139--142</pages>
<publisher>DFKI GmbH. Document</publisher>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="3238" citStr="Filippova and Strube (2007)" startWordPosition="487" endWordPosition="490">tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]o was filed]X . 2 [The flight]S originated at [Nuevo Laredo, Mexico]X , at [approximately 1300]X. s conditions plan flight laredo 1 S O X - 2 - - S X Figure 1: A short text (using NP-only mention detection), and i</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 139-142, Saarbrucken, Germany, June. DFKI GmbH. Document D-07-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="1058" citStr="Grosz et al., 1995" startWordPosition="149" endWordPosition="152">d coreference features to distinguish between important and unimportant entities. We improve the best result for WSJ document discrimination by 6%. 1 Introduction A well-written document is coherent (Halliday and Hasan, 1976)— it structures information so that each new piece of information is interpretable given the preceding context. Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation. Among the most popular models of coherence is the entity grid (Barzilay and Lapata, 2008), a statistical model based on Centering Theory (Grosz et al., 1995). The grid models the way texts focus on important entities, assigning them repeatedly to prominent syntactic roles. While the grid has been successful in a variety of applications, it is still a surprisingly unsophisticated model, and there have been few direct improvements to its simple feature set. We present an extension to the entity grid which distinguishes between different types of entity, resulting in significant gains in performance1. At its core, the grid model works by predicting whether an entity will appear in the next sentence ✶A public implementation is available via https:// b</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="2247" citStr="Haghighi and Klein, 2010" startWordPosition="334" endWordPosition="337">entation is available via https:// bitbucket.org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that &amp;quot;Clinton&amp;quot; will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself— the probability is the same whether the entity under discussion is &amp;quot;Hillary Clinton&amp;quot; or &amp;quot;wheat&amp;quot;. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and La</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385-393, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="664" citStr="Halliday and Hasan, 1976" startWordPosition="89" endWordPosition="92">pecific Features Micha Elsner Eugene Charniak School of Informatics Department of Computer Science University of Edinburgh Brown University, Providence, RI 02912 melsner0@gmail.com ec@cs.brown.edu Abstract We extend the popular entity grid representation for local coherence modeling. The grid abstracts away information about the entities it models; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant entities. We improve the best result for WSJ document discrimination by 6%. 1 Introduction A well-written document is coherent (Halliday and Hasan, 1976)— it structures information so that each new piece of information is interpretable given the preceding context. Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation. Among the most popular models of coherence is the entity grid (Barzilay and Lapata, 2008), a statistical model based on Centering Theory (Grosz et al., 1995). The grid models the way texts focus on important entities, assigning them repeatedly to prominent syntactic roles. While the grid has been successful in a variety of applications, it is still a surprising</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>1085--1090</pages>
<contexts>
<context position="2524" citStr="Lapata and Barzilay, 2005" startWordPosition="379" endWordPosition="382">t was the subject of sentence 1. The standard grid model uses no information about the entity itself— the probability is the same whether the entity under discussion is &amp;quot;Hillary Clinton&amp;quot; or &amp;quot;wheat&amp;quot;. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been fe</context>
<context position="5667" citStr="Lapata and Barzilay (2005)" startWordPosition="882" endWordPosition="885">ence improves results in only one of their target domains, and actually hurts for readability prediction. Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors. To build a model based on the grid, we treat the columns (entities) as independent, and look at local transitions between sentences. We model the 2Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). transitions using the generative approach given in Lapata and Barzilay (2005)3, in which the model estimates the probability of an entity&apos;s role in the next sentence, rij, given its history in the previous two sentences, ri_1j, ri_2j. It also uses a single entity-specific feature, salience, determined by counting the total number of times the entity is mentioned in the document. We denote this feature vector Fi;j. For example, the vector for &amp;quot;flight&amp;quot; after the last sentence of the example would be F3;flight = ❤X, S, sal = 2✐. Using two sentences of context and capping salience at 4, there are only 64 possible vectors, so we can learn an independent multinomial distribu</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IJCAI, pages 1085-1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the annual meeting of ACL,</booktitle>
<contexts>
<context position="14117" citStr="Lapata (2003)" startWordPosition="2299" endWordPosition="2300">us 21%)8. The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection. We report their scores in the table. This comparison is unfair, however, because the improvements from adding non-head nouns improve our baseline grid sufficiently to equal their discrimination result. State-of-the-art results on a different corpus and task were achieved by Soricut and Marcu (2006) using a log-linear mixture of an entity grid, IBM translation models, and a wordcorrespondence model based on Lapata (2003). ✽For insertion using the model on its own, the median changes less than the mean, and the change in median score is not significant. However, using the combined model, the change is significant. Table 3: Extended entity grid and combination model performance on 1004 WSJ test documents. Combination models incorporate pronoun coreference, discourse-new NP detection, and IBM model 1. indicates an extended model score better than its baseline counterpart at p=.05. To perform a fair comparison of our extended grid with these model-combining approaches, we train our own combined model incorporati</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the annual meeting of ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Plot induction and evolutionary search for story generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1562--1572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2785" citStr="McIntyre and Lapata, 2010" startWordPosition="418" endWordPosition="421">ant from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational L</context>
</contexts>
<marker>McIntyre, Lapata, 2010</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2010. Plot induction and evolutionary search for story generation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562-1572, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
<author>Joern Kottmann</author>
<author>Jason Baldridge</author>
<author>Gann Bierner</author>
</authors>
<date>2005</date>
<note>Opennlp: A java-based nlp toolkit. http://opennlp.sourceforge.net.</note>
<contexts>
<context position="9553" citStr="Morton et al. (2005)" startWordPosition="1534" endWordPosition="1537">alience, the same probabilities are assigned to occurrences of &amp;quot;Hillary Clinton&amp;quot;, &amp;quot;the airlines&amp;quot;, or &amp;quot;May 25th&amp;quot;, even though we know a priori that a document is more likely to be about Hillary Clinton than it is to be about May 25th. This problem is exacerbated by our same-head coreference heuristic, which sometimes creates spurious entities by lumping together mentions headed by nouns like &amp;quot;miles&amp;quot; or &amp;quot;dollars&amp;quot;. In this section, we add features that separate important entities from less important or spurious ones. Proper Does the entity have a proper mention? Named entity The majority OPENNLP Morton et al. (2005) named entity label for the coreferential chain. Modifiers The total number of modifiers in all mentions in the chain, bucketed by 5s. Singular Does the entity have a singular mention? 6Barzilay and Lapata (2008) uses NPs as mentions; we are unsure whether all other implementations do the same, but we believe we are the first to make the distinction explicit. News articles are likely to be about people and organizations, so we expect these named entity tags, and proper NPs in general, to be more important to the discourse. Entities with many modifiers throughout the document are also likely to</context>
</contexts>
<marker>Morton, Kottmann, Baldridge, Bierner, 2005</marker>
<rawString>Thomas Morton, Joern Kottmann, Jason Baldridge, and Gann Bierner. 2005. Opennlp: A java-based nlp toolkit. http://opennlp.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Advaith Siddharthan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatically learning cognitive status for multi-document summarization of newswire.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>241--248</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2288" citStr="Nenkova et al., 2005" startWordPosition="341" endWordPosition="344">org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that &amp;quot;Clinton&amp;quot; will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself— the probability is the same whether the entity under discussion is &amp;quot;Hillary Clinton&amp;quot; or &amp;quot;wheat&amp;quot;. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein e</context>
</contexts>
<marker>Nenkova, Siddharthan, McKeown, 2005</marker>
<rawString>Ani Nenkova, Advaith Siddharthan, and Kathleen McKeown. 2005. Automatically learning cognitive status for multi-document summarization of newswire. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 241-248, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic evaluation of linguistic quality in multidocument summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>544--554</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2830" citStr="Pitler et al., 2010" startWordPosition="424" endWordPosition="427">reference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portla</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2010</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multidocument summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 544-554, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</booktitle>
<contexts>
<context position="3008" citStr="Soricut and Marcu, 2006" startWordPosition="452" endWordPosition="455">ed-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but find no significant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a</context>
<context position="13993" citStr="Soricut and Marcu (2006)" startWordPosition="2276" endWordPosition="2280">el is significantly better than the standard grid on discrimination (84% versus 80%) and has a higher mean score on insertion (24% versus 21%)8. The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection. We report their scores in the table. This comparison is unfair, however, because the improvements from adding non-head nouns improve our baseline grid sufficiently to equal their discrimination result. State-of-the-art results on a different corpus and task were achieved by Soricut and Marcu (2006) using a log-linear mixture of an entity grid, IBM translation models, and a wordcorrespondence model based on Lapata (2003). ✽For insertion using the model on its own, the median changes less than the mean, and the change in median score is not significant. However, using the combined model, the change is significant. Table 3: Extended entity grid and combination model performance on 1004 WSJ test documents. Combination models incorporate pronoun coreference, discourse-new NP detection, and IBM model 1. indicates an extended model score better than its baseline counterpart at p=.05. To perfo</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>