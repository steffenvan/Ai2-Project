<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012095">
<title confidence="0.9948055">
Unsupervised Learning of Field Segmentation Models
for Information Extraction
</title>
<author confidence="0.98711">
Trond Grenager
</author>
<affiliation confidence="0.9819815">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.873527">
Stanford, CA 94305
</address>
<email confidence="0.998885">
grenager@cs.stanford.edu
</email>
<author confidence="0.993223">
Dan Klein
</author>
<affiliation confidence="0.986565">
Computer Science Division
</affiliation>
<address confidence="0.7778115">
U.C. Berkeley
Berkeley, CA 94709
</address>
<email confidence="0.999046">
klein@cs.berkeley.edu
</email>
<author confidence="0.994457">
Christopher D. Manning
</author>
<affiliation confidence="0.9837505">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.874111">
Stanford, CA 94305
</address>
<email confidence="0.999289">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99833435">
The applicability of many current information ex-
traction techniques is severely limited by the need
for supervised training data. We demonstrate that
for certain field structured extraction tasks, such
as classified advertisements and bibliographic ci-
tations, small amounts of prior knowledge can be
used to learn effective models in a primarily unsu-
pervised fashion. Although hidden Markov models
(HMMs) provide a suitable generative model for
field structured text, general unsupervised HMM
learning fails to learn useful structure in either of
our domains. However, one can dramatically im-
prove the quality of the learned structure by ex-
ploiting simple prior knowledge of the desired so-
lutions. In both domains, we found that unsuper-
vised methods can attain accuracies with 400 un-
labeled examples comparable to those attained by
supervised methods on 50 labeled examples, and
that semi-supervised methods can make good use
of small amounts of labeled data.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981725">
Information extraction is potentially one of the most
useful applications enabled by current natural lan-
guage processing technology. However, unlike gen-
eral tools like parsers or taggers, which generalize
reasonably beyond their training domains, extraction
systems must be entirely retrained for each appli-
cation. As an example, consider the task of turn-
ing a set of diverse classified advertisements into a
queryable database; each type of ad would require
tailored training data for a supervised system. Ap-
proaches which required little or no training data
would therefore provide substantial resource savings
and extend the practicality of extraction systems.
The term information extraction was introduced
in the MUC evaluations for the task of finding short
pieces of relevant information within a broader text
that is mainly irrelevant, and returning it in a struc-
tured form. For such “nugget extraction” tasks, the
use of unsupervised learning methods is difficult and
unlikely to be fully successful, in part because the
nuggets of interest are determined only extrinsically
by the needs of the user or task. However, the term
information extraction was in time generalized to a
related task that we distinguish as field segmenta-
tion. In this task, a document is regarded as a se-
quence of pertinent fields, and the goal is to segment
the document into fields, and to label the fields. For
example, bibliographic citations, such as the one in
Figure 1(a), exhibit clear field structure, with fields
such as author, title, and date. Classified advertise-
ments, such as the one in Figure 1(b), also exhibit
field structure, if less rigidly: an ad consists of de-
scriptions of attributes of an item or offer, and a set
of ads for similar items share the same attributes. In
these cases, the fields present a salient, intrinsic form
of linguistic structure, and it is reasonable to hope
that field segmentation models could be learned in
an unsupervised fashion.
In this paper, we investigate unsupervised learn-
ing of field segmentation models in two domains:
bibliographic citations and classified advertisements
for apartment rentals. General, unconstrained induc-
tion of HMMs using the EM algorithm fails to detect
useful field structure in either domain. However, we
demonstrate that small amounts of prior knowledge
can be used to greatly improve the learned model. In
both domains, we found that unsupervised methods
can attain accuracies with 400 unlabeled examples
comparable to those attained by supervised methods
on 50 labeled examples, and that semi-supervised
methods can make good use of small amounts of la-
beled data.
</bodyText>
<page confidence="0.979623">
371
</page>
<note confidence="0.969562">
Proceedings of the 43rd Annual Meeting of the ACL, pages 371–378,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.9983877">
(a) AUTH AUTH AUTH DATE DATE DATE DATE TTL TTL TTL TTL
Pearl , J. ( 1988 ) . Probabilistic Reasoning in Intelligent
TTL TTL TTL TTL TTL TTL TTL PUBL PUBL PUBL
Systems : Networks of Plausible Inference . Morgan Kaufmann .
(b) SIZE SIZE SIZE SIZE SIZE FEAT FEAT FEAT FEAT FEAT FEAT
Spacious 1 Bedroom apt . newly remodeled , gated , new
FEAT FEAT FEAT FEAT FEAT NBRHD NBRHD NBRHD NBRHD NBRHD NBRHD
appliance , new carpet , near public transportion , close to
NBRHD NBRHD NBRHD RENT RENT RENT CONTACT
580 freeway , $ 500.00 Deposit (510)655-0106
(c) RB
No
, PRP
, it
NNP
Monday
VBD RB NNP
was n’t Black
.
.
</figure>
<figureCaption confidence="0.9965565">
Figure 1: Examples of three domains for HMM learning: the bibliographic citation fields in (a) and classified advertisements for
apartment rentals shown in (b) exhibit field structure. Contrast these to part-of-speech tagging in (c) which does not.
</figureCaption>
<sectionHeader confidence="0.953687" genericHeader="method">
2 Hidden Markov Models
</sectionHeader>
<bodyText confidence="0.9999522">
Hidden Markov models (HMMs) are commonly
used to represent a wide range of linguistic phe-
nomena in text, including morphology, parts-of-
speech (POS), named entity mentions, and even
topic changes in discourse. An HMM consists of
a set of states S, a set of observations (in our case
words or tokens) W, a transition model specify-
ing P(stIst−1), the probability of transitioning from
state st−1 to state st, and an emission model specify-
ing P(w s) the probability of emitting word w while
in state s. For a good tutorial on general HMM tech-
niques, see Rabiner (1989).
For all of the unsupervised learning experiments
we fit an HMM with the same number of hidden
states as gold labels to an unannotated training set
using EM.1 To compute hidden state expectations
efficiently, we use the Forward-Backward algorithm
in the standard way. Emission models are initialized
to almost-uniform probability distributions, where
a small amount of noise is added to break initial
symmetry. Transition model initialization varies by
experiment. We run the EM algorithm to conver-
gence. Finally, we use the Viterbi algorithm with
the learned parameters to label the test data.
All baselines and experiments use the same tok-
enization, normalization, and smoothing techniques,
which were not extensively investigated. Tokeniza-
tion was performed in the style of the Penn Tree-
bank, and tokens were normalized in various ways:
numbers, dates, phone numbers, URLs, and email
</bodyText>
<footnote confidence="0.981108666666667">
1EM is a greedy hill-climbing algorithm designed for this
purpose, but it is not the only option; one could also use coordi-
nate ascent methods or sampling methods.
</footnote>
<bodyText confidence="0.999553333333333">
addresses were collapsed to dedicated tokens, and
all remaining tokens were converted to lowercase.
Unless otherwise noted, the emission models use
simple add-A smoothing, where A was 0.001 for su-
pervised techniques, and 0.2 for unsupervised tech-
niques.
</bodyText>
<sectionHeader confidence="0.992183" genericHeader="method">
3 Datasets and Evaluation
</sectionHeader>
<bodyText confidence="0.999708076923077">
The bibliographic citations data is described in
McCallum et al. (1999), and is distributed at
http://www.cs.umass.edu/~mccallum/. It consists of
500 hand-annotated citations, each taken from the
reference section of a different computer science re-
search paper. The citations are annotated with 13
fields, including author, title, date, journal, and so
on. The average citation has 35 tokens in 5.5 fields.
We split this data, using its natural order, into a 300-
document training set, a 100-document development
set, and a 100-document test set.
The classified advertisements data set is
novel, and consists of 8,767 classified ad-
vertisements for apartment rentals in the San
Francisco Bay Area downloaded in June 2004
from the Craigslist website. It is distributed at
http://www.stanford.edu/~grenager/. 302 of the
ads have been labeled with 12 fields, including
size, rent, neighborhood, features, and so on.
The average ad has 119 tokens in 8.7 fields. The
annotated data is divided into a 102-document
training set, a 100-document development set,
and a 100-document test set. The remaining 8465
documents form an unannotated training set.
In both cases, all system development and param-
eter tuning was performed on the development set,
</bodyText>
<page confidence="0.997274">
372
</page>
<table confidence="0.972126666666667">
size author DT
rent title JJ
features editor NN
restrictions journal NNS
neighborhood booktitle NNP
utilities volume PRP
available pages CC
contact publisher MD
photos location VBD
roomates tech VB
other institution TO
address date IN
</table>
<figure confidence="0.985216">
(a) (b) (c)
</figure>
<figureCaption confidence="0.969283333333333">
Figure 2: Matrix representations of the target transition structure in two field structured domains: (a) classified advertisements
(b) bibliographic citations. Columns and rows are indexed by the same sequence of fields. Also shown is (c) a submatrix of the
transition structure for a part-of-speech tagging task. In all cases the column labels are the same as the row labels.
</figureCaption>
<bodyText confidence="0.999824034482759">
and the test set was only used once, for running fi-
nal experiments. Supervised learning experiments
train on documents selected randomly from the an-
notated training set and test on the complete test set.
Unsupervised learning experiments also test on the
complete test set, but create a training set by first
adding documents from the test set (without anno-
tation), then adding documents from the annotated
training set (without annotation), and finally adding
documents from the unannotated training set. Thus
if an unsupervised training set is larger than the test
set, it fully contains the test set.
To evaluate our models, we first learn a set of
model parameters, and then use the parameterized
model to label the sequence of tokens in the test data
with the model’s hidden states. We then compare
the similarity of the guessed sequence to the human-
annotated sequence of gold labels, and compute ac-
curacy on a per-token basis.2 In evaluation of su-
pervised methods, the model states and gold labels
are the same. For models learned in a fully unsuper-
vised fashion, we map each model state in a greedy
fashion to the gold label to which it most often cor-
responds in the gold data. There is a worry with
this kind of greedy mapping: it increasingly inflates
the results as the number of hidden states grows. To
keep the accuracies meaningful, all of our models
have exactly the same number of hidden states as
gold labels, and so the comparison is valid.
</bodyText>
<footnote confidence="0.595516">
2This evaluation method is used by McCallum et al. (1999)
but otherwise is not very standard. Compared to other evalu-
ation methods for information extraction systems, it leads to a
lower penalty for boundary errors, and allows long fields also
contribute more to accuracy than short ones.
</footnote>
<sectionHeader confidence="0.976454" genericHeader="method">
4 Unsupervised Learning
</sectionHeader>
<bodyText confidence="0.999985117647059">
Consider the general problem of learning an HMM
from an unlabeled data set. Even abstracting away
from concrete search methods and objective func-
tions, the diversity and simultaneity of linguistic
structure is already worrying; in Figure 1 compare
the field structure in (a) and (b) to the parts-of-
speech in (c). If strong sequential correlations exist
at multiple scales, any fixed search procedure will
detect and model at most one of these levels of struc-
ture, not necessarily the level desired at the moment.
Worse, as experience with part-of-speech and gram-
mar learning has shown, induction systems are quite
capable of producing some uninterpretable mix of
various levels and kinds of structure.
Therefore, if one is to preferentially learn one
kind of inherent structure over another, there must
be some way of constraining the process. We could
hope that field structure is the strongest effect in
classified ads, while parts-of-speech is the strongest
effect in newswire articles (or whatever we would
try to learn parts-of-speech from). However, it is
hard to imagine how one could bleach the local
grammatical correlations and long-distance topical
correlations from our classified ads; they are still
English text with part-of-speech patterns. One ap-
proach is to vary the objective function so that the
search prefers models which detect the structures
which we have in mind. This is the primary way
supervised methods work, with the loss function rel-
ativized to training label patterns. However, for un-
supervised learning, the primary candidate for an
objective function is the data likelihood, and we
don’t have another suggestion here. Another ap-
proach is to inject some prior knowledge into the
</bodyText>
<page confidence="0.995886">
373
</page>
<bodyText confidence="0.999981444444444">
search procedure by carefully choosing the starting
point; indeed smart initialization has been critical
to success in many previous unsupervised learning
experiments. The central idea of this paper is that
we can instead restrict the entire search domain by
constraining the model class to reflect the desired
structure in the data, thereby directing the search to-
ward models of interest. We do this in several ways,
which are described in the following sections.
</bodyText>
<subsectionHeader confidence="0.988907">
4.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999989">
To situate our results, we provide three different
baselines (see Table 1). First is the most-frequent-
field accuracy, achieved by labeling all tokens with
the same single label which is then mapped to the
most frequent field. This gives an accuracy of 46.4%
on the advertisements data and 27.9% on the cita-
tions data. The second baseline method is to pre-
segment the unlabeled data using a crude heuristic
based on punctuation, and then to cluster the result-
ing segments using a simple Naive Bayes mixture
model with the Expectation-Maximization (EM) al-
gorithm. This approach achieves an accuracy of
62.4% on the advertisements data, and 46.5% on the
citations data.
As a final baseline, we trained a supervised first-
order HMM from the annotated training data using
maximum likelihood estimation. With 100 training
examples, supervised models achieve an accuracy of
74.4% on the advertisements data, and 72.5% on the
citations data. With 300 examples, supervised meth-
ods achieve accuracies of 80.4 on the citations data.
The learning curves of the supervised training ex-
periments for different amounts of training data are
shown in Figure 4. Note that other authors have
achieved much higher accuracy on the the citation
dataset using HMMs trained with supervision: Mc-
Callum et al. (1999) report accuracies as high as
92.9% by using more complex models and millions
of words of BibTeX training data.
</bodyText>
<subsectionHeader confidence="0.954181">
4.2 Unconstrained HMM Learning
</subsectionHeader>
<bodyText confidence="0.994102666666667">
From the supervised baseline above we know that
there is some first-order HMM over ISI states which
captures the field structure we’re interested in, and
we would like to find such a model without super-
vision. As a first attempt, we try fitting an uncon-
strained HMM, where the transition function is ini-
</bodyText>
<figureCaption confidence="0.9986375">
Figure 3: Matrix representations of typical transition models
learned by initializing the transition model uniformly.
</figureCaption>
<bodyText confidence="0.999970823529412">
tialized randomly, to the unannotated training data.
Not surprisingly, the unconstrained approach leads
to predictions which poorly align with the desired
field segmentation: with 400 unannotated training
documents, the accuracy is just 48.8% for the ad-
vertisements and 49.7% for the citations: better than
the single state baseline but far from the supervised
baseline. To illustrate what is (and isn’t) being
learned, compare typical transition models learned
by this method, shown in Figure 3, to the maximum-
likelihood transition models for the target annota-
tions, shown in Figure 2. Clearly, they aren’t any-
thing like the target models: the learned classified
advertisements matrix has some but not all of the
desired diagonal structure, and the learned citations
matrix has almost no mass on the diagonal, and ap-
pears to be modeling smaller scale structure.
</bodyText>
<subsectionHeader confidence="0.992376">
4.3 Diagonal Transition Models
</subsectionHeader>
<bodyText confidence="0.999861666666667">
To adjust our procedure to learn larger-scale pat-
terns, we can constrain the parametric form of the
transition model to be
</bodyText>
<equation confidence="0.9952895">
Q + (1−σ)
|S |if st = st−1
(1−σ) otherwise
|S|
</equation>
<bodyText confidence="0.9888035">
where |S |is the number of states, and Q is a global
free parameter specifying the self-loop probability:
</bodyText>
<figure confidence="0.985767363636363">
1
2
3
4
5
6
7
8
9
10
11
12
(a) Classified Advertisements
1
2
3
4
5
6
7
8
9
10
11
12
(b) Citations
P(st|st−1) =
⎧
⎨
⎩
374
(a) Classified advertisements
(b) Bibliographic citations
</figure>
<figureCaption confidence="0.9899305">
Figure 4: Learning curves for supervised learning and unsuper-
vised learning with a diagonal transition matrix on (a) classified
advertisements, and (b) bibliographic citations. Results are av-
eraged over 50 runs.
</figureCaption>
<bodyText confidence="0.999840789473684">
the probability of a state transitioning to itself. (Note
that the expected mean field length for transition
functions of this form is 1
1−a.) This constraint pro-
vides a notable performance improvement: with 400
unannotated training documents the accuracy jumps
from 48.8% to 70.0% for advertisements and from
49.7% to 66.3% for citations. The complete learning
curves for models of this form are shown in Figure 4.
We have tested training on more unannotated data;
the slope of the learning curve is leveling out, but
by training on 8000 unannotated ads, accuracy im-
proves significantly to 72.4%. On the citations task,
an accuracy of approximately 66% can be achieved
either using supervised training on 50 annotated ci-
tations, or unsupervised training using 400 unanno-
tated citations. 3
Although σ can easily be reestimated with EM
(even on a per-field basis), doing so does not yield
</bodyText>
<footnote confidence="0.9547214">
3We also tested training on 5000 additional unannotated ci-
tations collected from papers found on the Internet. Unfortu-
nately the addition of this data didn’t help accuracy. This prob-
ably results from the fact that the datasets were collected from
different sources, at different times.
</footnote>
<figureCaption confidence="0.9976295">
Figure 5: Unsupervised accuracy as a function of the expected
mean field length 1
</figureCaption>
<bodyText confidence="0.9263339">
1−σ for the classified advertisements dataset.
Each model was trained with 500 documents and tested on the
development set. Results are averaged over 50 runs.
better models.4 On the other hand, model accuracy
is not very sensitive to the exact choice of σ, as
shown in Figure 5 for the classified advertisements
task (the result for the citations task has a similar
shape). For the remaining experiments on the adver-
tisements data, we use σ = 0.9, and for those on the
citations data, we use σ = 0.5.
</bodyText>
<subsectionHeader confidence="0.997783">
4.4 Hierarchical Mixture Emission Models
</subsectionHeader>
<bodyText confidence="0.9999832">
Consider the highest-probability state emissions
learned by the diagonal model, shown in Figure 6(a).
In addition to its characteristic content words, each
state also emits punctuation and English function
words devoid of content. In fact, state 3 seems to
have specialized entirely in generating such tokens.
This can become a problem when labeling decisions
are made on the basis of the function words rather
than the content words. It seems possible, then,
that removing function words from the field-specific
emission models could yield an improvement in la-
beling accuracy.
One way to incorporate this knowledge into the
model is to delete stopwords, which, while perhaps
not elegant, has proven quite effective in the past.
A better founded way of making certain words un-
available to the model is to emit those words from
all states with equal probability. This can be accom-
plished with the following simple hierarchical mix-
ture emission model
</bodyText>
<equation confidence="0.998174">
Ph(w|s) = αP,(w) + (1 − α)P(w|s)
</equation>
<bodyText confidence="0.81185">
where P, is the common word distribution, and α is
</bodyText>
<footnote confidence="0.719414">
4While it may be surprising that disallowing reestimation of
the transition function is helpful here, the same has been ob-
served in acoustic modeling (Rabiner and Juang, 1993).
</footnote>
<page confidence="0.995984">
375
</page>
<figure confidence="0.965228714285714">
State 10 Most Common Words
1 . $ no ! month deposit , pets rent avail-
able
2 , . room and with in large living kitchen
-
3 . a the is and for this to , in
4 [NUM1] [NUM0] , bedroom bath / - .
car garage
5 , . and a in - quiet with unit building
6 - . [TIME] [PHONE] [DAY] call
[NUM8] at
(a)
State 10 Most Common Words
1 [NUM2] bedroom [NUM1] bath bed-
rooms large sq car ft garage
2 $ no month deposit pets lease rent avail-
able year security
3 kitchen room new , with living large
floors hardwood fireplace
4 [PHONE] call please at or for [TIME] to
[DAY] contact
5 san street at ave st # [NUM:DDD] fran-
cisco ca [NUM:DDDD]
6 of the yard with unit private back a
building floor
Comm. *CR* . , and - the in a / is with : of for
to
(b)
</figure>
<figureCaption confidence="0.99622025">
Figure 6: Selected state emissions from a typical model learned
from unsupervised data using the constrained transition func-
tion: (a) with a flat emission model, and (b) with a hierarchical
emission model.
</figureCaption>
<bodyText confidence="0.999514652173913">
a new global free parameter. In such a model, before
a state emits a token it flips a coin, and with probabil-
ity α it allows the common word distribution to gen-
erate the token, and with probability (1−α) it gener-
ates the token from its state-specific emission model
(see Vaithyanathan and Dom (2000) and Toutanova
et al. (2001) for more on such models). We tuned
α on the development set and found that a range of
values work equally well. We used a value of 0.5 in
the following experiments.
We ran two experiments on the advertisements
data, both using the fixed transition model described
in Section 4.3 and the hierarchical emission model.
First, we initialized the emission model of Pc to a
general-purpose list of stopwords, and did not rees-
timate it. This improved the average accuracy from
70.0% to 70.9%. Second, we learned the emission
model of Pc using EM reestimation. Although this
method did not yield a significant improvement in
accuracy, it learns sensible common words: Fig-
ure 6(b) shows a typical emission model learned
with this technique. Unfortunately, this technique
does not yield improvements on the citations data.
</bodyText>
<subsectionHeader confidence="0.981945">
4.5 Boundary Models
</subsectionHeader>
<bodyText confidence="0.999917157894737">
Another source of error concerns field boundaries.
In many cases, fields are more or less correct, but the
boundaries are off by a few tokens, even when punc-
tuation or syntax make it clear to a human reader
where the exact boundary should be. One way to ad-
dress this is to model the fact that in this data fields
often end with one of a small set of boundary tokens,
such as punctuation and new lines, which are shared
across states.
To accomplish this, we enriched the Markov pro-
cess so that each field s is now modeled by two
states, a non-final s− E 5− and a final s+ E 5+.
The transition model for final states is the same as
before, but the transition model for non-final states
has two new global free parameters: A, the probabil-
ity of staying within the field, and µ, the probability
of transitioning to the final state given that we are
staying in the field. The transition function for non-
final states is then
</bodyText>
<figure confidence="0.9084116">
(1 − µ)(A + (1−λ) if s0 = s−
)
|S−|
P(s0|s−) = µ(A + (1−λ) ) if s0 = s+
I if E 5−\s−
s0
|S−|
(�s
�)
0 otherwise.
</figure>
<bodyText confidence="0.9996984">
Note that it can bypass the final state, and transi-
tion directly to other non-final states with probabil-
ity (1 − A), which models the fact that not all field
occurrences end with a boundary token. The transi-
tion function for non-final states is then
</bodyText>
<equation confidence="0.99383525">
σ +S−
(1−σ) if s0 = s−
(|s−|) if s0 E 5−\s−
0 otherwise.
</equation>
<bodyText confidence="0.998831">
Note that this has the form of the standard diagonal
function. The reason for the self-loop from the fi-
nal state back to the non-final state is to allow for
field internal punctuation. We tuned the free param-
eters on the development set, and found that σ = 0.5
and A = 0.995 work well for the advertisements do-
main, and σ = 0.3 and A = 0.9 work well for the
citations domain. In all cases it works well to set
µ = 1 − A. Emissions from non-final states are as
</bodyText>
<equation confidence="0.98936275">
P(s0|s+) =
�
�� �
���
</equation>
<page confidence="0.993772">
376
</page>
<table confidence="0.999805916666667">
Ads Citations
Baseline 46.4 27.9
Segment and cluster 62.4 46.5
Supervised 74.4 72.5
Unsup. (learned trans) 48.8 49.7
Unsup. (diagonal trans) 70.0 66.3
+ Hierarchical (learned) 70.1 39.1
+ Hierarchical (given) 70.9 62.1
+ Boundary (learned) 70.4 64.3
+ Boundary (given) 71.9 68.2
+ Hier. + Bnd. (learned) 71.0 —
+ Hier. + Bnd. (given) 72.7 —
</table>
<tableCaption confidence="0.728399285714286">
Table 1: Summary of results. For each experiment, we report
percentage accuracy on the test set. Supervised experiments
use 100 training documents, and unsupervised experiments use
400 training documents. Because unsupervised techniques are
stochastic, those results are averaged over 50 runs, and differ-
ences greater than 1.0% are significant at p=0.05% or better ac-
cording to the t-test. The last 6 rows are not cumulative.
</tableCaption>
<bodyText confidence="0.999738052631579">
before (hierarchical or not depending on the experi-
ment), while all final states share a boundary emis-
sion model. Note that the boundary emissions are
not smoothed like the field emissions.
We tested both supplying the boundary token dis-
tributions and learning them with reestimation dur-
ing EM. In experiments on the advertisements data
we found that learning the boundary emission model
gives an insignificant raise from 70.0% to 70.4%,
while specifying the list of allowed boundary tokens
gives a significant increase to 71.9%. When com-
bined with the given hierarchical emission model
from the previous section, accuracy rises to 72.7%,
our best unsupervised result on the advertisements
data with 400 training examples. In experiments on
the citations data we found that learning boundary
emission model hurts accuracy, but that given the set
of boundary tokens it boosts accuracy significantly:
increasing it from 66.3% to 68.2%.
</bodyText>
<sectionHeader confidence="0.998503" genericHeader="method">
5 Semi-supervised Learning
</sectionHeader>
<bodyText confidence="0.999379454545455">
So far, we have largely focused on incorporating
prior knowledge in rather general and implicit ways.
As a final experiment we tested the effect of adding
a small amount of supervision: augmenting the large
amount of unannotated data we use for unsuper-
vised learning with a small amount of annotated
data. There are many possible techniques for semi-
supervised learning; we tested a particularly simple
one. We treat the annotated labels as observed vari-
ables, and when computing sufficient statistics in the
M-step of EM we add the observed counts from the
</bodyText>
<figureCaption confidence="0.999479">
Figure 7: Learning curves for semi-supervised learning on the
citations task. A separate curve is drawn for each number of
annotated documents. All results are averaged over 50 runs.
</figureCaption>
<bodyText confidence="0.999965375">
annotated documents to the expected counts com-
puted in the E-step. We estimate the transition
function using maximum likelihood from the an-
notated documents only, and do not reestimate it.
Semi-supervised results for the citations domain are
shown in Figure 7. Adding 5 annotated citations
yields no improvement in performance, but adding
20 annotated citations to 300 unannotated citations
boosts performance greatly from 65.2% to 71.3%.
We also tested the utility of this approach in the clas-
sified advertisement domain, and found that it did
not improve accuracy. We believe that this is be-
cause the transition information provided by the su-
pervised data is very useful for the citations data,
which has regular transition structure, but is not as
useful for the advertisements data, which does not.
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="method">
6 Previous Work
</sectionHeader>
<bodyText confidence="0.9999710625">
A good amount of prior research can be cast as
supervised learning of field segmentation models,
using various model families and applied to var-
ious domains. McCallum et al. (1999) were the
first to compare a number of supervised methods
for learning HMMs for parsing bibliographic cita-
tions. The authors explicitly claim that the domain
would be suitable for unsupervised learning, but
they do not present experimental results. McCallum
et al. (2000) applied supervised learning of Maxi-
mum Entropy Markov Models (MEMMs) to the do-
main of parsing Frequently Asked Question (FAQ)
lists into their component field structure. More re-
cently, Peng and McCallum (2004) applied super-
vised learning of Conditional Random Field (CRF)
sequence models to the problem of parsing the head-
</bodyText>
<page confidence="0.993959">
377
</page>
<bodyText confidence="0.999911411764706">
ers of research papers.
There has also been some previous work on un-
supervised learning of field segmentation models in
particular domains. Pasula et al. (2002) performs
limited unsupervised segmentation of bibliographic
citations as a small part of a larger probabilistic
model of identity uncertainty. However, their sys-
tem does not explicitly learn a field segmentation
model for the citations, and encodes a large amount
of hand-supplied information about name forms, ab-
breviation schemes, and so on. More recently, Barzi-
lay and Lee (2004) defined content models, which
can be viewed as field segmentation models occur-
ring at the level of discourse. They perform un-
supervised learning of these models from sets of
news articles which describe similar events. The
fields in that case are the topics discussed in those
articles. They consider a very different set of ap-
plications from the present work, and show that
the learned topic models improve performance on
two discourse-related tasks: information ordering
and extractive document summarization. Most im-
portantly, their learning method differs significantly
from ours; they use a complex and special purpose
algorithm, which is difficult to adapt, while we see
our contribution to be a demonstration of the inter-
play between model family and learned structure.
Because the structure of the HMMs they learn is
similar to ours it seems that their system could ben-
efit from the techniques of this paper. Finally, Blei
and Moreno (2001) use an HMM augmented by an
aspect model to automatically segment documents,
similar in goal to the system of Hearst (1997), but
using techniques more similar to the present work.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988384615385">
In this work, we have examined the task of learn-
ing field segmentation models using unsupervised
learning. In two different domains, classified ad-
vertisements and bibliographic citations, we showed
that by constraining the model class we were able
to restrict the search space of EM to models of in-
terest. We used unsupervised learning methods with
400 documents to yield field segmentation models
of a similar quality to those learned using supervised
learning with 50 documents. We demonstrated that
further refinements of the model structure, including
hierarchical mixture emission models and boundary
models, produce additional increases in accuracy.
Finally, we also showed that semi-supervised meth-
ods with a modest amount of labeled data can some-
times be effectively used to get similar good results,
depending on the nature of the problem.
While there are enough resources for the citation
task that much better numbers than ours can be and
have been obtained (with more knowledge and re-
source intensive methods), in domains like classi-
fied ads for lost pets or used bicycles unsupervised
learning may be the only practical option. In these
cases, we find it heartening that the present systems
do as well as they do, even without field-specific
prior knowledge.
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999841">
We would like to thank the reviewers for their con-
sideration and insightful comments.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953228571429">
R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic
content models, with applications to generation and summa-
rization. In Proceedings of HLT-NAACL 2004, pages 113–
120.
D. Blei and P. Moreno. 2001. Topic segmentation with an aspect
hidden Markov model. In Proceedings of the 24th SIGIR,
pages 343–348.
M. A. Hearst. 1997. TextTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):33–64.
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 1999.
A machine learning approach to building domain-specific
search engines. In IJCAI-1999.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy Markov models for information extraction and seg-
mentation. In Proceedings of the 17th ICML, pages 591–598.
Morgan Kaufmann, San Francisco, CA.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2002.
Identity uncertainty and citation matching. In Proceedings of
NIPS 2002.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using Conditional Random Fields.
In Proceedings ofHLT-NAACL 2004.
L. R. Rabiner and B.-H. Juang. 1993. Fundamentals of Speech
Recognition. Prentice Hall.
L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257–286.
K. Toutanova, F. Chen, K. Popat, and T. Hofmann. 2001. Text
classification in a hierarchical mixture model for small train-
ing sets. In CIKM ’01: Proceedings of the tenth interna-
tional conference on Information and knowledge manage-
ment, pages 105–113. ACM Press.
S. Vaithyanathan and B. Dom. 2000. Model-based hierarchical
clustering. In UAI-2000.
</reference>
<page confidence="0.998305">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960605">
<title confidence="0.998005">Unsupervised Learning of Field Segmentation Models for Information Extraction</title>
<author confidence="0.985415">Trond Grenager</author>
<affiliation confidence="0.999648">Computer Science Department Stanford University</affiliation>
<address confidence="0.999771">Stanford, CA 94305</address>
<email confidence="0.999728">grenager@cs.stanford.edu</email>
<author confidence="0.99998">Dan Klein</author>
<affiliation confidence="0.9975205">Computer Science Division U.C. Berkeley</affiliation>
<address confidence="0.999655">Berkeley, CA 94709</address>
<email confidence="0.999499">klein@cs.berkeley.edu</email>
<author confidence="0.999845">Christopher D Manning</author>
<affiliation confidence="0.999867">Computer Science Department Stanford University</affiliation>
<address confidence="0.999261">Stanford, CA 94305</address>
<email confidence="0.999624">manning@cs.stanford.edu</email>
<abstract confidence="0.99907280952381">The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that certain structured tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="27710" citStr="Barzilay and Lee (2004)" startWordPosition="4568" endWordPosition="4572">tional Random Field (CRF) sequence models to the problem of parsing the head377 ers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on. More recently, Barzilay and Lee (2004) defined content models, which can be viewed as field segmentation models occurring at the level of discourse. They perform unsupervised learning of these models from sets of news articles which describe similar events. The fields in that case are the topics discussed in those articles. They consider a very different set of applications from the present work, and show that the learned topic models improve performance on two discourse-related tasks: information ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a com</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of HLT-NAACL 2004, pages 113– 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>P Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden Markov model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th SIGIR,</booktitle>
<pages>343--348</pages>
<contexts>
<context position="28659" citStr="Blei and Moreno (2001)" startWordPosition="4721" endWordPosition="4724">pplications from the present work, and show that the learned topic models improve performance on two discourse-related tasks: information ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a complex and special purpose algorithm, which is difficult to adapt, while we see our contribution to be a demonstration of the interplay between model family and learned structure. Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper. Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work. 7 Conclusions In this work, we have examined the task of learning field segmentation models using unsupervised learning. In two different domains, classified advertisements and bibliographic citations, we showed that by constraining the model class we were able to restrict the search space of EM to models of interest. We used unsupervised learning methods with 400 documents to yield field segmentation models of a sim</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>D. Blei and P. Moreno. 2001. Topic segmentation with an aspect hidden Markov model. In Proceedings of the 24th SIGIR, pages 343–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="28782" citStr="Hearst (1997)" startWordPosition="4744" endWordPosition="4745">ation ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a complex and special purpose algorithm, which is difficult to adapt, while we see our contribution to be a demonstration of the interplay between model family and learned structure. Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper. Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work. 7 Conclusions In this work, we have examined the task of learning field segmentation models using unsupervised learning. In two different domains, classified advertisements and bibliographic citations, we showed that by constraining the model class we were able to restrict the search space of EM to models of interest. We used unsupervised learning methods with 400 documents to yield field segmentation models of a similar quality to those learned using supervised learning with 50 documents. We demonstrated that further refinements of the </context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. A. Hearst. 1997. TextTiling: Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
<author>J Rennie</author>
<author>K Seymore</author>
</authors>
<title>A machine learning approach to building domain-specific search engines.</title>
<date>1999</date>
<booktitle>In IJCAI-1999.</booktitle>
<contexts>
<context position="6992" citStr="McCallum et al. (1999)" startWordPosition="1102" endWordPosition="1105"> Penn Treebank, and tokens were normalized in various ways: numbers, dates, phone numbers, URLs, and email 1EM is a greedy hill-climbing algorithm designed for this purpose, but it is not the only option; one could also use coordinate ascent methods or sampling methods. addresses were collapsed to dedicated tokens, and all remaining tokens were converted to lowercase. Unless otherwise noted, the emission models use simple add-A smoothing, where A was 0.001 for supervised techniques, and 0.2 for unsupervised techniques. 3 Datasets and Evaluation The bibliographic citations data is described in McCallum et al. (1999), and is distributed at http://www.cs.umass.edu/~mccallum/. It consists of 500 hand-annotated citations, each taken from the reference section of a different computer science research paper. The citations are annotated with 13 fields, including author, title, date, journal, and so on. The average citation has 35 tokens in 5.5 fields. We split this data, using its natural order, into a 300- document training set, a 100-document development set, and a 100-document test set. The classified advertisements data set is novel, and consists of 8,767 classified advertisements for apartment rentals in t</context>
<context position="10307" citStr="McCallum et al. (1999)" startWordPosition="1641" endWordPosition="1644"> compute accuracy on a per-token basis.2 In evaluation of supervised methods, the model states and gold labels are the same. For models learned in a fully unsupervised fashion, we map each model state in a greedy fashion to the gold label to which it most often corresponds in the gold data. There is a worry with this kind of greedy mapping: it increasingly inflates the results as the number of hidden states grows. To keep the accuracies meaningful, all of our models have exactly the same number of hidden states as gold labels, and so the comparison is valid. 2This evaluation method is used by McCallum et al. (1999) but otherwise is not very standard. Compared to other evaluation methods for information extraction systems, it leads to a lower penalty for boundary errors, and allows long fields also contribute more to accuracy than short ones. 4 Unsupervised Learning Consider the general problem of learning an HMM from an unlabeled data set. Even abstracting away from concrete search methods and objective functions, the diversity and simultaneity of linguistic structure is already worrying; in Figure 1 compare the field structure in (a) and (b) to the parts-ofspeech in (c). If strong sequential correlatio</context>
<context position="14044" citStr="McCallum et al. (1999)" startWordPosition="2237" endWordPosition="2241"> As a final baseline, we trained a supervised firstorder HMM from the annotated training data using maximum likelihood estimation. With 100 training examples, supervised models achieve an accuracy of 74.4% on the advertisements data, and 72.5% on the citations data. With 300 examples, supervised methods achieve accuracies of 80.4 on the citations data. The learning curves of the supervised training experiments for different amounts of training data are shown in Figure 4. Note that other authors have achieved much higher accuracy on the the citation dataset using HMMs trained with supervision: McCallum et al. (1999) report accuracies as high as 92.9% by using more complex models and millions of words of BibTeX training data. 4.2 Unconstrained HMM Learning From the supervised baseline above we know that there is some first-order HMM over ISI states which captures the field structure we’re interested in, and we would like to find such a model without supervision. As a first attempt, we try fitting an unconstrained HMM, where the transition function is iniFigure 3: Matrix representations of typical transition models learned by initializing the transition model uniformly. tialized randomly, to the unannotate</context>
<context position="26569" citStr="McCallum et al. (1999)" startWordPosition="4391" endWordPosition="4394">ted citations boosts performance greatly from 65.2% to 71.3%. We also tested the utility of this approach in the classified advertisement domain, and found that it did not improve accuracy. We believe that this is because the transition information provided by the supervised data is very useful for the citations data, which has regular transition structure, but is not as useful for the advertisements data, which does not. 6 Previous Work A good amount of prior research can be cast as supervised learning of field segmentation models, using various model families and applied to various domains. McCallum et al. (1999) were the first to compare a number of supervised methods for learning HMMs for parsing bibliographic citations. The authors explicitly claim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the head377 er</context>
</contexts>
<marker>McCallum, Nigam, Rennie, Seymore, 1999</marker>
<rawString>A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 1999. A machine learning approach to building domain-specific search engines. In IJCAI-1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th ICML,</booktitle>
<pages>591--598</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="26840" citStr="McCallum et al. (2000)" startWordPosition="4433" endWordPosition="4436">ised data is very useful for the citations data, which has regular transition structure, but is not as useful for the advertisements data, which does not. 6 Previous Work A good amount of prior research can be cast as supervised learning of field segmentation models, using various model families and applied to various domains. McCallum et al. (1999) were the first to compare a number of supervised methods for learning HMMs for parsing bibliographic citations. The authors explicitly claim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the head377 ers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th ICML, pages 591–598. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pasula</author>
<author>B Marthi</author>
<author>B Milch</author>
<author>S Russell</author>
<author>I Shpitser</author>
</authors>
<title>Identity uncertainty and citation matching.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="27327" citStr="Pasula et al. (2002)" startWordPosition="4511" endWordPosition="4514">aim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the head377 ers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on. More recently, Barzilay and Lee (2004) defined content models, which can be viewed as field segmentation models occurring at the level of discourse. They perform unsupervised learning of these models from sets of news articles which describe similar event</context>
</contexts>
<marker>Pasula, Marthi, Milch, Russell, Shpitser, 2002</marker>
<rawString>H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2002. Identity uncertainty and citation matching. In Proceedings of NIPS 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate information extraction from research papers using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="27050" citStr="Peng and McCallum (2004)" startWordPosition="4466" endWordPosition="4469">e cast as supervised learning of field segmentation models, using various model families and applied to various domains. McCallum et al. (1999) were the first to compare a number of supervised methods for learning HMMs for parsing bibliographic citations. The authors explicitly claim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the head377 ers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviatio</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate information extraction from research papers using Conditional Random Fields. In Proceedings ofHLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B-H Juang</author>
</authors>
<title>Fundamentals of Speech Recognition.</title>
<date>1993</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="19175" citStr="Rabiner and Juang, 1993" startWordPosition="3084" endWordPosition="3087">y to incorporate this knowledge into the model is to delete stopwords, which, while perhaps not elegant, has proven quite effective in the past. A better founded way of making certain words unavailable to the model is to emit those words from all states with equal probability. This can be accomplished with the following simple hierarchical mixture emission model Ph(w|s) = αP,(w) + (1 − α)P(w|s) where P, is the common word distribution, and α is 4While it may be surprising that disallowing reestimation of the transition function is helpful here, the same has been observed in acoustic modeling (Rabiner and Juang, 1993). 375 State 10 Most Common Words 1 . $ no ! month deposit , pets rent available 2 , . room and with in large living kitchen - 3 . a the is and for this to , in 4 [NUM1] [NUM0] , bedroom bath / - . car garage 5 , . and a in - quiet with unit building 6 - . [TIME] [PHONE] [DAY] call [NUM8] at (a) State 10 Most Common Words 1 [NUM2] bedroom [NUM1] bath bedrooms large sq car ft garage 2 $ no month deposit pets lease rent available year security 3 kitchen room new , with living large floors hardwood fireplace 4 [PHONE] call please at or for [TIME] to [DAY] contact 5 san street at ave st # [NUM:DDD]</context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>L. R. Rabiner and B.-H. Juang. 1993. Fundamentals of Speech Recognition. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="5589" citStr="Rabiner (1989)" startWordPosition="887" endWordPosition="888">ing in (c) which does not. 2 Hidden Markov Models Hidden Markov models (HMMs) are commonly used to represent a wide range of linguistic phenomena in text, including morphology, parts-ofspeech (POS), named entity mentions, and even topic changes in discourse. An HMM consists of a set of states S, a set of observations (in our case words or tokens) W, a transition model specifying P(stIst−1), the probability of transitioning from state st−1 to state st, and an emission model specifying P(w s) the probability of emitting word w while in state s. For a good tutorial on general HMM techniques, see Rabiner (1989). For all of the unsupervised learning experiments we fit an HMM with the same number of hidden states as gold labels to an unannotated training set using EM.1 To compute hidden state expectations efficiently, we use the Forward-Backward algorithm in the standard way. Emission models are initialized to almost-uniform probability distributions, where a small amount of noise is added to break initial symmetry. Transition model initialization varies by experiment. We run the EM algorithm to convergence. Finally, we use the Viterbi algorithm with the learned parameters to label the test data. All </context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>F Chen</author>
<author>K Popat</author>
<author>T Hofmann</author>
</authors>
<title>Text classification in a hierarchical mixture model for small training sets.</title>
<date>2001</date>
<booktitle>In CIKM ’01: Proceedings of the tenth international conference on Information and knowledge management,</booktitle>
<pages>105--113</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="20443" citStr="Toutanova et al. (2001)" startWordPosition="3338" endWordPosition="3341">nit private back a building floor Comm. *CR* . , and - the in a / is with : of for to (b) Figure 6: Selected state emissions from a typical model learned from unsupervised data using the constrained transition function: (a) with a flat emission model, and (b) with a hierarchical emission model. a new global free parameter. In such a model, before a state emits a token it flips a coin, and with probability α it allows the common word distribution to generate the token, and with probability (1−α) it generates the token from its state-specific emission model (see Vaithyanathan and Dom (2000) and Toutanova et al. (2001) for more on such models). We tuned α on the development set and found that a range of values work equally well. We used a value of 0.5 in the following experiments. We ran two experiments on the advertisements data, both using the fixed transition model described in Section 4.3 and the hierarchical emission model. First, we initialized the emission model of Pc to a general-purpose list of stopwords, and did not reestimate it. This improved the average accuracy from 70.0% to 70.9%. Second, we learned the emission model of Pc using EM reestimation. Although this method did not yield a significa</context>
</contexts>
<marker>Toutanova, Chen, Popat, Hofmann, 2001</marker>
<rawString>K. Toutanova, F. Chen, K. Popat, and T. Hofmann. 2001. Text classification in a hierarchical mixture model for small training sets. In CIKM ’01: Proceedings of the tenth international conference on Information and knowledge management, pages 105–113. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vaithyanathan</author>
<author>B Dom</author>
</authors>
<title>Model-based hierarchical clustering.</title>
<date>2000</date>
<booktitle>In UAI-2000.</booktitle>
<contexts>
<context position="20415" citStr="Vaithyanathan and Dom (2000)" startWordPosition="3333" endWordPosition="3336">a [NUM:DDDD] 6 of the yard with unit private back a building floor Comm. *CR* . , and - the in a / is with : of for to (b) Figure 6: Selected state emissions from a typical model learned from unsupervised data using the constrained transition function: (a) with a flat emission model, and (b) with a hierarchical emission model. a new global free parameter. In such a model, before a state emits a token it flips a coin, and with probability α it allows the common word distribution to generate the token, and with probability (1−α) it generates the token from its state-specific emission model (see Vaithyanathan and Dom (2000) and Toutanova et al. (2001) for more on such models). We tuned α on the development set and found that a range of values work equally well. We used a value of 0.5 in the following experiments. We ran two experiments on the advertisements data, both using the fixed transition model described in Section 4.3 and the hierarchical emission model. First, we initialized the emission model of Pc to a general-purpose list of stopwords, and did not reestimate it. This improved the average accuracy from 70.0% to 70.9%. Second, we learned the emission model of Pc using EM reestimation. Although this meth</context>
</contexts>
<marker>Vaithyanathan, Dom, 2000</marker>
<rawString>S. Vaithyanathan and B. Dom. 2000. Model-based hierarchical clustering. In UAI-2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>