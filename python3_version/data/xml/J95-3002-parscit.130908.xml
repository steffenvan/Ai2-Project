<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987559666666667">
Robust Learning, Smoothing, and
Parameter Tying on Syntactic Ambiguity
Resolution
</title>
<author confidence="0.996261">
Tung-Hui Chiang* Yi-Chung Lin*t
</author>
<affiliation confidence="0.978432">
National Tsing Hua University National Tsing Hua University
</affiliation>
<author confidence="0.973633">
Keh-Yih Su*
</author>
<affiliation confidence="0.966347">
National Tsing Hua University
</affiliation>
<bodyText confidence="0.997965941176471">
Statistical approaches to natural language processing generally obtain the parameters by using the
maximum likelihood estimation (MLE) method. The MLE approaches, however, may fail to achieve
good performance in difficult tasks, because the discrimination and robustness issues are not taken
into consideration in the estimation processes. Motivated by that concern, a discrimination- and
robustness-oriented learning algorithm is proposed in this paper for minimizing the error rate. In
evaluating the robust learning procedure on a corpus of 1,000 sentences, 64.3% of the sentences
are assigned their correct syntactic structures, while only 53.1% accuracy rate is obtained with
the MLE approach.
In addition, parameters are usually estimated poorly when the training data is sparse. Smooth-
ing the parameters is thus important in the estimation process. Accordingly, we use a hybrid
approach combining the robust learning procedure with the smoothing method. The accuracy rate
of 69.8% is attained by using this approach. Finally, a parameter tying scheme is proposed to
tie those highly correlated but unreliably estimated parameters together so that the parameters
can be better trained in the learning process. With this tying scheme, the number of parameters
is reduced by a factor of 2,000 (from 8.7 x 108 to 4.2 x 105), and the accuracy rate for parse
tree selection is improved up to 70.3% when the robust learning procedure is applied on the tied
parameters.
</bodyText>
<sectionHeader confidence="0.991247" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998577909090909">
Resolution of syntactic ambiguity has been a focus in the field of natural language
processing for a long time. Both rule-based and statistics-based approaches have been
proposed to attack this problem in the past. For rule-based approaches, knowledge is
induced by linguistic experts and is encoded in terms of rules. Since a huge amount
of fine-grained knowledge is usually required to solve ambiguity problems, it is quite
difficult for a rule-based approach to acquire such kinds of knowledge. In addition, the
maintenance of consistency among the inductive rules is by no means easy. Therefore, a
rule-based approach, in general, fails to attain satisfactory performance for large-scale
applications.
In contrast, a statistical approach provides an objective measuring function to eval-
uate all possible alternative structures in terms of a set of parameters. Generally, the
</bodyText>
<footnote confidence="0.6385105">
* National Tsing Hua University, Department of Electrical Engineering, Hsinchu, Taiwan 300, R.O.C.
Email: kysu@bdc.com.tw.
</footnote>
<note confidence="0.883533">
Â© 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999859078431373">
statistics of parameters are estimated from a training corpus by using well-developed
statistical theorems. The linguistic uncertainty problems can thus be resolved on a solid
mathematical basis. Moreover, the knowledge acquired by a statistical method is al-
ways consistent because all the data in the corpus are jointly considered during the
acquisition process. Hence, compared with a rule-based method, the time required
for knowledge acquisition and the cost needed to maintain consistency among the ac-
quired knowledge sources are significantly reduced by adopting a statistical approach.
Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) pro-
posed a unified scoring function for resolving syntactic ambiguity With that scoring
function, various knowledge sources can be unified in a uniform formulation. Previous
work has demonstrated that this scoring function is able to provide high discrimina-
tion power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991;
Su and Chang 1990). In this paper, we start with a baseline system based on this
scoring function, and then proceed with different proposed enhancement methods. A
test set of 1,000 sentences, extracted from technical manuals, is used for evaluation. A
performance of 53.1% accuracy rate for parse tree selection is obtained for the base-
line system, when the parameters are estimated by using the maximum likelihood
estimation (MLE) method.
Note that it is the ranking of competitors, instead of the likelihood value, that
directly affects the performance of a disambiguation task. Maximizing the likelihood
values on the training corpus, therefore, does not necessarily lead to the minimum
error rate. In addition, the statistical variations between the training corpus and real
tasks are usually not taken into consideration in the estimation procedure. Thus, min-
imizing the error rate on the training corpus does not imply minimizing the error rate
in the task we are really concerned with.
To deal with the problems described above, a variety of discrimination-based learn-
ing algorithms have been adopted extensively in the field of speech recognition (Bahl
et al. 1988; Katagiri et al. 1991; Su and Lee 1991, 1994). Among those approaches, the
robustness issue was discussed in detail by Su and Lee (1991, 1994) in particular, and
encouraging results were observed. In this paper, a discrimination oriented adaptive
learning algorithm is first derived based on the scoring function mentioned above
and probabilistic gradient descent theory (Amari 1967; Katagiri, Lee, and Juang 1991).
The parameters of the scoring function are then learned from the training corpus us-
ing the discriminative learning algorithm. The accuracy rate for parse tree selection is
improved to 56.4% when the discriminative learning algorithm is applied.
In addition to the discriminative learning algorithm described above, a robust
learning procedure is further applied in order to consider the possible statistical vari-
ations between the training corpus and the real task. The robust learning process
continues adjusting the parameters even though the input training token has been
correctly recognized, until the score difference between the correct candidate and the
top competitor exceeds a preset threshold. The reason for this is to provide a tolerance
zone with a large margin for better preserving the correct ranking orders for data in
real tasks. An accuracy rate of 64.3% for parse tree selection is attained after this robust
learning algorithm is used.
The above-mentioned robust learning procedure starts with the parameters ob-
tained by the maximum likelihood estimation method. However, the MLE is notori-
ously unreliable when there is insufficient training data. The MLE for the probability
of a null event is zero, which is generally inappropriate for most applications. To
avoid the sparse training data problem, the parameters are first estimated by various
parameter smoothing methods (Good 1953; Katz 1987). An accuracy rate for parse
tree selection is improved to 69.8% by applying the robust learning procedure to the
</bodyText>
<page confidence="0.986562">
322
</page>
<note confidence="0.999075">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.999786277777778">
smoothed parameters. This result demonstrates that a better initial estimate of the pa-
rameters gives the robust learning procedure a chance to obtain better results when
many local maximal points exist.
Finally, a parameter tying scheme is proposed to reduce the number of parameters.
In this approach, some less reliably estimated but highly correlated parameters are tied
together, and then trained through the robust learning procedure. The probabilities of
the events that never appear in the training corpus can thus be trained more reliably.
This hybrid (tying + robust learning) approach reduces the number of parameters by
a factor of 2,000 (from 8.7 x 108 to 4.2 x 105) and achieves 70.3% accuracy rate for parse
tree selection.
This paper is organized as follows. A unified scoring function used for integrating
knowledge from lexical and syntactic levels is introduced in Section 2. The results
of using the unified scoring function are summarized in Section 3. In Section 4, the
discrimination- and robustness-oriented learning algorithm is derived. The effects of
the parameter smoothing techniques on the robust learning procedure are investigated
in Section 5. Next, the parameter tying scheme used to enhance parameter training
and reduce the number of parameters is described in Section 6. Finally, we discuss
our conclusions and describe the direction of future work.
</bodyText>
<sectionHeader confidence="0.860324" genericHeader="method">
2. A Unified Probabilistic Score Function
</sectionHeader>
<bodyText confidence="0.999698375">
Linguistic knowledge, including knowledge of lexicon, syntax, and semantics, is es-
sential for resolving syntactic ambiguities. To integrate various knowledge sources in a
uniform formulation, a unified probabilistic scoring function was proposed by Su et al.
(1991). This scoring function has been successfully applied to resolve ambiguity prob-
lems in an English-to-Chinese machine translation system (BehaviorTran) (Chen et al.
1991) and a spoken language processing system (Su, Chiang, and Lin 1991; 1992). The
unified probabilistic scoring function derived for the syntactic disambiguation task is
summarized in the following sections.
</bodyText>
<subsectionHeader confidence="0.979025">
2.1 Definition
</subsectionHeader>
<bodyText confidence="0.999854888888889">
An illustration of syntactic ambiguities for an input sentence W (= wri1 = {w1, w2, â¢ â¢ â¢
wr,}) is shown in Figure 1, where w, (i = 1, n) stands for the ith word of the input
sentence. In this figure, Lexk (1 &lt; k &lt; m) stands for the kth lexical sequence out
of M possible sequences. Synj,k (1 &lt; j &lt; Nk) is the jth alternative syntactic structure
corresponding to Lexk, and Nk is the number of possible syntactic structures associated
with Lexk. The disambiguation process is formulated as the process of finding the most
plausible syntactic structure Syn for for the input word sequence. In other words, this
process is to find the index (j, k) such that P(Syn,Lexk I w&apos;il) represents the maximum
value among different syntactic structures, as shown in Equation 1:
</bodyText>
<equation confidence="0.8626594">
k) = argmax{P(Syni*, Lexk I wi)} (1)
The integrated scoring function for the syntactic structure Syn k is defined as
Score(Synj,k) P(Syni,k,Lexk
= P(Synj,k I Lexk,w7) x P(Lexk I wi) (2)
Ssyn (Syni,k ) x Siex(Lexk)
</equation>
<page confidence="0.99763">
323
</page>
<figure confidence="0.996372142857143">
Computational Linguistics Volume 21, Number 3
Lexi Syn11
Lex2 SynN
â¢
w={ vv, N... NI Lexk Syn.,*
Syn Nk,k
Lexm SynNitum
</figure>
<figureCaption confidence="0.998087">
Figure 1
</figureCaption>
<bodyText confidence="0.918804428571429">
An illustration of the syntactic ambiguities for an input word sequence W.
where Ssyn (Synj,k) (= P(Syni,k I Lexk, w7)) denotes the syntactic scoring function, and
Slex(Lexk)( P(Lexk = w7)) denotes the lexical scoring function.
In the following derivation, we assume that little additional information can be
provided by the words w7 to the syntactic structure Syni,k after the lexical sequence
Lexk is given.1 The syntactic scoring function is thus approximated as the following
equation:
</bodyText>
<equation confidence="0.80645975">
Ssyn(Syni,k) = P(Syni,k Lexk,w7) P(Synj,k I Lexk) (3)
Accordingly, the integrated scoring function P(SyniA,Lexk I w7) is represented as fol-
lows:
P(Synj,k,Lexk I w7) P(Synj,k I Lexk) x P(Lexk I will). (4)
</equation>
<bodyText confidence="0.995882">
Detailed discussion of the lexical and syntactic scoring functions is given in the fol-
lowing sections.
</bodyText>
<subsectionHeader confidence="0.999655">
2.2 Lexical Score Function
</subsectionHeader>
<bodyText confidence="0.99667">
The lexical score for the kth lexical sequence Lexk associated with the input word se-
quence w7 is expressed as follows (Chiang, Lin, and Su 1992):
</bodyText>
<equation confidence="0.952194">
Siex(Lexk) = P(Lexk = P (ckk&amp;quot;,7 wi)
</equation>
<footnote confidence="0.975279333333333">
1 Note that the effect of word sense on the syntax disambiguation task is considered in a semantic
scoring function, which is not discussed in this paper. Interested readers are referred to Chang, Luo,
and Su (1992).
</footnote>
<page confidence="0.982239">
324
</page>
<note confidence="0.984319">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<equation confidence="0.99802725">
P I 4:7) x P (ckk:7)
P(w7)
SL(Lexic)
P(w7)
</equation>
<bodyText confidence="0.999945333333333">
where 4,, stands for the part of speech assigned to w,. Since P(w7) is the same for
all possible lexical sequences, it can be ignored without affecting the final results.
Therefore, we use Sr, (.) instead of S,â(.) in the following derivation.
Like the standard tagging procedures (Garside, Leech, and Sampson 1987; Church
1989; Merialdo 1991), the probability terms P(w7 I ckk:in) and P(ckk:in) in Equation 5 can
be approximated as follows, respectively:
</bodyText>
<equation confidence="0.974657466666667">
c&apos; ckk:7)
P (w = HP (wi I w1, c) R HP (w I cu).
i=1 i-1
P (ckk:7) = HP (ck,i I ckkV)
n bigram model
HP(ck,i I ck,i_i), trigram model
Hp(ck,iI Ck,i -1, Ck,i
i=1
Therefore, the lexical score Skx(Lexk) is expressed as:
stex (Lexk)
III(ck,i Ckk:ii. 1) X P(WiI Ck,i)
1=1
Hp(ck,,I ck,i_i) x P(w I cu), bigram model
HP(Ck,i I Ck,i-1,Ck,i-2) X P(Wi I Ck,i), trigram model (8)
i=1
</equation>
<subsectionHeader confidence="0.999864">
2.3 Syntactic Scoring Function
</subsectionHeader>
<bodyText confidence="0.999873833333333">
Conventional stochastic context-free grammar (CFG) approaches (Wright and Wrigley
1991) evaluate the likelihood probability of a syntactic tree by computing the product
of the probabilities associated with the grammar rules being applied. Such a formu-
lation implies that the application of a rule is both independent of the applications
of the other rules, and independent of the context under which a context-free rule is
applied. However, a language that can be expressed with a CFG does not imply that
the associated rules can be applied in an independent and context-free manner, as im-
plicitly assumed by a stochastic context-free grammar approach. To include contextual
information and consider the relationship among the grammar rules, in this paper we
follow the formulation in Su et al. (1989, 1991) for syntactic score evaluation.
To show the computing mechanism for the syntactic score, we take the tree in Fig-
ure 2 as an example. The basic derivation of the syntactic score includes the following
</bodyText>
<page confidence="0.996379">
325
</page>
<figure confidence="0.6820062">
Computational Linguistics Volume 21, Number 3
ACTION
REDUCE SHIFT
A L8 = { A I
z L7 = { B C }
B C
z Nt.z.. z
L6 = { B F G }
L5 = { B F c4}
D E F G L4 = { B C3 c4)
1 t 1 t2 1 t4 1 t5 L3 = {
1 D E C3 C4 }
C C2 C3 C4 L2 = { D C2 C3 C4 }
1
L1 = { C1 C2 C3 C4 }
</figure>
<figureCaption confidence="0.896917">
Figure 2
</figureCaption>
<bodyText confidence="0.990818833333333">
The decomposition of a given syntactic tree X into different phrase levels.
steps. First, the tree is decomposed into a number of phrase levels, such as L1, L2, â¢ â¢ â¢ , L8
in Figure 2. A phrase level is a sequence of symbols (terminal or nonterminal) that
acts as an intermediate result in parsing the input sentence, and is also called a senten-
tial form in formal language theory (Hoperoft and Ullman 1974). In the second step, we
formulate the transition between phrase levels as a context-sensitive rewriting process.
With the formulation, each transition probability between two phrase levels is calcu-
lated by consulting a finite-length window that comprises the symbols to be reduced
and their left and right contexts.
Let the label t, in Figure 2 be the time index for the ith state transition, which
corresponds to a reduce action, and L, be the ith phrase level. Then the syntactic score
of the tree in Figure 2 is defined as:
</bodyText>
<equation confidence="0.999851">
Ssyn(Treex) P(L8, L7, â¢ â¢ â¢ L2 L1)
= P(L8 L7, . , ) X P(L7 I L6, . , X â¢ â¢ â¢ X P(L2 L1)
P(L8 I L7) x P(L7 I L6) x â¢ â¢ â¢ x P(L2 I L1). (9)
</equation>
<bodyText confidence="0.9997028">
The transition probability between two phrase levels, say P(L7 I L6), is the product of
the probabilities of two events. Taking P(L7 I L6) as an example, the first probability
corresponds to the event that {F, G} are the constituents to be reduced, and the sec-
ond probability corresponds to the event that they are reduced to C. The transition
probability can thus be expressed as follows:
</bodyText>
<equation confidence="0.9993565">
P(L7 I L6) =- P(F,G are reduced I input is {B, F, G})
x P(C FG I F,G are reduced; input is {B,F,G}). (10)
</equation>
<bodyText confidence="0.99988325">
According to the results of our experiments, the first term is equal to one in most
cases, and it makes little contribution to discriminating different syntactic structures.
In addition, to simplify the computation, we approximate the full context {B, F, G}
with a window of finite length around {F, G}. The formulation for the syntactic scoring
</bodyText>
<page confidence="0.998955">
326
</page>
<note confidence="0.986591">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.748885">
function can thus be expressed as follows:
</bodyText>
<equation confidence="0.992208333333333">
Ssyn (Treex) P(A I {0},B,C, {$}) x P(C I {B}, F, G, {$})
x â¢ â¢ â¢ x P(D fob {c2, c3, c4})
P(A I 17, B, C, 7.7) x P(C I 16,F, G, r6) x â¢ â¢ â¢ x P(D
</equation>
<bodyText confidence="0.999842904761905">
where Treex is the parse tree X, $ and 0 correspond to the end-of-sentence marker and
the null symbol, respectively; and l and r, represent the left and right contexts to be
consulted in the ith phrase level, respectively. In the above equation, it is assumed that
each phrase level is highly correlated with its immediately preceding phrase level but
less correlated with other preceding phrase levels. In other words, the inter-level cor-
relation is assumed to be a first-order Markov process. In addition, for computational
feasibility, only a finite number of left and right contextual symbols are considered in
the formulation. If M left context symbols and N right context symbols are consulted
in evaluating Equation 9, the model is said to operate in the LmRN mode.
Notice that the last formula in Equation 9 corresponds to the rightmost deriva-
tion sequence in a generalized LR parser with left and right contexts taken into account
(Su et al. 1991). Such a formulation is particularly useful for a generalized LR pars-
ing algorithm, in which context-sensitive processing power is desirable. Although the
context-sensitive model in the above equation provides the ability to deal with intra-
level context-sensitivity, it fails to catch inter-level correlation. In addition, the formulation
of Equation 9 will result in the normalization problem (Su et al. 1991; Briscoe and Carroll
1993) when various syntactic trees have different number of nodes. An alternative for-
mulation, which compacts the highly correlated phrase levels into a single one, was
proposed by Su et al. (1991) to resolve the normalization problem. For instance, for the
syntactic tree in Figure 2, the syntactic score for the modified formulation is expressed
as follows:
</bodyText>
<equation confidence="0.996816">
Ssy(Treex) P(L8, L7, L6 I L5) X P(Ls I L4) x P(L4, 1,3 I L2) X P(L2 I L1)
â¢-â¢-z% P(L8 I L5) X P(L5 I L4) X P(L4 I L2) X P(L2 I L1). (12)
</equation>
<bodyText confidence="0.999849684210527">
Each pair of phrase levels in the above equation corresponds to a change in the LR
parser&apos;s stack before and after an input word is consumed by a shift operation. Because
the total number of shift actions, equal to the number of product terms in Equation 12,
is always the same for all alternative syntactic trees, the normalization problem is
resolved in such a formulation. Moreover, the formulation in Equation 12 provides
a way to consider both intra-level context-sensitivity and inter-level correlation of the
underlying context-free grammar. With such a formulation, the capability of context-
sensitive parsing (in probabilistic sense) can be achieved with a context-free grammar.
It is interesting to compare our frameworks (Su et al. 1991) with the work by
Briscoe and Carroll (1993) on probabilistic LR parsing. Instead of assigning probabili-
ties to the production rules as a conventional stochastic context-free grammar parser
does, Briscoe and Carroll distribute probability to each state so that the probabilities
of the transitions from a state sum to one; the preference to a SHIFT action is based
on one right context symbol (i.e., the lookahead symbol), and the preference for a RE-
DUCE action depends on the lookahead symbol and the previous state reached after
the REDUCE action. With such an approach, it is very easy to implement (mildly)
context-sensitive probabilistic parsing on existing LR parsers, and the probabilities
can be easily trained. The probabilities assigned to the states implicitly imply different
preferences for left-hand side contextual environment of the reduced symbol, since a
</bodyText>
<page confidence="0.993661">
327
</page>
<note confidence="0.464746">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.9999781">
state, in general, can indicate part of the past parsing history (i.e., the left context)
from which the current reduced symbol follows.
However, because of the implicit encoding of the parsing history, a state may
fail to distinguish some left contextual environments correctly. This is not surprising,
because the LR parsing table generator would merge certain states according to the
context-free grammar and the closure operations on the sets of items. Therefore, there
are cases in which the same string is reduced, under different left contexts, to the same
symbol at the same state and return to the same state after reduction. For instance, if
several identical constructs, e.g., [X â+ a], are allowed in a recursive structure X*, and
the input contains a Y followed by three (or more) consecutive Xs, e.g., &amp;quot;YXXX,&amp;quot; then
the reduction of the second and third Xs will return to the same state after the same
rule is applied at that state. Under such circumstances, the associated probabilities
for these two REDUCE actions will be identical and thus will not reflect the different
preferences between them.
In our framework, it is easy to tell that the first REDUCE action is applied when
the two left context symbols are {Y,X}, and the second REDUCE is applied when the
left context is two Xs under an L2R1 mode of operation. Because such recursion is not
rare, for example, in groups of adjectives, nouns, conjunction constructs, prepositional
phrases in English, the estimated scores will be affected by such differences. In other
words, we use context symbols explicitly and directly to evaluate the probabilities
of a substructure instead of using the parsing state to implicitly encode past history,
which may fail to provide a sufficient characterization of the left context. In addition,
explicitly using the left context symbols allows easy use of smoothing techniques, such
as deleted interpolation (Bahl, Jelinek, and Mercer 1983), clustering techniques (Brown
et al. 1992), and model refinement techniques (Lin, Chiang, and Su 1994) to estimate the
probabilities more reliably by changing the window sizes of the context and weighting
the various estimates dynamically. This kind of improvement is desirable when the
training data is limited.
Furthermore, Briscoe and Carroll (1993) use the geometric mean of the probabil-
ities, not their product, as the preference score, to avoid biasing their procedure in
favor of parse trees that have a smaller number of nodes (i.e., a smaller number of
rules being applied.) The geometric mean, however, fails to fit into the probabilis-
tic framework for disambiguation. In our approach, such a normalization problem is
avoided by considering a group of highly correlated phrase levels as a single phrase
level and evaluating the sequence of transitions for such phrase levels between the
SHIFT actions. Alternatively, it is also possible to consider each group of highly corre-
lated phrase levels as a joint event for evaluating its probability when enough data is
available. The optimization criteria are thus not compromised by the topologies of the
parse trees, because the number of SHIFT actions (i.e., the number of input tokens) is
fixed for an input sentence.
</bodyText>
<sectionHeader confidence="0.988884" genericHeader="method">
3. Baseline Model
</sectionHeader>
<bodyText confidence="0.99889875">
To establish a benchmark for examining the power of the proposed algorithms, we
begin with a baseline system, in which the parameters are estimated by using the MLE
method. Later, we will show how to improve the baseline model with the proposed
enhancement mechanisms.
</bodyText>
<subsectionHeader confidence="0.999242">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9764195">
First of all, 10,000 parsed sentences generated by BehaviorTran (Chen et al. 1991), a
commercialized English-to-Chinese machine translation system designed by Behavior
</bodyText>
<page confidence="0.998994">
328
</page>
<note confidence="0.99306">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.9838283">
Design Corporation (BDC), were collected. The domain for this corpus is computer
manuals and documents. The correct parts of speech and parse trees for the collected
sentences were verified by linguistic experts. The corpus was then randomly parti-
tioned into a training set of 9,000 sentences and a test set of the remaining 1,000
sentences to eliminate possible systematic biases. The average number of words per
sentence for the training set and the test set were 13.9 and 13.8, respectively. In the
training set, there were 1,030 unambiguous sentences, while 122 sentences were un-
ambiguous in the test set. On the average, there were 34.2 alternative parse trees per
sentence for the training set, and 31.2 for the test set. If we excluded those unambigu-
ous sentences, there were 38.49 and 35.38 alternative syntactic structures per sentence
for the training set and the test set, respectively.
3.1.1 Lexicon and Phrase Structure Rules. In the current system, there are 10,418
distinct lexicon entries, extracted from the 10,000-sentence corpus. The grammar is
composed of 1,088 phrase structure rules that are expressed in terms of 35 terminal
symbols (parts of speech) and 95 nonterminal symbols.
3.1.2 Language Models. Usually, a more complex model requires more parameters;
hence it frequently introduces more estimation error, although it may lead to less
modeling error. To investigate the effects of model complexity and estimation error on
the disambiguation task, the following models, which account for various lexical and
syntactic contextual information, were evaluated:
</bodyText>
<listItem confidence="0.98768025">
1. Lex(L12) + Syn(L1): this model uses a bigram model in computing lexical
scores and the Ll mode of operation in computing syntactic scores. The
number of parameters required is (10,418 x 35) + (35 x 35) +
(96,699 x 95) = 9, 492,260.3
2. Lex(L2)+Syn(L1): this model uses a trigram model in computing lexical
scores and the Ll mode of operation in computing syntactic scores. The
number of parameters required is (10,418 x 35) + (35 x 35 x 35) +
(96,699 x 95) = 9,533,910.
3. Lex(L1)+Syn(L2): this model uses a bigram model in computing lexical
scores and the L2 mode of operation in computing syntactic scores. The
number of parameters required is (10,418 x 35) + (35 x 35) +
(96,699 x 95 x 95) = 873,014,330.
4. Lex(L2)+Syn(L2): this model uses a trigram model in computing lexical
scores and the L2 mode of operation in computing syntactic scores. The
number of parameters required is (10,418 x 35) + (35 x 35 x 35) +
(96,699 x 95 x 95) = 873, 055,980.
</listItem>
<bodyText confidence="0.957041777777778">
2 L1 means to consult one left-hand side part of speech, and L2 means to consult two left-hand side
parts of speech.
3 The number of parameters for Lex(L1) and Lex(L2) modes is (Nu, x Nt) N? and (Nu, x Nt)
respectively, where Nu,(= 10,418) stands for the number of words in the lexicon, and Nt(= 35) denotes
the number of distinct terminal symbols (parts of speech). The number of parameters for Syn(L1) and
Syn(L2) modes is Np x Nnt and Np x Nit, respectively, where Nnt(= 95) denotes the number of
nonterminal symbols, and N (= 96,699) is the number of patterns corresponding to all possible reduce
actions. Each pattern is represented as a pair of [current symbols, reduced symbol]. For instance,
[{B,C},{A}l is the pattern corresponding to the reduce action A &lt;â BC in Figure 2.
</bodyText>
<page confidence="0.991708">
329
</page>
<note confidence="0.361287">
Computational Linguistics Volume 21, Number 3
</note>
<subsubsectionHeader confidence="0.581456">
3.1.3 Performance Evaluations. We will evaluate the above-mentioned models in two
</subsubsectionHeader>
<bodyText confidence="0.998708285714286">
measures: accuracy rate and selection power. The measure of accuracy rate of parse tree
selection has been widely used in the literature. However, this measure is unable to
identify which model is better if the average number of alternative syntactic structures
in various tasks is different. For example, a language model with 91% accuracy rate
for a task with an average of 1.1 alternative syntactic structures per sentence, which
corresponds to the performance of random selection, is by no means better than the
language model that attains 70% accuracy rate when there are an average of 100
alternative syntactic structures per sentence. Therefore, a measure, namely Selection
Power (SP), is proposed in this paper to give additional information for evaluation.
SP is defined as the average selection factor (SF) of the disambiguation mechanism on
the task of interest. The selection factor for an input sentence is defined as the least
proportion of all possible alternative structures that includes the selected syntactic
structure.&apos; A smaller SP value would, in principle, imply better disambiguation power.
Formally, SP is expressed as
</bodyText>
<equation confidence="0.6895605">
1 (13)
SP E[SF1 Nâ 2_, sf (i) = N 2_, ân
</equation>
<bodyText confidence="0.999894">
where sf (i) = is the selection factor for the ith sentence; n, is the total number of
alternative syntactic structures for the ith sentence; r, is the rank of the most preferred
candidate. The selection power for a disambiguation mechanism basically serves as
an indicator of the selection ability that includes the most preferred candidate within
a particular (N-best) region. A mechanism with a smaller SP value is more likely to
include the most preferred candidate for some given N-best hypotheses.
In general, the measures of accuracy rate and the selection power are highly cor-
related. But it is more informative to report performance with both accuracy rate and
selection power. Selection power supplements accuracy rate when two language mod-
els to be compared are tested on different tasks.
</bodyText>
<subsectionHeader confidence="0.999966">
3.2 Summary of Baseline Results
</subsectionHeader>
<bodyText confidence="0.974056263157895">
The performances of the various models in terms of accuracy rate and selection power
are shown in Table 1; the values in the parentheses correspond to performance exclud-
ing unambiguous sentences. Table 1 shows that better performance (both in terms of
accuracy rate and selection power) can be attained when more contextual informa-
tion is consulted (or when more parameters are used). The improvement in resolution
of syntactic ambiguity by using more lexical contextual information, however, is not
statistically significant&apos; when the consulted contextual information in the syntactic
models is fixed. For instance, the test set performance for the Lex(L1)+Syn(L2) model
is 52.8%, while the performance for the Lex(L2)+Syn(L2) model is only 53.1%. With this
small performance difference, we cannot reject the hypothesis that the performance of
the Lex(L1)+Syn(L2) model is the same as that of the Lex(L1)+Syn(L2) model. On the
other hand, if the consulted lexical contexts are fixed, the performance of the syntactic
disambiguation process is improved significantly by using more syntactic contextual
4 The term &amp;quot;most preferred candidate&amp;quot; means the syntactic structure most preferred by people even
when there is more than one arguably correct syntactic structure. However, throughout this paper, both
the expressions &amp;quot;most preferred syntactic structure&amp;quot; and &amp;quot;correct syntactic structure&amp;quot; refer to the syntactic
structure most preferred by our linguistic experts.
5 The conclusions drawn throughout this paper are all examined based on the testing hypothesis
procedure for a significance level a = 0.01 (Gillick and Cox 1989).
</bodyText>
<page confidence="0.997648">
330
</page>
<note confidence="0.99566">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<tableCaption confidence="0.998525">
Table 1
</tableCaption>
<table confidence="0.7323084">
The baseline performance: (a) training set; (b) test set. Values in parentheses correspond to
performance excluding unambiguous sentences.
Part-of-Speech Accuracy Rate Parse Tree
in Word in Sentence Accuracy Rate Selection
Model (%) (%) (%) Power
</table>
<figure confidence="0.904621230769231">
Lex(L1)+Syn(L1) 99.62 (99.59) 95.6 (95.0) 75.4 (72.3) 0.34 (0.26)
Lex(L2)+Syn(L1) 99.64 (99.61) 95.9 (95.4) 75.8 (72.7) 0.34 (0.26)
Lex(L1)+Syn(L2) 99.67 (99.64) 96.1 (95.6) 78.7 (75.9) 0.34 (0.25)
Lex(L2)+Syn(L2) 99.69 (99.67) 96.5 (96.0) 79.0 (76.4) 0.33 (0.25)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
in Word in Sentence Accuracy Rate Selection
Model (%) (%) (%) Power
Lex(L1)+Syn(L1) 98.89 (98.80) 88.7 (87.13) 49.3 (42.3) 0.45 (0.38)
Lex(L2)+Syn(L1) 98.93 (98.84) 88.9 (87.36) 49.7 (42.7) 0.45 (0.38)
Lex(L1)+Syn(L2) 98.82 (98.71) 88.0 (86.33) 52.8 (46.2) 0.44 (0.37)
Lex(L2)+Syn(L2) 98.89 (98.79) 88.5 (86.90) 53.1 (46.6) 0.44 (0.37)
(b) Test set performance
</figure>
<bodyText confidence="0.99985225">
information. For example, a 53.1% accuracy rate is attained for the Lex(L2)+Syn(L2)
model, while the accuracy rate is 49.7% for the Lex(L2)+Syn(L1) model. This result
indicates that the context-free assumption adopted by most stochastic parsers might
not hold.
</bodyText>
<sectionHeader confidence="0.804666" genericHeader="method">
4. Discrimination- and Robustness-Oriented Learning
</sectionHeader>
<bodyText confidence="0.999951705882353">
Although MLE possesses many nice properties (Kendall and Stuart 1979), the criterion
of maximizing likelihood value is not equivalent to that of minimizing the error rate
in a training set. The maximum likelihood approach achieves disambiguation indirectly
and implicitly through the estimation procedure. However, correct disambiguation
only depends on the ranks, rather than the likelihood values, of the candidates. In other
words, correct recognition will still be obtained if the score of the correct candidate is
the highest, even though the likelihood values of the various candidates are estimated
poorly. Motivated by this concern, a discrimination-oriented learning procedure is
proposed in this paper to adjust the parameters iteratively such that the correct ranking
orders can be achieved.
A general adaptive learning algorithm for minimizing the error rate in the train-
ing set was proposed by Amari (1967) using the probability descent (PD) method.
The extension of PD, namely the generalized probability descent method (GPD), was
also developed by Katagiri, Lee, and Juang (1991). However, minimizing the error
rate in the training set cannot guarantee that the error rate in the test set is also min-
imized. Discrimination-based learning procedures, in general, tend to overtune the
training set performance unless the number of available data is several times larger
</bodyText>
<page confidence="0.986584">
331
</page>
<note confidence="0.448484">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999724125">
than the number of parameters (based on our experience). Overtuning the training
set performance usually causes performance on the test set to deteriorate. Hence, the
robustness issue, which concerns the possible statistical variations between the training
set and the test set, must be taken into consideration when we adopt an adaptive
learning procedure. In this section, we start with a learning algorithm derived from
the probabilistic descent procedure (Katagiri, Lee, and juang 1991). The robust learn-
ing algorithm explored by Su and Lee (1991, 1994) is then introduced to enhance the
robustness of the system.
</bodyText>
<subsectionHeader confidence="0.96974">
4.1 Discrimination-Oriented Learning
</subsectionHeader>
<bodyText confidence="0.999965666666667">
To link the syntactic disambiguation process with the learning procedure, a discrim-
ination function, namely g ,k(w7), for the syntactic tree Syni,k, corresponding to the
lexical sequence Lexk and the input sentence (or word sequence) &amp;it, is defined as
</bodyText>
<equation confidence="0.9930562">
gi,k (wiz ) = log P (Syn),k, Lexk I wi) (14)
Since log() is a monotonic increasing function, we can rewrite the criterion for syn-
tactic disambiguation in Equation 1 as the following equation:
= argmax{gi,k (w7 )1 (15)
,k
</equation>
<bodyText confidence="0.980906666666667">
According to Equation 2, Equation 8, and Equation 12, the discrimination function
can be further derived as follows:
gi,k log Ssyn (Syni) + log S (Lexk) (16)
= - 1E1 [- logP [_ logP (ck,i ckk:ii-1,w7)]
i=1 i=1
In n
â E As2y, (j, i) +
i=1
= -114) j,k112
where Alex(k, i) = [- log P(co I ckk:ii-1,w7)11/2; Awl( j=
) [- log P(I,j,, I L-1)] /2. (Di), =
[)&apos;.syn (i, 1), Alex (k, 1), â¢ â¢ , Asyn (j, Alex (k, n)] is regarded as a parameter vector composed
of the lexical and syntactic score components, and I is defined as the Euclidean
norm of the vector 43j,k. However, in such a formulation, the lexical scores as well as the
syntactic scores are assumed to contribute equally to the disambiguation process. This
assumption is inappropriate because different linguistic information may contribute
differently to various disambiguation tasks. Moreover, the preference scores related to
various types of linguistic information may have different dynamic ranges. Therefore,
different scores should be assigned different weights to account for both the contri-
bution in discrimination and the dynamic ranges. The discrimination function is thus
modified into the following form:
</bodyText>
<equation confidence="0.992797">
g j,k = Wsyn synU i) W lex E Aiex (k, i)
(1) j,k 2 (17)
</equation>
<page confidence="0.99453">
332
</page>
<note confidence="0.996362">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.999414333333333">
where zoâ and wsyn stand for the lexical and syntactic weights, respectively; they are
set to 1.0 initially. â ij,k corresponds to a transformation of the original vector 1.j,k and
is represented as the following equation:
</bodyText>
<equation confidence="0.998887">
j,k = [WlynAsyn(j,1), W ilexAlex(k, 1),. â¢ â¢ , wlyn Asp., (j, n), wIlexAtex(k, n)]
[aksyn(j,1), (k, 1), . . , :\syn(j, n), 5kiex(k, n)] (18)
</equation>
<bodyText confidence="0.995712166666667">
The whole parameter set, denoted by A, thus includes the lexical weight, wtex, the
syntactic weight, wsyn, the lexical parameters Alex = {Atex(0)}vid and the syntactic
parameters Asyn = {Asyn (0) }v,,j; i.e.,
A = {wiex, wsyn } u Alex U Asyn (19)
The decision rule for the classifier to select the desired output, according to Eq. (17),
is represented as follows:
</bodyText>
<equation confidence="0.918567">
(j, ic) = argmax gpc (W7 )
j,k
or
(j, k) = argmax â
</equation>
<bodyText confidence="0.979052">
Let the correct syntactic structure associated with the input sentence be Syne.
Then the misclassification distance, denoted by d 5.,k, for selecting the syntactic structure
</bodyText>
<equation confidence="0.925334">
Syn ik as the final output is defined by the following equation:
i
d3k(w7; A) = [-ga,0 (w7)] -
3,k (21)
</equation>
<bodyText confidence="0.972504666666667">
Such a definition makes the distance be the difference of the lengths (or norms) of the
score vectors in the parameter space. Furthermore, di,k is differentiable with respect
to the parameters. Note that according to the definition in Equation 21, an error will
</bodyText>
<equation confidence="0.936489">
occur if c &gt; 0 i.e., 10Â«,011 &gt;
3,k (i3j,k
</equation>
<bodyText confidence="0.9838326">
Next, similar to the probabilistic-descent approach (Amari 1967), a loss function
13,1(A) is defined as a nondecreasing and differentiable function of the misclassification
distance; i.e., 111(A) = 1(c 1 0,(w&apos;il; A)). To approximate the zero-one loss function defined
for the minimum-error-rate classification, the loss function, as in Amari (1967), is de-
fined as
</bodyText>
<equation confidence="0.629151666666667">
1(d ) =,k
{ tan-1 (2!)d3 &gt;
0 otherwise (22)
</equation>
<bodyText confidence="0.998548333333333">
where do is a small positive constant. It has been proved by Amari (1967) that the
average loss function will decrease if the adjustments in the learning process satisfy
the following equation:
</bodyText>
<equation confidence="0.983646">
ANA = At + 5A,
.511t = (23)
(20)
</equation>
<page confidence="0.991228">
333
</page>
<note confidence="0.656096">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999700166666667">
where e(t) is a positive function, which usually decreases with time, to control the
convergence speed of the learning process; LI is a positive-definite matrix, which is
assumed to be an identity matrix in the current implementation, and V is the gradient
operator. Hence, it follows from Equation 23 that the ith syntactic parameter compo-
nent 411) (a, i), corresponding to the correct candidate, Syncâ,0 in the (t + 1)-th iteration,
would be adjusted according to the following equation:
</bodyText>
<equation confidence="0.86351475">
AgI1) (a, i) = 4t )n (a, i) + A A syni.) (a, i),
{
Ayt;;1) (a, i) = )4t )n (a, i), if II II &gt; II :13 j,kII,
otherwise,
where A4t)1 (a, i) is the amount of adjustment and is computed as follows:
do Ag)n (a, i)
â (t) d2 + 4 Wsyn
j,k
</equation>
<bodyText confidence="0.9852675">
Meanwhile, the syntactic parameter component corresponding to the top incorrect can-
didate would be adjusted according to the following formulae:
</bodyText>
<equation confidence="0.9704674">
{4tI1) (i, i) = 4t)n Ci, i) â AAyt)n (j, i), if 0.,a II &gt; 1143 j,k11,
AV) ( i) = Ag)â(j,i), otherwise,
do w
= --6(t) + 4 sYn 114&gt;j,kil
3,k
</equation>
<bodyText confidence="0.996062">
The learning rules for adjusting the lexical parameters can be represented in a similar
manner:
</bodyText>
<listItem confidence="0.887666">
1. For the lexical parameters corresponding to the correct candidates:
</listItem>
<equation confidence="0.933739785714286">
{Ae,tx+1) 63, i) = 4etx)(0, i) â A Aet)x (0/ i), if 114011 &gt; Ilii 5,k11, (27)
A etx+1) (0, i) = 4etx.) (0, i), otherwise,
do lex
(0, i)
A4et)x 03, 0 = âc(t) d2 d2 Wlex -
1143) ce,011
2. For the lexical parameters corresponding to the top candidate:
At+1) (1c, i) =&apos;ex (k, i) â4t i), if &gt; 114&apos;5)(11,
lex
(28)
4e71) (ic, i) = 4etx) (1C, i), otherwise,
A4)x(ic, i) = --E(t) dO
2 2 Wlex
d + do j,kii
</equation>
<page confidence="0.995995">
334
</page>
<note confidence="0.996588">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.854495">
In addition, the syntactic and lexical weights are adjusted as follows:
</bodyText>
<equation confidence="0.999177166666667">
wg-nFl) ,__ z4 4_ Awct) ,
{
&apos;n if 11(13.,oll &gt; 0 j,k11,
(t+1) (t)
Wsyn = Wsyn, otherwise
â n
(t) â¬(t) do E )4n (a, i) E As2,u, o
2 d2 + d6
1143a,o II 11433,k11
T4etx+1) i4etx)
{ + Aw( et x) ,
i if 1143âa,3ll &gt; 114 )j,k11,
(t+1) (t)
Wlex = Wlex, otherwise,
ân
E (0, i)
1=1
(t) â¬(t) do
Lawlex = â¢
2 d2 + d2
0
2e (k,
i=1
II 4)a,o II 0j,k11
</equation>
<bodyText confidence="0.999985913043478">
As the parameters are adjusted according to the learning rules described above, the
score of the correct candidate will increase and the score of the incorrect candidate
will decrease from iteration to iteration until the correct candidate is selected.
The ratio of the syntactic weight to the lexical weight, i.e., wsyn/wtex, finally turns
out to be 1.3 for the Lex(L2)+Syn(L2) model after the discriminative learning procedure
is applied. This ratio varies with the adopted language models, but is always larger
than 1.0. This result matches our expectation, because the syntactic score should pro-
vide more discrimination power than the lexical score in the syntactic disambiguation
task.
The experimental results of using the discriminative learning procedure with 20
iterations are shown in Table 2. For comparison, the corresponding results before
learning, i.e., the baseline results, are repeated in the upper row of each table entry.
For the Lex(L2)+Syn(L2) model, the accuracy rate for parse tree disambiguation in the
trairting set is improved from 79.04% to 92.77%, which corresponds to a 65.5% error
reduction rate. However, only a 7.03% error reduction rate is observed in the test set,
from 53.10% to 56.40%. Similar tendencies are also observed for the other models.
Since the discriminative learning procedure only aims at minimizing the error
rate in the training set, the training set performance can usually be tuned very closely
to 100% when a large number of parameters are available. However, the performance
improvement for the test set is far less than that for the training set, since the statistical
variations between the training set and the test set are not taken into consideration
in the learning procedure. For investigating robustness issues in more detail, a robust
learning procedure and the associated analyses are provided in the following section.
</bodyText>
<subsectionHeader confidence="0.998956">
4.2 Robust Learning
</subsectionHeader>
<bodyText confidence="0.9999662">
As discussed in the previous section, the discriminative learning approach aims at
minimizing the training set errors. The error rate measured in the training set is, in
general, over-optimistic (Efron and Gong 1983), because the training set performance
can be tuned to approach 100% by using a large number of parameters. The parameters
obtained in such a way frequently fail to attain an optimal performance when used in
</bodyText>
<page confidence="0.993588">
335
</page>
<note confidence="0.439497">
Computational Linguistics Volume 21, Number 3
</note>
<tableCaption confidence="0.989948">
Table 2
</tableCaption>
<table confidence="0.961336884615385">
Performance with discriminative learning: (a) training set; (b) test set. Values in parentheses
correspond to performance excluding unambiguous sentences.
Model Part-of-Speech Accuracy Rate Parse Tree
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
Lex(L1)+Syn(L1) 99.62 (99.59) 95.57 (94.99) 75.43 (72.26) 0.34 (0.26)
+ Discrimination Learning 99.95 (99.94) 99.32 (99.23) 92.04 (91.02) 0.30 (0.21)
Lex(L2)+Syn(L1) 99.64 (99.61) 95.93 (95.41) 75.81 (72.69) 0.34 (0.26)
+ Discrimination Learning 99.97 (99.96) 99.53 (99.47) 92.29 (91.29) 0.30 (0.21)
Lex(L1)+Syn(L2) 99.67 (99.64) 96.07 (95.56) 78.69 (75.93) 0.34 (0.25)
+ Discrimination Learning 99.96 (99.95) 99.40 (99.32) 92.54 (91.58) 0.30 (0.21)
Lex(L2)+Syn(L2) 99.69 (99.67) 96.46 (96.00) 79.04 (76.34) 0.33 (0.25)
+ Discrimination Learning 99.97 (99.97) 99.61 (99.56) 92.77 (91.83) 0.30 (0.21)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
in Word in Sentence Accuracy Rate Selection
Model (%) (%) (%) Power
Lex(L1)+Syn(L1) 98.89 (98.80) 88.7 (87.1) 49.3 (42.3) 0.45 (0.38)
+ Discrimination Learning 98.82 (98.72) 88.0 (86.3) 55.5 (49.3) 0.42 (0.34)
Lex(L2)+Syn(L1) 98.93 (98.84) 88.9 (87.4) 49.7 (42.7) 0.45 (0.38)
+ Discrimination Learning 99.05 (98.97) 90.1 (88.7) 55.3 (49.1) 0.42 (0.34)
Lex(L1)+Syn(L2) 98.82 (98.71) 88.0 (86.3) 52.8 (46.3) 0.44 (0.37)
+ Discrimination Learning 98.88 (98.78) 88.2 (88.6) 56.6 (50.6) 0.42 (0.34)
Lex(L2)+Syn(L2) 98.89 (98.79) 88.5 (86.9) 53.1 (46.6) 0.44 (0.37)
+ Discrimination Learning 98.92 (98.83) 88.3 (86.7) 56.4 (50.3) 0.42 (0.34)
(b) Test set performance
</table>
<bodyText confidence="0.9919925">
a real application. This over-tuning phenomenon happens mainly because of the lack
of sufficient sampling data and the possible statistical variations between the training
set and the test set.
To achieve better performance for a real application, one must deal with statistical
variation problems. Most adaptive learning procedures stop adjusting the parameters
once the input training token has been classified correctly For such learning proce-
dures, the distance between the correct candidate and other competitive ones may be
too small to cover the possible statistical variations between the training corpus and
the real application. To remedy this problem, Su and Lee (1991, 1994) suggested that
the distance margin between the correct candidate and the top competitor should be
enlarged, even though the input token is correctly recognized, until the margin ex-
ceeds a given threshold. A large distance margin would provide a tolerance region
in the neighborhood of the decision boundary to allow possible data scattering in the
real applications (Su and Lee 1994). A promising result has been observed by applying
</bodyText>
<page confidence="0.998552">
336
</page>
<note confidence="0.997119">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.999595">
this robust learning procedure to recognize the alphabet of E-set in English (Su and
Lee 1991, 1994).
To enhance robustness, the learning rules from Equation 24 to Equation 30 are
modified as follows. Following the notations in the previous section, the correct syn-
tactic structure is denoted by Syna,,r3 , and the syntactic structure of the strongest com-
petitor is denoted by Syn 5J(, whose score may either rank first or second.
</bodyText>
<listItem confidence="0.681894">
1. For the syntactic and lexical parameters corresponding to the correct
candidate:
</listItem>
<equation confidence="0.974669538461538">
{4V)(a, 0 = A g) ti (a, i) + A A g)n ( a , i), if (IR H â 114).,a11) &lt;6
A(t+1) (a, i) = 4t )n (a, i), otherwise
(31)
{Ar1) (0, i) = A (t)ex (0, i) + ,6,Aet ,) (0, i), (14â11 â 1143c011) &lt; (32)
lex (0, i) = 4-)x(p,i), otherwise
2. For the syntactic and lexical parameters corresponding to the strongest
competitor:
{(t+1) (i , i) = 4t)n(j, i) â A Ag)n(j , i) , if (114 3 j,k11 â 114)a,a11) &lt;6,
)4t1-1) C 7/ i) = A yt)n (i, i), otherwise
A etx+1) (ic 0 = (t)( i) 0 _ A A: (k, i),
{
Aetx+1) (k, 0 = A lext) (IC , i), if (0 11 â 0.4311) &lt; 6 ,
otherwise
</equation>
<bodyText confidence="0.9984165">
The learning rules of the syntactic and lexical weights are modified as
follows:
</bodyText>
<equation confidence="0.999398692307692">
(t+1) (t) A (t)
1
Wsyn = Wsyn + zaWsyn
W(,1-1) â W()n, if (14 j,k11 â *oil) &lt; 6 ,
otherwise
{,,, &amp;quot;I(t+1) (t) , A (t)
lex â W -T-
lex &amp;quot;W lex&apos;
(t-1-1) (t)
wlex = W lex&amp;quot;
if (14 â 0.4311) &lt; 6,
otherwise
(36)
</equation>
<bodyText confidence="0.999824357142857">
The margin 6 in the above equations can be assigned either absolutely or relatively,
as suggested in Su and Lee (1991, 1994). Currently, the relative mode with a 30%
passing rate (i.e., 30% of the training tokens pass through the margin) is used in our
implementation.
The simulation results, compared with the results obtained by using the discrim-
inative learning procedure, are shown in Table 3. Table 3(a) shows that performances
with robust learning in the training set are a little worse than those with discrimination
learning for the L1 syntactic language models. Nevertheless, they are a little better for
the L2 syntactic language model. All these differences, however, are not statistically
significant. On the contrary, the results with robust learning for the test set, as shown
in Table 3(b), are much better in all cases. The robust learning procedure achieves
more than 8% improvement compared with the discriminative learning procedure for
all language models. It is evident that the robust learning procedure is superior to the
discriminative learning procedure in the test set.
</bodyText>
<page confidence="0.995716">
337
</page>
<note confidence="0.551903">
Computational Linguistics Volume 21, Number 3
</note>
<tableCaption confidence="0.991454">
Table 3
</tableCaption>
<table confidence="0.922594724137931">
Performance with robust learning: (a) training set; (b) test set. Values in parentheses
correspond to performance excluding unambiguous sentences.
Model *Learning Part-of-Speech Accuracy Rate Parse Tree
Procedure
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
+DL 99.95 (99.94) 99.32 (99.23) 92.04 (91.02) 0.42 (0.34)
Lex(L1)+Syn(L1) +RL 99.90 (99.89) 98.77 (98.61) 91.84 (90.79) 0.38 (0.29)
+DL 99.97 (99.96) 99.53 (99.47) 92.29 (91.29) 0.42 (0.34)
Lex(L2)+Syn(L1) +RL 99.92 (99.92) 99.06 (98.93) 92.08 (91.05) 0.38 (0.30)
+DL 99.96 (99.95) 99.40 (99.32) 92.54 (91.58) 0.42 (0.34)
Lex(L1)+Syn(L2) +RL 99.92 (99.92) 99.03 (98.91) 92.94 (92.03) 0.38 (0.30)
+DL 99.97 (99.97) 99.61 (99.56) 92.77 (91.83) 0.42 (0.34)
Lex(L2)+Syn(L2) +RL 99.93 (99.93) 99.19 (99.08) 93.12 (92.23) 0.38 (0.30)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
*Learning in Word in Sentence Accuracy Rate Selection
Model Procedure (%) (%) (%) Power
+DL 98.82 (98.72) 88.0 (86.3) 55.5 (49.3) 0.42 (0.34)
Lex(L1)+Syn(L1) +RL 99.23 (99.16) 91.5 (90.3) 63.8 (58.8) 0.38 (0.29)
+DL 99.05 (98.97) 90.1 (88.7) 55.3 (49.1) 0.42 (0.34)
Lex(L2)+Syn(L1) +RL 99.27 (99.21) 91.5 (90.3) 64.2 (59.2) 0.38 (0.29)
+DL 98.88 (98.78) 88.2 (88.6) 56.6 (50.6) 0.42 (0.34)
Lex(L1)+Syn(L2) +RL 99.19 (99.12) 90.9 (89.6) 63.7 (58.7) 0.38 (0.30)
+DL 98.92 (93.83) 88.3 (86.7) 56.4 (50.3) 0.42 (0.34)
Lex(L2)+Syn(L2) +RL 99.18 (99.10) 90.7 (89.4) 64.3 (59.3) 0.38 (0.30)
(b) Test set performance
*DL and RL denote &amp;quot;Discriminative Learning&amp;quot; and &amp;quot;Robust Learning,&amp;quot; respectively
5. Parameter Smoothing for Sparse Data
</table>
<bodyText confidence="0.999760333333333">
The above-mentioned robust learning algorithm starts with the initial parameters esti-
mated by using MLE method. MLE, however, frequently suffers from the large estima-
tion error caused by the lack of sufficient training data in many statistical approaches.
For example, MLE gives a zero probability to events that were never observed in the
training set. Therefore, MLE fails to provide a reliable result if only a small number of
sampling data are available. To overcome this problem, Good (1953) proposed using
Turing&apos;s formula as an improved estimate over the well-known MLE. In addition, Katz
(1987) proposed a different smoothing technique, called the Back-Off procedure, for
smoothing unreliably estimated n-gram parameters with their correlated (n-1)-gram
parameters. To investigate the effects of parameter smoothing on robust learning, both
these techniques are used to smooth the estimated parameters, and then the robust
learning procedure is applied based on those smoothed parameters. These two smooth-
</bodyText>
<page confidence="0.998682">
338
</page>
<note confidence="0.997109">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<bodyText confidence="0.9996715">
ing techniques are first summarized in the following section. The investigation for the
smoothing/robust learning hybrid approach is presented next.
</bodyText>
<subsectionHeader confidence="0.995989">
5.1 The Smoothing Procedures
</subsectionHeader>
<bodyText confidence="0.909884666666667">
5.1.1 Turing&apos;s Formula. Let N be the sample size (the number of training tokens) and
lir be the number of events that occur exactly r times. Then the following equation
holds:
</bodyText>
<equation confidence="0.99768">
N = r â¢ nr (37)
</equation>
<bodyText confidence="0.775369">
The maximum likelihood estimate PAR, for the probability of an event e occurring r times
is defined as follows:
</bodyText>
<equation confidence="0.97426675">
PML(e) (38)
The estimate based on Turing&apos;s formula (Good 1953) is given by the following equation:
PTU(e) = N71-* (39)
where r* (r + 1)n nr+1 (40)
The total probability estimate, using Turing&apos;s formula, for all the events that actu-
ally occurred in the sample space is equal to
EPT.(e) = â ni (41)
e:C(e)&gt;0
</equation>
<bodyText confidence="0.999933">
where C(e) stands for the frequency count of the event e in the sample. This, in turn,
leads to the following equation:
</bodyText>
<equation confidence="0.8251925">
(42)
e:C(e)=0
</equation>
<bodyText confidence="0.9287485">
According to Turing&apos;s formula, the probability mass n1/N is then equally distributed
over the events that never occur in the sample.
</bodyText>
<listItem confidence="0.657757666666667">
5.1.2 Back-off Procedure. Katz (1987) proposed a back-off procedure to estimate pa-
rameters for an m-gram model, i.e., the conditional probability of a word given the
(m-1) preceding words. This procedure is summarized as follows:
</listItem>
<figure confidence="0.79102425">
PBF (wm I 4-1 ) = { PTU(Wm I WT-1), if C(w) &gt; 0
a (w2&apos;)PBF(wm I al2m-1) if C(w) = 0, and C(w) &gt; 0,
PBF(Wm I U1211-1) if E c(wr) = 0
Wm
</figure>
<bodyText confidence="0.937208">
where
</bodyText>
<equation confidence="0.9920405">
1 _ E PBF(Wm wr-&apos;)
a(4-1) =
â PsF(wm w1)
wâ,:C(zer) &gt;0
</equation>
<page confidence="0.991066">
339
</page>
<note confidence="0.51001">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.438473">
is a normalized factor such that
</bodyText>
<equation confidence="0.812296">
E PBF(w. I wT) + E PBF(w. = 1 (45)
</equation>
<bodyText confidence="0.997766956521739">
Compared with Turing&apos;s formula, the probability for an m-gram that does not occur
in the sample is &amp;quot;backed off&amp;quot; to refer to its corresponding (m-1)-gram probability.
Table 4 gives the experimental results for using the maximum likelihood (ML), Tur-
ing (TU) and back-off (BF) estimation procedures. The results show that smoothing the
unreliable parameters degrades the training set performance; however, it improves the
performance for the test set. Among the estimators, the maximum likelihood estimator
provides the best results for the training set, but it is the worst on the test set. Both
Turing&apos;s and the back-off procedures perform better than the maximum likelihood
procedure. This means that smoothing unreliable parameters is absolutely essential if
only limited training data are available.
Compared with Turing&apos;s procedure, the back-off procedure is 1 â 2% worse in
all cases. After examining the estimated parameters by using these two smoothing
procedures, we found that some syntactic parameters for null events were assigned
very large values by the Back-Off procedure, while they were assigned small proba-
bilities by Turing&apos;s formula. A typical example is shown as follows. The reduce action
&amp;quot;n quan â+ NLM*&amp;quot; given the left contexts [P*, N2] never occurred in the training set.
But, the probability of P( n quan NLM* I [n quan] reduced; L2=P*, L1=N2) is finally
replaced by the probability of P( n quan NLM* [n quan] reduced) in the Back-Off
estimation procedure. Since the probability P( n quan NLM* I [n quan] reduced) has
a large value (= 0.25), the probability P( n quan NLM* I [n quan] are reduced; L2=P*,
L1=N2) is accordingly large also. From the estimation point of view, the parameters
for null events may be assigned better estimated values by using the Back-Off method;
however, these parameters do not necessarily guarantee that the discrimination power
will be better improved. Take the sentence &amp;quot;A stack of pinfeed paper three inches high may
be placed underneath it&amp;quot; as an example. The decomposed phrase levels and the corre-
sponding syntactic scores for the correct and the top candidate are shown in Table 5
(a) and (b), respectively. We find that the main factor affecting the tree selection is the
sixth phrase level, which corresponds to the reduce action &amp;quot;n quan NLM*&amp;quot; with the
left two contextual symbols P* and N2 for the top candidate. As described above, the
probability P( n quan --* NLM* [n quan] reduced; L2=P*, L1=N2) is assigned a large
value in the Back-Off estimation procedure. However, to correctly select the right syn-
tactic structure in this example, P( quan QUAN I [quan] reduced; L2=P*, L1=N2)
should be greater than P( n quan â+ NLM* I [n quan] reduced; L2=P*, L1=N2). This
requirement may not be met by any estimation procedure, since the above two prob-
abilities are estimated from two different outcome spaces (one conditioned on [quan],
and the other conditioned on [n, qua n]). Therefore, even though the Back-Off procedure
may give better estimates for the parameters, it cannot guarantee that the recognition
result can be improved. The comparison between Turing&apos;s procedure and the Back-
Off procedure thus varies in different cases. In fact, the Back-Off estimation did show
better results in our previous research (Lin, Chiang, and Su 1994). Nevertheless, we
will show in the next section that the selection of a smoothing method is not crucial
after the robust learning procedure has been applied.
Furthermore, comparing the results in Table 3 and Table 4, we find that the perfor-
mance with the robust learning procedure is much better than that with the smooth-
ing techniques. Although both the adaptive learning procedures and the smoothing
techniques show improvement, the robust learning procedure, which emphasizes dis-
</bodyText>
<page confidence="0.996882">
340
</page>
<note confidence="0.998276">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<tableCaption confidence="0.91764975">
Table 4
Performance for lexical and syntactic disambiguation with various estimators. Values in
parentheses correspond to performance excluding unambiguous sentences. (ML: Maximum
Likelihood estimator; TU: TUring&apos;s formula; BF: Back-Off technique.)
</tableCaption>
<table confidence="0.983455909090909">
Model Estimation Part-of-Speech Accuracy Rate Parse Tree
Method
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
ML 99.62 (99.59) 95.57 (94.99) 75.43 (72.26) 0.34 (0.26)
Lex(L1)+Syn(L1) TU 99.43 (99.38) 93.47 (92.62) 69.63 (65.71) 0.36 (0.28)
BF 99.40 (99.35) 93.18 (92.30) 67.33 (63.11) 0.37 (0.28)
ML 99.64 (99.61) 95.93 (95.41) 75.81 (72.69) 0.34 (0.26)
Lex(L2)+Syn(L1) TU 99.48 (99.44) 94.09 (93.32) 70.12 (66.26) 0.36 (0.28)
BF 99.45 (99.41) 93.81 (93.01) 67.86 (63.70) 0.37 (0.28)
ML 99.67 (99.64) 96.07 (95.56) 78.69 (75.93) 0.34 (0.25)
Lex(L1)+Syn(L2) TU 99.45 (99.40) 93.79 (92.99) 72.13 (68.53) 0.35 (0.27)
BF 99.39 (99.33) 93.03 (92.13) 67.48 (63.27) 0.36 (0.28)
ML 99.69 (99.67) 96.46 (96.00) 79.04 (76.34) 0.33 (0.25)
Lex(L2)+Syn(L2) TU 99.49 (99.45) 94.22 (93.48) 72.48 (68.92) 0.35 (0.27)
BF 99.44 (99.39) 93.56 (92.72) 67.87 (63.71) 0.36 (0.28)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
Estimation in Word in Sentence Accuracy Rate Selection
Model Method (%) (%) (%) Power
ML 98.89 (98.80) 88.7 (87.1) 49.3 (42.3) 0.45 (0.38)
Lex(L1)+Syn(L1) TU 99.03 (98.95) 89.5 (88.0) 53.9 (47.5) 0.43 (0.36)
BF 99.01 (98.92) 88.9 (87.4) 52.4 (45.8) 0.44 (0.36)
ML 98.93 (98.84) 88.9 (87.4) 49.7 (42.7) 0.45 (0.38)
Lex(L2)+Syn(L1) TU 99.08 (99.00) 90.0 (88.6) 54.3 (48.0) 0.43 (0.35)
BF 99.09 (99.01) 90.1 (88.7) 53.2 (46.7) 0.44 (0.36)
ML 98.82 (98.71) 88.0 (86.3) 52.8 (46.2) 0.44 (0.37)
Lex(L1)+Syn(L2) TU 98.98 (98.89) 89.1 (87.6) 56.5 (50.5) 0.42 (0.34)
BF 99.02 (98.94) 89.1 (87.6) 54.4 (48.1) 0.43 (0.35)
ML 98.89 (98.79) 88.5 (86.9) 53.1 (46.6) 0.44 (0.37)
Lex(L2)+Syn(L2) TU 99.05 (98.97) 89.7 (88.3) 56.6 (50.6) 0.42 (0.34)
BF 99.10 (99.02) 90.1 (88.7) 55.1 (48.9) 0.43 (0.35)
(b) Test set performance
</table>
<bodyText confidence="0.997081333333333">
crimination capability rather than merely improving estimation process, achieves a
better result. Since the philosophies of performance improvement for these two al-
gorithms are different (one from the estimation point of view and the other from the
discrimination point of view), it is interesting to combine these two algorithms and
investigate the effect of the robust learning procedure on the smoothed parameters.
Detailed discussion on this hybrid approach is addressed in the following section.
</bodyText>
<page confidence="0.99588">
341
</page>
<note confidence="0.628074">
Computational Linguistics Volume 21, Number 3
</note>
<tableCaption confidence="0.6770616">
Table 5
The decomposed phrase levels associated with the sentence &amp;quot;A stack of pinfeed paper three inches
high may be placed underneath it,&amp;quot; and the corresponding scores with the Back-Off estimation
method for (a) the correct candidate and (b) the top candidate. The shaded rows indicate the
different patterns between the two parse trees.
</tableCaption>
<table confidence="0.94444906060606">
word L2 L1 current symbols -r reduced symbol score
1 A $ $ art-DET -0.0030
2 stack $ $ DET n -r N2 -0.5483
3 of $ N2 p -4 -0.0072
4 Pinfeed N2 P&apos; n -&gt; N1 -0,4221
5 &apos;,ape, N2 P&apos; N1 n -&gt; N2 -0,3611
6 three P&apos; N2 . quan-QUAN 1111111111
7 inches N2 ()URN n -r n
-1.8434
8 high $ $ 142 P&apos; N2 OLJAN n a -) N3 -11924
9 may $ N3 modl mod 1 -0.5172
10 be $ N3 mod1 be -) AUX -0.0048
11 placed N3 AUX v --- V1 -0.4007
12 underneath AUX V1 p -âº P. -0.0044
13 it $ $ N3 AUX V1 P&apos; pron -+82 -1.7924
(a) Correct Candidate
word L2 L1 current symbols -r reduced symbol score
1 A $ $ art -r DET -0.0030
2 stack $ $ DET n -) N2 -0_5483
3 of $ N2 p -, P. -0.0072
4 pinfeed N2 P&apos; n -b N2 -0.3668
5 Palger P&apos; N2 n -) n -0.3528
-1.2297
10, ir P* N2 , _ â&gt; NLM* %
.p,quan
7 inches P&apos; N2 ULM&apos; n -, N2
8 high P&apos; N2 N2 a -) N3 -1.1606
9 may N2 N3 mod1 -r AUX -0.0199
10 be $ $ N2 P&apos; N2 N3 AUX v -r N3 -1.1606
11 placed $ N3 v -4 V1 -1.1324
12 underneath N3 V1 p -r P&apos; -0.0084
13 it $ $ N3 VI P&apos; pron â+82 -02500
(b) Tap Candidate
</table>
<subsectionHeader confidence="0.999785">
5.2 Robust Learning on the Smoothed Parameters
</subsectionHeader>
<bodyText confidence="0.999917444444444">
The hybrid approach first uses a smoothing technique to estimate the initial param-
eters. Afterwards, the robust learning procedure is applied based on the smoothed
parameters. The advantages of this approach are two-fold. First, the power of the
scoring function is enhanced since the smoothing techniques can reduce the estima-
tion errors, especially for unseen events. Second, the parameters estimated from the
smoothing techniques give the robust learning procedure a better initial point and are
more likely to reach a better solution when many local optima exist in the parameter
space. In other words, the smoothing techniques indirectly prevent the learning pro-
cess from being trapped in a poor local optimum, although reducing the estimation
</bodyText>
<page confidence="0.992268">
342
</page>
<note confidence="0.998291">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<tableCaption confidence="0.989316">
Table 6
</tableCaption>
<table confidence="0.958344378378378">
Performance with the smoothing/robust learning hybrid approach. Values in parentheses
correspond to performance excluding unambiguous sentences. (ML+RL: Maximum Likelihood
estimator/Robust Learning; TU+RL: TUring&apos;s formula/Robust Learning; BF+RL: Back-Off
technique/Robust Learning.)
Model Estimation/ Part-of-Speech Accuracy Rate Parse Tree
Learning
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
ML+RL 99.90 (99.89) 98.77 (98.61) 91.84 (90.79) 0.31 (0.22)
Lex(L1)+Syn(L1) TU+RL 99.88 (99.87) 98.59 (98.41) 90.89 (89.71) 0.31 (0.22)
BF+RL 99.88 (99.87) 98.57 (98.38) 90.76 (89.56) 0.31 (0.22)
ML+RL 99.92 (99.92) 99.06 (98.93) 92.08 (91.05) 0.31 (0.22)
Lex(L2)+Syn(L1) TU+RL 99.90 (99.89) 98.82 (98.67) 91.20 (90.06) 0.31 (0.22)
BF+RL 99.89 (99.89) 98.76 (98.59) 90.93 (89.76) 0.31 (0.22)
ML+RL 99.92 (99.92) 99.03 (98.91) 92.94 (92.03) 0.30 (0.21)
Lex(L1)+Syn(L2) TU+RL 99.90 (99.90) 98.89 (98.71) 91.72 (90.65) 0.31 (0.22)
BF+RL 99.89 (99.88) 98.74 (98.58) 91.18 (90.04) 0.31 (0.22)
ML+RL 99.93 (99.93) 99.19 (99.08) 93.12 (92.23) 0.30 (0.21)
Lex(L2)+Syn(L2) TU+RL 99.91 (99.90) 98.92 (98.78) 91.79 (90.73) 0.31 (0.22)
BF+RL 99.90 (99.89) 98.90 (98.76) 91.40 (90.29) 0.31 (0.22)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
Estimation/ in Word in Sentence Accuracy Rate Selection
Model Learning (%) (%) (%) Power
ML+RL 99.23 (99.16) 91.5 (90.3) 63.8 (58.8) 0.38 (0.30)
Lex(L1)+Syn(L1) TU+RL 99.37 (99.32) 92.3 (91.7) 67.1 (62.5) 0.37 (0.28)
BF+RL 99.34 (99.28) 92.3 (91.2) 67.1 (62.5) 0.37 (0.28)
ML+RL 99.27 (99.21) 91.5 (90.3) 64.2 (59.2) 0.38 (0.29)
Lex(L2)+Syn(L1) TU+RL 99.39 (99.33) 92.8 (91.8) 68.0 (63.6) 0.37 (0.28)
BF+RL 99.36 (99.30) 92.5 (91.5) 67.9 (63.4) 0.37 (0.28)
ML+RL 99.19 (99.12) 90.9 (89.6) 63.7 (58.7) 0.38 (0.30)
Lex(L1)+Syn(L2) TU+RL 99.38 (99.32) 92.9 (91.9) 69.3 (65.0) 0.37 (0.28)
BF+RL 99.37 (99.32) 92.8 (91.8) 69.1 (64.8) 0.37 (0.28)
ML+RL 99.18 (99.10) 90.7 (89.4) 64.3 (59.3) 0.38 (0.30)
Lex(L2)+Syn(L2) TU+RL 99.45 (99.40) 93.7 (92.8) 69.8 (65.6) 0.37 (0.28)
BF+RL 99.39 (99.34) 93.3 (92.4) 69.2 (64.9) 0.37 (0.28)
(b) Test set performance
</table>
<bodyText confidence="0.8174515">
errors by using these methods does not directly improve the discrimination capabil-
ity Experimental results using this hybrid approach are shown in Table 6, where the
results using the (ML+RL) mode are also listed for reference.
Significant improvement, compared with the (ML+RL) mode, has been observed
</bodyText>
<page confidence="0.997038">
343
</page>
<note confidence="0.751425">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999897857142857">
by using the smoothed parameters at the initial step before the robust learning proce-
dure is applied. With this hybrid approach, better results are obtained using a more
complex language model, such as Lex(L2)+Syn(L2). However, there is no significant
performance difference achieved by using the (TU+RL) and the (BF+RL) approaches
for all language models, even though Turing&apos;s smoothing formula was shown to be-
have better than the Back-Off procedure before applying the robust learning proce-
dure. This is not surprising because starting the robust learning procedure with dif-
ferent initial points would still lead to the same local optimum if the starting region,
where the initial points are located, has only one local optimum. By using Turing&apos;s
formula/Robust Learning hybrid approach for the Lex(L2)+Syn(L2) model, the ac-
curacy rate for parse tree selection is improved to 69.2%, which corresponds to a
34.3% error reduction compared with the baseline of 53.1% accuracy. The superiority
in terms of both discrimination and robustness for the hybrid approach is thus clearly
demonstrated.
</bodyText>
<sectionHeader confidence="0.952227" genericHeader="method">
6. Parameter Tying
</sectionHeader>
<bodyText confidence="0.999988235294118">
The investigation described in Section 5 has shown that smoothing is essential before
the robust learning procedure is applied. Nevertheless, although we get better initial
estimates by smoothing parameters corresponding to rare events, these parameters
still cannot be trained well in the robust learning procedure, because such parameters
are seldom or never touched by the training process. Unfortunately, this problem
occurs frequently in statistical language modeling. This happens because, in general,
to reduce modeling errors, a model accounting for more contextual information is
desired. However, a model incorporating more contextual information would have
a larger number of null event parameters, which will not be touched in the learning
procedure.
To overcome this problem, a novel approach is proposed in this paper to train the
null event parameters by tying them to their highly correlated parameters, and then
adjusting them through the robust learning procedure. Basically, the reasons for using
this approach are two-fold. First, the number of parameters can be reduced by using
the tying scheme. Secondly, this tying scheme gives parameters for rare events more
chance to be touched in the learning procedure and thus they can be trained more
reliably. The details are addressed below.
</bodyText>
<subsectionHeader confidence="0.999708">
6.1 Tying Procedure
</subsectionHeader>
<bodyText confidence="0.996121">
The tying procedure includes the following two steps:
</bodyText>
<listItem confidence="0.732238">
1. Initial Estimation: For an m-gram model, the conditional probability
</listItem>
<equation confidence="0.629897">
P(xm I 4-1) is estimated by the following equation:
EyEv c(xi,
</equation>
<bodyText confidence="0.758377666666667">
where V denotes the vocabulary and C(.) stands for the frequency count
of an event in the training set. If 7
-ryEV C(Xi,. . ,Xm-1,Y) &gt; Qd, where Qd
is a present threshold, it is assumed that the estimated value of
P(xm I xr-1) is reliable and no action is required in this situation. On the
other hand, if EEV 1 c(x/â¢â¢ / xn, y) &lt; Qd, the estimated value of
y - r
P(xm I x1) is regarded as unreliable. In this case, P(xm I 4-1) is
substituted by the smoothed value of the (m - 1)-gram probability
</bodyText>
<figure confidence="0.5614155">
P(xm I C(xi,
(46)
</figure>
<page confidence="0.998263">
344
</page>
<note confidence="0.998175">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<tableCaption confidence="0.997942">
Table 7
</tableCaption>
<table confidence="0.9502355">
The number of parameters before and after the tying process. Note that the parameters of
P(W I C) are not tied.
Model Number of Number of Number of
Lexical Parameter Syntactic Total
Parameter Parameters
P(WIC) P(CilCj)
Before Tying 304,630 1225 96699*95 9,492,260
Lex(L1)+Syn(L1) After Tying 304,630 760 98,195 403,595
Ratio 1.0 0.62 0.0106 0.0425
Before Tying 304,630 42875 96699*95 9,533,910
Lex(L2)+Syn(L1) After Tying 304,630 4,199 98,195 407,024
Ratio 1.0 0.098 0.0106 0.0427
Before Tying 304,630 1225 96699*(95*95) 873,014,330
Lex(L1)+Syn(L2) After Tying 304,630 760 112,114 417,504
Ratio 1.0 0.62 0.00013 0.000478
Before Tying 304,630 42875 96699*(95*95) 873,055,980
Lex(L2)+Syn(L2) After Tying 304,630 4,199 112,114 420,943
Ratio 1.0 0.098 0.00013 0.000482
</table>
<bodyText confidence="0.835032272727273">
P(xâ, x2m-1). Currently, Qd is set to ten times the size of the possible
outcomes of xnâ i.e., Qd = [10 x (the number of possible tags)] for the
part-of-speech transition parameters.
2. Tying Procedure: Consider the m-gram events {x1,.. â¢ , xm-i, Vy, E V,
which have the same (m-1)-gram history {xi, , }. Each of the
probabilities P(y, I xl, ,xm_i), `91y, E V is first assigned a smoothed
value in the above step. To give these parameters more chance to be
trained during the robust learning process, we tie together the
parameters whose corresponding events appear less than Qn times in the
training set. That is, the parameters P(yk I xi, x2, â¢ â¢ â¢ , xm_i), yk c V. are
tied if the associated events satisfy the following conditions:
</bodyText>
<equation confidence="0.879014">
Ec(xi, . â¢ â¢ ,xm_i,y,) &lt; Qd, and C(xi, â¢ â¢ â¢ , xtn-i, yk) &lt;Q, yk E V, (47)
y, EV
</equation>
<bodyText confidence="0.99888725">
where (2,, is currently set to 2.
The numbers of parameters before and after tying for each language model are tab-
ulated in Table 7. This table shows that the number of parameters is greatly reduced
after the tying process, especially for the L2 syntactic models.
</bodyText>
<subsectionHeader confidence="0.999883">
6.2 Robust Learning on the Tied Parameters
</subsectionHeader>
<bodyText confidence="0.999945285714286">
After the parameters are estimated and tied through the tying procedure, the robust
learning algorithm is applied on the tied parameters. The experimental results are
shown in Table 8. The results with the TU+RL hybrid approach are also listed for ref-
erence. The performance with the Tying/Robust Learning hybrid approach, as shown
in Table 8, deteriorates somewhat in the training set because the tying procedure de-
creases the modeling resolution. However, the test set performance with this hybrid
approach is slightly (but not significantly) better than the Turing&apos;s formula/Robust
</bodyText>
<page confidence="0.996505">
345
</page>
<note confidence="0.624963">
Computational Linguistics Volume 21, Number 3
</note>
<tableCaption confidence="0.7197775">
Table 8
Performance of different language models with the various hybrid approaches. Values in
parentheses correspond to performance excluding unambiguous sentences. TY+RL: TYing
parameters/Robust Learning.
</tableCaption>
<table confidence="0.97652472">
Model Estimation/ Part-of-Speech Accuracy Rate Parse Tree
Learning
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
TU+RL 99.88 (99.87) 98.59 (98.41) 90.89 (89.71) 0.31 (0.22)
Lex(L1)+Syn(L1) TY+RL 99.86 (99.85) 98.33 (98.12) 89.81 (88.49) 0.31 (0.23)
TU+RL 99.90 (99.89) 98.82 (98.67) 91.20 (90.06) 0.31 (0.22)
Lex(L2)+Syn(L1) TY+RL 99.87 (99.86) 98.48 (98.28) 89.89 (88.58) 0.31 (0.23)
TU+RL 99.90 (99.90) 98.89 (98.71) 91.72 (90.65) 0.30 (0.21)
Lex(L1)+Syn(L2) TY+RL 99.88 (99.87) 98.60 (98.42) 90.61 (89.40) 0.31 (0.22)
TU+RL 99.91 (99.90) 98.92 (98.78) 91.79 (90.73) 0.30 (0.21)
Lex(L2)+Syn(L2) TY+RL 99.89 (99.88) 98.80 (98.64) 90.71 (89.51) 0.31 (0.22)
(a) Training set performance
Part-of-Speech Accuracy Rate Parse Tree
Estimation/ in Word in Sentence Accuracy Rate Selection
Model Learning (%) (%) (%) Power
TU+RL 99.37 (99.32) 92.7 (91.7) 67.1 (62.5) 0.37 (0.28)
Lex(L1)+Syn(L1) TY+RL 99.36 (99.31) 92.8 (91.8) 67.5 (63.0) 0.37 (0.28)
TU+RL 99.39 (99.33) 92.8 (91.8) 68.0 (63.6) 0.37 (0.28)
Lex(L2)+Syn(L1) TY+RL 99.39 (99.33) 92.9 (91.9) 68.3 (63.9) 0.37 (0.28)
TU+RL 99.38 (99.32) 92.9 (91.9) 69.3 (65.0) 0.37 (0.28)
Lex(L1)+Syn(L2) TY+RL 99.39 (99.33) 92.9 (91.9) 69.4 (65.2) 0.36 (0.28)
TU+RL 99.45 (99.40) 93.7 (92.8) 69.8 (65.6) 0.37 (0.28)
Lex(L2)+Syn(L2) TY+RL 99.43 (99.38) 93.5 (92.6) 70.3 (66.2) 0.36 (0.27)
(b) Test set performance
</table>
<bodyText confidence="0.9989595">
Learning approach. In addition, it reduces the large number of parameters, and thus
greatly eases the memory constraints for implementing the system.
A summary illustrating the performance improvement by using the proposed
enhancement mechanisms for the Lex(L2)+Syn(L2) model is shown in Table 9. The
proposed tying approach, after being combined with the robust learning procedure,
significantly reduces the error rate compared with the baseline (36.67% error reduction
is achieved, from 53.1% to 70.3%). Moreover, the number of parameters is reduced to
less than 1/2000 of the original parameter space.
</bodyText>
<sectionHeader confidence="0.900251" genericHeader="conclusions">
7. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999589333333333">
An integrated scoring function capable of incorporating various knowledge sources to
resolve syntactic ambiguity problems is explored in this paper. In the baseline model,
the parameters are estimated by using the maximum likelihood method. The MLE
</bodyText>
<page confidence="0.996337">
346
</page>
<note confidence="0.998235">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<tableCaption confidence="0.997755">
Table 9
</tableCaption>
<table confidence="0.894444857142857">
Summary of performance for the Lex(L2)+Syn(L2) model using various performance
enhancement methods. Values in parentheses correspond to performance excluding
unambiguous sentences.
Model: Lex(L2)+Syn(L2) Testing Set Performance
Part-of-Speech Accuracy Rate Parse Tree
in Word in Sentence Accuracy Rate Selection
(%) (%) (%) Power
Baseline 98.89 (98.79) 88.50 (86.90) 53.1 (46.6) 0.44 (0.37)
+ Robust Learning 99.18 (99.10) 90.70 (89.41) 64.3 (59.3) 0.38 (0.30)
+ TU Smoothing 99.05 (98.97) 89.70 (88.27) 56.6 (50.6) 0.42 (0.34)
+ TU Smoothing 99.45 (99.40) 93.70 (92.82) 69.8 (65.6) 0.37 (0.28)
+ Robust Learning 99.43 (99.38) 93.50 (92.60) 70.3 (66.2) 0.36 (0.27)
+ Tying parameter
+ Robust Learning
</table>
<bodyText confidence="0.999101733333333">
approach fails to achieve satisfactory performance because the discrimination and ro-
bustness issues are not considered in the estimation process. To improve performance,
a discrimination- and robustness-oriented method is adopted to directly pursue the
correct ranking orders of possible alternative syntactic structures. In addition, this
learning procedure is able to resolve problems resulting from statistical variations
between the training corpus and real tasks.
The effects of parameter smoothing for null events with Turing&apos;s formula and
the Back-Off method are investigated in this paper. A better initial estimate of the
parameters makes the robust learning procedure achieve better performance when
many local optima exist in the parameter space. Significant improvement of 34.3%
error reduction rate is attained when we apply the robust learning procedure on the
smoothed parameters.
Finally, a parameter tying scheme for rare events is proposed so that the unreli-
ably estimated parameters are tied and trained together through the robust learning
procedure. Thus, this approach makes it possible to tune all the parameters through
the learning process. In addition, the number of parameters is significantly reduced
with the tying process. The reduction of the number of parameters is over 99% for
each language model. Moreover, the accuracy rate of 70.3% for parse tree selection, or
36.7% error reduction rate, is obtained by using this novel approach.
To explore the areas for further improving the system, the remaining errors have
been examined. It was found that a very large portion of errors result from attachment
problems, including prepositional phrase (PP) attachment and modification scope for
adverbial phrases, adjective phrases, and relative clauses, while less than 10% of the
errors arise because of incorrect part-of-speech tagging. To further improve the lexical
scoring module, some refinement mechanisms developed for our part-of-speech tagger
(Lin, Chiang, and Su 1994) will be incorporated into this system. As for the attachment
problems, we found that the system appears to have a preference for local attachment,
which is not always inappropriate. The current model fails to deal with such problems
because only syntactic information from two left contextual nonterminal symbols is
consulted for computation. To resolve the attachment problems, integrating seman-
</bodyText>
<page confidence="0.990554">
347
</page>
<note confidence="0.725357">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999899428571429">
tic information, such as word sense collocations, would be required. In addition, to
enable the system to take into account information associated with long-distance de-
pendency, we plan to modify the syntactic model so that it can evaluate structural
dependency across various subtrees in the parse history. A large number of parameters
will inevitably be required for such a formulation, and a large training corpus is thus
needed for training. A bootstrapping procedure for parameter estimation with respect
to a very large corpus, therefore, will be applied in future research.
</bodyText>
<sectionHeader confidence="0.990522" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98721075">
This research is supported by the R.O.C.
National Science Council under NSC
82-0408-E-007-059 project. We would like to
thank the Behavior Design Corporation
(BDC) for providing us with the parsed
corpus. Jing-Shin Chang has given valuable
suggestions for writing this paper, in
particular for the comparison with Briscoe
and Carroll&apos;s approach. Also, four
anonymous reviewers&apos; comments on earlier
drafts were very helpful to us in preparing
the final version.
</bodyText>
<sectionHeader confidence="0.992068" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99956238372093">
Amari, Shunichi (1967). &amp;quot;A theory of
adaptive pattern classifiers.&amp;quot; IEEE Trans.
on Electronic Computers EC-16,299-307.
Bahl, Lalit R.; Brown, Peter E; deSouza,
Peter V.; and Mercer, Robert L. (1988).
&amp;quot;A new algorithm for the estimation of
hidden Markov model parameters.&amp;quot; In
Proceedings, IEEE 1988 International
Conference on Acoustics, Speech, and Signal
Processing. New York, 493-496.
Bahl, Lalit R.; Jelinek, Frederick; and Mercer,
Robert (1983). &amp;quot;A maximum likelihood
approach to continuous speech
recognition.&amp;quot; IEEE Trans. on Pattern
Analysis and Machine Intelligence
PAMI-5(2), 179-190.
Briscoe, Ted, and Carroll, John (1993).
&amp;quot;Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars.&amp;quot;
Computational Linguistics, 19(1), 25-59.
Brown, Peter F.; Della Pietra, Vincert J.;
deSouza, Peter V.; Lai, Jenifer C.; and
Mercer, Robert L. (1992). &amp;quot;Class-based
n-gram models of natural language.&amp;quot;
Computational Linguistics, 18(4), 467-479.
Chang, Jing-Shin; Luo, Yi-Fen; and Su,
Keh-Yih (1992). &amp;quot;GPSM: A generalized
probabilistic semantic model for
ambiguity resolution.&amp;quot; In Proceedings, 30th
Annual Meeting of the Association for
Computational Linguistics. University of
Delaware, Newark, 177-184.
Chen, Shu-Chuan; Chang, Jing-Shin; Wang,
Jong-Nae; and Su, Keh-Yih (1991).
&amp;quot;ArchTran: A corpus-based
statistics-oriented English-Chinese
machine translation system.&amp;quot; In
Proceedings, Machine Translation Summit III.
Washington, D.C., 33-40.
Chiang, Tung-Hui; Lin, Yi-Chung; and Su,
Keh-Yih (1992). &amp;quot;Syntactic ambiguity
resolution using a discrimination and
robustness oriented adaptive learning
algorithm.&amp;quot; In Proceedings, Fifteenth
International Conference on Computational
Linguistics, Nantes, 352-358.
Church, Kenneth (1989). &amp;quot;A stochastic parts
program and noun phrase for unrestricted
text.&amp;quot; In Proceedings, IEEE 1989
International Conference on Acoustics, Speech,
and Signal Processing. Glasgow, 695-698.
Efron, Bradley, and Gong, Gail (1983). &amp;quot;A
leisurely look at the bootstrap, the
jackknife, and cross-validation.&amp;quot; The
American Statistics, 37(1), 36-48.
Garside, Roger; Leech, Geoffrey; and
Sampson, Geoffrey (1987). The
Computational Analysis of English: A
corpus-based approach. Longman.
Gillick, L. and Cox, S. J. (1989). &amp;quot;Some
statistical issues in the comparison of
speech recognition algorithm.&amp;quot; In
Proceedings, IEEE 1989 International
Conference on Acoustics, Speech, and Signal
Processing. Glasgow, 532-535.
Good, I. J. (1953). &amp;quot;The population
frequencies of species and the estimation
of population parameters.&amp;quot; Biometrika, 40,
237-264.
Hoperoft, John E., and Ullman, Jeffrey D.
(1974). Formal Languages and Their Relation
to Automata. Addison-Wesley.
Katagiri, Shigeru; Lee, Chin-Hui and Juang,
Biing-Hwang (1991). &amp;quot;New discriminative
training algorithm based on the
generalized probabilistic descent
method.&amp;quot; In Proceedings, 1991 IEEE
Workshop Neural Networks for Signal
Processing. Piscataway, New Jersey,
299-308.
Katz, Slava M. (1987). &amp;quot;Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer.&amp;quot; IEEE Transactions on Acoustics,
Speech and Signal Processing. ASSP-35,
</reference>
<page confidence="0.99863">
348
</page>
<note confidence="0.994387">
Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying
</note>
<reference confidence="0.999700833333333">
400-401.
Kendall, Maurice, and Stuart, Alan (1979).
The Advanced Theory of Statistics.
Macmillan.
Lin, Yi-Chung; Chiang, Tung-Hui; and Su,
Keh-Yih (1994). &amp;quot;Automatic model
refinementâwith an application to
tagging.&amp;quot; In Proceedings, 15th International
Conference on Computational Linguistics.
Kyoto, 148-153.
Merialdo, Bernard (1991). &amp;quot;Tagging text
with a probabilistic model.&amp;quot; In
Proceedings, the IEEE 1991 International
Conference on Acoustic, Speech, and Signal
Processing. Toronto, 809-812.
Su, Keh-Yih, and Chang, Jing-Shin (1988).
&amp;quot;Semantic and syntactic aspects of score
function.&amp;quot; In Proceedings, 12th International
Conference on Computational Linguistics.
Budapest, 22-27.
Su, Keh-Yih, and Chang, Jing-Shin (1990).
&amp;quot;Some key issues in designing MT
systems.&amp;quot; Machine Translation, 5(4),
265-300.
Su, Keh-Yih, and Lee, Chin-Hui (1991).
&amp;quot;Robustness and discrimination oriented
speech recognition using weighted HMM
and subspace projection approaches.&amp;quot; In
Proceedings, IEEE 1991 International
Conference on Acoustic, Speech, and Signal
Processing. Toronto, 541-544.
Su, Keh-Yih, and Lee, Chin-Hui (1994).
&amp;quot;Speech recognition using weighted
HMM and subspace projection
approaches.&amp;quot; IEEE Trans. on Speech and
Audio Processing, 2(1), 69-79.
Su, Keh-Yih; Chang, Jing-Shin; and Lin,
Yi-Chung (1992). &amp;quot;A discriminative
approach for ambiguity resolution based
on a semantic score function.&amp;quot; In
Proceedings, 1992 International Conference on
Spoken Language Processing. Banff, 149-152.
Su, Keh-Yih; Chiang, Tung-Hui; and Lin,
Yi-Chung (1991). &amp;quot;A robustness and
discrimination oriented score function for
integrating speech and language
processing.&amp;quot; In Proceedings, 2nd European
Conference on Speech Communication and
Technology. Genova, 207-210.
Su, Keh-Yih; Wang, Jong-Nae; Su, Mei-Hui;
and Chang, Jing-Shin (1991). &amp;quot;GLR
parsing with scoring.&amp;quot; In Generalized LR
Parsing, edited by Masaru Tomita, 93-112.
Kluwer Academic Publisher.
Su, Keh-Yih; Wang, Jong-Nae; Su, Mei-Hui;
and Chang, Jing-Shin (1989). &amp;quot;A
sequential truncation parsing algorithm
based on the score function.&amp;quot; In
Proceedings of 1989 International Workshop
on Parsing Technologies (IWPT-89).
Pittsburgh, 95-104.
Wright, J. H., and Wrigley, E. N. (1991).
&amp;quot;GLR parsing with probability.&amp;quot; In
Generalized LR Parsing, edited by Masaru
Tomita, 113-128. Kluwer Academic
Publisher.
</reference>
<page confidence="0.999245">
349
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684435">
<title confidence="0.994313666666667">Robust Learning, Smoothing, and Parameter Tying on Syntactic Ambiguity Resolution</title>
<author confidence="0.960924">Tung-Hui Chiang Yi-Chung Lint</author>
<affiliation confidence="0.975057">National Tsing Hua University National Tsing Hua University</affiliation>
<author confidence="0.822147">Keh-Yih Su</author>
<affiliation confidence="0.997845">National Tsing Hua University</affiliation>
<abstract confidence="0.993394470588235">Statistical approaches to natural language processing generally obtain the parameters by using the maximum likelihood estimation (MLE) method. The MLE approaches, however, may fail to achieve good performance in difficult tasks, because the discrimination and robustness issues are not taken into consideration in the estimation processes. Motivated by that concern, a discriminationand robustness-oriented learning algorithm is proposed in this paper for minimizing the error rate. In evaluating the robust learning procedure on a corpus of 1,000 sentences, 64.3% of the sentences are assigned their correct syntactic structures, while only 53.1% accuracy rate is obtained with the MLE approach. In addition, parameters are usually estimated poorly when the training data is sparse. Smoothing the parameters is thus important in the estimation process. Accordingly, we use a hybrid approach combining the robust learning procedure with the smoothing method. The accuracy rate of 69.8% is attained by using this approach. Finally, a parameter tying scheme is proposed to tie those highly correlated but unreliably estimated parameters together so that the parameters can be better trained in the learning process. With this tying scheme, the number of parameters reduced by a factor of 2,000 (from 8.7 x to 4.2 x and the accuracy rate for parse tree selection is improved up to 70.3% when the robust learning procedure is applied on the tied parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shunichi Amari</author>
</authors>
<title>A theory of adaptive pattern classifiers.&amp;quot;</title>
<date>1967</date>
<journal>IEEE Trans. on Electronic Computers</journal>
<pages>16--299</pages>
<contexts>
<context position="5364" citStr="Amari 1967" startWordPosition="809" endWordPosition="810"> rate in the task we are really concerned with. To deal with the problems described above, a variety of discrimination-based learning algorithms have been adopted extensively in the field of speech recognition (Bahl et al. 1988; Katagiri et al. 1991; Su and Lee 1991, 1994). Among those approaches, the robustness issue was discussed in detail by Su and Lee (1991, 1994) in particular, and encouraging results were observed. In this paper, a discrimination oriented adaptive learning algorithm is first derived based on the scoring function mentioned above and probabilistic gradient descent theory (Amari 1967; Katagiri, Lee, and Juang 1991). The parameters of the scoring function are then learned from the training corpus using the discriminative learning algorithm. The accuracy rate for parse tree selection is improved to 56.4% when the discriminative learning algorithm is applied. In addition to the discriminative learning algorithm described above, a robust learning procedure is further applied in order to consider the possible statistical variations between the training corpus and the real task. The robust learning process continues adjusting the parameters even though the input training token </context>
<context position="32546" citStr="Amari (1967)" startWordPosition="5250" endWordPosition="5251">owever, correct disambiguation only depends on the ranks, rather than the likelihood values, of the candidates. In other words, correct recognition will still be obtained if the score of the correct candidate is the highest, even though the likelihood values of the various candidates are estimated poorly. Motivated by this concern, a discrimination-oriented learning procedure is proposed in this paper to adjust the parameters iteratively such that the correct ranking orders can be achieved. A general adaptive learning algorithm for minimizing the error rate in the training set was proposed by Amari (1967) using the probability descent (PD) method. The extension of PD, namely the generalized probability descent method (GPD), was also developed by Katagiri, Lee, and Juang (1991). However, minimizing the error rate in the training set cannot guarantee that the error rate in the test set is also minimized. Discrimination-based learning procedures, in general, tend to overtune the training set performance unless the number of available data is several times larger 331 Computational Linguistics Volume 21, Number 3 than the number of parameters (based on our experience). Overtuning the training set p</context>
<context position="36914" citStr="Amari 1967" startWordPosition="5971" endWordPosition="5972"> associated with the input sentence be Syne. Then the misclassification distance, denoted by d 5.,k, for selecting the syntactic structure Syn ik as the final output is defined by the following equation: i d3k(w7; A) = [-ga,0 (w7)] - 3,k (21) Such a definition makes the distance be the difference of the lengths (or norms) of the score vectors in the parameter space. Furthermore, di,k is differentiable with respect to the parameters. Note that according to the definition in Equation 21, an error will occur if c &gt; 0 i.e., 10Â«,011 &gt; 3,k (i3j,k Next, similar to the probabilistic-descent approach (Amari 1967), a loss function 13,1(A) is defined as a nondecreasing and differentiable function of the misclassification distance; i.e., 111(A) = 1(c 1 0,(w&apos;il; A)). To approximate the zero-one loss function defined for the minimum-error-rate classification, the loss function, as in Amari (1967), is defined as 1(d ) =,k { tan-1 (2!)d3 &gt; 0 otherwise (22) where do is a small positive constant. It has been proved by Amari (1967) that the average loss function will decrease if the adjustments in the learning process satisfy the following equation: ANA = At + 5A, .511t = (23) (20) 333 Computational Linguistics</context>
</contexts>
<marker>Amari, 1967</marker>
<rawString>Amari, Shunichi (1967). &amp;quot;A theory of adaptive pattern classifiers.&amp;quot; IEEE Trans. on Electronic Computers EC-16,299-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter E Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
</authors>
<title>A new algorithm for the estimation of hidden Markov model parameters.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, IEEE 1988 International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>493--496</pages>
<location>New York,</location>
<contexts>
<context position="4981" citStr="Bahl et al. 1988" startWordPosition="749" endWordPosition="752">f a disambiguation task. Maximizing the likelihood values on the training corpus, therefore, does not necessarily lead to the minimum error rate. In addition, the statistical variations between the training corpus and real tasks are usually not taken into consideration in the estimation procedure. Thus, minimizing the error rate on the training corpus does not imply minimizing the error rate in the task we are really concerned with. To deal with the problems described above, a variety of discrimination-based learning algorithms have been adopted extensively in the field of speech recognition (Bahl et al. 1988; Katagiri et al. 1991; Su and Lee 1991, 1994). Among those approaches, the robustness issue was discussed in detail by Su and Lee (1991, 1994) in particular, and encouraging results were observed. In this paper, a discrimination oriented adaptive learning algorithm is first derived based on the scoring function mentioned above and probabilistic gradient descent theory (Amari 1967; Katagiri, Lee, and Juang 1991). The parameters of the scoring function are then learned from the training corpus using the discriminative learning algorithm. The accuracy rate for parse tree selection is improved to</context>
</contexts>
<marker>Bahl, Brown, deSouza, Mercer, 1988</marker>
<rawString>Bahl, Lalit R.; Brown, Peter E; deSouza, Peter V.; and Mercer, Robert L. (1988). &amp;quot;A new algorithm for the estimation of hidden Markov model parameters.&amp;quot; In Proceedings, IEEE 1988 International Conference on Acoustics, Speech, and Signal Processing. New York, 493-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.&amp;quot;</title>
<date>1983</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence</journal>
<volume>5</volume>
<issue>2</issue>
<pages>179--190</pages>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, Lalit R.; Jelinek, Frederick; and Mercer, Robert (1983). &amp;quot;A maximum likelihood approach to continuous speech recognition.&amp;quot; IEEE Trans. on Pattern Analysis and Machine Intelligence PAMI-5(2), 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--59</pages>
<contexts>
<context position="17248" citStr="Briscoe and Carroll 1993" startWordPosition="2824" endWordPosition="2827">mode. Notice that the last formula in Equation 9 corresponds to the rightmost derivation sequence in a generalized LR parser with left and right contexts taken into account (Su et al. 1991). Such a formulation is particularly useful for a generalized LR parsing algorithm, in which context-sensitive processing power is desirable. Although the context-sensitive model in the above equation provides the ability to deal with intralevel context-sensitivity, it fails to catch inter-level correlation. In addition, the formulation of Equation 9 will result in the normalization problem (Su et al. 1991; Briscoe and Carroll 1993) when various syntactic trees have different number of nodes. An alternative formulation, which compacts the highly correlated phrase levels into a single one, was proposed by Su et al. (1991) to resolve the normalization problem. For instance, for the syntactic tree in Figure 2, the syntactic score for the modified formulation is expressed as follows: Ssy(Treex) P(L8, L7, L6 I L5) X P(Ls I L4) x P(L4, 1,3 I L2) X P(L2 I L1) â¢-â¢-z% P(L8 I L5) X P(L5 I L4) X P(L4 I L2) X P(L2 I L1). (12) Each pair of phrase levels in the above equation corresponds to a change in the LR parser&apos;s stack before and</context>
<context position="18531" citStr="Briscoe and Carroll (1993)" startWordPosition="3042" endWordPosition="3045">cause the total number of shift actions, equal to the number of product terms in Equation 12, is always the same for all alternative syntactic trees, the normalization problem is resolved in such a formulation. Moreover, the formulation in Equation 12 provides a way to consider both intra-level context-sensitivity and inter-level correlation of the underlying context-free grammar. With such a formulation, the capability of contextsensitive parsing (in probabilistic sense) can be achieved with a context-free grammar. It is interesting to compare our frameworks (Su et al. 1991) with the work by Briscoe and Carroll (1993) on probabilistic LR parsing. Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a REDUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action. With such an approach, it is very easy to implement (mildly) context-sensitive probabilistic p</context>
<context position="21711" citStr="Briscoe and Carroll (1993)" startWordPosition="3543" endWordPosition="3546">te to implicitly encode past history, which may fail to provide a sufficient characterization of the left context. In addition, explicitly using the left context symbols allows easy use of smoothing techniques, such as deleted interpolation (Bahl, Jelinek, and Mercer 1983), clustering techniques (Brown et al. 1992), and model refinement techniques (Lin, Chiang, and Su 1994) to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically. This kind of improvement is desirable when the training data is limited. Furthermore, Briscoe and Carroll (1993) use the geometric mean of the probabilities, not their product, as the preference score, to avoid biasing their procedure in favor of parse trees that have a smaller number of nodes (i.e., a smaller number of rules being applied.) The geometric mean, however, fails to fit into the probabilistic framework for disambiguation. In our approach, such a normalization problem is avoided by considering a group of highly correlated phrase levels as a single phrase level and evaluating the sequence of transitions for such phrase levels between the SHIFT actions. Alternatively, it is also possible to co</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, Ted, and Carroll, John (1993). &amp;quot;Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot; Computational Linguistics, 19(1), 25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Della Pietra</author>
<author>J Vincert</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>467--479</pages>
<contexts>
<context position="21401" citStr="Brown et al. 1992" startWordPosition="3497" endWordPosition="3500"> example, in groups of adjectives, nouns, conjunction constructs, prepositional phrases in English, the estimated scores will be affected by such differences. In other words, we use context symbols explicitly and directly to evaluate the probabilities of a substructure instead of using the parsing state to implicitly encode past history, which may fail to provide a sufficient characterization of the left context. In addition, explicitly using the left context symbols allows easy use of smoothing techniques, such as deleted interpolation (Bahl, Jelinek, and Mercer 1983), clustering techniques (Brown et al. 1992), and model refinement techniques (Lin, Chiang, and Su 1994) to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically. This kind of improvement is desirable when the training data is limited. Furthermore, Briscoe and Carroll (1993) use the geometric mean of the probabilities, not their product, as the preference score, to avoid biasing their procedure in favor of parse trees that have a smaller number of nodes (i.e., a smaller number of rules being applied.) The geometric mean, however, fails to fit into the probabi</context>
</contexts>
<marker>Brown, Pietra, Vincert, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F.; Della Pietra, Vincert J.; deSouza, Peter V.; Lai, Jenifer C.; and Mercer, Robert L. (1992). &amp;quot;Class-based n-gram models of natural language.&amp;quot; Computational Linguistics, 18(4), 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Yi-Fen Luo</author>
<author>Keh-Yih Su</author>
</authors>
<title>GPSM: A generalized probabilistic semantic model for ambiguity resolution.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 30th Annual Meeting of the Association</booktitle>
<pages>177--184</pages>
<institution>for Computational Linguistics. University of Delaware,</institution>
<location>Newark,</location>
<marker>Chang, Luo, Su, 1992</marker>
<rawString>Chang, Jing-Shin; Luo, Yi-Fen; and Su, Keh-Yih (1992). &amp;quot;GPSM: A generalized probabilistic semantic model for ambiguity resolution.&amp;quot; In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics. University of Delaware, Newark, 177-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu-Chuan Chen</author>
<author>Jing-Shin Chang</author>
<author>Jong-Nae Wang</author>
<author>Su</author>
</authors>
<date>1991</date>
<contexts>
<context position="3807" citStr="Chen et al. 1991" startWordPosition="564" endWordPosition="567">ased method, the time required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach. Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) proposed a unified scoring function for resolving syntactic ambiguity With that scoring function, various knowledge sources can be unified in a uniform formulation. Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991; Su and Chang 1990). In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods. A test set of 1,000 sentences, extracted from technical manuals, is used for evaluation. A performance of 53.1% accuracy rate for parse tree selection is obtained for the baseline system, when the parameters are estimated by using the maximum likelihood estimation (MLE) method. Note that it is the ranking of competitors, instead of the likelihood value, that directly affects the performance of a disambiguation task. Maximizing the li</context>
<context position="8867" citStr="Chen et al. 1991" startWordPosition="1341" endWordPosition="1344">nd reduce the number of parameters is described in Section 6. Finally, we discuss our conclusions and describe the direction of future work. 2. A Unified Probabilistic Score Function Linguistic knowledge, including knowledge of lexicon, syntax, and semantics, is essential for resolving syntactic ambiguities. To integrate various knowledge sources in a uniform formulation, a unified probabilistic scoring function was proposed by Su et al. (1991). This scoring function has been successfully applied to resolve ambiguity problems in an English-to-Chinese machine translation system (BehaviorTran) (Chen et al. 1991) and a spoken language processing system (Su, Chiang, and Lin 1991; 1992). The unified probabilistic scoring function derived for the syntactic disambiguation task is summarized in the following sections. 2.1 Definition An illustration of syntactic ambiguities for an input sentence W (= wri1 = {w1, w2, â¢ â¢ â¢ wr,}) is shown in Figure 1, where w, (i = 1, n) stands for the ith word of the input sentence. In this figure, Lexk (1 &lt; k &lt; m) stands for the kth lexical sequence out of M possible sequences. Synj,k (1 &lt; j &lt; Nk) is the jth alternative syntactic structure corresponding to Lexk, and Nk is t</context>
<context position="23027" citStr="Chen et al. 1991" startWordPosition="3757" endWordPosition="3760">y when enough data is available. The optimization criteria are thus not compromised by the topologies of the parse trees, because the number of SHIFT actions (i.e., the number of input tokens) is fixed for an input sentence. 3. Baseline Model To establish a benchmark for examining the power of the proposed algorithms, we begin with a baseline system, in which the parameters are estimated by using the MLE method. Later, we will show how to improve the baseline model with the proposed enhancement mechanisms. 3.1 Experimental Setup First of all, 10,000 parsed sentences generated by BehaviorTran (Chen et al. 1991), a commercialized English-to-Chinese machine translation system designed by Behavior 328 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying Design Corporation (BDC), were collected. The domain for this corpus is computer manuals and documents. The correct parts of speech and parse trees for the collected sentences were verified by linguistic experts. The corpus was then randomly partitioned into a training set of 9,000 sentences and a test set of the remaining 1,000 sentences to eliminate possible systematic biases. The average number of words per sentence for the training</context>
</contexts>
<marker>Chen, Chang, Wang, Su, 1991</marker>
<rawString>Chen, Shu-Chuan; Chang, Jing-Shin; Wang, Jong-Nae; and Su, Keh-Yih (1991).</rawString>
</citation>
<citation valid="false">
<title>ArchTran: A corpus-based statistics-oriented English-Chinese machine translation system.&amp;quot;</title>
<booktitle>In Proceedings, Machine Translation Summit III.</booktitle>
<pages>33--40</pages>
<location>Washington, D.C.,</location>
<marker></marker>
<rawString>&amp;quot;ArchTran: A corpus-based statistics-oriented English-Chinese machine translation system.&amp;quot; In Proceedings, Machine Translation Summit III. Washington, D.C., 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Yi-Chung Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Syntactic ambiguity resolution using a discrimination and robustness oriented adaptive learning algorithm.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Fifteenth International Conference on Computational Linguistics, Nantes,</booktitle>
<pages>352--358</pages>
<marker>Chiang, Lin, Su, 1992</marker>
<rawString>Chiang, Tung-Hui; Lin, Yi-Chung; and Su, Keh-Yih (1992). &amp;quot;Syntactic ambiguity resolution using a discrimination and robustness oriented adaptive learning algorithm.&amp;quot; In Proceedings, Fifteenth International Conference on Computational Linguistics, Nantes, 352-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase for unrestricted text.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, IEEE 1989 International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>695--698</pages>
<location>Glasgow,</location>
<contexts>
<context position="11868" citStr="Church 1989" startWordPosition="1843" endWordPosition="1844">disambiguation task is considered in a semantic scoring function, which is not discussed in this paper. Interested readers are referred to Chang, Luo, and Su (1992). 324 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying P I 4:7) x P (ckk:7) P(w7) SL(Lexic) P(w7) where 4,, stands for the part of speech assigned to w,. Since P(w7) is the same for all possible lexical sequences, it can be ignored without affecting the final results. Therefore, we use Sr, (.) instead of S,â(.) in the following derivation. Like the standard tagging procedures (Garside, Leech, and Sampson 1987; Church 1989; Merialdo 1991), the probability terms P(w7 I ckk:in) and P(ckk:in) in Equation 5 can be approximated as follows, respectively: c&apos; ckk:7) P (w = HP (wi I w1, c) R HP (w I cu). i=1 i-1 P (ckk:7) = HP (ck,i I ckkV) n bigram model HP(ck,i I ck,i_i), trigram model Hp(ck,iI Ck,i -1, Ck,i i=1 Therefore, the lexical score Skx(Lexk) is expressed as: stex (Lexk) III(ck,i Ckk:ii. 1) X P(WiI Ck,i) 1=1 Hp(ck,,I ck,i_i) x P(w I cu), bigram model HP(Ck,i I Ck,i-1,Ck,i-2) X P(Wi I Ck,i), trigram model (8) i=1 2.3 Syntactic Scoring Function Conventional stochastic context-free grammar (CFG) approaches (Wrigh</context>
</contexts>
<marker>Church, 1989</marker>
<rawString>Church, Kenneth (1989). &amp;quot;A stochastic parts program and noun phrase for unrestricted text.&amp;quot; In Proceedings, IEEE 1989 International Conference on Acoustics, Speech, and Signal Processing. Glasgow, 695-698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Gail Gong</author>
</authors>
<title>A leisurely look at the bootstrap, the jackknife, and cross-validation.&amp;quot;</title>
<date>1983</date>
<journal>The American Statistics,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>36--48</pages>
<contexts>
<context position="41709" citStr="Efron and Gong 1983" startWordPosition="6801" endWordPosition="6804">le. However, the performance improvement for the test set is far less than that for the training set, since the statistical variations between the training set and the test set are not taken into consideration in the learning procedure. For investigating robustness issues in more detail, a robust learning procedure and the associated analyses are provided in the following section. 4.2 Robust Learning As discussed in the previous section, the discriminative learning approach aims at minimizing the training set errors. The error rate measured in the training set is, in general, over-optimistic (Efron and Gong 1983), because the training set performance can be tuned to approach 100% by using a large number of parameters. The parameters obtained in such a way frequently fail to attain an optimal performance when used in 335 Computational Linguistics Volume 21, Number 3 Table 2 Performance with discriminative learning: (a) training set; (b) test set. Values in parentheses correspond to performance excluding unambiguous sentences. Model Part-of-Speech Accuracy Rate Parse Tree in Word in Sentence Accuracy Rate Selection (%) (%) (%) Power Lex(L1)+Syn(L1) 99.62 (99.59) 95.57 (94.99) 75.43 (72.26) 0.34 (0.26) +</context>
</contexts>
<marker>Efron, Gong, 1983</marker>
<rawString>Efron, Bradley, and Gong, Gail (1983). &amp;quot;A leisurely look at the bootstrap, the jackknife, and cross-validation.&amp;quot; The American Statistics, 37(1), 36-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
<author>Geoffrey Sampson</author>
</authors>
<title>The Computational Analysis of English: A corpus-based approach.</title>
<date>1987</date>
<publisher>Longman.</publisher>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey (1987). The Computational Analysis of English: A corpus-based approach. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>S J Cox</author>
</authors>
<title>Some statistical issues in the comparison of speech recognition algorithm.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, IEEE 1989 International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<pages>532--535</pages>
<location>Glasgow,</location>
<contexts>
<context position="30288" citStr="Gillick and Cox 1989" startWordPosition="4923" endWordPosition="4926">he syntactic disambiguation process is improved significantly by using more syntactic contextual 4 The term &amp;quot;most preferred candidate&amp;quot; means the syntactic structure most preferred by people even when there is more than one arguably correct syntactic structure. However, throughout this paper, both the expressions &amp;quot;most preferred syntactic structure&amp;quot; and &amp;quot;correct syntactic structure&amp;quot; refer to the syntactic structure most preferred by our linguistic experts. 5 The conclusions drawn throughout this paper are all examined based on the testing hypothesis procedure for a significance level a = 0.01 (Gillick and Cox 1989). 330 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying Table 1 The baseline performance: (a) training set; (b) test set. Values in parentheses correspond to performance excluding unambiguous sentences. Part-of-Speech Accuracy Rate Parse Tree in Word in Sentence Accuracy Rate Selection Model (%) (%) (%) Power Lex(L1)+Syn(L1) 99.62 (99.59) 95.6 (95.0) 75.4 (72.3) 0.34 (0.26) Lex(L2)+Syn(L1) 99.64 (99.61) 95.9 (95.4) 75.8 (72.7) 0.34 (0.26) Lex(L1)+Syn(L2) 99.67 (99.64) 96.1 (95.6) 78.7 (75.9) 0.34 (0.25) Lex(L2)+Syn(L2) 99.69 (99.67) 96.5 (96.0) 79.0 (76.4) 0.33 (0.25) (a) </context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>Gillick, L. and Cox, S. J. (1989). &amp;quot;Some statistical issues in the comparison of speech recognition algorithm.&amp;quot; In Proceedings, IEEE 1989 International Conference on Acoustics, Speech, and Signal Processing. Glasgow, 532-535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.&amp;quot;</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<pages>237--264</pages>
<contexts>
<context position="6804" citStr="Good 1953" startWordPosition="1029" endWordPosition="1030"> the correct ranking orders for data in real tasks. An accuracy rate of 64.3% for parse tree selection is attained after this robust learning algorithm is used. The above-mentioned robust learning procedure starts with the parameters obtained by the maximum likelihood estimation method. However, the MLE is notoriously unreliable when there is insufficient training data. The MLE for the probability of a null event is zero, which is generally inappropriate for most applications. To avoid the sparse training data problem, the parameters are first estimated by various parameter smoothing methods (Good 1953; Katz 1987). An accuracy rate for parse tree selection is improved to 69.8% by applying the robust learning procedure to the 322 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying smoothed parameters. This result demonstrates that a better initial estimate of the parameters gives the robust learning procedure a chance to obtain better results when many local maximal points exist. Finally, a parameter tying scheme is proposed to reduce the number of parameters. In this approach, some less reliably estimated but highly correlated parameters are tied together, and then traine</context>
<context position="49361" citStr="Good (1953)" startWordPosition="8022" endWordPosition="8023">and RL denote &amp;quot;Discriminative Learning&amp;quot; and &amp;quot;Robust Learning,&amp;quot; respectively 5. Parameter Smoothing for Sparse Data The above-mentioned robust learning algorithm starts with the initial parameters estimated by using MLE method. MLE, however, frequently suffers from the large estimation error caused by the lack of sufficient training data in many statistical approaches. For example, MLE gives a zero probability to events that were never observed in the training set. Therefore, MLE fails to provide a reliable result if only a small number of sampling data are available. To overcome this problem, Good (1953) proposed using Turing&apos;s formula as an improved estimate over the well-known MLE. In addition, Katz (1987) proposed a different smoothing technique, called the Back-Off procedure, for smoothing unreliably estimated n-gram parameters with their correlated (n-1)-gram parameters. To investigate the effects of parameter smoothing on robust learning, both these techniques are used to smooth the estimated parameters, and then the robust learning procedure is applied based on those smoothed parameters. These two smooth338 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying ing tech</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. J. (1953). &amp;quot;The population frequencies of species and the estimation of population parameters.&amp;quot; Biometrika, 40, 237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Formal Languages and Their Relation to Automata.</title>
<date>1974</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="14085" citStr="Hoperoft and Ullman 1974" startWordPosition="2262" endWordPosition="2265">TION REDUCE SHIFT A L8 = { A I z L7 = { B C } B C z Nt.z.. z L6 = { B F G } L5 = { B F c4} D E F G L4 = { B C3 c4) 1 t 1 t2 1 t4 1 t5 L3 = { 1 D E C3 C4 } C C2 C3 C4 L2 = { D C2 C3 C4 } 1 L1 = { C1 C2 C3 C4 } Figure 2 The decomposition of a given syntactic tree X into different phrase levels. steps. First, the tree is decomposed into a number of phrase levels, such as L1, L2, â¢ â¢ â¢ , L8 in Figure 2. A phrase level is a sequence of symbols (terminal or nonterminal) that acts as an intermediate result in parsing the input sentence, and is also called a sentential form in formal language theory (Hoperoft and Ullman 1974). In the second step, we formulate the transition between phrase levels as a context-sensitive rewriting process. With the formulation, each transition probability between two phrase levels is calculated by consulting a finite-length window that comprises the symbols to be reduced and their left and right contexts. Let the label t, in Figure 2 be the time index for the ith state transition, which corresponds to a reduce action, and L, be the ith phrase level. Then the syntactic score of the tree in Figure 2 is defined as: Ssyn(Treex) P(L8, L7, â¢ â¢ â¢ L2 L1) = P(L8 L7, . , ) X P(L7 I L6, . , X â¢</context>
</contexts>
<marker>Hoperoft, Ullman, 1974</marker>
<rawString>Hoperoft, John E., and Ullman, Jeffrey D. (1974). Formal Languages and Their Relation to Automata. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeru Katagiri</author>
<author>Chin-Hui Lee</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>New discriminative training algorithm based on the generalized probabilistic descent method.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 1991 IEEE Workshop Neural Networks for Signal Processing.</booktitle>
<pages>299--308</pages>
<location>Piscataway, New Jersey,</location>
<contexts>
<context position="5003" citStr="Katagiri et al. 1991" startWordPosition="753" endWordPosition="756"> task. Maximizing the likelihood values on the training corpus, therefore, does not necessarily lead to the minimum error rate. In addition, the statistical variations between the training corpus and real tasks are usually not taken into consideration in the estimation procedure. Thus, minimizing the error rate on the training corpus does not imply minimizing the error rate in the task we are really concerned with. To deal with the problems described above, a variety of discrimination-based learning algorithms have been adopted extensively in the field of speech recognition (Bahl et al. 1988; Katagiri et al. 1991; Su and Lee 1991, 1994). Among those approaches, the robustness issue was discussed in detail by Su and Lee (1991, 1994) in particular, and encouraging results were observed. In this paper, a discrimination oriented adaptive learning algorithm is first derived based on the scoring function mentioned above and probabilistic gradient descent theory (Amari 1967; Katagiri, Lee, and Juang 1991). The parameters of the scoring function are then learned from the training corpus using the discriminative learning algorithm. The accuracy rate for parse tree selection is improved to 56.4% when the discri</context>
</contexts>
<marker>Katagiri, Lee, Juang, 1991</marker>
<rawString>Katagiri, Shigeru; Lee, Chin-Hui and Juang, Biing-Hwang (1991). &amp;quot;New discriminative training algorithm based on the generalized probabilistic descent method.&amp;quot; In Proceedings, 1991 IEEE Workshop Neural Networks for Signal Processing. Piscataway, New Jersey, 299-308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot;</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing.</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="6816" citStr="Katz 1987" startWordPosition="1031" endWordPosition="1032">t ranking orders for data in real tasks. An accuracy rate of 64.3% for parse tree selection is attained after this robust learning algorithm is used. The above-mentioned robust learning procedure starts with the parameters obtained by the maximum likelihood estimation method. However, the MLE is notoriously unreliable when there is insufficient training data. The MLE for the probability of a null event is zero, which is generally inappropriate for most applications. To avoid the sparse training data problem, the parameters are first estimated by various parameter smoothing methods (Good 1953; Katz 1987). An accuracy rate for parse tree selection is improved to 69.8% by applying the robust learning procedure to the 322 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying smoothed parameters. This result demonstrates that a better initial estimate of the parameters gives the robust learning procedure a chance to obtain better results when many local maximal points exist. Finally, a parameter tying scheme is proposed to reduce the number of parameters. In this approach, some less reliably estimated but highly correlated parameters are tied together, and then trained through th</context>
<context position="49467" citStr="Katz (1987)" startWordPosition="8038" endWordPosition="8039">rse Data The above-mentioned robust learning algorithm starts with the initial parameters estimated by using MLE method. MLE, however, frequently suffers from the large estimation error caused by the lack of sufficient training data in many statistical approaches. For example, MLE gives a zero probability to events that were never observed in the training set. Therefore, MLE fails to provide a reliable result if only a small number of sampling data are available. To overcome this problem, Good (1953) proposed using Turing&apos;s formula as an improved estimate over the well-known MLE. In addition, Katz (1987) proposed a different smoothing technique, called the Back-Off procedure, for smoothing unreliably estimated n-gram parameters with their correlated (n-1)-gram parameters. To investigate the effects of parameter smoothing on robust learning, both these techniques are used to smooth the estimated parameters, and then the robust learning procedure is applied based on those smoothed parameters. These two smooth338 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying ing techniques are first summarized in the following section. The investigation for the smoothing/robust learning </context>
<context position="51045" citStr="Katz (1987)" startWordPosition="8290" endWordPosition="8291"> PML(e) (38) The estimate based on Turing&apos;s formula (Good 1953) is given by the following equation: PTU(e) = N71-* (39) where r* (r + 1)n nr+1 (40) The total probability estimate, using Turing&apos;s formula, for all the events that actually occurred in the sample space is equal to EPT.(e) = â ni (41) e:C(e)&gt;0 where C(e) stands for the frequency count of the event e in the sample. This, in turn, leads to the following equation: (42) e:C(e)=0 According to Turing&apos;s formula, the probability mass n1/N is then equally distributed over the events that never occur in the sample. 5.1.2 Back-off Procedure. Katz (1987) proposed a back-off procedure to estimate parameters for an m-gram model, i.e., the conditional probability of a word given the (m-1) preceding words. This procedure is summarized as follows: PBF (wm I 4-1 ) = { PTU(Wm I WT-1), if C(w) &gt; 0 a (w2&apos;)PBF(wm I al2m-1) if C(w) = 0, and C(w) &gt; 0, PBF(Wm I U1211-1) if E c(wr) = 0 Wm where 1 _ E PBF(Wm wr-&apos;) a(4-1) = â PsF(wm w1) wâ,:C(zer) &gt;0 339 Computational Linguistics Volume 21, Number 3 is a normalized factor such that E PBF(w. I wT) + E PBF(w. = 1 (45) Compared with Turing&apos;s formula, the probability for an m-gram that does not occur in the samp</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. (1987). &amp;quot;Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot; IEEE Transactions on Acoustics, Speech and Signal Processing. ASSP-35, 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
<author>Alan Stuart</author>
</authors>
<title>The Advanced Theory of Statistics.</title>
<date>1979</date>
<publisher>Macmillan.</publisher>
<contexts>
<context position="31696" citStr="Kendall and Stuart 1979" startWordPosition="5121" endWordPosition="5124">.3) 0.45 (0.38) Lex(L2)+Syn(L1) 98.93 (98.84) 88.9 (87.36) 49.7 (42.7) 0.45 (0.38) Lex(L1)+Syn(L2) 98.82 (98.71) 88.0 (86.33) 52.8 (46.2) 0.44 (0.37) Lex(L2)+Syn(L2) 98.89 (98.79) 88.5 (86.90) 53.1 (46.6) 0.44 (0.37) (b) Test set performance information. For example, a 53.1% accuracy rate is attained for the Lex(L2)+Syn(L2) model, while the accuracy rate is 49.7% for the Lex(L2)+Syn(L1) model. This result indicates that the context-free assumption adopted by most stochastic parsers might not hold. 4. Discrimination- and Robustness-Oriented Learning Although MLE possesses many nice properties (Kendall and Stuart 1979), the criterion of maximizing likelihood value is not equivalent to that of minimizing the error rate in a training set. The maximum likelihood approach achieves disambiguation indirectly and implicitly through the estimation procedure. However, correct disambiguation only depends on the ranks, rather than the likelihood values, of the candidates. In other words, correct recognition will still be obtained if the score of the correct candidate is the highest, even though the likelihood values of the various candidates are estimated poorly. Motivated by this concern, a discrimination-oriented le</context>
</contexts>
<marker>Kendall, Stuart, 1979</marker>
<rawString>Kendall, Maurice, and Stuart, Alan (1979). The Advanced Theory of Statistics. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Chung Lin</author>
<author>Tung-Hui Chiang</author>
<author>Keh-Yih Su</author>
</authors>
<title>Automatic model refinementâwith an application to tagging.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics. Kyoto,</booktitle>
<pages>148--153</pages>
<marker>Lin, Chiang, Su, 1994</marker>
<rawString>Lin, Yi-Chung; Chiang, Tung-Hui; and Su, Keh-Yih (1994). &amp;quot;Automatic model refinementâwith an application to tagging.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics. Kyoto, 148-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, the IEEE 1991 International Conference on Acoustic, Speech, and Signal Processing.</booktitle>
<pages>809--812</pages>
<location>Toronto,</location>
<contexts>
<context position="11884" citStr="Merialdo 1991" startWordPosition="1845" endWordPosition="1846">n task is considered in a semantic scoring function, which is not discussed in this paper. Interested readers are referred to Chang, Luo, and Su (1992). 324 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying P I 4:7) x P (ckk:7) P(w7) SL(Lexic) P(w7) where 4,, stands for the part of speech assigned to w,. Since P(w7) is the same for all possible lexical sequences, it can be ignored without affecting the final results. Therefore, we use Sr, (.) instead of S,â(.) in the following derivation. Like the standard tagging procedures (Garside, Leech, and Sampson 1987; Church 1989; Merialdo 1991), the probability terms P(w7 I ckk:in) and P(ckk:in) in Equation 5 can be approximated as follows, respectively: c&apos; ckk:7) P (w = HP (wi I w1, c) R HP (w I cu). i=1 i-1 P (ckk:7) = HP (ck,i I ckkV) n bigram model HP(ck,i I ck,i_i), trigram model Hp(ck,iI Ck,i -1, Ck,i i=1 Therefore, the lexical score Skx(Lexk) is expressed as: stex (Lexk) III(ck,i Ckk:ii. 1) X P(WiI Ck,i) 1=1 Hp(ck,,I ck,i_i) x P(w I cu), bigram model HP(Ck,i I Ck,i-1,Ck,i-2) X P(Wi I Ck,i), trigram model (8) i=1 2.3 Syntactic Scoring Function Conventional stochastic context-free grammar (CFG) approaches (Wright and Wrigley 19</context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>Merialdo, Bernard (1991). &amp;quot;Tagging text with a probabilistic model.&amp;quot; In Proceedings, the IEEE 1991 International Conference on Acoustic, Speech, and Signal Processing. Toronto, 809-812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Jing-Shin Chang</author>
</authors>
<title>Semantic and syntactic aspects of score function.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 12th International Conference on Computational Linguistics. Budapest,</booktitle>
<pages>22--27</pages>
<contexts>
<context position="3444" citStr="Su and Chang (1988)" startWordPosition="505" endWordPosition="508">e estimated from a training corpus by using well-developed statistical theorems. The linguistic uncertainty problems can thus be resolved on a solid mathematical basis. Moreover, the knowledge acquired by a statistical method is always consistent because all the data in the corpus are jointly considered during the acquisition process. Hence, compared with a rule-based method, the time required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach. Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) proposed a unified scoring function for resolving syntactic ambiguity With that scoring function, various knowledge sources can be unified in a uniform formulation. Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991; Su and Chang 1990). In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods. A test set of 1,000 sentences, extracted from technical manuals, is used f</context>
</contexts>
<marker>Su, Chang, 1988</marker>
<rawString>Su, Keh-Yih, and Chang, Jing-Shin (1988). &amp;quot;Semantic and syntactic aspects of score function.&amp;quot; In Proceedings, 12th International Conference on Computational Linguistics. Budapest, 22-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Jing-Shin Chang</author>
</authors>
<title>Some key issues in designing MT systems.&amp;quot;</title>
<date>1990</date>
<journal>Machine Translation,</journal>
<volume>5</volume>
<issue>4</issue>
<pages>265--300</pages>
<contexts>
<context position="3827" citStr="Su and Chang 1990" startWordPosition="568" endWordPosition="571">ime required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach. Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) proposed a unified scoring function for resolving syntactic ambiguity With that scoring function, various knowledge sources can be unified in a uniform formulation. Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991; Su and Chang 1990). In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods. A test set of 1,000 sentences, extracted from technical manuals, is used for evaluation. A performance of 53.1% accuracy rate for parse tree selection is obtained for the baseline system, when the parameters are estimated by using the maximum likelihood estimation (MLE) method. Note that it is the ranking of competitors, instead of the likelihood value, that directly affects the performance of a disambiguation task. Maximizing the likelihood values on t</context>
</contexts>
<marker>Su, Chang, 1990</marker>
<rawString>Su, Keh-Yih, and Chang, Jing-Shin (1990). &amp;quot;Some key issues in designing MT systems.&amp;quot; Machine Translation, 5(4), 265-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Robustness and discrimination oriented speech recognition using weighted HMM and subspace projection approaches.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, IEEE 1991 International Conference on Acoustic, Speech, and Signal Processing.</booktitle>
<pages>541--544</pages>
<location>Toronto,</location>
<contexts>
<context position="5020" citStr="Su and Lee 1991" startWordPosition="757" endWordPosition="760">likelihood values on the training corpus, therefore, does not necessarily lead to the minimum error rate. In addition, the statistical variations between the training corpus and real tasks are usually not taken into consideration in the estimation procedure. Thus, minimizing the error rate on the training corpus does not imply minimizing the error rate in the task we are really concerned with. To deal with the problems described above, a variety of discrimination-based learning algorithms have been adopted extensively in the field of speech recognition (Bahl et al. 1988; Katagiri et al. 1991; Su and Lee 1991, 1994). Among those approaches, the robustness issue was discussed in detail by Su and Lee (1991, 1994) in particular, and encouraging results were observed. In this paper, a discrimination oriented adaptive learning algorithm is first derived based on the scoring function mentioned above and probabilistic gradient descent theory (Amari 1967; Katagiri, Lee, and Juang 1991). The parameters of the scoring function are then learned from the training corpus using the discriminative learning algorithm. The accuracy rate for parse tree selection is improved to 56.4% when the discriminative learning</context>
<context position="33610" citStr="Su and Lee (1991" startWordPosition="5410" endWordPosition="5413">al times larger 331 Computational Linguistics Volume 21, Number 3 than the number of parameters (based on our experience). Overtuning the training set performance usually causes performance on the test set to deteriorate. Hence, the robustness issue, which concerns the possible statistical variations between the training set and the test set, must be taken into consideration when we adopt an adaptive learning procedure. In this section, we start with a learning algorithm derived from the probabilistic descent procedure (Katagiri, Lee, and juang 1991). The robust learning algorithm explored by Su and Lee (1991, 1994) is then introduced to enhance the robustness of the system. 4.1 Discrimination-Oriented Learning To link the syntactic disambiguation process with the learning procedure, a discrimination function, namely g ,k(w7), for the syntactic tree Syni,k, corresponding to the lexical sequence Lexk and the input sentence (or word sequence) &amp;it, is defined as gi,k (wiz ) = log P (Syn),k, Lexk I wi) (14) Since log() is a monotonic increasing function, we can rewrite the criterion for syntactic disambiguation in Equation 1 as the following equation: = argmax{gi,k (w7 )1 (15) ,k According to Equation</context>
<context position="44247" citStr="Su and Lee (1991" startWordPosition="7168" endWordPosition="7171">because of the lack of sufficient sampling data and the possible statistical variations between the training set and the test set. To achieve better performance for a real application, one must deal with statistical variation problems. Most adaptive learning procedures stop adjusting the parameters once the input training token has been classified correctly For such learning procedures, the distance between the correct candidate and other competitive ones may be too small to cover the possible statistical variations between the training corpus and the real application. To remedy this problem, Su and Lee (1991, 1994) suggested that the distance margin between the correct candidate and the top competitor should be enlarged, even though the input token is correctly recognized, until the margin exceeds a given threshold. A large distance margin would provide a tolerance region in the neighborhood of the decision boundary to allow possible data scattering in the real applications (Su and Lee 1994). A promising result has been observed by applying 336 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying this robust learning procedure to recognize the alphabet of E-set in English (Su an</context>
<context position="46228" citStr="Su and Lee (1991" startWordPosition="7546" endWordPosition="7549">)n(j, i) â A Ag)n(j , i) , if (114 3 j,k11 â 114)a,a11) &lt;6, )4t1-1) C 7/ i) = A yt)n (i, i), otherwise A etx+1) (ic 0 = (t)( i) 0 _ A A: (k, i), { Aetx+1) (k, 0 = A lext) (IC , i), if (0 11 â 0.4311) &lt; 6 , otherwise The learning rules of the syntactic and lexical weights are modified as follows: (t+1) (t) A (t) 1 Wsyn = Wsyn + zaWsyn W(,1-1) â W()n, if (14 j,k11 â *oil) &lt; 6 , otherwise {,,, &amp;quot;I(t+1) (t) , A (t) lex â W -Tlex &amp;quot;W lex&apos; (t-1-1) (t) wlex = W lex&amp;quot; if (14 â 0.4311) &lt; 6, otherwise (36) The margin 6 in the above equations can be assigned either absolutely or relatively, as suggested in Su and Lee (1991, 1994). Currently, the relative mode with a 30% passing rate (i.e., 30% of the training tokens pass through the margin) is used in our implementation. The simulation results, compared with the results obtained by using the discriminative learning procedure, are shown in Table 3. Table 3(a) shows that performances with robust learning in the training set are a little worse than those with discrimination learning for the L1 syntactic language models. Nevertheless, they are a little better for the L2 syntactic language model. All these differences, however, are not statistically significant. On </context>
</contexts>
<marker>Su, Lee, 1991</marker>
<rawString>Su, Keh-Yih, and Lee, Chin-Hui (1991). &amp;quot;Robustness and discrimination oriented speech recognition using weighted HMM and subspace projection approaches.&amp;quot; In Proceedings, IEEE 1991 International Conference on Acoustic, Speech, and Signal Processing. Toronto, 541-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Speech recognition using weighted HMM and subspace projection approaches.&amp;quot;</title>
<date>1994</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>69--79</pages>
<contexts>
<context position="44638" citStr="Su and Lee 1994" startWordPosition="7230" endWordPosition="7233">he distance between the correct candidate and other competitive ones may be too small to cover the possible statistical variations between the training corpus and the real application. To remedy this problem, Su and Lee (1991, 1994) suggested that the distance margin between the correct candidate and the top competitor should be enlarged, even though the input token is correctly recognized, until the margin exceeds a given threshold. A large distance margin would provide a tolerance region in the neighborhood of the decision boundary to allow possible data scattering in the real applications (Su and Lee 1994). A promising result has been observed by applying 336 Tung-Hui Chiang et al. Robust Learning, Smoothing, and Parameter Tying this robust learning procedure to recognize the alphabet of E-set in English (Su and Lee 1991, 1994). To enhance robustness, the learning rules from Equation 24 to Equation 30 are modified as follows. Following the notations in the previous section, the correct syntactic structure is denoted by Syna,,r3 , and the syntactic structure of the strongest competitor is denoted by Syn 5J(, whose score may either rank first or second. 1. For the syntactic and lexical parameters</context>
</contexts>
<marker>Su, Lee, 1994</marker>
<rawString>Su, Keh-Yih, and Lee, Chin-Hui (1994). &amp;quot;Speech recognition using weighted HMM and subspace projection approaches.&amp;quot; IEEE Trans. on Speech and Audio Processing, 2(1), 69-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Jing-Shin Chang</author>
<author>Yi-Chung Lin</author>
</authors>
<title>A discriminative approach for ambiguity resolution based on a semantic score function.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 1992 International Conference on Spoken Language Processing.</booktitle>
<pages>149--152</pages>
<location>Banff,</location>
<marker>Su, Chang, Lin, 1992</marker>
<rawString>Su, Keh-Yih; Chang, Jing-Shin; and Lin, Yi-Chung (1992). &amp;quot;A discriminative approach for ambiguity resolution based on a semantic score function.&amp;quot; In Proceedings, 1992 International Conference on Spoken Language Processing. Banff, 149-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Tung-Hui Chiang</author>
<author>Yi-Chung Lin</author>
</authors>
<title>A robustness and discrimination oriented score function for integrating speech and language processing.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 2nd European Conference on Speech Communication and Technology. Genova,</booktitle>
<pages>207--210</pages>
<contexts>
<context position="3465" citStr="Su et al. (1991)" startWordPosition="510" endWordPosition="513">ing corpus by using well-developed statistical theorems. The linguistic uncertainty problems can thus be resolved on a solid mathematical basis. Moreover, the knowledge acquired by a statistical method is always consistent because all the data in the corpus are jointly considered during the acquisition process. Hence, compared with a rule-based method, the time required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach. Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) proposed a unified scoring function for resolving syntactic ambiguity With that scoring function, various knowledge sources can be unified in a uniform formulation. Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991; Su and Chang 1990). In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods. A test set of 1,000 sentences, extracted from technical manuals, is used for evaluation. A perf</context>
<context position="8698" citStr="Su et al. (1991)" startWordPosition="1318" endWordPosition="1321">the parameter smoothing techniques on the robust learning procedure are investigated in Section 5. Next, the parameter tying scheme used to enhance parameter training and reduce the number of parameters is described in Section 6. Finally, we discuss our conclusions and describe the direction of future work. 2. A Unified Probabilistic Score Function Linguistic knowledge, including knowledge of lexicon, syntax, and semantics, is essential for resolving syntactic ambiguities. To integrate various knowledge sources in a uniform formulation, a unified probabilistic scoring function was proposed by Su et al. (1991). This scoring function has been successfully applied to resolve ambiguity problems in an English-to-Chinese machine translation system (BehaviorTran) (Chen et al. 1991) and a spoken language processing system (Su, Chiang, and Lin 1991; 1992). The unified probabilistic scoring function derived for the syntactic disambiguation task is summarized in the following sections. 2.1 Definition An illustration of syntactic ambiguities for an input sentence W (= wri1 = {w1, w2, â¢ â¢ â¢ wr,}) is shown in Figure 1, where w, (i = 1, n) stands for the ith word of the input sentence. In this figure, Lexk (1 &lt; </context>
<context position="16812" citStr="Su et al. 1991" startWordPosition="2760" endWordPosition="2763"> level but less correlated with other preceding phrase levels. In other words, the inter-level correlation is assumed to be a first-order Markov process. In addition, for computational feasibility, only a finite number of left and right contextual symbols are considered in the formulation. If M left context symbols and N right context symbols are consulted in evaluating Equation 9, the model is said to operate in the LmRN mode. Notice that the last formula in Equation 9 corresponds to the rightmost derivation sequence in a generalized LR parser with left and right contexts taken into account (Su et al. 1991). Such a formulation is particularly useful for a generalized LR parsing algorithm, in which context-sensitive processing power is desirable. Although the context-sensitive model in the above equation provides the ability to deal with intralevel context-sensitivity, it fails to catch inter-level correlation. In addition, the formulation of Equation 9 will result in the normalization problem (Su et al. 1991; Briscoe and Carroll 1993) when various syntactic trees have different number of nodes. An alternative formulation, which compacts the highly correlated phrase levels into a single one, was </context>
<context position="18487" citStr="Su et al. 1991" startWordPosition="3034" endWordPosition="3037">consumed by a shift operation. Because the total number of shift actions, equal to the number of product terms in Equation 12, is always the same for all alternative syntactic trees, the normalization problem is resolved in such a formulation. Moreover, the formulation in Equation 12 provides a way to consider both intra-level context-sensitivity and inter-level correlation of the underlying context-free grammar. With such a formulation, the capability of contextsensitive parsing (in probabilistic sense) can be achieved with a context-free grammar. It is interesting to compare our frameworks (Su et al. 1991) with the work by Briscoe and Carroll (1993) on probabilistic LR parsing. Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a REDUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action. With such an approach, it is very easy to implemen</context>
</contexts>
<marker>Su, Chiang, Lin, 1991</marker>
<rawString>Su, Keh-Yih; Chiang, Tung-Hui; and Lin, Yi-Chung (1991). &amp;quot;A robustness and discrimination oriented score function for integrating speech and language processing.&amp;quot; In Proceedings, 2nd European Conference on Speech Communication and Technology. Genova, 207-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Jong-Nae Wang</author>
<author>Mei-Hui Su</author>
<author>Jing-Shin Chang</author>
</authors>
<title>GLR parsing with scoring.&amp;quot;</title>
<date>1991</date>
<booktitle>In Generalized LR Parsing, edited by Masaru Tomita,</booktitle>
<pages>93--112</pages>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="3465" citStr="Su et al. (1991)" startWordPosition="510" endWordPosition="513">ing corpus by using well-developed statistical theorems. The linguistic uncertainty problems can thus be resolved on a solid mathematical basis. Moreover, the knowledge acquired by a statistical method is always consistent because all the data in the corpus are jointly considered during the acquisition process. Hence, compared with a rule-based method, the time required for knowledge acquisition and the cost needed to maintain consistency among the acquired knowledge sources are significantly reduced by adopting a statistical approach. Among the statistical approaches, Su and Chang (1988) and Su et al. (1991) proposed a unified scoring function for resolving syntactic ambiguity With that scoring function, various knowledge sources can be unified in a uniform formulation. Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications (Su, Chiang, and Lin 1992; Chen et al. 1991; Su and Chang 1990). In this paper, we start with a baseline system based on this scoring function, and then proceed with different proposed enhancement methods. A test set of 1,000 sentences, extracted from technical manuals, is used for evaluation. A perf</context>
<context position="8698" citStr="Su et al. (1991)" startWordPosition="1318" endWordPosition="1321">the parameter smoothing techniques on the robust learning procedure are investigated in Section 5. Next, the parameter tying scheme used to enhance parameter training and reduce the number of parameters is described in Section 6. Finally, we discuss our conclusions and describe the direction of future work. 2. A Unified Probabilistic Score Function Linguistic knowledge, including knowledge of lexicon, syntax, and semantics, is essential for resolving syntactic ambiguities. To integrate various knowledge sources in a uniform formulation, a unified probabilistic scoring function was proposed by Su et al. (1991). This scoring function has been successfully applied to resolve ambiguity problems in an English-to-Chinese machine translation system (BehaviorTran) (Chen et al. 1991) and a spoken language processing system (Su, Chiang, and Lin 1991; 1992). The unified probabilistic scoring function derived for the syntactic disambiguation task is summarized in the following sections. 2.1 Definition An illustration of syntactic ambiguities for an input sentence W (= wri1 = {w1, w2, â¢ â¢ â¢ wr,}) is shown in Figure 1, where w, (i = 1, n) stands for the ith word of the input sentence. In this figure, Lexk (1 &lt; </context>
<context position="16812" citStr="Su et al. 1991" startWordPosition="2760" endWordPosition="2763"> level but less correlated with other preceding phrase levels. In other words, the inter-level correlation is assumed to be a first-order Markov process. In addition, for computational feasibility, only a finite number of left and right contextual symbols are considered in the formulation. If M left context symbols and N right context symbols are consulted in evaluating Equation 9, the model is said to operate in the LmRN mode. Notice that the last formula in Equation 9 corresponds to the rightmost derivation sequence in a generalized LR parser with left and right contexts taken into account (Su et al. 1991). Such a formulation is particularly useful for a generalized LR parsing algorithm, in which context-sensitive processing power is desirable. Although the context-sensitive model in the above equation provides the ability to deal with intralevel context-sensitivity, it fails to catch inter-level correlation. In addition, the formulation of Equation 9 will result in the normalization problem (Su et al. 1991; Briscoe and Carroll 1993) when various syntactic trees have different number of nodes. An alternative formulation, which compacts the highly correlated phrase levels into a single one, was </context>
<context position="18487" citStr="Su et al. 1991" startWordPosition="3034" endWordPosition="3037">consumed by a shift operation. Because the total number of shift actions, equal to the number of product terms in Equation 12, is always the same for all alternative syntactic trees, the normalization problem is resolved in such a formulation. Moreover, the formulation in Equation 12 provides a way to consider both intra-level context-sensitivity and inter-level correlation of the underlying context-free grammar. With such a formulation, the capability of contextsensitive parsing (in probabilistic sense) can be achieved with a context-free grammar. It is interesting to compare our frameworks (Su et al. 1991) with the work by Briscoe and Carroll (1993) on probabilistic LR parsing. Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a REDUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action. With such an approach, it is very easy to implemen</context>
</contexts>
<marker>Su, Wang, Su, Chang, 1991</marker>
<rawString>Su, Keh-Yih; Wang, Jong-Nae; Su, Mei-Hui; and Chang, Jing-Shin (1991). &amp;quot;GLR parsing with scoring.&amp;quot; In Generalized LR Parsing, edited by Masaru Tomita, 93-112. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Jong-Nae Wang</author>
<author>Mei-Hui Su</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A sequential truncation parsing algorithm based on the score function.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of 1989 International Workshop on Parsing Technologies (IWPT-89).</booktitle>
<pages>95--104</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="13200" citStr="Su et al. (1989" startWordPosition="2062" endWordPosition="2065">ties associated with the grammar rules being applied. Such a formulation implies that the application of a rule is both independent of the applications of the other rules, and independent of the context under which a context-free rule is applied. However, a language that can be expressed with a CFG does not imply that the associated rules can be applied in an independent and context-free manner, as implicitly assumed by a stochastic context-free grammar approach. To include contextual information and consider the relationship among the grammar rules, in this paper we follow the formulation in Su et al. (1989, 1991) for syntactic score evaluation. To show the computing mechanism for the syntactic score, we take the tree in Figure 2 as an example. The basic derivation of the syntactic score includes the following 325 Computational Linguistics Volume 21, Number 3 ACTION REDUCE SHIFT A L8 = { A I z L7 = { B C } B C z Nt.z.. z L6 = { B F G } L5 = { B F c4} D E F G L4 = { B C3 c4) 1 t 1 t2 1 t4 1 t5 L3 = { 1 D E C3 C4 } C C2 C3 C4 L2 = { D C2 C3 C4 } 1 L1 = { C1 C2 C3 C4 } Figure 2 The decomposition of a given syntactic tree X into different phrase levels. steps. First, the tree is decomposed into a nu</context>
</contexts>
<marker>Su, Wang, Su, Chang, 1989</marker>
<rawString>Su, Keh-Yih; Wang, Jong-Nae; Su, Mei-Hui; and Chang, Jing-Shin (1989). &amp;quot;A sequential truncation parsing algorithm based on the score function.&amp;quot; In Proceedings of 1989 International Workshop on Parsing Technologies (IWPT-89). Pittsburgh, 95-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>GLR parsing with probability.&amp;quot;</title>
<date>1991</date>
<booktitle>In Generalized LR Parsing, edited by Masaru Tomita,</booktitle>
<pages>113--128</pages>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="12487" citStr="Wright and Wrigley 1991" startWordPosition="1947" endWordPosition="1950"> 1989; Merialdo 1991), the probability terms P(w7 I ckk:in) and P(ckk:in) in Equation 5 can be approximated as follows, respectively: c&apos; ckk:7) P (w = HP (wi I w1, c) R HP (w I cu). i=1 i-1 P (ckk:7) = HP (ck,i I ckkV) n bigram model HP(ck,i I ck,i_i), trigram model Hp(ck,iI Ck,i -1, Ck,i i=1 Therefore, the lexical score Skx(Lexk) is expressed as: stex (Lexk) III(ck,i Ckk:ii. 1) X P(WiI Ck,i) 1=1 Hp(ck,,I ck,i_i) x P(w I cu), bigram model HP(Ck,i I Ck,i-1,Ck,i-2) X P(Wi I Ck,i), trigram model (8) i=1 2.3 Syntactic Scoring Function Conventional stochastic context-free grammar (CFG) approaches (Wright and Wrigley 1991) evaluate the likelihood probability of a syntactic tree by computing the product of the probabilities associated with the grammar rules being applied. Such a formulation implies that the application of a rule is both independent of the applications of the other rules, and independent of the context under which a context-free rule is applied. However, a language that can be expressed with a CFG does not imply that the associated rules can be applied in an independent and context-free manner, as implicitly assumed by a stochastic context-free grammar approach. To include contextual information </context>
</contexts>
<marker>Wright, Wrigley, 1991</marker>
<rawString>Wright, J. H., and Wrigley, E. N. (1991). &amp;quot;GLR parsing with probability.&amp;quot; In Generalized LR Parsing, edited by Masaru Tomita, 113-128. Kluwer Academic Publisher.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>