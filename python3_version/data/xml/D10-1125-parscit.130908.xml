<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8276">
Dual Decomposition for Parsing with Non-Projective Head Automata
</title>
<author confidence="0.569556">
Terry Koo Alexander M. Rush Michael Collins Tommi Jaakkola David Sontag
</author>
<note confidence="0.708362">
MIT CSAIL, Cambridge, MA 02139, USA
</note>
<email confidence="0.987015">
{maestro,srush,mcollins,tommi,dsontag}@csail.mit.edu
</email>
<sectionHeader confidence="0.994534" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801733333333">
This paper introduces algorithms for non-
projective parsing based on dual decomposi-
tion. We focus on parsing algorithms for non-
projective head automata, a generalization of
head-automata models to non-projective struc-
tures. The dual decomposition algorithms are
simple and efficient, relying on standard dy-
namic programming and minimum spanning
tree algorithms. They provably solve an LP
relaxation of the non-projective parsing prob-
lem. Empirically the LP relaxation is very of-
ten tight: for many languages, exact solutions
are achieved on over 98% of test sentences.
The accuracy of our models is higher than pre-
vious work on a broad range of datasets.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894391304348">
Non-projective dependency parsing is useful for
many languages that exhibit non-projective syntactic
structures. Unfortunately, the non-projective parsing
problem is known to be NP-hard for all but the sim-
plest models (McDonald and Satta, 2007). There has
been a long history in combinatorial optimization of
methods that exploit structure in complex problems,
using methods such as dual decomposition or La-
grangian relaxation (Lemar´echal, 2001). Thus far,
however, these methods are not widely used in NLP.
This paper introduces algorithms for non-
projective parsing based on dual decomposition. We
focus on parsing algorithms for non-projective head
automata, a generalization of the head-automata
models of Eisner (2000) and Alshawi (1996) to non-
projective structures. These models include non-
projective dependency parsing models with higher-
order (e.g., sibling and/or grandparent) dependency
relations as a special case. Although decoding of full
parse structures with non-projective head automata
is intractable, we leverage the observation that key
components of the decoding can be efficiently com-
puted using combinatorial algorithms. In particular,
</bodyText>
<listItem confidence="0.884800266666667">
1. Decoding for individual head-words can be ac-
complished using dynamic programming.
2. Decoding for arc-factored models can be ac-
complished using directed minimum-weight
spanning tree (MST) algorithms.
The resulting parsing algorithms have the following
properties:
• They are efficient and easy to implement, relying
on standard dynamic programming and MST al-
gorithms.
• They provably solve a linear programming (LP)
relaxation of the original decoding problem.
• Empirically the algorithms very often give an ex-
act solution to the decoding problem, in which
case they also provide a certificate of optimality.
</listItem>
<bodyText confidence="0.999878227272727">
In this paper we first give the definition for non-
projective head automata, and describe the parsing
algorithm. The algorithm can be viewed as an in-
stance of Lagrangian relaxation; we describe this
connection, and give convergence guarantees for the
method. We describe a generalization to models
that include grandparent dependencies. We then in-
troduce a perceptron-driven training algorithm that
makes use of point 1 above.
We describe experiments on non-projective pars-
ing for a number of languages, and in particu-
lar compare the dual decomposition algorithm to
approaches based on general-purpose linear pro-
gramming (LP) or integer linear programming (ILP)
solvers (Martins et al., 2009). The accuracy of our
models is higher than previous work on a broad
range of datasets. The method gives exact solutions
to the decoding problem, together with a certificate
of optimality, on over 98% of test examples for many
of the test languages, with parsing times ranging be-
tween 0.021 seconds/sentence for the most simple
languages/models, to 0.295 seconds/sentence for the
</bodyText>
<page confidence="0.931011">
1288
</page>
<note confidence="0.8157885">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995864">
most complex settings. The method compares favor-
ably to previous work using LP/ILP formulations,
both in terms of efficiency, and also in terms of the
percentage of exact solutions returned.
While the focus of the current paper is on non-
projective dependency parsing, the approach opens
up new ways of thinking about parsing algorithms
for lexicalized formalisms such as TAG (Joshi and
Schabes, 1997), CCG (Steedman, 2000), and pro-
jective head automata.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971871794872">
McDonald et al. (2005) describe MST-based parsing
for non-projective dependency parsing models with
arc-factored decompositions; McDonald and Pereira
(2006) make use of an approximate (hill-climbing)
algorithm for parsing with more complex models.
McDonald and Pereira (2006) and McDonald and
Satta (2007) describe complexity results for non-
projective parsing, showing that parsing for a variety
of models is NP-hard. Riedel and Clarke (2006) de-
scribe ILP methods for the problem; Martins et al.
(2009) recently introduced alternative LP and ILP
formulations. Our algorithm differs in that we do not
use general-purpose LP or ILP solvers, instead using
an MST solver in combination with dynamic pro-
gramming; thus we leverage the underlying struc-
ture of the problem, thereby deriving more efficient
decoding algorithms.
Both dual decomposition and Lagrangian relax-
ation have a long history in combinatorial optimiza-
tion. Our work was originally inspired by recent
work on dual decomposition for inference in graph-
ical models (Wainwright et al., 2005; Komodakis
et al., 2007). However, the non-projective parsing
problem has a very different structure from these
models, and the decomposition we use is very dif-
ferent in nature from those used in graphical mod-
els. Other work has made extensive use of de-
composition approaches for efficiently solving LP
relaxations for graphical models (e.g., Sontag et
al. (2008)). Methods that incorporate combinato-
rial solvers within loopy belief propagation (LBP)
(Duchi et al., 2007; Smith and Eisner, 2008) are
also closely related to our approach. Unlike LBP,
our method has strong theoretical guarantees, such
as guaranteed convergence and the possibility of a
certificate of optimality.
Finally, in other recent work, Rush et al. (2010)
describe dual decomposition approaches for other
NLP problems.
</bodyText>
<sectionHeader confidence="0.971424" genericHeader="method">
3 Sibling Models
</sectionHeader>
<bodyText confidence="0.999984733333333">
This section describes a particular class of models,
sibling models; the next section describes a dual-
decomposition algorithm for decoding these models.
Consider the dependency parsing problem for a
sentence with n words. We define the index set
for dependency parsing to be Z = {(i, j) : i E
{0 ... n}, j E {1... n}, i =� j}. A dependency
parse is a vector y = {y(i, j) : (i, j) E Z}, where
y(i, j) = 1 if a dependency with head word i and
modifier j is in the parse, 0 otherwise. We use i = 0
for the root symbol. We define Y to be the set of all
well-formed non-projective dependency parses (i.e.,
the set of directed spanning trees rooted at node 0).
Given a function f : Y H R that assigns scores to
parse trees, the optimal parse is
</bodyText>
<equation confidence="0.99611">
y* = argmax f(y) (1)
yEY
</equation>
<bodyText confidence="0.984497666666667">
A particularly simple definition of f(y) is f(y) =
E(i,j)E-T y(i, j)θ(i, j) where θ(i, j) is the score for
dependency (i, j). Models with this form are often
referred to as arc-factored models. In this case the
optimal parse tree y* can be found efficiently using
MST algorithms (McDonald et al., 2005).
This paper describes algorithms that compute y*
for more complex definitions of f(y); in this sec-
tion, we focus on algorithms for models that capture
interactions between sibling dependencies. To this
end, we will find it convenient to define the follow-
ing notation. Given a vector y, define
</bodyText>
<equation confidence="0.95536">
y|i = {y(i,j) : j = 1 ... n,j =�i}
</equation>
<bodyText confidence="0.99282125">
Hence y|i specifies the set of modifiers to word i;
note that the vectors y|i for i = 0 ... n form a parti-
tion of the full set of variables.
We then assume that f(y) takes the form
</bodyText>
<equation confidence="0.992658333333333">
n
f(y) = E fi(y|i) (2)
i=0
</equation>
<bodyText confidence="0.9980292">
Thus f(y) decomposes into a sum of terms, where
each fi considers modifiers to the i’th word alone.
In the general case, finding y* =
argmaxyEY f(y) under this definition of f(y)
is an NP-hard problem. However for certain
</bodyText>
<page confidence="0.990567">
1289
</page>
<bodyText confidence="0.999841666666667">
definitions of fi, it is possible to efficiently compute
argmaxy|i∈Zi fi(y|i) for any value of i, typically
using dynamic programming. (Here we use Zi to
refer to the set of all possible values for y|i: specifi-
cally, Z0 = {0,1}n and for i =� 0, Zi = {0,1}n−1.)
In these cases we can efficiently compute
</bodyText>
<equation confidence="0.97844175">
z∗ = argmax
z∈Z f(z) = argmax fi(z|i) (3)
z∈Z i
�
</equation>
<bodyText confidence="0.997324458333333">
where Z = {z : z|i E Zi for i = 0 ... n} by
simply computing z∗|i = argmaxz|i∈Zi fi(z|i) for
i = 0 ... n. Eq. 3 can be considered to be an approx-
imation to Eq. 1, where we have replaced Y with
Z. We will make direct use of this approximation
in the dual decomposition parsing algorithm. Note
that Y C Z, and in all but trivial cases, Y is a strict
subset of Z. For example, a structure z E Z could
have z(i, j) = z(j, i) = 1 for some (i, j); it could
contain longer cycles; or it could contain words that
do not modify exactly one head. Nevertheless, with
suitably powerful functions fi—for example func-
tions based on discriminative models—z∗ may be a
good approximation to y∗. Later we will see that
dual decomposition can effectively use MST infer-
ence to rule out ill-formed structures.
We now give the main assumption underlying sib-
ling models:
Assumption 1(Sibling Decompositions) A model
f(y) satisfies the sibling-decomposition assumption
if: 1) f(y) = Eni=0 fi(y|i) for some set offunctions
f0 ... fn. 2) For any i E {0 ... n}, for any value
of the variables u(i, j) E R for j = 1... n, it is
possible to compute
</bodyText>
<equation confidence="0.910211666666667">
argmax
y|i∈Zi �
�fi(y|i) − u(i,j)y(i,j)
�
j
in polynomial time.
</equation>
<bodyText confidence="0.969307666666667">
The second condition includes additional terms in-
volving u(i, j) variables that modify the scores of
individual dependencies. These terms are benign for
most definitions of fi, in that they do not alter de-
coding complexity. They will be of direct use in the
dual decomposition parsing algorithm.
Example 1: Bigram Sibling Models. Recall that
y|i is a binary vector specifying which words are
modifiers to the head-word i. Define l1 ... lp to be
the sequence of left modifiers to word i under y|i,
and r1 ... rq to be the set of right modifiers (e.g.,
consider the case where n = 5, i = 3, and we have
y(3,1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1:
in this case p = 1, l1 = 2, and q = 1, r1 = 4). In
bigram sibling models, we have
</bodyText>
<equation confidence="0.999693">
p+1 q+1
fi(y|i) = E gL(i, lk−1, lk) + E gR(i, rk−1, rk)
k=1 k=1
</equation>
<bodyText confidence="0.788710166666667">
where l0 = r0 = START is the initial state, and
lp+1 = rq+1 = END is the end state. The functions
gL and gR assign scores to bigram dependencies to
the left and right of the head. Under this model cal-
culating argmaxy|i∈Zi (fi (y|i) − Ej u(i,j)y(i, j))
takes O(n2) time using dynamic programming,
hence the model satisfies Assumption 1.
Example 2: Head Automata Head-automata
models constitute a second important model type
that satisfy the sibling-decomposition assumption
(bigram sibling models are a special case of head
automata). These models make use of functions
gR(i, s, s0, r) where s E S, s0 E S are variables in a
set of possible states S, and r is an index of a word
in the sentence such that i &lt; r &lt; n. The function
gR returns a cost for taking word r as the next depen-
dency, and transitioning from state s to s0. A similar
function gL is defined for left modifiers. We define
</bodyText>
<equation confidence="0.95473725">
fi(y|i, s0 ... sq, t0 ... tp) =
q p
E gR(i, sk−1, sk, rk) + E gL(i, tk−1, tk, ll)
k=1 k=1
</equation>
<bodyText confidence="0.9997505">
to be the joint score for dependencies y|i, and left
and right state sequences s0 ... sq and t0 ... tp. We
specify that s0 = t0 = START and sq = tp = END.
In this case we define
</bodyText>
<equation confidence="0.9953315">
fi(y|i) = max
s0...sq,t0...tp fi(y|i, s0 ... sq, t0 ... tp)
</equation>
<bodyText confidence="0.9998875">
and it follows that argmaxy|i∈Zi fi(y|i) can be com-
puted in O(n|S|2) time using a variant of the Viterbi
algorithm, hence the model satisfies the sibling-
decomposition assumption.
</bodyText>
<sectionHeader confidence="0.967832" genericHeader="method">
4 The Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999967333333333">
We now describe the dual decomposition parsing al-
gorithm for models that satisfy Assumption 1. Con-
sider the following generalization of the decoding
</bodyText>
<page confidence="0.982219">
1290
</page>
<figureCaption confidence="0.865285666666667">
Figure 1: The parsing algorithm for sibling decompos-
able models. αk &gt; 0 for k = 1... K are step sizes, see
Appendix A for details.
</figureCaption>
<bodyText confidence="0.493283">
problem from Eq. 1, where f(y) = Ei fi(y|i),
</bodyText>
<equation confidence="0.993509">
h(y) = E(i,j)∈I &apos;Y(i,j)y(i,j), and &apos;Y(i,j) E R for
all (i, j):1
argmax f(z) + h(y) (4)
z∈Z,y∈Y
such that z(i, j) = y(i, j) for all (i, j) E Z (5)
</equation>
<bodyText confidence="0.992050035714286">
Although the maximization w.r.t. z is taken over the
set i, the constraints in Eq. 5 ensure that z = y for
some y E Y, and hence that z E Y.
Without the z(i, j) = y(i, j) constraints, the
objective would decompose into the separate max-
imizations z∗ = argmaxz∈Z f(z), and y∗ =
argmaxy∈Y h(y), which can be easily solved us-
ing dynamic programming and MST, respectively.
Thus, it is these constraints that complicate the op-
timization. Our approach gets around this difficulty
by introducing new variables, u(i, j), that serve to
enforce agreement between the y(i, j) and z(i, j)
variables. In the next section we will show that these
u(i, j) variables are actually Lagrange multipliers
for the z(i, j) = y(i, j) constraints.
Our parsing algorithm is shown in Figure 1. At
each iteration k, the algorithm finds y(k) E Y us-
ing an MST algorithm, and z(k) E i through sep-
arate decoding of the (n + 1) sibling models. The
u(k) variables are updated if y(k)(i, j) =� z(k)(i, j)
1This is equivalent to Eq. 1 when γ(i, j) = 0 for all (i, j).
In some cases, however, it is convenient to have a model with
non-zero values for the γ variables; see the Appendix. Note that
this definition of h(y) allows argmaxy∈Y h(y) to be calculated
efficiently, using MST inference.
for some (i, j); these updates modify the objective
functions for the two decoding steps, and intuitively
encourage the y(k) and z(k) variables to be equal.
</bodyText>
<subsectionHeader confidence="0.997981">
4.1 Lagrangian Relaxation
</subsectionHeader>
<bodyText confidence="0.940565">
Recall that the main difficulty in solving Eq. 4 was
the z = y constraints. We deal with these con-
straints using Lagrangian relaxation (Lemar´echal,
2001). We first introduce Lagrange multipliers u =
{u(i, j) : (i, j) E Z}, and define the Lagrangian
</bodyText>
<equation confidence="0.928227">
L(u, y, z) = (6)
� �
u(i,j) y(i,j) − z(i,j)
If L∗ is the optimal value of Eq. 4 subject to the
constraints in Eq. 5, then for any value of u,
L∗ = max L(u, y, z) (7)
z∈Z,y∈Y,y=z
</equation>
<bodyText confidence="0.995902">
This follows because if y = z, the right term in Eq. 6
is zero for any value of u. The dual objective L(u)
is obtained by omitting the y = z constraint:
</bodyText>
<equation confidence="0.999905142857143">
L(u) = max
= max f(z) −
z∈Z,y∈Y
z∈Z i,j
� � u(i, j)z(i, j) �
+ max h(y) + u(i,j)y(i,j) .
y∈Y � � �
</equation>
<bodyText confidence="0.9081814">
i,j
Since L(u) maximizes over a larger space (y may
not equal z), we have that L∗ G L(u) (compare this
to Eq. 7). The dual problem, which our algorithm
optimizes, is to obtain the tightest such upper bound,
</bodyText>
<equation confidence="0.965354">
L(u). (8)
</equation>
<bodyText confidence="0.995896571428571">
The dual objective L(u) is convex, but not differen-
tiable. However, we can use a subgradient method
to derive an algorithm that is similar to gradient de-
scent, and which minimizes L(u). A subgradient of
a convex function L(u) at u is a vector du such that
for all v E R|I|, L(v) &gt; L(u) + du · (v − u). By
standard results,
</bodyText>
<equation confidence="0.960291833333334">
du(k) = y(k) − z(k)
is a subgradient for L(u) at u = u(k), where z(k) =
argmaxz∈Z f(z) − &amp;,j u(k)(i, j)z(i, j) and y(k) =
Set u(1)(i, j) , 0 for all (i, j) E Z
fork = 1 to K do
�
y(k) , argmax (γ(i, j) + u(k)(i, j))y(i, j)
y∈Y (i,j)∈I
for i E {0 ... n},
z|i , argmax � u(k)(i, j)z(i, j))
(k) (fi(z|i) −
z|i∈Zi j
if y(k)(i, j) = z(k)(i, j) for all (i, j) E Zthen
return (y(k),z(k))
for all (i, j) E Z,
u(k+1)(i, j) , u(k)(i, j)+αk(z(k)(i, j)−y(k)(i, j))
return (y(K), z(K))
�
f(z) + h(y) +
(i,j)∈I
y, z)
L(u,
(Dual problem) min
u∈R|I|
</equation>
<page confidence="0.60395">
1291
</page>
<bodyText confidence="0.885246571428571">
argmaxy∈Y h(y) + Ei,j u(k)(i, j)y(i, j). Subgra-
dient optimization methods are iterative algorithms
with updates that are similar to gradient descent:
we omit the details, except to note that when the LP
relaxation is not tight, the optimal primal solution to
the LP relaxation could be recovered by averaging
methods (Nedi´c and Ozdaglar, 2009).
</bodyText>
<equation confidence="0.991721">
u(k+1) = u(k) − αkdu(k) = u(k) − αk(y(k) − z(k)),
</equation>
<bodyText confidence="0.999507">
where αk is a step size. It is easily verified that the
algorithm in Figure 1 uses precisely these updates.
</bodyText>
<subsectionHeader confidence="0.949292">
4.2 Formal Guarantees
</subsectionHeader>
<bodyText confidence="0.998546">
With an appropriate choice of the step sizes αk, the
subgradient method can be shown to solve the dual
problem, i.e.
</bodyText>
<equation confidence="0.971437">
L(u(k)) = min
u
</equation>
<bodyText confidence="0.9830355">
See Korte and Vygen (2008), page 120, for details.
As mentioned before, the dual provides an up-
per bound on the optimum of the primal problem
(Eq. 4),
</bodyText>
<equation confidence="0.959234">
max f(z) + h(y) � min L(u). (9)
z∈Z,y∈Y,y=z u∈Rj�j
</equation>
<bodyText confidence="0.9996928">
However, we do not necessarily have strong
duality—i.e., equality in the above equation—
because the sets i and Y are discrete sets. That
said, for some functions h(y) and f(z) strong du-
ality does hold, as stated in the following:
</bodyText>
<construct confidence="0.88182">
Theorem 1 If for some k E {1... K} in the al-
gorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all
(i, j) E Z, then (y(k), z(k)) is a solution to the max-
imization problem in Eq. 4.
Proof. We have that f(z(k)) + h(y(k)) =
</construct>
<bodyText confidence="0.995225866666667">
L(u(k), z(k), y(k)) = L(u(k)), where the last equal-
ity is because y(k), z(k) are defined as the respective
argmax’s. Thus, the inequality in Eq. 9 is tight, and
(y(k), z(k)) and u(k) are primal and dual optimal.
Although the algorithm is not guaranteed to sat-
isfy y(k) = z(k) for some k, by Theorem 1 if it does
reach such a state, then we have the guarantee of an
exact solution to Eq. 4, with the dual solution u pro-
viding a certificate of optimality. We show in the
experiments that this occurs very frequently, in spite
of the parsing problem being NP-hard.
It can be shown that Eq. 8 is the dual of an LP
relaxation of the original problem. When the con-
ditions of Theorem 1 are satisfied, it means that the
LP relaxation is tight for this instance. For brevity
</bodyText>
<sectionHeader confidence="0.979605" genericHeader="method">
5 Grandparent Dependency Models
</sectionHeader>
<bodyText confidence="0.999758">
In this section we extend the approach to consider
grandparent relations. In grandparent models each
parse tree y is represented as a vector
</bodyText>
<equation confidence="0.672569">
y = {y(i,j) : (i, j) E Z} U {y↑(i,j) : (i, j) E Z}
</equation>
<bodyText confidence="0.999687333333333">
where we have added a second set of duplicate vari-
ables, y↑(i, j) for all (i, j) E Z. The set of all valid
parse trees is then defined as
</bodyText>
<equation confidence="0.992614833333333">
Y = {y : y(i, j) variables forma directed tree,
y↑(i, j) = y(i, j) for all (i, j) E Z}
We again partition the variables into n + 1 subsets,
y|0 ... y|n, by (re)defining
y|i = {y(i, j) : j = 1... n, j =� i}
U{y↑(k, i) : k = 0 ... n, k =� i}
</equation>
<bodyText confidence="0.9989696">
So as before y|i contains variables y(i, j) which in-
dicate which words modify the i’th word. In addi-
tion, y|i includes y↑(k, i) variables that indicate the
word that word i itself modifies.
The set of all possible values of y|i is now
</bodyText>
<equation confidence="0.997176">
ii = {y|i : y(i,j) E {0, 1} for j = 1 ... n,j =�i;
y↑(k, i) E {0, 1} for k = 0 ... n, k =� i;
E y↑(k,i) = 1}
k
</equation>
<bodyText confidence="0.999154">
Hence the y(i, j) variables can take any values, but
only one of the y↑(k, i) variables can be equal to 1
(as only one word can be a parent of word i). As be-
fore, we define i = {y : y|i E ii for i = 0 ... n}.
We introduce the following assumption:
</bodyText>
<figure confidence="0.899609857142857">
Assumption 2 (GS Decompositions)
A model f(y) satisfies the grandparent/sibling-
decomposition (GSD) assumption if: 1) f(z) =
Eni=0 fi(z|i) for some set of functions f0 ... fn. 2)
For any i E {0 ... n}, for any value of the variables
u(i, j) E R for j = 1... n, and v(k, i) E R for
k = 0 ... n, it is possible to compute
lim
k→∞
L(u).
argmax � � v(k, i)z↑(k, i))
zji∈Zi (fi(z|i)− u(i, j)z(i, j)−
j k
in polynomial time.
</figure>
<page confidence="0.88601">
1292
</page>
<bodyText confidence="0.463539">
Again, it follows that we can approxi-
</bodyText>
<equation confidence="0.996684666666667">
mate y∗ = argmaxy∈Y Pni=0 fi(y|i) by
z∗ = argmaxz∈Z Pni=0 fi(z|i), by defining
z∗|i = argmaxz|i∈Zi fi(z|i) for i = 0 ... n. The
</equation>
<bodyText confidence="0.949839818181818">
resulting vector z∗ may be deficient in two respects.
First, the variables z∗(i, j) may not form a well-
formed directed spanning tree. Second, we may
have z∗↑(i, j) =� z∗(i, j) for some values of (i, j).
Example 3: Grandparent/Sibling Models An
important class of models that satisfy Assumption 2
are defined as follows. Again, for a vector y|i de-
fine l1 ... lp to be the sequence of left modifiers to
word i under y|i, and r1 ... rq to be the set of right
modifiers. Define k∗ to the value for k such that
y↑(k, i) = 1. Then the model is defined as follows:
</bodyText>
<equation confidence="0.999636">
p+1 q+1
fi(y|i) = X gL(i, k∗, lj−1, lj)+ X gR(i,k∗,rj−1,rj)
j=1 j=1
</equation>
<bodyText confidence="0.999895333333333">
This is very similar to the bigram-sibling model, but
with the modification that the gL and gR functions
depend in addition on the value for k∗. This al-
lows these functions to model grandparent depen-
dencies such as (k∗, i, lj) and sibling dependencies
such as (i, lj−1, lj). Finding z∗|i under the definition
can be accomplished in O(n3) time, by decoding the
model using dynamic programming separately for
each of the O(n) possible values of k∗, and pick-
ing the value for k∗ that gives the maximum value
under these decodings.
A dual-decomposition algorithm for models that
satisfy the GSD assumption is shown in Figure 2.
The algorithm can be justified as an instance of La-
grangian relaxation applied to the problem
</bodyText>
<equation confidence="0.9785666">
argmax f(z) + h(y) (10)
z∈Z,y∈Y
with constraints
z(i, j) = y(i, j) for all (i, j) E Z (11)
z↑(i, j) = y(i, j) for all (i, j) E Z (12)
</equation>
<bodyText confidence="0.9961154">
The algorithm employs two sets of Lagrange mul-
tipliers, u(i, j) and v(i, j), corresponding to con-
straints in Eqs. 11 and 12. As in Theorem 1, if at any
point in the algorithm z(k) = y(k), then (z(k), y(k))
is an exact solution to the problem in Eq. 10.
</bodyText>
<figureCaption confidence="0.991552">
Figure 2: The parsing algorithm for grandparent/sibling-
decomposable models.
</figureCaption>
<sectionHeader confidence="0.969258" genericHeader="method">
6 The Training Algorithm
</sectionHeader>
<bodyText confidence="0.998812181818182">
In our experiments we make use of discriminative
linear models, where for an input sentence x, the
score for a parse y is f(y) = w &apos; φ(x, y) where
w E Rd is a parameter vector, and φ(x, y) E Rd
is a feature-vector representing parse tree y in con-
junction with sentence x. We will assume that the
features decompose in the same way as the sibling-
decomposable or grandparent/sibling-decomposable
models, that is φ(x, y) = Pni=0 φ(x, y|i) for some
feature vector definition φ(x, y|i). In the bigram sib-
ling models in our experiments, we assume that
</bodyText>
<equation confidence="0.993233666666667">
φ(x, y|i) = p+1X φL(x, i, lk−1, lk) + 9+ 1 φR(x, i, rk−1, rk)
k=1 X
k=1
</equation>
<bodyText confidence="0.999170636363636">
where as before l1 ... lp and r1 ... rq are left and
right modifiers under y|i, and where φL and φR
are feature vector definitions. In the grandparent
models in our experiments, we use a similar defi-
nition with feature vectors φL(x, i, k∗, lk−1, lk) and
φR(x, i, k∗, rk−1, rk), where k∗ is the parent for
word i under y|i.
We train the model using the averaged perceptron
for structured problems (Collins, 2002). Given the
i’th example in the training set, (x(i), y(i)), the per-
ceptron updates are as follows:
</bodyText>
<listItem confidence="0.9997085">
• z∗ = argmaxy∈Z w &apos; φ(x(i), y)
• If z∗ =� y(i), w = w+φ(x(i),y(i))−φ(x(i),z∗)
</listItem>
<equation confidence="0.998577083333333">
Set u(1)(i, j) +- 0, v(1)(i, j) +- 0 for all (i, j) E Z
fork = 1 to K do
X
y(k) +- argmax y(i, j)θ(i, j)
yEY (i�j)EZ
where θ(i, j) = γ(i, j) + u(k)(i, j) + v(k)(i, j).
for i E {0 ... n},
z|i +- argmax
(k)
X
(fi(z|i) −
j
z|iEZi
u(k)(i, j)z(i, j)
X− v(k)(j, i)zt(j, i))
j
if y(k)(i, j) = z(k)(i, j) = z(k)
t (i, j) for all (i, j) E Z
then
return (y(k),z(k))
for all (i, j) E Z,
u(k+1)(i,j) +- u(k)(i, j)+αk(z(k)(i, j)−y(k)(i, j))
v(k+1)(i,j) +- v(k)(i, j)+αk(z(k) t(i, j)−y(k)(i, j))
return (y(K),z(K))
</equation>
<page confidence="0.784691">
1293
</page>
<bodyText confidence="0.999972411764706">
The first step involves inference over the set Z,
rather than Y as would be standard in the percep-
tron. Thus, decoding during training can be achieved
by dynamic programming over head automata alone,
which is very efficient.
Our training approach is closely related to local
training methods (Punyakanok et al., 2005). We
have found this method to be effective, very likely
because Z is a superset of Y. Our training algo-
rithm is also related to recent work on training using
outer bounds (see, e.g., (Taskar et al., 2003; Fin-
ley and Joachims, 2008; Kulesza and Pereira, 2008;
Martins et al., 2009)). Note, however, that the LP re-
laxation optimized by dual decomposition is signifi-
cantly tighter than Z. Thus, an alternative approach
would be to use the dual decomposition algorithm
for inference during training.
</bodyText>
<sectionHeader confidence="0.99937" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999975642857143">
We report results on a number of data sets. For
comparison to Martins et al. (2009), we perform ex-
periments for Danish, Dutch, Portuguese, Slovene,
Swedish and Turkish data from the CoNLL-X
shared task (Buchholz and Marsi, 2006), and En-
glish data from the CoNLL-2008 shared task (Sur-
deanu et al., 2008). We use the official training/test
splits for these data sets, and the same evaluation
methodology as Martins et al. (2009). For com-
parison to Smith and Eisner (2008), we also re-
port results on Danish and Dutch using their alter-
nate training/test split. Finally, we report results on
the English WSJ treebank, and the Prague treebank.
We use feature sets that are very similar to those
described in Carreras (2007). We use marginal-
based pruning, using marginals calculated from an
arc-factored spanning tree model using the matrix-
tree theorem (McDonald and Satta, 2007; Smith and
Smith, 2007; Koo et al., 2007).
In all of our experiments we set the value K, the
maximum number of iterations of dual decompo-
sition in Figures 1 and 2, to be 5,000. If the al-
gorithm does not terminate—i.e., it does not return
(y(k), z(k)) within 5,000 iterations—we simply take
the parse y(k) with the maximum value of f(y(k)) as
the output from the algorithm. At first sight 5,000
might appear to be a large number, but decoding is
still fast—see Sections 7.3 and 7.4 for discussion.2
</bodyText>
<footnote confidence="0.947386">
2Note also that the feature vectors 0 and inner products w·0
</footnote>
<bodyText confidence="0.999847333333333">
The strategy for choosing step sizes αk is described
in Appendix A, along with other details.
We first discuss performance in terms of accu-
racy, success in recovering an exact solution, and
parsing speed. We then describe additional experi-
ments examining various aspects of the algorithm.
</bodyText>
<subsectionHeader confidence="0.989704">
7.1 Accuracy
</subsectionHeader>
<bodyText confidence="0.999979357142857">
Table 1 shows results for previous work on the var-
ious data sets, and results for an arc-factored model
with pure MST decoding with our features. (We use
the acronym UAS (unlabeled attachment score) for
dependency accuracy.) We also show results for the
bigram-sibling and grandparent/sibling (G+S) mod-
els under dual decomposition. Both the bigram-
sibling and G+S models show large improvements
over the arc-factored approach; they also compare
favorably to previous work—for example the G+S
model gives better results than all results reported in
the CoNLL-X shared task, on all languages. Note
that we use different feature sets from both Martins
et al. (2009) and Smith and Eisner (2008).
</bodyText>
<subsectionHeader confidence="0.995333">
7.2 Success in Recovering Exact Solutions
</subsectionHeader>
<bodyText confidence="0.989745157894737">
Next, we consider how often our algorithms return
an exact solution to the original optimization prob-
lem, with a certificate—i.e., how often the algo-
rithms in Figures 1 and 2 terminate with y(k) = z(k)
for some value of k &lt; 5000 (and are thus optimal,
by Theorem 1). The CertS and CertG columns in Ta-
ble 1 give the results for the sibling and G+S models
respectively. For all but one setting3 over 95% of the
test sentences are decoded exactly, with 99% exact-
ness in many cases.
For comparison, we also ran both the single-
commodity flow and multiple-commodity flow LP
relaxations of Martins et al. (2009) with our mod-
els and features. We measure how often these re-
laxations terminate with an exact solution. The re-
sults in Table 2 show that our method gives exact
solutions more often than both of these relaxations.4
In computing the accuracy figures for Martins et al.
only need to be computed once, thus saving computation.
</bodyText>
<footnote confidence="0.9991316">
3The exception is Slovene, which has the smallest training
set at only 1534 sentences.
4Note, however, that it is possible that the Martins et al. re-
laxations would have given a higher proportion of integral solu-
tions if their relaxation was used during training.
</footnote>
<page confidence="0.946976">
1294
</page>
<table confidence="0.991803">
Ma09 MST Sib G+S Best CertS CertG TimeS TimeG TrainS TrainG
Dan 91.18 89.74 91.08 91.78 91.54 99.07 98.45 0.053 0.169 0.051 0.109
Dut 85.57 82.33 84.81 85.81 85.57 98.19 97.93 0.035 0.120 0.046 0.048
Por 92.11 90.68 92.57 93.03 92.11 99.65 99.31 0.047 0.257 0.077 0.103
Slo 85.61 82.39 84.89 86.21 85.61 90.55 95.27 0.158 0.295 0.054 0.130
Swe 90.60 88.79 90.10 91.36 90.60 98.71 98.97 0.035 0.141 0.036 0.055
Tur 76.34 75.66 77.14 77.55 76.36 98.72 99.04 0.021 0.047 0.016 0.036
Eng1 91.16 89.20 91.18 91.59 — 98.65 99.18 0.082 0.200 0.032 0.076
Eng2 — 90.29 92.03 92.57 — 98.96 99.12 0.081 0.168 0.032 0.076
Sm08 MST Sib G+S — CertS CertG TimeS TimeG TrainS TrainG
Dan 86.5 87.89 89.58 91.00 — 98.50 98.50 0.043 0.120 0.053 0.065
Dut 88.5 88.86 90.87 91.76 — 98.00 99.50 0.036 0.046 0.050 0.054
Mc06 MST Sib G+S — CertS CertG TimeS TimeG TrainS TrainG
PTB 91.5 90.10 91.96 92.46 — 98.89 98.63 0.062 0.210 0.028 0.078
PDT 85.2 84.36 86.44 87.32 — 96.67 96.43 0.063 0.221 0.019 0.051
</table>
<tableCaption confidence="0.975578">
Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our first-
</tableCaption>
<figureCaption confidence="0.677090363636364">
order baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via
dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08:
The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald
and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task
(Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald
and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples
for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for
test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing,
test decoding was carried out on identical machines with zero additional load; however, training was conducted on
machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on
the CoNLL-08 validation set, following Martins et al. (2009). Eng2: UAS when testing on the CoNLL-08 test set.
</figureCaption>
<bodyText confidence="0.975064777777778">
(2009), we project fractional solutions to a well-
formed spanning tree, as described in that paper.
Finally, to better compare the tightness of our
LP relaxation to that of earlier work, we consider
randomly-generated instances. Table 2 gives results
for our model and the LP relaxations of Martins et al.
(2009) with randomly generated scores on automata
transitions. We again recover exact solutions more
often than the Martins et al. relaxations. Note that
with random parameters the percentage of exact so-
lutions is significantly lower, suggesting that the ex-
actness of decoding of the trained models is a special
case. We speculate that this is due to the high perfor-
mance of approximate decoding with i in place of
Y under the trained models for fi; the training algo-
rithm described in section 6 may have the tendency
to make the LP relaxation tight.
% of Head Automata Recomputed
</bodyText>
<subsectionHeader confidence="0.929686">
7.3 Speed
</subsectionHeader>
<tableCaption confidence="0.9697095">
Table 1, columns TimeS and TimeG, shows decod-
ing times for the dual decomposition algorithms.
Table 2 gives speed comparisons to Martins et al.
(2009). Our method gives significant speed-ups over
</tableCaption>
<figure confidence="0.944347">
0 1000 2000 3000 4000 5000
Iterations of Dual Decomposition
</figure>
<figureCaption confidence="0.988323333333333">
Figure 3: The average percentage of head automata that
must be recomputed on each iteration of dual decompo-
sition on the PTB validation set.
</figureCaption>
<bodyText confidence="0.994803666666667">
the Martins et al. (2009) method, presumably be-
cause it leverages the underlying structure of the
problem, rather than using a generic solver.
</bodyText>
<subsectionHeader confidence="0.999783">
7.4 Lazy Decoding
</subsectionHeader>
<bodyText confidence="0.999928333333333">
Here we describe an important optimization in the
dual decomposition algorithms. Consider the algo-
rithm in Figure 1. At each iteration we must find
</bodyText>
<equation confidence="0.498360333333333">
z|i = argmax
(k)
zliEZi
</equation>
<page confidence="0.227507">
25
</page>
<figure confidence="0.991165222222222">
20
30
15
10
5
0
% recomputed, g+s
% recomputed, sib
(fi(z|i) − � u(k)(2, j)z(2, j))
</figure>
<page confidence="0.941364">
9
1295
</page>
<table confidence="0.993169384615385">
Percentage
Sib Acc Int Time Rand
LP(S) 92.14 88.29 0.14 11.7
LP(M) 92.17 93.18 0.58 30.6
ILP 92.19 100.0 1.44 100.0
DD-5000 92.19 98.82 0.08 35.6
DD-250 92.23 89.29 0.03 10.2
G+S Acc Int Time Rand
LP(S) 92.60 91.64 0.23 0.0
LP(M) 92.58 94.41 0.75 0.0
ILP 92.70 100.0 1.79 100.0
DD-5000 92.71 98.76 0.23 6.8
DD-250 92.66 85.47 0.12 0.0
</table>
<tableCaption confidence="0.9088238">
Table 2: A comparison of dual decomposition with lin-
ear programs described by Martins et al. (2009). LP(S):
Linear Program relaxation based on single-commodity
flow. LP(M): Linear Program relaxation based on
multi-commodity flow. ILP: Exact Integer Linear Pro-
</tableCaption>
<bodyText confidence="0.959492785714286">
gram. DD-5000/DD-250: Dual decomposition with non-
projective head automata, with K = 5000/250. Upper
results are for the sibling model, lower results are G+S.
Columns give scores for UAS accuracy, percentage of so-
lutions which are integral, and solution speed in seconds
per sentence. These results are for Section 22 of the PTB.
The last column is the percentage of integral solutions on
a random problem of length 10 words. The (I)LP experi-
ments were carried out using Gurobi, a high-performance
commercial-grade solver.
for i = 0 ... n. However, if for some i, u(k)(i, j) =
u(k−1)(i, j) for all j, then z(k) |�= z(k−1)
|� . In
lazy decoding we immediately set z(k)
</bodyText>
<equation confidence="0.889001">
|� = z(k−1)
</equation>
<bodyText confidence="0.914477571428572">
|� if
u(k)(i, j) = u(k−1)(i, j) for all j; this check takes
O(n) time, and saves us from decoding with the i’th
automaton. In practice, the updates to u are very
sparse, and this condition occurs very often in prac-
tice. Figure 3 demonstrates the utility of this method
for both sibling automata and G+S automata.
</bodyText>
<subsectionHeader confidence="0.994046">
7.5 Early Stopping
</subsectionHeader>
<bodyText confidence="0.999980181818182">
We also ran experiments varying the value of K—
the maximum number of iterations—in the dual de-
composition algorithms. As before, if we do not find
y(k) = z(k) for some value of k G K, we choose
the y(k) with optimal value for f(y(k)) as the final
solution. Figure 4 shows three graphs: 1) the accu-
racy of the parser on PTB validation data versus the
value for K; 2) the percentage of examples where
y(k) = z(k) at some point during the algorithm,
hence the algorithm returns a certificate of optimal-
ity; 3) the percentage of examples where the solution
</bodyText>
<figure confidence="0.9814055">
0 200 400 600 800 1000
Maximum Number of Dual Decomposition Iterations
</figure>
<figureCaption confidence="0.997024">
Figure 4: The behavior of the dual-decomposition parser
with sibling automata as the value of K is varied.
</figureCaption>
<table confidence="0.998629666666667">
Sib P-Sib G+S P-G+S
PTB 92.19 92.34 92.71 92.70
PDT 86.41 85.67 87.40 86.43
</table>
<tableCaption confidence="0.996751">
Table 3: UAS of projective and non-projective decoding
</tableCaption>
<bodyText confidence="0.939854583333333">
for the English (PTB) and Czech (PDT) validation sets.
Sib/G+S: as in Table 1. P-Sib/P-G+S: Projective versions
of Sib/G+S, where the MST component has been re-
placed with the Eisner (2000) first-order projective parser.
returned is the same as the solution for the algorithm
with K = 5000 (our original setting). It can be seen
for K as small as 250 we get very similar accuracy
to K = 5000 (see Table 2). In fact, for this set-
ting the algorithm returns the same solution as for
K = 5000 on 99.59% of the examples. However
only 89.29% of these solutions are produced with a
certificate of optimality (y(k) = z(k)).
</bodyText>
<subsectionHeader confidence="0.968192">
7.6 How Good is the Approximation z∗?
</subsectionHeader>
<bodyText confidence="0.999954272727273">
We ran experiments measuring the quality of z∗ =
argmaxz∈Z f(z), where f(z) is given by the
perceptron-trained bigram-sibling model. Because
z∗ may not be a well-formed tree with n dependen-
cies, we report precision and recall rather than con-
ventional dependency accuracy. Results on the PTB
validation set were 91.11%/88.95% precision/recall,
which is accurate considering the unconstrained na-
ture of the predictions. Thus the z∗ approximation is
clearly a good one; we suspect that this is one reason
for the good convergence results for the method.
</bodyText>
<subsectionHeader confidence="0.995539">
7.7 Importance of Non-Projective Decoding
</subsectionHeader>
<bodyText confidence="0.999900333333333">
It is simple to adapt the dual-decomposition algo-
rithms in figures 1 and 2 to give projective depen-
dency structures: the set Y is redefined to be the set
</bodyText>
<figure confidence="0.996704777777778">
100
90
80
70
60
50
% validation UAS
% certificates
% match K=5000
</figure>
<page confidence="0.984394">
1296
</page>
<bodyText confidence="0.999932444444445">
of all projective structures, with the arg max over Y
being calculated using a projective first-order parser
(Eisner, 2000). Table 3 shows results for projec-
tive and non-projective parsing using the dual de-
composition approach. For Czech data, where non-
projective structures are common, non-projective
decoding has clear benefits. In contrast, there is little
difference in accuracy between projective and non-
projective decoding on English.
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="method">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999968">
We have described dual decomposition algorithms
for non-projective parsing, which leverage existing
dynamic programming and MST algorithms. There
are a number of possible areas for future work. As
described in section 7.7, the algorithms can be easily
modified to consider projective structures by replac-
ing Y with the set of projective trees, and then using
first-order dependency parsing algorithms in place
of MST decoding. This method could be used to
derive parsing algorithms that include higher-order
features, as an alternative to specialized dynamic
programming algorithms. Eisner (2000) describes
extensions of head automata to include word senses;
we have not discussed this issue in the current pa-
per, but it is simple to develop dual decomposition
algorithms for this case, using similar methods to
those used for the grandparent models. The gen-
eral approach should be applicable to other lexical-
ized syntactic formalisms, and potentially also to de-
coding in syntax-driven translation. In addition, our
dual decomposition approach is well-suited to paral-
lelization. For example, each of the head-automata
could be optimized independently in a multi-core or
GPU architecture. Finally, our approach could be
used with other structured learning algorithms, e.g.
Meshi et al. (2010).
</bodyText>
<sectionHeader confidence="0.81477" genericHeader="method">
A Implementation Details
</sectionHeader>
<bodyText confidence="0.992147333333333">
This appendix describes details of the algorithm,
specifically choice of the step sizes αk, and use of
the -y(i, j) parameters.
</bodyText>
<subsectionHeader confidence="0.767065">
A.1 Choice of Step Sizes
</subsectionHeader>
<bodyText confidence="0.996628461538462">
We have found the following method to be effec-
tive. First, define S = f(z(1)) − f(y(1)), where
(z(1), y(1)) is the output of the algorithm on the first
iteration (note that we always have S &gt; 0 since
f(z(1)) = L(u(1))). Then define αk = S/(1 + 77k),
where 77k is the number of times that L(u(k�)) &gt;
L(u(k&apos;−1)) for k&apos; &lt; k. Hence the learning rate drops
at a rate of 1/(1 + t), where t is the number of times
that the dual increases from one iteration to the next.
A.2 Use of the -y(i, j) Parameters
The parsing algorithms both consider a general-
ized problem that includes -y(i, j) parameters. We
now describe how these can be useful. Re-
call that the optimization problem is to solve
argmaxzEZ,yEY f(z) + h(y), subject to a set of
agreement constraints. In our models, f(z) can
be written as f&apos;(z) + Eij α(i, j)z(i, j) where
f&apos;(z) includes only terms depending on higher-
order (non arc-factored features), and α(i, j) are
weights that consider the dependency between i
and j alone. For any value of 0 &lt; Q &lt;
1, the problem argmaxzEZ,yEY f2(z) + h2(y) is
equivalent to the original problem, if f2(z) =
f&apos;(z) + (1 − Q) &amp;,j α(i,j)z(i,j) and h2(y) =
Q Ei,j α(i, j)y(i, j). We have simply shifted the
α(i, j) weights from one model to the other. While
the optimization problem remains the same, the al-
gorithms in Figure 1 and 2 will converge at differ-
ent rates depending on the value for Q. In our ex-
periments we set Q = 0.001, which puts almost
all the weight in the head-automata models, but al-
lows weights on spanning tree edges to break ties in
MST inference in a sensible way. We suspect this is
important in early iterations of the algorithm, when
many values for u(i, j) or v(i, j) will be zero, and
where with Q = 0 many spanning tree solutions y(k)
would be essentially random, leading to very noisy
updates to the u(i, j) and v(i, j) values. We have
not tested other values for Q.
</bodyText>
<sectionHeader confidence="0.769628" genericHeader="conclusions">
Acknowledgments MIT gratefully acknowledges the
</sectionHeader>
<reference confidence="0.864152222222222">
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. A. Rush was supported by the GALE program of
the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was
supported by a Google PhD Fellowship.
</reference>
<page confidence="0.996358">
1297
</page>
<sectionHeader confidence="0.993643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999829631578948">
H. Alshawi. 1996. Head Automata and Bilingual Tiling:
Translation with Minimal Representations. In Proc.
ACL, pages 167–176.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
CoNLL, pages 149–164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. EMNLP-
CoNLL, pages 957–961.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. EMNLP, pages
1–8.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Us-
ing Combinatorial Optimization within Max-Product
Belief Propagation. In NIPS, pages 369–376.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29–62.
T. Finley and T. Joachims. 2008. Training structural
svms when exact inference is intractable. In ICML,
pages 304–311.
A.K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. Handbook of Formal Languages: Beyond
Words, 3:69–123.
N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF
Optimization via Dual Decomposition: Message-
Passing Revisited. In Proc. ICCV.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured Prediction Models via the Matrix-Tree The-
orem. In Proc. EMNLP-CoNLL, pages 141–150.
B.H. Korte and J. Vygen. 2008. Combinatorial Opti-
mization: Theory and Algorithms. Springer Verlag.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In NIPS.
C. Lemar´echal. 2001. Lagrangian Relaxation. In Com-
putational Combinatorial Optimization, Optimal or
Provably Near-Optimal Solutions [based on a Spring
School], pages 112–156, London, UK. Springer-
Verlag.
A.F.T. Martins, D. Das, N.A. Smith, and E.P. Xing. 2008.
Stacking Dependency Parsers. In Proc. EMNLP,
pages 157–166.
A.F.T. Martins, N.A. Smith., and E.P. Xing. 2009. Con-
cise Integer Linear Programming Formulations for De-
pendency Parsing. In Proc. ACL, pages 342–350.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. EACL, pages 81–88.
R. McDonald and G. Satta. 2007. On the Complexity of
Non-Projective Data-Driven Dependency Parsing. In
Proc. IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proc. HLT-EMNLP, pages 523–
530.
O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson.
2010. Learning Efficiently with Approximate Infer-
ence via Dual Losses. In Proc. ICML.
A. Nedi´c and A. Ozdaglar. 2009. Approximate
Primal Solutions and Rate Analysis for Dual Sub-
gradient Methods. SIAM Journal on Optimization,
19(4):1757–1780.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers. In
Proc. ACL, pages 950–958.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and Inference over Constrained Output. In
Proc. IJCAI, pages 1124–1129.
S. Riedel and J. Clarke. 2006. Incremental Integer Linear
Programming for Non-projective Dependency Parsing.
In Proc. EMNLP, pages 129–137.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relaxations for Natural Language Processing. In
Proc. EMNLP.
D.A. Smith and J. Eisner. 2008. Dependency Parsing by
Belief Propagation. In Proc. EMNLP, pages 145–156.
D.A. Smith and N.A. Smith. 2007. Probabilistic Mod-
els of Nonprojective Dependency Trees. In Proc.
EMNLP-CoNLL, pages 132–140.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP Relaxations for MAP
using Message Passing. In Proc. UAI.
M. Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies.
In Proc. CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697–3717.
</reference>
<page confidence="0.992456">
1298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321321">
<title confidence="0.996938">Dual Decomposition for Parsing with Non-Projective Head Automata</title>
<author confidence="0.999565">Terry Koo Alexander M Rush Michael Collins Tommi Jaakkola David</author>
<affiliation confidence="0.376764">MIT CSAIL, Cambridge, MA 02139,</affiliation>
<abstract confidence="0.9865823125">This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government. A. Rush was supported by the GALE program of the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was supported by a Google PhD Fellowship.</title>
<marker></marker>
<rawString>support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government. A. Rush was supported by the GALE program of the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was supported by a Google PhD Fellowship.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head Automata and Bilingual Tiling: Translation with Minimal Representations.</title>
<date>1996</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>167--176</pages>
<contexts>
<context position="1646" citStr="Alshawi (1996)" startWordPosition="236" endWordPosition="237">the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case. Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms. In particular, 1. Decoding for individual head-words can be accomplished using dynamic programming. 2. Decoding for arc-factored models can be accomplished using directed minimum-weight spanning tree </context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head Automata and Bilingual Tiling: Translation with Minimal Representations. In Proc. ACL, pages 167–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="23993" citStr="Buchholz and Marsi, 2006" startWordPosition="4342" endWordPosition="4345">lso related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the m</context>
<context position="28993" citStr="Buchholz and Marsi, 2006" startWordPosition="5190" endWordPosition="5193">7.32 — 96.67 96.43 0.063 0.221 0.019 0.051 Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proc. CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a Higher-Order Projective Dependency Parser. In</title>
<date>2007</date>
<booktitle>Proc. EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="24482" citStr="Carreras (2007)" startWordPosition="4428" endWordPosition="4429">ments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y(k), z(k)) within 5,000 iterations—we simply take the parse y(k) with the maximum value of f(y(k)) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a Higher-Order Projective Dependency Parser. In Proc. EMNLPCoNLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="22253" citStr="Collins, 2002" startWordPosition="4028" endWordPosition="4029">y) = Pni=0 φ(x, y|i) for some feature vector definition φ(x, y|i). In the bigram sibling models in our experiments, we assume that φ(x, y|i) = p+1X φL(x, i, lk−1, lk) + 9+ 1 φR(x, i, rk−1, rk) k=1 X k=1 where as before l1 ... lp and r1 ... rq are left and right modifiers under y|i, and where φL and φR are feature vector definitions. In the grandparent models in our experiments, we use a similar definition with feature vectors φL(x, i, k∗, lk−1, lk) and φR(x, i, k∗, rk−1, rk), where k∗ is the parent for word i under y|i. We train the model using the averaged perceptron for structured problems (Collins, 2002). Given the i’th example in the training set, (x(i), y(i)), the perceptron updates are as follows: • z∗ = argmaxy∈Z w &apos; φ(x(i), y) • If z∗ =� y(i), w = w+φ(x(i),y(i))−φ(x(i),z∗) Set u(1)(i, j) +- 0, v(1)(i, j) +- 0 for all (i, j) E Z fork = 1 to K do X y(k) +- argmax y(i, j)θ(i, j) yEY (i�j)EZ where θ(i, j) = γ(i, j) + u(k)(i, j) + v(k)(i, j). for i E {0 ... n}, z|i +- argmax (k) X (fi(z|i) − j z|iEZi u(k)(i, j)z(i, j) X− v(k)(j, i)zt(j, i)) j if y(k)(i, j) = z(k)(i, j) = z(k) t (i, j) for all (i, j) E Z then return (y(k),z(k)) for all (i, j) E Z, u(k+1)(i,j) +- u(k)(i, j)+αk(z(k)(i, j)−y(k)(i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>D Tarlow</author>
<author>G Elidan</author>
<author>D Koller</author>
</authors>
<title>Using Combinatorial Optimization within Max-Product Belief Propagation. In</title>
<date>2007</date>
<booktitle>NIPS,</booktitle>
<pages>369--376</pages>
<contexts>
<context position="5936" citStr="Duchi et al., 2007" startWordPosition="881" endWordPosition="884">rial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency </context>
</contexts>
<marker>Duchi, Tarlow, Elidan, Koller, 2007</marker>
<rawString>J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Using Combinatorial Optimization within Max-Product Belief Propagation. In NIPS, pages 369–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<contexts>
<context position="1627" citStr="Eisner (2000)" startWordPosition="233" endWordPosition="234">s. Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case. Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms. In particular, 1. Decoding for individual head-words can be accomplished using dynamic programming. 2. Decoding for arc-factored models can be accomplished using directed minimum-we</context>
<context position="34137" citStr="Eisner (2000)" startWordPosition="6062" endWordPosition="6063">lgorithm, hence the algorithm returns a certificate of optimality; 3) the percentage of examples where the solution 0 200 400 600 800 1000 Maximum Number of Dual Decomposition Iterations Figure 4: The behavior of the dual-decomposition parser with sibling automata as the value of K is varied. Sib P-Sib G+S P-G+S PTB 92.19 92.34 92.71 92.70 PDT 86.41 85.67 87.40 86.43 Table 3: UAS of projective and non-projective decoding for the English (PTB) and Czech (PDT) validation sets. Sib/G+S: as in Table 1. P-Sib/P-G+S: Projective versions of Sib/G+S, where the MST component has been replaced with the Eisner (2000) first-order projective parser. returned is the same as the solution for the algorithm with K = 5000 (our original setting). It can be seen for K as small as 250 we get very similar accuracy to K = 5000 (see Table 2). In fact, for this setting the algorithm returns the same solution as for K = 5000 on 99.59% of the examples. However only 89.29% of these solutions are produced with a certificate of optimality (y(k) = z(k)). 7.6 How Good is the Approximation z∗? We ran experiments measuring the quality of z∗ = argmaxz∈Z f(z), where f(z) is given by the perceptron-trained bigram-sibling model. Be</context>
<context position="35543" citStr="Eisner, 2000" startWordPosition="6304" endWordPosition="6305">cision/recall, which is accurate considering the unconstrained nature of the predictions. Thus the z∗ approximation is clearly a good one; we suspect that this is one reason for the good convergence results for the method. 7.7 Importance of Non-Projective Decoding It is simple to adapt the dual-decomposition algorithms in figures 1 and 2 to give projective dependency structures: the set Y is redefined to be the set 100 90 80 70 60 50 % validation UAS % certificates % match K=5000 1296 of all projective structures, with the arg max over Y being calculated using a projective first-order parser (Eisner, 2000). Table 3 shows results for projective and non-projective parsing using the dual decomposition approach. For Czech data, where nonprojective structures are common, non-projective decoding has clear benefits. In contrast, there is little difference in accuracy between projective and nonprojective decoding on English. 8 Conclusions We have described dual decomposition algorithms for non-projective parsing, which leverage existing dynamic programming and MST algorithms. There are a number of possible areas for future work. As described in section 7.7, the algorithms can be easily modified to cons</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. Advances in Probabilistic and Other Parsing Technologies, pages 29–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Training structural svms when exact inference is intractable.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="23485" citStr="Finley and Joachims, 2008" startWordPosition="4260" endWordPosition="4264">(k+1)(i,j) +- v(k)(i, j)+αk(z(k) t(i, j)−y(k)(i, j)) return (y(K),z(K)) 1293 The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. Our training algorithm is also related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the offic</context>
</contexts>
<marker>Finley, Joachims, 2008</marker>
<rawString>T. Finley and T. Joachims. 2008. Training structural svms when exact inference is intractable. In ICML, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-Adjoining Grammars. Handbook of Formal Languages: Beyond Words,</title>
<date>1997</date>
<pages>3--69</pages>
<contexts>
<context position="4348" citStr="Joshi and Schabes, 1997" startWordPosition="640" endWordPosition="643">nds/sentence for the 1288 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternat</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-Adjoining Grammars. Handbook of Formal Languages: Beyond Words, 3:69–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF Optimization via Dual Decomposition: MessagePassing Revisited. In</title>
<date>2007</date>
<booktitle>Proc. ICCV.</booktitle>
<contexts>
<context position="5490" citStr="Komodakis et al., 2007" startWordPosition="812" endWordPosition="815">e ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergen</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF Optimization via Dual Decomposition: MessagePassing Revisited. In Proc. ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured Prediction Models via the Matrix-Tree Theorem.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="24678" citStr="Koo et al., 2007" startWordPosition="4457" endWordPosition="4460"> 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y(k), z(k)) within 5,000 iterations—we simply take the parse y(k) with the maximum value of f(y(k)) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2Note also that the feature vectors 0 and inner products w·0 The strategy for choosing step sizes αk is described in Appendix A, along with oth</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured Prediction Models via the Matrix-Tree Theorem. In Proc. EMNLP-CoNLL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Korte</author>
<author>J Vygen</author>
</authors>
<title>Combinatorial Optimization: Theory and Algorithms.</title>
<date>2008</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="16216" citStr="Korte and Vygen (2008)" startWordPosition="2844" endWordPosition="2847">on methods are iterative algorithms with updates that are similar to gradient descent: we omit the details, except to note that when the LP relaxation is not tight, the optimal primal solution to the LP relaxation could be recovered by averaging methods (Nedi´c and Ozdaglar, 2009). u(k+1) = u(k) − αkdu(k) = u(k) − αk(y(k) − z(k)), where αk is a step size. It is easily verified that the algorithm in Figure 1 uses precisely these updates. 4.2 Formal Guarantees With an appropriate choice of the step sizes αk, the subgradient method can be shown to solve the dual problem, i.e. L(u(k)) = min u See Korte and Vygen (2008), page 120, for details. As mentioned before, the dual provides an upper bound on the optimum of the primal problem (Eq. 4), max f(z) + h(y) � min L(u). (9) z∈Z,y∈Y,y=z u∈Rj�j However, we do not necessarily have strong duality—i.e., equality in the above equation— because the sets i and Y are discrete sets. That said, for some functions h(y) and f(z) strong duality does hold, as stated in the following: Theorem 1 If for some k E {1... K} in the algorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all (i, j) E Z, then (y(k), z(k)) is a solution to the maximization problem in Eq. 4. Proof. We have</context>
</contexts>
<marker>Korte, Vygen, 2008</marker>
<rawString>B.H. Korte and J. Vygen. 2008. Combinatorial Optimization: Theory and Algorithms. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>F Pereira</author>
</authors>
<title>Structured learning with approximate inference.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="23512" citStr="Kulesza and Pereira, 2008" startWordPosition="4265" endWordPosition="4268">(z(k) t(i, j)−y(k)(i, j)) return (y(K),z(K)) 1293 The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. Our training algorithm is also related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits fo</context>
</contexts>
<marker>Kulesza, Pereira, 2008</marker>
<rawString>A. Kulesza and F. Pereira. 2008. Structured learning with approximate inference. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lemar´echal</author>
</authors>
<title>Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School],</title>
<date>2001</date>
<pages>112--156</pages>
<publisher>SpringerVerlag.</publisher>
<location>London, UK.</location>
<marker>Lemar´echal, 2001</marker>
<rawString>C. Lemar´echal. 2001. Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School], pages 112–156, London, UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>D Das</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Stacking Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="29157" citStr="Martins et al. (2008)" startWordPosition="5219" endWordPosition="5222">. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2: UAS when testing on the CoNLL-08 test s</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>A.F.T. Martins, D. Das, N.A. Smith, and E.P. Xing. 2008. Stacking Dependency Parsers. In Proc. EMNLP, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise Integer Linear Programming Formulations for Dependency Parsing. In</title>
<date>2009</date>
<booktitle>Proc. ACL,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="3365" citStr="Martins et al., 2009" startWordPosition="489" endWordPosition="492">mata, and describe the parsing algorithm. The algorithm can be viewed as an instance of Lagrangian relaxation; we describe this connection, and give convergence guarantees for the method. We describe a generalization to models that include grandparent dependencies. We then introduce a perceptron-driven training algorithm that makes use of point 1 above. We describe experiments on non-projective parsing for a number of languages, and in particular compare the dual decomposition algorithm to approaches based on general-purpose linear programming (LP) or integer linear programming (ILP) solvers (Martins et al., 2009). The accuracy of our models is higher than previous work on a broad range of datasets. The method gives exact solutions to the decoding problem, together with a certificate of optimality, on over 98% of test examples for many of the test languages, with parsing times ranging between 0.021 seconds/sentence for the most simple languages/models, to 0.295 seconds/sentence for the 1288 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics most complex sett</context>
<context position="4919" citStr="Martins et al. (2009)" startWordPosition="724" endWordPosition="727">d formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective</context>
<context position="23535" citStr="Martins et al., 2009" startWordPosition="4269" endWordPosition="4272">eturn (y(K),z(K)) 1293 The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. Our training algorithm is also related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and </context>
<context position="26159" citStr="Martins et al. (2009)" startWordPosition="4703" endWordPosition="4706">on the various data sets, and results for an arc-factored model with pure MST decoding with our features. (We use the acronym UAS (unlabeled attachment score) for dependency accuracy.) We also show results for the bigram-sibling and grandparent/sibling (G+S) models under dual decomposition. Both the bigramsibling and G+S models show large improvements over the arc-factored approach; they also compare favorably to previous work—for example the G+S model gives better results than all results reported in the CoNLL-X shared task, on all languages. Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008). 7.2 Success in Recovering Exact Solutions Next, we consider how often our algorithms return an exact solution to the original optimization problem, with a certificate—i.e., how often the algorithms in Figures 1 and 2 terminate with y(k) = z(k) for some value of k &lt; 5000 (and are thus optimal, by Theorem 1). The CertS and CertG columns in Table 1 give the results for the sibling and G+S models respectively. For all but one setting3 over 95% of the test sentences are decoded exactly, with 99% exactness in many cases. For comparison, we also ran both the singlecommod</context>
<context position="28740" citStr="Martins et al. (2009)" startWordPosition="5146" endWordPosition="5149">0 — 98.50 98.50 0.043 0.120 0.053 0.065 Dut 88.5 88.86 90.87 91.76 — 98.00 99.50 0.036 0.046 0.050 0.054 Mc06 MST Sib G+S — CertS CertG TimeS TimeG TrainS TrainG PTB 91.5 90.10 91.96 92.46 — 98.89 98.63 0.062 0.210 0.028 0.078 PDT 85.2 84.36 86.44 87.32 — 96.67 96.43 0.063 0.221 0.019 0.051 Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. T</context>
<context position="30072" citStr="Martins et al. (2009)" startWordPosition="5357" endWordPosition="5360">ed out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2: UAS when testing on the CoNLL-08 test set. (2009), we project fractional solutions to a wellformed spanning tree, as described in that paper. Finally, to better compare the tightness of our LP relaxation to that of earlier work, we consider randomly-generated instances. Table 2 gives results for our model and the LP relaxations of Martins et al. (2009) with randomly generated scores on automata transitions. We again recover exact solutions more often than the Martins et al. relaxations. Note that with random parameters the percentage of exact solutions is significantly lower, suggesting that the exactness of decoding of the trained models is a special case. We speculate that this is due to the high performance of approximate decoding with i in place of Y under the trained models for fi; the training algorithm described in section 6 may have the tendency to make the LP relaxation tight. % of Head Automata Recomputed 7.3 Speed Table 1, column</context>
<context position="31915" citStr="Martins et al. (2009)" startWordPosition="5676" endWordPosition="5679">. Consider the algorithm in Figure 1. At each iteration we must find z|i = argmax (k) zliEZi 25 20 30 15 10 5 0 % recomputed, g+s % recomputed, sib (fi(z|i) − � u(k)(2, j)z(2, j)) 9 1295 Percentage Sib Acc Int Time Rand LP(S) 92.14 88.29 0.14 11.7 LP(M) 92.17 93.18 0.58 30.6 ILP 92.19 100.0 1.44 100.0 DD-5000 92.19 98.82 0.08 35.6 DD-250 92.23 89.29 0.03 10.2 G+S Acc Int Time Rand LP(S) 92.60 91.64 0.23 0.0 LP(M) 92.58 94.41 0.75 0.0 ILP 92.70 100.0 1.79 100.0 DD-5000 92.71 98.76 0.23 6.8 DD-250 92.66 85.47 0.12 0.0 Table 2: A comparison of dual decomposition with linear programs described by Martins et al. (2009). LP(S): Linear Program relaxation based on single-commodity flow. LP(M): Linear Program relaxation based on multi-commodity flow. ILP: Exact Integer Linear Program. DD-5000/DD-250: Dual decomposition with nonprojective head automata, with K = 5000/250. Upper results are for the sibling model, lower results are G+S. Columns give scores for UAS accuracy, percentage of solutions which are integral, and solution speed in seconds per sentence. These results are for Section 22 of the PTB. The last column is the percentage of integral solutions on a random problem of length 10 words. The (I)LP exper</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A.F.T. Martins, N.A. Smith., and E.P. Xing. 2009. Concise Integer Linear Programming Formulations for Dependency Parsing. In Proc. ACL, pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms. In</title>
<date>2006</date>
<booktitle>Proc. EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="4573" citStr="McDonald and Pereira (2006)" startWordPosition="670" endWordPosition="673">s most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the</context>
<context position="28871" citStr="McDonald and Pereira (2006)" startWordPosition="5169" endWordPosition="5172">ertS CertG TimeS TimeG TrainS TrainG PTB 91.5 90.10 91.96 92.46 — 98.89 98.63 0.062 0.210 0.028 0.078 PDT 85.2 84.36 86.44 87.32 — 96.67 96.43 0.063 0.221 0.019 0.051 Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proc. EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the Complexity of Non-Projective Data-Driven Dependency Parsing. In</title>
<date>2007</date>
<booktitle>Proc. IWPT.</booktitle>
<contexts>
<context position="1148" citStr="McDonald and Satta, 2007" startWordPosition="161" endWordPosition="164">ng on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets. 1 Introduction Non-projective dependency parsing is useful for many languages that exhibit non-projective syntactic structures. Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higher</context>
<context position="4722" citStr="McDonald and Satta (2007)" startWordPosition="692" endWordPosition="695">e percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial</context>
<context position="24636" citStr="McDonald and Satta, 2007" startWordPosition="4449" endWordPosition="4452">from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y(k), z(k)) within 5,000 iterations—we simply take the parse y(k) with the maximum value of f(y(k)) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2Note also that the feature vectors 0 and inner products w·0 The strategy for choosing step sizes αk </context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the Complexity of Non-Projective Data-Driven Dependency Parsing. In Proc. IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proc. HLT-EMNLP, pages 523– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Meshi</author>
<author>D Sontag</author>
<author>T Jaakkola</author>
<author>A Globerson</author>
</authors>
<title>Learning Efficiently with Approximate Inference via Dual Losses. In</title>
<date>2010</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="37166" citStr="Meshi et al. (2010)" startWordPosition="6544" endWordPosition="6547">iscussed this issue in the current paper, but it is simple to develop dual decomposition algorithms for this case, using similar methods to those used for the grandparent models. The general approach should be applicable to other lexicalized syntactic formalisms, and potentially also to decoding in syntax-driven translation. In addition, our dual decomposition approach is well-suited to parallelization. For example, each of the head-automata could be optimized independently in a multi-core or GPU architecture. Finally, our approach could be used with other structured learning algorithms, e.g. Meshi et al. (2010). A Implementation Details This appendix describes details of the algorithm, specifically choice of the step sizes αk, and use of the -y(i, j) parameters. A.1 Choice of Step Sizes We have found the following method to be effective. First, define S = f(z(1)) − f(y(1)), where (z(1), y(1)) is the output of the algorithm on the first iteration (note that we always have S &gt; 0 since f(z(1)) = L(u(1))). Then define αk = S/(1 + 77k), where 77k is the number of times that L(u(k�)) &gt; L(u(k&apos;−1)) for k&apos; &lt; k. Hence the learning rate drops at a rate of 1/(1 + t), where t is the number of times that the dual</context>
</contexts>
<marker>Meshi, Sontag, Jaakkola, Globerson, 2010</marker>
<rawString>O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. 2010. Learning Efficiently with Approximate Inference via Dual Losses. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nedi´c</author>
<author>A Ozdaglar</author>
</authors>
<title>Approximate Primal Solutions and Rate Analysis for Dual Subgradient Methods.</title>
<date>2009</date>
<journal>SIAM Journal on Optimization,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Nedi´c, Ozdaglar, 2009</marker>
<rawString>A. Nedi´c and A. Ozdaglar. 2009. Approximate Primal Solutions and Rate Analysis for Dual Subgradient Methods. SIAM Journal on Optimization, 19(4):1757–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating GraphBased and Transition-Based Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="29130" citStr="Nivre and McDonald (2008)" startWordPosition="5214" endWordPosition="5217">k. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing, test decoding was carried out on identical machines with zero additional load; however, training was conducted on machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on the CoNLL-08 validation set, following Martins et al. (2009). Eng2: UAS when tes</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating GraphBased and Transition-Based Dependency Parsers. In Proc. ACL, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and Inference over Constrained Output. In</title>
<date>2005</date>
<booktitle>Proc. IJCAI,</booktitle>
<pages>1124--1129</pages>
<contexts>
<context position="23254" citStr="Punyakanok et al., 2005" startWordPosition="4218" endWordPosition="4221">x (k) X (fi(z|i) − j z|iEZi u(k)(i, j)z(i, j) X− v(k)(j, i)zt(j, i)) j if y(k)(i, j) = z(k)(i, j) = z(k) t (i, j) for all (i, j) E Z then return (y(k),z(k)) for all (i, j) E Z, u(k+1)(i,j) +- u(k)(i, j)+αk(z(k)(i, j)−y(k)(i, j)) v(k+1)(i,j) +- v(k)(i, j)+αk(z(k) t(i, j)−y(k)(i, j)) return (y(K),z(K)) 1293 The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. Our training algorithm is also related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we p</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learning and Inference over Constrained Output. In Proc. IJCAI, pages 1124–1129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental Integer Linear Programming for Non-projective Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>129--137</pages>
<contexts>
<context position="4859" citStr="Riedel and Clarke (2006)" startWordPosition="713" endWordPosition="716">up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al.</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental Integer Linear Programming for Non-projective Dependency Parsing. In Proc. EMNLP, pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In</title>
<date>2010</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="6194" citStr="Rush et al. (2010)" startWordPosition="921" endWordPosition="924">e models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be Z = {(i, j) : i E {0 ... n}, j E {1... n}, i =� j}. A dependency parse is a vector y = {y(i, j) : (i, j) E Z}, where y(i, j) = 1 if a dependency with head word i and modifier j is in the parse, 0 otherwise. We use i = 0 for the root symbol. We </context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency Parsing by Belief Propagation.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="5961" citStr="Smith and Eisner, 2008" startWordPosition="885" endWordPosition="888">ur work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. Consider the dependency parsing problem for a sentence with n words. We define the index set for dependency parsing to be Z = {(i, j)</context>
<context position="24234" citStr="Smith and Eisner (2008)" startWordPosition="4384" endWordPosition="4387">significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm doe</context>
<context position="26187" citStr="Smith and Eisner (2008)" startWordPosition="4708" endWordPosition="4711">and results for an arc-factored model with pure MST decoding with our features. (We use the acronym UAS (unlabeled attachment score) for dependency accuracy.) We also show results for the bigram-sibling and grandparent/sibling (G+S) models under dual decomposition. Both the bigramsibling and G+S models show large improvements over the arc-factored approach; they also compare favorably to previous work—for example the G+S model gives better results than all results reported in the CoNLL-X shared task, on all languages. Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008). 7.2 Success in Recovering Exact Solutions Next, we consider how often our algorithms return an exact solution to the original optimization problem, with a certificate—i.e., how often the algorithms in Figures 1 and 2 terminate with y(k) = z(k) for some value of k &lt; 5000 (and are thus optimal, by Theorem 1). The CertS and CertG columns in Table 1 give the results for the sibling and G+S models respectively. For all but one setting3 over 95% of the test sentences are decoded exactly, with 99% exactness in many cases. For comparison, we also ran both the singlecommodity flow and multiple-commod</context>
<context position="28811" citStr="Smith and Eisner (2008)" startWordPosition="5159" endWordPosition="5162">98.00 99.50 0.036 0.046 0.050 0.054 Mc06 MST Sib G+S — CertS CertG TimeS TimeG TrainS TrainG PTB 91.5 90.10 91.96 92.46 — 98.89 98.63 0.062 0.210 0.028 0.078 PDT 85.2 84.36 86.44 87.32 — 96.67 96.43 0.063 0.221 0.019 0.051 Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our firstorder baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al. (2009). Sm08: The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. (2009, Table 1); note that the latter includes McDonald and Pereira (2006), Nivre and McDonald (2008), and Martins et al. (2008). CertS/CertG: Percent of test examples for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consis</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D.A. Smith and J. Eisner. 2008. Dependency Parsing by Belief Propagation. In Proc. EMNLP, pages 145–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic Models of Nonprojective Dependency Trees.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>132--140</pages>
<contexts>
<context position="24659" citStr="Smith and Smith, 2007" startWordPosition="4453" endWordPosition="4456"> task (Surdeanu et al., 2008). We use the official training/test splits for these data sets, and the same evaluation methodology as Martins et al. (2009). For comparison to Smith and Eisner (2008), we also report results on Danish and Dutch using their alternate training/test split. Finally, we report results on the English WSJ treebank, and the Prague treebank. We use feature sets that are very similar to those described in Carreras (2007). We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007). In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000. If the algorithm does not terminate—i.e., it does not return (y(k), z(k)) within 5,000 iterations—we simply take the parse y(k) with the maximum value of f(y(k)) as the output from the algorithm. At first sight 5,000 might appear to be a large number, but decoding is still fast—see Sections 7.3 and 7.4 for discussion.2 2Note also that the feature vectors 0 and inner products w·0 The strategy for choosing step sizes αk is described in Appendi</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D.A. Smith and N.A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proc. EMNLP-CoNLL, pages 132–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>T Meltzer</author>
<author>A Globerson</author>
<author>T Jaakkola</author>
<author>Y Weiss</author>
</authors>
<title>Tightening LP Relaxations for MAP using Message Passing. In</title>
<date>2008</date>
<booktitle>Proc. UAI.</booktitle>
<contexts>
<context position="5829" citStr="Sontag et al. (2008)" startWordPosition="866" endWordPosition="869">cient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems. 3 Sibling Models This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models. C</context>
</contexts>
<marker>Sontag, Meltzer, Globerson, Jaakkola, Weiss, 2008</marker>
<rawString>D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. 2008. Tightening LP Relaxations for MAP using Message Passing. In Proc. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proc. CoNLL.</booktitle>
<publisher>MIT</publisher>
<contexts>
<context position="4370" citStr="Steedman, 2000" startWordPosition="645" endWordPosition="646">edings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. 2 Related Work McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models. McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formula</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. MIT Press. M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="23458" citStr="Taskar et al., 2003" startWordPosition="4256" endWordPosition="4259">)(i, j)−y(k)(i, j)) v(k+1)(i,j) +- v(k)(i, j)+αk(z(k) t(i, j)−y(k)(i, j)) return (y(K),z(K)) 1293 The first step involves inference over the set Z, rather than Y as would be standard in the perceptron. Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient. Our training approach is closely related to local training methods (Punyakanok et al., 2005). We have found this method to be effective, very likely because Z is a superset of Y. Our training algorithm is also related to recent work on training using outer bounds (see, e.g., (Taskar et al., 2003; Finley and Joachims, 2008; Kulesza and Pereira, 2008; Martins et al., 2009)). Note, however, that the LP relaxation optimized by dual decomposition is significantly tighter than Z. Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training. 7 Experiments We report results on a number of data sets. For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et a</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>MAP estimation via agreement on trees: message-passing and linear programming.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Information Theory,</booktitle>
<volume>51</volume>
<pages>3697--3717</pages>
<contexts>
<context position="5465" citStr="Wainwright et al., 2005" startWordPosition="808" endWordPosition="811">and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations. Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms. Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization. Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007). However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models. Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)). Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach. Unlike LBP, our method has strong theoretical guarantees, suc</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2005</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP estimation via agreement on trees: message-passing and linear programming. In IEEE Transactions on Information Theory, volume 51, pages 3697–3717.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>