<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006493">
<title confidence="0.983818">
Language-Independent Parsing with Empty Elements
</title>
<author confidence="0.871714">
Shu Cai and David Chiang
</author>
<affiliation confidence="0.696717">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.663482">
Marina del Rey, CA 90292
</address>
<email confidence="0.987485">
{shucai,chiang}@isi.edu
</email>
<author confidence="0.986277">
Yoav Goldberg
</author>
<affiliation confidence="0.998155">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.74209">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.967547">
yoavg@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.983158" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992338833333333">
We present a simple, language-independent
method for integrating recovery of empty ele-
ments into syntactic parsing. This method out-
performs the best published method we are
aware of on English and a recently published
method on Chinese.
</bodyText>
<sectionHeader confidence="0.992069" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859346153846">
Empty elements in the syntactic analysis of a sen-
tence are markers that show where a word or phrase
might otherwise be expected to appear, but does not.
They play an important role in understanding the
grammatical relations in the sentence. For example,
in the tree of Figure 2a, the first empty element (*)
marks where John would be if believed were in the
active voice (someone believed...), and the second
empty element (*T*) marks where the man would be
if who were not fronted (John was believed to admire
who?).
Empty elements exist in many languages and serve
different purposes. In languages such as Chinese and
Korean, where subjects and objects can be dropped
to avoid duplication, empty elements are particularly
important, as they indicate the position of dropped
arguments. Figure 1 gives an example of a Chinese
parse tree with empty elements. The first empty el-
ement (*pro*) marks the subject of the whole sen-
tence, a pronoun inferable from context. The second
empty element (*PRO*) marks the subject of the de-
pendent VP (shíshī fǎlǜ tiáowén).
The Penn Treebanks (Marcus et al., 1993; Xue
et al., 2005) contain detailed annotations of empty
elements. Yet most parsing work based on these
resources has ignored empty elements, with some
</bodyText>
<equation confidence="0.966017">
PVP
212
IP
VP P PV I
</equation>
<bodyText confidence="0.87357">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
gories by the use of lattice parsing. The method
is language-independent and performs very well on
both languages we tested it on: for English, it out-
performs the best published method we are aware of
(Schmid, 2006), and for Chinese, it outperforms the
method of Yang and Xue (2010).1
</bodyText>
<sectionHeader confidence="0.914666" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999960769230769">
Our method is fairly simple. We take a state-of-the-
art parsing model, the Berkeley parser (Petrov et al.,
2006), train it on data with explicit empty elements,
and test it on word lattices that can nondeterminis-
tically insert empty elements anywhere. The idea is
that the state-splitting of the parsing model will en-
able it to learn where to expect empty elements to be
inserted into the test sentences.
Tree transformations Prior to training, we alter
the annotation of empty elements so that the termi-
nal label is a consistent symbol (E), the preterminal
label is the type of the empty element, and -NONE-
is deleted (see Figure 2b). This simplifies the lat-
tices because there is only one empty symbol, and
helps the parsing model to learn dependencies be-
tween nonterminal labels and empty-category types
because there is no intervening -NONE-.
Then, following Schmid (2006), if a constituent
contains an empty element that is linked to another
node with label X, then we append /X to its label.
If there is more than one empty element, we pro-
cess them bottom-up (see Figure 2b). This helps the
parser learn to expect where to find empty elements.
In our experiments, we did this only for elements of
type *T*. Finally, we train the Berkeley parser on the
preprocessed training data.
Lattice parsing Unlike the training data, the test
data does not mark any empty elements. We allow
the parser to produce empty elements by means of
lattice-parsing (Chappelier et al., 1999), a general-
ization of CKY parsing allowing it to parse a word-
lattice instead of a predetermined list of terminals.
Lattice parsing adds a layer of flexibility to exist-
ing parsing technology, and allows parsing in sit-
uations where the yield of the tree is not known
in advance. Lattice parsing originated in the speech
1Unfortunately, not enough information was available to
carry out comparison with the method of Chung and Gildea
(2010).
processing community (Hall, 2005; Chappelier et
al., 1999), and was recently applied to the task
of joint clitic-segmentation and syntactic-parsing in
Hebrew (Goldberg and Tsarfaty, 2008; Goldberg
and Elhadad, 2011) and Arabic (Green and Man-
ning, 2010). Here, we use lattice parsing for empty-
element recovery.
We use a modified version of the Berkeley parser
which allows handling lattices as input.2 The modifi-
cation is fairly straightforward: Each lattice arc cor-
respond to a lexical item. Lexical items are now in-
dexed by their start and end states rather than by
their sentence position, and the initialization proce-
dure of the CKY chart is changed to allow lexical
items of spans greater than 1. We then make the nec-
essary adjustments to the parsing algorithm to sup-
port this change: trying rules involving preterminals
even when the span is greater than 1, and not relying
on span size for identifying lexical items.
At test time, we first construct a lattice for each
test sentence that allows 0, 1, or 2 empty symbols
(E) between each pair of words or at the start/end of
the sentence. Then we feed these lattices through our
lattice parser to produce trees with empty elements.
Finally, we reverse the transformations that had been
applied to the training data.
</bodyText>
<sectionHeader confidence="0.994775" genericHeader="method">
3 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.999719">
Evaluation metrics for empty-element recovery are
not well established, and previous studies use a vari-
ety of metrics. We review several of these here and
additionally propose a unified evaluation of parsing
and empty-element recovery.3
If A and B are multisets, let A(x) be the number
of occurrences of x in A, let |A |= ∑x A(x), and
let A ∩ B be the multiset such that (A ∩ B)(x) =
min(A(x), B(x)). If T is the multiset of “items” in the
trees being tested and G is the multiset of “items” in
the gold-standard trees, then
</bodyText>
<equation confidence="0.96272">
|G ∩ T ||G ∩ T|
precision = recall =
|T ||G|
2
F1 =
</equation>
<table confidence="0.5558365">
1 + 1
precision recall
</table>
<footnote confidence="0.4712442">
2The modified parser is available at http://www.cs.bgu.
ac.il/~yoavg/software/blatt/
3We provide a scoring script which supports all of these eval-
uation metrics. The code is available at http://www.isi.edu/
~chiang/software/eevalb.py .
</footnote>
<page confidence="0.586871">
213
</page>
<table confidence="0.874885928571429">
SBARQ
SQ N
PV
S
Section System Labeled Labeled All Labeled
Empty Brackets Empty Elements Brackets
P R F1 P R F1 P R F1
00 Schmid (2006) 88.3 82.9 85.5 89.4 83.8 86.5 87.1 85.6 86.3
split 5x merge 50% 91.0 79.8 85.0 93.1 81.8 87.1 90.4 88.7 89.5
split 6x merge 50% 91.9 81.1 86.1 93.6 82.4 87.6 90.4 89.1 89.7
split 6x merge 75% 92.7 80.7 86.3 94.6 82.0 87.9 90.3 88.5 89.3
split 7x merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7
23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4
split 6x merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3
</table>
<tableCaption confidence="0.999255">
Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.
</tableCaption>
<table confidence="0.999771375">
Unlabeled Labeled All Labeled
Empty Elements Empty Elements Brackets
Task System P R F1 P R F1 P R F1
Dev split 5x merge 50% 82.5 58.0 68.1 72.6 51.8 60.5 84.6 80.7 82.6
split 6x merge 50% 76.4 60.5 67.5 68.2 55.1 60.9 83.2 81.3 82.2
split 7x merge 50% 74.9 58.7 65.8 65.9 52.5 58.5 82.7 81.1 81.9
Test Yang and Xue (2010) 80.3 57.9 63.2
split 6x merge 50% 74.0 61.3 67.0 66.0 54.5 58.6 82.7 80.8 81.7
</table>
<tableCaption confidence="0.999792">
Table 2: Results on Penn (Chinese) Treebank.
</tableCaption>
<bodyText confidence="0.999776">
Chinese We also experimented on a subset of
the Penn Chinese Treebank 6.0. For comparabil-
ity with previous work (Yang and Xue, 2010),
we trained the parser on sections 0081–0900, used
sections 0041–0080 for development, and sections
0001–0040 and 0901–0931 for testing. The results
are shown in Table 2. We selected the 6th split-merge
cycle based on the labeled empty elements F1 mea-
sure. The unlabeled empty elements column shows
that our system outperforms the baseline system of
Yang and Xue (2010). We also analyzed the empty-
element recall by type (Table 3). Our system outper-
formed that of Yang and Xue (2010) especially on
*pro*, used for dropped arguments, and *T*, used
for relative clauses and topicalization.
</bodyText>
<sectionHeader confidence="0.966955" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999711125">
The empty-element recovery method we have
presented is simple, highly effective, and fully
integrated with state of the art parsing. We hope
to exploit cross-lingual information about empty
elements in machine translation. Chung and
Gildea (2010) have shown that such information
indeed helps translation, and we plan to extend this
work by handling more empty categories (rather
</bodyText>
<table confidence="0.99989525">
Type Total Correct Recall
Gold YX Ours YX Ours
*pro* 290 125 159 43.1 54.8
*PRO* 299 196 199 65.6 66.6
*T* 578 338 388 58.5 67.1
*RNR* 32 20 15 62.5 46.9
*OP* 134 20 65 14.9 48.5
* 19 5 3 26.3 15.8
</table>
<tableCaption confidence="0.9445585">
Table 3: Recall on different types of empty categories.
YX = (Yang and Xue, 2010), Ours = split 6x.
</tableCaption>
<bodyText confidence="0.9992147">
than just *pro* and *PRO*), and to incorporate them
into a syntax-based translation model instead of a
phrase-based model.
We also plan to extend our work here to recover
coindexation information (links between a moved el-
ement and the trace which marks the position it was
moved from). As a step towards shallow semantic
analysis, this may further benefit other natural lan-
guage processing tasks such as machine translation
and summary generation.
</bodyText>
<sectionHeader confidence="0.951664" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999882">
We would like to thank Slav Petrov for his help in
running the Berkeley parser, and Yaqin Yang, Bert
</bodyText>
<page confidence="0.815291">
215
</page>
<bodyText confidence="0.999683444444444">
Xue, Tagyoung Chung, and Dan Gildea for their an-
swering our many questions. We would also like
to thank our colleagues in the Natural Language
Group at ISI for meaningful discussions and the
anonymous reviewers for their thoughtful sugges-
tions. This work was supported in part by DARPA
under contracts HR0011-06-C-0022 (subcontract to
BBN Technologies) and DOI-NBC N10AP20031,
and by NSF under contract IIS-0908532.
</bodyText>
<sectionHeader confidence="0.983289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657084745763">
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. DARPA Speech and Natu-
ral Language Workshop.
Richard Campbell. 2004. Using linguistic principles to
recover empty categories. In Proc. ACL.
J.-C. Chappelier, M. Rajman, R. Aragdes, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Proc. Traitement Automatique du Langage Naturel
(TALN).
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Proc.
EMNLP.
Peter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proc. EMNLP.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proc. NAACL
HLT.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFG-LA lat-
tice parser. In Proc. of ACL.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis. In
Proc of COLING-2010.
Keith B. Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University, Providence, RI, USA.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proc. ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313–330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proc. LREC.
Helmut Schmid. 2006. Trace prediction and recovery
with unlexicalized PCFGs and slash features. In Proc.
COLING-ACL.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proc. COLING.
</reference>
<page confidence="0.963158">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.173232">
<title confidence="0.998777">Language-Independent Parsing with Empty Elements</title>
<author confidence="0.98656">Cai</author>
<affiliation confidence="0.997914">USC Information Sciences</affiliation>
<address confidence="0.992004">4676 Admiralty Way, Suite</address>
<author confidence="0.923385">Marina del Rey</author>
<author confidence="0.923385">CA</author>
<email confidence="0.997848">shucai@isi.edu</email>
<email confidence="0.997848">chiang@isi.edu</email>
<degree confidence="0.4115775">Yoav Ben Gurion University of the</degree>
<affiliation confidence="0.981114">Department of Computer</affiliation>
<address confidence="0.918395">POB 653 Be’er Sheva, 84105,</address>
<email confidence="0.996069">yoavg@cs.bgu.ac.il</email>
<abstract confidence="0.996952571428571">We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proc. DARPA Speech and Natural Language Workshop.</booktitle>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proc. DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Campbell</author>
</authors>
<title>Using linguistic principles to recover empty categories.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Campbell, 2004</marker>
<rawString>Richard Campbell. 2004. Using linguistic principles to recover empty categories. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
<author>R Aragdes</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Proc. Traitement Automatique du Langage Naturel (TALN).</booktitle>
<contexts>
<context position="3824" citStr="Chappelier et al., 1999" startWordPosition="623" endWordPosition="626">, following Schmid (2006), if a constituent contains an empty element that is linked to another node with label X, then we append /X to its label. If there is more than one empty element, we process them bottom-up (see Figure 2b). This helps the parser learn to expect where to find empty elements. In our experiments, we did this only for elements of type *T*. Finally, we train the Berkeley parser on the preprocessed training data. Lattice parsing Unlike the training data, the test data does not mark any empty elements. We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Go</context>
</contexts>
<marker>Chappelier, Rajman, Aragdes, Rozenknop, 1999</marker>
<rawString>J.-C. Chappelier, M. Rajman, R. Aragdes, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In Proc. Traitement Automatique du Langage Naturel (TALN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of empty categories on machine translation.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="4261" citStr="Chung and Gildea (2010)" startWordPosition="696" endWordPosition="699">tice parsing Unlike the training data, the test data does not mark any empty elements. We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modified version of the Berkeley parser which allows handling lattices as input.2 The modification is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and th</context>
<context position="8502" citStr="Chung and Gildea (2010)" startWordPosition="1436" endWordPosition="1439">ments F1 measure. The unlabeled empty elements column shows that our system outperforms the baseline system of Yang and Xue (2010). We also analyzed the emptyelement recall by type (Table 3). Our system outperformed that of Yang and Xue (2010) especially on *pro*, used for dropped arguments, and *T*, used for relative clauses and topicalization. 5 Discussion and Future Work The empty-element recovery method we have presented is simple, highly effective, and fully integrated with state of the art parsing. We hope to exploit cross-lingual information about empty elements in machine translation. Chung and Gildea (2010) have shown that such information indeed helps translation, and we plan to extend this work by handling more empty categories (rather Type Total Correct Recall Gold YX Ours YX Ours *pro* 290 125 159 43.1 54.8 *PRO* 299 196 199 65.6 66.6 *T* 578 338 388 58.5 67.1 *RNR* 32 20 15 62.5 46.9 *OP* 134 20 65 14.9 48.5 * 19 5 3 26.3 15.8 Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6x. than just *pro* and *PRO*), and to incorporate them into a syntax-based translation model instead of a phrase-based model. We also plan to extend our work here to recov</context>
</contexts>
<marker>Chung, Gildea, 2010</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2010. Effects of empty categories on machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Antecedent recovery: Experiments with a trace tagger. In</title>
<date>2003</date>
<booktitle>Proc. EMNLP.</booktitle>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Peter Dienes and Amit Dubey. 2003. Antecedent recovery: Experiments with a trace tagger. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Seth Kulick</author>
<author>Mitchell Marcus</author>
</authors>
<title>Fully parsing the Penn Treebank.</title>
<date>2006</date>
<booktitle>In Proc. NAACL HLT.</booktitle>
<marker>Gabbard, Kulick, Marcus, 2006</marker>
<rawString>Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006. Fully parsing the Penn Treebank. In Proc. NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4478" citStr="Goldberg and Elhadad, 2011" startWordPosition="726" endWordPosition="729">arsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modified version of the Berkeley parser which allows handling lattices as input.2 The modification is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving pre</context>
</contexts>
<marker>Goldberg, Elhadad, 2011</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2011. Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4449" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="722" endWordPosition="725">9), a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modified version of the Berkeley parser which allows handling lattices as input.2 The modification is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this chang</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Better Arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proc of COLING-2010.</booktitle>
<contexts>
<context position="4515" citStr="Green and Manning, 2010" startWordPosition="732" endWordPosition="736">e instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modified version of the Berkeley parser which allows handling lattices as input.2 The modification is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving preterminals even when the span is great</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher D. Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In Proc of COLING-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith B Hall</author>
</authors>
<title>Best-first word-lattice parsing: techniques for integrated syntactic language modeling.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University,</institution>
<location>Providence, RI, USA.</location>
<contexts>
<context position="4295" citStr="Hall, 2005" startWordPosition="702" endWordPosition="703"> data does not mark any empty elements. We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of flexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modified version of the Berkeley parser which allows handling lattices as input.2 The modification is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the </context>
</contexts>
<marker>Hall, 2005</marker>
<rawString>Keith B. Hall. 2005. Best-first word-lattice parsing: techniques for integrated syntactic language modeling. Ph.D. thesis, Brown University, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1682" citStr="Marcus et al., 1993" startWordPosition="266" endWordPosition="269">ed (John was believed to admire who?). Empty elements exist in many languages and serve different purposes. In languages such as Chinese and Korean, where subjects and objects can be dropped to avoid duplication, empty elements are particularly important, as they indicate the position of dropped arguments. Figure 1 gives an example of a Chinese parse tree with empty elements. The first empty element (*pro*) marks the subject of the whole sentence, a pronoun inferable from context. The second empty element (*PRO*) marks the subject of the dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some PVP 212 IP VP P PV I Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL.</booktitle>
<contexts>
<context position="2462" citStr="Petrov et al., 2006" startWordPosition="391" endWordPosition="394"> IP VP P PV I Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperforms the method of Yang and Xue (2010).1 2 Method Our method is fairly simple. We take a state-of-theart parsing model, the Berkeley parser (Petrov et al., 2006), train it on data with explicit empty elements, and test it on word lattices that can nondeterministically insert empty elements anywhere. The idea is that the state-splitting of the parsing model will enable it to learn where to expect empty elements to be inserted into the test sentences. Tree transformations Prior to training, we alter the annotation of empty elements so that the terminal label is a consistent symbol (E), the preterminal label is the type of the empty element, and -NONEis deleted (see Figure 2b). This simplifies the lattices because there is only one empty symbol, and help</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mary Harper</author>
<author>Eugene Charniak</author>
<author>Bonnie Dorr</author>
<author>Mark Johnson</author>
<author>Jeremy G Kahn</author>
</authors>
<title>SParseval: Evaluation metrics for parsing speech. In</title>
<date>2006</date>
<booktitle>Proc. LREC.</booktitle>
<location>Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin</location>
<marker>Roark, Harper, Charniak, Dorr, Johnson, Kahn, 2006</marker>
<rawString>Brian Roark, Mary Harper, Eugene Charniak, Bonnie Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin Stewart, and Lisa Yung. 2006. SParseval: Evaluation metrics for parsing speech. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Trace prediction and recovery with unlexicalized PCFGs and slash features.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL.</booktitle>
<contexts>
<context position="2272" citStr="Schmid, 2006" startWordPosition="360" endWordPosition="361">ks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some PVP 212 IP VP P PV I Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperforms the method of Yang and Xue (2010).1 2 Method Our method is fairly simple. We take a state-of-theart parsing model, the Berkeley parser (Petrov et al., 2006), train it on data with explicit empty elements, and test it on word lattices that can nondeterministically insert empty elements anywhere. The idea is that the state-splitting of the parsing model will enable it to learn where to expect empty elements to be inserted into the test sentences. Tree transformations Prior to training, we alter the annotation of empty elements so that the terminal label is a con</context>
<context position="6531" citStr="Schmid (2006)" startWordPosition="1089" endWordPosition="1090">uch that (A ∩ B)(x) = min(A(x), B(x)). If T is the multiset of “items” in the trees being tested and G is the multiset of “items” in the gold-standard trees, then |G ∩ T ||G ∩ T| precision = recall = |T ||G| 2 F1 = 1 + 1 precision recall 2The modified parser is available at http://www.cs.bgu. ac.il/~yoavg/software/blatt/ 3We provide a scoring script which supports all of these evaluation metrics. The code is available at http://www.isi.edu/ ~chiang/software/eevalb.py . 213 SBARQ SQ N PV S Section System Labeled Labeled All Labeled Empty Brackets Empty Elements Brackets P R F1 P R F1 P R F1 00 Schmid (2006) 88.3 82.9 85.5 89.4 83.8 86.5 87.1 85.6 86.3 split 5x merge 50% 91.0 79.8 85.0 93.1 81.8 87.1 90.4 88.7 89.5 split 6x merge 50% 91.9 81.1 86.1 93.6 82.4 87.6 90.4 89.1 89.7 split 6x merge 75% 92.7 80.7 86.3 94.6 82.0 87.9 90.3 88.5 89.3 split 7x merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7 23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4 split 6x merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3 Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer. Unlabeled Labeled All Labeled Empty Elements Empty Elements Brackets Ta</context>
</contexts>
<marker>Schmid, 2006</marker>
<rawString>Helmut Schmid. 2006. Trace prediction and recovery with unlexicalized PCFGs and slash features. In Proc. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="1701" citStr="Xue et al., 2005" startWordPosition="270" endWordPosition="273"> to admire who?). Empty elements exist in many languages and serve different purposes. In languages such as Chinese and Korean, where subjects and objects can be dropped to avoid duplication, empty elements are particularly important, as they indicate the position of dropped arguments. Figure 1 gives an example of a Chinese parse tree with empty elements. The first empty element (*pro*) marks the subject of the whole sentence, a pronoun inferable from context. The second empty element (*PRO*) marks the subject of the dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some PVP 212 IP VP P PV I Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperf</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqin Yang</author>
<author>Nianwen Xue</author>
</authors>
<title>Chasing the ghost: recovering empty categories in the Chinese Treebank. In</title>
<date>2010</date>
<booktitle>Proc. COLING.</booktitle>
<contexts>
<context position="2339" citStr="Yang and Xue (2010)" startWordPosition="370" endWordPosition="373">nnotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some PVP 212 IP VP P PV I Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperforms the method of Yang and Xue (2010).1 2 Method Our method is fairly simple. We take a state-of-theart parsing model, the Berkeley parser (Petrov et al., 2006), train it on data with explicit empty elements, and test it on word lattices that can nondeterministically insert empty elements anywhere. The idea is that the state-splitting of the parsing model will enable it to learn where to expect empty elements to be inserted into the test sentences. Tree transformations Prior to training, we alter the annotation of empty elements so that the terminal label is a consistent symbol (E), the preterminal label is the type of the empty </context>
<context position="7382" citStr="Yang and Xue (2010)" startWordPosition="1254" endWordPosition="1257"> 89.3 split 7x merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7 23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4 split 6x merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3 Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer. Unlabeled Labeled All Labeled Empty Elements Empty Elements Brackets Task System P R F1 P R F1 P R F1 Dev split 5x merge 50% 82.5 58.0 68.1 72.6 51.8 60.5 84.6 80.7 82.6 split 6x merge 50% 76.4 60.5 67.5 68.2 55.1 60.9 83.2 81.3 82.2 split 7x merge 50% 74.9 58.7 65.8 65.9 52.5 58.5 82.7 81.1 81.9 Test Yang and Xue (2010) 80.3 57.9 63.2 split 6x merge 50% 74.0 61.3 67.0 66.0 54.5 58.6 82.7 80.8 81.7 Table 2: Results on Penn (Chinese) Treebank. Chinese We also experimented on a subset of the Penn Chinese Treebank 6.0. For comparability with previous work (Yang and Xue, 2010), we trained the parser on sections 0081–0900, used sections 0041–0080 for development, and sections 0001–0040 and 0901–0931 for testing. The results are shown in Table 2. We selected the 6th split-merge cycle based on the labeled empty elements F1 measure. The unlabeled empty elements column shows that our system outperforms the baseline sy</context>
<context position="8915" citStr="Yang and Xue, 2010" startWordPosition="1517" endWordPosition="1520">e presented is simple, highly effective, and fully integrated with state of the art parsing. We hope to exploit cross-lingual information about empty elements in machine translation. Chung and Gildea (2010) have shown that such information indeed helps translation, and we plan to extend this work by handling more empty categories (rather Type Total Correct Recall Gold YX Ours YX Ours *pro* 290 125 159 43.1 54.8 *PRO* 299 196 199 65.6 66.6 *T* 578 338 388 58.5 67.1 *RNR* 32 20 15 62.5 46.9 *OP* 134 20 65 14.9 48.5 * 19 5 3 26.3 15.8 Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6x. than just *pro* and *PRO*), and to incorporate them into a syntax-based translation model instead of a phrase-based model. We also plan to extend our work here to recover coindexation information (links between a moved element and the trace which marks the position it was moved from). As a step towards shallow semantic analysis, this may further benefit other natural language processing tasks such as machine translation and summary generation. Acknowledgements We would like to thank Slav Petrov for his help in running the Berkeley parser, and Yaqin Yang, Bert 215 Xue, Tagyou</context>
</contexts>
<marker>Yang, Xue, 2010</marker>
<rawString>Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost: recovering empty categories in the Chinese Treebank. In Proc. COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>