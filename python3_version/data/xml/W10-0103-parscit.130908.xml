<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009065">
<title confidence="0.9754685">
D-Confidence: an active learning strategy which efficiently identifies small
classes
</title>
<author confidence="0.912712">
Nuno Escudeiro Alipio Jorge
</author>
<affiliation confidence="0.516932">
Instituto Superior de Engenharia do Porto LIAAD-INESC PORTO L.A.
</affiliation>
<address confidence="0.8775366">
Rua Dr Ant´onio Bernardino de Almeida, 431 Rua de Ceuta, 118 - 6
Porto, P-4200-072 PORTO, Portugal Porto, P-4050-190 PORTO, Portugal
LIAAD-INESC PORTO L.A. amjorge@fc.up.pt
Rua de Ceuta, 118 - 6
Porto, P-4050-190 PORTO, Portugal
</address>
<email confidence="0.99583">
nfe@isep.ipp.pt
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918428571429">
In some classification tasks, such as those re-
lated to the automatic building and mainte-
nance of text corpora, it is expensive to ob-
tain labeled examples to train a classifier. In
such circumstances it is common to have mas-
sive corpora where a few examples are la-
beled (typically a minority) while others are
not. Semi-supervised learning techniques try
to leverage the intrinsic information in unla-
beled examples to improve classification mod-
els. However, these techniques assume that
the labeled examples cover all the classes to
learn which might not stand. In the pres-
ence of an imbalanced class distribution get-
ting labeled examples from minority classes
might be very costly if queries are randomly
selected. Active learning allows asking an or-
acle to label new examples, that are criteri-
ously selected, and does not assume a previ-
ous knowledge of all classes. D-Confidence
is an active learning approach that is effective
when in presence of imbalanced training sets.
In this paper we discuss the performance of d-
Confidence over text corpora. We show empir-
ically that d-Confidence reduces the number
of queries required to identify examples from
all classes to learn when compared to confi-
dence, a common active learning criterion.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853368421053">
Classification tasks require a number of previously
labeled cases. A major bottleneck is that case label-
ing is a laborious task requiring significant human
effort. This effort is particularly high in the case of
text documents, web pages and other unstructured
objects.
The effort required to retrieve representative la-
beled examples to learn a classification model is not
only related to the number of distinct classes (Adami
et al., 2005); it is also related to class distribution in
the available pool of examples. On a highly imbal-
anced class distribution, it is particularly demanding
to identify examples from minority classes. These,
however, may be important in terms of represen-
tativeness. Failing to identify cases from under-
represented classes may have costs. Minority classes
may correspond to specific information needs which
are relevant for specific subgroups of users. In many
situations, such as fraud detection, clinical diagno-
sis, news (Ribeiro and Escudeiro, 2008) and Web
resources (Escudeiro and Jorge, 2006), we face the
problem of imbalanced class distributions.
The aim of our current work is to get a classifica-
tion model that is able to fully recognize the target
concept, including all the classes to learn no mater
how frequent or rare they are.
Our main goal is to identify representative exam-
ples for each class in the absence of previous de-
scriptions of some or all the classes. Furthermore,
this must be achieved with a reduced number of la-
beled examples in order to reduce the labeling effort.
There are several learning schemes available for
classification. The supervised setting allows users
to specify arbitrary concepts. However, it requires a
fully labeled training set, which is prohibitive when
the labeling cost is high and, besides that, it requires
labeled cases from all classes. Semi-supervised
learning allows users to state specific needs without
</bodyText>
<page confidence="0.995219">
18
</page>
<note confidence="0.9665675">
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 18–26,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999938705882353">
requiring extensive labeling (Chapelle et al, 2006)
but still requires that labeled examples fully cover
the target concept. Unsupervised learning does not
require any labeling but users have no chance to tai-
lor clusters to their specific needs and there is no
guarantee that the induced clusters are aligned with
the classes to learn. In active learning, that seems
more adequate to our goals, the learner is allowed to
ask an oracle (typically a human) to label examples
– these requests are called queries. The most infor-
mative queries are selected by the learning algorithm
instead of being randomly selected as is the case in
supervised learning.
In this paper we evaluate the performance of d-
Confidence (Escudeiro and Jorge, 2009) on text cor-
pora. D-Confidence is an active learning approach
that tends to explore unseen regions in case space,
thus selecting cases from unseen classes faster –
with fewer queries – than traditional active learn-
ing approaches. D-Confidence selects queries based
on a criterion that aggregates the posterior classifier
confidence – a traditional active learning criterion –
and the distance between queries and known classes.
This criterion is biased towards cases that do not be-
long to known classes (low confidence) and that are
located in unseen areas in case space (high distance
to known classes). D-confidence is more effective
than confidence alone in achieving an homogeneous
coverage of target classes.
In the rest of this paper we start by reviewing ac-
tive learning, in section 2. Section 3 describes d-
Confidence. The evaluation process is presented in
section 4 and we state our conclusions and expecta-
tions for future work in section 5.
</bodyText>
<sectionHeader confidence="0.960576" genericHeader="method">
2 Active Learning
</sectionHeader>
<bodyText confidence="0.999973508474576">
Active learning approaches (Angluin, 1988; Cohn
et al., 1994; Muslea et al., 2006) reduce label com-
plexity – the number of queries that are necessary
and sufficient to learn a concept – by analyzing un-
labeled cases and selecting the most useful ones
once labeled. Queries may be artificially generated
(Baum, 1991) – the query construction paradigm
– or selected from a pool (Cohn et al., 1990) or a
stream of data – the query filtering paradigm. Our
current work is developed under the query filtering
approach.
The general idea in active learning is to estimate
the value of labeling one unlabeled case. Query-By-
Committee (Seung et al., 1992), for example, uses
a set of classifiers – the committee – to identify the
case with the highest disagreement. Schohn et al.
(2000) worked on active learning for Support Vec-
tor Machines (SVM) selecting queries – cases to be
labeled – by their proximity to the dividing hyper-
plane. Their results are, in some cases, better than if
all available data is used to train. Cohn et al. (1996)
describe an optimal solution for pool-based active
learning that selects the case that, once labeled and
added to the training set, produces the minimum ex-
pected error. This approach, however, requires high
computational effort. Previous active learning ap-
proaches (providing non-optimal solutions) aim at
reducing uncertainty by selecting the next query as
the unlabeled example on which the classifier is less
confident.
Batch mode active learning – selecting a batch of
queries instead of a single one before retraining – is
useful when computational time for training is crit-
ical. Brinker (2003) proposes a selection strategy,
tailored for SVM, that combines closeness to the di-
viding hyperplane – assuring a reduction in the ver-
sion space close to one half – with diversity among
selected cases – assuring that newly added examples
provide additional reduction of version space. Hoi et
al. (2006) suggest a new batch mode active learning
relying on the Fisher information matrix to ensure
small redundancy among selected cases. Li et al.
(2006) compute diversity within selected cases from
their conditional error.
Dasgupta (2005) defines theoretical bounds show-
ing that active learning has exponentially smaller la-
bel complexity than supervised learning under some
particular and restrictive constraints. This work is
extended in Kaariainen (2006) by relaxing some of
these constraints. An important conclusion of this
work is that the gains of active learning are much
more evident in the initial phase of the learning
process, after which these gains degrade and the
speed of learning drops to that of passive learn-
ing. Agnostic Active learning (Balcan et al., 2006),
A2, achieves an exponential improvement over the
usual sample complexity of supervised learning in
the presence of arbitrary forms of noise. This model
is studied by Hanneke (2007) setting general bounds
</bodyText>
<page confidence="0.998549">
19
</page>
<bodyText confidence="0.999914958333333">
on label complexity.
All these approaches assume that we have an ini-
tial labeled set covering all the classes of interest.
Clustering has also been explored to provide
an initial structure to data or to suggest valuable
queries. Adami et al. (2005) merge clustering and
oracle labeling to bootstrap a predefined hierarchy
of classes. Although the original clusters provide
some structure to the input, this approach still de-
mands for a high validation effort, especially when
these clusters are not aligned with class labels. Das-
gupta et al. (2008) propose a cluster-based method
that consistently improves label complexity over su-
pervised learning. Their method detects and exploits
clusters that are loosely aligned with class labels.
Among other paradigms, it is common that active
learning methods select the queries which are clos-
est to the decision boundary of the current classi-
fier. These methods focus on improving the decision
functions for the classes that are already known, i.e.,
those having labeled cases present in the training set.
The work presented in this paper diverges classifier
attention to other regions increasing the chances of
finding new labels.
</bodyText>
<sectionHeader confidence="0.946659" genericHeader="method">
3 D-Confidence Active Learning
</sectionHeader>
<bodyText confidence="0.999955894736842">
Given a target concept with an arbitrary number of
classes together with a sample of unlabeled exam-
ples from the target space (the working set), our
purpose is to identify representative cases covering
all classes while posing as few queries as possible,
where a query consists of requesting a label to a spe-
cific case. The working set is assumed to be repre-
sentative of the class space – the representativeness
assumption (Liu and Motoda, 2001).
Active learners commonly search for queries in
the neighborhood of the decision boundary, where
class uncertainty is higher. Limiting case selec-
tion to the uncertainty region seems adequate when
we have at least one labeled case from each class.
This class representativeness is assumed by all ac-
tive learning methods. In such a scenario, selecting
queries from the uncertainty region is very effective
in reducing version space.
Nevertheless, our focus is on text corpora where
only few labeled examples exist and when we are
still looking for exemplary cases to qualify the con-
cept to learn. Under these circumstances – while
we do not have labeled cases covering all classes
– the uncertainty region, as perceived by the active
learner, is just a subset of the real uncertainty region.
Being limited to this partial view of the concept, the
learner is more likely to waste queries. The amount
of the uncertainty region that the learner misses is re-
lated to the number of classes to learn that have not
yet been identified as well as to the class distribution
in the training set.
The intuition behinf d-Confidence is that query
selection should be based not only on classifier con-
fidence but also on distance to previously labeled
cases. In the presence of two cases with equally low
confidence d-Confidence selects the one that is far-
ther apart from what is already know, i.e., from pre-
viously labeled cases.
</bodyText>
<subsectionHeader confidence="0.999248">
3.1 D-Confidence
</subsectionHeader>
<bodyText confidence="0.999977655172414">
Common active learning approaches rely on classi-
fier confidence to select queries (Angluin, 1988) and
assume that the pre-labeled set covers all the labels
to learn – this assumption does not hold in our sce-
nario. These approaches use the current classifica-
tion model at each iteration to compute the posterior
confidence on each known class for each unlabeled
case. Then, they select, as the next query, the unla-
beled case with the lowest confidence.
D-Confidence, weighs the confidence of the clas-
sifier with the inverse of the distance between the
case at hand and previously known classes.
This bias is expected to favor a faster coverage
of case space, exhibiting a tendency to explore un-
known areas. As a consequence, it provides faster
convergence than confidence alone. This drift to-
wards unexplored regions and unknown classes is
achieved by selecting the case with the lowest d-
Confidence as the next query. Lowest d-Confidence
is achieved by combining low confidence – probably
indicating cases from unknown classes – with high
distance to known classes – pointing to unseen re-
gions in the case space. This effect produces signif-
icant differences in the behavior of the learning pro-
cess. Common active learners focus on the uncer-
tainty region asking queries that are expected to nar-
row it down. The issue is that the uncertainty region
is determined by the labels we known at a given it-
eration. Focusing our search for queries exclusively
</bodyText>
<page confidence="0.994278">
20
</page>
<tableCaption confidence="0.999686">
Table 1: d-Confidence algorithm.
</tableCaption>
<listItem confidence="0.979720125">
(1) given W; L1 and K
(2) compute distance among cases in W
(3) i = 1
(4) while (not stopping criteria) {
(5) Ui = W − Li
(6) learn hi from Li
(7) apply hi to Ui generating confi(uj, ck)
(8) for(ujinUi){
</listItem>
<equation confidence="0.970332">
(9) disti(uj, ck) = aggrIndivDistk(ui, ck)
(10) dconfi(uj, ck) = confi(uj,ck)
disti(uj,ck)
(11) dCi(uj) = agConfk(dconfi(uj,ck))
(12) }
(13) qi = uj : dCi(uj) = minu(dCi(u))
(14) Li+1 = LiU &lt; qi, label(qi) &gt;
(15) i + +
(16) }
</equation>
<bodyText confidence="0.999507103448276">
on this region, while we are still looking for exem-
plary cases on some labels that are not yet known, is
not effective. Unknown classes hardly come by un-
less they are represented in the current uncertainty
region.
In Table 1 we present the d-Confidence algorithm
– an active learning proposal specially tailored to
achieve a class representative coverage fast.
W is the working set, a representative sample of
cases from the problem space. Li is a subset of W.
Members of Li are the cases in W whose labels are
known at iteration i. U, a subset of W, is the set
of unlabeled examples. At iteration i, Ui is the (set)
difference between W and Li; K is the number of
target concept classes, ck; hi represents the classifier
learned at iteration i; qi is the query at iteration i;
Ci is the set of classes known at iteration i – that
is the set of distinct classes from all Li elements;
confi(uj, ck) is the posterior confidence on class ck
given case uj, at iteration i.
D-Confidence for unlabeled cases is computed at
steps (8) to (12) in Table 1 as explained below. In
(13) the case with the minimum d-Confidence is se-
lected as the next query. This query is added to the
labeled set (14), and removed from the unlabeled
pool, and the whole process iterates.
Computing d-Confidence d-Confidence is ob-
tained as the ratio between confidence and distance
among cases and known classes (Equation 1).
</bodyText>
<equation confidence="0.9927985">
conf (ck|u) 1 (1)
(dist (u, Xlabj,k)) J
</equation>
<bodyText confidence="0.999905933333333">
For a given unlabeled case, u, the classifier gen-
erates the posterior confidence w.r.t. known classes
(7). Confidence is then divided by an indicator of
the distance, dist(), between unlabeled case u and
all labeled cases belonging to class ck, Xlabj,k (9).
This distance indicator is the median of the dis-
tances between case u and all cases in Xlabj,k. The
median is expected to soften the effect of outliers.
At step (10) we compute dconfi(u,ck) – the d-
Confidence for each known class, ck, given the case
u – by dividing class confidence for a given case by
aggregated distance to that class.
Finally, d-Confidence of the case is computed,
dCi(u), as the maximum d-Confidence on individ-
ual classes, agConfk(confi(u, ck)) , at step (11).
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.994282333333333">
D-Confidence was evaluated on two text corpora.
We have selected a stratified sample from the 20
Newsgroups (NG) – with 500 documents – and an-
other one from the R52 set of the Reuters-21578
collection (R52) – with 1000 documents. The NG
dataset has documents from 20 distinct classes while
the R52 dataset has documents from 52 distinct
classes. These samples have been selected because
they have distinct class distributions.
The class distribution of NG is fairly balanced
(Figure 1) with a maximum frequency of 35 and a
minimum frequency of 20.
</bodyText>
<figureCaption confidence="0.999343">
Figure 1: Class distribution in NG dataset
</figureCaption>
<figure confidence="0.987057">
median
arg max
k
</figure>
<page confidence="0.94614">
21
</page>
<bodyText confidence="0.8814245">
On the other hand, the R52 dataset presents an
highly imbalanced class distribution (Figure 2).
</bodyText>
<figureCaption confidence="0.994403">
Figure 2: Class distribution in R52 dataset
</figureCaption>
<bodyText confidence="0.99986475">
The most frequent class in R52 has a frequency of
435 while the least frequent has only 2 examples in
the dataset. This dataset has 42 classes, out of 52,
with a fequency below 10.
</bodyText>
<subsectionHeader confidence="0.990795">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999993269230769">
We have used Support Vector Machine classifiers
(SVM) with linear kernels in all experiments.
In all the experiments we have compared the per-
formance of d-Confidence against confidence – a
common active learning setting where query selec-
tion is based on low posterior confidence of the cur-
rent classifier. This comparison is important to eval-
uate our proposal since d-Confidence is derived from
confidence by means of an aggregation with distance
in case space. Comparing both these criteria, one
against the other, will provide evidence on the per-
formance gains, or losses, of d-Confidence on text
when compared to confidence, its baseline.
We have performed 10-fold cross validation on all
datasets for standard confidence and d-Confidence
active learning. The labels in the training set are hid-
den from the classifier. In each iteration, the active
learning algorithm asks for the label of a single case.
For the initial iteration in each fold we give two la-
beled cases – from two distinct classes – to the clas-
sifier. The two initial classes are chosen for each
fold, so that different class combinations occur in
different folds. Given an initial class to be present in
L1, the specific cases to include in L1 are randomly
sampled from the set of cases on that class. Given
the fold, the same L1 is used for all experiments.
</bodyText>
<sectionHeader confidence="0.515695" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999386136363636">
Our experiments assess the ability of d-Confidence
to reduce the labeling effort when compared to con-
fidence.
We have recorded, for each dataset, the number
of distinct labels already identified and the progress
of the error on the test set for each iteration (gen-
eralization error). From these, we have computed,
for each dataset, the mean number of known classes
and mean generalization error in each iteration over
all the cross validation folds (Figures 3 and 4).
The chart legends use c for confidence, dc for d-
Confidence, a for generalization error and kc for the
number of known classes. For convenience of rep-
resentation the number of classes that are known at
each iteration has been normalized to the total num-
ber of classes in the dataset thus being transformed
into the percentage of known classes instead of the
absolute number of known classes. This way the
number of known classes and generalization error
are both bounded in the same range (between 0 and
1) and we can conveniently represented them in the
same chart.
</bodyText>
<figureCaption confidence="0.99552">
Figure 3: Known classes and error in NG dataset
</figureCaption>
<bodyText confidence="0.999858333333333">
Means are micro-averages – all the cases are
equally weighted – over all iterations for a given
dataset and a given selection criterion (confidence
or d-Confidence). Besides the overall number of
queries required to retrieve labels from all classes
and generalization error, we have also observed the
mean number of queries that are required to retrieve
the first case for each class (Tables 2 to 4) – referred
to as first hit.
</bodyText>
<page confidence="0.99866">
22
</page>
<figureCaption confidence="0.999802">
Figure 4: Known classes and error in R52 dataset
</figureCaption>
<bodyText confidence="0.8216492">
We have performed significance tests, t-tests, for
the differences of the means observed when using
confidence and d-Confidence. Statistically different
means, at a significance level of 5%, are bold faced.
When computing first hit for a given class we
have omitted the experiments where the labeled set
for the first iteration contains cases from that class.
Figures 5 and 6 give an overview of the number of
queries that are required in each setting to first hit a
given number of distinct classes.
</bodyText>
<tableCaption confidence="0.9938845">
Table 2: Class distribution (freq) and first hit (c-fh and
dc-fh) for the NG dataset.
</tableCaption>
<table confidence="0.998128818181818">
Class Freq c-fh dc-fh
1 29 36.9 35.7
2 22 41.9 41.1
3 21 57.3 76.9
4 34 23.5 5.9
5 35 18.9 20.2
6 24 37.1 15.4
7 21 53.6 11.3
8 24 32.9 13.1
9 25 36.3 9.1
10 22 41.1 48.9
11 22 42.5 3.5
12 24 28.6 4.3
13 28 18.8 20.4
14 28 25.8 5.4
15 22 27.4 6.2
16 28 14.9 2.6
17 23 21.4 27.9
18 26 34.5 7.7
19 22 22.2 21.2
20 20 26.7 6.9
mean 32.1 19.2
</table>
<figureCaption confidence="0.774153">
Figure 5: Queries required to identify bunches of distinct
classes in NG dataset
</figureCaption>
<bodyText confidence="0.3674636">
A benchmark based on random selection is also
provided – averaged over 10 random samples. We
have recorded the number of queries required to
identify bunches of distinct classes in multiples of
10 for R52 and multiples of 4 in NG.
</bodyText>
<figureCaption confidence="0.9928295">
Figure 6: Queries required to identify bunches of distinct
classes in R52 dataset
</figureCaption>
<page confidence="0.99857">
23
</page>
<tableCaption confidence="0.9625245">
Table 3: Class distribution (Freq) and first hit (c-fh and
dc-fh) for the R52 dataset. Only for those classes where
d-Confidence outperforms confidence with statistical sig-
nificance at 5% significance level.
</tableCaption>
<table confidence="0.999925642857143">
Class Freq c-fh dc-fh
1 239 10.1 1.6
2 5 7.2 1.3
8 3 103.8 76.6
9 7 68.6 6.6
10 2 80.0 10.0
11 40 83.4 41.7
14 2 173.7 110.6
15 3 115.6 64.7
16 7 96.7 16.8
18 5 68.7 62.9
22 2 244.4 197.6
23 30 153.4 36.7
25 4 173.3 102.9
26 2 214.1 123.9
27 5 206.7 184.9
28 2 213.3 85.2
29 2 137.6 44.8
30 3 159.3 52.1
31 2 159.1 144.8
32 2 179.7 123.9
33 30 160.8 76.1
34 15 175.6 108.7
36 2 167.4 107.8
37 3 118.0 99.5
40 2 140.0 104.7
43 4 313.1 256.4
44 14 216.3 144.5
46 12 206 126.7
47 2 233.7 167
48 3 153.2 84.1
49 35 226 106.9
50 3 144.3 75.5
51 3 148.5 51.1
52 2 258.8 196.5
mean 156.2 94.0
Class Freq c-fh dc-fh
3 3 11.2 18.0
4 2 36.4 72.9
5 6 23.1 50.7
6 11 39.7 49.7
7 4 40.1 89.1
12 2 128.8 136.0
13 435 91.9 107.8
17 9 117.0 135.6
19 2 123.6 19.1
20 3 171.7 171.1
21 2 196.2 224.0
24 4 118.6 178.7
35 4 146.1 183.5
38 3 158.5 166.4
39 2 152.2 150.4
41 5 143.6 154.5
42 3 188.9 202.8
45 3 175.5 198.7
mean 114.6 128.3
</table>
<tableCaption confidence="0.957574">
Table 4: Class distribution (Freq) and first hit (c-fh and
dc-fh) for the R52 dataset. Only for those classes where
d-Confidence does not outperforms confidence.
</tableCaption>
<page confidence="0.995474">
24
</page>
<figureCaption confidence="0.9934115">
Figure 7: Average gain of d-Confidence to confidence.
Classes are sorted by increasing order of their frequency.
</figureCaption>
<subsectionHeader confidence="0.949479">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999988264150944">
The charts in Figures 3 and 4 confirm the results
that have been previously reported for standard non-
textual datasets (Escudeiro and Jorge, 2009), w.r.t.
identification of cases from unknown classes, i.e.,
d-Confidence reduces the labeling effort that is re-
quired to identify examples from all classes. How-
ever, the error rate gets worse in the R52 dataset.
D-Confidence gets to know more classes from the
target concept earlier although less sharply. In the
R52 dataset we are exchanging accuracy by repre-
sentativeness. This might be desirable or not, de-
pending on the specifc task we are dealing with. If
we are trying to learn a target concept but we do not
know examples from all the classes to learn – for in-
stance if we are in the early stage of a classification
problem – this effect might be desirable so we can
get a full specification of the target concept with a
reduced labeling effort.
It is interesting to notice that d-Confidence out-
performs confidence to a greater extent on minority
classes. This is obvious in R52 if we compute the
cumulative average of the gain in labeling effort that
is provided by d-Confidence when compared to con-
fidence (Figure 7).
The gain for each class is defined as the number of
queries required by d-Confidence to first hit the class
minus the ones that are required by confidence. To
compute the moving average, these gains are sorted
in increasing order of the class frequency. The aver-
age gain starts at -128, for a class with frequency 2,
and decreases to the overall average of -36 as class
frequency increases up to 435. The bigger gains are
observed in the minority classes. Although not as
obvious as in R52 this same behaviour is also ob-
served in the NG dataset.
Figures 5 and 6, as well as Tables 2 to 4, show that
d-Confidence reduces the labeling effort required to
identify unknown classes when compared to confi-
dence. When selecting cases to label randomly, the
first bunch of 10 distinct classes is found as fast as
with d-Confidence but, from there on, when rare
classes come by, d-Confidence takes the lead. The
outcome is quite different in the NG dataset. In this
dataset d-Confidence still outperforms confidence
but it is beaten by random selection of cases after
identifying 13.3 classes on average (after 22 queries
on average). This observation led us to suspect that
when in presence of balanced datasets, d-Confidence
identifies new classes faster than random selection in
the initial phase of the learning process but selecting
cases by chance is better to identify cases in the lat-
est stage of collecting exemplary cases, when few
classes remain undetected.
</bodyText>
<sectionHeader confidence="0.998045" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99997925">
The evaluation procedure that we have performed
provided statistical evidence on the performance of
d-Confidence over text corpora when compared to
confidence. Although the evaluation has been per-
formed only on two datasets, the conclusions we
have reached point out some interesting results.
D-Confidence reduces the labeling effort and
identifies exemplary cases for all classes faster that
confidence. This gain is bigger for minority classes,
which are the ones where the benefits are more rele-
vant.
D-Confidence performs better in imbalanced
datasets where it provides significant gains that
greatly reduce the labeling effort. For balanced
datasets, d-Confidence seems to be valuable in the
early stage of the classification task, when few
classes are known. In the later stages, random se-
lection of cases seems faster in identifying the few
missing classes. However, d-Confidence consis-
tently outperforms confidence.
The main drawback of d-Confidence when ap-
plied on imbalanced text corpora is that the reduc-
tion in the labeling effort that is achieved in iden-
tifying unknown classes is obtained at the cost of
</bodyText>
<page confidence="0.99284">
25
</page>
<bodyText confidence="0.9996256">
increasing error. This increase in error is probably
due to the fact that we are diverting the classifier
from focusing on the decision function of the major-
ity classes to focus on finding new, minority, classes.
As a consequence the classification model gener-
ated by d-Confidence is able of identifying more dis-
tinct classes faster but gets less sharp in each one of
them. This is particularly harmful for accuracy since
a more fuzzy decision boundary for majority classes
might cause many erroneous guesses with a negative
impact on error.
We are now exploring semi-supervised learning
to leverage the intrinsic value of unlabeled cases so
we can benefit from the reduction in labeling effort
provided by d-Confidence without increasing error.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.999640368421053">
G. Adami, P. Avesani, and D. Sona. Clustering doc-
uments into a web directory for bootstrapping a su-
pervised classification. Data and Knowledge Engi-
neering, 54:301325, 2005.
D. Angluin. Queries and concept learning. Ma-
chine Learning, 2:319342, 1988.
M.-F. Balcan, A. Beygelzimer, and J. Langford.
Agnostic active learning. In In ICML, pages 6572.
ICML, 2006.
E. Baum. Neural net algorithms that learn in
polynomial time from examples and queries. IEEE
Transactions in Neural Networks, 2:519, 1991.
K. Brinker. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on
Machine Learning, 2003.
O. Chapelle, B. Schoelkopf and A. Zien (Eds).
Semi-supervised Learning. MIT Press, Cambridge,
MA, 2006.
D. Cohn, L. Atlas, and R. Ladner. Training con-
nectionist networks with queries and selective sam-
pling. In Advances in Neural Information Process-
ing Systems, 1990.
D. Cohn, L. Atlas, and R. Ladner. Improving gen-
eralization with active learning. Machine Learning,
(15):201221, 1994.
D. Cohn, Z. Ghahramani, and M. Jordan. Active
learning with statistical models. Journal of Artificial
Intelligence Research, 4:129145, 1996.
S. Dasgupta. Coarse sample complexity bonds for
active learning. In Advances in Neural Information
Processing Systems 18. 2005.
S. Dasgupta and D. Hsu. Hierarchical sampling
for active learning. In Proceedings of the 25th Inter-
national Conference on Machine Learning, 2008.
N. Escudeiro and A.M. Jorge. Efficient coverage
of case space with active learning. In P. M. L. M.
R. Lus Seabra Lopes, Nuno Lau, editor, Progress in
Artificial Intelligence, Proceedings of the 14th Por-
tuguese Conference on Artificial Intelligence (EPIA
2009), volume 5816, pages 411422. Springer, 2009.
S. Hanneke. A bound on the label complexity
of agnostic active learning. In Proceedings of the
24th International Conference on Machine Learn-
ing, 2007.
S. Hoi, R. Jin, and M. Lyu. Large-scale text cat-
egorization by batch mode active learning. In Pro-
ceedings of the World Wide Web Conference, 2006.
M. Kaariainen. Algorithmic Learning Theory,
chapter Active learning in the non-realizable case,
pages 63 77. Springer Berlin / Heidelberg, 2006.
M. Li and I. Sethi. Confidence-based active learn-
ing. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 28:12511261, 2006.
H. Liu and H. Motoda. Instance Selection and
Construction for Data Mining. Kluver Academic
Publishers, 2001.
I. Muslea, S. Minton, and C. A. Knoblock. Active
learning with multiple views. Journal of Artificial
Intelligence Research, 27:203233, 2006.
P. Ribeiro and N. Escudeiro. On-line news ‘a la
carte. In Proceedings of the European Conference on
the Use of Modern Information and Communication
Technologies, 2008.
N. Roy and A. McCallum. Toward optimal active
learning through sampling estimation of error reduc-
tion. In Proceedings of the International Conference
on Machine Learning, 2001.
G. Schohn and D. Cohn. Less is more: Active
learning with support vector machines. In Proceed-
ings of the International Conference on Machine
Learning, 2000.
H. Seung, M. Opper, and H. Sompolinsky. Query
by committee. In Proceedings of the 5th An-
nual Workshop on Computational Learning Theory,
1992.
</reference>
<page confidence="0.998033">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229829">
<title confidence="0.9660005">D-Confidence: an active learning strategy which efficiently identifies small classes</title>
<author confidence="0.997383">Nuno Alipio</author>
<affiliation confidence="0.8117005">Instituto Superior de Engenharia do LIAAD-INESC PORTO Rua Dr Ant´onio Bernardino de Almeida, Rua de Ceuta, 118 -</affiliation>
<address confidence="0.751161">Porto, P-4200-072 PORTO, Porto, P-4050-190 PORTO,</address>
<affiliation confidence="0.673227">LIAAD-INESC PORTO amjorge@fc.up.pt Rua de Ceuta, 118 -</affiliation>
<address confidence="0.858626">Porto, P-4050-190 PORTO,</address>
<email confidence="0.980771">nfe@isep.ipp.pt</email>
<abstract confidence="0.998847620689655">In some classification tasks, such as those related to the automatic building and maintenance of text corpora, it is expensive to obtain labeled examples to train a classifier. In such circumstances it is common to have massive corpora where a few examples are labeled (typically a minority) while others are not. Semi-supervised learning techniques try to leverage the intrinsic information in unlabeled examples to improve classification models. However, these techniques assume that the labeled examples cover all the classes to learn which might not stand. In the presence of an imbalanced class distribution getting labeled examples from minority classes might be very costly if queries are randomly selected. Active learning allows asking an oracle to label new examples, that are criteriously selected, and does not assume a previous knowledge of all classes. D-Confidence is an active learning approach that is effective when in presence of imbalanced training sets. In this paper we discuss the performance of d- Confidence over text corpora. We show empirically that d-Confidence reduces the number of queries required to identify examples from all classes to learn when compared to confidence, a common active learning criterion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Adami</author>
<author>P Avesani</author>
<author>D Sona</author>
</authors>
<title>Clustering documents into a web directory for bootstrapping a supervised classification.</title>
<date>2005</date>
<journal>Data and Knowledge Engineering,</journal>
<volume>54</volume>
<contexts>
<context position="2126" citStr="Adami et al., 2005" startWordPosition="331" endWordPosition="334">d-Confidence reduces the number of queries required to identify examples from all classes to learn when compared to confidence, a common active learning criterion. 1 Introduction Classification tasks require a number of previously labeled cases. A major bottleneck is that case labeling is a laborious task requiring significant human effort. This effort is particularly high in the case of text documents, web pages and other unstructured objects. The effort required to retrieve representative labeled examples to learn a classification model is not only related to the number of distinct classes (Adami et al., 2005); it is also related to class distribution in the available pool of examples. On a highly imbalanced class distribution, it is particularly demanding to identify examples from minority classes. These, however, may be important in terms of representativeness. Failing to identify cases from underrepresented classes may have costs. Minority classes may correspond to specific information needs which are relevant for specific subgroups of users. In many situations, such as fraud detection, clinical diagnosis, news (Ribeiro and Escudeiro, 2008) and Web resources (Escudeiro and Jorge, 2006), we face </context>
<context position="8644" citStr="Adami et al. (2005)" startWordPosition="1381" endWordPosition="1384"> the learning process, after which these gains degrade and the speed of learning drops to that of passive learning. Agnostic Active learning (Balcan et al., 2006), A2, achieves an exponential improvement over the usual sample complexity of supervised learning in the presence of arbitrary forms of noise. This model is studied by Hanneke (2007) setting general bounds 19 on label complexity. All these approaches assume that we have an initial labeled set covering all the classes of interest. Clustering has also been explored to provide an initial structure to data or to suggest valuable queries. Adami et al. (2005) merge clustering and oracle labeling to bootstrap a predefined hierarchy of classes. Although the original clusters provide some structure to the input, this approach still demands for a high validation effort, especially when these clusters are not aligned with class labels. Dasgupta et al. (2008) propose a cluster-based method that consistently improves label complexity over supervised learning. Their method detects and exploits clusters that are loosely aligned with class labels. Among other paradigms, it is common that active learning methods select the queries which are closest to the de</context>
</contexts>
<marker>Adami, Avesani, Sona, 2005</marker>
<rawString>G. Adami, P. Avesani, and D. Sona. Clustering documents into a web directory for bootstrapping a supervised classification. Data and Knowledge Engineering, 54:301325, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Queries and concept learning.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--319342</pages>
<contexts>
<context position="5524" citStr="Angluin, 1988" startWordPosition="874" endWordPosition="875">een queries and known classes. This criterion is biased towards cases that do not belong to known classes (low confidence) and that are located in unseen areas in case space (high distance to known classes). D-confidence is more effective than confidence alone in achieving an homogeneous coverage of target classes. In the rest of this paper we start by reviewing active learning, in section 2. Section 3 describes dConfidence. The evaluation process is presented in section 4 and we state our conclusions and expectations for future work in section 5. 2 Active Learning Active learning approaches (Angluin, 1988; Cohn et al., 1994; Muslea et al., 2006) reduce label complexity – the number of queries that are necessary and sufficient to learn a concept – by analyzing unlabeled cases and selecting the most useful ones once labeled. Queries may be artificially generated (Baum, 1991) – the query construction paradigm – or selected from a pool (Cohn et al., 1990) or a stream of data – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 19</context>
<context position="11576" citStr="Angluin, 1988" startWordPosition="1860" endWordPosition="1861">y region that the learner misses is related to the number of classes to learn that have not yet been identified as well as to the class distribution in the training set. The intuition behinf d-Confidence is that query selection should be based not only on classifier confidence but also on distance to previously labeled cases. In the presence of two cases with equally low confidence d-Confidence selects the one that is farther apart from what is already know, i.e., from previously labeled cases. 3.1 D-Confidence Common active learning approaches rely on classifier confidence to select queries (Angluin, 1988) and assume that the pre-labeled set covers all the labels to learn – this assumption does not hold in our scenario. These approaches use the current classification model at each iteration to compute the posterior confidence on each known class for each unlabeled case. Then, they select, as the next query, the unlabeled case with the lowest confidence. D-Confidence, weighs the confidence of the classifier with the inverse of the distance between the case at hand and previously known classes. This bias is expected to favor a faster coverage of case space, exhibiting a tendency to explore unknow</context>
</contexts>
<marker>Angluin, 1988</marker>
<rawString>D. Angluin. Queries and concept learning. Machine Learning, 2:319342, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-F Balcan</author>
<author>A Beygelzimer</author>
<author>J Langford</author>
</authors>
<title>Agnostic active learning.</title>
<date>2006</date>
<booktitle>In In ICML,</booktitle>
<pages>6572</pages>
<publisher>ICML,</publisher>
<contexts>
<context position="8187" citStr="Balcan et al., 2006" startWordPosition="1307" endWordPosition="1310">ute diversity within selected cases from their conditional error. Dasgupta (2005) defines theoretical bounds showing that active learning has exponentially smaller label complexity than supervised learning under some particular and restrictive constraints. This work is extended in Kaariainen (2006) by relaxing some of these constraints. An important conclusion of this work is that the gains of active learning are much more evident in the initial phase of the learning process, after which these gains degrade and the speed of learning drops to that of passive learning. Agnostic Active learning (Balcan et al., 2006), A2, achieves an exponential improvement over the usual sample complexity of supervised learning in the presence of arbitrary forms of noise. This model is studied by Hanneke (2007) setting general bounds 19 on label complexity. All these approaches assume that we have an initial labeled set covering all the classes of interest. Clustering has also been explored to provide an initial structure to data or to suggest valuable queries. Adami et al. (2005) merge clustering and oracle labeling to bootstrap a predefined hierarchy of classes. Although the original clusters provide some structure to </context>
</contexts>
<marker>Balcan, Beygelzimer, Langford, 2006</marker>
<rawString>M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In In ICML, pages 6572. ICML, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Baum</author>
</authors>
<title>Neural net algorithms that learn in polynomial time from examples and queries.</title>
<date>1991</date>
<journal>IEEE Transactions in Neural Networks,</journal>
<volume>2</volume>
<contexts>
<context position="5797" citStr="Baum, 1991" startWordPosition="921" endWordPosition="922">n homogeneous coverage of target classes. In the rest of this paper we start by reviewing active learning, in section 2. Section 3 describes dConfidence. The evaluation process is presented in section 4 and we state our conclusions and expectations for future work in section 5. 2 Active Learning Active learning approaches (Angluin, 1988; Cohn et al., 1994; Muslea et al., 2006) reduce label complexity – the number of queries that are necessary and sufficient to learn a concept – by analyzing unlabeled cases and selecting the most useful ones once labeled. Queries may be artificially generated (Baum, 1991) – the query construction paradigm – or selected from a pool (Cohn et al., 1990) or a stream of data – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 1992), for example, uses a set of classifiers – the committee – to identify the case with the highest disagreement. Schohn et al. (2000) worked on active learning for Support Vector Machines (SVM) selecting queries – cases to be labeled – by their proximity to the dividing h</context>
</contexts>
<marker>Baum, 1991</marker>
<rawString>E. Baum. Neural net algorithms that learn in polynomial time from examples and queries. IEEE Transactions in Neural Networks, 2:519, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Brinker</author>
</authors>
<title>Incorporating diversity in active learning with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning,</booktitle>
<contexts>
<context position="7112" citStr="Brinker (2003)" startWordPosition="1139" endWordPosition="1140"> et al. (1996) describe an optimal solution for pool-based active learning that selects the case that, once labeled and added to the training set, produces the minimum expected error. This approach, however, requires high computational effort. Previous active learning approaches (providing non-optimal solutions) aim at reducing uncertainty by selecting the next query as the unlabeled example on which the classifier is less confident. Batch mode active learning – selecting a batch of queries instead of a single one before retraining – is useful when computational time for training is critical. Brinker (2003) proposes a selection strategy, tailored for SVM, that combines closeness to the dividing hyperplane – assuring a reduction in the version space close to one half – with diversity among selected cases – assuring that newly added examples provide additional reduction of version space. Hoi et al. (2006) suggest a new batch mode active learning relying on the Fisher information matrix to ensure small redundancy among selected cases. Li et al. (2006) compute diversity within selected cases from their conditional error. Dasgupta (2005) defines theoretical bounds showing that active learning has exp</context>
</contexts>
<marker>Brinker, 2003</marker>
<rawString>K. Brinker. Incorporating diversity in active learning with support vector machines. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Schoelkopf</author>
<author>A Zien</author>
</authors>
<title>Semi-supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="3829" citStr="Chapelle et al, 2006" startWordPosition="596" endWordPosition="599">fort. There are several learning schemes available for classification. The supervised setting allows users to specify arbitrary concepts. However, it requires a fully labeled training set, which is prohibitive when the labeling cost is high and, besides that, it requires labeled cases from all classes. Semi-supervised learning allows users to state specific needs without 18 Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 18–26, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics requiring extensive labeling (Chapelle et al, 2006) but still requires that labeled examples fully cover the target concept. Unsupervised learning does not require any labeling but users have no chance to tailor clusters to their specific needs and there is no guarantee that the induced clusters are aligned with the classes to learn. In active learning, that seems more adequate to our goals, the learner is allowed to ask an oracle (typically a human) to label examples – these requests are called queries. The most informative queries are selected by the learning algorithm instead of being randomly selected as is the case in supervised learning.</context>
</contexts>
<marker>Chapelle, Schoelkopf, Zien, 2006</marker>
<rawString>O. Chapelle, B. Schoelkopf and A. Zien (Eds). Semi-supervised Learning. MIT Press, Cambridge, MA, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>L Atlas</author>
<author>R Ladner</author>
</authors>
<title>Training connectionist networks with queries and selective sampling.</title>
<date>1990</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<contexts>
<context position="5877" citStr="Cohn et al., 1990" startWordPosition="934" endWordPosition="937">art by reviewing active learning, in section 2. Section 3 describes dConfidence. The evaluation process is presented in section 4 and we state our conclusions and expectations for future work in section 5. 2 Active Learning Active learning approaches (Angluin, 1988; Cohn et al., 1994; Muslea et al., 2006) reduce label complexity – the number of queries that are necessary and sufficient to learn a concept – by analyzing unlabeled cases and selecting the most useful ones once labeled. Queries may be artificially generated (Baum, 1991) – the query construction paradigm – or selected from a pool (Cohn et al., 1990) or a stream of data – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 1992), for example, uses a set of classifiers – the committee – to identify the case with the highest disagreement. Schohn et al. (2000) worked on active learning for Support Vector Machines (SVM) selecting queries – cases to be labeled – by their proximity to the dividing hyperplane. Their results are, in some cases, better than if all available data i</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1990</marker>
<rawString>D. Cohn, L. Atlas, and R. Ladner. Training connectionist networks with queries and selective sampling. In Advances in Neural Information Processing Systems, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>L Atlas</author>
<author>R Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<marker>Cohn, Atlas, Ladner, 2012</marker>
<rawString>D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, (15):201221, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>Z Ghahramani</author>
<author>M Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>4</volume>
<contexts>
<context position="6512" citStr="Cohn et al. (1996)" startWordPosition="1045" endWordPosition="1048">ata – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 1992), for example, uses a set of classifiers – the committee – to identify the case with the highest disagreement. Schohn et al. (2000) worked on active learning for Support Vector Machines (SVM) selecting queries – cases to be labeled – by their proximity to the dividing hyperplane. Their results are, in some cases, better than if all available data is used to train. Cohn et al. (1996) describe an optimal solution for pool-based active learning that selects the case that, once labeled and added to the training set, produces the minimum expected error. This approach, however, requires high computational effort. Previous active learning approaches (providing non-optimal solutions) aim at reducing uncertainty by selecting the next query as the unlabeled example on which the classifier is less confident. Batch mode active learning – selecting a batch of queries instead of a single one before retraining – is useful when computational time for training is critical. Brinker (2003)</context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129145, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
</authors>
<title>Coarse sample complexity bonds for active learning.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 18.</booktitle>
<contexts>
<context position="7648" citStr="Dasgupta (2005)" startWordPosition="1224" endWordPosition="1225">ng – is useful when computational time for training is critical. Brinker (2003) proposes a selection strategy, tailored for SVM, that combines closeness to the dividing hyperplane – assuring a reduction in the version space close to one half – with diversity among selected cases – assuring that newly added examples provide additional reduction of version space. Hoi et al. (2006) suggest a new batch mode active learning relying on the Fisher information matrix to ensure small redundancy among selected cases. Li et al. (2006) compute diversity within selected cases from their conditional error. Dasgupta (2005) defines theoretical bounds showing that active learning has exponentially smaller label complexity than supervised learning under some particular and restrictive constraints. This work is extended in Kaariainen (2006) by relaxing some of these constraints. An important conclusion of this work is that the gains of active learning are much more evident in the initial phase of the learning process, after which these gains degrade and the speed of learning drops to that of passive learning. Agnostic Active learning (Balcan et al., 2006), A2, achieves an exponential improvement over the usual samp</context>
</contexts>
<marker>Dasgupta, 2005</marker>
<rawString>S. Dasgupta. Coarse sample complexity bonds for active learning. In Advances in Neural Information Processing Systems 18. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>D Hsu</author>
</authors>
<title>Hierarchical sampling for active learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<marker>Dasgupta, Hsu, 2008</marker>
<rawString>S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In Proceedings of the 25th International Conference on Machine Learning, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Escudeiro</author>
<author>A M Jorge</author>
</authors>
<title>Efficient coverage of case space with active learning.</title>
<date>2009</date>
<booktitle>Progress in Artificial Intelligence, Proceedings of the 14th Portuguese Conference on Artificial Intelligence (EPIA 2009),</booktitle>
<volume>5816</volume>
<pages>411422</pages>
<editor>In P. M. L. M. R. Lus Seabra Lopes, Nuno Lau, editor,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="4514" citStr="Escudeiro and Jorge, 2009" startWordPosition="710" endWordPosition="713">arget concept. Unsupervised learning does not require any labeling but users have no chance to tailor clusters to their specific needs and there is no guarantee that the induced clusters are aligned with the classes to learn. In active learning, that seems more adequate to our goals, the learner is allowed to ask an oracle (typically a human) to label examples – these requests are called queries. The most informative queries are selected by the learning algorithm instead of being randomly selected as is the case in supervised learning. In this paper we evaluate the performance of dConfidence (Escudeiro and Jorge, 2009) on text corpora. D-Confidence is an active learning approach that tends to explore unseen regions in case space, thus selecting cases from unseen classes faster – with fewer queries – than traditional active learning approaches. D-Confidence selects queries based on a criterion that aggregates the posterior classifier confidence – a traditional active learning criterion – and the distance between queries and known classes. This criterion is biased towards cases that do not belong to known classes (low confidence) and that are located in unseen areas in case space (high distance to known class</context>
<context position="22328" citStr="Escudeiro and Jorge, 2009" startWordPosition="3794" endWordPosition="3797">6 19 2 123.6 19.1 20 3 171.7 171.1 21 2 196.2 224.0 24 4 118.6 178.7 35 4 146.1 183.5 38 3 158.5 166.4 39 2 152.2 150.4 41 5 143.6 154.5 42 3 188.9 202.8 45 3 175.5 198.7 mean 114.6 128.3 Table 4: Class distribution (Freq) and first hit (c-fh and dc-fh) for the R52 dataset. Only for those classes where d-Confidence does not outperforms confidence. 24 Figure 7: Average gain of d-Confidence to confidence. Classes are sorted by increasing order of their frequency. 4.3 Discussion The charts in Figures 3 and 4 confirm the results that have been previously reported for standard nontextual datasets (Escudeiro and Jorge, 2009), w.r.t. identification of cases from unknown classes, i.e., d-Confidence reduces the labeling effort that is required to identify examples from all classes. However, the error rate gets worse in the R52 dataset. D-Confidence gets to know more classes from the target concept earlier although less sharply. In the R52 dataset we are exchanging accuracy by representativeness. This might be desirable or not, depending on the specifc task we are dealing with. If we are trying to learn a target concept but we do not know examples from all the classes to learn – for instance if we are in the early st</context>
</contexts>
<marker>Escudeiro, Jorge, 2009</marker>
<rawString>N. Escudeiro and A.M. Jorge. Efficient coverage of case space with active learning. In P. M. L. M. R. Lus Seabra Lopes, Nuno Lau, editor, Progress in Artificial Intelligence, Proceedings of the 14th Portuguese Conference on Artificial Intelligence (EPIA 2009), volume 5816, pages 411422. Springer, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hanneke</author>
</authors>
<title>A bound on the label complexity of agnostic active learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<contexts>
<context position="8369" citStr="Hanneke (2007)" startWordPosition="1337" endWordPosition="1338">pervised learning under some particular and restrictive constraints. This work is extended in Kaariainen (2006) by relaxing some of these constraints. An important conclusion of this work is that the gains of active learning are much more evident in the initial phase of the learning process, after which these gains degrade and the speed of learning drops to that of passive learning. Agnostic Active learning (Balcan et al., 2006), A2, achieves an exponential improvement over the usual sample complexity of supervised learning in the presence of arbitrary forms of noise. This model is studied by Hanneke (2007) setting general bounds 19 on label complexity. All these approaches assume that we have an initial labeled set covering all the classes of interest. Clustering has also been explored to provide an initial structure to data or to suggest valuable queries. Adami et al. (2005) merge clustering and oracle labeling to bootstrap a predefined hierarchy of classes. Although the original clusters provide some structure to the input, this approach still demands for a high validation effort, especially when these clusters are not aligned with class labels. Dasgupta et al. (2008) propose a cluster-based </context>
</contexts>
<marker>Hanneke, 2007</marker>
<rawString>S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th International Conference on Machine Learning, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hoi</author>
<author>R Jin</author>
<author>M Lyu</author>
</authors>
<title>Large-scale text categorization by batch mode active learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the World Wide Web Conference,</booktitle>
<pages>63--77</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg,</location>
<contexts>
<context position="7414" citStr="Hoi et al. (2006)" startWordPosition="1187" endWordPosition="1190">mal solutions) aim at reducing uncertainty by selecting the next query as the unlabeled example on which the classifier is less confident. Batch mode active learning – selecting a batch of queries instead of a single one before retraining – is useful when computational time for training is critical. Brinker (2003) proposes a selection strategy, tailored for SVM, that combines closeness to the dividing hyperplane – assuring a reduction in the version space close to one half – with diversity among selected cases – assuring that newly added examples provide additional reduction of version space. Hoi et al. (2006) suggest a new batch mode active learning relying on the Fisher information matrix to ensure small redundancy among selected cases. Li et al. (2006) compute diversity within selected cases from their conditional error. Dasgupta (2005) defines theoretical bounds showing that active learning has exponentially smaller label complexity than supervised learning under some particular and restrictive constraints. This work is extended in Kaariainen (2006) by relaxing some of these constraints. An important conclusion of this work is that the gains of active learning are much more evident in the initi</context>
</contexts>
<marker>Hoi, Jin, Lyu, 2006</marker>
<rawString>S. Hoi, R. Jin, and M. Lyu. Large-scale text categorization by batch mode active learning. In Proceedings of the World Wide Web Conference, 2006. M. Kaariainen. Algorithmic Learning Theory, chapter Active learning in the non-realizable case, pages 63 77. Springer Berlin / Heidelberg, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>I Sethi</author>
</authors>
<title>Confidence-based active learning.</title>
<date>2006</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>28</volume>
<marker>Li, Sethi, 2006</marker>
<rawString>M. Li and I. Sethi. Confidence-based active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28:12511261, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>H Motoda</author>
</authors>
<title>Instance Selection and Construction for Data Mining.</title>
<date>2001</date>
<publisher>Kluver Academic Publishers,</publisher>
<contexts>
<context position="10047" citStr="Liu and Motoda, 2001" startWordPosition="1604" endWordPosition="1607"> the training set. The work presented in this paper diverges classifier attention to other regions increasing the chances of finding new labels. 3 D-Confidence Active Learning Given a target concept with an arbitrary number of classes together with a sample of unlabeled examples from the target space (the working set), our purpose is to identify representative cases covering all classes while posing as few queries as possible, where a query consists of requesting a label to a specific case. The working set is assumed to be representative of the class space – the representativeness assumption (Liu and Motoda, 2001). Active learners commonly search for queries in the neighborhood of the decision boundary, where class uncertainty is higher. Limiting case selection to the uncertainty region seems adequate when we have at least one labeled case from each class. This class representativeness is assumed by all active learning methods. In such a scenario, selecting queries from the uncertainty region is very effective in reducing version space. Nevertheless, our focus is on text corpora where only few labeled examples exist and when we are still looking for exemplary cases to qualify the concept to learn. Unde</context>
</contexts>
<marker>Liu, Motoda, 2001</marker>
<rawString>H. Liu and H. Motoda. Instance Selection and Construction for Data Mining. Kluver Academic Publishers, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Muslea</author>
<author>S Minton</author>
<author>C A Knoblock</author>
</authors>
<title>Active learning with multiple views.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>27</volume>
<contexts>
<context position="5565" citStr="Muslea et al., 2006" startWordPosition="880" endWordPosition="883">s criterion is biased towards cases that do not belong to known classes (low confidence) and that are located in unseen areas in case space (high distance to known classes). D-confidence is more effective than confidence alone in achieving an homogeneous coverage of target classes. In the rest of this paper we start by reviewing active learning, in section 2. Section 3 describes dConfidence. The evaluation process is presented in section 4 and we state our conclusions and expectations for future work in section 5. 2 Active Learning Active learning approaches (Angluin, 1988; Cohn et al., 1994; Muslea et al., 2006) reduce label complexity – the number of queries that are necessary and sufficient to learn a concept – by analyzing unlabeled cases and selecting the most useful ones once labeled. Queries may be artificially generated (Baum, 1991) – the query construction paradigm – or selected from a pool (Cohn et al., 1990) or a stream of data – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 1992), for example, uses a set of classifie</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2006</marker>
<rawString>I. Muslea, S. Minton, and C. A. Knoblock. Active learning with multiple views. Journal of Artificial Intelligence Research, 27:203233, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ribeiro</author>
<author>N Escudeiro</author>
</authors>
<title>On-line news ‘a la carte.</title>
<date>2008</date>
<booktitle>In Proceedings of the European Conference on the Use of Modern Information and Communication Technologies,</booktitle>
<contexts>
<context position="2670" citStr="Ribeiro and Escudeiro, 2008" startWordPosition="413" endWordPosition="416">ion model is not only related to the number of distinct classes (Adami et al., 2005); it is also related to class distribution in the available pool of examples. On a highly imbalanced class distribution, it is particularly demanding to identify examples from minority classes. These, however, may be important in terms of representativeness. Failing to identify cases from underrepresented classes may have costs. Minority classes may correspond to specific information needs which are relevant for specific subgroups of users. In many situations, such as fraud detection, clinical diagnosis, news (Ribeiro and Escudeiro, 2008) and Web resources (Escudeiro and Jorge, 2006), we face the problem of imbalanced class distributions. The aim of our current work is to get a classification model that is able to fully recognize the target concept, including all the classes to learn no mater how frequent or rare they are. Our main goal is to identify representative examples for each class in the absence of previous descriptions of some or all the classes. Furthermore, this must be achieved with a reduced number of labeled examples in order to reduce the labeling effort. There are several learning schemes available for classif</context>
</contexts>
<marker>Ribeiro, Escudeiro, 2008</marker>
<rawString>P. Ribeiro and N. Escudeiro. On-line news ‘a la carte. In Proceedings of the European Conference on the Use of Modern Information and Communication Technologies, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>A McCallum</author>
</authors>
<title>Toward optimal active learning through sampling estimation of error reduction.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<marker>Roy, McCallum, 2001</marker>
<rawString>N. Roy and A. McCallum. Toward optimal active learning through sampling estimation of error reduction. In Proceedings of the International Conference on Machine Learning, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Schohn</author>
<author>D Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<marker>Schohn, Cohn, 2000</marker>
<rawString>G. Schohn and D. Cohn. Less is more: Active learning with support vector machines. In Proceedings of the International Conference on Machine Learning, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the 5th Annual Workshop on Computational Learning Theory,</booktitle>
<contexts>
<context position="6127" citStr="Seung et al., 1992" startWordPosition="976" endWordPosition="979">s (Angluin, 1988; Cohn et al., 1994; Muslea et al., 2006) reduce label complexity – the number of queries that are necessary and sufficient to learn a concept – by analyzing unlabeled cases and selecting the most useful ones once labeled. Queries may be artificially generated (Baum, 1991) – the query construction paradigm – or selected from a pool (Cohn et al., 1990) or a stream of data – the query filtering paradigm. Our current work is developed under the query filtering approach. The general idea in active learning is to estimate the value of labeling one unlabeled case. Query-ByCommittee (Seung et al., 1992), for example, uses a set of classifiers – the committee – to identify the case with the highest disagreement. Schohn et al. (2000) worked on active learning for Support Vector Machines (SVM) selecting queries – cases to be labeled – by their proximity to the dividing hyperplane. Their results are, in some cases, better than if all available data is used to train. Cohn et al. (1996) describe an optimal solution for pool-based active learning that selects the case that, once labeled and added to the training set, produces the minimum expected error. This approach, however, requires high computa</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of the 5th Annual Workshop on Computational Learning Theory, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>