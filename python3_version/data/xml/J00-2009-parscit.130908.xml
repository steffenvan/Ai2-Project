<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001473">
<note confidence="0.787917">
Computational Linguistics Volume 26, Number 2
</note>
<title confidence="0.9654515">
Lexical Semantics and Knowledge Representation in Multilingual
Text Generation
</title>
<author confidence="0.637019">
Manfred Stede
</author>
<affiliation confidence="0.488153">
(Technische Universitat Berlin)
Boston: Kluwer Academic Publishers
(The Kluwer international series in
</affiliation>
<bodyText confidence="0.94505425">
engineering and computer science,
volume 492), 1999, xv+219 pp;
hardbound, ISBN 0-7923-8419-9,
$129.00, £84.00, Dfl 265.00
</bodyText>
<note confidence="0.151521">
Reviewed by
</note>
<author confidence="0.586463">
Barbara Di Eugenio
</author>
<affiliation confidence="0.696933">
University of Illinois at Chicago
</affiliation>
<bodyText confidence="0.999687333333333">
Suppose you were to describe Tom draining oil from the engine of his car in 20 seconds.
As a competent English speaker, depending on context, you could choose among at
least the following subtly different descriptions, called paraphrases by Stede:
</bodyText>
<listItem confidence="0.9991765">
1. For 20 seconds, the oil drained from the engine.
2. The engine drained in 20 seconds.
3. Within 20 seconds, Tom drained the engine.
4. Tom drained the engine of the oil in 20 seconds.
</listItem>
<bodyText confidence="0.99932235">
In contrast, a state-of-the-art natural language generation (NLG) system would likely
be able to produce only one of them, modulo variations introduced by, for exam-
ple, passivization, topicalization, or pronominalization. The problem is not simply to
choose among paraphrases, but to be able to produce them to start with.
In this book, an extended version of his dissertation, Stede provides the theoretical
and practical means of endowing a text generator with such capability. He concen-
trates on tactical planning, namely, on choosing an appropriate verbalization for some
content that another component of the NLG system (the text planner) has assembled
in response to communicative goals. Stede&apos;s approach promotes lexicalization as the
central step in this process and the lexicon itself as the link between the language-
independent domain model and the language-specific resources necessary to generate
the surface sentence. Stede&apos;s ultimate goal is ambitious: he seeks to provide an archi-
tecture that can be used as is to perform generation of the same content in different
languages—even lexical entries are to be reused when possible. In his view, multi-
lingual generation, including the problem of language divergences, can be seen as a
mere extension of the monolingual paraphrase task. Whereas the methods proposed
are very compelling with respect to monolingual generation, it is not clear whether
the multilingual goal has been fully achieved. I will come back to this point at the end
of this review.
The book is structured in three main parts:
</bodyText>
<listItem confidence="0.939924">
• Chapters 1 through 3 provide the introduction and background on NLG,
on lexicalizatiort, and on lexical variation. Further background is
</listItem>
<page confidence="0.972618">
270
</page>
<subsectionHeader confidence="0.46153">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9868895">
sprinkled throughout the book, in particular regarding verbal aspect
(Chapter 4) and lexical semantics (Chapter 6).
</bodyText>
<listItem confidence="0.801443875">
• Chapters 4 through 8 constitute the core of the book; I will discuss them
below.
• Finally, Chapter 9 showcases the paraphrasing capabilities of the
generator through a number of examples. Chapter 10 is an interesting
although occasionally weak description of how the approach can be
extended to generate paragraphs, not just sentences. Chapter 11
summarizes the relation between the author&apos;s and others&apos; approaches,
and speculates on a few directions for future research.
</listItem>
<bodyText confidence="0.983034194444444">
At first I found Chapters 4 through 8 slightly overwhelming, as they introduce sev-
eral levels of representation, each with its own terminology and acronyms. However,
at a second, more-careful, reading, everything falls into place. The resulting approach
has at its center a lexicon that partly implements current theories of lexical seman-
tics such as Jackendoff&apos;s (1990) and Levin&apos;s (1993). The lexicon is used to mediate
and map between a language-independent domain model and a language-dependent
ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea
of a two-level representation accommodating language-neutral and language-specific
requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss
[1993], and Di Eugenio [1998]), Stede is among the few who make effective use of
those two levels in a complex system.
Chapter 4 presents the domain model built by means of the description logic
system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in ver-
balizations of situations, to use his own neutral term. Thus, the domain model contains
a representation for categories such as states, activities, and events that Stede nicely
synthesizes from linguistic work on aspect.&apos;
Chapter 5 discusses the different levels of representation used by the generator:
the language-neutral level of situation specifications (SitSpecs), built as instantiations
of portions of the domain model; and the language-specific level of semantic sentence
specifications (SemSpecs), written in terms of Upper Model concepts and relations.
Importantly, SemSpecs are lexicalized. A SemSpec for a sentence is built by collecting
all possible verbalization options that cover a subset of the meaning expressed by
the SitSpec to be verbalized, and combining them appropriately. In the same way
that the domain model provides the foundation for the well-formedness of SitSpecs,
the Upper Model guarantees that a SemSpec can actually be correctly converted to
linguistic output.
Chapter 6 describes the lexicon. Among other information, each lexical entry pro-
vides a denotation, which is a small excerpt of domain-model concepts and relations
and represents the meaning associated with the lexical entry and a partial SemSpec
(PSemSpec), which describes the contribution of this lexical entry to the SemSpec
representing the sentence. A PSemSpec can point to lexical entries in different lan-
guages if they are equivalent in denotation (e.g., rise in English and steigen in German).
The correspondence between denotation and PSemSpec is maintained by coindexed
variables.
Chapter 7 presents the lexical resources to generate verbal alternations. Stede is in-
terested in alternations that change verbal meanings, such as the resultative—causative
</bodyText>
<footnote confidence="0.9615105">
1 Stede argues that it is not contradictory to develop a language-independent ontology by exploiting
some linguistic research.
</footnote>
<page confidence="0.99019">
271
</page>
<note confidence="0.643311">
Computational Linguistics Volume 26, Number 2
</note>
<bodyText confidence="0.999970135135135">
alternation that transforms an activity into an event (e.g., paraphrase 4 in the example
at the beginning of this review is the resultative—causative version of paraphrase 1).
Stede encodes such transformations, which he derives from work by Jackendoff (1990)
and Levin (1993), as rules that are attached to the lexical entry of each verb that can un-
dergo that transformation. Stede assumes that these rules are monotonic, that is, they
only add components of meaning to a base form but don&apos;t eliminate any. As Stede
concedes, this assumption may be too strong, although it appears to be appropriate
for the examples he presents.
Chapter 8 presents the computational architecture (his system is called MOOSE)
and the procedures that first generate all possible verbalization options (interestingly,
by exploiting LOOM subsumption check) and then combine them to find the best
one. The architecture has a &amp;quot;plug&amp;quot; for preference parameters, such as formality, that
are used to choose a paraphrase over another. The link between preference parame-
ters and SemSpecs is through other aspects of lexical entries also discussed in Chap-
ter 6, such as connotation, that concern stylistic features. As previously mentioned,
Chapter 9 shows the computational architecture at work through a range of exam-
ples.
Two aspects of the book left me somewhat unsatisfied. I did not expect a formal
evaluation, which is still an open problem for NLG, and that appears to be very
difficult given the complex nature of Stede&apos;s work. However, I would have liked some
information regarding the size of the knowledge base and of the lexicon. I would also
have liked to know whether an exhaustive experimentation with the prototype has
been carried out and answers to questions such as: does MOOSE ever fail to produce
a verbalization? does MOOSE ever produce an infelicitous one?
The second aspect is whether the multilingual goal has been satisfactorily achieved,
given the closeness of the two chosen languages, English and German. Obviously, dif-
ferent languages require different lexicons and at least different portions of the Upper
Model. However, it is unclear whether the computational flow of information em-
bodied in the architecture in Stede&apos;s Figure 8.1 can remain unchanged when tackling
less closely related languages. Stede acknowledges this problem at the beginning of
Chapter 3, but does not revisit it later; in fact he does not even mention it as an issue
for future work.
I recommend the book not only to researchers interested in text generation and in
machine translation, but to everybody interested in the relationship between language-
independent knowledge representation and language-specific ontologies. Whereas the
hypothesis that the missing link between the two is the lexicon is not surprising
nowadays, Stede&apos;s specific proposal is well defined and effective.
</bodyText>
<sectionHeader confidence="0.97917" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.99714988">
Bateman, John A. 1990. Upper modeling:
Organizing knowledge for natural
language processing. In Proceedings of the
Fifth International Workshop on Natural
Language Generation, Pittsburgh, PA.
Di Eugenio, Barbara. 1998. An action
representation formalism to interpret
natural language instructions.
Computational Intelligence, 14(1):89-133.
Dorr, Bonnie J. and Clare R. Voss. 1993.
Machine translation of spatial expressions:
Defining the relation between an
interlingua and a knowledge
representation system. In Proceedings of the
12th National Conference on Artificial
Intelligence (AAAI-93).
Jackendoff, Ray. 1990. Semantic Structures.
The MIT Press, Cambridge, MA.
Levin, Beth C. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press.
MacGregor, Robert and Mark H. Burstein.
1991. Using a description classifier to
enhance knowledge representation. IEEE
Expert, 6(3):41-46.
</reference>
<page confidence="0.963102">
272
</page>
<reference confidence="0.95179075">
Book Reviews
Nirenburg, Sergei and Lori Levin. 1992.
Syntax-driven and ontology-driven lexical
semantics. In James Pustejovsky and
Sabine Bergler, editors, Lexical Semantics and
Knowledge Representation. Springer Verlag.
Barbara Di Eugenio is an assistant professor in the department of Electrical Engineering and Com-
puter Science at the University of Illinois at Chicago. She is interested in the interpretation and
generation of instructional text and dialogue, specifically in issues of knowledge representation
and lexical semantics concerning action verbs. Di Eugenio&apos;s address is: EECS Department, 1120
SE0 (M/C 154), 851 S. Morgan St, University of Illinois at Chicago, Chicago, IL, 60607, USA;
e-mail: bdieugen@eecs.uic.edu
</reference>
<page confidence="0.998588">
273
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128311">
<title confidence="0.893458666666667">Computational Linguistics Volume 26, Number 2 Lexical Semantics and Knowledge Representation in Multilingual Text Generation</title>
<author confidence="0.998922">Manfred Stede</author>
<affiliation confidence="0.81626825">(Technische Universitat Berlin) Boston: Kluwer Academic Publishers (The Kluwer international series in engineering and computer science,</affiliation>
<address confidence="0.612977">volume 492), 1999, xv+219 pp; hardbound, ISBN 0-7923-8419-9, $129.00, £84.00, Dfl 265.00</address>
<note confidence="0.895529">Reviewed by</note>
<author confidence="0.999208">Barbara Di_Eugenio</author>
<affiliation confidence="0.998086">University of Illinois at Chicago</affiliation>
<abstract confidence="0.998174692307692">Suppose you were to describe Tom draining oil from the engine of his car in 20 seconds. As a competent English speaker, depending on context, you could choose among at least the following subtly different descriptions, called paraphrases by Stede: 1. For 20 seconds, the oil drained from the engine. 2. The engine drained in 20 seconds. 3. Within 20 seconds, Tom drained the engine. 4. Tom drained the engine of the oil in 20 seconds. In contrast, a state-of-the-art natural language generation (NLG) system would likely be able to produce only one of them, modulo variations introduced by, for example, passivization, topicalization, or pronominalization. The problem is not simply to choose among paraphrases, but to be able to produce them to start with. In this book, an extended version of his dissertation, Stede provides the theoretical and practical means of endowing a text generator with such capability. He concenon planning, on choosing an appropriate verbalization for some that another component of the NLG system (the planner) assembled in response to communicative goals. Stede&apos;s approach promotes lexicalization as the central step in this process and the lexicon itself as the link between the languageindependent domain model and the language-specific resources necessary to generate the surface sentence. Stede&apos;s ultimate goal is ambitious: he seeks to provide an architecture that can be used as is to perform generation of the same content in different languages—even lexical entries are to be reused when possible. In his view, multilingual generation, including the problem of language divergences, can be seen as a mere extension of the monolingual paraphrase task. Whereas the methods proposed are very compelling with respect to monolingual generation, it is not clear whether the multilingual goal has been fully achieved. I will come back to this point at the end of this review.</abstract>
<note confidence="0.558269">The book is structured in three main parts:</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John A Bateman</author>
</authors>
<title>Upper modeling: Organizing knowledge for natural language processing.</title>
<date>1990</date>
<booktitle>In Proceedings of the Fifth International Workshop on Natural Language Generation,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3729" citStr="Bateman 1990" startWordPosition="573" endWordPosition="574">nd speculates on a few directions for future research. At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms. However, at a second, more-careful, reading, everything falls into place. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff&apos;s (1990) and Levin&apos;s (1993). The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system. Chapter 4 presents the domain model built by means of the description logic system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in verbalizations of situations, to use his own neutral term. Thus, the domain model contains a representation for categories such as states, a</context>
</contexts>
<marker>Bateman, 1990</marker>
<rawString>Bateman, John A. 1990. Upper modeling: Organizing knowledge for natural language processing. In Proceedings of the Fifth International Workshop on Natural Language Generation, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
</authors>
<title>An action representation formalism to interpret natural language instructions.</title>
<date>1998</date>
<journal>Computational Intelligence,</journal>
<pages>14--1</pages>
<marker>Di Eugenio, 1998</marker>
<rawString>Di Eugenio, Barbara. 1998. An action representation formalism to interpret natural language instructions. Computational Intelligence, 14(1):89-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Clare R Voss</author>
</authors>
<title>Machine translation of spatial expressions: Defining the relation between an interlingua and a knowledge representation system.</title>
<date>1993</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-93).</booktitle>
<marker>Dorr, Voss, 1993</marker>
<rawString>Dorr, Bonnie J. and Clare R. Voss. 1993. Machine translation of spatial expressions: Defining the relation between an interlingua and a knowledge representation system. In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6390" citStr="Jackendoff (1990)" startWordPosition="967" endWordPosition="968">iables. Chapter 7 presents the lexical resources to generate verbal alternations. Stede is interested in alternations that change verbal meanings, such as the resultative—causative 1 Stede argues that it is not contradictory to develop a language-independent ontology by exploiting some linguistic research. 271 Computational Linguistics Volume 26, Number 2 alternation that transforms an activity into an event (e.g., paraphrase 4 in the example at the beginning of this review is the resultative—causative version of paraphrase 1). Stede encodes such transformations, which he derives from work by Jackendoff (1990) and Levin (1993), as rules that are attached to the lexical entry of each verb that can undergo that transformation. Stede assumes that these rules are monotonic, that is, they only add components of meaning to a base form but don&apos;t eliminate any. As Stede concedes, this assumption may be too strong, although it appears to be appropriate for the examples he presents. Chapter 8 presents the computational architecture (his system is called MOOSE) and the procedures that first generate all possible verbalization options (interestingly, by exploiting LOOM subsumption check) and then combine them </context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Jackendoff, Ray. 1990. Semantic Structures. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth C Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6407" citStr="Levin (1993)" startWordPosition="970" endWordPosition="971">ents the lexical resources to generate verbal alternations. Stede is interested in alternations that change verbal meanings, such as the resultative—causative 1 Stede argues that it is not contradictory to develop a language-independent ontology by exploiting some linguistic research. 271 Computational Linguistics Volume 26, Number 2 alternation that transforms an activity into an event (e.g., paraphrase 4 in the example at the beginning of this review is the resultative—causative version of paraphrase 1). Stede encodes such transformations, which he derives from work by Jackendoff (1990) and Levin (1993), as rules that are attached to the lexical entry of each verb that can undergo that transformation. Stede assumes that these rules are monotonic, that is, they only add components of meaning to a base form but don&apos;t eliminate any. As Stede concedes, this assumption may be too strong, although it appears to be appropriate for the examples he presents. Chapter 8 presents the computational architecture (his system is called MOOSE) and the procedures that first generate all possible verbalization options (interestingly, by exploiting LOOM subsumption check) and then combine them to find the best </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth C. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert MacGregor</author>
<author>Mark H Burstein</author>
</authors>
<title>Using a description classifier to enhance knowledge representation.</title>
<date>1991</date>
<journal>IEEE Expert,</journal>
<pages>6--3</pages>
<contexts>
<context position="4151" citStr="MacGregor and Burstein 1991" startWordPosition="637" endWordPosition="640">ackendoff&apos;s (1990) and Levin&apos;s (1993). The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system. Chapter 4 presents the domain model built by means of the description logic system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in verbalizations of situations, to use his own neutral term. Thus, the domain model contains a representation for categories such as states, activities, and events that Stede nicely synthesizes from linguistic work on aspect.&apos; Chapter 5 discusses the different levels of representation used by the generator: the language-neutral level of situation specifications (SitSpecs), built as instantiations of portions of the domain model; and the language-specific level of semantic sentence specifications (SemSpecs), written in terms of Upper Model concepts and relati</context>
</contexts>
<marker>MacGregor, Burstein, 1991</marker>
<rawString>MacGregor, Robert and Mark H. Burstein. 1991. Using a description classifier to enhance knowledge representation. IEEE Expert, 6(3):41-46.</rawString>
</citation>
<citation valid="false">
<institution>Book Reviews</institution>
<marker></marker>
<rawString>Book Reviews</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Lori Levin</author>
</authors>
<title>Syntax-driven and ontology-driven lexical semantics.</title>
<date>1992</date>
<booktitle>In James Pustejovsky and Sabine Bergler, editors, Lexical Semantics and Knowledge Representation.</booktitle>
<publisher>Springer Verlag.</publisher>
<marker>Nirenburg, Levin, 1992</marker>
<rawString>Nirenburg, Sergei and Lori Levin. 1992. Syntax-driven and ontology-driven lexical semantics. In James Pustejovsky and Sabine Bergler, editors, Lexical Semantics and Knowledge Representation. Springer Verlag.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Barbara Di</author>
</authors>
<title>Eugenio is an assistant professor in the department of Electrical Engineering and Computer Science at the University of Illinois at Chicago. She is interested in the interpretation and generation of instructional text and dialogue, specifically in issues of knowledge representation and lexical semantics concerning action verbs.</title>
<booktitle>Di Eugenio&apos;s address is: EECS Department, 1120 SE0 (M/C 154), 851 S.</booktitle>
<institution>Morgan St, University of Illinois at Chicago,</institution>
<location>Chicago, IL, 60607, USA;</location>
<note>e-mail: bdieugen@eecs.uic.edu</note>
<marker>Di, </marker>
<rawString>Barbara Di Eugenio is an assistant professor in the department of Electrical Engineering and Computer Science at the University of Illinois at Chicago. She is interested in the interpretation and generation of instructional text and dialogue, specifically in issues of knowledge representation and lexical semantics concerning action verbs. Di Eugenio&apos;s address is: EECS Department, 1120 SE0 (M/C 154), 851 S. Morgan St, University of Illinois at Chicago, Chicago, IL, 60607, USA; e-mail: bdieugen@eecs.uic.edu</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>