<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000610">
<note confidence="0.964713">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 110-117
Edmonton, May-June 2003
</note>
<title confidence="0.815787">
Language and Task Independent Text Categorization
with Simple Language Models
</title>
<author confidence="0.992646">
Fuchun Peng Dale Schuurmans Shaojun Wang
</author>
<affiliation confidence="0.977081">
School of Computer Science, University of Waterloo
200 University Avenue West, Waterloo, Ontario, Canada, N2L 3G1
</affiliation>
<email confidence="0.888547">
{f3peng, dale, sjwang}@cs.uwaterloo.ca
</email>
<sectionHeader confidence="0.994264" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995529411765">
We present a simple method for language inde-
pendent and task independent text categoriza-
tion learning, based on character-level n-gram
language models. Our approach uses simple
information theoretic principles and achieves
effective performance across a variety of lan-
guages and tasks without requiring feature se-
lection or extensive pre-processing. To demon-
strate the language and task independence of
the proposed technique, we present experimen-
tal results on several languages—Greek, En-
glish, Chinese and Japanese—in several text
categorization problems—language identifica-
tion, authorship attribution, text genre classifi-
cation, and topic detection. Our experimental
results show that the simple approach achieves
state of the art performance in each case.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978557894737">
Text categorization concerns the problem of automati-
cally assigning given text passages (paragraphs or doc-
uments) into predefined categories. Due to the rapid ex-
plosion of texts in digital form, text categorization has
become an important area of research owing to the need
to automatically organize and index large text collections
in various ways. Such techniques are currently being ap-
plied in many areas, including language identification,
authorship attribution (Stamatatos et al., 2000), text genre
classification (Kesseler et al., 1997; Stamatatos et al.,
2000), topic identification (Dumais et al., 1998; Lewis,
1992; McCallum, 1998; Yang, 1999), and subjective sen-
timent classification (Turney, 2002).
Many standard machine learning techniques have been
applied to automated text categorization problems, such
as naive-Bayes classifiers, support vector machines, lin-
ear least squares models, neural networks, and K-nearest
neighbor classifiers (Yang, 1999; Sebastiani, 2002). A
common aspect of these approaches is that they treat text
categorization as a standard classification problem, and
thereby reduce the learning process to two simple steps:
feature engineering, and classification learning over the
feature space. Of these two steps, feature engineering is
critical to achieving good performance in text categoriza-
tion problems. Once good features are identified, almost
any reasonable technique for learning a classifier seems
to perform well (Scott, 1999).
Unfortunately, the standard classification learning
methodology has several drawbacks for text categoriza-
tion. First, feature construction is usually language de-
pendent. Various techniques such as stop-word removal
or stemming require language specific knowledge to de-
sign adequately. Moreover, whether one can use a purely
word-level approach is itself a language dependent issue.
In many Asian languages such as Chinese or Japanese,
identifying words from character sequences is hard, and
any word-based approach must suffer added complexity
in coping with segmentation errors. Second, feature se-
lection is task dependent. For example, tasks like au-
thorship attribution or genre classification require atten-
tion to linguistic style markers (Stamatatos et al., 2000),
whereas topic detection systems rely more heavily on bag
of words features. Third, there are an enormous num-
ber of possible features to consider in text categorization
problems, and standard feature selection approaches do
not always cope well in such circumstances. For exam-
ple, given an enormous number of features, the cumu-
lative effect of uncommon features can still have an im-
portant effect on classification accuracy, even though in-
frequent features contribute less information than com-
mon features individually. Consequently, throwing away
uncommon features is usually not an appropriate strat-
egy in this domain (Aizawa, 2001). Another problem is
that feature selection normally uses indirect tests, such
as χ2 or mutual information, which involve setting arbi-
trary thresholds and conducting a heuristic greedy search
to find good feature sets. Finally, by treating text cate-
gorization as a classical classification problem, standard
approaches can ignore the fact that texts are written in
natural language, meaning that they have many implicit
regularities that can be well modeled with specific tools
from natural language processing.
In this paper, we propose a straightforward text cate-
gorization learning method based on learning category-
specific, character-level, n-gram language models. Al-
though this is a very simple approach, it has not yet been
systematically investigated in the literature. We find that,
surprisingly, we obtain competitive (and often superior)
results to more sophisticated learning and feature con-
struction techniques, while requiring almost no feature
engineering or pre-processing. In fact, the overall ap-
proach requires almost no language specific or task spe-
cific pre-processing to achieve effective performance.
The success of this simple method, we think, is due to
the effectiveness of well known statistical language mod-
eling techniques, which surprisingly have had little sig-
nificant impact on the learning algorithms normally ap-
plied to text categorization. Nevertheless, statistical lan-
guage modeling is also concerned with modeling the se-
mantic, syntactic, lexicographical and phonological regu-
larities of natural language—and would seem to provide a
natural foundation for text categorization problems. One
interesting difference, however, is that instead of explic-
itly pre-computing features and selecting a subset based
on arbitrary decisions, the language modeling approach
simply considers all character (or word) subsequences
occurring in the text as candidate features, and implic-
itly considers the contribution of every feature in the fi-
nal model. Thus, the language modeling approach com-
pletely avoids a potentially error-prone feature selection
process. Also, by applying character-level language mod-
els, one also avoids the word segmentation problems that
arise in many Asian languages, and thereby achieves a
language independent method for constructing accurate
text categorizers.
</bodyText>
<sectionHeader confidence="0.879787" genericHeader="method">
2 n-Gram Language Modeling
</sectionHeader>
<bodyText confidence="0.966098333333333">
The dominant motivation for language modeling has tra-
ditionally come from speech recognition, but language
models have recently become widely used in many other
application areas.
The goal of language modeling is to predict the prob-
ability of naturally occurring word sequences, s =
w1w2...wN; or more simply, to put high probability on
word sequences that actually occur (and low probability
on word sequences that never occur). Given a word se-
quence w1w2...wN to be used as a test corpus, the quality
of a language model can be measured by the empirical
perplexity and entropy scores on this corpus
</bodyText>
<equation confidence="0.998161333333333">
� � �
Perplexity = N �
Entropy = log2 Perplexity (2)
</equation>
<bodyText confidence="0.9998948">
where the goal is to minimize these measures.
The simplest and most successful approach to lan-
guage modeling is still based on the n-gram model. By
the chain rule of probability one can write the probability
of any word sequence as
</bodyText>
<equation confidence="0.997819333333333">
N
Pr(w1w2...wN) = Pr(wi|w1...wi−1) (3)
i=1
</equation>
<bodyText confidence="0.997662">
An n-gram model approximates this probability by
assuming that the only words relevant to predicting
Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.
</bodyText>
<equation confidence="0.999044">
Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1)
</equation>
<bodyText confidence="0.999760666666667">
A straightforward maximum likelihood estimate of n-
gram probabilities from a corpus is given by the observed
frequency of each of the patterns
</bodyText>
<equation confidence="0.9999745">
Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi)
#(wi−n+1...wi−1) (4)
</equation>
<bodyText confidence="0.999852555555556">
where #(.) denotes the number of occurrences of a spec-
ified gram in the training corpus. Although one could at-
tempt to use simple n-gram models to capture long range
dependencies in language, attempting to do so directly
immediately creates sparse data problems: Using grams
of length up to n entails estimating the probability of Wn
events, where W is the size of the word vocabulary. This
quickly overwhelms modern computational and data re-
sources for even modest choices of n (beyond 3 to 6).
Also, because of the heavy tailed nature of language (i.e.
Zipf’s law) one is likely to encounter novel n-grams that
were never witnessed during training in any test corpus,
and therefore some mechanism for assigning non-zero
probability to novel n-grams is a central and unavoidable
issue in statistical language modeling. One standard ap-
proach to smoothing probability estimates to cope with
sparse data problems (and to cope with potentially miss-
ing n-grams) is to use some sort of back-off estimator.
</bodyText>
<equation confidence="0.959585142857143">
Pr(wi|wi−n+1...wi−1)
ˆPr(wi|wi−n+1...wi−1),
if #(wi−n+1...wi) &gt; 0
β(wi−n+1...wi−1) x Pr(wi|wi−n+2...wi−1),
otherwise
(5)
1 (1)
Pr(wi|w1 ... wi−1)
N
i=1
⎧
⎨⎪⎪
⎪⎪⎩
=
</equation>
<bodyText confidence="0.996823">
where Using Bayes rule, this can be rewritten as
</bodyText>
<equation confidence="0.869409166666667">
ˆPr(wi|wi−n+1...wi−1) =discount #(wi−n+1...wi) c* = arg max {Pr(D|c) Pr(c)} (9)
#(wi−n+1...wi−1) cEC {Pr(D|c)} (10)
(6) = arg max
cEC
is the discounted probability and β(wi−n+1...wi−1) is a { N }Prc(wi|wi−n+1...wi−1) (11)
normalization constant = arg max i=1
cEC
β(wi−n+1...wi−1) =
ˆPr(x|wi−n+1...wi−1)
xE(w;_n+1...w;_1x)
1 − EˆPr(x|wi−n+2...wi−1)
xE(w;_n+1...w;_1x)
</equation>
<bodyText confidence="0.9999745">
The discounted probability (6) can be computed
with different smoothing techniques, including absolute
smoothing, Good-Turing smoothing, linear smoothing,
and Witten-Bell smoothing (Chen and Goodman, 1998).
The details of the smoothing techniques are omitted here
for simplicity.
The language models described above use individual
words as the basic unit, although one could instead con-
sider models that use individual characters as the ba-
sic unit. The remaining details remain the same in this
case. The only difference is that the character vocabu-
lary is always much smaller than the word vocabulary,
which means that one can normally use a much higher
order, n, in a character-level n-gram model (although the
text spanned by a character model is still usually less
than that spanned by a word model). The benefits of the
character-level model in the context of text classification
are several-fold: it avoids the need for explicit word seg-
mentation in the case of Asian languages, it captures im-
portant morphological properties of an author’s writing,
it models the typos and misspellings that are common in
informal texts, it can still discover useful inter-word and
inter-phrase features, and it greatly reduces the sparse
data problems associated with large vocabulary models.
In this paper, we experiment with character-level models
to achieve flexibility and language independence.
</bodyText>
<sectionHeader confidence="0.922387" genericHeader="method">
3 Language Models as Text Classifiers
</sectionHeader>
<bodyText confidence="0.996415166666667">
Our approach to applying language models to text cat-
egorization is to use Bayesian decision theory. Assume
we wish to classify a text D into a category c E C =
{c1, ..., cjCj}. A natural choice is to pick the category c
that has the largest posterior probability given the text.
That is,
</bodyText>
<equation confidence="0.994102">
c* = arg max{Pr(c|D)} (8)
cEC
</equation>
<bodyText confidence="0.9999961">
where deducing Eq. (10) from Eq. (9) assumes uniformly
weighted categories (since we have no other prior knowl-
edge). Here, Pr(D|c) is the likelihood of D under cate-
gory c, which can be computed by Eq. (11). Likelihood is
related to perplexity and entropy by Eq. (1) and Eq. (2).
Therefore, our approach is to learn a separate language
model for each category, by training on a data set from
that category. Then, to categorize a new text D, we sup-
ply D to each language model, evaluate the likelihood
(or entropy) of D under the model, and pick the winning
category according to Eq. (10).
The inference of an n-gram based text classifier is
very similar to a naive-Bayes classifier. In fact, n-gram
classifiers are a straightforward generalization of naive-
Bayes: A uni-gram classifier with Laplace smoothing
corresponds exactly to the traditional naive-Bayes clas-
sifier. However, n-gram language models, for larger n,
possess many advantages over naive-Bayes classifiers, in-
cluding modeling longer context and applying superior
smoothing techniques in the presence of sparse data.
</bodyText>
<sectionHeader confidence="0.996641" genericHeader="method">
4 Experimental Comparison
</sectionHeader>
<bodyText confidence="0.999994733333333">
We now proceed to present our results on several text
categorization problems on different languages. Specif-
ically, we consider language identification, Greek author-
ship attribution, Greek genre classification, English topic
detection, Chinese topic detection and Japanese topic de-
tection.
For the sake of consistency with previous re-
search (Aizawa, 2001; He et al., 2000; Stamatatos et
al., 2000), we measure categorization performance by the
overall accuracy, which is the number of correctly iden-
tified texts divided by the total number of texts consid-
ered. We also measure the performance with Macro F-
measure, which is the average of the F-measures across
all categories. F-measure is a combination of precision
and recall (Yang, 1999).
</bodyText>
<subsectionHeader confidence="0.99658">
4.1 Language Identification
</subsectionHeader>
<bodyText confidence="0.998921">
The first text categorization problem we examined was
language identification—a useful pre-processing step in
information retrieval. Language identification is proba-
bly the easiest text classification problem because of the
significant morphological differences between languages,
</bodyText>
<table confidence="0.8852039">
1 −
(7)
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.57 0.53 0.55 0.49 0.55 0.49 0.55 0.49
2 0.85 0.84 0.80 0.75 0.84 0.83 0.84 0.82
3 0.90 0.89 0.79 0.72 0.89 0.88 0.89 0.87
4 0.87 0.85 0.79 0.72 0.85 0.82 0.88 0.86
5 0.86 0.85 0.79 0.72 0.87 0.85 0.86 0.83
6 0.86 0.83 0.79 0.73 0.87 0.85 0.86 0.83
</table>
<tableCaption confidence="0.998533">
Table 1: Results on Greek authorship attribution
</tableCaption>
<table confidence="0.999873111111111">
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.31 0.55 0.30 0.54 0.30 0.54 0.30 0.54
2 0.86 0.86 0.60 0.52 0.82 0.81 0.86 0.86
3 0.77 0.75 0.65 0.59 0.79 0.77 0.85 0.85
4 0.69 0.65 0.58 0.50 0.74 0.69 0.76 0.74
5 0.66 0.61 0.56 0.49 0.69 0.66 0.73 0.70
6 0.62 0.57 0.49 0.53 0.67 0.63 0.71 0.68
7 0.63 0.58 0.49 0.53 0.66 0.62 0.70 0.68
</table>
<tableCaption confidence="0.999729">
Table 2: Results on Greek text genre classification
</tableCaption>
<bodyText confidence="0.999874625">
even when they are based on the same character set.1 In
our experiments, we considered one chapter of Bible that
had been translated into 6 different languages: English,
French, German, Italian, Latin and Spanish. In each
case, we reserved twenty sentences from each language
for testing and used the remainder for training. For this
task, with only bi-gram character-level models and any
smoothing technique, we achieved 100% accuracy.
</bodyText>
<subsectionHeader confidence="0.985377">
4.2 Authorship Attribution
</subsectionHeader>
<bodyText confidence="0.999984842105264">
The second text categorization problem we examined was
author attribution. A famous example is the case of the
Federalist Papers, of which twelve instances are claimed
to have been written both by Alexander Hamilton and
James Madison (Holmes and Forsyth, 1995). Authorship
attribution is more challenging than language identifica-
tion because the difference among the authors is much
more subtle than that among different languages. We con-
sidered a data set used by (Stamatatos et al., 2000) con-
sisting of 20 texts written by 10 different modern Greek
authors (totaling 200 documents). In each case, 10 texts
from each author were used for training and the remain-
ing 10 for testing.
The results using different orders of n-gram models
and different smoothing techniques are shown in Table 1.
With 3-grams and absolute smoothing, we observe 90%
accuracy. This result compares favorably to the 72%
accuracy reported in (Stamatatos et al., 2000) which is
based on linear least square fit (LLSF).
</bodyText>
<subsectionHeader confidence="0.99803">
4.3 Text Genre Classification
</subsectionHeader>
<bodyText confidence="0.954114">
The third problem we examined was text genre classifi-
cation, which is an important application in information
retrieval (Kesseler et al., 1997; Lee et al., 2002). We con-
sidered a Greek data set used by (Stamatatos et al., 2000)
consisting of 20 texts of 10 different styles extracted from
various sources (200 documents total). For each style, we
used 10 texts as training data and the remaining 10 as test-
ing.
1Language identification from speech is much harder.
The results of learning an n-gram based text classifier
are shown in Table 2. The 86% accuracy obtained with
bi-gram models compares favorably to the 82% reported
in (Stamatatos et al., 2000), which again is based on a
much deeper NLP analysis.
</bodyText>
<subsectionHeader confidence="0.989727">
4.4 Topic Detection
</subsectionHeader>
<bodyText confidence="0.999879428571429">
The fourth problem we examined was topic detection in
text, which is a heavily researched text categorization
problem (Dumais et al., 1998; Lewis, 1992; McCallum,
1998; Yang, 1999; Sebastiani, 2002). Here we demon-
strate the language independence of the language mod-
eling approach by considering experiments on English,
Chinese and Japanese data sets.
</bodyText>
<subsectionHeader confidence="0.536943">
4.4.1 English Data
</subsectionHeader>
<bodyText confidence="0.999936105263158">
The English 20 Newsgroup data has been widely used
in topic detection research (McCallum, 1998; Rennie,
2001).2 This collection consists of 19,974 non-empty
documents distributed evenly across 20 newsgroups. We
use the newsgroups to form our categories, and randomly
select 80% of the documents to be used for training and
set aside the remaining 20% for testing.
In this case, as before, we merely considered text to be
a sequence of characters, and learned character-level n-
gram models. The resulting classification accuracies are
reported in in Table 3. With 3-gram (or higher order)
models, we consistently obtain accurate performance,
peaking at 89% accuracy in the case of 6-gram models
with Witten-Bell smoothing. (We note that word-level
models were able to achieve 88% accuracy in this case.)
These results compare favorably to the state of the art re-
sult of 87.5% accuracy reported in (Rennie, 2001), which
was based on a combination of an SVM with error correct
output coding (ECOC).
</bodyText>
<subsectionHeader confidence="0.82058">
4.4.2 Chinese Data
</subsectionHeader>
<bodyText confidence="0.999639666666667">
Chinese topic detection is often thought to be more
challenging than English, because words are not white-
space delimited in Chinese text. This fact seems to
</bodyText>
<footnote confidence="0.658352">
2http://www.ai.mit.edu/˜ jrennie/20Newsgroups/
</footnote>
<table confidence="0.999443727272727">
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.22 0.21 0.22 0.21 0.22 0.21 0.22 0.21
2 0.68 0.66 0.69 0.67 0.68 0.67 0.67 0.65
3 0.86 0.86 0.86 0.86 0.86 0.85 0.86 0.86
4 0.88 0.88 0.88 0.87 0.87 0.87 0.89 0.88
5 0.89 0.88 0.87 0.87 0.88 0.88 0.89 0.89
6 0.89 0.88 0.88 0.88 0.88 0.88 0.89 0.89
7 0.89 0.88 0.88 0.87 0.88 0.88 0.89 0.89
8 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89
9 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89
</table>
<tableCaption confidence="0.83018">
Table 3: Topic detection results on English 20 Newsgroup
data
</tableCaption>
<table confidence="0.999779166666667">
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.77
2 0.80 0.80 0.80 0.80 0.79 0.79 0.80 0.80
3 0.80 0.80 0.81 0.81 0.80 0.80 0.80 0.80
4 0.80 0.80 0.81 0.81 0.81 0.80 0.80 0.80
</table>
<tableCaption confidence="0.99964">
Table 4: Chinese topic detection results
</tableCaption>
<bodyText confidence="0.999952">
require word segmentation to be performed as a pre-
processing step before further classification (He et al.,
2000). However, we avoid the need for explicit segmen-
tation by simply using a character level n-gram classifier.
For Chinese topic detection we considered a data set
investigated in (He et al., 2000). The corpus in this case
is a subset of the TREC-5 data set created for research on
Chinese text retrieval. To make the data set suitable for
text categorization, documents were first clustered into
101 groups that shared the same headline (as indicated
by an SGML tag) and the six most frequent groups were
selected to make a Chinese text categorization data set.
In each group, 500 documents were randomly selected
for training and 100 documents were reserved for testing.
We observe over 80% accuracy for this task, using bi-
gram (2 Chinese characters) or higher order models. This
is the same level of performance reported in (He et al.,
2000) for an SVM approach using word segmentation
and feature selection.
</bodyText>
<subsectionHeader confidence="0.818084">
4.4.3 Japanese Data
</subsectionHeader>
<bodyText confidence="0.998234">
Japanese poses the same word segmentation issues as
Chinese. Word segmentation is also thought to be neces-
sary for Japanese text categorization (Aizawa, 2001), but
we avoid the need again by considering character level
language models.
We consider the Japanese topic detection data inves-
tigated by (Aizawa, 2001). This data set was con-
</bodyText>
<table confidence="0.9990889">
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.29
2 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.62
3 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.72
4 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.77
5 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.77
6 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.77
7 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.76
8 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76
</table>
<tableCaption confidence="0.999382">
Table 5: Japanese topic detection results
</tableCaption>
<bodyText confidence="0.999744285714286">
verted from the NTCIR-J1 data set originally created for
Japanese text retrieval research. The data has 24 cate-
gories. The testing set contains 10,000 documents dis-
tributed unevenly between categories (with a minimum of
56 and maximum of 2696 documents per category). This
imbalanced distribution causes some difficulty since we
assumed a uniform prior over categories. Although this is
easily remedied, we did not fix the problem here. Never-
theless, we obtain experimental results in Table 5 that still
show an 84% accuracy rate on this problem (for 6-gram
or higher order models). This is the same level of per-
formance as that reported in (Aizawa, 2001), which uses
an SVM approach with word segmentation, morphology
analysis and feature selection.
</bodyText>
<sectionHeader confidence="0.990576" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999986571428572">
The perplexity of a test document under a language model
depends on several factors. The two most influential fac-
tors are the order, n, of the n-gram model and the smooth-
ing technique used. Different choices will result in differ-
ent perplexities, which could influence the final decision
in using Eq. (10). We now experimentally assess the in-
fluence of each of these factors below.
</bodyText>
<subsectionHeader confidence="0.997513">
5.1 Effects of n-Gram Order
</subsectionHeader>
<bodyText confidence="0.9999695">
The order n is a key factor in n-gram language models.
If n is too small then the model will not capture enough
context. However, if n is too large then this will create
severe sparse data problems. Both extremes result in a
larger perplexity than the optimal context length. Figures
1 and 2 illustrate the influence of order n on classifica-
tion performance and on language model quality in the
previous five experiments (all using absolute smoothing).
Note that in this case the entropy (bits per character) is
the average entropy across all testing documents. From
the curves, one can see that as the order increases, classi-
fication accuracy increases and testing entropy decreases,
presumably because the longer context better captures the
regularities of the text. However, at some point accu-
</bodyText>
<figure confidence="0.997417333333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
Greek authorship attribution
Greek genre classification
English Topic Detection
Chinese Topic Detection
Japanese Topic Detection
0.21
2 3 4 5 6 7 8
order n or n−gram model
</figure>
<figureCaption confidence="0.7820325">
Figure 1: Influence of the order n on the classification
performance
</figureCaption>
<figure confidence="0.9799976875">
Greek authorship attribution
Greek genre classification
English Topic Detection
Chinese Topic Detection
Japanese Topic Detection
9
8
7
6
5
4
3
1 2 3 4 5 6 7 8 9
order n or n−gram model
11
10
</figure>
<figureCaption confidence="0.999809">
Figure 2: The entropy of different n-gram models
</figureCaption>
<bodyText confidence="0.999976888888889">
racy begins to decrease and entropy begins to increase
as the sparse data problems begin to set in. Interest-
ingly, the effect is more pronounced in some experiments
(Greek genre classification) but less so in other experi-
ments (topic detection under any language). The sensi-
tivity in the Greek genre case could still be attributed to
the sparse data problem (the over-fitting problem in genre
classification could be more serious than the other prob-
lems, as seen from the entropy curves).
</bodyText>
<subsectionHeader confidence="0.91098">
5.2 Effects of Smoothing Technique
</subsectionHeader>
<bodyText confidence="0.983962222222222">
Another key factor affecting the performance of a lan-
guage model is the smoothing technique used. Figures 3
and 4 show the effects of smoothing techniques on clas-
sification accuracy and testing entropy (Chinese topic de-
tection and Japanese topic detection are not shown in the
figure to save space).
Here we find that, in most cases, the smoothing tech-
nique does not have a significant effect on text catego-
rization accuracy, because of the small vocabulary size of
</bodyText>
<figure confidence="0.99165592">
Greek authorship attribution
1
0.8
0.6
0.41
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6
Greek Genre Clasification
1
0.8
0.6
0.4
0.2
1 2 3 4 5 6 7
English Topic Detection
1
0.8
Absolute
Good−Turing
Linear
Witten−Bell
0.2
1 2 3 4 5 6 7 8 9
0.6
0.4
order n of n−gram models
</figure>
<figureCaption confidence="0.903495">
Figure 3: Influence of smoothing on accuracy
</figureCaption>
<figure confidence="0.991380961538462">
Greek authorship attribution
5
4.5
4
3.5
3
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6
Greek Genre Clasification
5.5
5
4.5
4
1 2 3 4 5 6 7
3.5
English Topic Detection
Absolute
Good−Turing
Linear
Witten−Bell
5
3.5
3
1 2 3 4 5 6 7 8 9
4.5
4
order n of n−gram models
</figure>
<figureCaption confidence="0.999976">
Figure 4: The entropy of different smoothing
</figureCaption>
<bodyText confidence="0.999952047619048">
character level n-gram models. However, there are two
exceptions—Greek authorship attribution and Greek text
genre classification—where Good-Turing smoothing is
not as effective as other techniques, even though it gives
better test entropy than some others. Since our goal is to
make a final decision based on the ranking of perplexi-
ties, not just their absolute values, a superior smoothing
method in the sense of perplexity reduction (i.e. from
the perspective of classical language modeling) does not
necessarily lead to a better decision from the perspec-
tive of categorization accuracy. In fact, in all our exper-
iments we have found that it is Witten-Bell smoothing,
not Good-Turing smoothing, that gives the best results in
terms of classification accuracy. Our observation is con-
sistent with previous research which reports that Witten-
Bell smoothing achieves benchmark performance in char-
acter level text compression (Bell et al., 1990). For the
most part, however, one can use any standard smooth-
ing technique in these problems and obtain comparable
performance, since the rankings they produce are almost
always the same.
</bodyText>
<subsectionHeader confidence="0.995238">
5.3 Relation to Previous Research
</subsectionHeader>
<bodyText confidence="0.999978675675676">
In principle, any language model can be used to perform
text categorization based on Eq. (10). However, n-gram
models are extremely simple and have been found to be
effective in many applications. For example, character
level n-gram language models can be easily applied to
any language, and even non-language sequences such as
DNA and music. Character level n-gram models are
widely used in text compression—e.g., the PPM model
(Bell et al., 1990)—and have recently been found to be
effective in text classification problems as well (Teahan
and Harper, 2001). The PPM model is a weighted lin-
ear interpolation n-gram models and has been set as a
benchmark in text compression for decades. Building an
adaptive PPM model is expensive however (Bell et al.,
1990), and our back-off models are relatively much sim-
pler. Using compression techniques for text categoriza-
tion has also been investigated in (Benedetto et al., 2002),
where the authors seek a model that yields the minimum
compression rate increase when a new test document is
introduced. However, this method is found not to be gen-
erally effective nor efficient (Goodman, 2002). In our ap-
proach, we evaluate the perplexity (or entropy) directly
on test documents, and find the outcome to be both effec-
tive and efficient.
Many previous researchers have realized the impor-
tance of n-gram models in designing language indepen-
dent text categorization systems (Cavnar and Trenkle,
1994; Damashek, 1995). However, they have used n-
grams as features for a traditional feature selection pro-
cess, and then deployed classifiers based on calculating
feature-vector similarities. Feature selection in such a
classical approach is critical, and many required proce-
dures, such as stop word removal, are actually language
dependent. In our approach, all n-grams are considered
as features and their importance is implicitly weighted by
their contribution to perplexity. Thus we avoid an error
prone preliminary feature selection step.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999875">
We have presented an extremely simple approach for lan-
guage and task independent text categorization based on
character level n-gram language modeling. The approach
is evaluated on four different languages and four differ-
ent text categorization problems. Surprisingly, we ob-
serve state of the art or better performance in each case.
We have also experimentally analyzed the influence of
two factors that can affect the accuracy of this approach,
and found that for the most part the results are robust
to perturbations of the basic method. The wide appli-
cability and simplicity of this approach makes it imme-
diately applicable to any sequential data (such as natu-
ral language, music, DNA) and yields effective baseline
performance. We are currently investigating more chal-
lenging problems like multiple category classification us-
ing the Reuters-21578 data set (Lewis, 1992) and subjec-
tive sentiment classification (Turney, 2002). To us, these
results suggest that basic statistical language modeling
ideas might be more relevant to other areas of natural lan-
guage processing than commonly perceived.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9915335">
Research supported by Bell University Labs and MI-
TACS.
</bodyText>
<sectionHeader confidence="0.997638" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936788732395">
A. Aizawa. 2001. Linguistic Techniques to Improve
the Performance of Automatic Text Categorization. In
Proceedings of the Sixth Natural Language Processing
Pacific Rim Symposium (NLPRS2001).
T. Bell, J. Cleary, and I. Witten. 1990. Text Compression.
Prentice Hall.
D. Benedetto, E. Caglioti, and V. Loreto. 2002. Lan-
guage Trees and Zipping. Physical Review Letters, 88.
W. Cavnar, J. Trenkle. 1994. N-Gram-Based Text
Categorization. Proceedings of 3rd Annual Sympo-
sium on Document Analysis and Information Retrieval
(SDAIR-94).
S. Chen and J. Goodman. 1998. An Empirical Study of
Smoothing Techniques for Language Modeling. Tech-
nical report, TR-10-98, Harvard University.
M. Damashek. 1995. Gauging Similarity with N-Grams:
Language-Independent Categorization of Text?. Sci-
ence, Vol. 267, 10 February, 843 - 848
S. Dumais, J. Platt, D. Heckerman and M. Sahami. 1998.
Inductive Learning Algorithms And Representations
For Text Categorization. In Proceedings ofACM Con-
ference on Information and Knowledge Management
(CIKM98), Nov. 1998, pp. 148-155.
J. Goodman. 2002. Comment on Language Trees and
Zipping. Unpublished Manuscript.
J. He, A. Tan, and C. Tan. 2000. A Comparative Study on
Chinese Text Categorization Methods. In Proceedings
of PRICAI’2000 International Workshop on Text and
Web Mining, p24-35.
D. Holmes, and R. Forsyth. 1995. The Federalist Revis-
ited: New Directions in Authorship Attribution. Liter-
ary and Linguistic Computing, 10, 111-127.
B. Kessler, G. Nunberg and H. Sch¨uze. 1997. Automatic
Detection of Text Genre. Proceedings of the Thirty-
Fifth Annual Meeting of the Association for Computa-
tional Linguistics (ACL1997).
Y. Lee and S. Myaeng. 2002. Text Genre Classifica-
tion with Genre-Revealing and Subject-Revealing Fea-
tures. Proceedings ofACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR2002).
D. Lewis. 1992. Representation and Learning in Infor-
mation Retrieval Phd thesis, Computer Science Dept-
ment, Univ. of Massachusetts.
A. McCallum and K. Nigam. 1998. A Comparison
of Event Models for Naive Bayes Text Classification.
Proceedings of AAAI-98 Workshop on ”Learning for
Text Categorization”, AAAI Presss.
J. Rennie. 2001. Improving Multi-class Text Classifi-
cation with Naive Bayes. Master’s Thesis. M.I.T. AI
Technical Report AITR-2001-004. 2001.
S. Scott and S. Matwin. 1999. Feature Engineering for
Text Classification. In Proceedings of the Sixteenth In-
ternational Conference on Machine Learning (ICML’
99), pp. 379-388.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1-47.
E. Stamatatos, N. Fakotakis and G. Kokkinakis. 2000.
Automatic Text Categorization in Terms of Genre and
Author. Computational Linguistics, 26 (4), 471-495.
W. Teahan and D. Harper. 2001. Using Compression-
Based Language Models for Text Categorization. Pro-
ceedings of 2001 Workshop on Language Modeling
and Information Retrieval.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Oritentation Applied to Unsupervised Classification
of Reviews. Proceedings of 40th Annual Conference of
Association for Computational Linguistics (ACL 2002)
Y. Yang. 1999. An Evaluation of Statistical Approaches
to Text Categorization. Information Retrieval, 1(1/2),
pp. 67–88.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838121">
<note confidence="0.972772">Proceedings of HLT-NAACL 2003 Main Papers , pp. 110-117 Edmonton, May-June 2003</note>
<title confidence="0.9991795">Language and Task Independent Text Categorization with Simple Language Models</title>
<author confidence="0.99217">Fuchun Peng Dale Schuurmans Shaojun</author>
<affiliation confidence="0.999124">School of Computer Science, University of</affiliation>
<address confidence="0.993706">200 University Avenue West, Waterloo, Ontario, Canada, N2L</address>
<email confidence="0.930074">dale,</email>
<abstract confidence="0.999598388888889">We present a simple method for language independent and task independent text categorizalearning, based on character-level language models. Our approach uses simple information theoretic principles and achieves effective performance across a variety of languages and tasks without requiring feature selection or extensive pre-processing. To demonstrate the language and task independence of the proposed technique, we present experimental results on several languages—Greek, English, Chinese and Japanese—in several text categorization problems—language identification, authorship attribution, text genre classification, and topic detection. Our experimental results show that the simple approach achieves state of the art performance in each case.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aizawa</author>
</authors>
<title>Linguistic Techniques to Improve the Performance of Automatic Text Categorization.</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS2001).</booktitle>
<contexts>
<context position="4016" citStr="Aizawa, 2001" startWordPosition="570" endWordPosition="571">ion systems rely more heavily on bag of words features. Third, there are an enormous number of possible features to consider in text categorization problems, and standard feature selection approaches do not always cope well in such circumstances. For example, given an enormous number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually. Consequently, throwing away uncommon features is usually not an appropriate strategy in this domain (Aizawa, 2001). Another problem is that feature selection normally uses indirect tests, such as χ2 or mutual information, which involve setting arbitrary thresholds and conducting a heuristic greedy search to find good feature sets. Finally, by treating text categorization as a classical classification problem, standard approaches can ignore the fact that texts are written in natural language, meaning that they have many implicit regularities that can be well modeled with specific tools from natural language processing. In this paper, we propose a straightforward text categorization learning method based on</context>
<context position="12465" citStr="Aizawa, 2001" startWordPosition="1869" endWordPosition="1870">ive-Bayes classifier. However, n-gram language models, for larger n, possess many advantages over naive-Bayes classifiers, including modeling longer context and applying superior smoothing techniques in the presence of sparse data. 4 Experimental Comparison We now proceed to present our results on several text categorization problems on different languages. Specifically, we consider language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection. For the sake of consistency with previous research (Aizawa, 2001; He et al., 2000; Stamatatos et al., 2000), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered. We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories. F-measure is a combination of precision and recall (Yang, 1999). 4.1 Language Identification The first text categorization problem we examined was language identification—a useful pre-processing step in information retrieval. Language identification is probably the easiest text</context>
<context position="19812" citStr="Aizawa, 2001" startWordPosition="3092" endWordPosition="3093">ix most frequent groups were selected to make a Chinese text categorization data set. In each group, 500 documents were randomly selected for training and 100 documents were reserved for testing. We observe over 80% accuracy for this task, using bigram (2 Chinese characters) or higher order models. This is the same level of performance reported in (He et al., 2000) for an SVM approach using word segmentation and feature selection. 4.4.3 Japanese Data Japanese poses the same word segmentation issues as Chinese. Word segmentation is also thought to be necessary for Japanese text categorization (Aizawa, 2001), but we avoid the need again by considering character level language models. We consider the Japanese topic detection data investigated by (Aizawa, 2001). This data set was conn Absolute Good-Turing Linear Witten-Bell Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac 1 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.29 2 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.62 3 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.72 4 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.77 5 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.77 6 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.77 7 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.76 8 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76 T</context>
<context position="21108" citStr="Aizawa, 2001" startWordPosition="3319" endWordPosition="3320">ly created for Japanese text retrieval research. The data has 24 categories. The testing set contains 10,000 documents distributed unevenly between categories (with a minimum of 56 and maximum of 2696 documents per category). This imbalanced distribution causes some difficulty since we assumed a uniform prior over categories. Although this is easily remedied, we did not fix the problem here. Nevertheless, we obtain experimental results in Table 5 that still show an 84% accuracy rate on this problem (for 6-gram or higher order models). This is the same level of performance as that reported in (Aizawa, 2001), which uses an SVM approach with word segmentation, morphology analysis and feature selection. 5 Analysis The perplexity of a test document under a language model depends on several factors. The two most influential factors are the order, n, of the n-gram model and the smoothing technique used. Different choices will result in different perplexities, which could influence the final decision in using Eq. (10). We now experimentally assess the influence of each of these factors below. 5.1 Effects of n-Gram Order The order n is a key factor in n-gram language models. If n is too small then the m</context>
</contexts>
<marker>Aizawa, 2001</marker>
<rawString>A. Aizawa. 2001. Linguistic Techniques to Improve the Performance of Automatic Text Categorization. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bell</author>
<author>J Cleary</author>
<author>I Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="25461" citStr="Bell et al., 1990" startWordPosition="4067" endWordPosition="4070"> perplexities, not just their absolute values, a superior smoothing method in the sense of perplexity reduction (i.e. from the perspective of classical language modeling) does not necessarily lead to a better decision from the perspective of categorization accuracy. In fact, in all our experiments we have found that it is Witten-Bell smoothing, not Good-Turing smoothing, that gives the best results in terms of classification accuracy. Our observation is consistent with previous research which reports that WittenBell smoothing achieves benchmark performance in character level text compression (Bell et al., 1990). For the most part, however, one can use any standard smoothing technique in these problems and obtain comparable performance, since the rankings they produce are almost always the same. 5.3 Relation to Previous Research In principle, any language model can be used to perform text categorization based on Eq. (10). However, n-gram models are extremely simple and have been found to be effective in many applications. For example, character level n-gram language models can be easily applied to any language, and even non-language sequences such as DNA and music. Character level n-gram models are w</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>T. Bell, J. Cleary, and I. Witten. 1990. Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Benedetto</author>
<author>E Caglioti</author>
<author>V Loreto</author>
</authors>
<title>Language Trees and Zipping. Physical Review Letters,</title>
<date>2002</date>
<volume>88</volume>
<contexts>
<context position="26605" citStr="Benedetto et al., 2002" startWordPosition="4251" endWordPosition="4254">n-language sequences such as DNA and music. Character level n-gram models are widely used in text compression—e.g., the PPM model (Bell et al., 1990)—and have recently been found to be effective in text classification problems as well (Teahan and Harper, 2001). The PPM model is a weighted linear interpolation n-gram models and has been set as a benchmark in text compression for decades. Building an adaptive PPM model is expensive however (Bell et al., 1990), and our back-off models are relatively much simpler. Using compression techniques for text categorization has also been investigated in (Benedetto et al., 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced. However, this method is found not to be generally effective nor efficient (Goodman, 2002). In our approach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effective and efficient. Many previous researchers have realized the importance of n-gram models in designing language independent text categorization systems (Cavnar and Trenkle, 1994; Damashek, 1995). However, they have used ngrams as features for a traditional featu</context>
</contexts>
<marker>Benedetto, Caglioti, Loreto, 2002</marker>
<rawString>D. Benedetto, E. Caglioti, and V. Loreto. 2002. Language Trees and Zipping. Physical Review Letters, 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cavnar</author>
<author>J Trenkle</author>
</authors>
<title>N-Gram-Based Text Categorization.</title>
<date>1994</date>
<booktitle>Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR-94).</booktitle>
<contexts>
<context position="27120" citStr="Cavnar and Trenkle, 1994" startWordPosition="4334" endWordPosition="4337">. Using compression techniques for text categorization has also been investigated in (Benedetto et al., 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced. However, this method is found not to be generally effective nor efficient (Goodman, 2002). In our approach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effective and efficient. Many previous researchers have realized the importance of n-gram models in designing language independent text categorization systems (Cavnar and Trenkle, 1994; Damashek, 1995). However, they have used ngrams as features for a traditional feature selection process, and then deployed classifiers based on calculating feature-vector similarities. Feature selection in such a classical approach is critical, and many required procedures, such as stop word removal, are actually language dependent. In our approach, all n-grams are considered as features and their importance is implicitly weighted by their contribution to perplexity. Thus we avoid an error prone preliminary feature selection step. 6 Conclusion We have presented an extremely simple approach f</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W. Cavnar, J. Trenkle. 1994. N-Gram-Based Text Categorization. Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR-94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical report, TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="9463" citStr="Chen and Goodman, 1998" startWordPosition="1385" endWordPosition="1388">= where Using Bayes rule, this can be rewritten as ˆPr(wi|wi−n+1...wi−1) =discount #(wi−n+1...wi) c* = arg max {Pr(D|c) Pr(c)} (9) #(wi−n+1...wi−1) cEC {Pr(D|c)} (10) (6) = arg max cEC is the discounted probability and β(wi−n+1...wi−1) is a { N }Prc(wi|wi−n+1...wi−1) (11) normalization constant = arg max i=1 cEC β(wi−n+1...wi−1) = ˆPr(x|wi−n+1...wi−1) xE(w;_n+1...w;_1x) 1 − EˆPr(x|wi−n+2...wi−1) xE(w;_n+1...w;_1x) The discounted probability (6) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing (Chen and Goodman, 1998). The details of the smoothing techniques are omitted here for simplicity. The language models described above use individual words as the basic unit, although one could instead consider models that use individual characters as the basic unit. The remaining details remain the same in this case. The only difference is that the character vocabulary is always much smaller than the word vocabulary, which means that one can normally use a much higher order, n, in a character-level n-gram model (although the text spanned by a character model is still usually less than that spanned by a word model). </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical report, TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Damashek</author>
</authors>
<title>Gauging Similarity with N-Grams:</title>
<date>1995</date>
<journal>Language-Independent Categorization of Text?. Science,</journal>
<volume>267</volume>
<pages>848</pages>
<contexts>
<context position="27137" citStr="Damashek, 1995" startWordPosition="4338" endWordPosition="4339">ques for text categorization has also been investigated in (Benedetto et al., 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced. However, this method is found not to be generally effective nor efficient (Goodman, 2002). In our approach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effective and efficient. Many previous researchers have realized the importance of n-gram models in designing language independent text categorization systems (Cavnar and Trenkle, 1994; Damashek, 1995). However, they have used ngrams as features for a traditional feature selection process, and then deployed classifiers based on calculating feature-vector similarities. Feature selection in such a classical approach is critical, and many required procedures, such as stop word removal, are actually language dependent. In our approach, all n-grams are considered as features and their importance is implicitly weighted by their contribution to perplexity. Thus we avoid an error prone preliminary feature selection step. 6 Conclusion We have presented an extremely simple approach for language and t</context>
</contexts>
<marker>Damashek, 1995</marker>
<rawString>M. Damashek. 1995. Gauging Similarity with N-Grams: Language-Independent Categorization of Text?. Science, Vol. 267, 10 February, 843 - 848</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive Learning Algorithms And Representations For Text Categorization.</title>
<date>1998</date>
<booktitle>In Proceedings ofACM Conference on Information and Knowledge Management (CIKM98),</booktitle>
<pages>148--155</pages>
<contexts>
<context position="1748" citStr="Dumais et al., 1998" startWordPosition="242" endWordPosition="245">duction Text categorization concerns the problem of automatically assigning given text passages (paragraphs or documents) into predefined categories. Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways. Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning ove</context>
<context position="16341" citStr="Dumais et al., 1998" startWordPosition="2508" endWordPosition="2511">ts of 10 different styles extracted from various sources (200 documents total). For each style, we used 10 texts as training data and the remaining 10 as testing. 1Language identification from speech is much harder. The results of learning an n-gram based text classifier are shown in Table 2. The 86% accuracy obtained with bi-gram models compares favorably to the 82% reported in (Stamatatos et al., 2000), which again is based on a much deeper NLP analysis. 4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002). Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets. 4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups. We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing. In t</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>S. Dumais, J. Platt, D. Heckerman and M. Sahami. 1998. Inductive Learning Algorithms And Representations For Text Categorization. In Proceedings ofACM Conference on Information and Knowledge Management (CIKM98), Nov. 1998, pp. 148-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Comment on Language Trees and Zipping. Unpublished Manuscript.</title>
<date>2002</date>
<contexts>
<context position="26817" citStr="Goodman, 2002" startWordPosition="4288" endWordPosition="4289">blems as well (Teahan and Harper, 2001). The PPM model is a weighted linear interpolation n-gram models and has been set as a benchmark in text compression for decades. Building an adaptive PPM model is expensive however (Bell et al., 1990), and our back-off models are relatively much simpler. Using compression techniques for text categorization has also been investigated in (Benedetto et al., 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced. However, this method is found not to be generally effective nor efficient (Goodman, 2002). In our approach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effective and efficient. Many previous researchers have realized the importance of n-gram models in designing language independent text categorization systems (Cavnar and Trenkle, 1994; Damashek, 1995). However, they have used ngrams as features for a traditional feature selection process, and then deployed classifiers based on calculating feature-vector similarities. Feature selection in such a classical approach is critical, and many required procedures, such as stop word re</context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>J. Goodman. 2002. Comment on Language Trees and Zipping. Unpublished Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J He</author>
<author>A Tan</author>
<author>C Tan</author>
</authors>
<title>A Comparative Study on Chinese Text Categorization Methods.</title>
<date>2000</date>
<booktitle>In Proceedings of PRICAI’2000 International Workshop on Text and Web Mining,</booktitle>
<pages>24--35</pages>
<contexts>
<context position="12482" citStr="He et al., 2000" startWordPosition="1871" endWordPosition="1874">sifier. However, n-gram language models, for larger n, possess many advantages over naive-Bayes classifiers, including modeling longer context and applying superior smoothing techniques in the presence of sparse data. 4 Experimental Comparison We now proceed to present our results on several text categorization problems on different languages. Specifically, we consider language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection. For the sake of consistency with previous research (Aizawa, 2001; He et al., 2000; Stamatatos et al., 2000), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered. We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories. F-measure is a combination of precision and recall (Yang, 1999). 4.1 Language Identification The first text categorization problem we examined was language identification—a useful pre-processing step in information retrieval. Language identification is probably the easiest text classification p</context>
<context position="18725" citStr="He et al., 2000" startWordPosition="2910" endWordPosition="2913">0.88 0.88 0.89 0.89 7 0.89 0.88 0.88 0.87 0.88 0.88 0.89 0.89 8 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89 9 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89 Table 3: Topic detection results on English 20 Newsgroup data n Absolute Good-Turing Linear Witten-Bell Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac 1 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.77 2 0.80 0.80 0.80 0.80 0.79 0.79 0.80 0.80 3 0.80 0.80 0.81 0.81 0.80 0.80 0.80 0.80 4 0.80 0.80 0.81 0.81 0.81 0.80 0.80 0.80 Table 4: Chinese topic detection results require word segmentation to be performed as a preprocessing step before further classification (He et al., 2000). However, we avoid the need for explicit segmentation by simply using a character level n-gram classifier. For Chinese topic detection we considered a data set investigated in (He et al., 2000). The corpus in this case is a subset of the TREC-5 data set created for research on Chinese text retrieval. To make the data set suitable for text categorization, documents were first clustered into 101 groups that shared the same headline (as indicated by an SGML tag) and the six most frequent groups were selected to make a Chinese text categorization data set. In each group, 500 documents were random</context>
</contexts>
<marker>He, Tan, Tan, 2000</marker>
<rawString>J. He, A. Tan, and C. Tan. 2000. A Comparative Study on Chinese Text Categorization Methods. In Proceedings of PRICAI’2000 International Workshop on Text and Web Mining, p24-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Holmes</author>
<author>R Forsyth</author>
</authors>
<title>The Federalist Revisited: New Directions in Authorship Attribution.</title>
<date>1995</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>10</volume>
<pages>111--127</pages>
<contexts>
<context position="14712" citStr="Holmes and Forsyth, 1995" startWordPosition="2240" endWordPosition="2243">of Bible that had been translated into 6 different languages: English, French, German, Italian, Latin and Spanish. In each case, we reserved twenty sentences from each language for testing and used the remainder for training. For this task, with only bi-gram character-level models and any smoothing technique, we achieved 100% accuracy. 4.2 Authorship Attribution The second text categorization problem we examined was author attribution. A famous example is the case of the Federalist Papers, of which twelve instances are claimed to have been written both by Alexander Hamilton and James Madison (Holmes and Forsyth, 1995). Authorship attribution is more challenging than language identification because the difference among the authors is much more subtle than that among different languages. We considered a data set used by (Stamatatos et al., 2000) consisting of 20 texts written by 10 different modern Greek authors (totaling 200 documents). In each case, 10 texts from each author were used for training and the remaining 10 for testing. The results using different orders of n-gram models and different smoothing techniques are shown in Table 1. With 3-grams and absolute smoothing, we observe 90% accuracy. This re</context>
</contexts>
<marker>Holmes, Forsyth, 1995</marker>
<rawString>D. Holmes, and R. Forsyth. 1995. The Federalist Revisited: New Directions in Authorship Attribution. Literary and Linguistic Computing, 10, 111-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kessler</author>
<author>G Nunberg</author>
<author>H Sch¨uze</author>
</authors>
<title>Automatic Detection of Text Genre.</title>
<date>1997</date>
<booktitle>Proceedings of the ThirtyFifth Annual Meeting of the Association for Computational Linguistics (ACL1997).</booktitle>
<marker>Kessler, Nunberg, Sch¨uze, 1997</marker>
<rawString>B. Kessler, G. Nunberg and H. Sch¨uze. 1997. Automatic Detection of Text Genre. Proceedings of the ThirtyFifth Annual Meeting of the Association for Computational Linguistics (ACL1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
<author>S Myaeng</author>
</authors>
<title>Text Genre Classification with Genre-Revealing and Subject-Revealing Features.</title>
<date>2002</date>
<booktitle>Proceedings ofACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2002).</booktitle>
<marker>Lee, Myaeng, 2002</marker>
<rawString>Y. Lee and S. Myaeng. 2002. Text Genre Classification with Genre-Revealing and Subject-Revealing Features. Proceedings ofACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Representation and Learning in Information Retrieval</title>
<date>1992</date>
<tech>Phd thesis,</tech>
<institution>Computer Science Deptment, Univ. of Massachusetts.</institution>
<contexts>
<context position="1761" citStr="Lewis, 1992" startWordPosition="246" endWordPosition="247">zation concerns the problem of automatically assigning given text passages (paragraphs or documents) into predefined categories. Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways. Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature</context>
<context position="16354" citStr="Lewis, 1992" startWordPosition="2512" endWordPosition="2513">yles extracted from various sources (200 documents total). For each style, we used 10 texts as training data and the remaining 10 as testing. 1Language identification from speech is much harder. The results of learning an n-gram based text classifier are shown in Table 2. The 86% accuracy obtained with bi-gram models compares favorably to the 82% reported in (Stamatatos et al., 2000), which again is based on a much deeper NLP analysis. 4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002). Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets. 4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups. We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing. In this case, as </context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D. Lewis. 1992. Representation and Learning in Information Retrieval Phd thesis, Computer Science Deptment, Univ. of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A Comparison of Event Models for Naive Bayes Text Classification.</title>
<date>1998</date>
<booktitle>Proceedings of AAAI-98 Workshop on ”Learning for Text Categorization”, AAAI</booktitle>
<publisher>Presss.</publisher>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. A Comparison of Event Models for Naive Bayes Text Classification. Proceedings of AAAI-98 Workshop on ”Learning for Text Categorization”, AAAI Presss.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rennie</author>
</authors>
<title>Improving Multi-class Text Classification with Naive Bayes.</title>
<date>2001</date>
<tech>Master’s Thesis. M.I.T. AI Technical Report AITR-2001-004.</tech>
<contexts>
<context position="16682" citStr="Rennie, 2001" startWordPosition="2561" endWordPosition="2562">s favorably to the 82% reported in (Stamatatos et al., 2000), which again is based on a much deeper NLP analysis. 4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002). Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets. 4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups. We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing. In this case, as before, we merely considered text to be a sequence of characters, and learned character-level ngram models. The resulting classification accuracies are reported in in Table 3. With 3-gram (or higher order) models, we consistently obtain accurate performance, peaking at 89% accuracy in the case of 6-gram models with Witten-Bell</context>
</contexts>
<marker>Rennie, 2001</marker>
<rawString>J. Rennie. 2001. Improving Multi-class Text Classification with Naive Bayes. Master’s Thesis. M.I.T. AI Technical Report AITR-2001-004. 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scott</author>
<author>S Matwin</author>
</authors>
<title>Feature Engineering for Text Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Machine Learning (ICML’ 99),</booktitle>
<pages>379--388</pages>
<marker>Scott, Matwin, 1999</marker>
<rawString>S. Scott and S. Matwin. 1999. Feature Engineering for Text Classification. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML’ 99), pp. 379-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<pages>34--1</pages>
<contexts>
<context position="2120" citStr="Sebastiani, 2002" startWordPosition="292" endWordPosition="293">are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space. Of these two steps, feature engineering is critical to achieving good performance in text categorization problems. Once good features are identified, almost any reasonable technique for learning a classifier seems to perform well (Scott, 1999). Unfortunately, the standard classification learning methodology has several drawbacks for text categorizat</context>
<context position="16401" citStr="Sebastiani, 2002" startWordPosition="2518" endWordPosition="2519">documents total). For each style, we used 10 texts as training data and the remaining 10 as testing. 1Language identification from speech is much harder. The results of learning an n-gram based text classifier are shown in Table 2. The 86% accuracy obtained with bi-gram models compares favorably to the 82% reported in (Stamatatos et al., 2000), which again is based on a much deeper NLP analysis. 4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002). Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets. 4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups. We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing. In this case, as before, we merely considered text to be a seque</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>Automatic Text Categorization</title>
<date>2000</date>
<booktitle>in Terms of Genre and Author. Computational Linguistics,</booktitle>
<volume>26</volume>
<issue>4</issue>
<pages>471--495</pages>
<contexts>
<context position="1629" citStr="Stamatatos et al., 2000" startWordPosition="225" endWordPosition="228">etection. Our experimental results show that the simple approach achieves state of the art performance in each case. 1 Introduction Text categorization concerns the problem of automatically assigning given text passages (paragraphs or documents) into predefined categories. Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways. Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification pro</context>
<context position="3381" citStr="Stamatatos et al., 2000" startWordPosition="471" endWordPosition="474">sually language dependent. Various techniques such as stop-word removal or stemming require language specific knowledge to design adequately. Moreover, whether one can use a purely word-level approach is itself a language dependent issue. In many Asian languages such as Chinese or Japanese, identifying words from character sequences is hard, and any word-based approach must suffer added complexity in coping with segmentation errors. Second, feature selection is task dependent. For example, tasks like authorship attribution or genre classification require attention to linguistic style markers (Stamatatos et al., 2000), whereas topic detection systems rely more heavily on bag of words features. Third, there are an enormous number of possible features to consider in text categorization problems, and standard feature selection approaches do not always cope well in such circumstances. For example, given an enormous number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually. Consequently, throwing away uncommon features is usually not an appropriate str</context>
<context position="12508" citStr="Stamatatos et al., 2000" startWordPosition="1875" endWordPosition="1878">n-gram language models, for larger n, possess many advantages over naive-Bayes classifiers, including modeling longer context and applying superior smoothing techniques in the presence of sparse data. 4 Experimental Comparison We now proceed to present our results on several text categorization problems on different languages. Specifically, we consider language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection. For the sake of consistency with previous research (Aizawa, 2001; He et al., 2000; Stamatatos et al., 2000), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered. We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories. F-measure is a combination of precision and recall (Yang, 1999). 4.1 Language Identification The first text categorization problem we examined was language identification—a useful pre-processing step in information retrieval. Language identification is probably the easiest text classification problem because of the sign</context>
<context position="14942" citStr="Stamatatos et al., 2000" startWordPosition="2276" endWordPosition="2279">his task, with only bi-gram character-level models and any smoothing technique, we achieved 100% accuracy. 4.2 Authorship Attribution The second text categorization problem we examined was author attribution. A famous example is the case of the Federalist Papers, of which twelve instances are claimed to have been written both by Alexander Hamilton and James Madison (Holmes and Forsyth, 1995). Authorship attribution is more challenging than language identification because the difference among the authors is much more subtle than that among different languages. We considered a data set used by (Stamatatos et al., 2000) consisting of 20 texts written by 10 different modern Greek authors (totaling 200 documents). In each case, 10 texts from each author were used for training and the remaining 10 for testing. The results using different orders of n-gram models and different smoothing techniques are shown in Table 1. With 3-grams and absolute smoothing, we observe 90% accuracy. This result compares favorably to the 72% accuracy reported in (Stamatatos et al., 2000) which is based on linear least square fit (LLSF). 4.3 Text Genre Classification The third problem we examined was text genre classification, which i</context>
</contexts>
<marker>Stamatatos, Fakotakis, Kokkinakis, 2000</marker>
<rawString>E. Stamatatos, N. Fakotakis and G. Kokkinakis. 2000. Automatic Text Categorization in Terms of Genre and Author. Computational Linguistics, 26 (4), 471-495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Teahan</author>
<author>D Harper</author>
</authors>
<title>Using CompressionBased Language Models for Text Categorization.</title>
<date>2001</date>
<booktitle>Proceedings of 2001 Workshop on Language Modeling and Information Retrieval.</booktitle>
<contexts>
<context position="26242" citStr="Teahan and Harper, 2001" startWordPosition="4191" endWordPosition="4194">ce are almost always the same. 5.3 Relation to Previous Research In principle, any language model can be used to perform text categorization based on Eq. (10). However, n-gram models are extremely simple and have been found to be effective in many applications. For example, character level n-gram language models can be easily applied to any language, and even non-language sequences such as DNA and music. Character level n-gram models are widely used in text compression—e.g., the PPM model (Bell et al., 1990)—and have recently been found to be effective in text classification problems as well (Teahan and Harper, 2001). The PPM model is a weighted linear interpolation n-gram models and has been set as a benchmark in text compression for decades. Building an adaptive PPM model is expensive however (Bell et al., 1990), and our back-off models are relatively much simpler. Using compression techniques for text categorization has also been investigated in (Benedetto et al., 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced. However, this method is found not to be generally effective nor efficient (Goodman, 2002). In our approach, we eva</context>
</contexts>
<marker>Teahan, Harper, 2001</marker>
<rawString>W. Teahan and D. Harper. 2001. Using CompressionBased Language Models for Text Categorization. Proceedings of 2001 Workshop on Language Modeling and Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Oritentation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>Proceedings of 40th Annual Conference of Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1846" citStr="Turney, 2002" startWordPosition="257" endWordPosition="258">phs or documents) into predefined categories. Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways. Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space. Of these two steps, feature engineering is critical to achieving good perform</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Oritentation Applied to Unsupervised Classification of Reviews. Proceedings of 40th Annual Conference of Association for Computational Linguistics (ACL 2002)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An Evaluation of Statistical Approaches to Text Categorization.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>67--88</pages>
<contexts>
<context position="1790" citStr="Yang, 1999" startWordPosition="250" endWordPosition="251">f automatically assigning given text passages (paragraphs or documents) into predefined categories. Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways. Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al., 2000), text genre classification (Kesseler et al., 1997; Stamatatos et al., 2000), topic identification (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002). Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002). A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space. Of these two steps, f</context>
<context position="12850" citStr="Yang, 1999" startWordPosition="1932" endWordPosition="1933">language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection. For the sake of consistency with previous research (Aizawa, 2001; He et al., 2000; Stamatatos et al., 2000), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered. We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories. F-measure is a combination of precision and recall (Yang, 1999). 4.1 Language Identification The first text categorization problem we examined was language identification—a useful pre-processing step in information retrieval. Language identification is probably the easiest text classification problem because of the significant morphological differences between languages, 1 − (7) n Absolute Good-Turing Linear Witten-Bell Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac 1 0.57 0.53 0.55 0.49 0.55 0.49 0.55 0.49 2 0.85 0.84 0.80 0.75 0.84 0.83 0.84 0.82 3 0.90 0.89 0.79 0.72 0.89 0.88 0.89 0.87 4 0.87 0.85 0.79 0.72 0.85 0.82 0.88 0.86 5 0.86 0.85 0.79 0.72 0.87 </context>
<context position="16382" citStr="Yang, 1999" startWordPosition="2516" endWordPosition="2517">ources (200 documents total). For each style, we used 10 texts as training data and the remaining 10 as testing. 1Language identification from speech is much harder. The results of learning an n-gram based text classifier are shown in Table 2. The 86% accuracy obtained with bi-gram models compares favorably to the 82% reported in (Stamatatos et al., 2000), which again is based on a much deeper NLP analysis. 4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al., 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002). Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets. 4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups. We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing. In this case, as before, we merely considered</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Y. Yang. 1999. An Evaluation of Statistical Approaches to Text Categorization. Information Retrieval, 1(1/2), pp. 67–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>