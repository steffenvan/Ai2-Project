<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001337">
<title confidence="0.948086">
Ranking vs. Regression in Machine Translation Evaluation
</title>
<author confidence="0.993278">
Kevin Duh*
</author>
<affiliation confidence="0.996863">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.919865">
Seattle, WA 98195
</address>
<email confidence="0.999251">
kevinduh@u.washington.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999707285714286">
Automatic evaluation of machine translation
(MT) systems is an important research topic
for the advancement of MT technology. Most
automatic evaluation methods proposed to
date are score-based: they compute scores that
represent translation quality, and MT systems
are compared on the basis of these scores.
We advocate an alternative perspective of au-
tomatic MT evaluation based on ranking. In-
stead ofproducing scores, we directly produce
a ranking over the set of MT systems to be
compared. This perspective is often simpler
when the evaluation goal is system compari-
son. We argue that it is easier to elicit human
judgments of ranking and develop a machine
learning approach to train on rank data. We
compare this ranking method to a score-based
regression method on WMT07 data. Results
indicate that ranking achieves higher correla-
tion to human judgments, especially in cases
where ranking-specific features are used.
</bodyText>
<sectionHeader confidence="0.98865" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999124511111111">
Automatic evaluation of machine translation (MT)
systems is an important research topic for the ad-
vancement of MT technology, since automatic eval-
uation methods can be used to quickly determine the
(approximate) quality of MT system outputs. This is
useful for tuning system parameters and for compar-
ing different techniques in cases when human judg-
ments for each MT output are expensivie to obtain.
Many automatic evaluation methods have been
proposed to date. Successful methods such as BLEU
&apos;work supported by an NSF Graduate Research Fellowship.
(Papineni et al., 2002) work by comparing MT out-
put with one or more human reference translations
and generating a similarity score. Methods differ by
the definition of similarity. For instance, BLEU and
ROUGE (Lin and Och, 2004) are based on n-gram
precisions, METEOR (Banerjee and Lavie, 2005)
and STM (Liu and Gildea, 2005) use word-class
or structural information, Kauchak (2006) leverages
on paraphrases, and TER (Snover et al., 2006) uses
edit-distances. Currently, BLEU is the most popu-
lar metric; it has been shown that it correlates well
with human judgments on the corpus level. How-
ever, finding a metric that correlates well with hu-
man judgments on the sentence-level is still an open
challenge (Blatz and others, 2003).
Machine learning approaches have been proposed
to address the problem of sentence-level evalua-
tion. (Corston-Oliver et al., 2001) and (Kulesza
and Shieber, 2004) train classifiers to discrim-
inate between human-like translations and auto-
matic translations, using features from the afore-
mentioned metrics (e.g. n-gram precisions). In con-
trast, (Albrecht and Hwa, 2007) argues for a re-
gression approach that directly predicts human ad-
equecy/fluency scores.
All the above methods are score-based in the
sense that they generate a score for each MT system
output. When the evaluation goal is to compare mul-
tiple MT systems, scores are first generated inde-
pendently for each system, then systems are ranked
by their respective scores. We think that this two-
step process may be unnecessarily complex. Why
solve a more difficult problem of predicting the qual-
ity of MT system outputs, when the goal is simply
</bodyText>
<page confidence="0.979296">
191
</page>
<note confidence="0.400745">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 191–194,
</note>
<page confidence="0.479373">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999742625">
to compare systems? In this regard, we propose a
ranking-based approach that directly ranks a set of
MT systems without going through the intermediary
of system-specific scores. Our approach requires (a)
training data in terms of human ranking judgments
of MT outputs, and (b) a machine learning algorithm
for learning and predicting rankings.1
The advantages of a ranking approach are:
</bodyText>
<listItem confidence="0.742962117647059">
• It is often easier for human judges to rank MT
outputs by preference than to assign absolute
scores (Vilar et al., 2007). This is because it is
difficult to quantify the quality of a translation
accurately, but relative easy to tell which one
of several translations is better. Thus human-
annotated data based on ranking may be less
costly to acquire.
• The inter- and intra-annotator agreement for
ranking is much more reasonable than that of
scoring. For instance, Callison-Burch (2007)
found the inter-annotator agreement (Kappa)
for scoring fluency/adequency to be around
.22-.25, whereas the Kappa for ranking is
around .37-.56. Thus human-annotated data
based on ranking may be more reliable to use.
• As mentioned earlier, when the final goal of
</listItem>
<bodyText confidence="0.987990512195122">
the evaluation is comparing systems, ranking
more directly solves the problem. A scoring
approach essentially addresses a more difficult
problem of estimating MT output quality.
Nevertheless, we note that score-based ap-
proaches remain important in cases when the ab-
solute difference between MT quality is desired.
For instance, one might wonder by how much does
the top-ranked MT system outperform the second-
ranked system, in which case a ranking-based ap-
proach provide no guidance.
In the following, Section 2 formulates the
sentence-level MT evaluation problem as a ranking
problem; Section 3 explains a machine learning ap-
proach for training and predicting rankings; this is
our submission to the WMT2008 Shared Evaluation
1Our ranking approach is similar to Ye et. al. (2007), who
was the first to advocate MT evaluation as a ranking problem.
Here we focus on comparing ranking vs. scoring approaches,
which was not done in previous work.
task. Ranking vs. scoring approaches are compared
in Section 4.
2 Formulation of the Ranking Problem
We formulate the sentence-level MT evaluation
problem as follows: Suppose there are T source sen-
tences to be translated. Let rt, t = 1..T be the set of
references2. Corresponding to each source sentence,
there are N MT system outputs o(n)
t , n = 1..N and
Mt (Mt G N) human evaluations. The evaluations
are represented as Mt-dimensional label vectors yt.
In a scoring approach, the elements of yt may cor-
respond to, e.g. a fluency score on a scale of 1 to 5.
In a ranking approach, they may correspond to rel-
ative scores that are used to represent ordering (e.g.
yt = [6; 1; 3] means that there are three outputs, and
the first is ranked best, followed by third, then sec-
ond.)
In order to do machine learning, we extract fea-
ture vectors x(n)
t from each pair of rt and o(n)
</bodyText>
<equation confidence="0.750960818181818">
t .3
The set �(x(n)
t , yt)}t=1..T forms the training set.
In a scoring approach, we train a function f with
f(x(n)
t ) � y(n). In a ranking approach, we train
f such that higher-ranked outputs have higher func-
tion values. In the example above, we would want:
f(x(n=1)
t ) &gt; f(x(n=3)
t ) &gt; f(x(n=2)
</equation>
<bodyText confidence="0.9600675">
t ). Once f is
trained, it can be applied to rank any new data: this is
done by extracting features from references/outputs
and sorting by function values.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="introduction">
3 Implementation
</sectionHeader>
<subsectionHeader confidence="0.999986">
3.1 Sentence-level scoring and ranking
</subsectionHeader>
<bodyText confidence="0.9999851">
We now describe the particular scoring and rank-
ing implementations we examined and submitted to
the WMT2008 Shared Evaluation task. In the scor-
ing approach, f is trained using RegressionSVM
(Drucker and others, 1996); in the ranking ap-
proach, we examined RankSVM (Joachims, 2002)
and RankBoost (Freund et al., 2003). We used only
linear kernels for RegressionSVM and RankSVM,
while allowed RankBoost to produce non-linear f
based on a feature thresholds.
</bodyText>
<footnote confidence="0.99921">
2Here we assume single reference for ease of notation; this
can be easily extended for multiple reference
3Only Mt (not N) features vectors are extracted in practice.
</footnote>
<page confidence="0.960701">
192
</page>
<table confidence="0.998977625">
ID Description
1-4 log of ngram precision, n=1..4
5 ratio of hypothesis and reference length
6-9 ngram precision, n=1..4
10-11 hypothesis and reference length
12 BLEU
13 Smooth BLEU
14-20 Intra-set features for ID 5-9, 12,13
</table>
<tableCaption confidence="0.794712">
Table 1: Feature set: Features 1-5 can be combined (with
uniform weights) to form the log(BLEU) score. Features
6-11 are redundant statistics, but scaled differently. Fea-
ture 12 is sentence-level BLEU; Feature 13 is a modified
version with add-1 count to each ngram precision (this
avoids prevalent zeros). Features 14-20 are only available
in the ranking approach; they are derived by comparing
different outputs within the same set to be ranked.
</tableCaption>
<bodyText confidence="0.9997292">
The complete feature set is shown in Table 1. We
restricted our feature set to traditional BLEU statis-
tics since our experimental goal is to directly com-
pare regression, ranking, and BLEU. Features 14-
20 are the only novel features proposed here. We
wanted to examine features that are enabled by a
ranking approach, but not possible for a scoring
approach. We thus introduce “intra-set features”,
which are statistics computed by observing the en-
tire set of existing features {x�n�
</bodyText>
<equation confidence="0.720421">
t }n=1..Mt.
</equation>
<bodyText confidence="0.99985565">
For instance: We define Feature 14 by looking at
the relative 1-gram precision (Feature 1) in the set of
Mt outputs. Feature 14 is set to value 1 for the out-
put which has the best 1-gram precision, and value 0
otherwise. Similarly, Feature 15 is a binary variable
that is 1 for the output with the best 2-gram preci-
sion, and 0 for all others. The advantage of intra-set
features is calibration. e.g. If the outputs for rt=1
all have relatively high BLEU compared to those
of rt=2, the basic BLEU features will vary widely
across the two sets, making it more difficult to fit a
ranking function. On the other hand, intra-set fea-
tures are of the same scale ([0,1] in this case) across
the two sets and therefore induce better margins.
While we have only explored one particular in-
stantiation of intra-set features, many other defini-
tions are imaginable. Novel intra-set features is a
promising research direction; experiments indicate
that they are most important in helping ranking out-
perform regression.
</bodyText>
<subsectionHeader confidence="0.998681">
3.2 Corpus-level ranking
</subsectionHeader>
<bodyText confidence="0.999998615384615">
Sentence-level evaluation generates a ranking for
each source sentence. How does one produce
an overall corpus-level ranking based on a set of
sentence-level rankings? This is known as the
“consensus ranking” or “rank aggregation” prob-
lem, which can be NP-hard under certain formula-
tions (Meil˘a et al., 2007). We use the FV heuristic
(Fligner and Verducci, 1988), which estimates the
empirical probability PZj that system i ranks above
system j from sentence-level rankings (i.e. PZj =
number of sentences where i ranks better than j, di-
vided by total number of sentences). The corpus-
level ranking of system i is then defined as Ej, PZj,.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="acknowledgments">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9991867">
For experiments, we split the provided development
data into train, dev, and test sets (see Table 2). The
data split is randomized at the level of different eval-
uation tracks (e.g. en-es.test, de-en.test are differ-
ent tracks) in order to ensure that dev/test are suffi-
ciently novel with respect to the training data. This
is important since machine learning approaches have
the risk of overfitting and spreading data from the
same track to both train and test could lead to over-
optimistic results.
</bodyText>
<table confidence="0.999147">
Train Dev Test
# tracks 8 3 3
# sets 1504 (63%) 514 (21%) 390 (16%)
# sent 6528 (58%) 2636 (23%) 2079 (19%)
</table>
<tableCaption confidence="0.996204">
Table 2: Data characteristics: the training data contains
8 tracks, which contained 6528 sentence evaluations or
1504 sets of human rankings (T = 1504).
</tableCaption>
<bodyText confidence="0.999577166666667">
In the first experiment, we compared Regression
SVM and Rank SVM (both used Features 1-12) by
training on varying amounts of training data. The
sentence-level rankings produced by each are com-
pared to human judgments using the Spearman rank
correlation coefficient (see Figure 1).
In the second experiment, we compared all rank-
ing and scoring methods discussed thus far. The full
training set is used; the dev set is used to tune the
cost parameter for the SVMs and number of itera-
tions for RankBoost, which is then applied without
modification to the test set. Table 3 shows the aver-
</bodyText>
<page confidence="0.997014">
193
</page>
<figure confidence="0.8382">
Percentage of training data
</figure>
<figureCaption confidence="0.9563636">
Figure 1: Ranking slightly outperforms Regression for
various amounts of training data. Regression results ap-
pear to be less stable, with a rise/fall in average Spear-
man coefficent around 20%, possibly because linear re-
gression functions become harder to fit with more data.
</figureCaption>
<table confidence="0.9998335">
Feature Dev Test
BLEU 1-5 .14 .05
Smoothed BLEU 1-5 .19 .24
Regression SVM 1-12 .33 .24
RankSVM 1-12 .34 .25
RankBoost 1-12 .29 .22
RankSVM 1-20 .52 .42
RankBoost 1-20 .51 .38
</table>
<tableCaption confidence="0.9657445">
Table 3: Average Spearman coefficients on Dev/Test. The
intra-set features gave the most significant gains (e.g. .42
on test of RankSVM). Refer to Table 1 to see what fea-
tures are used in each row. The SVM/RankBoost results
for features 1-12 and 1-5 are similar; only those of 1-12
are reported.
</tableCaption>
<figure confidence="0.977595923076923">
Data ablation results on Dev Set
0.36
0.34
0.32
0.3
0.28
0.26
RankSVM
RegressionSVM
0.22
0 10 20 30 40 50 60 70 80 90 100
Spearman coeff
0.24
</figure>
<bodyText confidence="0.735954333333333">
age Spearman coefficient for different methods and
different feature sets. There are several interesting
observations:
</bodyText>
<listItem confidence="0.9499982">
1. BLEU performs poorly, but SmoothedBLEU is
almost as good as the machine learning meth-
ods that use same set of basic BLEU features.
2. Rank SVM slightly outperforms RankBoost.
3. Regression SVM and Rank SVM gave simi-
</listItem>
<bodyText confidence="0.9761417">
lar results under the same feature set. How-
ever, Rank SVM gave significant improve-
ments when intra-set features are incorporated.
The last observation is particularly important: it
shows that the training criteria differences between
the ranking and regression is actually not critical.
Ranking can outperform regression, but only when
ranking-specific features are considered. Without
intra-set features, ranking methods may be suffering
the same calibration problems as regression.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99745856">
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In ACL.
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In ACL 2005 Wksp on
Intrinsic/Extrinsic Evaluation for MT/Summarization.
J. Blatz et al. 2003. Confidence estimation for machine
translation. Technical report, Johns Hopkins Univer-
sity, Natural Language Engineering Workshop.
C. Callison-Burch et al. 2007. (meta-) evaluation of ma-
chine translation. In ACL2007 SMT Workshop.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001. A
machine learning approach to the automatic evaluation
of machine translation. In ACL.
H. Drucker et al. 1996. Support vector regression ma-
chines. In NIPS.
M. Fligner and J. Verducci. 1988. Multistage ranking
models. Journal ofAmerican Statistical Assoc., 88.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003. An
efficient boosting method for combining preferences.
JMLR, 4.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In KDD.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In NAACL-HLT.
A. Kulesza and S. Shieber. 2004. A learning approach to
improving sentence-level mt evaluation. In TMI.
C.-Y. Lin and F. Och. 2004. Automatic evaluation of ma-
chine translation quality using longest common subse-
quence and skip-bigram statistics. In ACL.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In ACL 2005 Wksp on
Intrinsic/Extrinsic Evaluation for MT/Summarization.
M. Meil˘a, K. Phadnis, A. Patterson, and J. Bilmes. 2007.
Consensus ranking under the exponential model. In
Conf. on Uncertainty in Artificial Intelligence (UAI).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Conf. ofAssoc. for
Machine Translation in the Americas (AMTA-2006).
D. Vilar, G. Leusch, H. Ney, and R. Banchs. 2007. Hu-
man evaluation of machine translation through binary
system comparisons. In ACL2007 SMT Workshop.
Y. Ye, M. Zhou, and C.-Y. Lin. 2007. Sentence level
machine translation evaluation as a ranking problem.
In ACL2007 Wksp on Statistical Machine Translation.
</reference>
<page confidence="0.998658">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003384">
<title confidence="0.886328">Ranking vs. Regression in Machine Translation Evaluation</title>
<affiliation confidence="0.997519">Dept. of Electrical University of</affiliation>
<address confidence="0.983763">Seattle, WA</address>
<email confidence="0.999877">kevinduh@u.washington.edu</email>
<abstract confidence="0.98383083984375">Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology. Most automatic evaluation methods proposed to date are score-based: they compute scores that represent translation quality, and MT systems are compared on the basis of these scores. We advocate an alternative perspective of automatic MT evaluation based on ranking. Instead ofproducing scores, we directly produce a ranking over the set of MT systems to be compared. This perspective is often simpler when the evaluation goal is system comparison. We argue that it is easier to elicit human judgments of ranking and develop a machine learning approach to train on rank data. We compare this ranking method to a score-based regression method on WMT07 data. Results indicate that ranking achieves higher correlation to human judgments, especially in cases where ranking-specific features are used. 1 Motivation Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology, since automatic evaluation methods can be used to quickly determine the (approximate) quality of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. We think that this twostep process may be unnecessarily complex. Why solve a more difficult problem of predicting the quality of MT system outputs, when the goal is simply 191 of the Third Workshop on Statistical Machine pages 191–194, Ohio, USA, June 2008. Association for Computational Linguistics to compare systems? In this regard, we propose a ranking-based approach that directly ranks a set of MT systems without going through the intermediary of system-specific scores. Our approach requires (a) training data in terms of human ranking judgments of MT outputs, and (b) a machine learning algorithm learning and predicting The advantages of a ranking approach are: • It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better. Thus humanannotated data based on ranking may be less costly to acquire. • The interand intra-annotator agreement for ranking is much more reasonable than that of scoring. For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56. Thus human-annotated data based on ranking may be more reliable to use. • As mentioned earlier, when the final goal of the evaluation is comparing systems, ranking more directly solves the problem. A scoring approach essentially addresses a more difficult problem of estimating MT output quality. Nevertheless, we note that score-based approaches remain important in cases when the absolute difference between MT quality is desired. instance, one might wonder how much the top-ranked MT system outperform the secondranked system, in which case a ranking-based approach provide no guidance. In the following, Section 2 formulates the sentence-level MT evaluation problem as a ranking problem; Section 3 explains a machine learning approach for training and predicting rankings; this is our submission to the WMT2008 Shared Evaluation ranking approach is similar to Ye et. al. (2007), who was the first to advocate MT evaluation as a ranking problem. Here we focus on comparing ranking vs. scoring approaches, which was not done in previous work. task. Ranking vs. scoring approaches are compared in Section 4. 2 Formulation of the Ranking Problem We formulate the sentence-level MT evaluation as follows: Suppose there are sento be translated. Let the set of Corresponding to each source sentence, are system outputs human evaluations. The evaluations represented as label vectors a scoring approach, the elements of correspond to, e.g. a fluency score on a scale of 1 to 5. In a ranking approach, they may correspond to relative scores that are used to represent ordering (e.g. [6; 1; 3] that there are three outputs, and the first is ranked best, followed by third, then second.) In order to do machine learning, we extract feavectors each pair of set forms the training set. a scoring approach, we train a function In a ranking approach, we train that higher-ranked outputs have higher function values. In the example above, we would want: Once trained, it can be applied to rank any new data: this is done by extracting features from references/outputs and sorting by function values. 3 Implementation 3.1 Sentence-level scoring and ranking We now describe the particular scoring and ranking implementations we examined and submitted to the WMT2008 Shared Evaluation task. In the scorapproach, trained using RegressionSVM (Drucker and others, 1996); in the ranking approach, we examined RankSVM (Joachims, 2002) and RankBoost (Freund et al., 2003). We used only linear kernels for RegressionSVM and RankSVM, allowed RankBoost to produce non-linear based on a feature thresholds. we assume single reference for ease of notation; this can be easily extended for multiple reference features vectors are extracted in practice. 192 ID Description 1-4 log of ngram precision, n=1..4 5 ratio of hypothesis and reference length 6-9 ngram precision, n=1..4 10-11 hypothesis and reference length 12 BLEU 13 Smooth BLEU 14-20 Intra-set features for ID 5-9, 12,13 Table 1: Feature set: Features 1-5 can be combined (with uniform weights) to form the log(BLEU) score. Features 6-11 are redundant statistics, but scaled differently. Feature 12 is sentence-level BLEU; Feature 13 is a modified version with add-1 count to each ngram precision (this avoids prevalent zeros). Features 14-20 are only available in the ranking approach; they are derived by comparing different outputs within the same set to be ranked. The complete feature set is shown in Table 1. We restricted our feature set to traditional BLEU statistics since our experimental goal is to directly compare regression, ranking, and BLEU. Features 14- 20 are the only novel features proposed here. We wanted to examine features that are enabled by a ranking approach, but not possible for a scoring approach. We thus introduce “intra-set features”, are statistics computed by observing the enset of existing features For instance: We define Feature 14 by looking at the relative 1-gram precision (Feature 1) in the set of Feature 14 is set to value 1 for the output which has the best 1-gram precision, and value 0 otherwise. Similarly, Feature 15 is a binary variable that is 1 for the output with the best 2-gram precision, and 0 for all others. The advantage of intra-set is calibration. e.g. If the outputs for all have relatively high BLEU compared to those basic BLEU features will vary widely across the two sets, making it more difficult to fit a ranking function. On the other hand, intra-set feaare of the same scale this case) across the two sets and therefore induce better margins. While we have only explored one particular instantiation of intra-set features, many other definitions are imaginable. Novel intra-set features is a promising research direction; experiments indicate that they are most important in helping ranking outperform regression. 3.2 Corpus-level ranking Sentence-level evaluation generates a ranking for each source sentence. How does one produce an overall corpus-level ranking based on a set of sentence-level rankings? This is known as the “consensus ranking” or “rank aggregation” problem, which can be NP-hard under certain formulations (Meil˘a et al., 2007). We use the FV heuristic (Fligner and Verducci, 1988), which estimates the probability system above sentence-level rankings (i.e. of sentences where better than divided by total number of sentences). The corpusranking of system then defined as 4 Experiments For experiments, we split the provided development data into train, dev, and test sets (see Table 2). The data split is randomized at the level of different evaluation tracks (e.g. en-es.test, de-en.test are different tracks) in order to ensure that dev/test are sufficiently novel with respect to the training data. This is important since machine learning approaches have the risk of overfitting and spreading data from the same track to both train and test could lead to overoptimistic results. Train Dev Test Table 2: Data characteristics: the training data contains 8 tracks, which contained 6528 sentence evaluations or sets of human rankings In the first experiment, we compared Regression SVM and Rank SVM (both used Features 1-12) by training on varying amounts of training data. The sentence-level rankings produced by each are compared to human judgments using the Spearman rank correlation coefficient (see Figure 1). In the second experiment, we compared all ranking and scoring methods discussed thus far. The full training set is used; the dev set is used to tune the cost parameter for the SVMs and number of iterations for RankBoost, which is then applied without to the test set. Table 3 shows the aver- 193 Percentage of training data Figure 1: Ranking slightly outperforms Regression for various amounts of training data. Regression results appear to be less stable, with a rise/fall in average Spearman coefficent around 20%, possibly because linear regression functions become harder to fit with more data.</abstract>
<note confidence="0.885779222222222">Feature Dev Test BLEU 1-5 .14 .05 Smoothed BLEU 1-5 .19 .24 Regression SVM 1-12 .33 .24 RankSVM 1-12 .34 .25 RankBoost 1-12 .29 .22 RankSVM 1-20 .52 .42 RankBoost 1-20 .51 .38 Table 3: Average Spearman coefficients on Dev/Test. The</note>
<abstract confidence="0.9216872">intra-set features gave the most significant gains (e.g. .42 on test of RankSVM). Refer to Table 1 to see what features are used in each row. The SVM/RankBoost results for features 1-12 and 1-5 are similar; only those of 1-12 are reported.</abstract>
<note confidence="0.7308555">Data ablation results on Dev Set 0.36 0.34 0.32 0.3 0.28 0.26 RankSVM RegressionSVM 0.22</note>
<phone confidence="0.685074">0 10 20 30 40 50 60 70 80 90 100</phone>
<abstract confidence="0.988970685714286">Spearman coeff 0.24 age Spearman coefficient for different methods and different feature sets. There are several interesting observations: 1. BLEU performs poorly, but SmoothedBLEU is almost as good as the machine learning methods that use same set of basic BLEU features. 2. Rank SVM slightly outperforms RankBoost. 3. Regression SVM and Rank SVM gave similar results under the same feature set. However, Rank SVM gave significant improvements when intra-set features are incorporated. The last observation is particularly important: it shows that the training criteria differences between the ranking and regression is actually not critical. Ranking can outperform regression, but only when ranking-specific features are considered. Without intra-set features, ranking methods may be suffering the same calibration problems as regression. References J. Albrecht and R. Hwa. 2007. A re-examination of machine learning approaches for sentence-level MT eval- In S. Banerjee and A. Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correwith human judgments. In 2005 Wksp on Evaluation for J. Blatz et al. 2003. Confidence estimation for machine translation. Technical report, Johns Hopkins University, Natural Language Engineering Workshop. C. Callison-Burch et al. 2007. (meta-) evaluation of matranslation. In SMT S. Corston-Oliver, M. Gamon, and C. Brockett. 2001. A machine learning approach to the automatic evaluation machine translation. In Drucker et al. 1996. Support vector regression ma- In M. Fligner and J. Verducci. 1988. Multistage ranking ofAmerican Statistical 88. Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003. An efficient boosting method for combining preferences. 4. T. Joachims. 2002. Optimizing search engines using data. In D. Kauchak and R. Barzilay. 2006. Paraphrasing for evaluation. In A. Kulesza and S. Shieber. 2004. A learning approach to sentence-level mt evaluation. In Lin and F. Och. 2004. Automatic evaluation of matranslation quality using longest common subseand skip-bigram statistics. In Liu and D. Gildea. 2005. Syntactic features for evalof machine translation. In 2005 Wksp on Evaluation for M. Meil˘a, K. Phadnis, A. Patterson, and J. Bilmes. 2007. Consensus ranking under the exponential model. In on Uncertainty in Artificial Intelligence K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine In M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate targeted human annotation. In ofAssoc. for Translation in the Americas Vilar, G. Leusch, H. Ney, and R. Banchs. 2007. Human evaluation of machine translation through binary comparisons. In SMT Y. Ye, M. Zhou, and C.-Y. Lin. 2007. Sentence level machine translation evaluation as a ranking problem.</abstract>
<note confidence="0.559186">Wksp on Statistical Machine 194</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Albrecht</author>
<author>R Hwa</author>
</authors>
<title>A re-examination of machine learning approaches for sentence-level MT evaluation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2761" citStr="Albrecht and Hwa, 2007" startWordPosition="420" endWordPosition="423">U is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. We think that this twostep process may be unnecessarily complex. Why solve a more difficult problem of predicting the quality of MT system outputs, when the goal is simply 191 Proceedings of the Third Workshop on Statistical Machine Tran</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>J. Albrecht and R. Hwa. 2007. A re-examination of machine learning approaches for sentence-level MT evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL 2005 Wksp on Intrinsic/Extrinsic Evaluation for MT/Summarization.</booktitle>
<contexts>
<context position="1958" citStr="Banerjee and Lavie, 2005" startWordPosition="297" endWordPosition="300">of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU &apos;work supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) t</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL 2005 Wksp on Intrinsic/Extrinsic Evaluation for MT/Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Johns Hopkins University, Natural Language Engineering Workshop.</institution>
<marker>Blatz, 2003</marker>
<rawString>J. Blatz et al. 2003. Confidence estimation for machine translation. Technical report, Johns Hopkins University, Natural Language Engineering Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In ACL2007 SMT Workshop.</booktitle>
<contexts>
<context position="4342" citStr="Callison-Burch (2007)" startWordPosition="677" endWordPosition="678">f MT outputs, and (b) a machine learning algorithm for learning and predicting rankings.1 The advantages of a ranking approach are: • It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better. Thus humanannotated data based on ranking may be less costly to acquire. • The inter- and intra-annotator agreement for ranking is much more reasonable than that of scoring. For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56. Thus human-annotated data based on ranking may be more reliable to use. • As mentioned earlier, when the final goal of the evaluation is comparing systems, ranking more directly solves the problem. A scoring approach essentially addresses a more difficult problem of estimating MT output quality. Nevertheless, we note that score-based approaches remain important in cases when the absolute difference between MT quality is desired. For instance, one mig</context>
</contexts>
<marker>Callison-Burch, 2007</marker>
<rawString>C. Callison-Burch et al. 2007. (meta-) evaluation of machine translation. In ACL2007 SMT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>C Brockett</author>
</authors>
<title>A machine learning approach to the automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2524" citStr="Corston-Oliver et al., 2001" startWordPosition="386" endWordPosition="389"> based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. </context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>S. Corston-Oliver, M. Gamon, and C. Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
</authors>
<title>Support vector regression machines. In</title>
<date>1996</date>
<journal>Journal ofAmerican Statistical Assoc.,</journal>
<volume>88</volume>
<marker>Drucker, 1996</marker>
<rawString>H. Drucker et al. 1996. Support vector regression machines. In NIPS. M. Fligner and J. Verducci. 1988. Multistage ranking models. Journal ofAmerican Statistical Assoc., 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting method for combining preferences.</title>
<date>2003</date>
<journal>JMLR,</journal>
<volume>4</volume>
<contexts>
<context position="7251" citStr="Freund et al., 2003" startWordPosition="1171" endWordPosition="1174">function values. In the example above, we would want: f(x(n=1) t ) &gt; f(x(n=3) t ) &gt; f(x(n=2) t ). Once f is trained, it can be applied to rank any new data: this is done by extracting features from references/outputs and sorting by function values. 3 Implementation 3.1 Sentence-level scoring and ranking We now describe the particular scoring and ranking implementations we examined and submitted to the WMT2008 Shared Evaluation task. In the scoring approach, f is trained using RegressionSVM (Drucker and others, 1996); in the ranking approach, we examined RankSVM (Joachims, 2002) and RankBoost (Freund et al., 2003). We used only linear kernels for RegressionSVM and RankSVM, while allowed RankBoost to produce non-linear f based on a feature thresholds. 2Here we assume single reference for ease of notation; this can be easily extended for multiple reference 3Only Mt (not N) features vectors are extracted in practice. 192 ID Description 1-4 log of ngram precision, n=1..4 5 ratio of hypothesis and reference length 6-9 ngram precision, n=1..4 10-11 hypothesis and reference length 12 BLEU 13 Smooth BLEU 14-20 Intra-set features for ID 5-9, 12,13 Table 1: Feature set: Features 1-5 can be combined (with uniform</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003. An efficient boosting method for combining preferences. JMLR, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="7215" citStr="Joachims, 2002" startWordPosition="1167" endWordPosition="1168">her-ranked outputs have higher function values. In the example above, we would want: f(x(n=1) t ) &gt; f(x(n=3) t ) &gt; f(x(n=2) t ). Once f is trained, it can be applied to rank any new data: this is done by extracting features from references/outputs and sorting by function values. 3 Implementation 3.1 Sentence-level scoring and ranking We now describe the particular scoring and ranking implementations we examined and submitted to the WMT2008 Shared Evaluation task. In the scoring approach, f is trained using RegressionSVM (Drucker and others, 1996); in the ranking approach, we examined RankSVM (Joachims, 2002) and RankBoost (Freund et al., 2003). We used only linear kernels for RegressionSVM and RankSVM, while allowed RankBoost to produce non-linear f based on a feature thresholds. 2Here we assume single reference for ease of notation; this can be easily extended for multiple reference 3Only Mt (not N) features vectors are extracted in practice. 192 ID Description 1-4 log of ngram precision, n=1..4 5 ratio of hypothesis and reference length 6-9 ngram precision, n=1..4 10-11 hypothesis and reference length 12 BLEU 13 Smooth BLEU 14-20 Intra-set features for ID 5-9, 12,13 Table 1: Feature set: Featur</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In KDD. D. Kauchak and R. Barzilay. 2006. Paraphrasing for automatic evaluation. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>S Shieber</author>
</authors>
<title>A learning approach to improving sentence-level mt evaluation.</title>
<date>2004</date>
<booktitle>In TMI.</booktitle>
<contexts>
<context position="2556" citStr="Kulesza and Shieber, 2004" startWordPosition="391" endWordPosition="394">R (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions). In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores. All the above methods are score-based in the sense that they generate a score for each MT system output. When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores. We think that this twostep proce</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>A. Kulesza and S. Shieber. 2004. A learning approach to improving sentence-level mt evaluation. In TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1892" citStr="Lin and Och, 2004" startWordPosition="287" endWordPosition="290">can be used to quickly determine the (approximate) quality of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU &apos;work supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluatio</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In ACL 2005 Wksp on Intrinsic/Extrinsic Evaluation for</booktitle>
<contexts>
<context position="1989" citStr="Liu and Gildea, 2005" startWordPosition="303" endWordPosition="306">l for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU &apos;work supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminat</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>D. Liu and D. Gildea. 2005. Syntactic features for evaluation of machine translation. In ACL 2005 Wksp on Intrinsic/Extrinsic Evaluation for MT/Summarization. M. Meil˘a, K. Phadnis, A. Patterson, and J. Bilmes. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Consensus ranking under the exponential model.</title>
<date>2002</date>
<booktitle>In Conf. on Uncertainty in Artificial Intelligence</booktitle>
<contexts>
<context position="1686" citStr="Papineni et al., 2002" startWordPosition="253" endWordPosition="256">ranking-specific features are used. 1 Motivation Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology, since automatic evaluation methods can be used to quickly determine the (approximate) quality of MT system outputs. This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU &apos;work supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that cor</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Consensus ranking under the exponential model. In Conf. on Uncertainty in Artificial Intelligence (UAI). K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation. In</title>
<date>2006</date>
<booktitle>In Conf. ofAssoc. for Machine Translation in the Americas (AMTA-2006).</booktitle>
<contexts>
<context position="2102" citStr="Snover et al., 2006" startWordPosition="319" endWordPosition="322">tput are expensivie to obtain. Many automatic evaluation methods have been proposed to date. Successful methods such as BLEU &apos;work supported by an NSF Graduate Research Fellowship. (Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score. Methods differ by the definition of similarity. For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances. Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level. However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003). Machine learning approaches have been proposed to address the problem of sentence-level evaluation. (Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Bleu: a method for automatic evaluation of machine translation. In ACL. M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Conf. ofAssoc. for Machine Translation in the Americas (AMTA-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
<author>G Leusch</author>
<author>H Ney</author>
<author>R Banchs</author>
</authors>
<title>Human evaluation of machine translation through binary system comparisons.</title>
<date>2007</date>
<booktitle>In ACL2007 SMT Workshop.</booktitle>
<contexts>
<context position="3975" citStr="Vilar et al., 2007" startWordPosition="616" endWordPosition="619"> Translation, pages 191–194, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics to compare systems? In this regard, we propose a ranking-based approach that directly ranks a set of MT systems without going through the intermediary of system-specific scores. Our approach requires (a) training data in terms of human ranking judgments of MT outputs, and (b) a machine learning algorithm for learning and predicting rankings.1 The advantages of a ranking approach are: • It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better. Thus humanannotated data based on ranking may be less costly to acquire. • The inter- and intra-annotator agreement for ranking is much more reasonable than that of scoring. For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56. Thus human-annotated data based on ranking may be more reliable to use. • As mentioned </context>
</contexts>
<marker>Vilar, Leusch, Ney, Banchs, 2007</marker>
<rawString>D. Vilar, G. Leusch, H. Ney, and R. Banchs. 2007. Human evaluation of machine translation through binary system comparisons. In ACL2007 SMT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ye</author>
<author>M Zhou</author>
<author>C-Y Lin</author>
</authors>
<title>Sentence level machine translation evaluation as a ranking problem.</title>
<date>2007</date>
<booktitle>In ACL2007 Wksp on Statistical Machine Translation.</booktitle>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Y. Ye, M. Zhou, and C.-Y. Lin. 2007. Sentence level machine translation evaluation as a ranking problem. In ACL2007 Wksp on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>