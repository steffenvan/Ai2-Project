<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017410">
<title confidence="0.753307">
Markov Random Topic Fields
</title>
<author confidence="0.989111">
Hal Daum´e III
</author>
<affiliation confidence="0.865343333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.995452">
me@hal3.name
</email>
<sectionHeader confidence="0.994701" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882583333333">
Most approaches to topic modeling as-
sume an independence between docu-
ments that is frequently violated. We
present an topic model that makes use
of one or more user-specified graphs de-
scribing relationships between documents.
These graph are encoded in the form of a
Markov random field over topics and serve
to encourage related documents to have
similar topic structures. Experiments on
show upwards of a 10% improvement in
modeling performance.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997869375">
One often wishes to apply topic models to large
document collections. In these large collections,
we usually have meta-information about how one
document relates to another. Perhaps two docu-
ments share an author; perhaps one document cites
another; perhaps two documents are published in
the same journal or conference. We often believe
that documents related in such a way should have
similar topical structures. We encode this in a
probabilistic fashion by imposing an (undirected)
Markov random field (MRF) on top of a standard
topic model (see Section 3). The edge potentials
in the MRF encode the fact that “connected” doc-
uments should share similar topic structures, mea-
sured by some parameterized distance function.
Inference in the resulting model is complicated
by the addition of edge potentials in the MRF.
We demonstrate that a hybrid Gibbs/Metropolis-
Hastings sampler is able to efficiently explore the
posterior distribution (see Section 4).
In experiments (Section 5), we explore several
variations on our basic model. The first is to ex-
plore the importance of being able to tune the
strength of the potentials in the MRF as part of the
inference procedure. This turns out to be of utmost
importance. The second is to study the importance
of the form of the distance metric used to specify
the edge potentials. Again, this has a significant
impact on performance. Finally, we consider the
use of multiple graphs for a single model and find
that the power of combined graphs also leads to
significantly better models.
</bodyText>
<sectionHeader confidence="0.992113" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.995880142857143">
Probabilistic topic models propose that text can
be considered as a mixture of words drawn from
one or more “topics” (Deerwester et al., 1990;
Blei et al., 2003). The model we build on is la-
tent Dirichlet allocation (Blei et al., 2003) (hence-
forth, LDA). LDA stipulates the following gener-
ative model for a document collection:
</bodyText>
<listItem confidence="0.9868752">
1. For each document d = 1... D:
(a) Choose a topic mixture 0d ∼ Dir(α)
(b) For each word in d, n = 1... Nd:
i. Choose a topic zdn ∼ Mult(0d)
ii. Choose a word wdn ∼ Mult(,3zdn)
</listItem>
<bodyText confidence="0.999681">
Here, α is a hyperparameter vector of length K,
where K is the desired number of topics. Each
document has a topic distribution 0d over these
K topics and each word is associated with pre-
cisely one topic (indicated by zdn). Each topic
k = 1... K is a unigram distribution over words
(aka, a multinomial) parameterized by a vector
,3k. The associated graphical model for LDA is
shown in Figure 1. Here, we have added a few
additional hyperparameters: we place a Gam(a, b)
prior independently on each component of α and
a Dir(η, ... ,η) prior on each of the ,3s.
The joint distribution over all random variables
specified by LDA is:
</bodyText>
<equation confidence="0.993018333333333">
Yp(α, θ, z, β, w) = Gam(αk  |a, b)Dir(βk  |η) (1)
k
ri Dir(Bd  |α) Y Mult(zdn  |Bd)Mult(wdn  |βzdn)
</equation>
<bodyText confidence="0.850342666666667">
d n
Many inference methods have been developed
for this model; the approach upon which we
</bodyText>
<page confidence="0.980542">
293
</page>
<note confidence="0.965115">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 293–296,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.997862">
α 9
a
b
77
z w
0
K
N
D
</figure>
<figureCaption confidence="0.989493">
Figure 1: Graphical model for LDA.
</figureCaption>
<figure confidence="0.995835173913043">
Doc 6
e
Doc 1 Doc 2
e
z w
N
e
Doc 3
z w
N
z w
e
z w
N
N
Doc 5
Doc 4
e
z w
e
z w
N
N
</figure>
<bodyText confidence="0.9982935">
build is the collapsed Gibbs sampler (Griffiths and
Steyvers, 2006). Here, the random variables β and
θ are analytically integrated out. The main sam-
pling variables are the zdn indicators (as well as
the hyperparameters: q and a, b). The conditional
distribution for zdn conditioned on all other vari-
ables in the model gives the following Gibbs sam-
pling distribution p(zdn = k):
</bodyText>
<equation confidence="0.9808392">
#−dn
z=k,w=wdn + η
Pk&apos; (#z = wW—w
k&apos;
dn + η) (2)
</equation>
<bodyText confidence="0.97814305">
Here, #−dn
χ denotes the number of times event
χ occurs in the entire corpus, excluding word n
in document d. Intuitively, the first term is a
(smoothed) relative frequency of topic k occur-
ring; the second term is a (smoothed) relative fre-
quency of topic k giving rise to word wdn.
A Markov random field specifies a joint dis-
tribution over a collection of random variables
x1, ... , xN. An undirected graph structure stip-
ulates how the joint distribution factorizes over
these variables. Given a graph !9 = (V, E), where
V = {x1, ... , xN}, let C denote a subset of all
the cliques of !9. Then, the MRF specifies the joint
distribution as: p(x) = z jlcEC 0c(xc). Here,
Z = E. jlcEC 0c(xc) is the partition function,
xc is the subset of x contained in clique c and 0c
is any non-negative function that measures how
“good” a particular configuration of variables xc
is. The 0s are called potential functions.
</bodyText>
<sectionHeader confidence="0.974732" genericHeader="method">
3 Markov Random Topic Fields
</sectionHeader>
<bodyText confidence="0.9999881">
Suppose that we have access to a collection of
documents, but do not believe that these docu-
ments are all independent. In this case, the gener-
ative story of LDA no longer makes sense: related
documents are more likely to have “similar” topic
structures. For instance, in the scientific commu-
nity, if paper A cites paper B, we would (a priori)
expect the topic distributions for papers A and B
to be related. Similarly, if two papers share an au-
thor, we might expect them to be topically related.
</bodyText>
<figureCaption confidence="0.985955">
Figure 2: Example Markov Random Topic Field (variables
α and β are excluded for clarify).
</figureCaption>
<bodyText confidence="0.9992876">
Of if they are both published at EMNLP. Or if they
are published in the same year, or come out of the
same institution, or many other possibilities.
Regardless of the source of this notion of simi-
larity, we suppose that we can represent the rela-
tionship between documents in the form of a graph
!9 = (V, E). The vertices in this graph are the doc-
uments and the edges indicate relatedness. Note
that the resulting model will not be fully genera-
tive, but is still probabilistically well defined.
</bodyText>
<subsectionHeader confidence="0.999662">
3.1 Single Graph
</subsectionHeader>
<bodyText confidence="0.9998225">
There are multiple possibilities for augmenting
LDA with such graph structure. We could “link”
the topic distributions θ over related documents;
we could “like” the topic indicators z over related
documents. We consider the former because it
leads to a more natural model. The idea is to “un-
roll” the D-plate in the graphical model for LDA
(Figure 1) and connect (via undirected links) the
θ variables associated with connected documents.
Figure 2 shows an example MRTF over six docu-
ments, with thick edges connecting the θ variables
of “related” documents. Note that each θ still has
α as a parent and each w has β as a parent: these
are left off for figure clarity.
The model is a straightforward “integration” of
LDA and an MRF specified by the document re-
lationships !9. We begin with the joint distribution
specified by LDA (see Eq (1)) and add in edge po-
tentials for each edge in the document graph !9 that
“encourage” the topic distributions of neighboring
documents to be similar. The potentials all have
the form:
</bodyText>
<equation confidence="0.6922">
0d,d&apos;(θd, θd&apos;) = exp [−fd,d&apos;ρ(θd, θd&apos;)] (3)
</equation>
<bodyText confidence="0.9995958">
Here, fd d&apos; is a “measure of strength” of the im-
portance of the connection between d and d&apos; (and
will be inferred as part of the model). ρ is a dis-
tance metric measuring the dissimilarity between
θd and θd&apos;. For now, this is Euclidean distance
</bodyText>
<equation confidence="0.84526725">
#−dn
z=k + αk
Pk&apos;(#−dn
z=k&apos; + αk&apos;)
</equation>
<page confidence="0.977526">
294
</page>
<bodyText confidence="0.997481714285714">
(i.e., ρ(ed, ed0) = ||ed − ed0||); later, we show
that alternative distance metrics are preferable.
Adding the graph structure necessitates the ad-
dition of hyperparameters `e for every edge e ∈ E.
We place an exponential prior on each 1/`e with
parameter λ: p(`e  |λ) = λ exp(−λ/`e). Finally,
we place a vague Gam(λa, λb) prior on λ.
</bodyText>
<subsectionHeader confidence="0.999753">
3.2 Multiple Graphs
</subsectionHeader>
<bodyText confidence="0.999552714285714">
In many applications, there may be multiple
graphs that apply to the same data set, G1, ... , GJ.
In this case, we construct a single MRF based on
the union of these graph structures. Each edge now
has L-many parameters (one for each graph j) `je.
Each graph also has its own exponential prior pa-
rameter λj. Together, this yields:
</bodyText>
<equation confidence="0.981514333333333">
[ 1:
ψd,d0(ed, ed0) = exp −
j
</equation>
<bodyText confidence="0.9992955">
Here, the sum ranges only over those graphs
that have (d, d&apos;) in their edge set.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999861916666667">
Inference in MRTFs is somewhat complicated
from inference in LDA, due to the introduction
of the additional potential functions. In partic-
ular, while it is possible to analytically integrate
out e in LDA (due to multinomial/Dirichlet con-
jugacy), this is no longer possible in MRTFs. This
means that we must explicitly represent (and sam-
ple over) the topic distributions e in the MRTF.
This means that we must sample over the fol-
lowing set of variables: a, e, z, a and λ. Sam-
pling for α remains unchanged from the LDA
case. Sampling for variables except e is easy:
</bodyText>
<equation confidence="0.995617666666667">
#dn
z=k,w=wdn + η
zdn = k : θdk −dn (5)
&amp;0 (#z=k0,w=wdn + η)
( )
1/`d,d0 ∼ Exp λ + ρ(ed, ed0) (6)
( 1:
λ ∼ Gam λa + |E |, λb +
e
</equation>
<bodyText confidence="0.999956090909091">
The latter two follow from simple conjugacy.
When we use multiple graphs, we assign a sepa-
rate λ for each graph.
For sampling e, we resort to a Metropolis-
Hastings step. Our proposal distribution is the
Dirichlet posterior over e, given all the current as-
signments. The acceptance probability then just
depends on the graph distances. In particular,
once ed is drawn from the posterior Dirichlet, the
acceptance probability becomes Hd0EN(d) ψd,d0,
where N(d) denotes the neighbors of d. For each
</bodyText>
<figure confidence="0.975131">
0 200 400 600 800
# of iterations
</figure>
<figureCaption confidence="0.999879">
Figure 3: Held-out perplexity for different graphs.
</figureCaption>
<bodyText confidence="0.995691">
document, we run 10 Metropolis steps; the accep-
tance rates are roughly 25%.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999642580645161">
Our experiments are on a collection for 7441 doc-
ument abstracts crawled from CiteSeer. The crawl
was seeded with a collection of ten documents
from each of: ACL, EMNLP, SIGIR, ICML,
NIPS, UAI. This yields 650 thousand words of text
after remove stop words. We use the following
graphs (number in parens is the number of edges):
auth: shared author (47k)
book: shared booktitle/journal (227k)
cite: one cites the other (18k)
http: source file from same domain (147k)
time: published within one year (4122k)
year: published in the same year (2101k)
Other graph structures are of course possible, but
these were the most straightforward to cull.
The first thing we look at is convergence of
the samplers for the different graphs. See Fig-
ure 3. Here, we can see that the author graph and
the citation graph provide improved perplexity to
the straightforward LDA model (called “*none*”),
and that convergence occurs in a few hundred iter-
ations. Due to their size, the final two graphs led
to significantly slower inference than the first four,
so results with those graphs are incomplete.
Tuning Graph Parameters. The next item we
investigate is whether it is important to tune the
graph connectivity weights (the ` and λ variables).
It turns out this is incredibly important; see Fig-
ure 4. This is the same set of results as Figure 3,
but without ` and λ tuning. We see that the graph-
based methods do not improve over the baseline.
</bodyText>
<figure confidence="0.987001333333333">
140
130
120
110
100
90
80
auth
book
cite
http
time
*none*
year
]`j d,d0ρ(ed, ed0) (4)
)
`e (7)
perplexity
</figure>
<page confidence="0.541193">
295
</page>
<figure confidence="0.9933655">
0 200 400 600 800
# of iterations
</figure>
<figureCaption confidence="0.9978125">
Figure 4: Held-out perplexity for difference graph struc-
tures without graph parameter tuning.
</figureCaption>
<figure confidence="0.993148">
0 200 400 600 800
# of iterations
</figure>
<figureCaption confidence="0.999924">
Figure 5: Held-out perplexity for different distance metrics.
</figureCaption>
<bodyText confidence="0.995217631578947">
Distance Metric. Next, we investigate the use of
different distance metrics. We experiments with
Bhattacharyya, Hellinger, Euclidean and logistic-
Euclidean. See Figure 5 (this is just for the auth
graph). Here, we see that Bhattacharyya and
Hellinger (well motivated distances for probability
distributions) outperform the Euclidean metrics.
Using Multiple Graphs Finally, we compare
results using combinations of graphs. Here, we
run every sampler for 500 iterations and compute
standard deviations based on ten runs (year and
time are excluded). The results are in Table 1.
Here, we can see that adding graphs (almost) al-
ways helps and never hurts. By adding all the
graphs together, we are able to achieve an abso-
lute reduction in perplexity of 9 points (roughly
10%). As discussed, this hinges on the tuning of
the graph parameters to allow different graphs to
have different amounts of influence.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.995477666666667">
We have presented a graph-augmented model for
topic models and shown that a simple combined
Gibbs/MH sampler is efficient in these models.
</bodyText>
<tableCaption confidence="0.9357738">
Table 1: Comparison of held-out perplexities for vary-
ing graph structures with two standard deviation error bars;
grouped by number of graphs. Grey bars are indistinguish-
able from best model in previous group; blue bars are at least
two stddevs better; red bars are at least four stddevs better.
</tableCaption>
<bodyText confidence="0.9999313125">
Using data from the scientific domain, we have
shown that we can achieve significant reductions
in perplexity on held-out data using these mod-
els. Our model resembles recent work on hyper-
text topic models (Gruber et al., 2008; Sun et al.,
2008) and blog influence (Nallapati and Cohen,
2008), but is specifically tailored toward undi-
rected models. Ours is an alternative to the re-
cently proposed Markov Topic Models approach
(Wang et al., 2009). While the goal of these two
models is similar, the approaches differ fairly dra-
matically: we use the graph structure to inform
the per-document topic distributions; they use the
graph structure to inform the unigram models as-
sociated with each topic. It would be worthwhile
to directly compare these two approaches.
</bodyText>
<sectionHeader confidence="0.997917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9970390625">
David Blei, Andrew Ng, and Michael Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by latent semantic analysis. JASIS, 41(6).
Tom Griffiths and Mark Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to Meaning.
Amit Gruber, Michal Rosen-Zvi, , and Yair Weiss. 2008.
Latent topic models for hypertext. In UAI.
Ramesh Nallapati and William Cohen. 2008. Link-PLSA-
LDA: A new unsupervised model for topics and influence
of blogs. In Conference for Webblogs and Social Media.
Congkai Sun, Bin Gao, Zhenfu Cao, and Hang Li. 2008.
HTM: A topic model for hypertexts. In EMNLP.
Chong Wang, Bo Thiesson, Christopher Meek, and David
Blei. 2009. Markov topic models. In AI-Stats.
</reference>
<figure confidence="0.997109508474577">
140
130
120
110
100
90
80
auth
book
cite
http
*none*
time
year
perplexity
140
130
120
110
100
90
80
Bhattacharyya
Hellinger
Euclidean
Logit
92.1
92.2
90.2
88.4
87.9
*none*
http
book
cite
auth
book+http
cite+http
auth+http
book+cite
auth+book
auth+cite
book+cite+http
auth+cite+http
auth+book+http
auth+book+cite
all
87.9
85.5
85.3
83.7
83.1
89.9
88.6
88.0
86.9
85.1
84.3
perplexity
</figure>
<page confidence="0.98017">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857809">
<title confidence="0.999266">Markov Random Topic Fields</title>
<author confidence="0.975211">Hal Daum´e</author>
<affiliation confidence="0.9998395">School of Computing University of Utah</affiliation>
<address confidence="0.977878">Salt Lake City, UT 84112</address>
<email confidence="0.906096">me@hal3.name</email>
<abstract confidence="0.999241923076923">Most approaches to topic modeling assume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-specified graphs describing relationships between documents. These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures. Experiments on upwards of a in modeling performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>JMLR,</journal>
<volume>3</volume>
<contexts>
<context position="2296" citStr="Blei et al., 2003" startWordPosition="369" endWordPosition="372">o tune the strength of the potentials in the MRF as part of the inference procedure. This turns out to be of utmost importance. The second is to study the importance of the form of the distance metric used to specify the edge potentials. Again, this has a significant impact on performance. Finally, we consider the use of multiple graphs for a single model and find that the power of combined graphs also leads to significantly better models. 2 Background Probabilistic topic models propose that text can be considered as a mixture of words drawn from one or more “topics” (Deerwester et al., 1990; Blei et al., 2003). The model we build on is latent Dirichlet allocation (Blei et al., 2003) (henceforth, LDA). LDA stipulates the following generative model for a document collection: 1. For each document d = 1... D: (a) Choose a topic mixture 0d ∼ Dir(α) (b) For each word in d, n = 1... Nd: i. Choose a topic zdn ∼ Mult(0d) ii. Choose a word wdn ∼ Mult(,3zdn) Here, α is a hyperparameter vector of length K, where K is the desired number of topics. Each document has a topic distribution 0d over these K topics and each word is associated with precisely one topic (indicated by zdn). Each topic k = 1... K is a unig</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2276" citStr="Deerwester et al., 1990" startWordPosition="365" endWordPosition="368">mportance of being able to tune the strength of the potentials in the MRF as part of the inference procedure. This turns out to be of utmost importance. The second is to study the importance of the form of the distance metric used to specify the edge potentials. Again, this has a significant impact on performance. Finally, we consider the use of multiple graphs for a single model and find that the power of combined graphs also leads to significantly better models. 2 Background Probabilistic topic models propose that text can be considered as a mixture of words drawn from one or more “topics” (Deerwester et al., 1990; Blei et al., 2003). The model we build on is latent Dirichlet allocation (Blei et al., 2003) (henceforth, LDA). LDA stipulates the following generative model for a document collection: 1. For each document d = 1... D: (a) Choose a topic mixture 0d ∼ Dir(α) (b) For each word in d, n = 1... Nd: i. Choose a topic zdn ∼ Mult(0d) ii. Choose a word wdn ∼ Mult(,3zdn) Here, α is a hyperparameter vector of length K, where K is the desired number of topics. Each document has a topic distribution 0d over these K topics and each word is associated with precisely one topic (indicated by zdn). Each topic </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.</title>
<date>2006</date>
<contexts>
<context position="3800" citStr="Griffiths and Steyvers, 2006" startWordPosition="669" endWordPosition="672"> ,η) prior on each of the ,3s. The joint distribution over all random variables specified by LDA is: Yp(α, θ, z, β, w) = Gam(αk |a, b)Dir(βk |η) (1) k ri Dir(Bd |α) Y Mult(zdn |Bd)Mult(wdn |βzdn) d n Many inference methods have been developed for this model; the approach upon which we 293 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 293–296, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP α 9 a b 77 z w 0 K N D Figure 1: Graphical model for LDA. Doc 6 e Doc 1 Doc 2 e z w N e Doc 3 z w N z w e z w N N Doc 5 Doc 4 e z w e z w N N build is the collapsed Gibbs sampler (Griffiths and Steyvers, 2006). Here, the random variables β and θ are analytically integrated out. The main sampling variables are the zdn indicators (as well as the hyperparameters: q and a, b). The conditional distribution for zdn conditioned on all other variables in the model gives the following Gibbs sampling distribution p(zdn = k): #−dn z=k,w=wdn + η Pk&apos; (#z = wW—w k&apos; dn + η) (2) Here, #−dn χ denotes the number of times event χ occurs in the entire corpus, excluding word n in document d. Intuitively, the first term is a (smoothed) relative frequency of topic k occurring; the second term is a (smoothed) relative fre</context>
</contexts>
<marker>Griffiths, Steyvers, 2006</marker>
<rawString>Tom Griffiths and Mark Steyvers. 2006. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
</authors>
<title>Latent topic models for hypertext.</title>
<date>2008</date>
<booktitle>In UAI.</booktitle>
<marker>Gruber, Rosen-Zvi, 2008</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, , and Yair Weiss. 2008. Latent topic models for hypertext. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
<author>William Cohen</author>
</authors>
<title>Link-PLSALDA: A new unsupervised model for topics and influence of blogs.</title>
<date>2008</date>
<booktitle>In Conference for Webblogs and Social Media.</booktitle>
<marker>Nallapati, Cohen, 2008</marker>
<rawString>Ramesh Nallapati and William Cohen. 2008. Link-PLSALDA: A new unsupervised model for topics and influence of blogs. In Conference for Webblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congkai Sun</author>
<author>Bin Gao</author>
<author>Zhenfu Cao</author>
<author>Hang Li</author>
</authors>
<title>HTM: A topic model for hypertexts.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Sun, Gao, Cao, Li, 2008</marker>
<rawString>Congkai Sun, Bin Gao, Zhenfu Cao, and Hang Li. 2008. HTM: A topic model for hypertexts. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>Bo Thiesson</author>
<author>Christopher Meek</author>
<author>David Blei</author>
</authors>
<title>Markov topic models. In AI-Stats.</title>
<date>2009</date>
<marker>Wang, Thiesson, Meek, Blei, 2009</marker>
<rawString>Chong Wang, Bo Thiesson, Christopher Meek, and David Blei. 2009. Markov topic models. In AI-Stats.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>