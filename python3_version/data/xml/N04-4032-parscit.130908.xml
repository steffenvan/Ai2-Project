<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005892">
<title confidence="0.986403">
Parsing Conversational Speech Using Enhanced Segmentation
</title>
<author confidence="0.904196">
Jeremy G. Kahn and Mari Ostendorf
</author>
<affiliation confidence="0.946115">
SSLI, University of Washington, EE
</affiliation>
<email confidence="0.97104">
jgk,mo @ssli.ee.washington.edu
</email>
<author confidence="0.880817">
Ciprian Chelba
</author>
<affiliation confidence="0.831827">
Microsoft Research
</affiliation>
<email confidence="0.972655">
chelba@microsoft.com
</email>
<sectionHeader confidence="0.993325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9959202">
The lack of sentence boundaries and presence of dis-
fluencies pose difficulties for parsing conversational
speech. This work investigates the effects of au-
tomatically detecting these phenomena on a proba-
bilistic parser’s performance. We demonstrate that a
state-of-the-art segmenter, relative to a pause-based
segmenter, gives more than 45% of the possible er-
ror reduction in parser performance, and that presen-
tation of interruption points to the parser improves
performance over using sentence boundaries alone.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928322580645">
Parsing speech can be useful for a number of tasks, in-
cluding information extraction and question answering
from audio transcripts. However, parsing conversational
speech presents a different set of challenges than parsing
text: sentence boundaries are not well-defined, punctua-
tion is absent, and disfluencies (edits and restarts) impact
the structure of language.
Several efforts have looked at detecting sentence
boundaries in speech, e.g. (Kim and Woodland, 2001;
Huang and Zweig, 2002). Metadata extraction efforts,
like (Liu et al., 2003), extend this task to include iden-
tifying self-interruption points (IPs) that indicate a dis-
fluency or restart. This paper explores the usefulness of
identifying boundaries of sentence-like units (referred to
as SUs) and IPs in parsing conversational speech.
Early work in parsing conversational speech was rule-
based and limited in domain (Mayfield et al., 1995). Re-
sults from another rule-based system (Core and Schu-
bert, 1999) suggests that standard parsers can be used to
identify speech repairs in conversational speech. Work
in statistically parsing conversational speech (Charniak
and Johnson, 2001) has examined the performance of a
parser that removes edit regions in an earlier step. In con-
trast, we train a parser on the complete (human-specified)
segmentation, with edit-regions included. We choose to
work with all of the words within edit regions anticipating
that making the parallel syntactic structures of the edit re-
gion available to the parser can improve its performance
in identifying that structure. Our work makes use of the
Structured Language Model (SLM) as a parser and an ex-
isting SU-IP detection algorithm, described next.
</bodyText>
<sectionHeader confidence="0.994768" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997729">
2.1 Structured Language Model
</subsectionHeader>
<bodyText confidence="0.926626133333333">
The SLM assigns a probability to every sen-
tence and its every possible binary parse . The
terminals of are the words of with POS tags, and
the nodes of are annotated with phrase headwords and
non-terminal labels. Let be a sentence of length
words with added sentence boundary markers &lt;s&gt;
and &lt;/s&gt;. Let be the word
-prefix of the sentence — the words from the begin-
ning of the sentence up to the current position —
and the word-parse -prefix. Figure 1 shows a
word-parse -prefix; h_0 .. h_{-m} are the exposed
heads, each head being a pair (headword, non-terminal
label), or (word, POS tag) in the case of a root-only tree.
The exposed heads at a given position in the input sen-
tence are a function of the word-parse -prefix.
</bodyText>
<equation confidence="0.99529">
h_{-1} h_0 = (h_0.word, h_0.tag)
(&lt;s&gt;, SB) (w_p, t_p) (w_{p+1}, t_{p+1}) (w_k, t_k) w_{k+1}.... &lt;/s&gt;
</equation>
<figureCaption confidence="0.999638">
Figure 1: A word-parse -prefix
</figureCaption>
<bodyText confidence="0.993289">
The joint probability of a word sequence
and a complete parse can be broken into:
</bodyText>
<equation confidence="0.884677">
(1)
</equation>
<bodyText confidence="0.7464705">
where:
is the word-parse -prefix
is the word predicted by the WORD-PREDICTOR
is the tag assigned to by the TAGGER
is the number of operations the CONSTRUC-
TOR executes at sentence position before passing con-
trol to the WORD-PREDICTOR (the -th operation at
position is the null transition); is a function of
denotes the-th CONSTRUCTOR operation carried
out at position in the word string; the operations per-
formed by the CONSTRUCTOR are illustrated in Fig-
ures 2-3 and they ensure that all possible binary branch-
ing parses, with all possible headword and non-terminal
label assignments for the word sequence, can
</bodyText>
<equation confidence="0.9964685">
h_{-m} = (&lt;s&gt;, SB)
h’_{-1} = h_{-2} h’_0 = (h_{-1}.word, NTlabel)
</equation>
<figureCaption confidence="0.9999705">
Figure 2: Result of adjoin-left under NT label
Figure 3: Result of adjoin-right under NT label
</figureCaption>
<bodyText confidence="0.985327791666666">
be generated. The sequence of CONSTRUC-
TOR operations at position grows the word-parse
-prefix into a word-parse -prefix.
The SLM is based on three probabilities, each esti-
mated using deleted interpolation and parameterized (ap-
proximated) as follows:
Since the number of parses for a given word prefix
grows exponentially with ,, the state
space of our model is huge even for relatively short sen-
tences, so the search strategy uses pruning.
Each model component is initialized from a set of
parsed sentences after undergoing headword percolation
and binarization. The position of the headword within a
constituent is specified using a rule-based approach. As-
suming the index of the headword on the right-hand side
of the rule is , we binarize the constituent by following
one of the two binarization schemes in Figure 4. Inter-
mediate nodes created receive the label 1. The choice
between the two schemes is made according to the iden-
tity of the label on the left-hand-side of a rewrite rule.
An N-best EM variant is employed to jointly reestimate
the model parameters so that perplexity on training data
is decreased, i.e. increasing likelihood. Experimentally,
the reduction in perplexity carries over to the test set.
</bodyText>
<footnote confidence="0.489006">
1Any resemblance to X-bar theory is purely coincidental.
Y_1 Y_k Y_n Y_1 Y_k Y_n
</footnote>
<bodyText confidence="0.969154">
The SLM can be used for parsing either as a generative
model or as a conditional model . In
the latter case, the prediction is omit-
ted in Eq. (1). For further details on the SLM, see (Chelba
and Jelinek, 2000).
</bodyText>
<subsectionHeader confidence="0.952042">
2.2 SU and IP Detection
</subsectionHeader>
<bodyText confidence="0.999950074074074">
The system used here for SU and IP detection is (Kim
et al., 2004), modulo differences in training data. It
combines decision tree models of prosody with a hidden
event language model in a hidden Markov model (HMM)
framework for detecting events at each word boundary,
similar to (Liu et al., 2003). Differences include the use
of lexical pattern matching features (sequential matching
words or POS tags) as well as prosody cues in the de-
cision tree, and having a joint representation of SU and
IP boundary events rather than separate detectors. On
the DARPA RT-03F metadata test set (NIST, 2003), the
model has 35.0% slot error rate (SER) for SUs (75.7%
recall, 87.7% precision), and 68.8% SER for edit IPs
(41.8% recall, 79.8% precision) on reference transcripts,
using the rt eval scoring tool.2 While these error rates
are relatively high, it is a difficult task and the SU perfor-
mance is at the state of the art.
Since early work on “sentence” segmentation simply
looked at pause duration, we designed a decision tree
classifier to predict SU events based only on the pause
duration after a word boundary. This model served as a
baseline condition, referred to here as the “naive” predic-
tor since it makes no use of other prosodic or lexical cues
that are important for preventing IPs or hesitations from
triggering false SU detection. The naive predictor has
SU SER of 68.8%, roughly twice that of the HMM, with
a large loss in recall (43.2% recall, 79.0% precision).
</bodyText>
<sectionHeader confidence="0.997677" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999933142857143">
The data used in this work is the treebank (TB3) portion
of the Switchboard corpus of conversational telephone
speech, which includes sentence boundaries as well as
the reparandum and interruption point of disfluencies.
The data consists of 816 hand-transcribed conversation
sides (566K words), of which we reserve 128 conversa-
tion sides (61K words) for evaluation testing according to
the 1993 NIST evaluation choices.
We use a subset of Switchboard data – hand-annotated
for SUs and IPs – for training the SU/IP boundary event
detector, and for providing the oracle versions of these
events as a control in our experiments. The annotation
conventions for this data, referred to as V5 (Strassel,
2003), are slightly different from that used in the TB3 an-
</bodyText>
<footnote confidence="0.780509666666667">
2Note that the IP performance figures are not comparable to
those in the DARPA evaluation, since we restrict the focus to
IPs associated with edit disfluencies.
</footnote>
<figure confidence="0.989342043478261">
h_{-1} h_0
T’_0
T’_{-m+1}&lt;-&lt;s&gt;
............... T’_{-1}&lt;-T_{-2}
&lt;s&gt;
T_{-1} T_0
h’_{-1}=h_{-2}
h’_0 = (h_0.word, NTlabel)
h_0
T’_{-m+1}&lt;-&lt;s&gt;
............... T’_{-1}&lt;-T_{-2} T_{-1} T_0
h_{-1}
&lt;s&gt;
Z
A
Z’
Z
B
Z’
Z’
Z’
Z’
Z’
</figure>
<figureCaption confidence="0.999982">
Figure 4: Binarization schemes
</figureCaption>
<bodyText confidence="0.999960962962963">
notations in a few important ways. Notably for this work,
V5 annotates IPs for both conversational fillers (such as
filled pauses and discourse markers) and self-edit disflu-
encies, while TB3 represents only edit-related IPs. This
difference is addressed by explicitly distinguishing be-
tween these types in the IP detection model. In addition,
the V5 conventions define an SU as including only one
independent main clause, so the size of the “segments”
available for parsing is sometimes smaller than in TB3.
Further, the SU boundaries were determined by annota-
tors who actually listened to the speech signal, vs. anno-
tated from text alone as for TB3. One consequence of the
differences is a small amount of additional error due to
train/test mismatch. More importantly, the “ground truth”
for the syntactic structure must be mapped to the SU seg-
mentation, both for training and test.
In many cases, the original syntactic constituents span
multiple SUs, but we follow a simple rule in generat-
ing this new SU-based truth: only those constituents
contained entirely within an SU would be retained. In
practice, this means eliminating a few high-level con-
stituents. The effect is usually to change the interpreta-
tion of some sentence-level conjunctions to be discourse
markers, rather than conjoining two main clauses. This
change is arguably an improvement, since the SU anno-
tation relies on a human annotation that takes into con-
sideration acoustic information (not only the words).
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999785">
In all experiments, the SLM parser was trained on the
baseline truth Switchboard corpus described above, with
hand-annotated SUs and optionally IPs. For testing, the
system was presented with conversation sides segmented
according to the various SU-predictions, and evaluated on
its performance in predicting the true syntactic structure.
</bodyText>
<subsectionHeader confidence="0.981191">
4.1 Experimental Variables
</subsectionHeader>
<bodyText confidence="0.999944057142857">
We seek to explore how much impact current metadata
detection algorithms have over the naive pause-based
segmentation. To this end, we test along two experimen-
tal dimensions: SU segmentation and IP detection.
Some type of segmentation is critical to most parsers.
In the SU dimension, we tested three conditions. Across
these conditions, the parser training was held constant,
but the test segmentation varied across three cases: (i) or-
acle, hand-labeled SU segmentation; (ii) automatic, SU
segmentation from the automatic detection system using
both prosody and lexical cues (Kim et al., 2004); and (iii)
naive, SU segmentation from a decision tree predictor us-
ing only pause duration cues. The SUs are included as
words, similar to sentence boundaries in prior SLM work.
By varying the SU segmentation of the test data for our
system, we gain insight into how the performance of SU
detection changes the overall accuracy of the parser.
We expect interruption points to be useful to pars-
ing, since edit points often indicate a restart point, and
the preceding syntactic phrase should attach to the tree
differently. In the IP dimension, we examined two con-
ditions (present and absent). For each condition, we re-
trained the parser including hand-labeled IPs, since the
vocabulary of available “words” is different when the IP
is included as an input token. The two IP conditions are:
(a) No IP, training the parser on syntax that did not in-
clude IPs as words, and testing on segmented input that
also did not include IP tokens; and (b) IP, training and
testing on input that includes IPs as words. The incorpo-
ration of IPs as words may not be ideal, since it reduces
the number of true words available to an N-gram model at
a given point, but it has the advantages of simplicity and
consistency with SU treatment. Because the naive system
does not predict IPs, we only have experiments for 5 of
the 6 possible combinations.
</bodyText>
<subsectionHeader confidence="0.961001">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999947">
We evaluated parser performance by using bracket preci-
sion and recall scores, as well as bracket-crossing, using
the parseval metric (Sekine and Collins, 1997; Black et
al., 1991). This bracket-counting metric for parsers, re-
quires that the input words (and, by implication, sen-
tences) be held constant across test conditions. Since
our experiments deliberately vary the segmentation, we
needed to evaluate each conversation side as a single
“sentence” in order to obtain meaningful results across
different segmentations. We construct this top-level sen-
tence by attaching the parser’s proposed constituents for
each SU to a new top-level constituent (labeled TIPTOP).
Thus, we can compare two different segmentations of the
same data, because it ensures that the segmentations will
agree at least at the beginning and end of the conversa-
tion. Segmentation errors will of course cause some mis-
matches, but that possibility is what we are investigating.
For evaluation, we ignore the TIPTOP bracket (which
always contains the entire conversation side), so this tech-
nique does not interfere with accurate bracket counting,
but allows segmentation errors to be evaluated at the level
of bracket-counting. The SLM parser uses binary trees,
but the syntactic structures we are given as truth often
branch in N-ary ways, where . The parse trees
used for training the SLM use bar-level nodes to trans-
form N-ary trees into binary ones; the reverse mapping
of SLM-produced binary trees back to N-ary trees is done
by simply removing the bar-level constituents. Finally, to
compare the IP-present conditions with the non-IP condi-
tions, we ignore IP tokens when counting brackets.
</bodyText>
<subsectionHeader confidence="0.794517">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.848494">
Table 1 shows the parser performance: average bracket-
crossing (lower is better), precision and recall (higher is
</bodyText>
<table confidence="0.999294">
Avg. crossing Oracle+P Oracle Auto Naive
No IP 35.88 46.80 66.21 80.40
IP 35.03 44.56 58.09 —
%Recall Oracle+P Oracle Auto Naive
No IP 69.91 77.45 72.47 68.05
IP 70.36 77.98 74.25 —
%Precision Oracle+P Oracle Auto Naive
No IP 79.30 68.40 63.09 58.67
IP 79.65 68.42 64.46 —
</table>
<tableCaption confidence="0.999935">
Table 1: Bracket crossing, precision and recall results.
</tableCaption>
<bodyText confidence="0.999975206896552">
better). The number of bracket-crossings per “sentence”
is quite high, due to evaluating all text from a given con-
versation side into one “TIPTOP sentence”. Precision
and recall are regarding the bracketing of all the tokens
under consideration (i.e., not including bar-level brack-
ets, and not including IP token labeling). All differences
are highly significant ( according to a sign test
at conversation level) except for comparing oracle results
with and without IPs.
We find that the HMM-based SU detection system
achieves a 7% improvement in precision and recall over
the naive pause-based system, and an 18% reduction in
average bracket crossing. Further, the use of IPs as input
tokens improves parser performance, especially when the
segmentation is imperfect. While segmentation has an
impact on parsing, it is not the limiting factor: the best
possible bracketing respecting the automatic segmenta-
tion has a 96.50% recall and 99.35% precision.
Adding punctuation to the oracle case (Oracle+P) im-
proves performance, as seen more clearly with the F-
measure because of changes in the precision-recall bal-
ance. The F-measures goes from 72.6 for oracle/no-IP to
74.3 for oracle+P/no-IP to 74.7 for oracle+P/IP. The fact
that punctuation is useful on top of the oracle segmen-
tation suggests that a richer representation of structural
metadata would be beneficial. The reasons why IPs do
not have much of an impact in the oracle case are not
clear – it could be a modeling issue or it could be simply
that the IPs add robustness to the automatic segmentation.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999980409090909">
In comparison to the naive pause-based SU detector, us-
ing an SU detector based on prosody and lexical cues
gives us more than 45% of the possible gain to the best
possible (oracle) case, despite a relatively high SU er-
ror rate. We hypothesize that low SU and IP recall in
the naive segmenter created a much larger parse search
space, leading to more opportunities for errors. The im-
provement is promising, and suggests that research to im-
prove metadata extraction can have a direct impact on the
performance of other natural language applications that
deal with conversational speech.
The use of SUs and IPs as input words may result in
a loss of information, reducing the “true” word history
available to the parser component models. Further re-
search using the structured language model could incor-
porate these metadata directly into the model, allowing it
to take advantage of higher-level metadata without reduc-
ing the effective number of words available to the model.
In addition, just as the SLM is useful for both parsing and
language modeling, it could be used to predict metadata
for its own sake or to improve word recognition, with or
without the word-based representation.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995722">
We thank J. Kim for providing the SU-IP detection results, us-
ing tools developed under DARPA grant MDA904-02-C-0437.
This work is supported by NSF grant no. IIS085940. Any opin-
ions or conclusions expressed in this paper are those of the au-
thors and do not necessarily reflect the views of these agencies.
</bodyText>
<sectionHeader confidence="0.999078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984675862069">
E. Black et al. 1991. A procedure for quantitatively compar-
ing syntactic coverage of English grammars. In Proc. 4th
DARPA Speech &amp; Natural Lang. Workshop, pages 306–311.
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proc. 2nd NAACL, pages 118–126.
C. Chelba and F. Jelinek. 2000. Structured language modeling.
Computer Speech and Language, 14(4):283–332, October.
M. Core and K. Schubert. 1999. Speech repairs: A parsing
perspective. In Satellite Meeting JCPHS 99.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proc. Eurospeech.
J.-H. Kim and P. Woodland. 2001. The use of prosody in
a combined system for punctuation generation and speech
recognition. In Proc. Eurospeech, pages 2757–2760.
J. Kim, S. E. Schwarm, and M. Ostendorf. 2004. Detecting
structural metadata with decision trees and transformation-
based learning. In Proc. HLT-NAACL.
Y. Liu, E. Shriberg, and A. Stolcke. 2003. Automatic disflu-
ency identification in conversational speech using multiple
knowledge sources. In Proc. Eurospeech, volume 1, pages
957–960.
L. Mayfield et al. 1995. Parsing real input in JANUS: a
concept-based approach. In Proc. TMJ 95.
NIST. 2003. Rich Transcription Fall 2003 Evaluation Results.
http://www.nist.gov/speech/tests/rt/rt2003/fall/.
S. Sekine and M. Collins. 1997. EVALB. As in Collins ACL
1997;http://nlp.cs.nyu.edu/evalb/.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717524">
<title confidence="0.999988">Parsing Conversational Speech Using Enhanced Segmentation</title>
<author confidence="0.999541">G Kahn Ostendorf</author>
<affiliation confidence="0.961078">SSLI, University of Washington, EE</affiliation>
<email confidence="0.957705">jgk,mo@ssli.ee.washington.edu</email>
<author confidence="0.794992">Ciprian</author>
<affiliation confidence="0.994619">Microsoft Research</affiliation>
<email confidence="0.998423">chelba@microsoft.com</email>
<abstract confidence="0.997956636363636">The lack of sentence boundaries and presence of disfluencies pose difficulties for parsing conversational speech. This work investigates the effects of automatically detecting these phenomena on a probabilistic parser’s performance. We demonstrate that a state-of-the-art segmenter, relative to a pause-based segmenter, gives more than 45% of the possible error reduction in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>A procedure for quantitatively comparing syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proc. 4th DARPA Speech &amp; Natural Lang. Workshop,</booktitle>
<pages>306--311</pages>
<marker>Black, 1991</marker>
<rawString>E. Black et al. 1991. A procedure for quantitatively comparing syntactic coverage of English grammars. In Proc. 4th DARPA Speech &amp; Natural Lang. Workshop, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proc. 2nd NAACL,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="1899" citStr="Charniak and Johnson, 2001" startWordPosition="266" endWordPosition="269">ike (Liu et al., 2003), extend this task to include identifying self-interruption points (IPs) that indicate a disfluency or restart. This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech. Early work in parsing conversational speech was rulebased and limited in domain (Mayfield et al., 1995). Results from another rule-based system (Core and Schubert, 1999) suggests that standard parsers can be used to identify speech repairs in conversational speech. Work in statistically parsing conversational speech (Charniak and Johnson, 2001) has examined the performance of a parser that removes edit regions in an earlier step. In contrast, we train a parser on the complete (human-specified) segmentation, with edit-regions included. We choose to work with all of the words within edit regions anticipating that making the parallel syntactic structures of the edit region available to the parser can improve its performance in identifying that structure. Our work makes use of the Structured Language Model (SLM) as a parser and an existing SU-IP detection algorithm, described next. 2 Background 2.1 Structured Language Model The SLM assi</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>E. Charniak and M. Johnson. 2001. Edit detection and parsing for transcribed speech. In Proc. 2nd NAACL, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<booktitle>In Satellite Meeting JCPHS 99.</booktitle>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="5710" citStr="Chelba and Jelinek, 2000" startWordPosition="908" endWordPosition="911">en the two schemes is made according to the identity of the label on the left-hand-side of a rewrite rule. An N-best EM variant is employed to jointly reestimate the model parameters so that perplexity on training data is decreased, i.e. increasing likelihood. Experimentally, the reduction in perplexity carries over to the test set. 1Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n The SLM can be used for parsing either as a generative model or as a conditional model . In the latter case, the prediction is omitted in Eq. (1). For further details on the SLM, see (Chelba and Jelinek, 2000). 2.2 SU and IP Detection The system used here for SU and IP detection is (Kim et al., 2004), modulo differences in training data. It combines decision tree models of prosody with a hidden event language model in a hidden Markov model (HMM) framework for detecting events at each word boundary, similar to (Liu et al., 2003). Differences include the use of lexical pattern matching features (sequential matching words or POS tags) as well as prosody cues in the decision tree, and having a joint representation of SU and IP boundary events rather than separate detectors. On the DARPA RT-03F metadata</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>C. Chelba and F. Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332, October. M. Core and K. Schubert. 1999. Speech repairs: A parsing perspective. In Satellite Meeting JCPHS 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>G Zweig</author>
</authors>
<title>Maximum entropy model for punctuation annotation from speech.</title>
<date>2002</date>
<booktitle>In Proc. Eurospeech.</booktitle>
<contexts>
<context position="1240" citStr="Huang and Zweig, 2002" startWordPosition="168" endWordPosition="171">nd that presentation of interruption points to the parser improves performance over using sentence boundaries alone. 1 Introduction Parsing speech can be useful for a number of tasks, including information extraction and question answering from audio transcripts. However, parsing conversational speech presents a different set of challenges than parsing text: sentence boundaries are not well-defined, punctuation is absent, and disfluencies (edits and restarts) impact the structure of language. Several efforts have looked at detecting sentence boundaries in speech, e.g. (Kim and Woodland, 2001; Huang and Zweig, 2002). Metadata extraction efforts, like (Liu et al., 2003), extend this task to include identifying self-interruption points (IPs) that indicate a disfluency or restart. This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech. Early work in parsing conversational speech was rulebased and limited in domain (Mayfield et al., 1995). Results from another rule-based system (Core and Schubert, 1999) suggests that standard parsers can be used to identify speech repairs in conversational speech. Work in statistically</context>
</contexts>
<marker>Huang, Zweig, 2002</marker>
<rawString>J. Huang and G. Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-H Kim</author>
<author>P Woodland</author>
</authors>
<title>The use of prosody in a combined system for punctuation generation and speech recognition.</title>
<date>2001</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>2757--2760</pages>
<contexts>
<context position="1216" citStr="Kim and Woodland, 2001" startWordPosition="164" endWordPosition="167">in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone. 1 Introduction Parsing speech can be useful for a number of tasks, including information extraction and question answering from audio transcripts. However, parsing conversational speech presents a different set of challenges than parsing text: sentence boundaries are not well-defined, punctuation is absent, and disfluencies (edits and restarts) impact the structure of language. Several efforts have looked at detecting sentence boundaries in speech, e.g. (Kim and Woodland, 2001; Huang and Zweig, 2002). Metadata extraction efforts, like (Liu et al., 2003), extend this task to include identifying self-interruption points (IPs) that indicate a disfluency or restart. This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech. Early work in parsing conversational speech was rulebased and limited in domain (Mayfield et al., 1995). Results from another rule-based system (Core and Schubert, 1999) suggests that standard parsers can be used to identify speech repairs in conversational speec</context>
</contexts>
<marker>Kim, Woodland, 2001</marker>
<rawString>J.-H. Kim and P. Woodland. 2001. The use of prosody in a combined system for punctuation generation and speech recognition. In Proc. Eurospeech, pages 2757–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>S E Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Detecting structural metadata with decision trees and transformationbased learning.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="5802" citStr="Kim et al., 2004" startWordPosition="927" endWordPosition="930"> rule. An N-best EM variant is employed to jointly reestimate the model parameters so that perplexity on training data is decreased, i.e. increasing likelihood. Experimentally, the reduction in perplexity carries over to the test set. 1Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n The SLM can be used for parsing either as a generative model or as a conditional model . In the latter case, the prediction is omitted in Eq. (1). For further details on the SLM, see (Chelba and Jelinek, 2000). 2.2 SU and IP Detection The system used here for SU and IP detection is (Kim et al., 2004), modulo differences in training data. It combines decision tree models of prosody with a hidden event language model in a hidden Markov model (HMM) framework for detecting events at each word boundary, similar to (Liu et al., 2003). Differences include the use of lexical pattern matching features (sequential matching words or POS tags) as well as prosody cues in the decision tree, and having a joint representation of SU and IP boundary events rather than separate detectors. On the DARPA RT-03F metadata test set (NIST, 2003), the model has 35.0% slot error rate (SER) for SUs (75.7% recall, 87.</context>
<context position="10841" citStr="Kim et al., 2004" startWordPosition="1731" endWordPosition="1734">Experimental Variables We seek to explore how much impact current metadata detection algorithms have over the naive pause-based segmentation. To this end, we test along two experimental dimensions: SU segmentation and IP detection. Some type of segmentation is critical to most parsers. In the SU dimension, we tested three conditions. Across these conditions, the parser training was held constant, but the test segmentation varied across three cases: (i) oracle, hand-labeled SU segmentation; (ii) automatic, SU segmentation from the automatic detection system using both prosody and lexical cues (Kim et al., 2004); and (iii) naive, SU segmentation from a decision tree predictor using only pause duration cues. The SUs are included as words, similar to sentence boundaries in prior SLM work. By varying the SU segmentation of the test data for our system, we gain insight into how the performance of SU detection changes the overall accuracy of the parser. We expect interruption points to be useful to parsing, since edit points often indicate a restart point, and the preceding syntactic phrase should attach to the tree differently. In the IP dimension, we examined two conditions (present and absent). For eac</context>
</contexts>
<marker>Kim, Schwarm, Ostendorf, 2004</marker>
<rawString>J. Kim, S. E. Schwarm, and M. Ostendorf. 2004. Detecting structural metadata with decision trees and transformationbased learning. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Automatic disfluency identification in conversational speech using multiple knowledge sources.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<volume>1</volume>
<pages>957--960</pages>
<contexts>
<context position="1294" citStr="Liu et al., 2003" startWordPosition="176" endWordPosition="179">mproves performance over using sentence boundaries alone. 1 Introduction Parsing speech can be useful for a number of tasks, including information extraction and question answering from audio transcripts. However, parsing conversational speech presents a different set of challenges than parsing text: sentence boundaries are not well-defined, punctuation is absent, and disfluencies (edits and restarts) impact the structure of language. Several efforts have looked at detecting sentence boundaries in speech, e.g. (Kim and Woodland, 2001; Huang and Zweig, 2002). Metadata extraction efforts, like (Liu et al., 2003), extend this task to include identifying self-interruption points (IPs) that indicate a disfluency or restart. This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech. Early work in parsing conversational speech was rulebased and limited in domain (Mayfield et al., 1995). Results from another rule-based system (Core and Schubert, 1999) suggests that standard parsers can be used to identify speech repairs in conversational speech. Work in statistically parsing conversational speech (Charniak and Johnson, </context>
<context position="6034" citStr="Liu et al., 2003" startWordPosition="965" endWordPosition="968">t. 1Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n The SLM can be used for parsing either as a generative model or as a conditional model . In the latter case, the prediction is omitted in Eq. (1). For further details on the SLM, see (Chelba and Jelinek, 2000). 2.2 SU and IP Detection The system used here for SU and IP detection is (Kim et al., 2004), modulo differences in training data. It combines decision tree models of prosody with a hidden event language model in a hidden Markov model (HMM) framework for detecting events at each word boundary, similar to (Liu et al., 2003). Differences include the use of lexical pattern matching features (sequential matching words or POS tags) as well as prosody cues in the decision tree, and having a joint representation of SU and IP boundary events rather than separate detectors. On the DARPA RT-03F metadata test set (NIST, 2003), the model has 35.0% slot error rate (SER) for SUs (75.7% recall, 87.7% precision), and 68.8% SER for edit IPs (41.8% recall, 79.8% precision) on reference transcripts, using the rt eval scoring tool.2 While these error rates are relatively high, it is a difficult task and the SU performance is at th</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, 2003</marker>
<rawString>Y. Liu, E. Shriberg, and A. Stolcke. 2003. Automatic disfluency identification in conversational speech using multiple knowledge sources. In Proc. Eurospeech, volume 1, pages 957–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mayfield</author>
</authors>
<title>Parsing real input in JANUS: a concept-based approach.</title>
<date>1995</date>
<booktitle>In Proc. TMJ 95.</booktitle>
<marker>Mayfield, 1995</marker>
<rawString>L. Mayfield et al. 1995. Parsing real input in JANUS: a concept-based approach. In Proc. TMJ 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Rich Transcription Fall</title>
<date>2003</date>
<note>http://www.nist.gov/speech/tests/rt/rt2003/fall/.</note>
<contexts>
<context position="6332" citStr="NIST, 2003" startWordPosition="1016" endWordPosition="1017">d IP Detection The system used here for SU and IP detection is (Kim et al., 2004), modulo differences in training data. It combines decision tree models of prosody with a hidden event language model in a hidden Markov model (HMM) framework for detecting events at each word boundary, similar to (Liu et al., 2003). Differences include the use of lexical pattern matching features (sequential matching words or POS tags) as well as prosody cues in the decision tree, and having a joint representation of SU and IP boundary events rather than separate detectors. On the DARPA RT-03F metadata test set (NIST, 2003), the model has 35.0% slot error rate (SER) for SUs (75.7% recall, 87.7% precision), and 68.8% SER for edit IPs (41.8% recall, 79.8% precision) on reference transcripts, using the rt eval scoring tool.2 While these error rates are relatively high, it is a difficult task and the SU performance is at the state of the art. Since early work on “sentence” segmentation simply looked at pause duration, we designed a decision tree classifier to predict SU events based only on the pause duration after a word boundary. This model served as a baseline condition, referred to here as the “naive” predictor </context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. Rich Transcription Fall 2003 Evaluation Results. http://www.nist.gov/speech/tests/rt/rt2003/fall/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>M Collins</author>
</authors>
<date>1997</date>
<booktitle>EVALB. As in Collins ACL 1997;http://nlp.cs.nyu.edu/evalb/.</booktitle>
<contexts>
<context position="12344" citStr="Sekine and Collins, 1997" startWordPosition="1990" endWordPosition="1993">n segmented input that also did not include IP tokens; and (b) IP, training and testing on input that includes IPs as words. The incorporation of IPs as words may not be ideal, since it reduces the number of true words available to an N-gram model at a given point, but it has the advantages of simplicity and consistency with SU treatment. Because the naive system does not predict IPs, we only have experiments for 5 of the 6 possible combinations. 4.2 Evaluation We evaluated parser performance by using bracket precision and recall scores, as well as bracket-crossing, using the parseval metric (Sekine and Collins, 1997; Black et al., 1991). This bracket-counting metric for parsers, requires that the input words (and, by implication, sentences) be held constant across test conditions. Since our experiments deliberately vary the segmentation, we needed to evaluate each conversation side as a single “sentence” in order to obtain meaningful results across different segmentations. We construct this top-level sentence by attaching the parser’s proposed constituents for each SU to a new top-level constituent (labeled TIPTOP). Thus, we can compare two different segmentations of the same data, because it ensures tha</context>
</contexts>
<marker>Sekine, Collins, 1997</marker>
<rawString>S. Sekine and M. Collins. 1997. EVALB. As in Collins ACL 1997;http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Strassel</author>
</authors>
<title>Simple Metadata Annotation Specification V5.0. Linguistic Data Consortium.</title>
<date>2003</date>
<contexts>
<context position="7921" citStr="Strassel, 2003" startWordPosition="1280" endWordPosition="1281">onversational telephone speech, which includes sentence boundaries as well as the reparandum and interruption point of disfluencies. The data consists of 816 hand-transcribed conversation sides (566K words), of which we reserve 128 conversation sides (61K words) for evaluation testing according to the 1993 NIST evaluation choices. We use a subset of Switchboard data – hand-annotated for SUs and IPs – for training the SU/IP boundary event detector, and for providing the oracle versions of these events as a control in our experiments. The annotation conventions for this data, referred to as V5 (Strassel, 2003), are slightly different from that used in the TB3 an2Note that the IP performance figures are not comparable to those in the DARPA evaluation, since we restrict the focus to IPs associated with edit disfluencies. h_{-1} h_0 T’_0 T’_{-m+1}&lt;-&lt;s&gt; ............... T’_{-1}&lt;-T_{-2} &lt;s&gt; T_{-1} T_0 h’_{-1}=h_{-2} h’_0 = (h_0.word, NTlabel) h_0 T’_{-m+1}&lt;-&lt;s&gt; ............... T’_{-1}&lt;-T_{-2} T_{-1} T_0 h_{-1} &lt;s&gt; Z A Z’ Z B Z’ Z’ Z’ Z’ Z’ Figure 4: Binarization schemes notations in a few important ways. Notably for this work, V5 annotates IPs for both conversational fillers (such as filled pauses and di</context>
</contexts>
<marker>Strassel, 2003</marker>
<rawString>S. Strassel, 2003. Simple Metadata Annotation Specification V5.0. Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>