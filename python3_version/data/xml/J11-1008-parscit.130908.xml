<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99097">
Grammar Factorization by
Tree Decomposition
</title>
<author confidence="0.99965">
Daniel Gildea*
</author>
<affiliation confidence="0.9942">
University of Rochester
</affiliation>
<bodyText confidence="0.998759285714286">
We describe the application of the graph-theoretic property known as treewidth to the problem of
finding efficient parsing algorithms. This method, similar to the junction tree algorithm used in
graphical models for machine learning, allows automatic discovery of efficient algorithms such
as the O(n4) algorithm for bilexical grammars of Eisner and Satta. We examine the complexity
of applying this method to parsing algorithms for general Linear Context-Free Rewriting Sys-
tems. We show that any polynomial-time algorithm for this problem would imply an improved
approximation algorithm for the well-studied treewidth problem on general graphs.
</bodyText>
<sectionHeader confidence="0.981168" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999910565217391">
In this article, we describe meta-algorithms for parsing: algorithms for finding the
optimal parsing algorithm for a given grammar, with the constraint that rules in the
grammar are considered independently of one another. In order to have a common
representation for our algorithms to work with, we represent parsing algorithms
as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999;
Nederhof 2003). Weighted deduction systems consist of axioms and rules for building
items or partial results. Items are identified by square brackets, with their weights
written to the left. Figure 1 shows a rule for deducing a new item when parsing a
context free grammar (CFG) with the rule S → A B. The item below the line, called
the consequent, can be derived if the two items above the line, called the antecedents,
have been derived. Items have types, corresponding to grammar nonterminals in this
example, and variables, whose values range over positions in the string to be parsed.
We restrict ourselves to items containing position variables directly as arguments; no
other functions or operations are allowed to apply to variables. The consequent’s weight
is the product of the weights of the two antecedents and the rule weight w0. Implicit in
the notation is the fact that we take the maximum weight over all derivations of the same
item. Thus, the weighted deduction system corresponds to the Viterbi or max-product
algorithm for parsing. Applications of the same weighted deduction system with other
semirings are also possible (Goodman 1999).
The computational complexity of parsing depends on the total number of instanti-
ations of variables in the system’s deduction rules. If the total number of instantiations
is M, parsing is O(M) if there are no cyclic dependencies among instantiations, or,
equivalently, if all instantiations can be sorted topologically. In most parsing algorithms,
</bodyText>
<note confidence="0.945478">
* Computer Science Department, University of Rochester, Rochester NY 14627.
</note>
<email confidence="0.954221">
E-mail: gildea@cs.rochester.edu.
</email>
<note confidence="0.85274725">
Submission received: 28 June 2010; revised submission received: 20 September 2010; accepted for publication:
21 October 2010.
© 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
</note>
<equation confidence="0.778382">
w1: [A, x0, x1]
w2: [B, x1, x2]
w0w1w2: [S, x0, x2]
Figure 1
CFG parsing in weighted deduction notation.
</equation>
<bodyText confidence="0.999944">
variables range over positions in the input string. In order to determine complexity in
the length n of the input string, it is sufficient to count the number of unique position
variables in each rule. If all rules have at most k position variables, M = O(nk), and
parsing takes time O(nk) in the length of the input string. In the remainder of this article,
we will explore methods for minimizing k, the largest number of position variables in
any rule, among equivalent deduction systems. These methods directly minimize the
parsing complexity of the resulting deduction system. Although we will assume no
cyclic dependencies among rule instantiations for the majority of the article, we will
discuss the cyclic case in Section 2.2.
It is often possible to improve the computational complexity of a deduction rule
by decomposing the computation into two or more new rules, each having a smaller
number of variables than the original rule. We refer to this process as factorization. One
straightforward example of rule factorization is the binarization of a CFG, as shown in
Figure 2. Given a deduction rule for a CFG rule with r nonterminals on the righthand
side, and a total of r + 1 variables, an equivalent set of rules can be produced, each with
three variables, storing intermediate results that indicate that a substring of the original
rule’s righthand side has been recognized. This type of rule factorization produces an
O(n3) parser for any input CFG.
Another well-known instance of rule factorization is the hook trick of Eisner and
Satta (1999), which reduces the complexity of parsing for bilexicalized CFGs from
O(n5) to O(n4). The basic rule for bilexicalized parsing combines two CFG constituents
marked with lexical heads as shown in Figure 3a. Here items with type C indicate
constituents, with [C, x0, h, x1] indicating a constituent extending from position x0 to
position x1, headed by the word at position h. The item [D, m → h] is used to indicate
the weight assigned by the grammar to a bilexical dependency headed by the word at
</bodyText>
<equation confidence="0.9851825">
a)
w1: [A, x0, x1]
w2: [B, x1, x2]
w3: [C, x2, x3]
w4: [D, x3, x4]
w0w1w2w3w4: [S, x0, x4]
b)
w1: [A, x0, x1] w5: [X, x0, x2] w6: [Y, x0, x3]
w2: [B, x1, x2] w3: [C, x2, x3] w3: [D, x3, x4]
w1w2: [X, x0, x2] w3w5: [Y, x0, x3] w0w3w6: [S, x0, x3]
</equation>
<figureCaption confidence="0.87926">
Figure 2
</figureCaption>
<bodyText confidence="0.9573495">
Binarization of the CFG rule S → A B C D as rule factorization: The deduction rule above can be
factored into the three equivalent rule below.
</bodyText>
<page confidence="0.969482">
232
</page>
<note confidence="0.768746">
Gildea Grammar Factorization by Tree Decomposition
</note>
<equation confidence="0.858799222222222">
a)
wt: [D, m → h]
w1: [C, x0, h, x1]
w2: [C, x1, m, x2]
wtw1w2: [C,x0,h,x2]
b)
wt: [D, m → h] wh: [H, h, x1, x2]
w2: [C, x1, m, x2] w1: [C, x0, h, x1]
w2w2: [H,h,x1,x2] whw1: [C, x0, h, x2]
</equation>
<bodyText confidence="0.967848394736842">
Figure 3
Rule factorization for bilexicalized parsing.
position h with the word at position m as a modifier. The deduction rule is broken into
two steps, one which includes the weight for the bilexical grammar rule, and another
which identifies the boundaries of the new constituent, as shown in Figure 3b. The hook
trick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000),
and has been generalized to improve the complexity of machine translation decoding
under synchronous context-free grammars (SCFGs) with an n-gram language model
(Huang, Zhang, and Gildea 2005).
Rule factorization has also been studied in the context of parsing for SCFGs. Unlike
monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation
between nonterminals in the two languages, it may or may not be possible to reduce the
rank, or number of nonterminals on the righthand side, of a rule. Algorithms for finding
the optimal rank reduction of a specific rule are given by Zhang and Gildea (2007). The
complexity of synchronous parsing for a rule of rank r is O(n2r+2), so reducing rank
improves parsing complexity.
Rule factorization has also been applied to Linear Context-Free Rewriting Systems
(LCFRS), which generalize CFG, TAG, and SCFG to define a rewriting system where
nonterminals may have arbitrary fan-out, which indicates the number of continuous
spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987).
Recent work has examined the problem of factorization of LCFRS rules in order to
reduce rank without increasing grammar fan-out (G´omez-Rodr´ıguez et al. 2009), as well
as factorization with the goal of directly minimizing the parsing complexity of the new
grammar (Gildea 2010).
We define factorization as a process which applies to rules of the input grammar
independently. Individual rules are replaced with an equivalent set of new rules, which
must derive the same set of consequent items as the original rule given the same an-
tecedent items. While new intermediate items of distinct types may be produced, the set
of items and weights derived by the original weighted deduction system is unchanged.
This definition of factorization is broad enough to include all of the previous examples,
but does not include, for example, the fold/unfold operation applied to grammars by
Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold
operation of fold/unfold.
If we allow unrestricted transformations of the input deduction system, finding the
most efficient equivalent system is undecidable; this follows from the fact that it is un-
decidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir
1961), and would therefore be recognizable in constant time. Whereas the fold/unfold
operation of Johnson (2007) and Eisner and Blatz (2007) specifies a narrower class of
</bodyText>
<page confidence="0.993711">
233
</page>
<note confidence="0.294942">
Computational Linguistics Volume 37, Number 1
</note>
<bodyText confidence="0.998113545454545">
grammar transformations, no general algorithms are known for identifying an optimal
series of transformations in this setting. Considering input rules independently allows
us to provide algorithms for optimal factorization.
In this article, we wish to provide a general framework for factorization of deduc-
tive parsing systems in order to minimize computational complexity. We show how
to apply the graph-theoretic property of treewidth to the factorization problem, and
examine the question of whether efficient algorithms exist for optimizing the parsing
complexity of general parsing systems in this framework. In particular, we show that
the existence of a polynomial time algorithm for optimizing the parsing complexity of
general LCFRS rules would imply an improved approximation algorithm for the well-
studied problem of treewidth of general graphs.
</bodyText>
<sectionHeader confidence="0.779622" genericHeader="categories and subject descriptors">
2. Treewidth and Rule Factorization
</sectionHeader>
<bodyText confidence="0.998460571428571">
In this section, we introduce the graph-theoretic property known as treewidth, and
show how it can be applied to rule factorization.
A tree decomposition of a graph G = (V, E) is a type of tree having a subset of G’s
vertices at each node. We define the nodes of this tree T to be the set I, and its edges
to be the set F. The subset of V associated with node i of T is denoted by Xi. A tree
decomposition is therefore defined as a pair (�Xi  |i G I}, T = (I, F)) where each Xi, i G I
is a subset of V, and tree T has the following properties:
</bodyText>
<listItem confidence="0.985674833333333">
• Vertex cover: The nodes of the tree T cover all the vertices of G: UiEI Xi = V.
• Edge cover: Each edge in G is included in some node of T. That is, for all
edges (u, v) G E, there exists an i G I with u, v G Xi.
• Running intersection: The nodes of T containing a given vertex of G form a
connected subtree. Mathematically, for all i, j, k G I, if j is on the (unique)
path from i to k in T, then Xi n Xk C Xj.
</listItem>
<bodyText confidence="0.99902">
The treewidth of a tree decomposition (�Xi},T) is maxi |Xi |− 1. The treewidth of a
graph is the minimum treewidth over all tree decompositions:
</bodyText>
<equation confidence="0.998453">
tw(G) = min max |Xi |− 1
({Xi},T)ETD(G) i
</equation>
<bodyText confidence="0.998296071428571">
where TD(G) is the set of valid tree decompositions of G. We refer to a tree decomposi-
tion achieving the minimum possible treewidth as being optimal.
In general, more densely interconnected graphs have higher treewidth. Any tree
has treewidth = 1; a graph consisting of one large cycle has treewidth = 2, and a fully
connected graph of n vertices has treewidth = n − 1. Low treewidth indicates some tree-
like structure in the graph, as shown by the example with treewidth = 2 in Figure 4. As
an example of the running intersection property, note that the vertex N appears in three
adjacent nodes of the tree decomposition. Finding the treewidth of a graph is an NP-
complete problem (Arnborg, Corneil, and Proskurowski 1987). However, given a graph
of n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition in
time O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approxima-
tion algorithms and heuristics are known for the treewidth problem (Bodlaender et al.
1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005). Furthermore, for fixed k, optimal tree
decompositions can be computed in linear time (Bodlaender 1996).
</bodyText>
<page confidence="0.995788">
234
</page>
<figure confidence="0.556357">
Gildea Grammar Factorization by Tree Decomposition
</figure>
<figureCaption confidence="0.990318">
Figure 4
</figureCaption>
<bodyText confidence="0.972954176470588">
A tree decomposition of a graph is a set of overlapping clusters of the graph’s vertices, arranged
in a tree. This example has treewidth = 2.
We can factorize a deduction rule by representing the rule as a graph, which we
call a dependency graph, and searching for tree decompositions of this graph. For a
rule r having n variables V = {vi  |i E {1,..., n}}, m antecedent items Ai, i E {1, ..., m},
and consequent C, let V(Ai) ⊂ V be the variables appearing in antecedent Ai, and V(C)
be the variables appearing in the consequent. The dependency graph representation of
the rule is Gr = (V, E = US:A1,...,Am,C{(vi, vj)  |vi, vj E V(S)}). That is, we have a vertex for
each variable in the rule, and connect any two vertices that appear together in the same
antecedent, or that appear together in the consequent.
The dependency graph representation allows us to prove the following result con-
cerning parsing complexity:
Theorem 1
Given a deduction rule r for parsing where the input string is referenced only through
position variables appearing as arguments of antecedent and consequent items, the opti-
mal complexity of any factorization of rule r is O(ntw(Gr)+1), where Gr is the dependency
graph derived from r.
</bodyText>
<subsectionHeader confidence="0.565816">
Proof
</subsectionHeader>
<bodyText confidence="0.999960266666667">
One consequence of the definition of a tree decomposition is that, for any clique appear-
ing in the original graph Gr, there must exist a node in the tree decomposition T which
contains all the vertices in the clique. We use this fact to show that there is a one-to-
one correspondence between tree decompositions of a rule’s dependency graph Gr and
factorizations of the rule.
First, we need to show that any tree decomposition of Gr can be used as a factoriza-
tion of the original deduction rule. By our earlier definition, a factorization must derive
the same set of consequent items from a given set of antecedent items as the original
rule. Because Gr includes a clique connecting all variables in the consequent C, the tree
decomposition T must have a node Xc such that V(C) ⊆ Xc. We consider this node to be
the root of T. The original deduction rule can be factorized into a new set of rules, one
for each node in T. For node Xc, the factorized rule has C as a consequent, and all other
nodes Xi have a new partial result as a consequent, consisting of the variables Xi fl Xj,
where Xj is Xi’s neighbor on the path to the root node Xc. We must guarantee that the
factorized rule set yields the same result as the original rule, namely, the semiring sum
</bodyText>
<page confidence="0.967967">
235
</page>
<note confidence="0.273175">
Computational Linguistics Volume 37, Number 1
</note>
<bodyText confidence="0.982015">
over all variable values of the semiring product of the antecedents’ weights. The tree
structure of T corresponds to a factorization of this semiring expression. For example, if
we represent the CFG rule of Figure 2a with the generalized semiring expression:
</bodyText>
<equation confidence="0.983711">
� A(x0, x1) ® B(x1, x2) ® C(x2, x3) ® D(x3, x4)
x1x2x3
</equation>
<bodyText confidence="0.875198">
the factorization of this expression corresponding to the binarized rule is
</bodyText>
<equation confidence="0.9993615">
®(E)((@ A(x0, x1) ® B(x1,x2) ® C(x2,x3) ® D(x3,x4)
x3 x2 x1
</equation>
<bodyText confidence="0.999949227272727">
where semiring operations ® and ® have been interchanged as allowed by the depen-
dency graph for this rule.
Because each antecedent Ai is represented by a clique in the graph Gr, the tree
decomposition T must contain at least one node which includes all variables V(Ai).
We can choose one such node and multiply in the weight of Ai, given the values of
variables V(Ai), at this step of the expression. The running intersection property of the
tree decomposition guarantees that each variable has a consistent value at each point
where it is referenced in the factorization.
The same properties guarantee that any valid rule factorization corresponds to a
tree decomposition of the graph Gr. We consider the tree decomposition with a set Xi
for each new rule ri, consisting of all variables used in ri, and with tree edges T defined
by the producer/consumer relation over intermediate results in the rule factorization.
Each antecedent of the original rule must appear in some new rule in the factorization,
as must the consequent of the original rule. Therefore, all edges in the original rule’s
dependency graph Gr appear in some tree node Xi. Any variable that appears in two
rules in the factorization must appear in all intermediate rules in order to ensure that the
variable has a consistent value in all rules that reference it. This guarantees the running
intersection property of the tree decomposition (�Xi},T). Thus any rule factorization,
when viewed as a tree of sets of variables, has the properties that make it a valid tree
decomposition of Gr.
The theorem follows as a consequence of the one-to-one correspondence between
rule factorizations and tree decompositions. ■
</bodyText>
<subsectionHeader confidence="0.989388">
2.1 Computational Complexity
</subsectionHeader>
<bodyText confidence="0.999982666666667">
Factorization produces, for each input rule having m antecedents, at most m − 1 new
rules, each containing at most the same number of nonterminals and the same number
of variables as the input rule. Hence, the size of the new factorized grammar is O(|G|2),
and we avoid any possibility of an exponential increase in grammar size. Tighter bounds
can be achieved for specific classes of input grammars.
The computational complexity of optimal factorization with tree decomposition is
exponential in the size of the input rules. However, optimal factorization is generally
feasible whenever parsing with the unfactorized grammar is feasible. This is because,
for an input rule with f variables, parsing is O(n�) in the sentence length n. The
treewidth of this rule is at most f − 1, and can be computed in time O(&amp;1); generally we
expect n to be greater than f. One may also wish to accept only rules having treewidth
k and disregard the remainder, for example, when factorizing rules automatically
</bodyText>
<page confidence="0.932947">
236
</page>
<bodyText confidence="0.936839">
Gildea Grammar Factorization by Tree Decomposition
extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006;
Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea
2010). In this setting, the rules having treewidth k can be identified in time O(fk+2) using
the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again f
is the number of variables in the input rules), or in time O(f) using the algorithm of
Bodlaender (1996).
</bodyText>
<subsectionHeader confidence="0.996817">
2.2 Cyclic Dependencies
</subsectionHeader>
<bodyText confidence="0.999942933333333">
Although this article primarily addresses the case where there are no cyclic dependen-
cies between rule instantiations, we note here that our techniques carry over to the
cyclic case under certain conditions. If there are cycles in the rule dependencies, but
the semiring meets Knuth’s (1977) definition of a superior function, parsing takes time
O(M log M), where M is the number of rule instantiations, and the extra log M term
accounts for maintaining an agenda as a priority queue (Nederhof 2003). Cycles in
the rule dependencies may arise, for example, from chains of unary productions in a
CFG; the properties of superior functions guarantee that unbounded chains need not
be considered. The max-product semiring used in Viterbi parsing has this property,
assuming that all rule weights are less than one, whereas for exact computation with
the sum-product semiring, unbounded chains must be considered. As in the acyclic
case, M = O(nk) for parsing problems where rules have at most k variables. Under
the assumption of superior functions, parsing takes time O(nkklogn) with Knuth’s
algorithm. In this setting, as in the acyclic case, minimizing k with tree decomposition
minimizes parsing complexity.
</bodyText>
<subsectionHeader confidence="0.917345">
2.3 Related Applications of Treewidth
</subsectionHeader>
<bodyText confidence="0.999839">
The technique of using treewidth to minimize complexity has been applied to constraint
satisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen,
Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization for
databases (Chekuri and Rajaraman 1997). Our formulation of parsing is most closely
related to logic programming; in this area treewidth has been applied to limit complex-
ity in settings where either the deduction rules or the input database of ground facts
have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe
(2002) apply treewidth to nonrecursive datalog programs, our parsing programs have
unbounded recursion, as the depth of the parse tree is not fixed in advance. Our results
for parsing can be seen as a consequence of the fact that, even in the case of unbounded
recursion, the complexity of (unweighted) datalog programs is linear in the number of
possible rule instantiations (McAllester 2002).
</bodyText>
<listItem confidence="0.442538">
3. Examples of Treewidth for Parsing
</listItem>
<bodyText confidence="0.997138375">
In this section, we show how a few well-known parsing algorithms can be derived
automatically by finding the optimal tree decomposition of a dependency graph.
To aid in visualization of the graphical representation of deduction rules, we use a
factor graph representation based on that of Kschischang, Frey, and Loeliger (2001) for
Markov Random Fields. Our graphs have three types of nodes: variables, antecedents,
and consequents. Each antecedent node is connected to the variables it contains, and
represents the antecedent’s weight as a function of those variables. Antecedent nodes
are analogous to the factor nodes of Kschischang, Frey, and Loeliger (2001), and
</bodyText>
<page confidence="0.957641">
237
</page>
<figure confidence="0.878366">
Computational Linguistics Volume 37, Number 1
</figure>
<figureCaption confidence="0.97614">
Figure 5
</figureCaption>
<bodyText confidence="0.967208">
Factor graph for the binary CFG deduction rule of Figure 1.
consequent nodes are a new feature of this representation. We can think of consequents
as factors with weight = 1; they do not affect the weights computed, but serve to
guarantee that the consequent of the original rule can be found in one node of the tree
decomposition. We refer to both antecedent and consequent nodes as factor nodes. Re-
placing each factor node with a clique over its neighbor variables yields the dependency
graph Gr defined earlier. We represent variables with circles, antecedents with squares
labeled with the antecedent’s weight, and consequents with diamonds labeled c. An
example factor graph for the simple CFG rule of Figure 1 is shown in Figure 5.
</bodyText>
<subsectionHeader confidence="0.950248">
3.1 CFG Binarization
</subsectionHeader>
<bodyText confidence="0.8748445">
Figure 6a shows the factor graph derived from the monolingual CFG rule with four
children in Figure 2a. The dependency graph obtained by replacing each factor with
a clique of size 2 (a single edge) is a graph with one large cycle, shown in Figure 6b.
Finding the optimal tree decomposition yields a tree with nodes of size 3, {x0, xi, xi+1}
for each i, shown in Figure 6c. Each node in this tree decomposition corresponds to one
of the factored deduction rules in Figure 2b. Thus, the tree decomposition shows us how
</bodyText>
<footnote confidence="0.5369615">
Figure 6
Treewidth applied to CFG binarization.
</footnote>
<page confidence="0.972235">
238
</page>
<note confidence="0.556923">
Gildea Grammar Factorization by Tree Decomposition
</note>
<bodyText confidence="0.938643">
to parse in time O(n3); finding the tree decomposition of a long CFG rule is essentially
equivalent to converting to Chomsky Normal Form.
</bodyText>
<subsectionHeader confidence="0.981902">
3.2 The Hook Trick
</subsectionHeader>
<bodyText confidence="0.999512823529412">
The deduction rule for bilexicalized parsing shown in Figure 3a translates into the factor
graph shown in Figure 7a. Factor nodes are created for the two existing constituents
from the chart, with the first extending from position x0 in the string to x1, and the
second from x1 to x2. Both factor nodes are connected not only to the start and end
points, but also to the constituent’s head word, h for the first constituent and m for
the second (we show the construction of a left-headed constituent in the figure). An
additional factor is connected only to h and m to represent the bilexicalized rule weight,
expressed as a function of h and m, which is multiplied with the weight of the two
existing constituents to derive the weight of the new constituent. The new constituent
is represented by a consequent node at the top of the graph—the variables that will be
relevant for its further combination with other constituents are its end points x0 and x2
and its head word h.
Placing an edge between each pair of variable nodes that share a factor, we get Fig-
ure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c,
each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest
node of the tree decomposition has four variables, giving the O(n4) algorithm of Eisner
and Satta (1999).
</bodyText>
<subsectionHeader confidence="0.989361">
3.3 SCFG Parsing Strategies
</subsectionHeader>
<bodyText confidence="0.995967666666667">
SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure
simultaneously, and have become widely used as statistical models of machine transla-
tion (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one
</bodyText>
<figureCaption confidence="0.99154">
Figure 7
</figureCaption>
<bodyText confidence="0.874417">
Treewidth applied to bilexicalized parsing.
</bodyText>
<page confidence="0.9789">
239
</page>
<note confidence="0.47604">
Computational Linguistics Volume 37, Number 1
</note>
<bodyText confidence="0.997491333333333">
lefthand side nonterminal and two righthand side strings. Nonterminals in the two
strings are linked with superscript indices; symbols with the same index must be further
rewritten synchronously. For example,
</bodyText>
<equation confidence="0.976284">
X , A(1) B(2) C(3) D(4), A(1) B(2) C(3) D(4) (1)
is a rule with four children and no reordering, whereas
X , A(1) B(2) C(3) D(4), B(2) D(4) A(1) C(3) (2)
</equation>
<bodyText confidence="0.999551">
expresses a more complex reordering. In general, we can take indices in the first
righthand-side string to be consecutive, and associate a permutation π with the second
string. If we use Xi for 0 &lt; i &lt; n as a set of variables over nonterminal symbols (for
example, X1 and X2 may both stand for nonterminal A), we can write rules in the general
form:
</bodyText>
<equation confidence="0.9992918">
X0 , X(1)
1 ··· X(n)
n , X(π(1))
π(1) ··· X(π(n))
π(n)
</equation>
<bodyText confidence="0.999901357142857">
Unlike monolingual CFGs, SCFGs cannot always be binarized. In fact, the lan-
guages of string pairs generated by a synchronous grammar can be arranged in an
infinite hierarchy, with each rank &gt; 4 producing languages not possible with grammars
restricted to smaller rules (Aho and Ullman 1972). For any grammar with maximum
rank r, converting each rule into a single deduction rule yields an O(n2r+2) parsing
algorithm, because there are r + 1 boundary variables in each language. More efficient
parsing algorithms are often possible for specific permutations, and, by Theorem 1, the
best algorithm for a permutation can be found by computing the minimum-treewidth
tree decomposition of the graph derived from the SCFG deduction rule for a specific
permutation. For example, for the non-binarizable rule of Equation (2), the resulting
factor graph is shown in Figure 8a, where variables x0, ... , x4 indicate position variables
in one language of the synchronous grammar, and y0, ... , y4 are positions in the other
language. The optimal tree decomposition for this rule is shown in Figure 8c. For this
permutation, the optimal parsing algorithm takes time O(n8), because the largest node
in the tree decomposition of Figure 8c includes eight position variables. This result is
intermediate between the O(n6) for binarizable SCFGs, also known as Inversion Trans-
duction Grammars (Wu 1997), and the O(n10) that we would achieve by recognizing the
rule in a single deduction step.
Gildea and ˇStefankoviˇc (2007) use a combinatorial argument to show that as the
number of nonterminals r in an SCFG rule grows, the parsing complexity grows as
Ω(ncr) for some constant c. In other words, some very difficult permutations exist of all
lengths.
It is interesting to note that although applying the tree decomposition technique
to long CFG rules results in a deduction system equivalent to a binarized CFG, the
individual deduction steps in the best parsing strategy for an SCFG rule do not in
general correspond to SCFG rules. This is because the intermediate results may include
more than one span in each language. These intermediate deduction steps do, however,
correspond to LCFRS rules. We now turn to examine LCFRS in more detail.
</bodyText>
<page confidence="0.967016">
240
</page>
<note confidence="0.50647">
Gildea Grammar Factorization by Tree Decomposition
</note>
<figureCaption confidence="0.982825">
Figure 8
</figureCaption>
<bodyText confidence="0.953244">
Treewidth applied to the SCFG rule of Equation (2).
</bodyText>
<sectionHeader confidence="0.977093" genericHeader="general terms">
4. LCFRS Parsing Strategies
</sectionHeader>
<bodyText confidence="0.999980631578947">
LCFRS provides a generalization of a number of widely used formalisms in natural
language processing, including CFG, TAG, SCFG, and synchronous TAG. LCFRS has
also been used to model non-projective dependency grammars, and the LCFRS rules
extracted from dependency treebanks can be quite complex (Kuhlmann and Satta
2009), making factorization important. Similarly, LCFRS can model translation relations
beyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammars
extracted from word-aligned bilingual corpora can also be quite complex (Wellington,
Waxmonsky, and Melamed 2006). An algorithm for factorization of LCFRS rules is
presented by Gildea (2010), exploiting specific properties of LCFRS. The tree decompo-
sition method achieves the same results without requiring analysis specific to LCFRS.
In this section, we examine the complexity of rule factorization for general LCFRS
grammars.
The problem of finding the optimal factorization of an arbitrary deduction rule is
NP-complete. This follows from the NP-completeness of treewidth using the following
construction: Given a graph, create a deduction rule with a variable for each vertex in
the graph and an antecedent for each edge, containing the two variables associated with
the edge’s endpoints. The graphs produced by LCFRS grammar rules, however, have
certain properties which may make more efficient factorization algorithms possible. We
first define LCFRS precisely before examining the properties of these graphs.
</bodyText>
<page confidence="0.989724">
241
</page>
<note confidence="0.485889">
Computational Linguistics Volume 37, Number 1
</note>
<bodyText confidence="0.99835075">
An LCFRS is defined as a tuple G = (VT, VN, P, S), where VT is a set of terminal
symbols, VN is a set of nonterminal symbols, P is a set of productions, and S E VN is a
distinguished start symbol. Associated with each nonterminal B is a fan-out y(B), which
tells how many continuous spans B covers. Productions p E P take the form:
</bodyText>
<equation confidence="0.778627">
p : A -4 g(B1, B2, ... , Br) (3)
where A, B1,..., Br E VN, and g is a function
g : (V∗T)ϕ(B1) X ··· X (V∗T)ϕ(Br) -4 (V∗T)ϕ(A)
</equation>
<bodyText confidence="0.999956">
which specifies how to assemble the Eri=1 y(Bi) spans of the righthand side nontermi-
nals into the y(A) spans of the lefthand side nonterminal. The function g must be linear
and non-erasing, which means that if we write
</bodyText>
<equation confidence="0.899295">
g((s1,1,...,s1,ϕ(B1)),...,(s1,1, ... ,s1,ϕ(Br))) = (t1,...,tϕ(A))
</equation>
<bodyText confidence="0.9996738">
the tuple of strings (t1,...,tϕ(A)) on the righthand side contains each variable si,j from
the lefthand side exactly once, and may also contain terminals from VT. The process of
generating a string from an LCFRS grammar can be thought of as first choosing, top-
down, a production to expand each nonterminal, and then, bottom–up, applying the
functions associated with each production to build the string. As an example, the CFG
</bodyText>
<figure confidence="0.691784857142857">
S - 4AB
A - 4a
B - 4b
corresponds to the following grammar in LCFRS notation:
S -4 gS(A,B) gS((sA), (sB)) = (sAsB)
A -4 gA() gA() = (a)
B -4 gB() gB() = (b)
</figure>
<bodyText confidence="0.999551857142857">
Here, all nonterminals have fan-out = 1, reflected in the fact that all tuples defining the
productions’ functions contain just one string. As CFG is equivalent to LCFRS with fan-
out = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2. Higher values of
fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomial-
time parsing is possible for any fixed LCFRS grammar, but the degree of the polynomial
depends on the grammar. Parsing general LCFRS grammars, where the grammar is
considered part of the input, is NP-complete (Satta 1992).
</bodyText>
<subsectionHeader confidence="0.988063">
4.1 Graphs Derived from LCFRS Rules
</subsectionHeader>
<bodyText confidence="0.999729">
Given an LCFRS rule as defined previously, a weighted deduction rule for a bottom–
up parser can be derived by creating an antecedent for each righthand nonterminal,
a consequent for the lefthand side, and variables for all the boundaries of the non-
terminals in the rule. A nonterminal of fan-out f has 2f boundaries. Each boundary
</bodyText>
<page confidence="0.978321">
242
</page>
<note confidence="0.562819">
Gildea Grammar Factorization by Tree Decomposition
</note>
<bodyText confidence="0.999818363636364">
variable will occur exactly twice in the deduction rule: either in two antecedents, if two
nonterminals on the rule’s righthand side are adjacent, or once in an antecedent and
once in the consequent, if the variable indicates a boundary of any segment of the rule’s
lefthand side.
Converting such deduction rules into dependency graphs, we see that the cliques
of the dependency graph may be arbitrarily large, due to the unbounded fan-out of
LCFRS nonterminals. However, each vertex appears in only two cliques, because each
boundary variable in the rule is shared by exactly two nonterminals. In the remainder of
this section, we consider whether the problem of finding the optimal tree decomposition
of this restricted set of graphs is also NP-complete, or whether efficient algorithms may
be possible in the LCFRS setting.
</bodyText>
<subsectionHeader confidence="0.995023">
4.2 Approximation of Treewidth for General Graphs
</subsectionHeader>
<bodyText confidence="0.999478538461538">
We will show that an efficient algorithm for finding the factorization of an arbitrary
LCFRS production that optimizes parsing complexity would imply the existence of an
algorithm for treewidth that returns a result within a factor of 4∆(G) of the optimum,
where ∆(G) is the maximum degree of the input graph. Although such an approxima-
tion algorithm may be possible, it would require progress in fundamental problems in
graph theory.
Consider an arbitrary graph G = (V, E), and define k to be its treewidth, k = tw(G).
We wish to construct a new graph G&apos; = (V&apos;, E&apos;) from G in such a way that tw(G&apos;) =
tw(G) and every vertex in G&apos; has even degree. This can be accomplished by doubling
the graph’s edges in the manner shown in Figure 9. To double the edges, for every edge
e = (u, v) in E, we add a new vertex eˆ to G&apos; and add edges (u, ˆe) and (v, ˆe) to G&apos;. We also
include every edge in the original graph G in G&apos;. Now, every vertex v in G&apos; has degree =
2, if it is a newly created vertex, or twice the degree of v in G otherwise, and therefore
</bodyText>
<equation confidence="0.997989">
∆(G&apos;) = 2∆(G) (4)
</equation>
<bodyText confidence="0.9998996">
We now show that tw(G&apos;) = tw(G), under the assumption that tw(G) ≥ 3. Any tree
decomposition of G can be adapted to a tree decomposition of G&apos; by adding a node
containing {u, v, ˆe} for each edge e in the original graph, as shown in Figure 10. The new
node can be attached to a node containing u and v; because u and v are connected by
an edge in G, such a node must exist in G’s tree decomposition. The vertex eˆ will not
occur anywhere else in the tree decomposition, and the occurrences of u and v still form
a connected subtree. For each edge e = (u, v) in G&apos;, the tree decomposition must have a
node containing u and v; this is the case because, if e is an original edge from G, there
is already a node in the tree decomposition containing u and v, whereas if e is an edge
to a newly added vertex in G&apos;, one of the newly added nodes in the tree decomposition
</bodyText>
<figureCaption confidence="0.762362">
Figure 9
</figureCaption>
<bodyText confidence="0.88295">
An example graph Gex and the result Gex of doubling Gex’s edges.
</bodyText>
<page confidence="0.993718">
243
</page>
<figure confidence="0.827041">
Computational Linguistics Volume 37, Number 1
</figure>
<figureCaption confidence="0.951681">
Figure 10
</figureCaption>
<bodyText confidence="0.897987166666667">
Tree decompositions of Gex and Gex.
will contain its endpoints. We constructed the new tree decomposition by adding nodes
of size 3. Therefore, as long as the treewidth of G was at least 3, tw(G&apos;) &lt; tw(G). In
the other direction, because G is a subgraph of G&apos;, any tree decomposition of G&apos; forms a
valid tree decomposition of G after removing the vertices in G&apos; − G, and hence tw(G&apos;) &gt;
tw(G). Therefore,
</bodyText>
<equation confidence="0.99275">
tw(G&apos;) = tw(G) (5)
</equation>
<bodyText confidence="0.999921428571429">
Because every vertex in G&apos; has even degree, G&apos; has an Eulerian tour, that is, a
path visiting every edge exactly once, beginning and ending at the same vertex. Let
π = (π1,...,πn) be the sequence of vertices along such a tour, with π1 = πn. Note that
the sequence π contains repeated elements. Let µi, i G {1, ... , n} indicate how many
times we have visited πi on the ith step of the tour: µi = |{j  |πj = πi, j G {1,.. . , i}}|.
We now construct an LCFRS production P with |V&apos; |righthand side nonterminals from
the Eulerian tour:
</bodyText>
<equation confidence="0.997318">
P : X -4 g(B1,..., B|V,|)
g((s1,1,...,s1,φ(B1)),..., (s|V&apos;|,1,...,s|V&apos;|,φ(B|V,|))) = (sπ1,µ1 ···sπn,µn)
</equation>
<bodyText confidence="0.999430714285714">
The fan-out φ(Bi) of each nonterminal Bi is the number of times vertex i is visited on the
Eulerian tour. The fan-out of the lefthand side nonterminal X is one, and the lefthand
side is constructed by concatenating the spans of each nonterminal in the order specified
by the Eulerian tour.
For the example graph in Figure 9, one valid tour is
πex = (A, B, C, D, F, C, E, A, G, B, H, C, A)
This tour results in the following LCFRS production:
</bodyText>
<equation confidence="0.994218666666667">
Pex : X -4 gex(A, B, C, D, E, F, G, H)
gex((sA,1,sA,2,sA,3), (sB,1,sB,2), (sC,1,sC,2,sC,3), (sD,1), (sE,1), (sF,1), (sG,1), (sH,1)) =
(sA,1sB,1sC,1sD,1sF,1sC,2sE,1sA,2sG,1sB,2sH,1sC,3sA,3)
</equation>
<bodyText confidence="0.9972455">
We now construct dependency graph G&amp;quot; from the LCFRS production P by applying
the technique of Section 2. G&amp;quot; has n + 1 vertices, corresponding to the beginning and
</bodyText>
<page confidence="0.990148">
244
</page>
<note confidence="0.68436">
Gildea Grammar Factorization by Tree Decomposition
</note>
<bodyText confidence="0.94778225">
end points of the nonterminals in P. The edges in G&amp;quot; are formed by adding a clique for
each nonterminal in P connecting all its beginning and end points, that is, (2f ) edges
2
for a nonterminal of fan-out f. We must include a clique for X, the lefthand side of
the production. However, because the righthand side of the production begins and
ends with the same nonterminal, the vertices for the beginning and end points of X
are already connected, so the lefthand side does not affect the graph structure for the
entire production. By Theorem 1, the optimal parsing complexity of P is tw(G&amp;quot;) + 1.
The graphs G&apos; and G&amp;quot; are related in the following manner: Every edge in G&apos;
corresponds to a vertex in G&amp;quot;, and every vertex in G&apos; corresponds to a clique in G&amp;quot;.
We can identify vertices in G&amp;quot; with unordered pairs of vertices {u, v} in G&apos;. The edges in
G&amp;quot; are ({u, v}, {u, w}) Vu, v, w : u =� v, u =� w, v =� w. An example of G&amp;quot; derived from our
example production Pex is shown in Figure 11.
Any tree decomposition T&amp;quot; of G&amp;quot; can be transformed into a valid tree decomposi-
tion T&apos; of G&apos; by simply replacing each vertex in each node of T&amp;quot; with both corresponding
vertices in G&apos;. If T&amp;quot; witnesses a tree decomposition of optimal width k&amp;quot; = tw(G&amp;quot;), each
node in T&amp;quot; will produce a node of size at most 2k&amp;quot; in T&apos;. For any vertex v in G&apos;, one
node in T&amp;quot; must contain the clique corresponding to v in G&amp;quot;. Each vertex {v,w} in G&amp;quot;
must be found in a contiguous subtree of T&amp;quot;, and these subtrees all include the node
containing the clique for v. The occurrences of v in T&apos; are the union of these contiguous
subtrees, which must itself form a contiguous subtree. Furthermore, each edge (u, v) in
G&apos; corresponds to some vertex in G&amp;quot;, so u and v must occur together in some node of
T&apos;. Combining these two properties, we see that T&apos; is a valid tree decomposition of G&apos;.
From the construction, if SOL is the treewidth of T&apos;, we are guaranteed that
</bodyText>
<equation confidence="0.917477">
SOL ≤ 2tw(G&amp;quot;) (6)
</equation>
<bodyText confidence="0.995436666666667">
In the other direction, any tree decomposition T&apos; of G&apos; can be transformed into a
tree decomposition T&amp;quot; of G&amp;quot; by simply replacing each occurrence of vertex v in a node
of T&apos; with all vertices {v, w} in T&amp;quot;. The number of such vertices is the degree of v, 0(v).
</bodyText>
<figureCaption confidence="0.619751">
Figure 11
</figureCaption>
<subsectionHeader confidence="0.654477">
Dependency graph G~~
</subsectionHeader>
<bodyText confidence="0.816659333333333">
ex derived from the example of Figure 9. Vertex #A corresponds to the
beginning of the Eulerian tour through Gex and A# corresponds to the end of the tour; all other
vertices correspond to edges in Gex.
</bodyText>
<page confidence="0.993046">
245
</page>
<note confidence="0.590068">
Computational Linguistics Volume 37, Number 1
</note>
<bodyText confidence="0.990361">
Each vertex {v, w} occurs in a contiguous subtree of T&amp;quot; because v and w occurred in
contiguous subtrees of T&apos;, and had to co-occur in at least one node of T. Each edge
in G&amp;quot; comes from a clique for some vertex v in G&apos;, so the edge has both its endpoints
in any node of T&amp;quot; corresponding to a node of T&apos; that contained v. Thus T&amp;quot; is a valid tree
decomposition of G&amp;quot;. We expand each node in the tree decomposition by at most the
maximum degree of the graph ∆(G&apos;), and therefore
</bodyText>
<equation confidence="0.969239">
tw(G&amp;quot;) &lt; ∆(G&apos;)tw(G&apos;) (7)
</equation>
<bodyText confidence="0.991264">
Assume that we have an efficient algorithm for computing the optimal parsing
strategy of an arbitrary LCFRS rule. Consider the following algorithm for finding a tree
decomposition of an input graph G:
</bodyText>
<listItem confidence="0.959355333333333">
• Transform G to G&apos; of even degree, and construct LCFRS production P from
an Eulerian tour of G&apos;.
• Find the optimal parsing strategy for P.
• Translate this strategy into a tree decomposition of G&amp;quot; of treewidth k&amp;quot;, and
map this into a tree decomposition of G&apos;, and then remove all new nodes eˆ
to obtain a tree decomposition of G of treewidth SOL.
</listItem>
<bodyText confidence="0.840874">
If tw(G&amp;quot;) = k&amp;quot;, we have SOL &lt; 2k&amp;quot; from Equation (6), and k&amp;quot; &lt; ∆(G&apos;)tw(G&apos;) from
Equation (7). Putting these together:
SOL &lt; 2∆(G&apos;)tw(G&apos;)
and using Equations (4) and (5) to relate our result to the original graph G,
SOL &lt; 4∆(G)tw(G)
This last inequality proves the main result of this section
Theorem 2
An algorithm for finding the optimal parsing strategy of an arbitrary LCFRS production
would imply a 4∆(G) approximation algorithm for treewidth.
Whether such an approximation algorithm for treewidth is possible is an open prob-
lem. The best-known result is the O(\/log k) approximation result of Feige, Hajiaghayi,
and Lee (2005), which improves on the O(log k) result of Amir (2001). This indicates that,
although polynomial-time factorization of LCFRS rules to optimize parsing complexity
may be possible, it would require progress on general algorithms for treewidth.
</bodyText>
<sectionHeader confidence="0.971857" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.99993">
We have demonstrated that a number of techniques used for specific parsing prob-
lems can be found algorithmically from declarative specifications of the grammar.
Our method involves finding the optimal tree decomposition of a graph, which is in
general an NP-complete problem. However, the relation to tree decomposition allows
us to exploit existing algorithms for this problem, such as the linear time algorithm
of Bodlaender (1996) for graphs of bounded treewidth. In practice, grammar rules are
</bodyText>
<page confidence="0.989184">
246
</page>
<note confidence="0.678153">
Gildea Grammar Factorization by Tree Decomposition
</note>
<bodyText confidence="0.999872666666667">
typically small, and finding the tree decomposition is not computationally expensive,
and in fact is trivial in comparison to the original parsing problem. Given the special
structure of the graphs derived from LCFRS productions, however, we have explored
whether finding optimal tree decompositions of these graphs, and therefore optimal
parsing strategies for LCFRS productions, is also NP-complete. Although a polynomial
time algorithm for this problem would not necessarily imply that P = NP, it would
require progress on fundamental, well-studied problems in graph theory. Therefore, it
does not seem possible to exploit the special structure of graphs derived from LCFRS
productions.
</bodyText>
<sectionHeader confidence="0.980233" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988158571428571">
This work was funded by NSF grants
IIS-0546554 and IIS-0910611. We are grateful
to Giorgio Satta for extensive discussions on
grammar factorization, as well as for
feedback on earlier drafts from Mehdi Hafezi
Manshadi, Matt Post, and four anonymous
reviewers.
</bodyText>
<sectionHeader confidence="0.994844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99708634939759">
Aho, Albert V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation, and
Compiling, volume 1. Prentice-Hall,
Englewood Cliffs, NJ.
Amir, Eyal. 2001. Efficient approximation
for triangulation of minimum treewidth.
In 17th Conference on Uncertainty in
Artificial Intelligence, pages 7–15,
Seattle, WA.
Arnborg, Stefen, Derek G. Corneil, and
Andrzej Proskurowski. 1987. Complexity
of finding embeddings in a k-tree. SIAM
Journal of Algebraic and Discrete Methods,
8:277–284.
Bar-Hillel, Yehoshua, M. Perles, and
E. Shamir. 1961. On formal properties of
simple phrase structure grammars.
Zeitschrift f¨ur Phonetik, Sprachwissenschaft
und Kommunikationsforschung, 14:143–172.
Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on Their
Theory and Application, Addison-Wesley
Reading, MA, pages 116–150.
Bodlaender, H. L. 1996. A linear time
algorithm for finding tree decompositions
of small treewidth. SIAM Journal on
Computing, 25:1305–1317.
Bodlaender, Hans L., John R. Gilbert,
Hj´almt´yr Hafsteinsson, and Ton Kloks.
1995. Approximating treewidth,
pathwidth, frontsize, and shortest
elimination tree. Journal of Algorithms,
18(2):238–255.
Chekuri, Chandra and Anand Rajaraman.
1997. Conjunctive query containment
revisited. In Database Theory – ICDT ’97,
volume 1186 of Lecture Notes in Computer
Science. Springer, Berlin, pages 56–70.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201–228.
Dechter, Rina and Judea Pearl. 1989. Tree
clustering for constraint networks.
Artificial Intelligence, 38(3):353–366.
Eisner, Jason and John Blatz. 2007. Program
transformations for optimization of
parsing algorithms and other weighted
logic programs. In Shuly Wintner, editor,
Proceedings of FG 2006: The 11th Conference
on Formal Grammar. CSLI Publications,
pages 45–85, Malaga.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Conference
of the Association for Computational
Linguistics (ACL-99), pages 457–464,
College Park, MD.
Eisner, Jason and Giorgio Satta. 2000. A faster
parsing algorithm for lexicalized
tree-adjoining grammars. In Proceedings of
the 5th Workshop on Tree-Adjoining
Grammars and Related Formalisms (TAG+5),
pages 14–19, Paris.
Feige, Uriel, MohammadTaghi Hajiaghayi,
and James R. Lee. 2005. Improved
approximation algorithms for
minimum-weight vertex separators. In
STOC ’05: Proceedings of the thirty-seventh
annual ACM symposium on Theory of
computing, pages 563–572, Baltimore, MD.
Flum, J¨org, Markus Frick, and Martin Grohe.
2002. Query evaluation via
tree-decompositions. Journal of the ACM,
49(6):716–752.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What’s in a
translation rule? In Proceedings of the 2004
Meeting of the North American Chapter of the
Association for Computational Linguistics
(NAACL-04), pages 273–280, Boston, MA.
Gildea, Daniel. 2010. Optimal parsing
strategies for Linear Context-Free
</reference>
<page confidence="0.987666">
247
</page>
<note confidence="0.579888">
Computational Linguistics Volume 37, Number 1
</note>
<reference confidence="0.996292042372881">
Rewriting Systems. In Proceedings of the
2010 Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-10), pages 769–776,
Los Angeles, CA.
Gildea, Daniel and Daniel ˇStefankoviˇc. 2007.
Worst-case synchronous grammar rules. In
Proceedings of the 2007 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-07),
pages 147–154, Rochester, NY.
G´omez-Rodriguez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David Weir.
2009. Optimal reduction of rule length in
Linear Context-Free Rewriting Systems. In
Proceedings of the 2009 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-09),
pages 539–547, Boulder, CO.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573–605.
Huang, Liang, Hao Zhang, and Daniel
Gildea. 2005. Machine translation as
lexicalized parsing with hooks. In
International Workshop on Parsing
Technologies (IWPT05), pages 65–73,
Vancouver.
Huang, Liang, Hao Zhang, Daniel Gildea,
and Kevin Knight. 2009. Binarization of
synchronous context-free grammars.
Computational Linguistics, 35(4):559–595.
Jensen, Finn V., Steffen L. Lauritzen, and
Kristian G. Olesen.1990. Bayesian
updating in causal probabilistic networks
by local computations. Computational
Statistics Quarterly, 4:269–282.
Johnson, Mark. 2007. Transforming
projective bilexical dependency grammars
into efficiently-parsable CFGs with
unfold-fold. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 168–175,
Prague.
Knuth, D. 1977. A generalization of Dijkstra’s
algorithm. Information Processing Letters,
6(1):1–5.
Kschischang, F. R., B. J. Frey, and H. A.
Loeliger. 2001. Factor graphs and the
sum-product algorithm. IEEE Transactions
on Information Theory, 47(2):498–519.
Kuhlmann, Marco and Joakim Nivre. 2006.
Mildly non-projective dependency
structures. In Proceedings of the International
Conference on Computational
Linguistics/Association for Computational
Linguistics (COLING/ACL-06),
pages 507–514, Sydney.
Kuhlmann, Marco and Giorgio Satta. 2009.
Treebank grammar techniques for
non-projective dependency parsing. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL-09),
pages 478–486, Athens.
McAllester, David. 2002. On the complexity
analysis of static analyses. Journal of the
ACM, 49(4):512–537.
Melamed, I. Dan, Giorgio Satta, and Ben
Wellington. 2004. Generalized multitext
grammars. In Proceedings of the 42nd
Annual Conference of the Association for
Computational Linguistics (ACL-04),
pages 661–668, Barcelona.
Nederhof, M.-J. 2003. Weighted deductive
parsing and Knuth’s algorithm.
Computational Linguistics, 29(1):135–144.
Rambow, Owen and Giorgio Satta. 1999.
Independent parallelism in finite
copying parallel rewriting systems.
Theoretical Computer Science,
223(1-2):87–120.
Satta, Giorgio. 1992. Recognition of Linear
Context-Free Rewriting Systems. In
Proceedings of the 30th Annual Conference of
the Association for Computational Linguistics
(ACL-92), pages 89–95, Newark, DE.
Shafer, G. and P. Shenoy. 1990. Probability
propagation. Annals of Mathematics and
Artificial Intelligence, 2:327–353.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
The Journal of Logic Programming,
24(1-2):3–36.
Vijay-Shankar, K., D. L. Weir, and A. K. Joshi.
1987. Characterizing structural
descriptions produced by various
grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association
for Computational Linguistics (ACL-87),
pages 104–111, Stanford, CA.
Wellington, Benjamin, Sonjia Waxmonsky,
and I. Dan Melamed. 2006. Empirical
lower bounds on the complexity of
translational equivalence. In Proceedings of
the International Conference on Computational
Linguistics/Association for Computational
Linguistics (COLING/ACL-06),
pages 977–984, Sydney.
Wu, Dekai.1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377–403.
Zhang, Hao and Daniel Gildea. 2007.
Factorization of synchronous context-free
grammars in linear time. In NAACL
Workshop on Syntax and Structure in
Statistical Translation (SSST), pages 25–32,
Rochester, NY.
</reference>
<page confidence="0.99701">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562950">
<title confidence="0.8591565">Grammar Factorization by Tree Decomposition</title>
<affiliation confidence="0.876112">University of Rochester</affiliation>
<abstract confidence="0.982476857142857">We describe the application of the graph-theoretic property known as treewidth to the problem of finding efficient parsing algorithms. This method, similar to the junction tree algorithm used in graphical models for machine learning, allows automatic discovery of efficient algorithms such the for bilexical grammars of Eisner and Satta. We examine the complexity of applying this method to parsing algorithms for general Linear Context-Free Rewriting Systems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="25526" citStr="Aho and Ullman 1972" startWordPosition="4257" endWordPosition="4260"> first righthand-side string to be consecutive, and associate a permutation π with the second string. If we use Xi for 0 &lt; i &lt; n as a set of variables over nonterminal symbols (for example, X1 and X2 may both stand for nonterminal A), we can write rules in the general form: X0 , X(1) 1 ··· X(n) n , X(π(1)) π(1) ··· X(π(n)) π(n) Unlike monolingual CFGs, SCFGs cannot always be binarized. In fact, the languages of string pairs generated by a synchronous grammar can be arranged in an infinite hierarchy, with each rank &gt; 4 producing languages not possible with grammars restricted to smaller rules (Aho and Ullman 1972). For any grammar with maximum rank r, converting each rule into a single deduction rule yields an O(n2r+2) parsing algorithm, because there are r + 1 boundary variables in each language. More efficient parsing algorithms are often possible for specific permutations, and, by Theorem 1, the best algorithm for a permutation can be found by computing the minimum-treewidth tree decomposition of the graph derived from the SCFG deduction rule for a specific permutation. For example, for the non-binarizable rule of Equation (2), the resulting factor graph is shown in Figure 8a, where variables x0, ..</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Albert V. and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Amir</author>
</authors>
<title>Efficient approximation for triangulation of minimum treewidth.</title>
<date>2001</date>
<booktitle>In 17th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>7--15</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="11800" citStr="Amir 2001" startWordPosition="1963" endWordPosition="1964">ructure in the graph, as shown by the example with treewidth = 2 in Figure 4. As an example of the running intersection property, note that the vertex N appears in three adjacent nodes of the tree decomposition. Finding the treewidth of a graph is an NPcomplete problem (Arnborg, Corneil, and Proskurowski 1987). However, given a graph of n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition in time O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approximation algorithms and heuristics are known for the treewidth problem (Bodlaender et al. 1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005). Furthermore, for fixed k, optimal tree decompositions can be computed in linear time (Bodlaender 1996). 234 Gildea Grammar Factorization by Tree Decomposition Figure 4 A tree decomposition of a graph is a set of overlapping clusters of the graph’s vertices, arranged in a tree. This example has treewidth = 2. We can factorize a deduction rule by representing the rule as a graph, which we call a dependency graph, and searching for tree decompositions of this graph. For a rule r having n variables V = {vi |i E {1,..., n}}, m antecedent items Ai, i E {1, ..., m}</context>
<context position="40540" citStr="Amir (2001)" startWordPosition="6902" endWordPosition="6903"> from Equation (7). Putting these together: SOL &lt; 2∆(G&apos;)tw(G&apos;) and using Equations (4) and (5) to relate our result to the original graph G, SOL &lt; 4∆(G)tw(G) This last inequality proves the main result of this section Theorem 2 An algorithm for finding the optimal parsing strategy of an arbitrary LCFRS production would imply a 4∆(G) approximation algorithm for treewidth. Whether such an approximation algorithm for treewidth is possible is an open problem. The best-known result is the O(\/log k) approximation result of Feige, Hajiaghayi, and Lee (2005), which improves on the O(log k) result of Amir (2001). This indicates that, although polynomial-time factorization of LCFRS rules to optimize parsing complexity may be possible, it would require progress on general algorithms for treewidth. 5. Conclusion We have demonstrated that a number of techniques used for specific parsing problems can be found algorithmically from declarative specifications of the grammar. Our method involves finding the optimal tree decomposition of a graph, which is in general an NP-complete problem. However, the relation to tree decomposition allows us to exploit existing algorithms for this problem, such as the linear </context>
</contexts>
<marker>Amir, 2001</marker>
<rawString>Amir, Eyal. 2001. Efficient approximation for triangulation of minimum treewidth. In 17th Conference on Uncertainty in Artificial Intelligence, pages 7–15, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefen Arnborg</author>
<author>Derek G Corneil</author>
<author>Andrzej Proskurowski</author>
</authors>
<title>Complexity of finding embeddings in a k-tree.</title>
<date>1987</date>
<journal>SIAM Journal of Algebraic and Discrete Methods,</journal>
<pages>8--277</pages>
<marker>Arnborg, Corneil, Proskurowski, 1987</marker>
<rawString>Arnborg, Stefen, Derek G. Corneil, and Andrzej Proskurowski. 1987. Complexity of finding embeddings in a k-tree. SIAM Journal of Algebraic and Discrete Methods, 8:277–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars. Zeitschrift f¨ur Phonetik, Sprachwissenschaft und Kommunikationsforschung,</title>
<date>1961</date>
<pages>14--143</pages>
<location>MA,</location>
<note>Reprinted in</note>
<marker>Bar-Hillel, Perles, Shamir, 1961</marker>
<rawString>Bar-Hillel, Yehoshua, M. Perles, and E. Shamir. 1961. On formal properties of simple phrase structure grammars. Zeitschrift f¨ur Phonetik, Sprachwissenschaft und Kommunikationsforschung, 14:143–172. Reprinted in Y. Bar-Hillel. (1964). Language and Information: Selected Essays on Their Theory and Application, Addison-Wesley Reading, MA, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Bodlaender</author>
</authors>
<title>A linear time algorithm for finding tree decompositions of small treewidth.</title>
<date>1996</date>
<journal>SIAM Journal on Computing,</journal>
<pages>25--1305</pages>
<contexts>
<context position="11938" citStr="Bodlaender 1996" startWordPosition="1983" endWordPosition="1984">ote that the vertex N appears in three adjacent nodes of the tree decomposition. Finding the treewidth of a graph is an NPcomplete problem (Arnborg, Corneil, and Proskurowski 1987). However, given a graph of n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition in time O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approximation algorithms and heuristics are known for the treewidth problem (Bodlaender et al. 1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005). Furthermore, for fixed k, optimal tree decompositions can be computed in linear time (Bodlaender 1996). 234 Gildea Grammar Factorization by Tree Decomposition Figure 4 A tree decomposition of a graph is a set of overlapping clusters of the graph’s vertices, arranged in a tree. This example has treewidth = 2. We can factorize a deduction rule by representing the rule as a graph, which we call a dependency graph, and searching for tree decompositions of this graph. For a rule r having n variables V = {vi |i E {1,..., n}}, m antecedent items Ai, i E {1, ..., m}, and consequent C, let V(Ai) ⊂ V be the variables appearing in antecedent Ai, and V(C) be the variables appearing in the consequent. The </context>
<context position="18180" citStr="Bodlaender (1996)" startWordPosition="3046" endWordPosition="3047"> wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identified in time O(fk+2) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again f is the number of variables in the input rules), or in time O(f) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in the rule dependencies, but the semiring meets Knuth’s (1977) definition of a superior function, parsing takes time O(M log M), where M is the number of rule instantiations, and the extra log M term accounts for maintaining an agenda as a priority queue (Nederhof 2003). Cycles in the rule dependencies may arise, for example, from chains of un</context>
<context position="41175" citStr="Bodlaender (1996)" startWordPosition="6995" endWordPosition="6996">hat, although polynomial-time factorization of LCFRS rules to optimize parsing complexity may be possible, it would require progress on general algorithms for treewidth. 5. Conclusion We have demonstrated that a number of techniques used for specific parsing problems can be found algorithmically from declarative specifications of the grammar. Our method involves finding the optimal tree decomposition of a graph, which is in general an NP-complete problem. However, the relation to tree decomposition allows us to exploit existing algorithms for this problem, such as the linear time algorithm of Bodlaender (1996) for graphs of bounded treewidth. In practice, grammar rules are 246 Gildea Grammar Factorization by Tree Decomposition typically small, and finding the tree decomposition is not computationally expensive, and in fact is trivial in comparison to the original parsing problem. Given the special structure of the graphs derived from LCFRS productions, however, we have explored whether finding optimal tree decompositions of these graphs, and therefore optimal parsing strategies for LCFRS productions, is also NP-complete. Although a polynomial time algorithm for this problem would not necessarily im</context>
</contexts>
<marker>Bodlaender, 1996</marker>
<rawString>Bodlaender, H. L. 1996. A linear time algorithm for finding tree decompositions of small treewidth. SIAM Journal on Computing, 25:1305–1317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans L Bodlaender</author>
<author>John R Gilbert</author>
<author>Hj´almt´yr Hafsteinsson</author>
<author>Ton Kloks</author>
</authors>
<title>Approximating treewidth, pathwidth, frontsize, and shortest elimination tree.</title>
<date>1995</date>
<journal>Journal of Algorithms,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="11789" citStr="Bodlaender et al. 1995" startWordPosition="1959" endWordPosition="1962">dicates some treelike structure in the graph, as shown by the example with treewidth = 2 in Figure 4. As an example of the running intersection property, note that the vertex N appears in three adjacent nodes of the tree decomposition. Finding the treewidth of a graph is an NPcomplete problem (Arnborg, Corneil, and Proskurowski 1987). However, given a graph of n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition in time O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approximation algorithms and heuristics are known for the treewidth problem (Bodlaender et al. 1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005). Furthermore, for fixed k, optimal tree decompositions can be computed in linear time (Bodlaender 1996). 234 Gildea Grammar Factorization by Tree Decomposition Figure 4 A tree decomposition of a graph is a set of overlapping clusters of the graph’s vertices, arranged in a tree. This example has treewidth = 2. We can factorize a deduction rule by representing the rule as a graph, which we call a dependency graph, and searching for tree decompositions of this graph. For a rule r having n variables V = {vi |i E {1,..., n}}, m antecedent items Ai, i E </context>
</contexts>
<marker>Bodlaender, Gilbert, Hafsteinsson, Kloks, 1995</marker>
<rawString>Bodlaender, Hans L., John R. Gilbert, Hj´almt´yr Hafsteinsson, and Ton Kloks. 1995. Approximating treewidth, pathwidth, frontsize, and shortest elimination tree. Journal of Algorithms, 18(2):238–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chandra Chekuri</author>
<author>Anand Rajaraman</author>
</authors>
<title>Conjunctive query containment revisited.</title>
<date>1997</date>
<booktitle>In Database Theory – ICDT ’97,</booktitle>
<volume>1186</volume>
<pages>56--70</pages>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="19742" citStr="Chekuri and Rajaraman 1997" startWordPosition="3284" endWordPosition="3287">As in the acyclic case, M = O(nk) for parsing problems where rules have at most k variables. Under the assumption of superior functions, parsing takes time O(nkklogn) with Knuth’s algorithm. In this setting, as in the acyclic case, minimizing k with tree decomposition minimizes parsing complexity. 2.3 Related Applications of Treewidth The technique of using treewidth to minimize complexity has been applied to constraint satisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen, Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization for databases (Chekuri and Rajaraman 1997). Our formulation of parsing is most closely related to logic programming; in this area treewidth has been applied to limit complexity in settings where either the deduction rules or the input database of ground facts have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe (2002) apply treewidth to nonrecursive datalog programs, our parsing programs have unbounded recursion, as the depth of the parse tree is not fixed in advance. Our results for parsing can be seen as a consequence of the fact that, even in the case of unbounded recursion, the complexity of (unweight</context>
</contexts>
<marker>Chekuri, Rajaraman, 1997</marker>
<rawString>Chekuri, Chandra and Anand Rajaraman. 1997. Conjunctive query containment revisited. In Database Theory – ICDT ’97, volume 1186 of Lecture Notes in Computer Science. Springer, Berlin, pages 56–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="24319" citStr="Chiang 2007" startWordPosition="4050" endWordPosition="4051">d word h. Placing an edge between each pair of variable nodes that share a factor, we get Figure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c, each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest node of the tree decomposition has four variables, giving the O(n4) algorithm of Eisner and Satta (1999). 3.3 SCFG Parsing Strategies SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure simultaneously, and have become widely used as statistical models of machine translation (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one Figure 7 Treewidth applied to bilexicalized parsing. 239 Computational Linguistics Volume 37, Number 1 lefthand side nonterminal and two righthand side strings. Nonterminals in the two strings are linked with superscript indices; symbols with the same index must be further rewritten synchronously. For example, X , A(1) B(2) C(3) D(4), A(1) B(2) C(3) D(4) (1) is a rule with four children and no reordering, whereas X , A(1) B(2) C(3) D(4), B(2) D(4) A(1) C(3) (2) expresses a more complex reordering. In general, we can take indices in the first righth</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rina Dechter</author>
<author>Judea Pearl</author>
</authors>
<title>Tree clustering for constraint networks.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="19576" citStr="Dechter and Pearl 1989" startWordPosition="3261" endWordPosition="3264">his property, assuming that all rule weights are less than one, whereas for exact computation with the sum-product semiring, unbounded chains must be considered. As in the acyclic case, M = O(nk) for parsing problems where rules have at most k variables. Under the assumption of superior functions, parsing takes time O(nkklogn) with Knuth’s algorithm. In this setting, as in the acyclic case, minimizing k with tree decomposition minimizes parsing complexity. 2.3 Related Applications of Treewidth The technique of using treewidth to minimize complexity has been applied to constraint satisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen, Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization for databases (Chekuri and Rajaraman 1997). Our formulation of parsing is most closely related to logic programming; in this area treewidth has been applied to limit complexity in settings where either the deduction rules or the input database of ground facts have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe (2002) apply treewidth to nonrecursive datalog programs, our parsing programs have unbounded recursion, as the depth of the parse tre</context>
</contexts>
<marker>Dechter, Pearl, 1989</marker>
<rawString>Dechter, Rina and Judea Pearl. 1989. Tree clustering for constraint networks. Artificial Intelligence, 38(3):353–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>John Blatz</author>
</authors>
<title>Program transformations for optimization of parsing algorithms and other weighted logic programs.</title>
<date>2007</date>
<booktitle>In Shuly Wintner, editor, Proceedings of FG 2006: The 11th Conference on Formal Grammar. CSLI Publications,</booktitle>
<pages>45--85</pages>
<contexts>
<context position="8189" citStr="Eisner and Blatz (2007)" startWordPosition="1330" endWordPosition="1333">ation as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This definition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold operation of fold/unfold. If we allow unrestricted transformations of the input deduction system, finding the most efficient equivalent system is undecidable; this follows from the fact that it is undecidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir 1961), and would therefore be recognizable in constant time. Whereas the fold/unfold operation of Johnson (2007) and Eisner and Blatz (2007) specifies a narrower class of 233 Computational Linguistics Volume 37, Number 1 grammar transformations, no general algorith</context>
</contexts>
<marker>Eisner, Blatz, 2007</marker>
<rawString>Eisner, Jason and John Blatz. 2007. Program transformations for optimization of parsing algorithms and other weighted logic programs. In Shuly Wintner, editor, Proceedings of FG 2006: The 11th Conference on Formal Grammar. CSLI Publications, pages 45–85, Malaga.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Conference of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>457--464</pages>
<location>College Park, MD.</location>
<contexts>
<context position="4655" citStr="Eisner and Satta (1999)" startWordPosition="724" endWordPosition="727">ule. We refer to this process as factorization. One straightforward example of rule factorization is the binarization of a CFG, as shown in Figure 2. Given a deduction rule for a CFG rule with r nonterminals on the righthand side, and a total of r + 1 variables, an equivalent set of rules can be produced, each with three variables, storing intermediate results that indicate that a substring of the original rule’s righthand side has been recognized. This type of rule factorization produces an O(n3) parser for any input CFG. Another well-known instance of rule factorization is the hook trick of Eisner and Satta (1999), which reduces the complexity of parsing for bilexicalized CFGs from O(n5) to O(n4). The basic rule for bilexicalized parsing combines two CFG constituents marked with lexical heads as shown in Figure 3a. Here items with type C indicate constituents, with [C, x0, h, x1] indicating a constituent extending from position x0 to position x1, headed by the word at position h. The item [D, m → h] is used to indicate the weight assigned by the grammar to a bilexical dependency headed by the word at a) w1: [A, x0, x1] w2: [B, x1, x2] w3: [C, x2, x3] w4: [D, x3, x4] w0w1w2w3w4: [S, x0, x4] b) w1: [A, x</context>
<context position="24082" citStr="Eisner and Satta (1999)" startWordPosition="4014" endWordPosition="4017">rive the weight of the new constituent. The new constituent is represented by a consequent node at the top of the graph—the variables that will be relevant for its further combination with other constituents are its end points x0 and x2 and its head word h. Placing an edge between each pair of variable nodes that share a factor, we get Figure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c, each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest node of the tree decomposition has four variables, giving the O(n4) algorithm of Eisner and Satta (1999). 3.3 SCFG Parsing Strategies SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure simultaneously, and have become widely used as statistical models of machine translation (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one Figure 7 Treewidth applied to bilexicalized parsing. 239 Computational Linguistics Volume 37, Number 1 lefthand side nonterminal and two righthand side strings. Nonterminals in the two strings are linked with superscript indices; symbols with the same index must be further rewritten synchronously. For example, X , A</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Eisner, Jason and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Conference of the Association for Computational Linguistics (ACL-99), pages 457–464, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>A faster parsing algorithm for lexicalized tree-adjoining grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5),</booktitle>
<pages>14--19</pages>
<location>Paris.</location>
<contexts>
<context position="6193" citStr="Eisner and Satta 2000" startWordPosition="1017" endWordPosition="1020">ctorization by Tree Decomposition a) wt: [D, m → h] w1: [C, x0, h, x1] w2: [C, x1, m, x2] wtw1w2: [C,x0,h,x2] b) wt: [D, m → h] wh: [H, h, x1, x2] w2: [C, x1, m, x2] w1: [C, x0, h, x1] w2w2: [H,h,x1,x2] whw1: [C, x0, h, x2] Figure 3 Rule factorization for bilexicalized parsing. position h with the word at position m as a modifier. The deduction rule is broken into two steps, one which includes the weight for the bilexical grammar rule, and another which identifies the boundaries of the new constituent, as shown in Figure 3b. The hook trick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000), and has been generalized to improve the complexity of machine translation decoding under synchronous context-free grammars (SCFGs) with an n-gram language model (Huang, Zhang, and Gildea 2005). Rule factorization has also been studied in the context of parsing for SCFGs. Unlike monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation between nonterminals in the two languages, it may or may not be possible to reduce the rank, or number of nonterminals on the righthand side, of a rule. Algorithms for finding the optimal rank reduction of a specific rule are given by Zha</context>
</contexts>
<marker>Eisner, Satta, 2000</marker>
<rawString>Eisner, Jason and Giorgio Satta. 2000. A faster parsing algorithm for lexicalized tree-adjoining grammars. In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5), pages 14–19, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uriel Feige</author>
<author>MohammadTaghi Hajiaghayi</author>
<author>James R Lee</author>
</authors>
<title>Improved approximation algorithms for minimum-weight vertex separators.</title>
<date>2005</date>
<booktitle>In STOC ’05: Proceedings of the thirty-seventh annual ACM symposium on Theory of computing,</booktitle>
<pages>563--572</pages>
<location>Baltimore, MD.</location>
<marker>Feige, Hajiaghayi, Lee, 2005</marker>
<rawString>Feige, Uriel, MohammadTaghi Hajiaghayi, and James R. Lee. 2005. Improved approximation algorithms for minimum-weight vertex separators. In STOC ’05: Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 563–572, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Flum</author>
<author>Markus Frick</author>
<author>Martin Grohe</author>
</authors>
<title>Query evaluation via tree-decompositions.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>6</issue>
<marker>Flum, Frick, Grohe, 2002</marker>
<rawString>Flum, J¨org, Markus Frick, and Martin Grohe. 2002. Query evaluation via tree-decompositions. Journal of the ACM, 49(6):716–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04),</booktitle>
<pages>273--280</pages>
<location>Boston, MA.</location>
<contexts>
<context position="24305" citStr="Galley et al. 2004" startWordPosition="4046" endWordPosition="4049">0 and x2 and its head word h. Placing an edge between each pair of variable nodes that share a factor, we get Figure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c, each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest node of the tree decomposition has four variables, giving the O(n4) algorithm of Eisner and Satta (1999). 3.3 SCFG Parsing Strategies SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure simultaneously, and have become widely used as statistical models of machine translation (Galley et al. 2004; Chiang 2007). We write SCFG rules as productions with one Figure 7 Treewidth applied to bilexicalized parsing. 239 Computational Linguistics Volume 37, Number 1 lefthand side nonterminal and two righthand side strings. Nonterminals in the two strings are linked with superscript indices; symbols with the same index must be further rewritten synchronously. For example, X , A(1) B(2) C(3) D(4), A(1) B(2) C(3) D(4) (1) is a rule with four children and no reordering, whereas X , A(1) B(2) C(3) D(4), B(2) D(4) A(1) C(3) (2) expresses a more complex reordering. In general, we can take indices in th</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the 2004 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04), pages 273–280, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Optimal parsing strategies for Linear Context-Free Rewriting Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-10),</booktitle>
<pages>769--776</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7546" citStr="Gildea 2010" startWordPosition="1228" endWordPosition="1229">ule factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to define a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to reduce rank without increasing grammar fan-out (G´omez-Rodr´ıguez et al. 2009), as well as factorization with the goal of directly minimizing the parsing complexity of the new grammar (Gildea 2010). We define factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This definition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by</context>
<context position="17906" citStr="Gildea 2010" startWordPosition="3000" endWordPosition="3001">he unfactorized grammar is feasible. This is because, for an input rule with f variables, parsing is O(n�) in the sentence length n. The treewidth of this rule is at most f − 1, and can be computed in time O(&amp;1); generally we expect n to be greater than f. One may also wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identified in time O(fk+2) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again f is the number of variables in the input rules), or in time O(f) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in the rule dependencies, but the semiring meets Knuth’s (1977) definiti</context>
<context position="28278" citStr="Gildea (2010)" startWordPosition="4691" endWordPosition="4692"> formalisms in natural language processing, including CFG, TAG, SCFG, and synchronous TAG. LCFRS has also been used to model non-projective dependency grammars, and the LCFRS rules extracted from dependency treebanks can be quite complex (Kuhlmann and Satta 2009), making factorization important. Similarly, LCFRS can model translation relations beyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammars extracted from word-aligned bilingual corpora can also be quite complex (Wellington, Waxmonsky, and Melamed 2006). An algorithm for factorization of LCFRS rules is presented by Gildea (2010), exploiting specific properties of LCFRS. The tree decomposition method achieves the same results without requiring analysis specific to LCFRS. In this section, we examine the complexity of rule factorization for general LCFRS grammars. The problem of finding the optimal factorization of an arbitrary deduction rule is NP-complete. This follows from the NP-completeness of treewidth using the following construction: Given a graph, create a deduction rule with a variable for each vertex in the graph and an antecedent for each edge, containing the two variables associated with the edge’s endpoint</context>
</contexts>
<marker>Gildea, 2010</marker>
<rawString>Gildea, Daniel. 2010. Optimal parsing strategies for Linear Context-Free Rewriting Systems. In Proceedings of the 2010 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-10), pages 769–776, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel ˇStefankoviˇc</author>
</authors>
<title>Worst-case synchronous grammar rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-07),</booktitle>
<pages>147--154</pages>
<location>Rochester, NY.</location>
<marker>Gildea, ˇStefankoviˇc, 2007</marker>
<rawString>Gildea, Daniel and Daniel ˇStefankoviˇc. 2007. Worst-case synchronous grammar rules. In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-07), pages 147–154, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David Weir</author>
</authors>
<title>Optimal reduction of rule length in Linear Context-Free Rewriting Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-09),</booktitle>
<pages>539--547</pages>
<location>Boulder, CO.</location>
<marker>G´omez-Rodriguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>G´omez-Rodriguez, Carlos, Marco Kuhlmann, Giorgio Satta, and David Weir. 2009. Optimal reduction of rule length in Linear Context-Free Rewriting Systems. In Proceedings of the 2009 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-09), pages 539–547, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1153" citStr="Goodman 1999" startWordPosition="164" endWordPosition="165">ee Rewriting Systems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs. 1. Introduction In this article, we describe meta-algorithms for parsing: algorithms for finding the optimal parsing algorithm for a given grammar, with the constraint that rules in the grammar are considered independently of one another. In order to have a common representation for our algorithms to work with, we represent parsing algorithms as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999; Nederhof 2003). Weighted deduction systems consist of axioms and rules for building items or partial results. Items are identified by square brackets, with their weights written to the left. Figure 1 shows a rule for deducing a new item when parsing a context free grammar (CFG) with the rule S → A B. The item below the line, called the consequent, can be derived if the two items above the line, called the antecedents, have been derived. Items have types, corresponding to grammar nonterminals in this example, and variables, whose values range over positions in the string to be parsed. We rest</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In International Workshop on Parsing Technologies (IWPT05),</booktitle>
<pages>65--73</pages>
<location>Vancouver.</location>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>Huang, Liang, Hao Zhang, and Daniel Gildea. 2005. Machine translation as lexicalized parsing with hooks. In International Workshop on Parsing Technologies (IWPT05), pages 65–73, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Binarization of synchronous context-free grammars.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="17838" citStr="Huang et al. 2009" startWordPosition="2988" endWordPosition="2991">wever, optimal factorization is generally feasible whenever parsing with the unfactorized grammar is feasible. This is because, for an input rule with f variables, parsing is O(n�) in the sentence length n. The treewidth of this rule is at most f − 1, and can be computed in time O(&amp;1); generally we expect n to be greater than f. One may also wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identified in time O(fk+2) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again f is the number of variables in the input rules), or in time O(f) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in t</context>
</contexts>
<marker>Huang, Zhang, Gildea, Knight, 2009</marker>
<rawString>Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin Knight. 2009. Binarization of synchronous context-free grammars. Computational Linguistics, 35(4):559–595.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Finn V Jensen</author>
<author>Steffen L Lauritzen</author>
<author>Kristian G Olesen 1990</author>
</authors>
<title>Bayesian updating in causal probabilistic networks by local computations.</title>
<journal>Computational Statistics Quarterly,</journal>
<pages>4--269</pages>
<marker>Jensen, Lauritzen, 1990, </marker>
<rawString>Jensen, Finn V., Steffen L. Lauritzen, and Kristian G. Olesen.1990. Bayesian updating in causal probabilistic networks by local computations. Computational Statistics Quarterly, 4:269–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>168--175</pages>
<location>Prague.</location>
<contexts>
<context position="8161" citStr="Johnson (2007)" startWordPosition="1327" endWordPosition="1328"> We define factorization as a process which applies to rules of the input grammar independently. Individual rules are replaced with an equivalent set of new rules, which must derive the same set of consequent items as the original rule given the same antecedent items. While new intermediate items of distinct types may be produced, the set of items and weights derived by the original weighted deduction system is unchanged. This definition of factorization is broad enough to include all of the previous examples, but does not include, for example, the fold/unfold operation applied to grammars by Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold operation of fold/unfold. If we allow unrestricted transformations of the input deduction system, finding the most efficient equivalent system is undecidable; this follows from the fact that it is undecidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir 1961), and would therefore be recognizable in constant time. Whereas the fold/unfold operation of Johnson (2007) and Eisner and Blatz (2007) specifies a narrower class of 233 Computational Linguistics Volume 37, Number 1 grammar transfor</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Johnson, Mark. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168–175, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Knuth</author>
</authors>
<title>A generalization of Dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<issue>1</issue>
<marker>Knuth, 1977</marker>
<rawString>Knuth, D. 1977. A generalization of Dijkstra’s algorithm. Information Processing Letters, 6(1):1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Kschischang</author>
<author>B J Frey</author>
<author>H A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>47</volume>
<issue>2</issue>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>Kschischang, F. R., B. J. Frey, and H. A. Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47(2):498–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>507--514</pages>
<location>Sydney.</location>
<contexts>
<context position="17892" citStr="Kuhlmann and Nivre 2006" startWordPosition="2996" endWordPosition="2999">e whenever parsing with the unfactorized grammar is feasible. This is because, for an input rule with f variables, parsing is O(n�) in the sentence length n. The treewidth of this rule is at most f − 1, and can be computed in time O(&amp;1); generally we expect n to be greater than f. One may also wish to accept only rules having treewidth k and disregard the remainder, for example, when factorizing rules automatically 236 Gildea Grammar Factorization by Tree Decomposition extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006; Huang et al. 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea 2010). In this setting, the rules having treewidth k can be identified in time O(fk+2) using the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again f is the number of variables in the input rules), or in time O(f) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in the rule dependencies, but the semiring meets Knuth’s (</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Kuhlmann, Marco and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), pages 507–514, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09),</booktitle>
<pages>478--486</pages>
<location>Athens.</location>
<contexts>
<context position="27928" citStr="Kuhlmann and Satta 2009" startWordPosition="4641" endWordPosition="4644">e more than one span in each language. These intermediate deduction steps do, however, correspond to LCFRS rules. We now turn to examine LCFRS in more detail. 240 Gildea Grammar Factorization by Tree Decomposition Figure 8 Treewidth applied to the SCFG rule of Equation (2). 4. LCFRS Parsing Strategies LCFRS provides a generalization of a number of widely used formalisms in natural language processing, including CFG, TAG, SCFG, and synchronous TAG. LCFRS has also been used to model non-projective dependency grammars, and the LCFRS rules extracted from dependency treebanks can be quite complex (Kuhlmann and Satta 2009), making factorization important. Similarly, LCFRS can model translation relations beyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammars extracted from word-aligned bilingual corpora can also be quite complex (Wellington, Waxmonsky, and Melamed 2006). An algorithm for factorization of LCFRS rules is presented by Gildea (2010), exploiting specific properties of LCFRS. The tree decomposition method achieves the same results without requiring analysis specific to LCFRS. In this section, we examine the complexity of rule factorization for general LCFRS grammars. The problem </context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Kuhlmann, Marco and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09), pages 478–486, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
</authors>
<title>On the complexity analysis of static analyses.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="20436" citStr="McAllester 2002" startWordPosition="3398" endWordPosition="3399">this area treewidth has been applied to limit complexity in settings where either the deduction rules or the input database of ground facts have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe (2002) apply treewidth to nonrecursive datalog programs, our parsing programs have unbounded recursion, as the depth of the parse tree is not fixed in advance. Our results for parsing can be seen as a consequence of the fact that, even in the case of unbounded recursion, the complexity of (unweighted) datalog programs is linear in the number of possible rule instantiations (McAllester 2002). 3. Examples of Treewidth for Parsing In this section, we show how a few well-known parsing algorithms can be derived automatically by finding the optimal tree decomposition of a dependency graph. To aid in visualization of the graphical representation of deduction rules, we use a factor graph representation based on that of Kschischang, Frey, and Loeliger (2001) for Markov Random Fields. Our graphs have three types of nodes: variables, antecedents, and consequents. Each antecedent node is connected to the variables it contains, and represents the antecedent’s weight as a function of those va</context>
</contexts>
<marker>McAllester, 2002</marker>
<rawString>McAllester, David. 2002. On the complexity analysis of static analyses. Journal of the ACM, 49(4):512–537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Ben Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04),</booktitle>
<pages>661--668</pages>
<location>Barcelona.</location>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>Melamed, I. Dan, Giorgio Satta, and Ben Wellington. 2004. Generalized multitext grammars. In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04), pages 661–668, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1169" citStr="Nederhof 2003" startWordPosition="166" endWordPosition="167">ystems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs. 1. Introduction In this article, we describe meta-algorithms for parsing: algorithms for finding the optimal parsing algorithm for a given grammar, with the constraint that rules in the grammar are considered independently of one another. In order to have a common representation for our algorithms to work with, we represent parsing algorithms as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999; Nederhof 2003). Weighted deduction systems consist of axioms and rules for building items or partial results. Items are identified by square brackets, with their weights written to the left. Figure 1 shows a rule for deducing a new item when parsing a context free grammar (CFG) with the rule S → A B. The item below the line, called the consequent, can be derived if the two items above the line, called the antecedents, have been derived. Items have types, corresponding to grammar nonterminals in this example, and variables, whose values range over positions in the string to be parsed. We restrict ourselves t</context>
<context position="18705" citStr="Nederhof 2003" startWordPosition="3131" endWordPosition="3132">f variables in the input rules), or in time O(f) using the algorithm of Bodlaender (1996). 2.2 Cyclic Dependencies Although this article primarily addresses the case where there are no cyclic dependencies between rule instantiations, we note here that our techniques carry over to the cyclic case under certain conditions. If there are cycles in the rule dependencies, but the semiring meets Knuth’s (1977) definition of a superior function, parsing takes time O(M log M), where M is the number of rule instantiations, and the extra log M term accounts for maintaining an agenda as a priority queue (Nederhof 2003). Cycles in the rule dependencies may arise, for example, from chains of unary productions in a CFG; the properties of superior functions guarantee that unbounded chains need not be considered. The max-product semiring used in Viterbi parsing has this property, assuming that all rule weights are less than one, whereas for exact computation with the sum-product semiring, unbounded chains must be considered. As in the acyclic case, M = O(nk) for parsing problems where rules have at most k variables. Under the assumption of superior functions, parsing takes time O(nkklogn) with Knuth’s algorithm.</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Nederhof, M.-J. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, 29(1):135–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Giorgio Satta</author>
</authors>
<title>Independent parallelism in finite copying parallel rewriting systems.</title>
<date>1999</date>
<journal>Theoretical Computer Science,</journal>
<pages>223--1</pages>
<contexts>
<context position="30813" citStr="Rambow and Satta 1999" startWordPosition="5121" endWordPosition="5124">nonterminal, and then, bottom–up, applying the functions associated with each production to build the string. As an example, the CFG S - 4AB A - 4a B - 4b corresponds to the following grammar in LCFRS notation: S -4 gS(A,B) gS((sA), (sB)) = (sAsB) A -4 gA() gA() = (a) B -4 gB() gB() = (b) Here, all nonterminals have fan-out = 1, reflected in the fact that all tuples defining the productions’ functions contain just one string. As CFG is equivalent to LCFRS with fanout = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2. Higher values of fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomialtime parsing is possible for any fixed LCFRS grammar, but the degree of the polynomial depends on the grammar. Parsing general LCFRS grammars, where the grammar is considered part of the input, is NP-complete (Satta 1992). 4.1 Graphs Derived from LCFRS Rules Given an LCFRS rule as defined previously, a weighted deduction rule for a bottom– up parser can be derived by creating an antecedent for each righthand nonterminal, a consequent for the lefthand side, and variables for all the boundaries of the nonterminals in the rule. A nonterminal of fan-out f has 2f boundaries. Each bounda</context>
</contexts>
<marker>Rambow, Satta, 1999</marker>
<rawString>Rambow, Owen and Giorgio Satta. 1999. Independent parallelism in finite copying parallel rewriting systems. Theoretical Computer Science, 223(1-2):87–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Recognition of Linear Context-Free Rewriting Systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Conference of the Association for Computational Linguistics (ACL-92),</booktitle>
<pages>89--95</pages>
<location>Newark, DE.</location>
<contexts>
<context position="31046" citStr="Satta 1992" startWordPosition="5160" endWordPosition="5161">sB) A -4 gA() gA() = (a) B -4 gB() gB() = (b) Here, all nonterminals have fan-out = 1, reflected in the fact that all tuples defining the productions’ functions contain just one string. As CFG is equivalent to LCFRS with fanout = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2. Higher values of fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomialtime parsing is possible for any fixed LCFRS grammar, but the degree of the polynomial depends on the grammar. Parsing general LCFRS grammars, where the grammar is considered part of the input, is NP-complete (Satta 1992). 4.1 Graphs Derived from LCFRS Rules Given an LCFRS rule as defined previously, a weighted deduction rule for a bottom– up parser can be derived by creating an antecedent for each righthand nonterminal, a consequent for the lefthand side, and variables for all the boundaries of the nonterminals in the rule. A nonterminal of fan-out f has 2f boundaries. Each boundary 242 Gildea Grammar Factorization by Tree Decomposition variable will occur exactly twice in the deduction rule: either in two antecedents, if two nonterminals on the rule’s righthand side are adjacent, or once in an antecedent and</context>
</contexts>
<marker>Satta, 1992</marker>
<rawString>Satta, Giorgio. 1992. Recognition of Linear Context-Free Rewriting Systems. In Proceedings of the 30th Annual Conference of the Association for Computational Linguistics (ACL-92), pages 89–95, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Shafer</author>
<author>P Shenoy</author>
</authors>
<title>Probability propagation.</title>
<date>1990</date>
<journal>Annals of Mathematics and Artificial Intelligence,</journal>
<pages>2--327</pages>
<contexts>
<context position="19675" citStr="Shafer and Shenoy 1990" startWordPosition="3275" endWordPosition="3278">the sum-product semiring, unbounded chains must be considered. As in the acyclic case, M = O(nk) for parsing problems where rules have at most k variables. Under the assumption of superior functions, parsing takes time O(nkklogn) with Knuth’s algorithm. In this setting, as in the acyclic case, minimizing k with tree decomposition minimizes parsing complexity. 2.3 Related Applications of Treewidth The technique of using treewidth to minimize complexity has been applied to constraint satisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen, Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization for databases (Chekuri and Rajaraman 1997). Our formulation of parsing is most closely related to logic programming; in this area treewidth has been applied to limit complexity in settings where either the deduction rules or the input database of ground facts have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe (2002) apply treewidth to nonrecursive datalog programs, our parsing programs have unbounded recursion, as the depth of the parse tree is not fixed in advance. Our results for parsing can be seen as a consequence of the fact that, e</context>
</contexts>
<marker>Shafer, Shenoy, 1990</marker>
<rawString>Shafer, G. and P. Shenoy. 1990. Probability propagation. Annals of Mathematics and Artificial Intelligence, 2:327–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>The Journal of Logic Programming,</journal>
<pages>24--1</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. The Journal of Logic Programming, 24(1-2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shankar</author>
<author>D L Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics (ACL-87),</booktitle>
<pages>104--111</pages>
<location>Stanford, CA.</location>
<marker>Vijay-Shankar, Weir, Joshi, 1987</marker>
<rawString>Vijay-Shankar, K., D. L. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics (ACL-87), pages 104–111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>977--984</pages>
<location>Sydney.</location>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Wellington, Benjamin, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), pages 977–984, Sydney.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekai 1997 Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Wu, </marker>
<rawString>Wu, Dekai.1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Factorization of synchronous context-free grammars in linear time.</title>
<date>2007</date>
<booktitle>In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST),</booktitle>
<pages>25--32</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="6813" citStr="Zhang and Gildea (2007)" startWordPosition="1115" endWordPosition="1118">00), and has been generalized to improve the complexity of machine translation decoding under synchronous context-free grammars (SCFGs) with an n-gram language model (Huang, Zhang, and Gildea 2005). Rule factorization has also been studied in the context of parsing for SCFGs. Unlike monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation between nonterminals in the two languages, it may or may not be possible to reduce the rank, or number of nonterminals on the righthand side, of a rule. Algorithms for finding the optimal rank reduction of a specific rule are given by Zhang and Gildea (2007). The complexity of synchronous parsing for a rule of rank r is O(n2r+2), so reducing rank improves parsing complexity. Rule factorization has also been applied to Linear Context-Free Rewriting Systems (LCFRS), which generalize CFG, TAG, and SCFG to define a rewriting system where nonterminals may have arbitrary fan-out, which indicates the number of continuous spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987). Recent work has examined the problem of factorization of LCFRS rules in order to reduce rank without increasing grammar fan-out (G´omez-Rodr´ıgue</context>
</contexts>
<marker>Zhang, Gildea, 2007</marker>
<rawString>Zhang, Hao and Daniel Gildea. 2007. Factorization of synchronous context-free grammars in linear time. In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25–32, Rochester, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>