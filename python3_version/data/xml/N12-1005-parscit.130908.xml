<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001950">
<note confidence="0.91734825">
Continuous Space Translation Models with Neural Networks
Le Hai Son and Alexandre Allauzen and Franc¸ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
rue John von Neumann, 91403 Orsay cedex, France
</note>
<email confidence="0.987397">
Firstname.Lastname@limsi.fr
</email>
<sectionHeader confidence="0.996514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999552421052632">
The use of conventional maximum likelihood
estimates hinders the performance of existing
phrase-based translation models. For lack of
sufficient training data, most models only con-
sider a small amount of context. As a par-
tial remedy, we explore here several contin-
uous space translation models, where transla-
tion probabilities are estimated using a con-
tinuous representation of translation units in
lieu of standard discrete representations. In
order to handle a large set of translation units,
these representations and the associated esti-
mates are jointly computed using a multi-layer
neural network with a SOUL architecture. In
small scale and large scale English to French
experiments, we show that the resulting mod-
els can effectively be trained and used on top
of a n-gram translation system, delivering sig-
nificant improvements in performance.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999556">
The phrase-based approach to statistical machine
translation (SMT) is based on the following infer-
ence rule, which, given a source sentence s, selects
the target sentence t and the underlying alignment a
maximizing the following term:
</bodyText>
<equation confidence="0.978637666666667">
K
P(t, a|s) = Z(s) exp ( 1: Akfk(s, t, a)), (1)
k��
</equation>
<bodyText confidence="0.99893175">
where K feature functions (fk) are weighted by a
set of coefficients (Ak), and Z is a normalizing fac-
tor. The phrase-based approach differs from other
approaches by the hidden variables of the translation
</bodyText>
<page confidence="0.990542">
39
</page>
<bodyText confidence="0.999849">
process: the segmentation of a parallel sentence pair
into phrase pairs and the associated phrase align-
ments.
This formulation was introduced in (Zens et al.,
2002) as an extension of the word based mod-
els (Brown et al., 1993), then later motivated within
a discriminative framework (Och and Ney, 2004).
One motivation for integrating more feature func-
tions was to improve the estimation of the translation
model P(tIs), which was initially based on relative
frequencies, thus yielding poor estimates.
This is because the units of phrase-based mod-
els are phrase pairs, made of a source and a tar-
get phrase; such units are viewed as the events of
discrete random variables. The resulting representa-
tions of phrases (or words) thus entirely ignore the
morphological, syntactic and semantic relationships
that exist among those units in both languages. This
lack of structure hinders the generalization power of
the model and reduces its ability to adapt to other
domains. Another consequence is that phrase-based
models usually consider a very restricted context1.
This is a general issue in statistical Natural Lan-
guage Processing (NLP) and many possible reme-
dies have been proposed in the literature, such as,
for instance, using smoothing techniques (Chen and
Goodman, 1996), or working with linguistically en-
riched, or more abstract, representations. In statisti-
cal language modeling, another line of research con-
siders numerical representations, trained automat-
ically through the use of neural network (see eg.
</bodyText>
<footnote confidence="0.999031">
1typically a small number of preceding phrase pairs for the
n-gram based approach (Crego and Mari˜no, 2006), or no con-
text at all, for the standard approach of (Koehn et al., 2007).
</footnote>
<note confidence="0.6152685">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999327916666667">
(Collobert et al., 2011)). An influential proposal,
in this respect, is the work of (Bengio et al., 2003)
on continuous space language models. In this ap-
proach, n-gram probabilities are estimated using a
continuous representation of words in lieu of stan-
dard discrete representations. Experimental results,
reported for instance in (Schwenk, 2007) show sig-
nificant improvements in speech recognition appli-
cations. Recently, this model has been extended in
several promising ways (Mikolov et al., 2011; Kuo
et al., 2010; Liu et al., 2011). In the context of SMT,
Schwenk et al. (2007) is the first attempt to esti-
mate translation probabilities in a continuous space.
However, because of the proposed neural architec-
ture, the authors only consider a very restricted set
of translation units, and therefore report only a slight
impact on translation performance. The recent pro-
posal of (Le et al., 2011a) seems especially relevant,
as it is able, through the use of class-based models,
to handle arbitrarily large vocabularies and opens the
way to enhanced neural translation models.
In this paper, we explore various neural architec-
tures for translation models and consider three dif-
ferent ways to factor the joint probability P(s, t)
differing by the units (respectively phrase pairs,
phrases or words) that are projected in continuous
spaces. While these decompositions are theoreti-
cally straightforward, they were not considered in
the past because of data sparsity issues and of the
resulting weaknesses of conventional maximum like-
lihood estimates. Our main contribution is then to
show that such joint distributions can be efficiently
computed by neural networks, even for very large
context sizes; and that their use yields significant
performance improvements. These models are eval-
uated in a n-best rescoring step using the framework
of n-gram based systems, within which they inte-
grate easily. Note, however that they could be used
with any phrase-based system.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the n-gram based ap-
proach, and discuss various implementations of this
framework. We then describe, in Section 3, the neu-
ral architecture developed and explain how it can be
made to handle large vocabulary tasks as well as lan-
guage models over bilingual units. We finally re-
port, in Section 4, experimental results obtained on
a large-scale English to French translation task.
</bodyText>
<sectionHeader confidence="0.449804" genericHeader="method">
2 Variations on the n-gram approach
</sectionHeader>
<bodyText confidence="0.9999760625">
Even though n-gram translation models can be
integrated within standard phrase-based systems
(Niehues et al., 2011), the n-gram based frame-
work provides a more convenient way to introduce
our work and has also been used to build the base-
line systems used in our experiments. In the n-
gram based approach (Casacuberta and Vidal, 2004;
Mari˜no et al., 2006; Crego and Mari˜no, 2006), trans-
lation is divided in two steps: a source reordering
step and a translation step. Source reordering is
based on a set of learned rewrite rules that non-
deterministically reorder the input words so as to
match the target order thereby generating a lattice
of possible reorderings. Translation then amounts
to finding the most likely path in this lattice using a
n-gram translation model 2 of bilingual units.
</bodyText>
<subsectionHeader confidence="0.99607">
2.1 The standard n-gram translation model
</subsectionHeader>
<bodyText confidence="0.99990525">
n-gram translation models (TMs) rely on a spe-
cific decomposition of the joint probability P(s, t),
where s is a sequence of I reordered source words
(s1, ..., sI) and t contains J target words (t1,..., ti).
This sentence pair is further assumed to be de-
composed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL. In the approach of (Mari˜no et al., 2006),
this segmentation is a by-product of source reorder-
ing, and ultimately derives from initial word and
phrase alignments. In this framework, the basic
translation units are tuples, which are the analogous
of phrase pairs, and represent a matching u = (s, t)
between a source s and a target t phrase (see Fig-
ure 1). Using the n-gram assumption, the joint prob-
ability of a segmented sentence pair decomposes as:
</bodyText>
<equation confidence="0.999272666666667">
L
P(s, t) = P(ui|ui−1, ..., ui−n+1) (2)
i=1
</equation>
<bodyText confidence="0.9998694">
A first issue with this model is that the elementary
units are bilingual pairs, which means that the under-
lying vocabulary, hence the number of parameters,
can be quite large, even for small translation tasks.
Due to data sparsity issues, such models are bound
</bodyText>
<footnote confidence="0.999004">
2Like in the standard phrase-based approach, the translation
process also involves additional feature functions that are pre-
sented below.
</footnote>
<page confidence="0.999004">
40
</page>
<bodyText confidence="0.9999215">
to face severe estimation problems. Another prob-
lem with (2) is that the source and target sides play
symmetric roles, whereas the source side is known,
and the target side must be predicted.
</bodyText>
<subsectionHeader confidence="0.999714">
2.2 A factored n-gram translation model
</subsectionHeader>
<bodyText confidence="0.998534666666667">
To overcome some of these issues, the n-gram prob-
ability in equation (2) can be factored by decompos-
ing tuples in two (source and target) parts :
</bodyText>
<equation confidence="0.999866">
P(ui|ui−1, ..., ui−n+1) =
P(ti|si, si−1, ti−1, ..., si−n+1, ti−n+1) (3)
X P(si|si−1, ti−1..., si−n+1, ti−n+1)
</equation>
<bodyText confidence="0.989808">
Decomposition (3) involves two models: the first
term represents a TM, the second term is best viewed
as a reordering model. In this formulation, the TM
only predicts the target phrase, given its source and
target contexts.
Another benefit of this formulation is that the el-
ementary events now correspond either to source or
to target phrases, but never to pairs of such phrases.
The underlying vocabulary is thus obtained as the
union, rather than the cross product, of phrase in-
ventories. Finally note that the n-gram probability
P(ui|ui−1, ..., ui−n+1) could also factor as:
</bodyText>
<subsectionHeader confidence="0.998461">
2.3 A word factored translation model
</subsectionHeader>
<bodyText confidence="0.999673034482759">
A more radical way to address the data sparsity is-
sues is to take (source and target) words as the basic
units of the n-gram TM. This may seem to be a step
backwards, since the transition from word (Brown et
al., 1993) to phrase-based models (Zens et al., 2002)
is considered as one of the main recent improvement
in MT. One important motivation for considering
phrases rather than words was to capture local con-
text in translation and reordering. It should then be
stressed that the decomposition of phrases in words
is only re-introduced here as a way to mitigate the
parameter estimation problems. Translation units
are still pairs of phrases, derived from a bilingual
segmentation in tuples synchronizing the source and
target n-gram streams, as defined by equation (3).
In fact, the estimation policy described in section 3
will actually allow us to design n-gram models with
longer contexts than is typically possible in the con-
ventional n-gram approach.
Let ski denote the kth word of source tuple si.
Considering again the example of Figure 1, s111 is
to the source word nobel, si1 is to the source word
paix, and similarly t211 is the target word peace. We
finally denote hn−1(tki ) the sequence made of the
n − 1 words preceding tki in the target sentence: in
Figure 1, h3(t211) thus refers to the three word con-
text receive the nobel associated with the target word
peace. Using these notations, equation (3) is rewrit-
ten as:
</bodyText>
<equation confidence="0.996260333333333">
P(tki |hn−1(tki ), hn−1(s1i+1))
P(sk i ))i (5)
i |hn−1(t1 i ),hn−1(sk
</equation>
<bodyText confidence="0.977495266666667">
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words in ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P(s, t) using a 3- gram model is
P (nobel|[receive, the], [la, paix])
XP (peace|[the, nobel], [la, paix]).
Likewise, the contribution of the source phrase
s11 =nobel, de, la, paix is:
</bodyText>
<equation confidence="0.9751355">
P (nobel|[receive, the], [recevoir,le])
X P (de|[receive, the], [le,nobel])
X P (la|[receive, the], [nobel, de])
X P (paix|[receive, the], [de,la]).
</equation>
<bodyText confidence="0.9999815">
A benefit of this new formulation is that the involved
vocabularies only contain words, and are thus much
smaller. These models are thus less bound to be af-
fected by data sparsity issues. While the TM defined
by equation (5) derives from equation (3), a variation
can be equivalently derived from equation (4).
</bodyText>
<equation confidence="0.9158074375">
P (si|ti, si−1, ti−1, ..., si−n+1, ti−n+1)
(4)
X P(ti|si−1, ti−1, ..., si−n+1, ti−n+1)
i
P(s, t) =
HL
i=1
|ti|
H
k=1
X H |si|
k=1
41
org : .... à recevoir le prix nobel de la paix
T : ....
S : ....
</equation>
<figure confidence="0.974689933333333">
t8: to
s̅8: à
9: recevoir
̅
s
t̅9: receive
t̅10: the
s̅10: le
s̅11: nobel de la paix
t̅11: nobel peace
t̅12: prize
s̅12: prix
....
....
u8 u9 u10 u11 u12
</figure>
<figureCaption confidence="0.979322333333333">
Figure 1: Extract of a French-English sentence pair segmented in bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) ul, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
</figureCaption>
<sectionHeader confidence="0.995028" genericHeader="method">
3 The SOUL model
</sectionHeader>
<bodyText confidence="0.999944769230769">
In the previous section, we defined three different
n-gram translation models, based respectively on
equations (2), (3) and (5). As discussed above, a
major issue with such models is to reliably estimate
their parameters, the numbers of which grow expo-
nentially with the order of the model. This problem
is aggravated in natural language processing, due to
well known data sparsity issues. In this work, we
take advantage of the recent proposal of (Le et al.,
2011a): using a specific neural network architecture
(the Structured OUtput Layer model), it becomes
possible to handle large vocabulary language mod-
eling tasks, a solution that we adapt here to MT.
</bodyText>
<subsectionHeader confidence="0.999357">
3.1 Language modeling in a continuous space
</subsectionHeader>
<bodyText confidence="0.999875">
Let V be a finite vocabulary, n-gram language mod-
els (LMs) define distributions over finite sequences
of tokens (typically words) wL1 in V+ as follows:
</bodyText>
<equation confidence="0.999341">
P(wi|wi−1
i−n+1) (6)
</equation>
<bodyText confidence="0.999841547619048">
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains dozens of thousands words.
In spite of the simplifying n-gram assump-
tion, maximum likelihood estimation remains un-
reliable and tends to underestimate the proba-
bility of very rare n-grams. Smoothing tech-
niques, such as Kneser-Ney and Witten-Bell back-
off schemes (see (Chen and Goodman, 1996) for an
empirical overview, and (Teh, 2006) for a Bayesian
interpretation), perform back-off to lower order dis-
tributions, thus providing an estimate for the proba-
bility of these unseen events.
One of the most successful alternative to date is to
use distributed word representations (Bengio et al.,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated estimates are jointly computed
in a multi-layer neural network architecture. Fig-
ure 2 provides a partial representation of this kind
of model and helps figuring out their principles. To
compute the probability P(wi|wi−1
i−n+1), the n − 1
context words are projected in the same continu-
ous space using a shared matrix R; these continuous
word representations are then concatenated to build
a single vector that represents the context; after a
non-linear transformation, the probability distribu-
tion is computed using a softmax layer.
The major difficulty with the neural network ap-
proach remains the complexity of inference and
training, which largely depends on the size of the
output vocabulary (i.e. the number of words that
have to be predicted). One practical solution is to re-
strict the output vocabulary to a short-list composed
of the most frequent words (Schwenk, 2007). How-
ever, the usual size of the short-list is under 20k,
which does not seem sufficient to faithfully repre-
sent the translation models of section 2.
</bodyText>
<subsectionHeader confidence="0.99996">
3.2 Principles of SOUL
</subsectionHeader>
<bodyText confidence="0.99993">
To circumvent this problem, Structured Output
Layer (SOUL) LMs are introduced in (Le et al.,
2011a). Following Mnih and Hinton (2008), the
SOUL model combines the neural network approach
with a class-based LM (Brown et al., 1992). Struc-
</bodyText>
<equation confidence="0.987072666666667">
L
P(wL1 ) _
i=1
</equation>
<page confidence="0.955147">
42
</page>
<bodyText confidence="0.995159153846154">
turing the output layer and using word class informa-
tion makes the estimation of distributions over the
entire vocabulary computationally feasible.
To meet this goal, the output vocabulary is struc-
tured as a clustering tree, where each word belongs
to only one class and its associated sub-classes. If
wi denotes the ith word in a sentence, the sequence
c1:D(wi) = c1, ... , cD encodes the path for word wi
in the clustering tree, with D being the depth of the
tree, cd(wi) a class or sub-class assigned to wi, and
cD(wi) being the leaf associated with wi (the word
itself). The probability of wi given its history h can
then be computed as:
</bodyText>
<equation confidence="0.999573">
P(wi|h) =P(c1(wi)|h)
P(cd(wi)|h, c1:d−1). (7)
</equation>
<bodyText confidence="0.999777592592593">
There is a softmax function at each level of the tree
and each word ends up forming its own class (a leaf).
The SOUL model, represented on Figure 2, is thus
the same as for the standard model up to the output
layer. The main difference lies in the output struc-
ture which involves several layers with a softmax ac-
tivation function. The first (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d−1). Finally, the word layers es-
timate the word probabilities P(cD(wi)|h, c1:D−1).
Words in the short-list remain special, since each of
them represents a (final) class.
Training a SOUL model can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al., 2003),
this optimization is performed by stochastic back-
propagation. Details of the training procedure can
be found in (Le et al., 2011b).
Neural network architectures are also interesting
as they can easily handle larger contexts than typical
n-gram models. In the SOUL architecture, enlarging
the context mainly consists in increasing the size of
the projection layer, which corresponds to a simple
look-up operation. Increasing the context length at
the input layer thus only causes a linear growth in
complexity in the worst case (Schwenk, 2007).
</bodyText>
<figureCaption confidence="0.992142">
Figure 2: The architecture of a SOUL Neural Network
language model in the case of a 4-gram model.
</figureCaption>
<subsectionHeader confidence="0.951612">
3.3 Translation modeling with SOUL
</subsectionHeader>
<bodyText confidence="0.99995437037037">
The SOUL architecture was used successfully to
deliver (monolingual) LMs probabilities for speech
recognition (Le et al., 2011a) and machine transla-
tion (Allauzen et al., 2011) applications. In fact,
using this architecture, it is possible to estimate n-
gram distributions for any kind of discrete random
variables, such as a phrase or a tuple. The SOUL ar-
chitecture can thus be readily used as a replacement
for the standard n-gram TM described in section 2.1.
This is because all the random variables are events
over the same set of tuples.
Adopting this architecture for the other n-gram
TM respectively described by equations (3) and (5)
is more tricky, as they involve two different lan-
guages and thus two different vocabularies: the pre-
dicted unit is a target phrase (resp. word), whereas
the context is made of both source and target phrases
(resp. words). A subsequent modification of the
SOUL architecture was thus performed to make up
for “mixed” contexts: rather than projecting all the
context words or phrases into the same continuous
space (using the matrix R, see Figure 2), we used
two different projection matrices, one for each lan-
guage. The input layer is thus composed of two vec-
tors in two different spaces; these two representa-
tions are then combined through the hidden layer,
the other layers remaining unchanged.
</bodyText>
<figure confidence="0.998884">
shortlist
wi-1
shared input space
class
layer
input layer
0
...
0
1
0
0
R
sub-class
layers
wi-2
R
1
0
...
0
0
0
word
layers
R
wi-3
hidden layers
0
...
0
0
1
0
D
X ri
d=2
</figure>
<page confidence="0.999829">
43
</page>
<sectionHeader confidence="0.998091" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999985894736842">
We now turn to an experimental comparison of the
models introduced in Section 2. We first describe
the tasks and data that were used, before presenting
our n-gram based system and baseline set-up. Our
results are finally presented and discussed.
Let us first emphasize that the design and inte-
gration of a SOUL model for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two pass approach:
the first pass uses a conventional back-off n-gram
model to produce a k-best list (the k most likely
translations); in the second pass, the probability of
a m-gram SOUL model is computed for each hy-
pothesis, added as a new feature and the k-best list
is accordingly reordered3. In all the following ex-
periments, we used a fixed context size for SOUL of
m = 10, and used k = 300.
</bodyText>
<subsectionHeader confidence="0.997454">
4.1 Tasks and corpora
</subsectionHeader>
<bodyText confidence="0.999976285714286">
The two tasks considered in our experiments
are adapted from the text translation track of
IWSLT 2011 from English to French (the ”TED”
talk task): a small data scenario where the only
training data is a small in-domain corpus; and a large
scale condition using all the available training data.
In this article, we only provide a short overview of
the task; all the necessary details regarding this eval-
uation campaign are on the official website4.
The in-domain training data consists of 107,058
sentence pairs, whereas for the large scale task, all
the data available for the WMT 2011 evaluation5 are
added. For the latter task, the available parallel data
includes a large Web corpus, referred to as the Gi-
gaWord parallel corpus. This corpus is very noisy
and is accordingly filtered using a simple perplexity
criterion as explained in (Allauzen et al., 2011). The
total amount of training data is approximately 11.5
million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
As the provided development data was quite small,
</bodyText>
<footnote confidence="0.9556556">
3The probability estimated with the SOUL model is added as
a new feature to the score of an hypothesis given by Equation 1.
The coefficients are retuned before the reranking step.
4iwslt2011.org
5www.statmt.org/wmt11/
</footnote>
<table confidence="0.999262333333333">
Model Small Vocabulary size task
src task Large trg
trg src
Standard 317k 8847k
Phrase factored 96k 131k 4262k 3972k
Word factored 45k 53k 505k 492k
</table>
<tableCaption confidence="0.70509475">
Table 1: Vocabulary sizes for the English to French tasks
obtained with various SOUL translation (TM) models.
For the factored models, sizes are indicated for both
source (src) and target (trg) sides.
</tableCaption>
<bodyText confidence="0.998784363636364">
development and test set were inverted, and we fi-
nally used a development set of 1,664 sentences, and
a test set of 934 sentences. The table 1 provides the
sizes of the different vocabularies. The n-gram TMs
are estimated over a training corpus composed of tu-
ple sequences. Tuples are extracted from the word-
aligned parallel data (using MGIZA++6 with default
settings) in such a way that a unique segmentation
of the bilingual corpus is achieved, allowing to di-
rectly estimate bilingual n-gram models (see (Crego
and Mari˜no, 2006) for details).
</bodyText>
<subsectionHeader confidence="0.989946">
4.2 n-gram based translation system
</subsectionHeader>
<bodyText confidence="0.99996019047619">
The n-gram based system used here is based on an
open source implementation described in (Crego et
al., 2011). In a nutshell, the TM is implemented as
a stochastic finite-state transducer trained using a n-
gram model of (source, target) pairs as described in
section 2.1. Training this model requires to reorder
source sentences so as to match the target word or-
der. This is performed by a non-deterministic finite-
state reordering model, which uses part-of-speech
information generated by the TreeTagger to gener-
alize reordering patterns beyond lexical regularities.
In addition to the TM, fourteen feature functions
are included: a target-language model; four lexi-
con models; six lexicalized reordering models (Till-
mann, 2004; Crego et al., 2011); a distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model. The four lexicon mod-
els are similar to the ones used in standard phrase-
based systems: two scores correspond to the rela-
tive frequencies of the tuples and two lexical weights
are estimated from the automatically generated word
</bodyText>
<footnote confidence="0.489114">
6geek.kyloo.net/software
</footnote>
<page confidence="0.998855">
44
</page>
<bodyText confidence="0.999918058823529">
alignments. The weights associated to feature func-
tions are optimally combined using the Minimum
Error Rate Training (MERT) (Och, 2003). All the
results in BLEU are obtained as an average of 4 op-
timization runs7.
For the small task, the target LM is a standard
4-gram model estimated with the Kneser-Ney dis-
counting scheme interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1996), while for the large task, the target LM is ob-
tained by linear interpolation of several 4-gram mod-
els (see (Lavergne et al., 2011) for details). As for
the TM, all the available parallel corpora were sim-
ply pooled together to train a 3-gram model. Results
obtained with this large-scale system were found to
be comparable to some of the best official submis-
sions.
</bodyText>
<subsectionHeader confidence="0.995599">
4.3 Small task evaluation
</subsectionHeader>
<bodyText confidence="0.99855">
Table 2 summarizes the results obtained with the
baseline and different SOUL models, TMs and a
target LM. The first comparison concerns the stan-
dard n-gram TM, defined by equation (2), when es-
timated conventionally or as a SOUL model. Adding
the latter model yields a slight BLEU improvement
of 0.5 point over the baseline. When the SOUL TM
is phrased factored as defined in equation (3) the
gain is of 0.9 BLEU point instead. This difference
can be explained by the smaller vocabularies used
in the latter model, and its improved robustness to
data sparsity issues. Additional gains are obtained
with the word factored TM defined by equation (5):
a BLEU improvement of 0.8 point over the phrase
factored TM and of 1.7 point over the baseline are
respectively achieved. We assume that the observed
improvements can be explained by the joint effect of
a better smoothing and a longer context.
The comparison with the condition where we only
use a SOUL target LM is interesting as well. Here,
the use of the word factored TM still yields to a 0.6
BLEU improvement. This result shows that there
is an actual benefit in smoothing the TM estimates,
rather than only focus on the LM estimates.
Table 3 reports a comparison among the dif-
ferent components and variations of the word
</bodyText>
<footnote confidence="0.965315">
7The standard deviations are below 0.1 and thus omitted in
the reported results.
</footnote>
<table confidence="0.9982165">
Model BLEU test
dev
Baseline 31.4 25.8
Adding a SOUL model
Standard TM 32.0 26.3
Phrase factored TM 32.7 26.7
Word factored TM 33.6 27.5
Target LM 32.6 26.9
</table>
<tableCaption confidence="0.933262">
Table 2: Results for the small English to French task ob-
tained with the baseline system and with various SOUL
translation (TM) or target language (LM) models.
</tableCaption>
<table confidence="0.999810666666667">
Model BLEU test
dev
Adding a SOUL model
+ P(tk� |hn−1(tk� ),hn−1(s1 �+1)) 32.6 26.9
+ P(sk� |hn−1(t1�),hn−1(sk� )) 32.1 26.2
+ the combination of both 33.2 27.5
+ P(sk � |hn−1(sk � ),hn−1(t1 �+1)) 31.7 26.1
+ P(tk� |hn−1(s1�),hn−1(tk� )) 32.7 26.8
+ the combination of both 33.4 27.2
</table>
<tableCaption confidence="0.990885">
Table 3: Comparison of the different components and
variations of the word factored translation model.
</tableCaption>
<bodyText confidence="0.9999364">
factored TM. In the upper part of the table,
the model defined by equation (5) is evaluated
component by component: first the translation
term P(tk|hn−1(tk�),hn−1(s+1)), then its distor-
tion counterpart P(sk� |hn−1(t1� ), hn−1(sk� )) and fi-
nally their combination, which yields the joint prob-
ability of the sentence pair. Here, we observe that
the best improvement is obtained with the transla-
tion term, which is 0.7 BLEU point better than the
latter term. Moreover, the use of just a translation
term only yields a BLEU score equal to the one ob-
tained with the SOUL target LM, and its combina-
tion with the distortion term is decisive to attain the
additional gain of 0.6 BLEU point. The lower part of
the table provides the same comparison, but for the
variation of the word factored TM. Besides a similar
trend, we observe that this variation delivers slightly
lower results. This can be explained by the restricted
context used by the translation term which no longer
includes the current source phrase or word.
</bodyText>
<page confidence="0.996903">
45
</page>
<table confidence="0.999592777777778">
Model BLEU
dev test
Baseline 33.7 28.2
Adding a word factored SOUL TM
+ in-domain TM 35.2 29.4
+ out-of-domain TM 34.8 29.1
+ out-of-domain adapted TM 35.5 29.8
Adding a SOUL LM
+ out-of-domain adapted LM 35.0 29.2
</table>
<tableCaption confidence="0.959469666666667">
Table 4: Results for the large English to French trans-
lation task obtained by adding various SOUL translation
and language models (see text for details).
</tableCaption>
<subsectionHeader confidence="0.994496">
4.4 Large task evaluation
</subsectionHeader>
<bodyText confidence="0.999973233333333">
For the large-scale setting, the training material in-
creases drastically with the use of the additional out-
of-domain data for the baseline models. Results are
summarized in Table 4. The first observation is the
large increase of BLEU (+2.4 points) for the base-
line system over the small-scale baseline. For this
task, only the word factored TM is evaluated since
it significantly outperforms the others on the small
task (see section 4.3).
In a first scenario, we use a word factored TM,
trained only on the small in-domain corpus. Even
though the training corpus of the baseline TM is one
hundred times larger than this small in-domain data,
adding the SOUL TM still yields a BLEU increase
of 1.2 point8. In a second scenario, we increase the
training corpus for the SOUL, and include parts of
the out-of-domain data (the WMT part). The result-
ing BLEU score is here slightly worse than the one
obtained with just the in-domain TM, yet delivering
improved results with the respect to the baseline.
In a last attempt, we amended the training regime
of the neural network. In a fist step, we trained con-
ventionally a SOUL model using the same out-of-
domain parallel data as before. We then adapted this
model by running five additional epochs of the back-
propagation algorithm using the in-domain data. Us-
ing this adapted model yielded our best results to
date with a BLEU improvement of 1.6 points over
the baseline results. Moreover, the gains obtained
using this simple domain adaptation strategy are re-
</bodyText>
<footnote confidence="0.567631">
8Note that the in-domain data was already included in the
training corpus of the baseline TM.
</footnote>
<bodyText confidence="0.9997608">
spectively of +0.4 and +0.8 BLEU, as compared
with the small in-domain model and the large out-
of-domain model. These results show that the SOUL
TM can scale efficiently and that its structure is well
suited for domain adaptation.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999984710526316">
To the best of our knowledge, the first work on ma-
chine translation in continuous spaces is (Schwenk
et al., 2007), where the authors introduced the model
referred here to as the the standard n-gram trans-
lation model in Section 2.1. This model is an ex-
tension of the continuous space language model
of (Bengio et al., 2003), the basic unit is the tuple
(or equivalently the phrase pair). The resulting vo-
cabulary being too large to be handled by neural net-
works without a structured output layer, the authors
had thus to restrict the set of the predicted units to a
8k short-list . Moreover, in (Zamora-Martinez et al.,
2010), the authors propose a tighter integration of a
continuous space model with a n-gram approach but
only for the target LM.
A different approach, described in (Sarikaya et
al., 2008), divides the problem in two parts: first the
continuous representation is obtained by an adapta-
tion of the Latent Semantic Analysis; then a Gaus-
sian mixture model is learned using this continu-
ous representation and included in a hidden Markov
model. One problem with this approach is the sep-
aration between the training of the continuous rep-
resentation on the one hand, and the training of the
translation model on the other hand. In comparison,
in our approach, the representation and the predic-
tion are learned in a joined fashion.
Other ways to address the data sparsity issues
faced by translation model were also proposed in the
literature. Smoothing is obviously one possibility
(Foster et al., 2006). Another is to use factored lan-
guage models, introduced in (Bilmes and Kirchhoff,
2003), then adapted for translation models in (Koehn
and Hoang, 2007; Crego and Yvon, 2010). Such ap-
proaches require to use external linguistic analysis
tools which are error prone; moreover, they did not
seem to bring clear improvements, even when trans-
lating into morphologically rich languages.
</bodyText>
<page confidence="0.999157">
46
</page>
<sectionHeader confidence="0.998498" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993628571429">
In this paper, we have presented possible ways to use
a neural network architecture as a translation model.
A first contribution was to produce the first large-
scale neural translation model, implemented here in
the framework of the n-gram based models, tak-
ing advantage of a specific hierarchical architecture
(SOUL). By considering several decompositions of
the joint probability of a sentence pair, several bilin-
gual translation models were presented and dis-
cussed. As it turned out, using a factorization which
clearly distinguishes the source and target sides, and
only involves word probabilities, proved an effec-
tive remedy to data sparsity issues and provided sig-
nificant improvements over the baseline. Moreover,
this approach was also experimented within the sys-
tems we submitted to the shared translation task of
the seventh workshop on statistical machine trans-
lation (WMT 2012). These experimentations in a
large scale setup and for different language pair cor-
roborate the improvements reported in this article.
We also investigated various training regimes for
these models in a cross domain adaptation setting.
Our results show that adapting an out-of-domain
SOUL TM is both an effective and very fast way to
perform bilingual model adaptation. Adding up all
these novelties finally brought us a 1.6 BLEU point
improvement over the baseline. Even though our
experiments were carried out only within the frame-
work of n-gram based MT systems, using such mod-
els in other systems is straightforward. Future work
will thus aim at introducing them into conventional
phrase-based systems, such as Moses (Koehn et al.,
2007). Given that Moses only implicitly uses n-
gram based information, adding SOUL translation
models is expected to be even more helpful.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999738333333333">
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
</bodyText>
<sectionHeader confidence="0.998496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872714285715">
Alexandre Allauzen, Gilles Adda, H´el`ene Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aur´elien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc¸ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309–
315, Edinburgh, Scotland.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137–1155.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL ’03: Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology, pages 4–6.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205–
225.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL’96, pages 310–318, San Francisco.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.
Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199–215.
Josep M. Crego and Franc¸ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, pages 1–17.
Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49–58.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrase-table smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53–61, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181–184, Detroit, Michigan.
</reference>
<page confidence="0.986693">
47
</page>
<reference confidence="0.999882423913043">
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868–876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL’07, pages 177–180, Prague, Czech Republic.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for Arabic speech recognition. In Proc. ICASSP
2010.
Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and
Franc¸ois Yvon. 2011. LIMSI’s experiments in do-
main adaptation for IWSLT11. In Proceedings of
the eight International Workshop on Spoken Language
Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc¸ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP’11, pages 5524–5527.
Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857–2860.
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a
de Gispert, Patrick Lambert, Jos´e A.R. Fonollosa, and
Marta R. Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527–
549.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2011. Extensions
of recurrent neural network language model. In Proc.
of ICASSP’11, pages 5528–5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081–1088.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider context by using bilingual lan-
guage models in machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 198–206, Edinburgh, Scotland. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417–449, Decem-
ber.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ’03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160–167.
Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, Brian
Kingsbury, and Yuqing Gao. 2008. Machine trans-
lation in continuous space. In Proceedings of Inter-
speech, pages 2350–2353, Brisbane, Australia.
Holger Schwenk, Marta R. Costa-Juss`a, and Jos´e A.R.
Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 430–438, Prague, Czech Re-
public.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492–
518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL’06, pages 985–992, Sidney, Australia.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101–104.
Francisco Zamora-Martinez, Maria Jos´e Castro-Bleda,
and Holger Schwenk. 2010. N-gram-based Machine
Translation enhanced with Neural Networks for the
French-English BTEC-IWSLT’10 task. In Proceed-
ings of the seventh International Workshop on Spoken
Language Translation (IWSLT), pages 45–52.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
’02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18–32, London, UK. Springer-
Verlag.
</reference>
<page confidence="0.999345">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.269137">
<title confidence="0.996904">Continuous Space Translation Models with Neural Networks</title>
<author confidence="0.990596">Hai Son Allauzen</author>
<affiliation confidence="0.706857">Univ. Paris-Sud, France and</affiliation>
<address confidence="0.324805">rue John von Neumann, 91403 Orsay cedex,</address>
<email confidence="0.493775">Firstname.Lastname@limsi.fr</email>
<abstract confidence="0.9985216">The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alexandre Allauzen</author>
<author>Gilles Adda</author>
<author>H´el`ene BonneauMaynard</author>
<author>Josep M Crego</author>
</authors>
<location>Hai-Son Le, Aur´elien Max, Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,</location>
<marker>Allauzen, Adda, BonneauMaynard, Crego, </marker>
<rawString>Alexandre Allauzen, Gilles Adda, H´el`ene BonneauMaynard, Josep M. Crego, Hai-Son Le, Aur´elien Max, Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI @ WMT11.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>309--315</pages>
<location>Edinburgh, Scotland.</location>
<marker>Wisniewski, Yvon, 2011</marker>
<rawString>Guillaume Wisniewski, and Franc¸ois Yvon. 2011. LIMSI @ WMT11. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 309– 315, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="3618" citStr="Bengio et al., 2003" startWordPosition="557" endWordPosition="560">research considers numerical representations, trained automatically through the use of neural network (see eg. 1typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvements in speech recognition applications. Recently, this model has been extended in several promising ways (Mikolov et al., 2011; Kuo et al., 2010; Liu et al., 2011). In the context of SMT, Schwenk et al. (2007) is the first attempt to estimate translation probabilities in a continuous space. However, because of the proposed neural</context>
<context position="14192" citStr="Bengio et al., 2003" startWordPosition="2323" endWordPosition="2326">ere V typically contains dozens of thousands words. In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams. Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This turns n-grams distributions into smooth functions of the word representations. These representations and the associated estimates are jointly computed in a multi-layer neural network architecture. Figure 2 provides a partial representation of this kind of model and helps figuring out their principles. To compute the probability P(wi|wi−1 i−n+1), the n − 1 context words are projected in the same continuous space using a shared matrix R; these continuous word representations are then concatenated to b</context>
<context position="17206" citStr="Bengio et al., 2003" startWordPosition="2819" endWordPosition="2822"> model up to the output layer. The main difference lies in the output structure which involves several layers with a softmax activation function. The first (class layer) estimates the class probability P(c1(wi)|h), while other output sub-class layers estimate the sub-class probabilities P (cd(wi)|h, c1:d−1). Finally, the word layers estimate the word probabilities P(cD(wi)|h, c1:D−1). Words in the short-list remain special, since each of them represents a (final) class. Training a SOUL model can be achieved by maximizing the log-likelihood of the parameters on some training corpus. Following (Bengio et al., 2003), this optimization is performed by stochastic backpropagation. Details of the training procedure can be found in (Le et al., 2011b). Neural network architectures are also interesting as they can easily handle larger contexts than typical n-gram models. In the SOUL architecture, enlarging the context mainly consists in increasing the size of the projection layer, which corresponds to a simple look-up operation. Increasing the context length at the input layer thus only causes a linear growth in complexity in the worst case (Schwenk, 2007). Figure 2: The architecture of a SOUL Neural Network la</context>
<context position="30116" citStr="Bengio et al., 2003" startWordPosition="5001" endWordPosition="5004">included in the training corpus of the baseline TM. spectively of +0.4 and +0.8 BLEU, as compared with the small in-domain model and the large outof-domain model. These results show that the SOUL TM can scale efficiently and that its structure is well suited for domain adaptation. 5 Related work To the best of our knowledge, the first work on machine translation in continuous spaces is (Schwenk et al., 2007), where the authors introduced the model referred here to as the the standard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model of (Bengio et al., 2003), the basic unit is the tuple (or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors had thus to restrict the set of the predicted units to a 8k short-list . Moreover, in (Zamora-Martinez et al., 2010), the authors propose a tighter integration of a continuous space model with a n-gram approach but only for the target LM. A different approach, described in (Sarikaya et al., 2008), divides the problem in two parts: first the continuous representation is obtained by an adaptation of the Latent Se</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="31394" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="5215" endWordPosition="5218">earned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 46 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarc</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="15687" citStr="Brown et al., 1992" startWordPosition="2559" endWordPosition="2562">n the size of the output vocabulary (i.e. the number of words that have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed of the most frequent words (Schwenk, 2007). However, the usual size of the short-list is under 20k, which does not seem sufficient to faithfully represent the translation models of section 2. 3.2 Principles of SOUL To circumvent this problem, Structured Output Layer (SOUL) LMs are introduced in (Le et al., 2011a). Following Mnih and Hinton (2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). StrucL P(wL1 ) _ i=1 42 turing the output layer and using word class information makes the estimation of distributions over the entire vocabulary computationally feasible. To meet this goal, the output vocabulary is structured as a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the ith word in a sentence, the sequence c1:D(wi) = c1, ... , cD encodes the path for word wi in the clustering tree, with D being the depth of the tree, cd(wi) a class or sub-class assigned to wi, and cD(wi) being the leaf associated with wi (the word itself).</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1821" citStr="Brown et al., 1993" startWordPosition="282" endWordPosition="285"> rule, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K P(t, a|s) = Z(s) exp ( 1: Akfk(s, t, a)), (1) k�� where K feature functions (fk) are weighted by a set of coefficients (Ak), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation 39 process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P(tIs), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those u</context>
<context position="9397" citStr="Brown et al., 1993" startWordPosition="1509" endWordPosition="1512">ontexts. Another benefit of this formulation is that the elementary events now correspond either to source or to target phrases, but never to pairs of such phrases. The underlying vocabulary is thus obtained as the union, rather than the cross product, of phrase inventories. Finally note that the n-gram probability P(ui|ui−1, ..., ui−n+1) could also factor as: 2.3 A word factored translation model A more radical way to address the data sparsity issues is to take (source and target) words as the basic units of the n-gram TM. This may seem to be a step backwards, since the transition from word (Brown et al., 1993) to phrase-based models (Zens et al., 2002) is considered as one of the main recent improvement in MT. One important motivation for considering phrases rather than words was to capture local context in translation and reordering. It should then be stressed that the decomposition of phrases in words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams, as defined by equation (3). In fact, the estimation policy described in</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<pages>225</pages>
<contexts>
<context position="6309" citStr="Casacuberta and Vidal, 2004" startWordPosition="984" endWordPosition="987"> neural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely path in this lattice using a n-gram translation model 2 of bilingual units. 2.1 The standard n-gram translation model n-gram translation models (TMs) rely on a specific decomposition of the joint probability P(s, t)</context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>Francesco Casacuberta and Enrique Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(3):205– 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. ACL’96,</booktitle>
<pages>310--318</pages>
<location>San Francisco.</location>
<contexts>
<context position="2870" citStr="Chen and Goodman, 1996" startWordPosition="445" endWordPosition="448">te random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization power of the model and reduces its ability to adapt to other domains. Another consequence is that phrase-based models usually consider a very restricted context1. This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012</context>
<context position="13887" citStr="Chen and Goodman, 1996" startWordPosition="2275" endWordPosition="2278">finite vocabulary, n-gram language models (LMs) define distributions over finite sequences of tokens (typically words) wL1 in V+ as follows: P(wi|wi−1 i−n+1) (6) Modeling the joint distribution of several discrete random variables (such as words in a sentence) is difficult, especially in NLP applications where V typically contains dozens of thousands words. In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams. Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This turns n-grams distributions into smooth functions of the word representations. These representations and the associated estimates are jointly computed in a multi-layer neural network architecture. Fi</context>
<context position="24090" citStr="Chen and Goodman, 1996" startWordPosition="3965" endWordPosition="3968">o the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6geek.kyloo.net/software 44 alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7. For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to some of the best official submissions. 4.3 Small task evaluation Table 2 summarizes the results obtained with the baseline and different SOUL models, TMs and a target LM. The first comparison concerns the standard n-gram TM, defined by equation (2), when estimated conventio</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. ACL’96, pages 310–318, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="3537" citStr="Collobert et al., 2011" startWordPosition="543" endWordPosition="546">r more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvements in speech recognition applications. Recently, this model has been extended in several promising ways (Mikolov et al., 2011; Kuo et al., 2010; Liu et al., 2011). In the context of SMT, Schwenk et al. (2007) is the first attempt to estimate transla</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Improving statistical MT by coupling reordering and decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving statistical MT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Factored bilingual n-gram language models for statistical machine translation. Machine Translation,</title>
<date>2010</date>
<pages>1--17</pages>
<contexts>
<context position="31480" citStr="Crego and Yvon, 2010" startWordPosition="5229" endWordPosition="5232">em with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 46 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchical architecture (SOUL). By considering several decompositions of the joint probabil</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep M. Crego and Franc¸ois Yvon. 2010. Factored bilingual n-gram language models for statistical machine translation. Machine Translation, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2011</date>
<booktitle>N-code: an open-source Bilingual N-gram SMT Toolkit. Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. N-code: an open-source Bilingual N-gram SMT Toolkit. Prague Bulletin of Mathematical Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrase-table smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--61</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="31306" citStr="Foster et al., 2006" startWordPosition="5201" endWordPosition="5204">an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 46 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented </context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrase-table smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 53–61, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume I,</booktitle>
<pages>181--184</pages>
<location>Detroit, Michigan.</location>
<contexts>
<context position="24065" citStr="Kneser and Ney, 1995" startWordPosition="3961" endWordPosition="3964">n models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6geek.kyloo.net/software 44 alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7. For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to some of the best official submissions. 4.3 Small task evaluation Table 2 summarizes the results obtained with the baseline and different SOUL models, TMs and a target LM. The first comparison concerns the standard n-gram TM, defined by equation (2),</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume I, pages 181–184, Detroit, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="31457" citStr="Koehn and Hoang, 2007" startWordPosition="5225" endWordPosition="5228">Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 46 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchical architecture (SOUL). By considering several decomposition</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL’07,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="3289" citStr="Koehn et al., 2007" startWordPosition="511" endWordPosition="514">neral issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvement</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL’07, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Kwang Kuo</author>
<author>Lidia Mangu</author>
<author>Ahmad Emami</author>
<author>Imed Zitouni</author>
</authors>
<title>Morphological and syntactic features for Arabic speech recognition.</title>
<date>2010</date>
<booktitle>In Proc. ICASSP</booktitle>
<contexts>
<context position="4031" citStr="Kuo et al., 2010" startWordPosition="618" endWordPosition="621"> pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvements in speech recognition applications. Recently, this model has been extended in several promising ways (Mikolov et al., 2011; Kuo et al., 2010; Liu et al., 2011). In the context of SMT, Schwenk et al. (2007) is the first attempt to estimate translation probabilities in a continuous space. However, because of the proposed neural architecture, the authors only consider a very restricted set of translation units, and therefore report only a slight impact on translation performance. The recent proposal of (Le et al., 2011a) seems especially relevant, as it is able, through the use of class-based models, to handle arbitrarily large vocabularies and opens the way to enhanced neural translation models. In this paper, we explore various neu</context>
</contexts>
<marker>Kuo, Mangu, Emami, Zitouni, 2010</marker>
<rawString>Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and Imed Zitouni. 2010. Morphological and syntactic features for Arabic speech recognition. In Proc. ICASSP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Alexandre Allauzen</author>
<author>Hai-Son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI’s experiments in domain adaptation for IWSLT11.</title>
<date>2011</date>
<booktitle>In Proceedings of the eight International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="24221" citStr="Lavergne et al., 2011" startWordPosition="3989" endWordPosition="3992">ghts are estimated from the automatically generated word 6geek.kyloo.net/software 44 alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7. For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to some of the best official submissions. 4.3 Small task evaluation Table 2 summarizes the results obtained with the baseline and different SOUL models, TMs and a target LM. The first comparison concerns the standard n-gram TM, defined by equation (2), when estimated conventionally or as a SOUL model. Adding the latter model yields a slight BLEU improvement of 0.5 point over the baseline. When the SOUL TM</context>
</contexts>
<marker>Lavergne, Allauzen, Le, Yvon, 2011</marker>
<rawString>Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and Franc¸ois Yvon. 2011. LIMSI’s experiments in domain adaptation for IWSLT11. In Proceedings of the eight International Workshop on Spoken Language Translation (IWSLT), San Francisco, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hai-Son Le</author>
</authors>
<title>Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon. 2011a. Structured output layer neural network language model.</title>
<booktitle>In Proceedings of ICASSP’11,</booktitle>
<pages>5524--5527</pages>
<marker>Le, </marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon. 2011a. Structured output layer neural network language model. In Proceedings of ICASSP’11, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
</authors>
<title>Ilya Oparin, Abdel Messaoudi, Alexandre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon. 2011b. Large vocabulary SOUL neural network language models.</title>
<date>2011</date>
<booktitle>In Proceedings of InterSpeech</booktitle>
<marker>Le, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexandre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon. 2011b. Large vocabulary SOUL neural network language models. In Proceedings of InterSpeech 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xunying Liu</author>
<author>Mark J F Gales</author>
<author>Philip C Woodland</author>
</authors>
<title>Improving lvcsr system combination using neural network language model cross adaptation.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>2857--2860</pages>
<contexts>
<context position="4050" citStr="Liu et al., 2011" startWordPosition="622" endWordPosition="625">r´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvements in speech recognition applications. Recently, this model has been extended in several promising ways (Mikolov et al., 2011; Kuo et al., 2010; Liu et al., 2011). In the context of SMT, Schwenk et al. (2007) is the first attempt to estimate translation probabilities in a continuous space. However, because of the proposed neural architecture, the authors only consider a very restricted set of translation units, and therefore report only a slight impact on translation performance. The recent proposal of (Le et al., 2011a) seems especially relevant, as it is able, through the use of class-based models, to handle arbitrarily large vocabularies and opens the way to enhanced neural translation models. In this paper, we explore various neural architectures f</context>
</contexts>
<marker>Liu, Gales, Woodland, 2011</marker>
<rawString>Xunying Liu, Mark J. F. Gales, and Philip C. Woodland. 2011. Improving lvcsr system combination using neural network language model cross adaptation. In INTERSPEECH, pages 2857–2860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrick Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonollosa, and Marta R. Costa-Juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527– 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proc. of ICASSP’11,</booktitle>
<pages>5528--5531</pages>
<marker>Mikolov, Kombrink, Burget, Cernock´y, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Proc. of ICASSP’11, pages 5528–5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<volume>21</volume>
<pages>1081--1088</pages>
<editor>In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<contexts>
<context position="15591" citStr="Mnih and Hinton (2008)" startWordPosition="2543" endWordPosition="2546">e neural network approach remains the complexity of inference and training, which largely depends on the size of the output vocabulary (i.e. the number of words that have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed of the most frequent words (Schwenk, 2007). However, the usual size of the short-list is under 20k, which does not seem sufficient to faithfully represent the translation models of section 2. 3.2 Principles of SOUL To circumvent this problem, Structured Output Layer (SOUL) LMs are introduced in (Le et al., 2011a). Following Mnih and Hinton (2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). StrucL P(wL1 ) _ i=1 42 turing the output layer and using word class information makes the estimation of distributions over the entire vocabulary computationally feasible. To meet this goal, the output vocabulary is structured as a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the ith word in a sentence, the sequence c1:D(wi) = c1, ... , cD encodes the path for word wi in the clustering tree, with D being the depth of the tree, cd(wi) a cl</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, volume 21, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider context by using bilingual language models in machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>198--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="6093" citStr="Niehues et al., 2011" startWordPosition="945" endWordPosition="948">ed system. The rest of this paper is organized as follows. We first recollect, in Section 2, the n-gram based approach, and discuss various implementations of this framework. We then describe, in Section 3, the neural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider context by using bilingual language models in machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198–206, Edinburgh, Scotland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="1897" citStr="Och and Ney, 2004" startWordPosition="293" endWordPosition="296">e underlying alignment a maximizing the following term: K P(t, a|s) = Z(s) exp ( 1: Akfk(s, t, a)), (1) k�� where K feature functions (fk) are weighted by a set of coefficients (Ak), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation 39 process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P(tIs), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization po</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proc. of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="23819" citStr="Och, 2003" startWordPosition="3919" endWordPosition="3920">e included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6geek.kyloo.net/software 44 alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7. For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to som</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proc. of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Yonggang Deng</author>
<author>Mohamed Afify</author>
<author>Brian Kingsbury</author>
<author>Yuqing Gao</author>
</authors>
<title>Machine translation in continuous space.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>2350--2353</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="30599" citStr="Sarikaya et al., 2008" startWordPosition="5084" endWordPosition="5087">tandard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model of (Bengio et al., 2003), the basic unit is the tuple (or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors had thus to restrict the set of the predicted units to a 8k short-list . Moreover, in (Zamora-Martinez et al., 2010), the authors propose a tighter integration of a continuous space model with a n-gram approach but only for the target LM. A different approach, described in (Sarikaya et al., 2008), divides the problem in two parts: first the continuous representation is obtained by an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translati</context>
</contexts>
<marker>Sarikaya, Deng, Afify, Kingsbury, Gao, 2008</marker>
<rawString>Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, Brian Kingsbury, and Yuqing Gao. 2008. Machine translation in continuous space. In Proceedings of Interspeech, pages 2350–2353, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Marta R Costa-Juss`a</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Smooth bilingual n-gram translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>430--438</pages>
<location>Prague, Czech Republic.</location>
<marker>Schwenk, Costa-Juss`a, Fonollosa, 2007</marker>
<rawString>Holger Schwenk, Marta R. Costa-Juss`a, and Jos´e A.R. Fonollosa. 2007. Smooth bilingual n-gram translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 430–438, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="3860" citStr="Schwenk, 2007" startWordPosition="593" endWordPosition="594">e standard approach of (Koehn et al., 2007). 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvements in speech recognition applications. Recently, this model has been extended in several promising ways (Mikolov et al., 2011; Kuo et al., 2010; Liu et al., 2011). In the context of SMT, Schwenk et al. (2007) is the first attempt to estimate translation probabilities in a continuous space. However, because of the proposed neural architecture, the authors only consider a very restricted set of translation units, and therefore report only a slight impact on translation performance. The recent proposal of (Le et al., 2011a) seems especially relevant, as it is able, thr</context>
<context position="15285" citStr="Schwenk, 2007" startWordPosition="2494" endWordPosition="2495">ed in the same continuous space using a shared matrix R; these continuous word representations are then concatenated to build a single vector that represents the context; after a non-linear transformation, the probability distribution is computed using a softmax layer. The major difficulty with the neural network approach remains the complexity of inference and training, which largely depends on the size of the output vocabulary (i.e. the number of words that have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed of the most frequent words (Schwenk, 2007). However, the usual size of the short-list is under 20k, which does not seem sufficient to faithfully represent the translation models of section 2. 3.2 Principles of SOUL To circumvent this problem, Structured Output Layer (SOUL) LMs are introduced in (Le et al., 2011a). Following Mnih and Hinton (2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). StrucL P(wL1 ) _ i=1 42 turing the output layer and using word class information makes the estimation of distributions over the entire vocabulary computationally feasible. To meet this goal, the o</context>
<context position="17750" citStr="Schwenk, 2007" startWordPosition="2905" endWordPosition="2906">he parameters on some training corpus. Following (Bengio et al., 2003), this optimization is performed by stochastic backpropagation. Details of the training procedure can be found in (Le et al., 2011b). Neural network architectures are also interesting as they can easily handle larger contexts than typical n-gram models. In the SOUL architecture, enlarging the context mainly consists in increasing the size of the projection layer, which corresponds to a simple look-up operation. Increasing the context length at the input layer thus only causes a linear growth in complexity in the worst case (Schwenk, 2007). Figure 2: The architecture of a SOUL Neural Network language model in the case of a 4-gram model. 3.3 Translation modeling with SOUL The SOUL architecture was used successfully to deliver (monolingual) LMs probabilities for speech recognition (Le et al., 2011a) and machine translation (Allauzen et al., 2011) applications. In fact, using this architecture, it is possible to estimate ngram distributions for any kind of discrete random variables, such as a phrase or a tuple. The SOUL architecture can thus be readily used as a replacement for the standard n-gram TM described in section 2.1. This</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeh W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of ACL’06,</booktitle>
<pages>985--992</pages>
<location>Sidney, Australia.</location>
<contexts>
<context position="13930" citStr="Teh, 2006" startWordPosition="2284" endWordPosition="2285">istributions over finite sequences of tokens (typically words) wL1 in V+ as follows: P(wi|wi−1 i−n+1) (6) Modeling the joint distribution of several discrete random variables (such as words in a sentence) is difficult, especially in NLP applications where V typically contains dozens of thousands words. In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams. Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This turns n-grams distributions into smooth functions of the word representations. These representations and the associated estimates are jointly computed in a multi-layer neural network architecture. Figure 2 provides a partial representation of</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yeh W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of ACL’06, pages 985–992, Sidney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>101--104</pages>
<contexts>
<context position="23316" citStr="Tillmann, 2004" startWordPosition="3841" endWordPosition="3843">In a nutshell, the TM is implemented as a stochastic finite-state transducer trained using a ngram model of (source, target) pairs as described in section 2.1. Training this model requires to reorder source sentences so as to match the target word order. This is performed by a non-deterministic finitestate reordering model, which uses part-of-speech information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities. In addition to the TM, fourteen feature functions are included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6geek.kyloo.net/software 44 alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7. For the small task,</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL 2004, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Zamora-Martinez</author>
<author>Maria Jos´e Castro-Bleda</author>
<author>Holger Schwenk</author>
</authors>
<title>N-gram-based Machine Translation enhanced with Neural Networks for the French-English BTEC-IWSLT’10 task.</title>
<date>2010</date>
<booktitle>In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>45--52</pages>
<contexts>
<context position="30418" citStr="Zamora-Martinez et al., 2010" startWordPosition="5054" endWordPosition="5057">ork To the best of our knowledge, the first work on machine translation in continuous spaces is (Schwenk et al., 2007), where the authors introduced the model referred here to as the the standard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model of (Bengio et al., 2003), the basic unit is the tuple (or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors had thus to restrict the set of the predicted units to a 8k short-list . Moreover, in (Zamora-Martinez et al., 2010), the authors propose a tighter integration of a continuous space model with a n-gram approach but only for the target LM. A different approach, described in (Sarikaya et al., 2008), divides the problem in two parts: first the continuous representation is obtained by an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the</context>
</contexts>
<marker>Zamora-Martinez, Castro-Bleda, Schwenk, 2010</marker>
<rawString>Francisco Zamora-Martinez, Maria Jos´e Castro-Bleda, and Holger Schwenk. 2010. N-gram-based Machine Translation enhanced with Neural Networks for the French-English BTEC-IWSLT’10 task. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In KI ’02: Proceedings of the 25th Annual German Conference on AI,</booktitle>
<pages>18--32</pages>
<publisher>SpringerVerlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="1759" citStr="Zens et al., 2002" startWordPosition="269" endWordPosition="272">machine translation (SMT) is based on the following inference rule, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K P(t, a|s) = Z(s) exp ( 1: Akfk(s, t, a)), (1) k�� where K feature functions (fk) are weighted by a set of coefficients (Ak), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation 39 process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P(tIs), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological,</context>
<context position="9440" citStr="Zens et al., 2002" startWordPosition="1516" endWordPosition="1519"> is that the elementary events now correspond either to source or to target phrases, but never to pairs of such phrases. The underlying vocabulary is thus obtained as the union, rather than the cross product, of phrase inventories. Finally note that the n-gram probability P(ui|ui−1, ..., ui−n+1) could also factor as: 2.3 A word factored translation model A more radical way to address the data sparsity issues is to take (source and target) words as the basic units of the n-gram TM. This may seem to be a step backwards, since the transition from word (Brown et al., 1993) to phrase-based models (Zens et al., 2002) is considered as one of the main recent improvement in MT. One important motivation for considering phrases rather than words was to capture local context in translation and reordering. It should then be stressed that the decomposition of phrases in words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams, as defined by equation (3). In fact, the estimation policy described in section 3 will actually allow us to design</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-based statistical machine translation. In KI ’02: Proceedings of the 25th Annual German Conference on AI, pages 18–32, London, UK. SpringerVerlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>