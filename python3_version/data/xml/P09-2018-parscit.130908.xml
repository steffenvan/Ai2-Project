<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000885">
<title confidence="0.834128">
Directional Distributional Similarity for Lexical Expansion
</title>
<author confidence="0.779539">
Maayan Zhitomirsky-Geffet
</author>
<affiliation confidence="0.743212666666667">
Department of Information Science
Bar-Ilan University
Ramat Gan, Israel
</affiliation>
<email confidence="0.982792">
maayan.geffet@gmail.com
</email>
<author confidence="0.98684">
Lili Kotlerman, Ido Dagan, Idan Szpektor
</author>
<affiliation confidence="0.846828">
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
</affiliation>
<email confidence="0.969362">
lili.dav@gmail.com
{dagan,szpekti}@cs.biu.ac.il
</email>
<sectionHeader confidence="0.994705" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995325">
Distributional word similarity is most
commonly perceived as a symmetric re-
lation. Yet, one of its major applications
is lexical expansion, which is generally
asymmetric. This paper investigates the
nature of directional (asymmetric) similar-
ity measures, which aim to quantify distri-
butional feature inclusion. We identify de-
sired properties of such measures, specify
a particular one based on averaged preci-
sion, and demonstrate the empirical bene-
fit of directional measures for expansion.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999701875">
Much work on automatic identification of seman-
tically similar terms exploits Distributional Simi-
larity, assuming that such terms appear in similar
contexts. This has been now an active research
area for a couple of decades (Hindle, 1990; Lin,
1998; Weeds and Weir, 2003).
This paper is motivated by one of the prominent
applications of distributional similarity, namely
identifying lexical expansions. Lexical expansion
looks for terms whose meaning implies that of a
given target term, such as a query. It is widely
employed to overcome lexical variability in ap-
plications like Information Retrieval (IR), Infor-
mation Extraction (IE) and Question Answering
(QA). Often, distributional similarity measures are
used to identify expanding terms (e.g. (Xu and
Croft, 1996; Mandala et al., 1999)). Here we de-
note the relation between an expanding term u and
an expanded term v as ‘u → v’.
While distributional similarity is most promi-
nently modeled by symmetric measures, lexical
expansion is in general a directional relation. In
IR, for instance, a user looking for “baby food”
will be satisfied with documents about “baby pap”
or “baby juice” (‘pap → food’, ‘juice → food’);
but when looking for “frozen juice” she will not
be satisfied by “frozen food”. More generally, di-
rectional relations are abundant in NLP settings,
making symmetric similarity measures less suit-
able for their identification.
Despite the need for directional similarity mea-
sures, their investigation counts, to the best of
our knowledge, only few works (Weeds and Weir,
2003; Geffet and Dagan, 2005; Bhagat et al.,
2007; Szpektor and Dagan, 2008; Michelbacher et
al., 2007) and is utterly lacking. From an expan-
sion perspective, the common expectation is that
the context features characterizing an expanding
word should be largely included in those of the ex-
panded word.
This paper investigates the nature of directional
similarity measures. We identify their desired
properties, design a novel measure based on these
properties, and demonstrate its empirical advan-
tage in expansion settings over state-of-the-art
measures1. In broader prospect, we suggest that
asymmetric measures might be more suitable than
symmetric ones for many other settings as well.
</bodyText>
<sectionHeader confidence="0.977076" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999889571428571">
The distributional word similarity scheme follows
two steps. First, a feature vector is constructed
for each word by collecting context words as fea-
tures. Each feature is assigned a weight indicating
its “relevance” (or association) to the given word.
Then, word vectors are compared by some vector
similarity measure.
</bodyText>
<footnote confidence="0.993448333333333">
1Our directional term-similarity resource will be available
at http://aclweb.org/aclwiki/index.php?
title=Textual_Entailment_Resource_Pool
</footnote>
<page confidence="0.981371">
69
</page>
<note confidence="0.946095">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69–72,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.578375">
EfEFVu wu(f) + EfEFVv wv(f)
</equation>
<bodyText confidence="0.986214818181818">
where FVx is the feature vector of a word x and
wx(f) is the weight of the feature f in that word’s
vector, set to their pointwise mutual information.
Few works investigated a directional similarity
approach. Weeds and Weir (2003) and Weeds et
al. (2004) proposed a precision measure, denoted
here WeedsPrec, for identifying the hyponymy re-
lation and other generalization/specification cases.
It quantifies the weighted coverage (or inclusion)
of the candidate hyponym’s features (u) by the hy-
pernym’s (v) features:
</bodyText>
<equation confidence="0.9856425">
WeedsPrec(u → v) = Ef EFVunFVv wu(f )
E fEFVu wu(f)
</equation>
<bodyText confidence="0.98778312">
The assumption behind WeedsPrec is that if one
word is indeed a generalization of the other then
the features of the more specific word are likely to
be included in those of the more general one (but
not necessarily vice versa).
Extending this rationale to the textual entail-
ment setting, Geffet and Dagan (2005) expected
that if the meaning of a word u entails that of
v then all its prominent context features (under
a certain notion of “prominence”) would be in-
cluded in the feature vector of v as well. Their
experiments indeed revealed a strong empirical
correlation between such complete inclusion of
prominent features and lexical entailment, based
on web data. Yet, such complete inclusion cannot
be feasibly assessed using an off-line corpus, due
to the huge amount of required data.
Recently, (Szpektor and Dagan, 2008) tried
identifying the entailment relation between
lexical-syntactic templates using WeedsPrec, but
observed that it tends to promote unreliable rela-
tions involving infrequent templates. To remedy
this, they proposed to balance the directional
WeedsPrec measure by multiplying it with the
symmetric LIN measure, denoted here balPrec:
</bodyText>
<equation confidence="0.949293">
V
balPrec(u→v)= LIN(u, v)·WeedsPrec(u→v)
</equation>
<bodyText confidence="0.99992525">
Effectively, this measure penalizes infrequent tem-
plates having short feature vectors, as those usu-
ally yield low symmetric similarity with the longer
vectors of more common templates.
</bodyText>
<sectionHeader confidence="0.904963" genericHeader="method">
3 A Statistical Inclusion Measure
</sectionHeader>
<bodyText confidence="0.999613733333333">
Our research goal was to develop a directional
similarity measure suitable for learning asymmet-
ric relations, focusing empirically on lexical ex-
pansion. Thus, we aimed to quantify most effec-
tively the above notion of feature inclusion.
For a candidate pair ‘u → v’, we will refer to
the set of u’s features, which are those tested for
inclusion, as tested features. Amongst these fea-
tures, those found in v’s feature vector are termed
included features.
In preliminary data analysis of pairs of feature
vectors, which correspond to a known set of valid
and invalid expansions, we identified the follow-
ing desired properties for a distributional inclusion
measure. Such measure should reflect:
</bodyText>
<listItem confidence="0.995726444444445">
1. the proportion of included features amongst
the tested ones (the core inclusion idea).
2. the relevance of included features to the ex-
panding word.
3. the relevance of included features to the ex-
panded word.
4. that inclusion detection is less reliable if the
number of features of either expanding or ex-
panded word is small.
</listItem>
<subsectionHeader confidence="0.761527">
3.1 Average Precision as the Basis for an
Inclusion Measure
</subsectionHeader>
<bodyText confidence="0.999909">
As our starting point we adapted the Average
Precision (AP) metric, commonly used to score
ranked lists such as query search results. This
measure combines precision, relevance ranking
and overall recall (Voorhees and Harman, 1999):
</bodyText>
<equation confidence="0.98735">
EN r=1[P(r) · rel(r)]
AP =
</equation>
<bodyText confidence="0.985963333333333">
total number of relevant documents
where r is the rank of a retrieved document
amongst the N retrieved, rel(r) is an indicator
function for the relevance of that document, and
P(r) is precision at the given cut-off rank r.
In our case the feature vector of the expanded
word is analogous to the set of all relevant docu-
ments while tested features correspond to retrieved
documents. Included features thus correspond to
relevant retrieved documents, yielding the follow-
To date, most distributional similarity research
concentrated on symmetric measures, such as the
widely cited and competitive (as shown in (Weeds
and Weir, 2003)) LIN measure (Lin, 1998):
LIN(u v) _ Ef EFVunFUv [wu (f ) + wv (f
</bodyText>
<equation confidence="0.370682666666667">
, )]
70
r
</equation>
<bodyText confidence="0.99993035">
where fr is the feature at rank r in FVu.
This analogy yields a feature inclusion measure
that partly addresses the above desired properties.
Its score increases with a larger number of in-
cluded features (correlating with the 1st property),
while giving higher weight to highly ranked fea-
tures of the expanding word (2nd property).
To better meet the desired properties we in-
troduce two modifications to the above measure.
First, we use the number of tested features |FVu|
for normalization instead of |FVv|. This captures
better the notion of feature inclusion (1st property),
which targets the proportion of included features
relative to the tested ones.
Second, in the classical AP formula all relevant
documents are considered relevant to the same ex-
tent. However, features of the expanded word dif-
fer in their relevance within its vector (3rd prop-
erty). We thus reformulate rel(f) to give higher
relevance to highly ranked features in |FVv|:
</bodyText>
<equation confidence="0.987973">
rel = � 1 − ral k(f,FVv) if f ∈ FV
�(f) 0 l if f ∈/ FVv
</equation>
<bodyText confidence="0.998372333333333">
where rank(f, FVv) is the rank of f in FVv.
Incorporating these two modifications yields the
APinc measure:
</bodyText>
<equation confidence="0.985132666666667">
 |FVu Prl
APinc(u → v) = Er=1 ()·rel0 (fr)]
|FVu|
</equation>
<bodyText confidence="0.999225714285714">
Finally, we adopt the balancing approach in
(Szpektor and Dagan, 2008), which, as explained
in Section 2, penalizes similarity for infrequent
words having fewer features (4th property) (in our
version, we truncated LIN similarity lists after top
1000 words). This yields our proposed directional
measure balAPinc:
</bodyText>
<equation confidence="0.977152">
VI
balAPinc(u→v) = LIN(u, v) · APinc(u→v)
</equation>
<sectionHeader confidence="0.993908" genericHeader="method">
4 Evaluation and Results
</sectionHeader>
<subsectionHeader confidence="0.992123">
4.1 Evaluation Setting
</subsectionHeader>
<bodyText confidence="0.999908657894737">
We tested our similarity measure by evaluating its
utility for lexical expansion, compared with base-
lines of the LIN, WeedsPrec and balPrec measures
(Section 2) and a balanced version of AP (Sec-
tion 3), denoted balAP. Feature vectors were cre-
ated by parsing the Reuters RCV1 corpus and tak-
ing the words related to each term through a de-
pendency relation as its features (coupled with the
relation name and direction, as in (Lin, 1998)). We
considered for expansion only terms that occur at
least 10 times in the corpus, and as features only
terms that occur at least twice.
As a typical lexical expansion task we used
the ACE 2005 events dataset2. This standard IE
dataset specifies 33 event types, such as Attack,
Divorce, and Law Suit, with all event mentions
annotated in the corpus. For our lexical expan-
sion evaluation we considered the first IE subtask
of finding sentences that mention the event.
For each event we specified a set of representa-
tive words (seeds), by selecting typical terms for
the event (4 on average) from its ACE definition.
Next, for each similarity measure, the terms found
similar to any of the event’s seeds (‘u → seed’)
were taken as expansion terms. Finally, to mea-
sure the sole contribution of expansion, we re-
moved from the corpus all sentences that contain
a seed word and then extracted all sentences that
contain expansion terms as mentioning the event.
Each of these sentences was scored by the sum of
similarity scores of its expansion terms.
To evaluate expansion quality we compared the
ranked list of sentences for each event to the gold-
standard annotation of event mentions, using the
standard Average Precision (AP) evaluation mea-
sure. We report Mean Average Precision (MAP)
for all events whose AP value is at least 0.1 for at
least one of the tested measures3.
</bodyText>
<sectionHeader confidence="0.671725" genericHeader="evaluation">
4.1.1 Results
</sectionHeader>
<bodyText confidence="0.999838181818182">
Table 1 presents the results for the different tested
measures over the ACE experiment. It shows that
the symmetric LIN measure performs significantly
worse than the directional measures, assessing that
a directional approach is more suitable for the ex-
pansion task. In addition, balanced measures con-
sistently perform better than unbalanced ones.
According to the results, balAPinc is the best-
performing measure. Its improvement over all
other measures is statistically significant accord-
ing to the two-sided Wilcoxon signed-rank test
</bodyText>
<footnote confidence="0.8679398">
2http://projects.ldc.upenn.edu/ace/, training part.
3The remaining events seemed useless for our compar-
ative evaluation, since suitable expansion lists could not be
found for them by any of the distributional methods.
ing analogous measure in our terminology:
</footnote>
<equation confidence="0.9984745">
AP(u → v) = E|r=V1u  |[P(r) · rel(fr)]
|FVv|
� 1, if f ∈ FVv
rel(f) =
0, if f ∈/ FVv
P(r) = |included features in ranks 1 to r|
</equation>
<page confidence="0.996201">
71
</page>
<table confidence="0.9528145">
LIN WeedsPrec balPrec AP balAP balAPinc
0.068 0.044 0.237 0.089 0.202 0.312
</table>
<tableCaption confidence="0.9538575">
Table 1: MAP scores of the tested measures on the
ACE experiment.
</tableCaption>
<bodyText confidence="0.981972823529412">
seed LIN balAPinc
death murder, killing, inci- suicide, killing, fatal-
dent, arrest, violence ity, murder, mortality
marry divorce, murder, love, divorce, remarry,
dress, abduct father, kiss, care for
arrest detain, sentence, detain, extradite,
charge, jail, convict round up, apprehend,
imprison
birth abortion, pregnancy, wedding day,
resumption, seizure, dilation, birthdate,
passage circumcision, triplet
injure wound, kill, shoot, wound, maim, beat
detain, burn up, stab, gun down
Table 2: Top 5 expansion terms learned by LIN
and balAPinc for a sample of ACE seed words.
(Wilcoxon, 1945) at the 0.01 level. Table 2
presents a sample of the top expansion terms
learned for some ACE seeds with either LIN or
balAPinc, demonstrating the more accurate ex-
pansions generated by balAPinc. These results
support the design of our measure, based on the
desired properties that emerged from preliminary
data analysis for lexical expansion.
Finally, we note that in related experiments we
observed statistically significant advantages of the
balAPinc measure for an unsupervised text catego-
rization task (on the 10 most frequent categories in
the Reuters-21578 collection). In this setting, cat-
egory names were taken as seeds and expanded by
distributional similarity, further measuring cosine
similarity with categorized documents similarly to
IR query expansion. These experiments fall be-
yond the scope of this paper and will be included
in a later and broader description of our work.
</bodyText>
<sectionHeader confidence="0.997757" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.9996495">
This paper advocates the use of directional similar-
ity measures for lexical expansion, and potentially
for other tasks, based on distributional inclusion of
feature vectors. We first identified desired proper-
ties for an inclusion measure and accordingly de-
signed a novel directional measure based on av-
eraged precision. This measure yielded the best
performance in our evaluations. More generally,
the evaluations supported the advantage of multi-
ple directional measures over the typical symmet-
ric LIN measure.
Error analysis showed that many false sentence
extractions were caused by ambiguous expanding
and expanded words. In future work we plan to
apply disambiguation techniques to address this
problem. We also plan to evaluate the performance
of directional measures in additional tasks, and
compare it with additional symmetric measures.
</bodyText>
<sectionHeader confidence="0.996383" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9975626">
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886 and by the Israel
Science Foundation grant 1112/08.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99980303030303">
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An
unsupervised algorithm for learning directionality of
inference rules. In Proceedings of EMNLP-CoNLL.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of SI-
GIR.
L. Michelbacher, S. Evert, and H. Schutze. 2007.
Asymmetric association measures. In Proceedings
of RANLP.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING.
E. M. Voorhees and D. K. Harman, editors. 1999. The
Seventh Text REtrieval Conference (TREC-7), vol-
ume 7. NIST.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING.
F. Wilcoxon. 1945. Individual comparisons by ranking
methods. Biometrics Bulletin, 1:80–83.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings
of SIGIR.
</reference>
<page confidence="0.99872">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787471">
<title confidence="0.999014">Directional Distributional Similarity for Lexical Expansion</title>
<author confidence="0.997225">Maayan Zhitomirsky-Geffet</author>
<affiliation confidence="0.999907">Department of Information Science Bar-Ilan University</affiliation>
<address confidence="0.882378">Ramat Gan, Israel</address>
<email confidence="0.99989">maayan.geffet@gmail.com</email>
<author confidence="0.996978">Lili Kotlerman</author>
<author confidence="0.996978">Ido Dagan</author>
<author confidence="0.996978">Idan Szpektor</author>
<affiliation confidence="0.999803">Department of Computer Science Bar-Ilan University</affiliation>
<address confidence="0.947438">Ramat Gan, Israel</address>
<email confidence="0.999957">lili.dav@gmail.com</email>
<abstract confidence="0.995903461538462">Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bhagat</author>
<author>P Pantel</author>
<author>E Hovy</author>
</authors>
<title>LEDIR: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2447" citStr="Bhagat et al., 2007" startWordPosition="358" endWordPosition="361">al expansion is in general a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1. In broader prospect, we suggest that asymmetric measures might be more suitable than symmetri</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2426" citStr="Geffet and Dagan, 2005" startWordPosition="354" endWordPosition="357">ymmetric measures, lexical expansion is in general a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1. In broader prospect, we suggest that asymmetric measures might be more s</context>
<context position="4597" citStr="Geffet and Dagan (2005)" startWordPosition="685" endWordPosition="688">oposed a precision measure, denoted here WeedsPrec, for identifying the hyponymy relation and other generalization/specification cases. It quantifies the weighted coverage (or inclusion) of the candidate hyponym’s features (u) by the hypernym’s (v) features: WeedsPrec(u → v) = Ef EFVunFVv wu(f ) E fEFVu wu(f) The assumption behind WeedsPrec is that if one word is indeed a generalization of the other then the features of the more specific word are likely to be included in those of the more general one (but not necessarily vice versa). Extending this rationale to the textual entailment setting, Geffet and Dagan (2005) expected that if the meaning of a word u entails that of v then all its prominent context features (under a certain notion of “prominence”) would be included in the feature vector of v as well. Their experiments indeed revealed a strong empirical correlation between such complete inclusion of prominent features and lexical entailment, based on web data. Yet, such complete inclusion cannot be feasibly assessed using an off-line corpus, due to the huge amount of required data. Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates usin</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1091" citStr="Hindle, 1990" startWordPosition="145" endWordPosition="146">xpansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion. 1 Introduction Much work on automatic identification of semantically similar terms exploits Distributional Similarity, assuming that such terms appear in similar contexts. This has been now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an exp</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1102" citStr="Lin, 1998" startWordPosition="147" endWordPosition="148">h is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion. 1 Introduction Much work on automatic identification of semantically similar terms exploits Distributional Similarity, assuming that such terms appear in similar contexts. This has been now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an expanding term</context>
<context position="7704" citStr="Lin, 1998" startWordPosition="1176" endWordPosition="1177">s the rank of a retrieved document amongst the N retrieved, rel(r) is an indicator function for the relevance of that document, and P(r) is precision at the given cut-off rank r. In our case the feature vector of the expanded word is analogous to the set of all relevant documents while tested features correspond to retrieved documents. Included features thus correspond to relevant retrieved documents, yielding the followTo date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN(u v) _ Ef EFVunFUv [wu (f ) + wv (f , )] 70 r where fr is the feature at rank r in FVu. This analogy yields a feature inclusion measure that partly addresses the above desired properties. Its score increases with a larger number of included features (correlating with the 1st property), while giving higher weight to highly ranked features of the expanding word (2nd property). To better meet the desired properties we introduce two modifications to the above measure. First, we use the number of tested features |FVu| for normalization instead of |FVv|. This captures better the notion of feat</context>
<context position="9754" citStr="Lin, 1998" startWordPosition="1526" endWordPosition="1527">y lists after top 1000 words). This yields our proposed directional measure balAPinc: VI balAPinc(u→v) = LIN(u, v) · APinc(u→v) 4 Evaluation and Results 4.1 Evaluation Setting We tested our similarity measure by evaluating its utility for lexical expansion, compared with baselines of the LIN, WeedsPrec and balPrec measures (Section 2) and a balanced version of AP (Section 3), denoted balAP. Feature vectors were created by parsing the Reuters RCV1 corpus and taking the words related to each term through a dependency relation as its features (coupled with the relation name and direction, as in (Lin, 1998)). We considered for expansion only terms that occur at least 10 times in the corpus, and as features only terms that occur at least twice. As a typical lexical expansion task we used the ACE 2005 events dataset2. This standard IE dataset specifies 33 event types, such as Attack, Divorce, and Law Suit, with all event mentions annotated in the corpus. For our lexical expansion evaluation we considered the first IE subtask of finding sentences that mention the event. For each event we specified a set of representative words (seeds), by selecting typical terms for the event (4 on average) from it</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mandala</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>Combining multiple evidence from different types of thesaurus for query expansion.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1646" citStr="Mandala et al., 1999" startWordPosition="226" endWordPosition="229"> now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an expanding term u and an expanded term v as ‘u → v’. While distributional similarity is most prominently modeled by symmetric measures, lexical expansion is in general a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identifi</context>
</contexts>
<marker>Mandala, Tokunaga, Tanaka, 1999</marker>
<rawString>R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Combining multiple evidence from different types of thesaurus for query expansion. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Michelbacher</author>
<author>S Evert</author>
<author>H Schutze</author>
</authors>
<title>Asymmetric association measures.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="2501" citStr="Michelbacher et al., 2007" startWordPosition="366" endWordPosition="369">on. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1. In broader prospect, we suggest that asymmetric measures might be more suitable than symmetric ones for many other settings as well. 2 Background T</context>
</contexts>
<marker>Michelbacher, Evert, Schutze, 2007</marker>
<rawString>L. Michelbacher, S. Evert, and H. Schutze. 2007. Asymmetric association measures. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2473" citStr="Szpektor and Dagan, 2008" startWordPosition="362" endWordPosition="365">neral a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1. In broader prospect, we suggest that asymmetric measures might be more suitable than symmetric ones for many other sett</context>
<context position="5114" citStr="Szpektor and Dagan, 2008" startWordPosition="769" endWordPosition="772">necessarily vice versa). Extending this rationale to the textual entailment setting, Geffet and Dagan (2005) expected that if the meaning of a word u entails that of v then all its prominent context features (under a certain notion of “prominence”) would be included in the feature vector of v as well. Their experiments indeed revealed a strong empirical correlation between such complete inclusion of prominent features and lexical entailment, based on web data. Yet, such complete inclusion cannot be feasibly assessed using an off-line corpus, due to the huge amount of required data. Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. To remedy this, they proposed to balance the directional WeedsPrec measure by multiplying it with the symmetric LIN measure, denoted here balPrec: V balPrec(u→v)= LIN(u, v)·WeedsPrec(u→v) Effectively, this measure penalizes infrequent templates having short feature vectors, as those usually yield low symmetric similarity with the longer vectors of more common templates. 3 A Statistical Inclusion Measure Our re</context>
<context position="8986" citStr="Szpektor and Dagan, 2008" startWordPosition="1400" endWordPosition="1403">n of included features relative to the tested ones. Second, in the classical AP formula all relevant documents are considered relevant to the same extent. However, features of the expanded word differ in their relevance within its vector (3rd property). We thus reformulate rel(f) to give higher relevance to highly ranked features in |FVv|: rel = � 1 − ral k(f,FVv) if f ∈ FV �(f) 0 l if f ∈/ FVv where rank(f, FVv) is the rank of f in FVv. Incorporating these two modifications yields the APinc measure: |FVu Prl APinc(u → v) = Er=1 ()·rel0 (fr)] |FVu| Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4th property) (in our version, we truncated LIN similarity lists after top 1000 words). This yields our proposed directional measure balAPinc: VI balAPinc(u→v) = LIN(u, v) · APinc(u→v) 4 Evaluation and Results 4.1 Evaluation Setting We tested our similarity measure by evaluating its utility for lexical expansion, compared with baselines of the LIN, WeedsPrec and balPrec measures (Section 2) and a balanced version of AP (Section 3), denoted balAP. Feature vectors were created by parsing the Reut</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>The Seventh Text REtrieval Conference (TREC-7),</booktitle>
<volume>7</volume>
<editor>E. M. Voorhees and D. K. Harman, editors.</editor>
<publisher>NIST.</publisher>
<marker>1999</marker>
<rawString>E. M. Voorhees and D. K. Harman, editors. 1999. The Seventh Text REtrieval Conference (TREC-7), volume 7. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1125" citStr="Weeds and Weir, 2003" startWordPosition="149" endWordPosition="152">lly asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion. 1 Introduction Much work on automatic identification of semantically similar terms exploits Distributional Similarity, assuming that such terms appear in similar contexts. This has been now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an expanding term u and an expanded term</context>
<context position="2402" citStr="Weeds and Weir, 2003" startWordPosition="350" endWordPosition="353">ominently modeled by symmetric measures, lexical expansion is in general a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1. In broader prospect, we suggest that asymmetric </context>
<context position="3947" citStr="Weeds and Weir (2003)" startWordPosition="579" endWordPosition="582">ation) to the given word. Then, word vectors are compared by some vector similarity measure. 1Our directional term-similarity resource will be available at http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool 69 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69–72, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP EfEFVu wu(f) + EfEFVv wv(f) where FVx is the feature vector of a word x and wx(f) is the weight of the feature f in that word’s vector, set to their pointwise mutual information. Few works investigated a directional similarity approach. Weeds and Weir (2003) and Weeds et al. (2004) proposed a precision measure, denoted here WeedsPrec, for identifying the hyponymy relation and other generalization/specification cases. It quantifies the weighted coverage (or inclusion) of the candidate hyponym’s features (u) by the hypernym’s (v) features: WeedsPrec(u → v) = Ef EFVunFVv wu(f ) E fEFVu wu(f) The assumption behind WeedsPrec is that if one word is indeed a generalization of the other then the features of the more specific word are likely to be included in those of the more general one (but not necessarily vice versa). Extending this rationale to the t</context>
<context position="7679" citStr="Weeds and Weir, 2003" startWordPosition="1170" endWordPosition="1173">mber of relevant documents where r is the rank of a retrieved document amongst the N retrieved, rel(r) is an indicator function for the relevance of that document, and P(r) is precision at the given cut-off rank r. In our case the feature vector of the expanded word is analogous to the set of all relevant documents while tested features correspond to retrieved documents. Included features thus correspond to relevant retrieved documents, yielding the followTo date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN(u v) _ Ef EFVunFUv [wu (f ) + wv (f , )] 70 r where fr is the feature at rank r in FVu. This analogy yields a feature inclusion measure that partly addresses the above desired properties. Its score increases with a larger number of included features (correlating with the 1st property), while giving higher weight to highly ranked features of the expanding word (2nd property). To better meet the desired properties we introduce two modifications to the above measure. First, we use the number of tested features |FVu| for normalization instead of |FVv|. This captures </context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>J. Weeds and D. Weir. 2003. A general framework for distributional similarity. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
<author>D McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3971" citStr="Weeds et al. (2004)" startWordPosition="584" endWordPosition="587">Then, word vectors are compared by some vector similarity measure. 1Our directional term-similarity resource will be available at http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool 69 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69–72, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP EfEFVu wu(f) + EfEFVv wv(f) where FVx is the feature vector of a word x and wx(f) is the weight of the feature f in that word’s vector, set to their pointwise mutual information. Few works investigated a directional similarity approach. Weeds and Weir (2003) and Weeds et al. (2004) proposed a precision measure, denoted here WeedsPrec, for identifying the hyponymy relation and other generalization/specification cases. It quantifies the weighted coverage (or inclusion) of the candidate hyponym’s features (u) by the hypernym’s (v) features: WeedsPrec(u → v) = Ef EFVunFVv wu(f ) E fEFVu wu(f) The assumption behind WeedsPrec is that if one word is indeed a generalization of the other then the features of the more specific word are likely to be included in those of the more general one (but not necessarily vice versa). Extending this rationale to the textual entailment settin</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>J. Weeds, D. Weir, and D. McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wilcoxon</author>
</authors>
<title>Individual comparisons by ranking methods.</title>
<date>1945</date>
<journal>Biometrics Bulletin,</journal>
<pages>1--80</pages>
<contexts>
<context position="12802" citStr="Wilcoxon, 1945" startWordPosition="2018" endWordPosition="2019">on the ACE experiment. seed LIN balAPinc death murder, killing, inci- suicide, killing, fataldent, arrest, violence ity, murder, mortality marry divorce, murder, love, divorce, remarry, dress, abduct father, kiss, care for arrest detain, sentence, detain, extradite, charge, jail, convict round up, apprehend, imprison birth abortion, pregnancy, wedding day, resumption, seizure, dilation, birthdate, passage circumcision, triplet injure wound, kill, shoot, wound, maim, beat detain, burn up, stab, gun down Table 2: Top 5 expansion terms learned by LIN and balAPinc for a sample of ACE seed words. (Wilcoxon, 1945) at the 0.01 level. Table 2 presents a sample of the top expansion terms learned for some ACE seeds with either LIN or balAPinc, demonstrating the more accurate expansions generated by balAPinc. These results support the design of our measure, based on the desired properties that emerged from preliminary data analysis for lexical expansion. Finally, we note that in related experiments we observed statistically significant advantages of the balAPinc measure for an unsupervised text categorization task (on the 10 most frequent categories in the Reuters-21578 collection). In this setting, categor</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>F. Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1:80–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>W B Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1623" citStr="Xu and Croft, 1996" startWordPosition="222" endWordPosition="225">texts. This has been now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an expanding term u and an expanded term v as ‘u → v’. While distributional similarity is most prominently modeled by symmetric measures, lexical expansion is in general a directional relation. In IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suit</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>J. Xu and W. B. Croft. 1996. Query expansion using local and global document analysis. In Proceedings of SIGIR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>