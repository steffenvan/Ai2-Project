<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000114">
<title confidence="0.980664">
Learning to Follow Navigational Directions
</title>
<author confidence="0.998792">
Adam Vogel and Dan Jurafsky
</author>
<affiliation confidence="0.9939195">
Department of Computer Science
Stanford University
</affiliation>
<email confidence="0.993813">
{acvogel,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.997342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962315789474">
We present a system that learns to fol-
low navigational natural language direc-
tions. Where traditional models learn
from linguistic annotation or word distri-
butions, our approach is grounded in the
world, learning by apprenticeship from
routes through a map paired with English
descriptions. Lacking an explicit align-
ment between the text and the reference
path makes it difficult to determine what
portions of the language describe which
aspects of the route. We learn this corre-
spondence with a reinforcement learning
algorithm, using the deviation of the route
we follow from the intended path as a re-
ward signal. We demonstrate that our sys-
tem successfully grounds the meaning of
spatial terms like above and south into ge-
ometric properties of paths.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981444444444">
Spatial language usage is a vital component for
physically grounded language understanding sys-
tems. Spoken language interfaces to robotic assis-
tants (Wei et al., 2009) and Geographic Informa-
tion Systems (Wang et al., 2004) must cope with
the inherent ambiguity in spatial descriptions.
The semantics of imperative and spatial lan-
guage is heavily dependent on the physical set-
ting it is situated in, motivating automated learn-
ing approaches to acquiring meaning. Tradi-
tional accounts of learning typically rely on lin-
guistic annotation (Zettlemoyer and Collins, 2009)
or word distributions (Curran, 2003). In con-
trast, we present an apprenticeship learning sys-
tem which learns to imitate human instruction fol-
lowing, without linguistic annotation. Solved us-
ing a reinforcement learning algorithm, our sys-
tem acquires the meaning of spatial words through
</bodyText>
<listItem confidence="0.996952">
1. go vertically down until you’re underneath eh
diamond mine
2. then eh go right until you’re
3. you’re between springbok and highest view-
point
</listItem>
<figureCaption confidence="0.7396955">
Figure 1: A path appears on the instruction giver’s
map, who describes it to the instruction follower.
</figureCaption>
<bodyText confidence="0.999774083333333">
grounded interaction with the world. This draws
on the intuition that children learn to use spatial
language through a mixture of observing adult lan-
guage usage and situated interaction in the world,
usually without explicit definitions (Tanz, 1980).
Our system learns to follow navigational direc-
tions in a route following task. We evaluate our
approach on the HCRC Map Task corpus (Ander-
son et al., 1991), a collection of spoken dialogs
describing paths to take through a map. In this
setting, two participants, the instruction giver and
instruction follower, each have a map composed
of named landmarks. Furthermore, the instruc-
tion giver has a route drawn on her map, and it
is her task to describe the path to the instruction
follower, who cannot see the reference path. Our
system learns to interpret these navigational direc-
tions, without access to explicit linguistic annota-
tion.
We frame direction following as an apprentice-
ship learning problem and solve it with a rein-
forcement learning algorithm, extending previous
work on interpreting instructions by Branavan et
al. (2009). Our task is to learn a policy, or mapping
</bodyText>
<page confidence="0.978431">
806
</page>
<note confidence="0.943387">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999845090909091">
from world state to action, which most closely fol-
lows the reference route. Our state space com-
bines world and linguistic features, representing
both our current position on the map and the com-
municative content of the utterances we are inter-
preting. During training we have access to the ref-
erence path, which allows us to measure the util-
ity, or reward, for each step of interpretation. Us-
ing this reward signal as a form of supervision, we
learn a policy to maximize the expected reward on
unseen examples.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999954184210526">
Levit and Roy (2007) developed a spatial seman-
tics for the Map Task corpus. They represent
instructions as Navigational Information Units,
which decompose the meaning of an instruction
into orthogonal constituents such as the reference
object, the type of movement, and quantitative as-
pect. For example, they represent the meaning of
“move two inches toward the house” as a reference
object (the house), a path descriptor (towards), and
a quantitative aspect (two inches). These represen-
tations are then combined to form a path through
the map. However, they do not learn these rep-
resentations from text, leaving natural language
processing as an open problem. The semantics
in our paper is simpler, eschewing quantitative as-
pects and path descriptors, and instead focusing
on reference objects and frames of reference. This
simplifies the learning task, without sacrificing the
core of their representation.
Learning to follow instructions by interacting
with the world was recently introduced by Brana-
van et al. (2009), who developed a system which
learns to follow Windows Help guides. Our re-
inforcement learning formulation follows closely
from their work. Their approach can incorpo-
rate expert supervision into the reward function
in a similar manner to this paper, but is also able
to learn effectively from environment feedback
alone. The Map Task corpus is free form conversa-
tional English, whereas the Windows instructions
are written by a professional. In the Map Task cor-
pus we only observe expert route following behav-
ior, but are not told how portions of the text cor-
respond to parts of the path, leading to a difficult
learning problem.
The semantics of spatial language has been
studied for some time in the linguistics literature.
Talmy (1983) classifies the way spatial meaning is
</bodyText>
<figureCaption confidence="0.536292666666667">
Figure 2: The instruction giver and instruction fol-
lower face each other, and cannot see each others
maps.
</figureCaption>
<bodyText confidence="0.995459714285714">
encoded syntactically, and Fillmore (1997) studies
spatial terms as a subset of deictic language, which
depends heavily on non-linguistic context. Levin-
son (2003) conducted a cross-linguistic semantic
typology of spatial systems. Levinson categorizes
the frames of reference, or spatial coordinate sys-
tems1, into
</bodyText>
<listItem confidence="0.97010825">
1. Egocentric: Speaker/hearer centered frame
of reference. Ex: “the ball to your left”.
2. Allocentric: Speaker independent. Ex: “the
road to the north of the house”
</listItem>
<bodyText confidence="0.9995083125">
Levinson further classifies allocentric frames of
reference into absolute, which includes the cardi-
nal directions, and intrinsic, which refers to a fea-
tured side of an object, such as “the front of the
car”. Our spatial feature representation follows
this egocentric/allocentric distinction. The intrin-
sic frame of reference occurs rarely in the Map
Task corpus and is ignored, as speakers tend not
to mention features of the landmarks beyond their
names.
Regier (1996) studied the learning of spatial
language from static 2-D diagrams, learning to
distinguish between terms with a connectionist
model. He focused on the meaning of individual
terms, pairing a diagram with a given word. In
contrast, we learn from whole texts paired with a
</bodyText>
<footnote confidence="0.995261">
1Not all languages exhibit all frames of reference. Terms
for ‘up’ and ‘down’ are exhibited in most all languages, while
‘left’ and ‘right’ are absent in some. Gravity breaks the sym-
metry between ‘up’ and ‘down’ but no such physical distinc-
tion exists for ‘left’ and ‘right’, which contributes to the dif-
ficulty children have learning them.
</footnote>
<page confidence="0.99745">
807
</page>
<bodyText confidence="0.999976105263158">
path, which requires learning the correspondence
between text and world. We use similar geometric
features as Regier, capturing the allocentric frame
of reference.
Spatial semantics have also been explored in
physically grounded systems. Kuipers (2000) de-
veloped the Spatial Semantic Hierarchy, a knowl-
edge representation formalism for representing
different levels of granularity in spatial knowl-
edge. It combines sensory, metrical, and topolog-
ical information in a single framework. Kuipers
et al. demonstrate its effectiveness on a physical
robot, but did not address the learning problem.
More generally, apprenticeship learning is well
studied in the reinforcement learning literature,
where the goal is to mimic the behavior of an ex-
pert in some decision making domain. Notable ex-
amples include (Abbeel and Ng, 2004), who train
a helicopter controller from pilot demonstration.
</bodyText>
<sectionHeader confidence="0.993977" genericHeader="method">
3 The Map Task Corpus
</sectionHeader>
<bodyText confidence="0.990299181818182">
The HCRC Map Task Corpus (Anderson et al.,
1991) is a set of dialogs between an instruction
giver and an instruction follower. Each participant
has a map with small named landmarks. Addition-
ally, the instruction giver has a path drawn on her
map, and must communicate this path to the in-
struction follower in natural language. Figure 1
shows a portion of the instruction giver’s map and
a sample of the instruction giver language which
describes part of the path.
The Map Task Corpus consists of 128 dialogs,
together with 16 different maps. The speech has
been transcribed and segmented into utterances,
based on the length of pauses. We restrict our
attention to just the utterances of the instruction
giver, ignoring the instruction follower. This is to
reduce redundancy and noise in the data - the in-
struction follower rarely introduces new informa-
tion, instead asking for clarification or giving con-
firmation. The landmarks on the instruction fol-
lower map sometimes differ in location from the
instruction giver’s. We ignore this caveat, giving
the system access to the instruction giver’s land-
marks, without the reference path.
Our task is to build an automated instruction
follower. Whereas the original participants could
speak freely, our system does not have the ability
to query the instruction giver and must instead rely
only on the previously recorded dialogs.
Figure 3: Sample state transition. Both actions get
credit for visiting the great rock after the indian
country. Action a1 also gets credit for passing the
great rock on the correct side.
</bodyText>
<sectionHeader confidence="0.99438" genericHeader="method">
4 Reinforcement Learning Formulation
</sectionHeader>
<bodyText confidence="0.998494476190476">
We frame the direction following task as a sequen-
tial decision making problem. We interpret ut-
terances in order, where our interpretation is ex-
pressed by moving on the map. Our goal is to
construct a series of moves in the map which most
closely matches the expert path.
We define intermediate steps in our interpreta-
tion as states in a set 5, and interpretive steps as
actions drawn from a set A. To measure the fi-
delity of our path with respect to the expert, we
define a reward function R : 5 x A —* R+ which
measures the utility of choosing a particular action
in a particular state. Executing action a in state s
carries us to a new state s&apos;, and we denote this tran-
sition function by s&apos; = T (s, a). All transitions are
deterministic in this paper.2
For training we are given a set of dialogs D.
Each dialog d E D is segmented into utter-
ances (u1, ... , um) and is paired with a map,
which is composed of a set of named landmarks
(l1, ... , l&apos;).
</bodyText>
<subsectionHeader confidence="0.997525">
4.1 State
</subsectionHeader>
<bodyText confidence="0.974570416666667">
The states of our decision making problem com-
bine both our position in the dialog d and the path
we have taken so far on the map. A state s E 5 is
composed of s = (ui,l, c), where l is the named
landmark we are located next to and c is a cardinal
direction drawn from {North, South, East, West}
which determines which side of l we are on.
Lastly, ui is the utterance in d we are currently
interpreting.
2Our learning algorithm is not dependent on a determin-
istic transition function and can be applied to domains with
stochastic transitions, such as robot locomotion.
</bodyText>
<page confidence="0.987587">
808
</page>
<subsectionHeader confidence="0.988464">
4.2 Action
</subsectionHeader>
<bodyText confidence="0.999984461538462">
An action a E A is composed of a named land-
mark l, the target of the action, together with a
cardinal direction c which determines which side
to pass l on. Additionally, a can be the null action,
with l = l0 and c = c0. In this case, we interpret
an utterance without moving on the map. A target
l together with a cardinal direction c determine a
point on the map, which is a fixed distance from l
in the direction of c.
We make the assumption that at most one in-
struction occurs in a given utterance. This does not
always hold true - the instruction giver sometimes
chains commands together in a single utterance.
</bodyText>
<subsectionHeader confidence="0.990841">
4.3 Transition
</subsectionHeader>
<bodyText confidence="0.999728166666667">
Executing action a = (l0, c0) in state s = (ui,l, c)
leads us to a new state s0 = T(s,a). This tran-
sition moves us to the next utterance to interpret,
and moves our location to the target of the action.
If a is the null action, s = (ui+1, l, c), otherwise
s0 = (ui+1, l0, c0). Figure 3 displays the state tran-
sitions two different actions.
To form a path through the map, we connect
these state waypoints with a path planner3 based
on A∗, where the landmarks are obstacles. In a
physical system, this would be replaced with a
robot motion planner.
</bodyText>
<subsectionHeader confidence="0.98472">
4.4 Reward
</subsectionHeader>
<bodyText confidence="0.999508315789474">
We define a reward function R(s, a) which mea-
sures the utility of executing action a in state s.
We wish to construct a route which follows the
expert path as closely as possible. We consider a
proposed route P close to the expert path Pe if P
visits landmarks in the same order as Pe, and also
passes them on the correct side.
For a given transition s = (ui,l, c), a = (l0, c0),
we have a binary feature indicating if the expert
path moves from l to l0. In Figure 3, both a1 and
a2 visit the next landmark in the correct order.
To measure if an action is to the correct side of
a landmark, we have another binary feature indi-
cating if Pe passes l0 on side c. In Figure 3, only
a1 passes l0 on the correct side.
In addition, we have a feature which counts the
number of words in ui which also occur in the
name of l0. This encourages us to choose poli-
cies which interpret language relevant to a given
</bodyText>
<footnote confidence="0.9427615">
3We used the Java Path Planning Library, available at
http://www.cs.cmu.edu/˜ggordon/PathPlan/.
</footnote>
<bodyText confidence="0.870689">
landmark.
Our reward function is a linear combination of
these features.
</bodyText>
<subsectionHeader confidence="0.977764">
4.5 Policy
</subsectionHeader>
<bodyText confidence="0.997558333333333">
We formally define an interpretive strategy as a
policy 7r : 5 —* A, a mapping from states to ac-
tions. Our goal is to find a policy 7r which max-
imizes the expected reward Eπ[R(s, 7r(s))]. The
expected reward of following policy 7r from state
s is referred to as the value of s, expressed as
</bodyText>
<equation confidence="0.998478">
Vπ(s) = Eπ[R(s, 7r(s))] (1)
</equation>
<bodyText confidence="0.821906">
When comparing the utilities of executing an ac-
tion a in a state s, it is useful to define a function
</bodyText>
<equation confidence="0.9957015">
Qπ(s, a) = R(s, a) + V π(T(s, a))
= R(s, a) + Qπ(T(s, a), 7r(s)) (2)
</equation>
<bodyText confidence="0.999905666666667">
which measures the utility of executing a, and fol-
lowing the policy 7r for the remainder. A given Q
function implicitly defines a policy 7r by
</bodyText>
<equation confidence="0.81612">
Q(s, a). (3)
</equation>
<bodyText confidence="0.99963175">
Basic reinforcement learning methods treat
states as atomic entities, in essence estimating Vπ
as a table. However, at test time we are following
new directions for a map we haven’t previously
seen. Thus, we represent state/action pairs with a
feature vector 0(s, a) E RK. We then represent
the Q function as a linear combination of the fea-
tures,
</bodyText>
<equation confidence="0.839232">
Q(s, a) = BT0(s, a) (4)
</equation>
<bodyText confidence="0.999921">
and learn weights 0 which most closely approxi-
mate the true expected reward.
</bodyText>
<subsectionHeader confidence="0.837983">
4.6 Features
</subsectionHeader>
<bodyText confidence="0.995994533333333">
Our features 0(s, a) are a mixture of world and
linguistic information. The linguistic information
in our feature representation includes the instruc-
tion giver utterance and the names of landmarks
on the map. Additionally, we furnish our algo-
rithm with a list of English spatial terms, shown
in Table 1. Our feature set includes approximately
200 features. Learning exactly which words in-
fluence decision making is difficult; reinforcement
learning algorithms have problems with the large,
sparse feature vectors common in natural language
processing.
For a given state s = (u, l, c) and action a =
(l0, c0), our feature vector 0(s, a) is composed of
the following:
</bodyText>
<equation confidence="0.560283">
7r(s) = max
a
</equation>
<page confidence="0.990927">
809
</page>
<bodyText confidence="0.931312666666667">
above, below, under, underneath, over, bottom,
top, up, down, left, right, north, south, east, west,
on
</bodyText>
<tableCaption confidence="0.990944">
Table 1: The list of given spatial terms.
</tableCaption>
<listItem confidence="0.98242">
• Coherence: The number of words w E u that
occur in the name of l0
</listItem>
<figure confidence="0.696700318181818">
Input: Dialog set D
Reward function R
Feature function O
Transition function T
Learning rate αt
Output: Feature weights 0
1 Initialize 0 to small random values
2 until 0 converges do
3 foreach Dialog d E D do
4 Initialize s0 = (l1, u1, 0),
a0 — Pr(a0|s0; 0)
5 for t = 0; st non-terminal; t++ do
6 Act: st+1 = T (st, at)
7 Decide: at+1 — Pr(at+1|st+1; 0)
8 Update:
9 A &lt;-- R(st, at) + 0TO(st+1, at+1)
10 — 0TO(st, at)
11 0 &lt;-- 0 + αtO(st, at)A
12 end
13 end
14 end
15 return 0
</figure>
<figureCaption confidence="0.648736">
Algorithm 1: The SARSA learning algorithm.
</figureCaption>
<listItem confidence="0.85034505">
• Landmark Locality: Binary feature indicat-
ing if l0 is the closest landmark to l
• Direction Locality: Binary feature indicat-
ing if cardinal direction c0 is the side of l0
closest to (l, c)
• Null Action: Binary feature indicating if l0 =
NULL
• Allocentric Spatial: Binary feature which
conjoins the side c we pass the landmark on
with each spatial term w E u. This allows us
to capture that the word above tends to indi-
cate passing to the north of the landmark.
• Egocentric Spatial: Binary feature which
conjoins the cardinal direction we move in
with each spatial term w E u. For instance, if
(l, c) is above (l0, c0), the direction from our
current position is south. We conjoin this di-
rection with each spatial term, giving binary
features such as “the word down appears in
the utterance and we move to the south”.
</listItem>
<sectionHeader confidence="0.984678" genericHeader="method">
5 Approximate Dynamic Programming
</sectionHeader>
<bodyText confidence="0.998568947368421">
Given this feature representation, our problem is
to find a parameter vector 0 E RK for which
Q(s, a) = 0TO(s, a) most closely approximates
E[R(s, a)]. To learn these weights 0 we use
SARSA (Sutton and Barto, 1998), an online learn-
ing algorithm similar to Q-learning (Watkins and
Dayan, 1992).
Algorithm 1 details the learning algorithm,
which we follow here. We iterate over training
documents d E D. In a given state st, we act ac-
cording to a probabilistic policy defined in terms
of the Q function. After every transition we up-
date 0, which changes how we act in subsequent
steps.
Exploration is a key issue in any RL algorithm.
If we act greedily with respect to our current Q
function, we might never visit states which are ac-
tually higher in value. We utilize Boltzmann ex-
ploration, for which
</bodyText>
<equation confidence="0.9399325">
(5
Ea/ exp(1 0TO(st, a0)) )
</equation>
<bodyText confidence="0.9995939375">
The parameter T is referred to as the tempera-
ture, with a higher temperature causing more ex-
ploration, and a lower temperature causing more
exploitation. In our experiments T = 2.
Acting with this exploration policy, we iterate
through the training dialogs, updating our fea-
ture weights 0 as we go. The update step looks
at two successive state transitions. Suppose we
are in state st, execute action at, receive reward
rt = R(st, at), transition to state st+1, and there
choose action at+1. The variables of interest are
(st, at, rt, st+1, at+1), which motivates the name
SARSA.
Our current estimate of the Q function is
Q(s, a) = 0TO(s, a). By the Bellman equation,
for the true Q function
</bodyText>
<equation confidence="0.948036857142857">
Q(st, at) = R(st, at) + max Q(st+1, a0) (6)
a
After each action, we want to move 0 to minimize
the temporal difference,
R(st, at) + Q(st+1, at+1) — Q(st, at) (7)
Pr(at|st; 0) =
exp(1� 0TO(st, at))
</equation>
<page confidence="0.966837">
810
</page>
<figure confidence="0.9533005">
Map 4g
Map 10g
</figure>
<figureCaption confidence="0.953895">
Figure 4: Sample output from the SARSA policy. The dashed black line is the reference path and the
solid red line is the path the system follows.
</figureCaption>
<bodyText confidence="0.958173333333333">
For each feature φi(st, at), we change θi propor-
tional to this temporal difference, tempered by a
learning rate αt. We update θ according to
</bodyText>
<equation confidence="0.9963925">
θ = θ+αtφ(st, at)(R(st, at)
+ θTφ(st+1, at+1) − θTφ(st, at)) (8)
</equation>
<bodyText confidence="0.9906525">
Here αt is the learning rate, which decays over
time4. In our case, αt = 10
10+t, which was tuned on
the training set. We determine convergence of the
algorithm by examining the magnitude of updates
to θ. We stop the algorithm when
</bodyText>
<equation confidence="0.632071">
||θt+1 − θt||. &lt; E (9)
</equation>
<sectionHeader confidence="0.996745" genericHeader="method">
6 Experimental Design
</sectionHeader>
<bodyText confidence="0.999984333333333">
We evaluate our system on the Map Task corpus,
splitting the corpus into 96 training dialogs and 32
test dialogs. The whole corpus consists of approx-
imately 105,000 word tokens. The maps seen at
test time do not occur in the training set, but some
of the human participants are present in both.
</bodyText>
<footnote confidence="0.9731216">
4To guarantee convergence, we require Et αt = oo and
Et αt &lt; oo. Intuitively, the sum diverging guarantees we
can still learn arbitrarily far into the future, and the sum of
squares converging guarantees that our updates will converge
at some point.
</footnote>
<subsectionHeader confidence="0.996389">
6.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999869739130435">
We evaluate how closely the path P generated by
our system follows the expert path Pe. We mea-
sure this with respect to two metrics: the order
in which we visit landmarks and the side we pass
them on.
To determine the order Pe visits landmarks we
compute the minimum distance from Pe to each
landmark, and threshold it at a fixed value.
To score path P, we compare the order it visits
landmarks to the expert path. A transition l —* l&apos;
which occurs in P counts as correct if the same
transition occurs in Pe. Let |P |be the number
of landmark transitions in a path P, and N the
number of correct transitions in P. We define the
order precision as N/|P|, and the order recall as
N/|Pe|.
We also evaluate how well we are at passing
landmarks on the correct side. We calculate the
distance of Pe to each side of the landmark, con-
sidering the path to visit a side of the landmark
if the distance is below a threshold. This means
that a path might be considered to visit multiple
sides of a landmark, although in practice it is usu-
</bodyText>
<page confidence="0.99666">
811
</page>
<figureCaption confidence="0.718283">
Figure 5: This figure shows the relative weights of spatial features organized by spatial word. The top
</figureCaption>
<bodyText confidence="0.992024">
row shows the weights of allocentric (landmark-centered) features. For example, the top left figure shows
that when the word above occurs, our policy prefers to go to the north of the target landmark. The bottom
row shows the weights of egocentric (absolute) spatial features. The bottom left figure shows that given
the word above, our policy prefers to move in a southerly cardinal direction.
ally one. If C is the number of landmarks we pass
on the correct side, define the side precision as
C/SP1, and the side recall as C/|Pe|.
</bodyText>
<subsectionHeader confidence="0.999854">
6.2 Comparison Systems
</subsectionHeader>
<bodyText confidence="0.9999312">
The baseline policy simply visits the closest land-
mark at each step, taking the side of the landmark
which is closest. It pays no attention to the direc-
tion language.
We also compare against the policy gradient
learning algorithm of Branavan et al. (2009). They
parametrize a probabilistic policy Pr(sla; 0) as a
log-linear model, in a similar fashion to our explo-
ration policy. During training, the learning algo-
rithm adjusts the weights 0 according to the gradi-
ent of the value function defined by this distribu-
tion.
Reinforcement learning algorithms can be clas-
sified into value based and policy based. Value
methods estimate a value function V for each
state, then act greedily with respect to it. Pol-
icy learning algorithms directly search through
the space of policies. SARSA is a value based
method, and the policy gradient algorithm is pol-
icy based.
</bodyText>
<table confidence="0.9983576">
Visit Order Side
P R Fl P R Fl
Baseline 28.4 37.2 32.2 46.1 60.3 52.2
PG 31.1 43.9 36.4 49.5 69.9 57.9
SARSA 45.7 51.0 48.2 58.0 64.7 61.2
</table>
<tableCaption confidence="0.989935">
Table 2: Experimental results. Visit order shows
</tableCaption>
<bodyText confidence="0.988052333333333">
how well we follow the order in which the answer
path visits landmarks. ‘Side’ shows how success-
fully we pass on the correct side of landmarks.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999966117647059">
Table 2 details the quantitative performance of the
different algorithms. Both SARSA and the policy
gradient method outperform the baseline, but still
fall significantly short of expert performance. The
baseline policy performs surprisingly well, espe-
cially at selecting the correct side to visit a land-
mark.
The disparity between learning approaches and
gold standard performance can be attributed to
several factors. The language in this corpus is con-
versational, frequently ungrammatical, and con-
tains troublesome aspects of dialog such as con-
versational repairs and repetition. Secondly, our
action and feature space are relatively primitive,
and don’t capture the full range of spatial expres-
sion. Path descriptors, such as the difference be-
tween around and past are absent, and our feature
</bodyText>
<page confidence="0.991005">
812
</page>
<bodyText confidence="0.999479461538462">
representation is relatively simple.
The SARSA learning algorithm accrues more
reward than the policy gradient algorithm. Like
most gradient based optimization methods, policy
gradient algorithms oftentimes get stuck in local
maxima, and are sensitive to the initial conditions.
Furthermore, as the size of the feature vector K in-
creases, the space becomes even more difficult to
search. There are no guarantees that SARSA has
reached the best policy under our feature space,
and this is difficult to determine empirically. Thus,
some accuracy might be gained by considering
different RL algorithms.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999738588235294">
Examining the feature weights 0 sheds some light
on our performance. Figure 5 shows the relative
strength of weights for several spatial terms. Re-
call that the two main classes of spatial features in
0 are egocentric (what direction we move in) and
allocentric (on which side we pass a landmark),
combined with each spatial word.
Allocentric terms such as above and below tend
to be interpreted as going to the north and south
of landmarks, respectively. Interestingly, our sys-
tem tends to move in the opposite cardinal direc-
tion, i.e. the agent moves south in the egocen-
tric frame of reference. This suggests that people
use above when we are already above a landmark.
South slightly favors passing on the south side of
landmarks, and has a heavy tendency to move in
a southerly direction. This suggests that south is
used more frequently in an egocentric reference
frame.
Our system has difficulty learning the meaning
of right. Right is often used as a conversational
filler, and also for dialog alignment, such as
“right okay right go vertically up then
between the springboks and the highest
viewpoint.”
Furthermore, right can be used in both an egocen-
tric or allocentric reference frame. Compare
“go to the uh right of the mine”
which utilizes an allocentric frame, with
“right then go eh uh to your right hori-
zontally”
which uses an egocentric frame of reference. It
is difficult to distinguish between these meanings
without syntactic features.
</bodyText>
<sectionHeader confidence="0.997447" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999995176470588">
We presented a reinforcement learning system
which learns to interpret natural language direc-
tions. Critically, our approach uses no semantic
annotation, instead learning directly from human
demonstration. It successfully acquires a subset
of spatial semantics, using reinforcement learning
to derive the correspondence between instruction
language and features of paths. While our results
are still preliminary, we believe our model repre-
sents a significant advance in learning natural lan-
guage meaning, drawing its supervision from hu-
man demonstration rather than word distributions
or hand-labeled semantic tags. Framing language
acquisition as apprenticeship learning is a fruitful
research direction which has the potential to con-
nect the symbolic, linguistic domain to the non-
symbolic, sensory aspects of cognition.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999934333333333">
This research was partially supported by the Na-
tional Science Foundation via a Graduate Re-
search Fellowship to the first author and award
IIS-0811974 to the second author and by the Air
Force Research Laboratory (AFRL), under prime
contract no. FA8750-09-C-0181. Thanks to
Michael Levit and Deb Roy for providing digital
representations of the maps and a subset of the cor-
pus annotated with their spatial representation.
</bodyText>
<sectionHeader confidence="0.999451" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99876235">
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the Twenty-first International Con-
ference on Machine Learning. ACM Press.
A. Anderson, M. Bader, E. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. Mcallister,
J. Miller, C. Sotillo, H. Thompson, and R. Weinert.
1991. The HCRC map task corpus. Language and
Speech, 34, pages 351–366.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In ACL-IJCNLP
’09.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, University of Ed-
inburgh.
Charles Fillmore. 1997. Lectures on Deixis. Stanford:
CSLI Publications.
Benjamin Kuipers. 2000. The spatial semantic hierar-
chy. Artificial Intelligence, 119(1-2):191–233.
</reference>
<page confidence="0.989615">
813
</page>
<reference confidence="0.9995513125">
Stephen Levinson. 2003. Space In Language And
Cognition: Explorations In Cognitive Diversity.
Cambridge University Press.
Michael Levit and Deb Roy. 2007. Interpretation
of spatial language in a map navigation task. In
IEEE Transactions on Systems, Man, and Cybernet-
ics, Part B, 37(3), pages 667–679.
Terry Regier. 1996. The Human Semantic Potential:
Spatial Language and Constrained Connectionism.
The MIT Press.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Leonard Talmy. 1983. How language structures space.
In Spatial Orientation: Theory, Research, and Ap-
plication.
Christine Tanz. 1980. Studies in the acquisition of de-
ictic terms. Cambridge University Press.
Hongmei Wang, Alan M. Maceachren, and Guoray
Cai. 2004. Design of human-GIS dialogue for com-
munication of vague spatial concepts. In GIScience.
C. J. C. H. Watkins and P. Dayan. 1992. Q-learning.
Machine Learning, pages 8:279–292.
Yuan Wei, Emma Brunskill, Thomas Kollar, and
Nicholas Roy. 2009. Where to go: interpreting nat-
ural directions using global inference. In ICRA’09:
Proceedings of the 2009 IEEE international con-
ference on Robotics and Automation, pages 3761–
3767, Piscataway, NJ, USA. IEEE Press.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In ACL-IJCNLP ’09, pages
976–984.
</reference>
<page confidence="0.998536">
814
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985649">
<title confidence="0.999619">Learning to Follow Navigational Directions</title>
<author confidence="0.998803">Vogel Jurafsky</author>
<affiliation confidence="0.9998955">Department of Computer Science Stanford University</affiliation>
<email confidence="0.999064">acvogel@stanford.edu</email>
<email confidence="0.999064">jurafsky@stanford.edu</email>
<abstract confidence="0.99941235">We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of terms like geometric properties of paths.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter Abbeel</author>
<author>Andrew Y Ng</author>
</authors>
<title>Apprenticeship learning via inverse reinforcement learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the Twenty-first International Conference on Machine Learning.</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="8171" citStr="Abbeel and Ng, 2004" startWordPosition="1279" endWordPosition="1282">red in physically grounded systems. Kuipers (2000) developed the Spatial Semantic Hierarchy, a knowledge representation formalism for representing different levels of granularity in spatial knowledge. It combines sensory, metrical, and topological information in a single framework. Kuipers et al. demonstrate its effectiveness on a physical robot, but did not address the learning problem. More generally, apprenticeship learning is well studied in the reinforcement learning literature, where the goal is to mimic the behavior of an expert in some decision making domain. Notable examples include (Abbeel and Ng, 2004), who train a helicopter controller from pilot demonstration. 3 The Map Task Corpus The HCRC Map Task Corpus (Anderson et al., 1991) is a set of dialogs between an instruction giver and an instruction follower. Each participant has a map with small named landmarks. Additionally, the instruction giver has a path drawn on her map, and must communicate this path to the instruction follower in natural language. Figure 1 shows a portion of the instruction giver’s map and a sample of the instruction giver language which describes part of the path. The Map Task Corpus consists of 128 dialogs, togethe</context>
</contexts>
<marker>Abbeel, Ng, 2004</marker>
<rawString>Pieter Abbeel and Andrew Y. Ng. 2004. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-first International Conference on Machine Learning. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Anderson</author>
<author>M Bader</author>
<author>E Bard</author>
<author>E Boyle</author>
<author>G Doherty</author>
<author>S Garrod</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>J Mcallister</author>
<author>J Miller</author>
<author>C Sotillo</author>
<author>H Thompson</author>
<author>R Weinert</author>
</authors>
<title>The HCRC map task corpus.</title>
<date>1991</date>
<journal>Language and Speech,</journal>
<volume>34</volume>
<pages>351--366</pages>
<contexts>
<context position="2439" citStr="Anderson et al., 1991" startWordPosition="374" endWordPosition="378">re underneath eh diamond mine 2. then eh go right until you’re 3. you’re between springbok and highest viewpoint Figure 1: A path appears on the instruction giver’s map, who describes it to the instruction follower. grounded interaction with the world. This draws on the intuition that children learn to use spatial language through a mixture of observing adult language usage and situated interaction in the world, usually without explicit definitions (Tanz, 1980). Our system learns to follow navigational directions in a route following task. We evaluate our approach on the HCRC Map Task corpus (Anderson et al., 1991), a collection of spoken dialogs describing paths to take through a map. In this setting, two participants, the instruction giver and instruction follower, each have a map composed of named landmarks. Furthermore, the instruction giver has a route drawn on her map, and it is her task to describe the path to the instruction follower, who cannot see the reference path. Our system learns to interpret these navigational directions, without access to explicit linguistic annotation. We frame direction following as an apprenticeship learning problem and solve it with a reinforcement learning algorith</context>
<context position="8303" citStr="Anderson et al., 1991" startWordPosition="1301" endWordPosition="1304">for representing different levels of granularity in spatial knowledge. It combines sensory, metrical, and topological information in a single framework. Kuipers et al. demonstrate its effectiveness on a physical robot, but did not address the learning problem. More generally, apprenticeship learning is well studied in the reinforcement learning literature, where the goal is to mimic the behavior of an expert in some decision making domain. Notable examples include (Abbeel and Ng, 2004), who train a helicopter controller from pilot demonstration. 3 The Map Task Corpus The HCRC Map Task Corpus (Anderson et al., 1991) is a set of dialogs between an instruction giver and an instruction follower. Each participant has a map with small named landmarks. Additionally, the instruction giver has a path drawn on her map, and must communicate this path to the instruction follower in natural language. Figure 1 shows a portion of the instruction giver’s map and a sample of the instruction giver language which describes part of the path. The Map Task Corpus consists of 128 dialogs, together with 16 different maps. The speech has been transcribed and segmented into utterances, based on the length of pauses. We restrict </context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, Mcallister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>A. Anderson, M. Bader, E. Bard, E. Boyle, G. Doherty, S. Garrod, S. Isard, J. Kowtko, J. Mcallister, J. Miller, C. Sotillo, H. Thompson, and R. Weinert. 1991. The HCRC map task corpus. Language and Speech, 34, pages 351–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09.</booktitle>
<contexts>
<context position="3120" citStr="Branavan et al. (2009)" startWordPosition="483" endWordPosition="486">hrough a map. In this setting, two participants, the instruction giver and instruction follower, each have a map composed of named landmarks. Furthermore, the instruction giver has a route drawn on her map, and it is her task to describe the path to the instruction follower, who cannot see the reference path. Our system learns to interpret these navigational directions, without access to explicit linguistic annotation. We frame direction following as an apprenticeship learning problem and solve it with a reinforcement learning algorithm, extending previous work on interpreting instructions by Branavan et al. (2009). Our task is to learn a policy, or mapping 806 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics from world state to action, which most closely follows the reference route. Our state space combines world and linguistic features, representing both our current position on the map and the communicative content of the utterances we are interpreting. During training we have access to the reference path, which allows us to measure the utility, or reward, for each </context>
<context position="4900" citStr="Branavan et al. (2009)" startWordPosition="766" endWordPosition="770">use), a path descriptor (towards), and a quantitative aspect (two inches). These representations are then combined to form a path through the map. However, they do not learn these representations from text, leaving natural language processing as an open problem. The semantics in our paper is simpler, eschewing quantitative aspects and path descriptors, and instead focusing on reference objects and frames of reference. This simplifies the learning task, without sacrificing the core of their representation. Learning to follow instructions by interacting with the world was recently introduced by Branavan et al. (2009), who developed a system which learns to follow Windows Help guides. Our reinforcement learning formulation follows closely from their work. Their approach can incorporate expert supervision into the reward function in a similar manner to this paper, but is also able to learn effectively from environment feedback alone. The Map Task corpus is free form conversational English, whereas the Windows instructions are written by a professional. In the Map Task corpus we only observe expert route following behavior, but are not told how portions of the text correspond to parts of the path, leading to</context>
<context position="21918" citStr="Branavan et al. (2009)" startWordPosition="3788" endWordPosition="3791">t landmark. The bottom row shows the weights of egocentric (absolute) spatial features. The bottom left figure shows that given the word above, our policy prefers to move in a southerly cardinal direction. ally one. If C is the number of landmarks we pass on the correct side, define the side precision as C/SP1, and the side recall as C/|Pe|. 6.2 Comparison Systems The baseline policy simply visits the closest landmark at each step, taking the side of the landmark which is closest. It pays no attention to the direction language. We also compare against the policy gradient learning algorithm of Branavan et al. (2009). They parametrize a probabilistic policy Pr(sla; 0) as a log-linear model, in a similar fashion to our exploration policy. During training, the learning algorithm adjusts the weights 0 according to the gradient of the value function defined by this distribution. Reinforcement learning algorithms can be classified into value based and policy based. Value methods estimate a value function V for each state, then act greedily with respect to it. Policy learning algorithms directly search through the space of policies. SARSA is a value based method, and the policy gradient algorithm is policy base</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In ACL-IJCNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Richard Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1535" citStr="Curran, 2003" startWordPosition="231" endWordPosition="232">roduction Spatial language usage is a vital component for physically grounded language understanding systems. Spoken language interfaces to robotic assistants (Wei et al., 2009) and Geographic Information Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions. The semantics of imperative and spatial language is heavily dependent on the physical setting it is situated in, motivating automated learning approaches to acquiring meaning. Traditional accounts of learning typically rely on linguistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In contrast, we present an apprenticeship learning system which learns to imitate human instruction following, without linguistic annotation. Solved using a reinforcement learning algorithm, our system acquires the meaning of spatial words through 1. go vertically down until you’re underneath eh diamond mine 2. then eh go right until you’re 3. you’re between springbok and highest viewpoint Figure 1: A path appears on the instruction giver’s map, who describes it to the instruction follower. grounded interaction with the world. This draws on the intuition that children learn to use spatial la</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>James Richard Curran. 2003. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>Lectures on Deixis.</title>
<date>1997</date>
<publisher>CSLI Publications.</publisher>
<location>Stanford:</location>
<contexts>
<context position="5827" citStr="Fillmore (1997)" startWordPosition="920" endWordPosition="921">ck alone. The Map Task corpus is free form conversational English, whereas the Windows instructions are written by a professional. In the Map Task corpus we only observe expert route following behavior, but are not told how portions of the text correspond to parts of the path, leading to a difficult learning problem. The semantics of spatial language has been studied for some time in the linguistics literature. Talmy (1983) classifies the way spatial meaning is Figure 2: The instruction giver and instruction follower face each other, and cannot see each others maps. encoded syntactically, and Fillmore (1997) studies spatial terms as a subset of deictic language, which depends heavily on non-linguistic context. Levinson (2003) conducted a cross-linguistic semantic typology of spatial systems. Levinson categorizes the frames of reference, or spatial coordinate systems1, into 1. Egocentric: Speaker/hearer centered frame of reference. Ex: “the ball to your left”. 2. Allocentric: Speaker independent. Ex: “the road to the north of the house” Levinson further classifies allocentric frames of reference into absolute, which includes the cardinal directions, and intrinsic, which refers to a featured side o</context>
</contexts>
<marker>Fillmore, 1997</marker>
<rawString>Charles Fillmore. 1997. Lectures on Deixis. Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Kuipers</author>
</authors>
<title>The spatial semantic hierarchy.</title>
<date>2000</date>
<journal>Artificial Intelligence,</journal>
<pages>119--1</pages>
<contexts>
<context position="7601" citStr="Kuipers (2000)" startWordPosition="1194" endWordPosition="1195">ts paired with a 1Not all languages exhibit all frames of reference. Terms for ‘up’ and ‘down’ are exhibited in most all languages, while ‘left’ and ‘right’ are absent in some. Gravity breaks the symmetry between ‘up’ and ‘down’ but no such physical distinction exists for ‘left’ and ‘right’, which contributes to the difficulty children have learning them. 807 path, which requires learning the correspondence between text and world. We use similar geometric features as Regier, capturing the allocentric frame of reference. Spatial semantics have also been explored in physically grounded systems. Kuipers (2000) developed the Spatial Semantic Hierarchy, a knowledge representation formalism for representing different levels of granularity in spatial knowledge. It combines sensory, metrical, and topological information in a single framework. Kuipers et al. demonstrate its effectiveness on a physical robot, but did not address the learning problem. More generally, apprenticeship learning is well studied in the reinforcement learning literature, where the goal is to mimic the behavior of an expert in some decision making domain. Notable examples include (Abbeel and Ng, 2004), who train a helicopter contr</context>
</contexts>
<marker>Kuipers, 2000</marker>
<rawString>Benjamin Kuipers. 2000. The spatial semantic hierarchy. Artificial Intelligence, 119(1-2):191–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Levinson</author>
</authors>
<title>Space In Language And Cognition: Explorations In Cognitive Diversity.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5947" citStr="Levinson (2003)" startWordPosition="937" endWordPosition="939">fessional. In the Map Task corpus we only observe expert route following behavior, but are not told how portions of the text correspond to parts of the path, leading to a difficult learning problem. The semantics of spatial language has been studied for some time in the linguistics literature. Talmy (1983) classifies the way spatial meaning is Figure 2: The instruction giver and instruction follower face each other, and cannot see each others maps. encoded syntactically, and Fillmore (1997) studies spatial terms as a subset of deictic language, which depends heavily on non-linguistic context. Levinson (2003) conducted a cross-linguistic semantic typology of spatial systems. Levinson categorizes the frames of reference, or spatial coordinate systems1, into 1. Egocentric: Speaker/hearer centered frame of reference. Ex: “the ball to your left”. 2. Allocentric: Speaker independent. Ex: “the road to the north of the house” Levinson further classifies allocentric frames of reference into absolute, which includes the cardinal directions, and intrinsic, which refers to a featured side of an object, such as “the front of the car”. Our spatial feature representation follows this egocentric/allocentric dist</context>
</contexts>
<marker>Levinson, 2003</marker>
<rawString>Stephen Levinson. 2003. Space In Language And Cognition: Explorations In Cognitive Diversity. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Levit</author>
<author>Deb Roy</author>
</authors>
<title>Interpretation of spatial language in a map navigation task.</title>
<date>2007</date>
<journal>In IEEE Transactions on Systems, Man, and Cybernetics, Part B,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>667--679</pages>
<contexts>
<context position="3900" citStr="Levit and Roy (2007)" startWordPosition="613" endWordPosition="616"> Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics from world state to action, which most closely follows the reference route. Our state space combines world and linguistic features, representing both our current position on the map and the communicative content of the utterances we are interpreting. During training we have access to the reference path, which allows us to measure the utility, or reward, for each step of interpretation. Using this reward signal as a form of supervision, we learn a policy to maximize the expected reward on unseen examples. 2 Related Work Levit and Roy (2007) developed a spatial semantics for the Map Task corpus. They represent instructions as Navigational Information Units, which decompose the meaning of an instruction into orthogonal constituents such as the reference object, the type of movement, and quantitative aspect. For example, they represent the meaning of “move two inches toward the house” as a reference object (the house), a path descriptor (towards), and a quantitative aspect (two inches). These representations are then combined to form a path through the map. However, they do not learn these representations from text, leaving natural</context>
</contexts>
<marker>Levit, Roy, 2007</marker>
<rawString>Michael Levit and Deb Roy. 2007. Interpretation of spatial language in a map navigation task. In IEEE Transactions on Systems, Man, and Cybernetics, Part B, 37(3), pages 667–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The Human Semantic Potential: Spatial Language and Constrained Connectionism.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="6733" citStr="Regier (1996)" startWordPosition="1057" endWordPosition="1058">/hearer centered frame of reference. Ex: “the ball to your left”. 2. Allocentric: Speaker independent. Ex: “the road to the north of the house” Levinson further classifies allocentric frames of reference into absolute, which includes the cardinal directions, and intrinsic, which refers to a featured side of an object, such as “the front of the car”. Our spatial feature representation follows this egocentric/allocentric distinction. The intrinsic frame of reference occurs rarely in the Map Task corpus and is ignored, as speakers tend not to mention features of the landmarks beyond their names. Regier (1996) studied the learning of spatial language from static 2-D diagrams, learning to distinguish between terms with a connectionist model. He focused on the meaning of individual terms, pairing a diagram with a given word. In contrast, we learn from whole texts paired with a 1Not all languages exhibit all frames of reference. Terms for ‘up’ and ‘down’ are exhibited in most all languages, while ‘left’ and ‘right’ are absent in some. Gravity breaks the symmetry between ‘up’ and ‘down’ but no such physical distinction exists for ‘left’ and ‘right’, which contributes to the difficulty children have lea</context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>Terry Regier. 1996. The Human Semantic Potential: Spatial Language and Constrained Connectionism. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17264" citStr="Sutton and Barto, 1998" startWordPosition="2947" endWordPosition="2950"> of the landmark. • Egocentric Spatial: Binary feature which conjoins the cardinal direction we move in with each spatial term w E u. For instance, if (l, c) is above (l0, c0), the direction from our current position is south. We conjoin this direction with each spatial term, giving binary features such as “the word down appears in the utterance and we move to the south”. 5 Approximate Dynamic Programming Given this feature representation, our problem is to find a parameter vector 0 E RK for which Q(s, a) = 0TO(s, a) most closely approximates E[R(s, a)]. To learn these weights 0 we use SARSA (Sutton and Barto, 1998), an online learning algorithm similar to Q-learning (Watkins and Dayan, 1992). Algorithm 1 details the learning algorithm, which we follow here. We iterate over training documents d E D. In a given state st, we act according to a probabilistic policy defined in terms of the Q function. After every transition we update 0, which changes how we act in subsequent steps. Exploration is a key issue in any RL algorithm. If we act greedily with respect to our current Q function, we might never visit states which are actually higher in value. We utilize Boltzmann exploration, for which (5 Ea/ exp(1 0T</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>How language structures space.</title>
<date>1983</date>
<booktitle>In Spatial Orientation: Theory, Research, and Application.</booktitle>
<contexts>
<context position="5639" citStr="Talmy (1983)" startWordPosition="891" endWordPosition="892">m their work. Their approach can incorporate expert supervision into the reward function in a similar manner to this paper, but is also able to learn effectively from environment feedback alone. The Map Task corpus is free form conversational English, whereas the Windows instructions are written by a professional. In the Map Task corpus we only observe expert route following behavior, but are not told how portions of the text correspond to parts of the path, leading to a difficult learning problem. The semantics of spatial language has been studied for some time in the linguistics literature. Talmy (1983) classifies the way spatial meaning is Figure 2: The instruction giver and instruction follower face each other, and cannot see each others maps. encoded syntactically, and Fillmore (1997) studies spatial terms as a subset of deictic language, which depends heavily on non-linguistic context. Levinson (2003) conducted a cross-linguistic semantic typology of spatial systems. Levinson categorizes the frames of reference, or spatial coordinate systems1, into 1. Egocentric: Speaker/hearer centered frame of reference. Ex: “the ball to your left”. 2. Allocentric: Speaker independent. Ex: “the road to</context>
</contexts>
<marker>Talmy, 1983</marker>
<rawString>Leonard Talmy. 1983. How language structures space. In Spatial Orientation: Theory, Research, and Application.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Tanz</author>
</authors>
<title>Studies in the acquisition of deictic terms.</title>
<date>1980</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2282" citStr="Tanz, 1980" startWordPosition="349" endWordPosition="350">tation. Solved using a reinforcement learning algorithm, our system acquires the meaning of spatial words through 1. go vertically down until you’re underneath eh diamond mine 2. then eh go right until you’re 3. you’re between springbok and highest viewpoint Figure 1: A path appears on the instruction giver’s map, who describes it to the instruction follower. grounded interaction with the world. This draws on the intuition that children learn to use spatial language through a mixture of observing adult language usage and situated interaction in the world, usually without explicit definitions (Tanz, 1980). Our system learns to follow navigational directions in a route following task. We evaluate our approach on the HCRC Map Task corpus (Anderson et al., 1991), a collection of spoken dialogs describing paths to take through a map. In this setting, two participants, the instruction giver and instruction follower, each have a map composed of named landmarks. Furthermore, the instruction giver has a route drawn on her map, and it is her task to describe the path to the instruction follower, who cannot see the reference path. Our system learns to interpret these navigational directions, without acc</context>
</contexts>
<marker>Tanz, 1980</marker>
<rawString>Christine Tanz. 1980. Studies in the acquisition of deictic terms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongmei Wang</author>
<author>Alan M Maceachren</author>
<author>Guoray Cai</author>
</authors>
<title>Design of human-GIS dialogue for communication of vague spatial concepts.</title>
<date>2004</date>
<booktitle>In GIScience.</booktitle>
<contexts>
<context position="1154" citStr="Wang et al., 2004" startWordPosition="172" endWordPosition="175">ficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths. 1 Introduction Spatial language usage is a vital component for physically grounded language understanding systems. Spoken language interfaces to robotic assistants (Wei et al., 2009) and Geographic Information Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions. The semantics of imperative and spatial language is heavily dependent on the physical setting it is situated in, motivating automated learning approaches to acquiring meaning. Traditional accounts of learning typically rely on linguistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In contrast, we present an apprenticeship learning system which learns to imitate human instruction following, without linguistic annotation. Solved using a reinforcement learning algorithm, our system acquires the me</context>
</contexts>
<marker>Wang, Maceachren, Cai, 2004</marker>
<rawString>Hongmei Wang, Alan M. Maceachren, and Guoray Cai. 2004. Design of human-GIS dialogue for communication of vague spatial concepts. In GIScience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J C H Watkins</author>
<author>P Dayan</author>
</authors>
<date>1992</date>
<booktitle>Q-learning. Machine Learning,</booktitle>
<pages>8--279</pages>
<contexts>
<context position="17342" citStr="Watkins and Dayan, 1992" startWordPosition="2959" endWordPosition="2962">dinal direction we move in with each spatial term w E u. For instance, if (l, c) is above (l0, c0), the direction from our current position is south. We conjoin this direction with each spatial term, giving binary features such as “the word down appears in the utterance and we move to the south”. 5 Approximate Dynamic Programming Given this feature representation, our problem is to find a parameter vector 0 E RK for which Q(s, a) = 0TO(s, a) most closely approximates E[R(s, a)]. To learn these weights 0 we use SARSA (Sutton and Barto, 1998), an online learning algorithm similar to Q-learning (Watkins and Dayan, 1992). Algorithm 1 details the learning algorithm, which we follow here. We iterate over training documents d E D. In a given state st, we act according to a probabilistic policy defined in terms of the Q function. After every transition we update 0, which changes how we act in subsequent steps. Exploration is a key issue in any RL algorithm. If we act greedily with respect to our current Q function, we might never visit states which are actually higher in value. We utilize Boltzmann exploration, for which (5 Ea/ exp(1 0TO(st, a0)) ) The parameter T is referred to as the temperature, with a higher </context>
</contexts>
<marker>Watkins, Dayan, 1992</marker>
<rawString>C. J. C. H. Watkins and P. Dayan. 1992. Q-learning. Machine Learning, pages 8:279–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Wei</author>
<author>Emma Brunskill</author>
<author>Thomas Kollar</author>
<author>Nicholas Roy</author>
</authors>
<title>Where to go: interpreting natural directions using global inference.</title>
<date>2009</date>
<booktitle>In ICRA’09: Proceedings of the 2009 IEEE international conference on Robotics and Automation,</booktitle>
<pages>3761--3767</pages>
<publisher>IEEE Press.</publisher>
<location>Piscataway, NJ, USA.</location>
<contexts>
<context position="1099" citStr="Wei et al., 2009" startWordPosition="163" endWordPosition="166">t between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths. 1 Introduction Spatial language usage is a vital component for physically grounded language understanding systems. Spoken language interfaces to robotic assistants (Wei et al., 2009) and Geographic Information Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions. The semantics of imperative and spatial language is heavily dependent on the physical setting it is situated in, motivating automated learning approaches to acquiring meaning. Traditional accounts of learning typically rely on linguistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In contrast, we present an apprenticeship learning system which learns to imitate human instruction following, without linguistic annotation. Solved using a reinf</context>
</contexts>
<marker>Wei, Brunskill, Kollar, Roy, 2009</marker>
<rawString>Yuan Wei, Emma Brunskill, Thomas Kollar, and Nicholas Roy. 2009. Where to go: interpreting natural directions using global inference. In ICRA’09: Proceedings of the 2009 IEEE international conference on Robotics and Automation, pages 3761– 3767, Piscataway, NJ, USA. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09,</booktitle>
<pages>976--984</pages>
<contexts>
<context position="1498" citStr="Zettlemoyer and Collins, 2009" startWordPosition="224" endWordPosition="227">ve and south into geometric properties of paths. 1 Introduction Spatial language usage is a vital component for physically grounded language understanding systems. Spoken language interfaces to robotic assistants (Wei et al., 2009) and Geographic Information Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions. The semantics of imperative and spatial language is heavily dependent on the physical setting it is situated in, motivating automated learning approaches to acquiring meaning. Traditional accounts of learning typically rely on linguistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In contrast, we present an apprenticeship learning system which learns to imitate human instruction following, without linguistic annotation. Solved using a reinforcement learning algorithm, our system acquires the meaning of spatial words through 1. go vertically down until you’re underneath eh diamond mine 2. then eh go right until you’re 3. you’re between springbok and highest viewpoint Figure 1: A path appears on the instruction giver’s map, who describes it to the instruction follower. grounded interaction with the world. This draws on the intuition </context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In ACL-IJCNLP ’09, pages 976–984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>