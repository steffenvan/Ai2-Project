<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000268">
<title confidence="0.991624">
Head-driven Transition-based Parsing with Top-down Prediction
</title>
<author confidence="0.994496">
Katsuhiko Hayashi†, Taro Watanabe‡, Masayuki Asahara§, Yuji Matsumoto††Nara Institute of Science and Technology
</author>
<affiliation confidence="0.9518144">
Ikoma, Nara, 630-0192, Japan
‡National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
§National Institute for Japanese Language and Linguistics
Tachikawa, Tokyo, 190-8561, Japan
</affiliation>
<email confidence="0.986154">
katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp
masayu-a@ninjal.ac.jp, matsu@is.naist.jp
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997136">
This paper presents a novel top-down head-
driven parsing algorithm for data-driven pro-
jective dependency analysis. This algorithm
handles global structures, such as clause and
coordination, better than shift-reduce or other
bottom-up algorithms. Experiments on the
English Penn Treebank data and the Chinese
CoNLL-06 data show that the proposed algo-
rithm achieves comparable results with other
data-driven dependency parsing algorithms.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982618060606061">
Transition-based parsing algorithms, such as shift-
reduce algorithms (Nivre, 2004; Zhang and Clark,
2008), are widely used for dependency analysis be-
cause of the efficiency and comparatively good per-
formance. However, these parsers have one major
problem that they can handle only local information.
Isozaki et al. (2004) pointed out that the drawbacks
of shift-reduce parser could be resolved by incorpo-
rating top-down information such as root finding.
This work presents an O(n2) top-down head-
driven transition-based parsing algorithm which can
parse complex structures that are not trivial for shift-
reduce parsers. The deductive system is very similar
to Earley parsing (Earley, 1970). The Earley predic-
tion is tied to a particular grammar rule, but the pro-
posed algorithm is data-driven, following the current
trends of dependency parsing (Nivre, 2006; McDon-
ald and Pereira, 2006; Koo et al., 2010). To do the
prediction without any grammar rules, we introduce
a weighted prediction that is to predict lower nodes
from higher nodes with a statistical model.
To improve parsing flexibility in deterministic
parsing, our top-down parser uses beam search al-
gorithm with dynamic programming (Huang and
Sagae, 2010). The complexity becomes O(n2 * b)
where b is the beam size. To reduce prediction er-
rors, we propose a lookahead technique based on a
FIRST function, inspired by the LL(1) parser (Aho
and Ullman, 1972). Experimental results show that
the proposed top-down parser achieves competitive
results with other data-driven parsing algorithms.
2 Definition of Dependency Graph
A dependency graph is defined as follows.
</bodyText>
<construct confidence="0.994763">
Definition 2.1 (Dependency Graph) Given an in-
put sentence W = n0 ... nn where n0 is a spe-
cial root node $, a directed graph is defined as
GW = (VW, AW) where VW = {0, 1, ... , nj is a
set of (indices of) nodes and AW C VW x VW is a
set of directed arcs. The set of arcs is a set of pairs
(x, y) where x is a head and y is a dependent of x.
x 4∗ l denotes a path from x to l. A directed graph
GW = (VW, AW) is well-formed if and only if:
</construct>
<listItem confidence="0.9412424">
• There is no node x such that (x, 0) E AW.
• If (x, y) E AW then there is no node x′ such
that (x′, y) E AW and x′ =� x.
• There is no subset of arcs {(x0, x1), (x1, x2),
..., (xl−1, xl)1 C AW such that x0 = xl.
</listItem>
<bodyText confidence="0.961647333333333">
These conditions are refered to ROOT, SINGLE-
HEAD, and ACYCLICITY, and we call an well-
formed directed graph as a dependency graph.
</bodyText>
<construct confidence="0.861453">
Definition 2.2 (PROJECTIVITY) A dependency
graph GW = (VW, AW) is projective if and only if,
</construct>
<page confidence="0.992954">
657
</page>
<note confidence="0.708752">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 657–665,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<equation confidence="0.989130272727273">
input: W = n0 ... n,,,
axiom(p0): 0 : ⟨1, 0, n + 1, n0⟩ : ∅
state p
predx: � �� �
predy: ℓ : ⟨i, h, j, sd|...|s0⟩ : ∃k : i ≤ k &lt; h
scan: ℓ + 1 : ⟨i, k, h, sd−1  |...  |s0  |nk⟩ : {p}
state p
� �� �
ℓ : ⟨i,h,j,sd|...|s0⟩ : ∃k : i ≤ k &lt; j ∧ h &lt; i
ℓ + 1 : ⟨i,k,j,sd−1|...|s0|nk⟩ : {p}
ℓ : ⟨i, h, j, sd|...|s0⟩ : 7r i = h
ℓ + 1 : ⟨i + 1, h, j, sd|...|s0⟩ : 7r
comp:
q ∈ �, h &lt; i
ℓ + 1 : ⟨i, h′, j′, s′ d|...|s′ 1|s′ 0ys0⟩ : �′
goal: 3n : ⟨n + 1, 0, n + 1, s0⟩ : ∅
state q
� �� �
:⟨ , h′,j′, s′d|...|s′0⟩ : 7r′
state p
� �� �
ℓ : ⟨i, h, j, sd|...|s0⟩ : 7r
</equation>
<figureCaption confidence="0.99915">
Figure 1: The non-weighted deductive system of top-down dependency parsing algorithm: means “take anything”.
</figureCaption>
<construct confidence="0.680501">
for every arc (x, y) ∈ AW and node l in x &lt; l &lt; y
or y &lt; l &lt; x, there is a path x →∗ l or y →∗ l.
</construct>
<bodyText confidence="0.99788975">
The proposed algorithm in this paper is for projec-
tive dependency graphs. If a projective dependency
graph is connected, we call it a dependency tree,
and if not, a dependency forest.
</bodyText>
<sectionHeader confidence="0.952069" genericHeader="introduction">
3 Top-down Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999830333333333">
Our proposed algorithm is a transition-based algo-
rithm, which uses stack and queue data structures.
This algorithm formally uses the following state:
</bodyText>
<equation confidence="0.776542">
ℓ : ⟨i, h, j, S⟩ : 7r
</equation>
<bodyText confidence="0.99894997368421">
where ℓ is a step size, S is a stack of trees sd|...|s0
where s0 is a top tree and d is a window size for
feature extraction, i is an index of node on the top
of the input node queue, h is an index of root node
of s0, j is an index to indicate the right limit (j −
1 inclusive) of predy, and 7r is a set of pointers to
predictor states, which are states just before putting
the node in h onto stack S. In the deterministic case,
7r is a singleton set except for the initial state.
This algorithm has four actions, predictx(predx),
predicty(predy), scan and complete(comp). The
deductive system of the top-down algorithm is
shown in Figure 1. The initial state p0 is a state ini-
tialized by an artificial root node n0. This algorithm
applies one action to each state selected from appli-
cable actions in each step. Each of three kinds of
actions, pred, scan, and comp, occurs n times, and
this system takes 3n steps for a complete analysis.
Action predx puts nk onto stack S selected from
the input queue in the range, i ≤ k &lt; h, which is
to the left of the root nh in the stack top. Similarly,
action predy puts a node nk onto stack S selected
from the input queue in the range, h &lt; i ≤ k &lt; j,
which is to the right of the root nh in the stack top.
The node ni on the top of the queue is scanned if it
is equal to the root node nh in the stack top. Action
comp creates a directed arc (h′, h) from the root h′
of s′0 on a predictor state q to the root h of s0 on a
current state p if h &lt; i 1.
The precondition i &lt; h of action predx means
that the input nodes in i ≤ k &lt; h have not been
predicted yet. Predx, scan and predy do not con-
flict with each other since their preconditions i &lt; h,
i = h and h &lt; i do not hold at the same time.
However, this algorithm faces a predy-comp con-
flict because both actions share the same precondi-
tion h &lt; i, which means that the input nodes in
1 ≤ k ≤ h have been predicted and scanned. This
</bodyText>
<footnote confidence="0.544008333333333">
1In a single root tree, the special root symbol $o has exactly
one child node. Therefore, we do not apply comp action to a
state if its condition satisfies sl.h = no n ℓ ≠ 3n − 1.
</footnote>
<page confidence="0.981952">
658
</page>
<figure confidence="0.916407857142857">
step state stack queue action state information
0 p0 $0 I1 saw2 a3 girl4 – (1, 0, 5) : 0
1 p1 $0|saw2 I1 saw2 a3 girl4 predy (1, 2, 5) : {p0}
2 p2 saw2|I1 I1 saw2 a3 girl4 predx (1,1, 2) : {p1}
3 p3 saw2|I1 saw2 a3 girl4 scan (2,1, 2) : {p1}
4 p4 $0|I1xsaw2 saw2 a3 girl4 comp (2, 2, 5) : {p0}
5 p5 $0|I1xsaw2 a3 girl4 scan (3, 2, 5) : {p0}
6 ps I1xsaw2|girl4 a3 girl4 predy (3, 4, 5) : {p5}
7 p7 girl4|a3 a3 girl4 predx (3, 3, 4) : {ps}
8 ps girl4|a3 girl4 scan (4, 3, 4) : {ps}
9 p9 I1xsaw2|a3xgirl4 girl4 comp (4, 4, 5) : {p5}
10 p10 I1xsaw2|a3xgirl4 scan (5, 4, 5) : {p5}
11 p11 $0|I1xsaw2ygirl4 comp (5, 2, 5) : {p0}
12 p12 $0ysaw2 comp (5, 0, 5) : 0
</figure>
<figureCaption confidence="0.826810333333333">
Figure 2: Stages of the top-down deterministic parsing process for a sentence “I saw a girl”. We follow a convention
and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example,
we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1.
</figureCaption>
<bodyText confidence="0.9995635">
parser constructs left and right children of a head
node in a left-to-right direction by scanning the head
node prior to its right children. Figure 2 shows an
example for parsing a sentence “I saw a girl”.
</bodyText>
<sectionHeader confidence="0.993834" genericHeader="method">
4 Correctness
</sectionHeader>
<bodyText confidence="0.9899468">
To prove the correctness of the system in Figure
1 for the projective dependency graph, we use the
proof strategy of (Nivre, 2008a). The correct deduc-
tive system is both sound and complete.
Theorem 4.1 The deductive system in Figure 1 is
correct for the class of dependency forest.
Proof 4.1 To show soundness, we show that Gp0 =
(VW, 0), which is a directed graph defined by the
axiom, is well-formed and projective, and that every
transition preserves this property.
</bodyText>
<listItem confidence="0.971316615384615">
• ROOT: The node 0 is a root in Gp0, and the
node 0 is on the top of stack ofp0. The two pred
actions put a word onto the top of stack, and
predict an arc from root or its descendant to
the child. The comp actions add the predicted
arcs which include no arc of (x, 0).
• SINGLE-HEAD: Gp0 is single-head. A node
y is no longer in stack and queue after a comp
action creates an arc (x, y). The node y cannot
make any arc (x′, y) after the removal.
• ACYCLICITY: Gp0 is acyclic. A cycle is cre-
ated only if an arc (x, y) is added when there
is a directed path y --+* x. The node x is no
</listItem>
<bodyText confidence="0.63924275">
longer in stack and queue when the directed
path y --+* x was made by adding an arc (l, x).
There is no chance to add the arc (x, y) on the
directed path y --+* x.
</bodyText>
<listItem confidence="0.981035269230769">
• PROJECTIVITY: Gp0 is projective. Projec-
tivity is violated by adding an arc (x, y) when
there is a node l in x &lt; l &lt; y or y &lt; l &lt; x
with the path to or from the outside of the span
x and y. When pred,,, creates an arc relation
from x to y, the node y cannot be scanned be-
fore all nodes l in x &lt; l &lt; y are scanned and
completed. When pred, creates an arc rela-
tion from x to y, the node y cannot be scanned
before all nodes k in k &lt; y are scanned and
completed, and the node x cannot be scanned
before all nodes l in y &lt; l &lt; x are scanned
and completed. In those processes, the node l
in x &lt; l &lt; y or y &lt; l &lt; x does not make a
path to or from the outside of the span x and y,
and a path x --+* l or y --+* l is created. ❑
To show completeness, we show that for any sen-
tence W, and dependency forest GW = (VW, AW),
there is a transition sequence C0,m such that Gp. =
GW by an inductive method.
• If |W |= 1, the projective dependency graph
for W is GW = ({0}, 0) and Gp0 = GW.
• Assume that the claim holds for sentences with
length less or equal to t, and assume that
|W  |= t + 1 and GW = (VW, AW). The sub-
graph GW′ is defined as (VW − t, A−t) where
</listItem>
<page confidence="0.981922">
659
</page>
<figure confidence="0.56520475">
s2.h h
s ...
�.
. . . s�
. .
h
l1
� l�
</figure>
<page confidence="0.981231">
661
</page>
<bodyText confidence="0.991772">
Pred takes either pred, or pred,,,. Beam search is
performed based on the following linear order for
the two states p and p′ at the same step, which have
(cfw, cin) and (c′fw, c′in) respectively:
</bodyText>
<equation confidence="0.634405">
p ≻ p′ iff cfw &lt; c′fw or cfw = c′fw n cin &lt; c′in. (9)
</equation>
<bodyText confidence="0.999953">
We prioritize the forward cost over the inside cost
since forward cost pertains to longer action sequence
and is better suited to evaluate hypothesis states than
inside cost (Nederhof, 2003).
</bodyText>
<subsectionHeader confidence="0.937163">
5.4 FIRST Function for Lookahead
</subsectionHeader>
<bodyText confidence="0.99518225">
Top-down backtrack parser usually reduces back-
tracking by precomputing the set FIRST(·) (Aho and
Ullman, 1972). We define the set FIRST(·) for our
top-down dependency parser:
</bodyText>
<equation confidence="0.99663">
FIRST(t’) = {ld.t|ld E lmdescendant(Tree,t’)
Tree E Corpus} (10)
</equation>
<bodyText confidence="0.999913375">
where t’ is a POS-tag, Tree is a correct depen-
dency tree which exists in Corpus, a function
lmdescendant(Tree, t’) returns the set of the leftmost
descendant node ld of each nodes in Tree whose
POS-tag is t’, and ld.t denotes a POS-tag of ld.
Though our parser does not backtrack, it looks ahead
when selecting possible child nodes at the prediction
step by using the function FIRST. In case of pred.,:
</bodyText>
<equation confidence="0.9926696">
dk : i&lt; k &lt; h n nz.t E FIRST(nk.t)
state p
� � 1
ℓ : (i, h,7, sd|...|s0) :
ℓ + 1 : (i, k, h, sd−1|...|s0|nk) : {p}
</equation>
<bodyText confidence="0.999890166666667">
where nz.t is a POS-tag of the node nz on the top of
the queue, and nk.t is a POS-tag in kth position of
an input nodes. The case for pred,,, is the same. If
there are no nodes which satisfy the condition, our
top-down parser creates new states for all nodes, and
pushes them into hypo in line 9 of Algorithm 1.
</bodyText>
<sectionHeader confidence="0.994256" genericHeader="method">
6 Time Complexity
</sectionHeader>
<bodyText confidence="0.999947285714286">
Our proposed top-down algorithm has three kinds
of actions which are scan, comp and predict. Each
scan and comp actions occurs n times when parsing
a sentence with the length n. Predict action also oc-
curs n times in which a child node is selected from
a node sequence in the input queue. Thus, the algo-
rithm takes the following times for prediction:
</bodyText>
<equation confidence="0.809843">
n + (n − 1) + ··· + 1 =
</equation>
<bodyText confidence="0.999965333333333">
As n2 for prediction is the most dominant factor, the
time complexity of the algorithm is O(n2) and that
of the algorithm with beam search is O(n2 * b).
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999814107142857">
Alshawi (1996) proposed head automaton which
recognizes an input sentence top-down. Eisner
and Satta (1999) showed that there is a cubic-time
parsing algorithm on the formalism of the head
automaton grammars, which are equivalently con-
verted into split-head bilexical context-free gram-
mars (SBCFGs) (McAllester, 1999; Johnson, 2007).
Although our proposed algorithm does not employ
the formalism of SBCFGs, it creates left children
before right children, implying that it does not have
spurious ambiguities as well as parsing algorithms
on the SBCFGs. Head-corner parsing algorithm
(Kay, 1989) creates dependency tree top-down, and
in this our algorithm has similar spirit to it.
Yamada and Matsumoto (2003) applied a shift-
reduce algorithm to dependency analysis, which is
known as arc-standard transition-based algorithm
(Nivre, 2004). Nivre (2003) proposed another
transition-based algorithm, known as arc-eager al-
gorithm. The arc-eager algorithm processes right-
dependent top-down, but this does not involve the
prediction of lower nodes from higher nodes. There-
fore, the arc-eager algorithm is a totally bottom-up
algorithm. Zhang and Clark (2008) proposed a com-
bination approach of the transition-based algorithm
with graph-based algorithm (McDonald and Pereira,
2006), which is the same as our combination model
of stack-based and prediction models.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999977142857143">
Experiments were performed on the English Penn
Treebank data and the Chinese CoNLL-06 data. For
the English data, we split WSJ part of it into sections
02-21 for training, section 22 for development and
section 23 for testing. We used Yamada and Mat-
sumoto (2003)’s head rules to convert phrase struc-
ture to dependency structure. For the Chinese data,
</bodyText>
<equation confidence="0.6902245">
∑n i= n(n + 1) . (11)
z 2
</equation>
<page confidence="0.981824">
662
</page>
<table confidence="0.994770714285714">
time accuracy complete root
McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 –
Koo10 (Koo and Collins, 2010) – 93.04 – –
Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – –
2nd-MST* 0.13 92.3 43.7 96.0
Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5
Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 –
Zhang08 (Sh beam 64) – 91.4 41.8 –
Zhang08 (Sh+Graph beam 64) – 92.1 45.4 –
Huang10 (beam+DP) 0.04 92.1 – –
Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1
Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 –
top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2
top-down* (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6
</table>
<tableCaption confidence="0.971341">
Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled
attachment score, complete is a sentence complete rate, and root is a correct root rate. * indicates our experiments.
</tableCaption>
<figure confidence="0.9314755">
0 10 20 30 40 50 60 70
length of input sentence
</figure>
<figureCaption confidence="0.993332">
Figure 5: Scatter plot of parsing time against sentence
length, comparing with top-down, 2nd-MST and shift-
reduce parsers (beam size: 8, pred size: 5)
</figureCaption>
<bodyText confidence="0.9990375">
we used the information of words and fine-grained
POS-tags for features. We also implemented and ex-
perimented Huang and Sagae (2010)’s arc-standard
shift-reduce parser. For the 2nd-order Eisner-Satta
algorithm, we used MSTParser (McDonald, 2012).
We used an early update version of averaged per-
ceptron algorithm (Collins and Roark, 2004) for
training of shift-reduce and top-down parsers. A
set of feature templates in (Huang and Sagae, 2010)
were used for the stack-based model, and a set of
feature templates in (McDonald and Pereira, 2006)
were used for the 2nd-order prediction model. The
weighted prediction and stack-based models of top-
down parser were jointly trained.
</bodyText>
<sectionHeader confidence="0.699279" genericHeader="method">
8.1 Results for English Data
</sectionHeader>
<bodyText confidence="0.9983915">
During training, we fixed the prediction size and
beam size to 5 and 16, respectively, judged by pre-
</bodyText>
<table confidence="0.9966052">
accuracy complete root
oracle (sh+mst) 94.3 52.3 97.7
oracle (top+sh) 94.2 51.7 97.6
oracle (top+mst) 93.8 50.7 97.1
oracle (top+sh+mst) 94.9 55.3 98.1
</table>
<tableCaption confidence="0.89159425">
Table 2: Oracle score, choosing the highest accuracy
parse for each sentence on test data from results of top-
down (beam 8, pred 5) and shift-reduce (beam 8) and
MST(2nd) parsers in Table 1.
</tableCaption>
<table confidence="0.9996275">
accuracy complete root
top-down (beam:8, pred:5) 90.9 80.4 93.0
shift-reduce (beam:8) 90.8 77.6 93.5
2nd-MST 91.4 79.3 94.2
oracle (sh+mst) 94.0 85.1 95.9
oracle (top+sh) 93.8 84.0 95.6
oracle (top+mst) 93.6 84.2 95.3
oracle (top+sh+mst) 94.7 86.5 96.3
</table>
<tableCaption confidence="0.999233">
Table 3: Results for Chinese Data (CoNLL-06)
</tableCaption>
<bodyText confidence="0.995991642857143">
liminary experiments on development data. After
25 iterations of perceptron training, we achieved
92.94 unlabeled accuracy for top-down parser with
the FIRST function and 93.01 unlabeled accuracy
for shift-reduce parser on development data by set-
ting the beam size to 8 for both parsers and the pre-
diction size to 5 in top-down parser. These trained
models were used for the following testing.
We compared top-down parsing algorithm with
other data-driven parsing algorithms in Table 1.
Top-down parser achieved comparable unlabeled ac-
curacy with others, and outperformed them on the
sentence complete rate. On the other hand, top-
down parser was less accurate than shift-reduce
</bodyText>
<figure confidence="0.9973094">
parsing time (cpu sec)
0.8
0.6
0.4
0.2
0
1
&amp;quot;shift-reduce&amp;quot;
&amp;quot;2nd-mst&amp;quot;
&amp;quot;top-down&amp;quot;
</figure>
<page confidence="0.994828">
663
</page>
<table confidence="0.9855602">
No.717 Little Lily , as Ms. Cunningham calls7 herself in the book , really was14 n’t ordinary.
shift-reduce 2 7 2 2 6 4 14 7 7 11 9 7 14 0 14 14 14
2nd-MST 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
top-down 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
correct 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
No.127 resin, used to make garbage bags, milk jugs, housewares, toys and meat packaging25 , among other items.
shift-reduce 25 9 9 13 11 15 13 25 18 25 25 25 25 25 25 25 7 25 25 29 27 4
2nd-MST 29 9 9 13 11 15 13 29 18 29 29 29 29 25 25 25 29 25 25 29 7 4
top-down 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
correct 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
</table>
<tableCaption confidence="0.974242">
Table 4: Two examples on which top-down parser is superior to two bottom-up parsers: In correct analysis, the boxed
portion is the head of the underlined portion. Bottom-up parsers often mistake to capture the relation.
</tableCaption>
<bodyText confidence="0.999929833333333">
parser on the correct root measure. In step 0, top-
down parser predicts a child node, a root node of
a complete tree, using little syntactic information,
which may lead to errors in the root node selection.
Therefore, we think that it is important to seek more
suitable features for the prediction in future work.
Figure 5 presents the parsing time against sen-
tence length. Our proposed top-down parser is the-
oretically slower than shift-reduce parser and Fig-
ure 5 empirically indicates the trends. The domi-
nant factor comes from the score calculation, and
we will leave it for future work. Table 2 shows
the oracle score for test data, which is the score
of the highest accuracy parse selected for each sen-
tence from results of several parsers. This indicates
that the parses produced by each parser are differ-
ent from each other. However, the gains obtained by
the combination of top-down and 2nd-MST parsers
are smaller than other combinations. This is because
top-down parser uses the same features as 2nd-MST
parser, and these are more effective than those of
stack-based model. It is worth noting that as shown
in Figure 5, our O(n2 *b) (b = 8) top-down parser is
much faster than O(n3) Eisner-Satta CKY parsing.
</bodyText>
<sectionHeader confidence="0.878228" genericHeader="evaluation">
8.2 Results for Chinese Data (CoNLL-06)
</sectionHeader>
<bodyText confidence="0.999916444444444">
We also experimented on the Chinese data. Fol-
lowing English experiments, shift-reduce parser was
trained by setting beam size to 16, and top-down
parser was trained with the beam size and the predic-
tion size to 16 and 5, respectively. Table 3 shows the
results on the Chinese test data when setting beam
size to 8 for both parsers and prediction size to 5 in
top-down parser. The trends of the results are almost
the same as those of the English results.
</bodyText>
<subsectionHeader confidence="0.995985">
8.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999985866666667">
Table 4 shows two interesting results, on which top-
down parser is superior to either shift-reduce parser
or 2nd-MST parser. The sentence No.717 contains
an adverbial clause structure between the subject
and the main verb. Top-down parser is able to han-
dle the long-distance dependency while shift-reudce
parser cannot correctly analyze it. The effectiveness
on the clause structures implies that our head-driven
parser may handle non-projective structures well,
which are introduced by Johansonn’s head rule (Jo-
hansson and Nugues, 2007). The sentence No.127
contains a coordination structure, which it is diffi-
cult for bottom-up parsers to handle, but, top-down
parser handles it well because its top-down predic-
tion globally captures the coordination.
</bodyText>
<sectionHeader confidence="0.996767" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999985666666667">
This paper presents a novel head-driven parsing al-
gorithm and empirically shows that it is as practi-
cal as other dependency parsing algorithms. Our
head-driven parser has potential for handling non-
projective structures better than other non-projective
dependency algorithms (McDonald et al., 2005; At-
tardi, 2006; Nivre, 2008b; Koo et al., 2010). We are
in the process of extending our head-driven parser
for non-projective structures as our future work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998620333333333">
We would like to thank Kevin Duh for his helpful
comments and to the anonymous reviewers for giv-
ing valuable comments.
</bodyText>
<page confidence="0.997803">
664
</page>
<sectionHeader confidence="0.985616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997132113636364">
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation and Compiling, volume 1: Parsing.
Prentice-Hall.
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. the 10th
CoNLL, pages 166–170.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. the 42nd ACL.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94–102.
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. the 37th ACL, pages 457–464.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742–750.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational rerank-
ing on packed-shared dependency forests. In Proc.
EMNLP, pages 1479–1488.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the 48th
ACL, pages 1077–1086.
H. Isozaki, H. Kazawa, and T. Hirao. 2004. A determin-
istic word dependency analyzer enhanced with prefer-
ence learning. In Proc. the 21st COLING, pages 275–
281.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. NODALIDA.
M. Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proc. the 45th ACL, pages 168–
175.
M. Kay. 1989. Head driven parsing. In Proc. the IWPT.
K. Kitagawa and K. Tanaka-Ishii. 2010. Tree-based de-
terministic dependency parsing — an application to
nivre’s method —. In Proc. the 48th ACL 2010 Short
Papers, pages 189–193, July.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1–11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. EMNLP, pages
1288–1298.
D. McAllester. 1999. A reformulation of eisner and
satta’s cubic time parser for split head automata gram-
mars. http://ttic.uchicago.edu/ dmcallester/.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81–88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP, pages 523–
530.
R. McDonald. 2012. Minimum spanning tree parser.
http://www.seas.upenn.edu/ strctlrn/MSTParser.
M.-J. Nederhof. 2003. Weighted deductive parsing
and knuth’s algorithm. Computational Linguistics,
29:135–143.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. the IWPT, pages 149–160.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proc. the ACL Workshop Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50–57.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
J. Nivre. 2008a. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513–553.
J. Nivre. 2008b. Sorting out dependency parsing. In
Proc. the CoTAL, pages 16–27.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165–201.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195–206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In
Proc. EMNLP, pages 562–571.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
the 49th ACL, pages 188–193.
</reference>
<page confidence="0.998384">
665
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212808">
<title confidence="0.99123">Head-driven Transition-based Parsing with Top-down Prediction</title>
<author confidence="0.537931">Taro Masayuki Yuji Institute of Science</author>
<affiliation confidence="0.4295335">Ikoma, Nara, 630-0192, Institute of Information and Communications</affiliation>
<address confidence="0.404786">Sorakugun, Kyoto, 619-0289,</address>
<affiliation confidence="0.860053">Institute for Japanese Language and</affiliation>
<address confidence="0.872412">Tachikawa, Tokyo, 190-8561,</address>
<email confidence="0.9373165">katsuhiko-h@is.naist.jp,masayu-a@ninjal.ac.jp,matsu@is.naist.jp</email>
<abstract confidence="0.998989363636364">This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation and Compiling,</booktitle>
<volume>1</volume>
<publisher>Parsing. Prentice-Hall.</publisher>
<contexts>
<context position="2359" citStr="Aho and Ullman, 1972" startWordPosition="331" endWordPosition="334">llowing the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms. 2 Definition of Dependency Graph A dependency graph is defined as follows. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 ... nn where n0 is a special root node $, a directed graph is defined as GW = (VW, AW) where VW = {0, 1, ... , nj is a set of (indices of) nodes and AW C VW x VW is a set of directed arcs. The set of arcs is a set of pairs (x, y) where x is a head and y is a dependent of x. x 4∗ l denotes a path from x to l. A directed graph</context>
<context position="11113" citStr="Aho and Ullman, 1972" startWordPosition="2182" endWordPosition="2185">s ... �. . . . s� . . h l1 � l� 661 Pred takes either pred, or pred,,,. Beam search is performed based on the following linear order for the two states p and p′ at the same step, which have (cfw, cin) and (c′fw, c′in) respectively: p ≻ p′ iff cfw &lt; c′fw or cfw = c′fw n cin &lt; c′in. (9) We prioritize the forward cost over the inside cost since forward cost pertains to longer action sequence and is better suited to evaluate hypothesis states than inside cost (Nederhof, 2003). 5.4 FIRST Function for Lookahead Top-down backtrack parser usually reduces backtracking by precomputing the set FIRST(·) (Aho and Ullman, 1972). We define the set FIRST(·) for our top-down dependency parser: FIRST(t’) = {ld.t|ld E lmdescendant(Tree,t’) Tree E Corpus} (10) where t’ is a POS-tag, Tree is a correct dependency tree which exists in Corpus, a function lmdescendant(Tree, t’) returns the set of the leftmost descendant node ld of each nodes in Tree whose POS-tag is t’, and ld.t denotes a POS-tag of ld. Though our parser does not backtrack, it looks ahead when selecting possible child nodes at the prediction step by using the function FIRST. In case of pred.,: dk : i&lt; k &lt; h n nz.t E FIRST(nk.t) state p � � 1 ℓ : (i, h,7, sd|..</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation and Compiling, volume 1: Parsing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In Proc. the ICSLP.</booktitle>
<contexts>
<context position="12647" citStr="Alshawi (1996)" startWordPosition="2479" endWordPosition="2480">hem into hypo in line 9 of Algorithm 1. 6 Time Complexity Our proposed top-down algorithm has three kinds of actions which are scan, comp and predict. Each scan and comp actions occurs n times when parsing a sentence with the length n. Predict action also occurs n times in which a child node is selected from a node sequence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + ··· + 1 = As n2 for prediction is the most dominant factor, the time complexity of the algorithm is O(n2) and that of the algorithm with beam search is O(n2 * b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency t</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In Proc. the ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proc. the 10th CoNLL,</booktitle>
<pages>166--170</pages>
<marker>Attardi, 2006</marker>
<rawString>G. Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proc. the 10th CoNLL, pages 166–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. the 42nd ACL.</booktitle>
<contexts>
<context position="15941" citStr="Collins and Roark, 2004" startWordPosition="3003" endWordPosition="3006"> a sentence complete rate, and root is a correct root rate. * indicates our experiments. 0 10 20 30 40 50 60 70 length of input sentence Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16, respectively, judged by preaccuracy complete root oracle (sh+mst) 94.3 52.3 97.7 oracle (top+sh) 94.2 51.7 97.6 oracle (top+mst) 93.8 50.7 97.1 oracle (top+sh+</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. the 42nd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1631" citStr="Earley, 1970" startWordPosition="213" endWordPosition="214">e, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce pr</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proc. the 37th ACL,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="12740" citStr="Eisner and Satta (1999)" startWordPosition="2490" endWordPosition="2493">rithm has three kinds of actions which are scan, comp and predict. Each scan and comp actions occurs n times when parsing a sentence with the length n. Predict action also occurs n times in which a child node is selected from a node sequence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + ··· + 1 = As n2 for prediction is the most dominant factor, the time complexity of the algorithm is O(n2) and that of the algorithm with beam search is O(n2 * b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003)</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. M. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proc. the 37th ACL, pages 457–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. the HLT-NAACL,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="14617" citStr="Goldberg and Elhadad, 2010" startWordPosition="2781" endWordPosition="2784">Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – – 2nd-MST* 0.13 92.3 43.7 96.0 Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5 Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 – Zhang08 (Sh beam 64) – 91.4 41.8 – Zhang08 (Sh+Graph beam 64) – 92.1 45.4 – Huang10 (beam+DP) 0.04 92.1 – – Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1 Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 – top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2 top-down* (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6 Table 1: Results for test data: Time measu</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Y. Goldberg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proc. the HLT-NAACL, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hayashi</author>
<author>T Watanabe</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>The third-order variational reranking on packed-shared dependency forests.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1479--1488</pages>
<contexts>
<context position="14534" citStr="Hayashi et al., 2011" startWordPosition="2767" endWordPosition="2770">is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – – 2nd-MST* 0.13 92.3 43.7 96.0 Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5 Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 – Zhang08 (Sh beam 64) – 91.4 41.8 – Zhang08 (Sh+Graph beam 64) – 92.1 45.4 – Huang10 (beam+DP) 0.04 92.1 – – Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1 Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 – top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2 top-down* (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4</context>
</contexts>
<marker>Hayashi, Watanabe, Asahara, Matsumoto, 2011</marker>
<rawString>K. Hayashi, T. Watanabe, M. Asahara, and Y. Matsumoto. 2011. The third-order variational reranking on packed-shared dependency forests. In Proc. EMNLP, pages 1479–1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. the 48th ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="2158" citStr="Huang and Sagae, 2010" startWordPosition="295" endWordPosition="298"> for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms. 2 Definition of Dependency Graph A dependency graph is defined as follows. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 ... nn where n0 is a special root node $, a directed graph is defined as GW = (VW, AW) where VW = {0, 1, ... , nj is a set of</context>
<context position="15736" citStr="Huang and Sagae (2010)" startWordPosition="2975" endWordPosition="2978">.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6 Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled attachment score, complete is a sentence complete rate, and root is a correct root rate. * indicates our experiments. 0 10 20 30 40 50 60 70 length of input sentence Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed t</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. the 48th ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>H Kazawa</author>
<author>T Hirao</author>
</authors>
<title>A deterministic word dependency analyzer enhanced with preference learning.</title>
<date>2004</date>
<booktitle>In Proc. the 21st COLING,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="1265" citStr="Isozaki et al. (2004)" startWordPosition="156" endWordPosition="159">ures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 1 Introduction Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the predict</context>
</contexts>
<marker>Isozaki, Kazawa, Hirao, 2004</marker>
<rawString>H. Isozaki, H. Kazawa, and T. Hirao. 2004. A deterministic word dependency analyzer enhanced with preference learning. In Proc. the 21st COLING, pages 275– 281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proc. NODALIDA.</booktitle>
<contexts>
<context position="20993" citStr="Johansson and Nugues, 2007" startWordPosition="3915" endWordPosition="3919">f the results are almost the same as those of the English results. 8.3 Analysis of Results Table 4 shows two interesting results, on which topdown parser is superior to either shift-reduce parser or 2nd-MST parser. The sentence No.717 contains an adverbial clause structure between the subject and the main verb. Top-down parser is able to handle the long-distance dependency while shift-reudce parser cannot correctly analyze it. The effectiveness on the clause structures implies that our head-driven parser may handle non-projective structures well, which are introduced by Johansonn’s head rule (Johansson and Nugues, 2007). The sentence No.127 contains a coordination structure, which it is difficult for bottom-up parsers to handle, but, top-down parser handles it well because its top-down prediction globally captures the coordination. 9 Conclusion This paper presents a novel head-driven parsing algorithm and empirically shows that it is as practical as other dependency parsing algorithms. Our head-driven parser has potential for handling nonprojective structures better than other non-projective dependency algorithms (McDonald et al., 2005; Attardi, 2006; Nivre, 2008b; Koo et al., 2010). We are in the process of</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proc. NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In Proc. the 45th ACL,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="12965" citStr="Johnson, 2007" startWordPosition="2524" endWordPosition="2525">equence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + ··· + 1 = As n2 for prediction is the most dominant factor, the time complexity of the algorithm is O(n2) and that of the algorithm with beam search is O(n2 * b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In Proc. the 45th ACL, pages 168– 175. M. Kay. 1989. Head driven parsing. In Proc. the IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kitagawa</author>
<author>K Tanaka-Ishii</author>
</authors>
<title>Tree-based deterministic dependency parsing — an application to nivre’s method —.</title>
<date>2010</date>
<booktitle>In Proc. the 48th ACL 2010 Short Papers,</booktitle>
<pages>189--193</pages>
<contexts>
<context position="14679" citStr="Kitagawa and Tanaka-Ishii, 2010" startWordPosition="2790" endWordPosition="2793">n Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – – 2nd-MST* 0.13 92.3 43.7 96.0 Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5 Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 – Zhang08 (Sh beam 64) – 91.4 41.8 – Zhang08 (Sh+Graph beam 64) – 92.1 45.4 – Huang10 (beam+DP) 0.04 92.1 – – Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1 Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 – top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2 top-down* (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6 Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an u</context>
</contexts>
<marker>Kitagawa, Tanaka-Ishii, 2010</marker>
<rawString>K. Kitagawa and K. Tanaka-Ishii. 2010. Tree-based deterministic dependency parsing — an application to nivre’s method —. In Proc. the 48th ACL 2010 Short Papers, pages 189–193, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. the 48th ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="14489" citStr="Koo and Collins, 2010" startWordPosition="2758" endWordPosition="2761">algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – – 2nd-MST* 0.13 92.3 43.7 96.0 Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5 Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 – Zhang08 (Sh beam 64) – 91.4 41.8 – Zhang08 (Sh+Graph beam 64) – 92.1 45.4 – Huang10 (beam+DP) 0.04 92.1 – – Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1 Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 – top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2 top-down* (beam 8, 16, 32</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. the 48th ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="1846" citStr="Koo et al., 2010" startWordPosition="247" endWordPosition="250">l information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive resu</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proc. EMNLP, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
</authors>
<title>A reformulation of eisner and satta’s cubic time parser for split head automata grammars.</title>
<date>1999</date>
<note>http://ttic.uchicago.edu/ dmcallester/.</note>
<contexts>
<context position="12949" citStr="McAllester, 1999" startWordPosition="2522" endWordPosition="2523">cted from a node sequence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + ··· + 1 = As n2 for prediction is the most dominant factor, the time complexity of the algorithm is O(n2) and that of the algorithm with beam search is O(n2 * b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager al</context>
</contexts>
<marker>McAllester, 1999</marker>
<rawString>D. McAllester. 1999. A reformulation of eisner and satta’s cubic time parser for split head automata grammars. http://ttic.uchicago.edu/ dmcallester/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1827" citStr="McDonald and Pereira, 2006" startWordPosition="242" endWordPosition="246">at they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achiev</context>
<context position="13905" citStr="McDonald and Pereira, 2006" startWordPosition="2657" endWordPosition="2660">lgorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hay</context>
<context position="16146" citStr="McDonald and Pereira, 2006" startWordPosition="3037" endWordPosition="3040">omparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16, respectively, judged by preaccuracy complete root oracle (sh+mst) 94.3 52.3 97.7 oracle (top+sh) 94.2 51.7 97.6 oracle (top+mst) 93.8 50.7 97.1 oracle (top+sh+mst) 94.9 55.3 98.1 Table 2: Oracle score, choosing the highest accuracy parse for each sentence on test data from results of topdown (beam 8, pred 5) and shift-reduce (beam 8) and MST(2nd) parsers in Tabl</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. HLT-EMNLP, pages 523– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Minimum spanning tree parser.</title>
<date>2012</date>
<note>http://www.seas.upenn.edu/ strctlrn/MSTParser.</note>
<contexts>
<context position="15849" citStr="McDonald, 2012" startWordPosition="2990" endWordPosition="2991">ime per sentence in seconds. Accuracy is an unlabeled attachment score, complete is a sentence complete rate, and root is a correct root rate. * indicates our experiments. 0 10 20 30 40 50 60 70 length of input sentence Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16, respectively, judged by preaccuracy complete root oracle (sh+mst) 9</context>
</contexts>
<marker>McDonald, 2012</marker>
<rawString>R. McDonald. 2012. Minimum spanning tree parser. http://www.seas.upenn.edu/ strctlrn/MSTParser.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--135</pages>
<contexts>
<context position="10968" citStr="Nederhof, 2003" startWordPosition="2163" endWordPosition="2164">th length less or equal to t, and assume that |W |= t + 1 and GW = (VW, AW). The subgraph GW′ is defined as (VW − t, A−t) where 659 s2.h h s ... �. . . . s� . . h l1 � l� 661 Pred takes either pred, or pred,,,. Beam search is performed based on the following linear order for the two states p and p′ at the same step, which have (cfw, cin) and (c′fw, c′in) respectively: p ≻ p′ iff cfw &lt; c′fw or cfw = c′fw n cin &lt; c′in. (9) We prioritize the forward cost over the inside cost since forward cost pertains to longer action sequence and is better suited to evaluate hypothesis states than inside cost (Nederhof, 2003). 5.4 FIRST Function for Lookahead Top-down backtrack parser usually reduces backtracking by precomputing the set FIRST(·) (Aho and Ullman, 1972). We define the set FIRST(·) for our top-down dependency parser: FIRST(t’) = {ld.t|ld E lmdescendant(Tree,t’) Tree E Corpus} (10) where t’ is a POS-tag, Tree is a correct dependency tree which exists in Corpus, a function lmdescendant(Tree, t’) returns the set of the leftmost descendant node ld of each nodes in Tree whose POS-tag is t’, and ld.t denotes a POS-tag of ld. Though our parser does not backtrack, it looks ahead when selecting possible child</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M.-J. Nederhof. 2003. Weighted deductive parsing and knuth’s algorithm. Computational Linguistics, 29:135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proc. the IWPT,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="13482" citStr="Nivre (2003)" startWordPosition="2600" endWordPosition="2601">rted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoN</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proc. the IWPT, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proc. the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1025" citStr="Nivre, 2004" startWordPosition="120" endWordPosition="121">.jp, taro.watanabe@nict.go.jp masayu-a@ninjal.ac.jp, matsu@is.naist.jp Abstract This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 1 Introduction Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley,</context>
<context position="13468" citStr="Nivre, 2004" startWordPosition="2598" endWordPosition="2599">valently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and t</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>J. Nivre. 2004. Incrementality in deterministic dependency parsing. In Proc. the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>34--513</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1799" citStr="Nivre, 2006" startWordPosition="240" endWordPosition="241">or problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 * b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the pro</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>J. Nivre. 2006. Inductive Dependency Parsing. Springer. J. Nivre. 2008a. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34:513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Sorting out dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. the CoTAL,</booktitle>
<pages>16--27</pages>
<contexts>
<context position="8245" citStr="Nivre, 2008" startWordPosition="1575" endWordPosition="1576">”. We follow a convention and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example, we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1. parser constructs left and right children of a head node in a left-to-right direction by scanning the head node prior to its right children. Figure 2 shows an example for parsing a sentence “I saw a girl”. 4 Correctness To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). The correct deductive system is both sound and complete. Theorem 4.1 The deductive system in Figure 1 is correct for the class of dependency forest. Proof 4.1 To show soundness, we show that Gp0 = (VW, 0), which is a directed graph defined by the axiom, is well-formed and projective, and that every transition preserves this property. • ROOT: The node 0 is a root in Gp0, and the node 0 is on the top of stack ofp0. The two pred actions put a word onto the top of stack, and predict an arc from root or its descendant to the child. The comp actions add the predicted arcs which include no arc of</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>J. Nivre. 2008b. Sorting out dependency parsing. In Proc. the CoTAL, pages 16–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. the IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="13340" citStr="Yamada and Matsumoto (2003)" startWordPosition="2579" endWordPosition="2582">wn. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combinat</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. the IWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="1049" citStr="Zhang and Clark, 2008" startWordPosition="122" endWordPosition="125">anabe@nict.go.jp masayu-a@ninjal.ac.jp, matsu@is.naist.jp Abstract This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 1 Introduction Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley predi</context>
<context position="13783" citStr="Zhang and Clark (2008)" startWordPosition="2641" endWordPosition="2644">gorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing using beam-search. In Proc. EMNLP, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proc. the 49th ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="14942" citStr="Zhang and Nivre, 2011" startWordPosition="2840" endWordPosition="2843">structure. For the Chinese data, ∑n i= n(n + 1) . (11) z 2 662 time accuracy complete root McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 – Koo10 (Koo and Collins, 2010) – 93.04 – – Hayashi11 (Hayashi et al., 2011) 0.3 92.89 – – 2nd-MST* 0.13 92.3 43.7 96.0 Goldberg10 (Goldberg and Elhadad, 2010) – 89.7 37.5 91.5 Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) – 91.3 41.7 – Zhang08 (Sh beam 64) – 91.4 41.8 – Zhang08 (Sh+Graph beam 64) – 92.1 45.4 – Huang10 (beam+DP) 0.04 92.1 – – Huang10* (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1 Zhang11 (beam 64) (Zhang and Nivre, 2011) – 93.07 49.59 – top-down* (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2 top-down* (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6 Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled attachment score, complete is a sentence complete rate, and root is a correct root rate. * indicates our experiments. 0 10 20 30 40 50 60 70 length of input sentence Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down,</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Y. Zhang and J. Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proc. the 49th ACL, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>