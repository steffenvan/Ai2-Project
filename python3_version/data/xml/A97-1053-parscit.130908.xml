<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.962081333333333">
Learning Probabilistic Sub categorization Preference
by Identifying Case Dependencies and
Optimal Noun Class Generalization Level*
</title>
<author confidence="0.969047">
Takehito Utsuro Yuji Matsumoto
</author>
<affiliation confidence="0.97436">
Graduate School of Information Science, Nara Institute of Science and Technology
</affiliation>
<address confidence="0.646347">
8916-5, Takayama-cho, Ikoma-shi, Nara, 630-01, JAPAN
</address>
<email confidence="0.937779">
futsuro,matsulOis.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.9931" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868736842105">
This paper proposes a novel method of learning
probabilistic subcategorization preference. In the
method, for the purpose of coping with the ambi-
guities of case dependencies and noun class gen-
eralization of argument/adjunct nouns, we intro-
duce a data structure which represents a tuple
of independent partial subcategorization frames.
Each collocation of a verb and argument/adjunct
nouns is assumed to be generated from one of the
possible tuples of independent partial subcatego-
rization frames. Parameters of subcategorization
preference are then estimated so as to maximize
the subcategorization preference function for each
collocation of a verb and argument/adjunct nouns
in the training corpus. We also describe the results
of the experiments on learning probabilistic sub-
categorization preference from the EDR Japanese
bracketed corpus, as well as those on evaluating
the performance of subcategorization preference.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992467">
In corpus-based NLP, extraction of linguistic knowl-
edge such as lexical/semantic collocation is one of the
most important issues and has been intensively stud-
ied in recent years. In those research, extracted lex-
ical/semantic collocation is especially useful in terms
of ranking parses in syntactic analysis as well as au-
tomatic construction of lexicon for NLP.
For example, in the context of syntactic disam-
biguation, Black (1993) and Magerman (1995) pro-
posed statistical parsing models based-on decision-
tree learning techniques, which incorporated not
only syntactic but also lexical/semantic information
in the decision-trees. As lexical/semantic informa-
tion, Black (1993) used about 50 semantic categories,
while Magerman (1995) used lexical forms of words.
Collins (1996) proposed a statistical parser which
is based on probabilities of dependencies between
head-words in the parse tree. In those works, led-
cal/semantic collocation are used for ranking parses
in syntactic analysis.
</bodyText>
<note confidence="0.8302805">
The authors would like to thank Dr. Hang Li of NEC
GSLC Research Laboratories, Dr. Kentaro Inui of Tokyo
Institute of Technology, Dr. Koiti Hasida of Electrotech-
nical Laboratory, Dr. Takashi Miyata of Nara Institute of
Science and Technology, and also anonymous reviewers of
ANLP97 for valuable comments on this work.
</note>
<bodyText confidence="0.998850142857143">
On the other hand, in the context of automatic lex-
icon construction, the emphasis is mainly on the ex-
traction of lexical/semantic collocational knowledge of
specific words rather than its use in sentence parsing.
For example, Haruno (1995) applied an information-
theoretic data compression technique to corpus-based
case frame learning, and proposed a method of find-
ing case frames of verbs as compressed representation
of verb-noun collocational data in corpus. The work
concentrated on the extraction of declarative repre-
sentation of case frames and did not consider their
performance in sentence parsing.
This paper focuses on extracting lexical/semantic
collocational knowledge of verbs for the purpose of ap-
plying it to ranking parses in syntactic analysis. More
specifically, we propose a novel method for learning
parameters for calculating subcategorization prefer-
ence functions of verbs. In general, when learning lex-
ical/semantic collocational knowledge of verbs from
corpus, it is necessary to cope with the following two
types of ambiguities:
</bodyText>
<listItem confidence="0.804584">
1) The ambiguity of case dependencies
2) The ambiguity of noun class generalization
</listItem>
<bodyText confidence="0.97973696">
1) is caused by the fact that, only by observing each
verb-noun collocation in corpus, it is not decidable
which cases are dependent on each other and which
cases are optional and independent of other cases. 2)
is caused by the fact that, only by observing each verb-
noun collocation in corpus, it is not decidable which
superordinate class generates each observed leaf class
in the verb-noun collocation.
So far, there exist several researches which worked
on these two issues in learning collocational knowl-
edge of verbs and also evaluated the results in
terms of syntactic disambiguation. Resnik (1993)
and Li and Abe (1995) studied how to find an opti-
mal abstraction level of an argument noun in a tree-
structured thesaurus. Although they evaluated the
obtained abstraction level of the argument noun by its
performance in syntactic disambiguation, their works
are limited to only one argument. Li and Abe (1996)
also studied a method for learning dependencies be-
tween case slots and evaluated the discovered depen-
dencies in the syntactic disambiguation task. They
first obtained optimal abstraction levels of the argu-
ment nouns by the method in Li and Abe (1995), and
then tried to discover dependencies between the class-
based case slots. They reported that dependencies
</bodyText>
<page confidence="0.997614">
364
</page>
<bodyText confidence="0.999963277777778">
were discovered only at the slot-level and not at the
class-level.
Compared with those previous works, this paper
proposes to cope with the above two ambiguities in
a uniform way. First, we introduce a data structure
which represents a tuple of independent partial sub-
categorization frames. Each collocation of a verb and
argument/adjunct nouns is assumed to be generated
from one of the possible tuples of independent par-
tial subcategorization frames. Then, parameters of
subcategorization preference are estimated so as to
maximize the subcategorization preference function
for each collocation of a verb and argument/adjunct
nouns in the training corpus. We describe the results
of the experiments on learning probabilistic subcate-
gorization preference from the EDR Japanese brack-
eted corpus (EDR, 1995), as well as those on evaluat-
ing the performance of subcategorization preference.
</bodyText>
<sectionHeader confidence="0.962883" genericHeader="introduction">
2 Data Structure
</sectionHeader>
<subsectionHeader confidence="0.933615">
2.1 Verb-Noun Collocation
</subsectionHeader>
<bodyText confidence="0.9985965">
Verb-noun collocation is a data structure for the collo-
cation of a verb and all of its argument/adjunct nouns.
A verb-noun collocation e is represented by a feature
structure which consists of the verb v and all the pairs
of co-occurring case-markers p and thesaurus classes
c of case-marked nouns:1
</bodyText>
<equation confidence="0.99535075">
pred : v
p1 :
e =
Pk Ck
</equation>
<bodyText confidence="0.931540142857143">
We assume that a thesaurus is a tree-structured type
hierarchy in which each node represents a semantic
class, and each thesaurus class , ck in a verb-
noun collocation is a leaf class. We also introduce
as the superordinate-subordinate relation of classes in
a thesaurus: c1 c2 means that c1 is subordinate to
C2.
</bodyText>
<subsectionHeader confidence="0.998321">
2.2 Subcategorization Frame
</subsectionHeader>
<bodyText confidence="0.99353925">
A subcategorization frame f is represented by a feature
structure which consists of a verb v and the pairs of
case-markers p and sense restriction c of case-marked
argument/adjunct nouns:
</bodyText>
<equation confidence="0.988432666666667">
[pp ire.d: v
Ci
pi : ci
</equation>
<bodyText confidence="0.995212833333333">
Sense restriction , ci of case-marked argu-
ment/adjunct nouns are represented by classes at ar-
bitrary levels of the thesaurus. A subcategorization
frame f can be divided into two parts: one is the
verbal part f , containing the verb v while the other
is the nominal part fp containing all the pairs of
&apos;Although we ignore sense ambiguities of case-marked
nouns in this definition, in section 5.2, we briefly mention
how we deal with sense ambiguities of case-marked nouns
in the current implementation.
case-markers p and sense restriction c of case-marked
nouns.
</bodyText>
<equation confidence="0.755261">
f = fAfp = { pred : v j A [
</equation>
<subsectionHeader confidence="0.954101">
2.3 Subsumption Relation
</subsectionHeader>
<bodyText confidence="0.997073666666667">
We introduce subsumption relation -&lt; f of a verb-noun
collocation e and a subcategorization frame f:
e f if. for each case-marker pi in f and
its noun class cif, there exists the
same case-marker pi in e and its
noun class cie is subordinate to
cif, i.e. cie cif
The subsumption relation --&lt;f is applicable also as a
subsumption relation of two subcategorization frames.
</bodyText>
<sectionHeader confidence="0.992526" genericHeader="method">
3 A Model of Generating Verb-Noun
Collocation
</sectionHeader>
<bodyText confidence="0.999168857142857">
In this section, we introduce a model of generat-
ing a verb-noun collocation from subcategorization
frame(s). In order to cope with the ambiguities of
case dependencies and noun class generalization in
this model, we introduce a data structure which repre-
sents a tuple of independent partial subcategorization
frames.
</bodyText>
<subsectionHeader confidence="0.666401666666667">
3.1 Generating a Verb-Noun Collocation
from Independent Partial
Subcategorization Frames
</subsectionHeader>
<bodyText confidence="0.9997734">
First, we describe the idea of generating a verb-noun
collocation from a subcategorization frame, or a tuple
of partial subcategorization frames.
Generation from a Subcategorization Frame
Suppose a verb-noun collocation e is given as:
</bodyText>
<equation confidence="0.962207">
[pred : v
Pk Cke
pm :c,
•
</equation>
<bodyText confidence="0.999499">
Then, let us consider a subcategorization frame f
which can generate e. We assume that f has exactly
the same case-markers as e has,2 and each semantic
class cif of a case-marked noun of f is superordinate
to the corresponding leaf semantic class Cie of e:
</bodyText>
<equation confidence="0.99423575">
pp e .de: fv
f = , c2, c,1 (t= . k) (2)
•
Pk : Ck f
</equation>
<bodyText confidence="0.845607">
Then, we denote the generation of the verb-noun
collocation e from the subcategorization frame f as:
f e
Next, we describe the idea of generating a verb-noun
collocation from a tuple of partial subcategorization
frames which are independent of each other.
&apos;Since we do not consider ellipsis of argument nouns
when generating a verb-noun collocation from a subcate-
gorization frame, the subcategorization frame f is required
to have exactly the same case-markers as e.
</bodyText>
<equation confidence="0.9907675">
(1)
e =
</equation>
<page confidence="0.994572">
365
</page>
<subsectionHeader confidence="0.98779">
Partial Subcategorization Frame
</subsectionHeader>
<bodyText confidence="0.998318333333333">
First, we define a partial subcategorization frame fi
of f as a subcategorization frame which has the same
verb v as f as well as some of the case-markers of f and
their semantic classes. Then, we can find a division
of f into a tuple (fi , , fn) of partial subcategoriza-
tion frames of f, where any pair fi and fi, (i i&apos;)
</bodyText>
<listItem confidence="0.952148833333333">
do not have common case-markers and the unification
Ii A • • • A fn of all the partial subcategorization frames
equals to f:
I= A • • • A f„
pred : v
fj= pii : ci; V j\iji pi; ji
&apos; (i, z&apos; =1, ,n, i i&apos;)
•
•
Independence of Partial Subcategorization
Frames
We allow the division of f into a tuple , fn)
of partial subcategorization frames as in the equation
(3) only when the partial subcategorization frames fi ,
• • • , can be regarded as events occurring indepen-
dently of each other. With some corpus, usually we
can estimate the conditional probabilities p(f I v) and
p(fiI v) of the (partial) subcategorization frames f
</listItem>
<bodyText confidence="0.999286285714286">
and fi (i = 1, ,n) given the verb v. According
to the estimated probabilities, we can judge whether
fi,. • • ,.fn are independent of each other as follows.
First, we estimate the conditional probability p(f I
v) of a (partial) subcategorization frame f by sum-
ming up the conditional probabilities p(e I v) of all
the verb-noun collocations e given the verb v, where
</bodyText>
<equation confidence="0.653401333333333">
e is subsumed by f (e f)3
r(f I v) E p(e I v) (5)
e-&lt;/f
</equation>
<bodyText confidence="0.906770666666667">
The conditional joint probability v) is
also estimated by summing up p(e I v) where e is
subsumed by all of , fn (e -L&lt;.f ,
</bodyText>
<equation confidence="0.9853585">
Alb.. • fn I v) E p(e v) (6)
ejfi .....
</equation>
<bodyText confidence="0.998987333333333">
Then, we give a formal definition of independence
of partial subcategorization frames according to the
estimated conditional probabilities:
partial subcategorization frames , fn are in-
dependent if, any pair fi and f (i j) do not
have common case-markers, and for every sub-
set fi„ , fii of j of these partial subcategoriza-
tion frames (j = 2,... , n), the following equation
holds:
</bodyText>
<equation confidence="0.966826">
P(f.i, • • • , f., I v) = P(f.i I v) -P(f., Iv) (7)
</equation>
<bodyText confidence="0.9599995">
Since it is too strict to judge the independence of
partial subcategorization frames by the equation (7),
</bodyText>
<footnote confidence="0.863158666666667">
3The probability p(e I v) can be estimated as
freg(e)1 freg(v) by M.L.E. (maximum likelihood estima-
tion) directly from the training corpus.
</footnote>
<bodyText confidence="0.997145">
we relax the constraint of independence using a re-
laxation parameter a (0 &lt;a &lt; 1). Partial subcatego-
rization frames fn are judged as independent
if, for every subset of j of these partial
subcategorization frames (j = 2, ... , n), the following
inequalities hold:
</bodyText>
<equation confidence="0.8868355">
P(f.i, • • • , f., I v) 1
a 5- P(f,i I v) • (ft.) v) -c7
</equation>
<subsectionHeader confidence="0.876902">
Generation from Independent Partial
Subcategorization Frames
</subsectionHeader>
<bodyText confidence="0.99931075">
Now, as in the case of the generation from a sub-
categorization frame f, we denote the generation of e
from a tuple (fi , , fn) of independent partial sub-
categorization frames of f as below:
</bodyText>
<equation confidence="0.442688">
(fi,• • • , e
</equation>
<subsectionHeader confidence="0.994772">
3.2 The Ambiguity of Case Dependencies
</subsectionHeader>
<bodyText confidence="0.894644888888889">
This section describes the problem of the ambiguity
of case dependencies when observing verb-noun collo-
cation in corpus. This problem is caused by the fact
that, only by observing each verb-noun collocation in
corpus, it is not decidable which cases are dependent
on each other and which cases are optional and inde-
pendent of other cases.
For example, consider the following example:
Example 1
</bodyText>
<equation confidence="0.724549333333333">
Kodomo-ga kouen-de juusu-wo nomu.
child-NOM park-at juice-ACC drink
(A child drinks juice at the park.)
</equation>
<bodyText confidence="0.699541">
The verb-noun collocation is represented as a feature
structure e below:
</bodyText>
<equation confidence="0.997931">
e = [pred : nomu
ga : c,
de: ci,
wo :
</equation>
<bodyText confidence="0.994093888888889">
In this feature structure e, cc, cp and ci repre-
sent the leaf classes (in the thesaurus) of the nouns
&amp;quot;kodomo(child)&amp;quot;, &amp;quot;kouen(park)&amp;quot;, and &amp;quot;juusu(juice)&amp;quot;.
Next, we assume that the concepts &amp;quot;human&amp;quot;, &amp;quot;place
&amp;quot;, and &amp;quot;beverage&amp;quot; are superordinate to &amp;quot;kodomo(chi/d)
&amp;quot;, &amp;quot;kouen(park)&amp;quot;, and &amp;quot;juusu(juice)&amp;quot;, respectively,
and introduce the corresponding classes chum, cpic,
and Cb„. Then, the following superordinate-
subordinate relations hold:
</bodyText>
<subsectionHeader confidence="0.879772">
Cc -&lt;c Chum, Cp c Cpl, C-.&lt;c Chet,
</subsectionHeader>
<bodyText confidence="0.999949416666667">
Allowing these superordinate classes as sense restric-
tion in subcategorization frames, let us consider the
several patterns of subcategorization frames which can
generate the verb-noun collocation e. Those patterns
of subcategorization frames vary according to the de-
pendencies of cases within them.
If the three cases &amp;quot;ga(NOM)&amp;quot;, &amp;quot;wo(ACC)&amp;quot;, and
&amp;quot;de(at)&amp;quot; are dependent on each other and it is not
possible to find any division into a tuple of several in-
dependent partial subcategorization frames, e can be
regarded as generated from a subcategorization frame
containing all of the three cases:
</bodyText>
<equation confidence="0.997739428571429">
[pred : nomu
(
g . chum
wo: Cbev
de : epic
(8)
1 e (9)
</equation>
<page confidence="0.985749">
366
</page>
<bodyText confidence="0.9992372">
Otherwise, if only the two cases &amp;quot;ga(NOM)&amp;quot; and
&amp;quot;wo(ACC)&amp;quot; are dependent on each other and the
&amp;quot;de(at)&amp;quot; case is independent of those two cases, e can
be regarded as generated from the following tuple of
independent partial subcategorization frames:
</bodyText>
<equation confidence="0.972336">
[pred : nomu I) e (10)
de : epic
</equation>
<bodyText confidence="0.9710236">
Otherwise, if all the three cases &amp;quot;ga(NOM)&amp;quot;,
&amp;quot;wo(ACC)&amp;quot;, and &amp;quot;de(at)&amp;quot; are independent of each
other, e can be regarded as generated from the fol-
lowing tuple of independent partial subcategorization
frames, each of which contains only one case:
</bodyText>
<equation confidence="0.75585525">
pred : nomu &apos; pred : nomu &apos; pred : nomu )
I[ WO : Cbev de : epic
\ ga : Chum
(11)
</equation>
<subsectionHeader confidence="0.9562165">
3.3 The Ambiguity of Noun Class
Generalization
</subsectionHeader>
<bodyText confidence="0.999968384615385">
This section describes the problem of the ambiguity
of noun class generalization when observing verb-noun
collocation in corpus. This problem is caused by the
fact that, only by observing each verb-noun colloca-
tion in corpus, it is not decidable which superordinate
class generates each observed leaf class in the verb-
noun collocation.
For example, let us again consider Example 1.
We assume that the concepts &amp;quot;animal&amp;quot; and &amp;quot;liquid&amp;quot;
are superordinate to &amp;quot;human&amp;quot; and &amp;quot;beverage&amp;quot;, re-
spectively, and introduce the corresponding classes
cani and cuq. Then, the following superordinate-
subordinate relations hold:
</bodyText>
<subsubsectionHeader confidence="0.884403">
Chum -&lt;c Cani, Cben -&lt;c Cliq
</subsubsectionHeader>
<bodyText confidence="0.999927785714286">
If we additionally allow these superordinate classes
as sense restriction in subcategorization frames, we
can consider several additional patterns of subcate-
gorization frames which can generate the verb-noun
collocation e, along with those patterns described in
the previous section.
Suppose that only the two cases &amp;quot;ga(NOM)&amp;quot; and
&amp;quot;wo(ACC)&amp;quot; are dependent on each other and the
&amp;quot;de(at)&amp;quot; case is independent of those two cases as in
the formula (10). Since the leaf class cc (&amp;quot;child&amp;quot;) can
be generated from either churn or cani, and also the
leaf class ci (&amp;quot;juice&amp;quot;) can be generated from either
cben or cuq, e can be regarded as generated according
to either of the four formulas (10) and (12),-,(14):
</bodyText>
<footnote confidence="0.383947555555556">
pred : nomu
ga : Cani
WO: Chet,
pred : nomu
ga : Chum
WO : CIiq
pred : nomu
ga : Cans
WO: CIiq
</footnote>
<sectionHeader confidence="0.924245" genericHeader="method">
3.4 A Model of Generating Verb-Noun
Collocation
</sectionHeader>
<bodyText confidence="0.999976846153846">
When observing each verb-noun collocation e, as we
described in the previous two sections, the ambiguities
of case dependencies and noun class generalization re-
main, and it is necessary to consider every possible
tuple of independent partial subcategorization frames
which can generate the observed verb-noun collocation
e. In order to cope with these ambiguities, we intro-
duce two sets: one is a set F of tuples . , fn)
of independent partial subcategorization frames and
the other is a set E of verb-noun collocations e. The
generation of a verb-noun collocation from a tuple of
independent partial subcategorization frames can be
regarded as a mapping it from F to E:
</bodyText>
<equation confidence="0.78242">
it: F E (15)
</equation>
<bodyText confidence="0.983831642857143">
Usually, for each given verb-noun collocation in
E, there exist several possible tuples of independent
partial subcategorization frames in F. Thus, it is a
many-to-one mapping. The mapping from a tuple
fr,) of independent partial subcategorization
frames to a verb-noun collocation e can be denoted
also as follows:
--+ e (16)
When observing a verb-noun collocation e, we as-
sume this many-to-one mapping it and consider every
possible tuple of independent partial subcategoriza-
tion frames which can generate e, according to the
ambiguities of case dependencies and noun class gen-
eralization.
</bodyText>
<subsectionHeader confidence="0.8104095">
3.5 Parameters of Generating Verb-Noun
Collocation
</subsectionHeader>
<bodyText confidence="0.9999625">
Before we give definitions of subcategorization prefer-
ence functions in the next section, we introduce the
parameter q(fk I v) of generating verb-noun colloca-
tion, which is used in the calculation of the subcate-
gorization preference. The parameter q(fk I v) can be
regarded as the conditional probability of the partial
subcategorization frame fk and could be estimated in
the similar way as the p(f I v) in the formula (5).
However, it is the parameter of generating verb-noun
collocation and have to be estimated so as to maxi-
mize the subcategorization preference function for the
training corpus.
One solution of this parameter estimation process
might be to regard the model of generating verb-noun
collocation as a probabilistic model and then to apply
the maximum likelihood estimation method. When
estimating the parameters from the training sample,
we have to note that each verb-noun collocation is
ambiguous since it could be interpreted in several dif-
ferent ways according to case dependencies and opti-
mal noun class generalization levels. As for param-
eter estimation of probabilistic models from ambigu-
ous training sample, EM algorithm(Baum, 1972) is a
well-known solution and has been studied for years.
In EM algorithm, parameters are assigned to events,
and it is required that parameters sum up to 1. How-
ever, since two subcategorization frames could have
the same case and a subsumption relation could hold
</bodyText>
<table confidence="0.972667777777778">
([
pred :hnomu
wo : cbco
[[[pred : nomu e (12)
de : epic e (13)
pred : nomu e (14)
de : epic
pred : nonzu
de : eple
</table>
<page confidence="0.992755">
367
</page>
<bodyText confidence="0.999693545454545">
between their sense restrictions, they may have over-
lap and the requirement that parameters sum up to
1 is not satisfiable. Therefore, it is not so straightfor-
ward to apply EM algorithm to the task of parameter
estimation of generating verb-noun collocation.
Instead of introducing a probabilistic model of gen-
erating verb-noun collocation&apos;, in this paper, we em-
ploy more general framework which is applicable to
various measures of subcategorization preference in-
cluding the probability of generating verb-noun collo-
cation. In the framework, the process of parameter
estimation is regarded as a general optimization prob-
lem of maximizing the subcategorization preference
function for the training corpus.
In order to describe the framework, first we intro-
duce the probability p((ii, • • • , fn )i ei e•) of gen-
erating a verb-noun collocation ei in the set 3E from a
tuple njin the set F, given ei, and denote
it as a conditional probability p((fi, • • • , I es).
Then, for each ei in E, we can consider a probability
distribution P((ii, • • • , fn)i I ei) over the set F of tu-
ples of independent partial subcategorization frames:
</bodyText>
<equation confidence="0.918738">
E
el • - • ei
F (fi, • • .,.f.,)1 : ... :
(ii, • • • , fn&amp;quot;)m P((fi, ••• , Mi I ei)
• • •
</equation>
<bodyText confidence="0.988609">
isfies the following axiom of the probability:
Each probability distribution p((fi,..., fn)i I ei) sat-
E, . • • , fo, I ei) = 1 for all i
According to the probability distribution p((fi,...,
fn)iI ei) of generating ei from (Ii, fn)i, we esti-
mate the frequency of the subcategorization frame fk
and then estimate the parameter q(fk I v) as below:
</bodyText>
<equation confidence="0.99886275">
&gt;1 • p((fi,. • • ,fk,...,2 I ei)
freq(fk) ii
q( f k Iv) freq(v) freq(v)
(17)
</equation>
<bodyText confidence="0.982524">
When learning probabilistic subcategorization pref-
erence (section 5), we estimate the probability distri-
bution P((fi, • • • , fn)jI ei) for each ei so as to maxi-
mize the subcategorization preference function for ei.
</bodyText>
<sectionHeader confidence="0.975217" genericHeader="method">
4 Subcategorization Preference
Functions
</sectionHeader>
<bodyText confidence="0.9963475">
This section introduces a function 0 which measures
the subcategorization preference when generating a
verb-noun collocation e from a tuple ,f) of
independent partial subcategorization frames:
</bodyText>
<equation confidence="0.976541">
e) (18)
</equation>
<bodyText confidence="0.968592">
In this paper, we introduce a subcategorization pref-
erence function which is based-on the idea of Kullback
Leibler distance.5
</bodyText>
<footnote confidence="0.783806857142857">
4Another alternative of solving the problem of learn-
ing probabilistic subcategorization preference based-on a
probabilistic model is to -regard the problem as the con-
struction of probabilistic models from the training sample.
We will discuss this issue in section 7.
51n Utsuro and Matsumoto (1997), we defined another
subcategorization preference function Op which is based-
</footnote>
<subsectionHeader confidence="0.9930235">
4.1 Nominal Parts of (Partial)
Subcategorization Frames
</subsectionHeader>
<bodyText confidence="0.949432">
First, let fp, fpi, , pnhe the nominal parts of
(partial) subcategorization frames f, , fn in the
equations (2) and (4), respectively:
</bodyText>
<equation confidence="0.990965">
[pi :CIf
Pk Ck f
fp = .fPi A A fPn
ViVj&apos; pi; 0 pvi,
(i, i&apos; =1,...,n, i i&apos;)
</equation>
<bodyText confidence="0.99996">
As in the case of the parameters q(fi I v) of fi
given the verb v, we estimate the probability p(f)
of the nominal part fpi in the whole corpus and call it
the parameter q(f) of fpi in the whole training cor-
pus. We estimate the frequency of f,, throughout the
whole training corpus and then estimate the parame-
ter q(f2) of fpi as below:
</bodyText>
<equation confidence="0.975502">
E freq(fk)
q(f,k) v
EE Au, . • • , fk, • • • , I e.)
v i,j (19)
</equation>
<subsectionHeader confidence="0.957116">
4.2 cbkl: Kullback Leibler Distance
</subsectionHeader>
<bodyText confidence="0.999935545454545">
Rather than the simple conditional probability, this
preference function is intended to measure the
information-theoretic association of the verb v and the
nominal part of the subcategorization frame.
The Kullback Leibler (KL) distance is a measure
of the distance between two probability distribution.
p(X) and q(X) is defined as below(Cover and Thomas,
tributions p(X) and q(X), the KL distance D(pllq) of
Given a random variable X and two probability dis-
1991), where each term can be regarded as the dis-
tance of two probabilities p(x) and q(x) of an event x:
</bodyText>
<equation confidence="0.976128">
MAO = E p(x) log P(x)
q(x)
xEX
</equation>
<bodyText confidence="0.999761933333333">
In order to apply the idea of the KL distance to mea-
suring the association of the verb v and the nominal
part fp of f, we introduce a random variable Fp which
takes fp as its value. We also introduce the probability
distribution p(Fp) of Fp and the conditional proba-
bility distribution p(Fp I v) of Fp given the verb v.
Then, the KL distance of p(Fp I v) and p(Fp) is
denoted as D(p(Fp v)IIp(Fp)) and each term of it
can be regarded as the distance of two probabilities
p(fpI v) and p(fp). We assume that the larger this
distance is, the stronger the association of fp and v
is, and measure the association of fp and v with this
on the probability of generating the verb-noun collocation
and described experimental results of applying Op to the
task of learning probabilistic sub categorization.
</bodyText>
<equation confidence="0.974411">
fp =
fpi =
</equation>
<page confidence="0.991671">
368
</page>
<bodyText confidence="0.978782666666667">
distance of the two probabilities p( fp I u) and p( fp).
With this idea. the subcategorization preference func-
tion 0k1 is now formally defined as below:6 7
</bodyText>
<equation confidence="0.883110666666667">
oki((fi fn) e)
(22)
(21) is derived from the independence of the partial
</equation>
<bodyText confidence="0.999914333333333">
subcategorization frames 11, fr, In (22), we use
the parameters q( f1 I v) and g(f pi) as an approxima-
tion of the probabilities p( fpi v) and p(fpi).
</bodyText>
<sectionHeader confidence="0.8521755" genericHeader="method">
5 Learning Probabilistic
Subcategorization Preference
</sectionHeader>
<bodyText confidence="0.999439666666667">
The problem of learning subcategorization preference
can be formalized as an optimization problem of es-
timating the probability distribution p((fi, • • • , f):7 I
ei) (in section 3.5) of generating ei from , fn)i
(and then the parameters g(fpk I v) and g(fpk)) so as
to maximize the value of the subcategorization pref-
erence function for the whole training corpus. In
this paper, we give only an approximate solution to
this problem: we estimate the probability distribu-
tion PK(ii, • • , frOi ei) for each ei so as to maximize
the value of the subcategorization preference function
only for ei, not for the whole training corpus.
</bodyText>
<subsectionHeader confidence="0.9829">
5.1 Problem Setting
</subsectionHeader>
<bodyText confidence="0.973928">
Let the training corpus S be the set of verb-noun col-
location e. We define the subcategorization preference
i&apos;(e) of a verb-noun collocation e as the maximum of
the subcategorization preference function 0 (the for-
mula (18)) of generating e from a tuple fn).
</bodyText>
<equation confidence="0.582941">
(b(e) = max (/)((fi f„) —+e) (23)
</equation>
<bodyText confidence="0.987993388888889">
Now, the problem of learning probabilistic subcate-
gorization preference is stated as:
for every verb-noun collocation e in 5, es-
timating the probability distribution p((11,
6Resnik (1993) applys the idea of the KL distance to
measuring the association of a verb v and its object noun
class c. Our definition of Oki corresponds to an extension of
Resnik&apos;s association score, which considers dependencies of
more than one case-markers in a subcategorization frame.
7Another related measure is Dunning (1993)&apos;s likeli-
hood ratio tests for binomial and multinomial distribu-
tions, which are claimed to be effective even with very
much smaller volumes of text than is necessary for other
tests based on assumed normal distributions.
....1n)i I e) of generating e from (h.
.... N.,. under the constraint that the value
of the subcategorization preference o(e) is
maximized.
</bodyText>
<subsectionHeader confidence="0.954341">
5.2 Learning Algorithm
</subsectionHeader>
<equation confidence="0.93342">
F(e) = {(fi f.) el (24)
</equation>
<bodyText confidence="0.998756666666667">
F(e) contains a tuple (f) consisting of only one
subcategorization frame f only if f can not be di-
vided into several independent partial subcategoriza-
tion frames.
Then, we assume that each element of F(e) occurs
evenly and estimate the initial conditional probability
</bodyText>
<equation confidence="0.8823214">
distribution p((fl, • • • ,1)3 I e) of generating e from
Kul, • • • , in), as an approximation below:
1
P((fi, • • • , f.))I e) (25)
IF (e)I
</equation>
<footnote confidence="0.307868">
5.2.1 Approximate Estimation of
Verb-Independent Parameters
</footnote>
<bodyText confidence="0.95295875">
Using the initial conditional probability distribution
of p((fi, • • • , I e) as in the formula (25), the ini-
tial values of the verb-independent parameters g(fpk)
are estimated by the formulas (19). In the current im-
plementation of the learning algorithm, we use these
initial values as approximate estimation of those verb-
independent parameters and probabilities throughout
the learning process.
</bodyText>
<subsectionHeader confidence="0.396428">
5.2.2 Iterative Reestimation of
Verb-Dependent Parameters
</subsectionHeader>
<bodyText confidence="0.9803202">
Verb-dependent parameters g(fk I v)(= g(fpk I v))
are iteratively estimated so as to maximize the sub-
categorization preference gi(e) for every verb-noun col-
location e in the training corpus E. As a learning al-
gorithm, we employ the following stingy algorithm:
</bodyText>
<sectionHeader confidence="0.761398" genericHeader="method">
1. Initialization
</sectionHeader>
<bodyText confidence="0.999831125">
As with the case of the verb-independent param-
eters, for each verb-noun collocatoin e in 5, the set
F(e) is initially constructed according to the defini-
tion in (24). Then, the initial conditional probability
distribution of p((f1,••• , fn), e) and the initial val-
ues of the verb-dependent parameters g(fk I v) are
estimated as (25) and (17), respectively.
81n the current implementation, we deal with sense am-
biguities of case-marked nouns and case ambiguities of
Japanese topic-marking post-positional particles such as
&amp;quot;ha(TOPIC)&amp;quot;, &amp;quot;mo(ALSO)&amp;quot;, and &amp;quot;dake(ONLY)&amp;quot;. When
constructing the set F(e), we consider all the possible com-
bination of senses of semantically ambiguous nouns and
cases of topic-marking post-positional particles. These
ambiguities can be resolved by maximizing the subcate-
gorization preference function (section 5.2.2).
</bodyText>
<figure confidence="0.7988365">
p( fp I u) log p(fp I r)
P(fP)
flP(hi I v) TIPUPi I V)
x log &apos;=1„
Hp(f)
11q(fPi r)
x log i=ln
q(f)
</figure>
<bodyText confidence="0.985641142857143">
(20) First. we identify independent partial subcategoriza-
tion frames according to the condition of (8). Then,
let E(v) be the set of verb-noun collocations contain-
ing the verb r in the training corpus E. Let F(e) be the
(21) set of tuples (11 fn) of independent partial sub-
categorization frames which can generate e and satisfy
the independence condition of (8).8
</bodyText>
<page confidence="0.999306">
369
</page>
<tableCaption confidence="0.999794">
Table 1: The Result of Learning Probabilistic Sub-
</tableCaption>
<table confidence="0.994515875">
categorization Preference for &amp;quot;kau(buy,incur)&amp;quot; kl,
a=0.9
E = {(fpl, • • • JP.)}(Eg.) ii&apos;kl Egs.
1 [wo(ACC):14(Products)] 1.88 158
2 [wo(ACC):13721-8(kabu(stock))] 0.27 15
3 [ga(NOM):12(Human)] 0.27 40
4 No(ACC):15(Nature)] 0.21 25
5 kara(from):12(Human)] 0.19 14
6 [de(at):12(Shop,Place)] 0.17 18
7 [ga(NOM):12(Human), 0.16 6
wo(ACC):13721-8(kabu(stock))]
8 [wo(ACC):13010(hukyou(disgust))] 0.12 6
9 [wo(ACC):11961-1(Currency)] 0.10 6
10 [ga(NOM):12(Human),wo(ACC): 0.09 4
1456(Musical Instruments)]
(11th,s450th) 196
</table>
<sectionHeader confidence="0.897621" genericHeader="method">
2. Iterative Reestimation
</sectionHeader>
<bodyText confidence="0.994232909090909">
The subcategorization preference (e) are maxi-
mized by repeatedly searching the set F(e) for tuples
,f,) which give the maximum subcategoriza-
tion preference and removing other tuples from F(e).
The following two steps are repeated until the values
of the parameters q( f k v) converge.
(2a) For each verb-noun collocatoin e in 1, set F(e)
as the set of tuples , f72) of independent
partial subcategorization frames which can gen-
erate e and give the maximum subcategorization
preference in the equation (23).
</bodyText>
<equation confidence="0.896540166666667">
fr (e) e)=(7)(e)}
(2b) Set the values of the conditional probabilities
P((fi, • • • , fn)jI e) as below and the parameters
q(fkI v) as (17), respectively: 1
f.).7 e) -
IF(e)i
</equation>
<sectionHeader confidence="0.994916" genericHeader="evaluation">
6 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.997503">
6.1 Corpus and Thesaurus
</subsectionHeader>
<bodyText confidence="0.999947">
As the training and test corpus, we used the EDR
Japanese bracketed corpus (EDR, 1995), which con-
tains about 210,000 sentences collected from newspa-
per and magazine articles. From the EDR corpus,
we extracted 153,014 verb-noun collocations of 835
verbs which appear more than 50 times in the cor-
pus. These verb-noun collocations contain about 270
case-markers. We constructed the training set 4 from
these 153,014 verb-noun collocations.
We used `Bunrui Goi Hyou&apos;(BGH) (NLRI, 1993)
as the Japanese thesaurus. BGH has a six-layered
abstraction hierarchy and more than 60,000 words are
assigned at the leaves and its nominal part contains
about 45,000 words. Five classes are allocated at the
next level from the root node.
</bodyText>
<subsectionHeader confidence="0.96178">
6.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999535433333333">
From the training set S, we first estimated the values
of verb-independent parameters as in section 5.2.1,
and then iteratively reestimated verb-dependent pa-
rameters of the subcategorization preference function
Oki for 10 verbs as in section 5.2.2. For each of the
10 verbs, the numbers of verb-noun collocations are
100 500. We made experiments with the indepen-
dence parameter a =0.5/0.7/0.9. In the iterative rees-
timation procedure, the values of the verb-dependent
parameters converged after 2 5 iterations.
For the 10 verbs, about 75% of the verb-noun collo-
cations have only one case-marked noun. The rate
that tuples of partial subcategorization frames are
judged as independent increases as the value of the
independence parameter a decreases. This rate in-
creases from 1.4% (a =0.9) to 12.1% (a =0.5).
As an example, for the verb &amp;quot;kau(buy,incur)&amp;quot;, Ta-
ble 1 shows the set F(e) of tuples of independent
partial subcategorization frames which give maximum
subcategorization preference. The table lists the sets
F(e) with 10 highest preference values of Ok1, along
with the numbers (the column &apos;Egs.&apos;) of verb-noun
collocations for each F(e), which are judged as gen-
erated from it&apos;. Since about 75% of the verb-noun
collocations have only one case-marked noun, most
of the 10 high-scored sets have only one case-marked
noun. However, the 10 high-scored sets cover about
60% of the verb-noun collocations in the training set,
and they can be regarded as typical subcategorization
frames of the verb &amp;quot;kau(buy,incur)&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.7536315">
6.3 Evaluation of Subcategorization
Preference
</subsectionHeader>
<bodyText confidence="0.97045852631579">
We evaluate the performance of the estimated param-
eters of the subcategorization preference as follows.
Suppose that the following word sequence repre-
sents a verb-final Japanese sentence with a subordi-
nate clause, where Nx, N2k are nouns, px, , p2k
are case-marking post-positional particles, Vi, v2 are
verbs, and the first verb v1 is the head verb of the
subordinate clause.
Nx-px-Nii-pii-• • •- -P21 -• • •-N2k-P2k-V2
We consider the subcategorization ambiguity of the
post-positional phrase Nx-p: i.e, whether .Arx-px is
subcategorized by v1 or v2.
We use held-out verb-noun collocations of the verbs
v1 and v2 which are not used in the training. They
are like those verb-noun collocations eci and ea in
the left side below. Next, we generate erroneous verb-
noun collocations e€1 of vi and ee2 of v2 as those in the
right side below, by choosing a case element px: INT, at
random and moving it from v1 to v2.
</bodyText>
<equation confidence="0.9480371">
Error
pred : vi
pii N11
eel=
:
pred : V2
P21 N21
ee2 =
P2k N2k
Px N.
</equation>
<bodyText confidence="0.497812">
9In each subcategorization frame, Japanese noun
classes of BGH thesaurus are represented as numerical
codes, in which each digit denotes the choice of the branch
in the thesaurus.
</bodyText>
<figure confidence="0.9646321">
Correct
- pred :
: N11
eci
pu :Nij
: Nr
pred : V2
p21 : N21
ec2
P21 N2k
</figure>
<page confidence="0.990128">
370
</page>
<tableCaption confidence="0.995946">
Table 2: Accuracies of Subcategorization Preference
</tableCaption>
<table confidence="0.941668666666666">
with (I) k (%)
Independent Any
a=0.5 a=0.9 a=0.5 a=0.9
Optimal + 81.7 70.7 65.8 68.6
- 2.2 3.3 27.1 6.0
Initial + 16.1 25.6 7.1 25.0
- 0 0.4 0 0.4
Accuracy 97.8 96.3 72.9 93.6
Applicability 83.9 74.0 92.9 74.6
</table>
<bodyText confidence="0.9998462">
Then, we compare the sum 0(eci) 0(e,2) of the
maximums (in the definition (23)) of Oki for the cor-
rect pair with the sum iA,(eei ) ((ee2) of those for the
erroneous pair, and calculate the rate that the correct
pair has the greater value.
For the purpose of evaluating the effectiveness
of factors of learning probabilistic subcategorization
preference, we perform experiments with different set-
tings and compare their results. The following two
options are examined:
</bodyText>
<listItem confidence="0.973325">
• Whether the subcategorization preference func-
tion uses tuples of partial subcategorization
frames judged as independent (&amp;quot;Independent&amp;quot;), or
any tuples (&amp;quot;Any&amp;quot;).
• The independence parameter a =0.5/0.9.
</listItem>
<bodyText confidence="0.999796161290323">
For three Japanese verbs &amp;quot;kau (buy,incur)&amp;quot;, &amp;quot;nomu
(drink)&amp;quot;, and &amp;quot;kasaneru (pile up, repeat)&amp;quot;, we ex-
tracted pairs of correct verb-noun collocations and
evaluated the performance of subcategorization pref-
erence. Table 2 gives the results averaged over ex-
tracted pairs, including the accuracies of subcat-
egorization preference. The difference of &amp;quot;Opti-
mal&amp;quot; /&amp;quot;Initial&amp;quot; means that initial values of the pa-
rameters are used instead of optimized values (section
5.2.2) when the subcategorization preference function
is not applicable to the given verb-noun collocation
and returns zero. The line &amp;quot;Accuracy&amp;quot; lists the sums
of both &amp;quot;Optimal&amp;quot; and &amp;quot;Initial&amp;quot; accuracies, while the
line &amp;quot;Applicability&amp;quot; lists the percentages of positive
values of the subcategorization preference function
with optimized parameters.
It is natural that the settings with more weak con-
ditions on the independence judgment of partial sub-
categorization frames result in higher applicabilities.
The setting with independent tuples of partial subcat-
egorization frames achieves higher accuracy than that
with any tuples, and this result claims that the result
of the independence judgment is effective when apply-
ing the estimated parameters to the task of subcate-
gorization preference. Even in the case of the setting
with any tuples, the setting with a =0.5 gives poorer
accuracy than that of a =0.9. In this case, the differ-
ence of the independence parameter a affects only the
parameter estimation stage. This result claims that
the independence judgment process is effective also
when estimating parameters from the training corpus.
</bodyText>
<sectionHeader confidence="0.992352" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978272727273">
This paper proposed a novel method of learning
probabilistic subcategorization preference of verbs.
We described a part of the results of the exper-
iments on learning probabilistic subcategorization
preference from the EDR Japanese bracketed cor-
pus, as well as those on evaluating the performance
of subcategorization preference. Although the scale
of the evaluation experiment was relatively small,
we achieved accuracies higher than 96%. The de-
tails of the experimental results are available in
Utsuro and Matsumoto (1997). As we mentioned in
section 3.5, probabilistic model construction methods
might be also applicable to the task of learning prob-
abilistic subcategorization preference. We have al-
ready applied the maximum entropy methods(Pietra,
Pietra, and Lafferty, 1995; Berger, Pietra, and Pietra,
1996) to this task(Utsuro, Miyata, and Matsumoto,
1997) and are also planning to evaluate the effective-
ness of the MDL principle(Rissanen, 1989) when com-
bining with the maximum entropy method. Their re-
sults will be compared with those of the method pro-
posed in this paper and reported in the near future.
</bodyText>
<sectionHeader confidence="0.99793" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975211156862745">
Baum, L. E. 1972. An inequality and associated maximization
technique in statistical estimation for probabilistic functions of
markov processes. Inequalities, 3:1-8.
Berger, A. L., S. A. D. Pietra, and V. J. D. Pietra. 1996. A maxi-
mum entropy approach to natural language processing. Compu-
tational Linguistics, 22(1):39-71.
Black, E. 1993. Towards history-based grammars: Using richer
models for probabilistic parsing. In Proceedings of the 31st An-
nual Meeting of ACL, pages 31-37.
Collins, M. 1996. A new statistical parser based on bigram lexical
dependencies. In Proceedings of the 34th Annual Meeting of
ACL, pages 184-191.
Cover, T. M. and J. A. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons, Inc.
Dunning, T. 1993. Accurate methods for the statistics of surprise
and coincidence. Computational Linguistics, 19(1):61-74.
EDR, (Japan Electronic Dictionary Research Institute, Ltd), 1995.
EDR Electronic Dictionary Technical Guide.
Haruno, M. 1995. Verbal case frame acquisition as data compres-
sion. In Proceedings of the 5th International Workshop on Nat-
ural Language Understanding and Logic Programming.
Li, H. and N. Abe. 1995. Generalizing case frames using a the-
saurus and the MDL principle. In Proceedings of International
Conference on Recent Advances in Natural Language Process-
ing, pages 239-248.
Li, H. and N. Abe. 1996. Learning dependencies between case
frame slots. In Proceedings of the 16th COLING, pages 10-15.
Magerman, D. M. 1995. Statistical decision-tree models for pars-
ing. In Proceedings of the 93rd Annual Meeting of ACL, pages
276-283.
NLRI, (National Language Research Institute), 1993. Word List
by Semantic Principles. Syuei Syuppan. (in Japanese).
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. Inducing fea-
tures of random fields. CMU Technical Report CMU-CS-95-144,
School of Computer Science, Carnegie Mellon University.
Resnik, P. 1993. Semantic classes and syntactic ambiguity. In Pro-
ceedings of the Human Language Technology Workshop, pages
278-283.
Rissanen, J. 1989. Stochastic Complexity in Statistical Inquiry,
volume 15 of Series in Computer Science. World Scientific Pub-
lishing Company.
Utsuro, T. and Y. Matsumoto. 1997. Learning proba-
bilistic subcategorization preference and its application to
syntactic disambiguation. Information Science Technical
Report NAIST-IS-TR97006, Nara Institute of Science and
Technology. (http://www.aist-nara.ac.jp/IS/TechReport/
re-
port..gz/97006.ps.gz).
Utsuro, T., T. Miyata, and Y. Matsumoto. 1997. Maximum en-
tropy parameter learning of subcategorization preference. (sub-
mitted to the 35th Annual Meeting of ACL).
</reference>
<page confidence="0.998035">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592425">
<title confidence="0.798176333333333">Learning Probabilistic Sub categorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level*</title>
<author confidence="0.79581">Takehito Utsuro Yuji Matsumoto</author>
<affiliation confidence="0.997511">Graduate School of Information Science, Nara Institute of Science and Technology</affiliation>
<address confidence="0.99931">8916-5, Takayama-cho, Ikoma-shi, Nara, 630-01, JAPAN</address>
<email confidence="0.991302">futsuro,matsulOis.aist-nara.ac.jp</email>
<abstract confidence="0.9995415">This paper proposes a novel method of learning probabilistic subcategorization preference. In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames. Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames. Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus. We also describe the results of the experiments on learning probabilistic subpreference from the bracketed corpus, as well as those on evaluating the performance of subcategorization preference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="18504" citStr="Baum, 1972" startWordPosition="3032" endWordPosition="3033">zation preference function for the training corpus. One solution of this parameter estimation process might be to regard the model of generating verb-noun collocation as a probabilistic model and then to apply the maximum likelihood estimation method. When estimating the parameters from the training sample, we have to note that each verb-noun collocation is ambiguous since it could be interpreted in several different ways according to case dependencies and optimal noun class generalization levels. As for parameter estimation of probabilistic models from ambiguous training sample, EM algorithm(Baum, 1972) is a well-known solution and has been studied for years. In EM algorithm, parameters are assigned to events, and it is required that parameters sum up to 1. However, since two subcategorization frames could have the same case and a subsumption relation could hold ([ pred :hnomu wo : cbco [[[pred : nomu e (12) de : epic e (13) pred : nomu e (14) de : epic pred : nonzu de : eple 367 between their sense restrictions, they may have overlap and the requirement that parameters sum up to 1 is not satisfiable. Therefore, it is not so straightforward to apply EM algorithm to the task of parameter esti</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L. E. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A. L., S. A. D. Pietra, and V. J. D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of ACL,</booktitle>
<pages>31--37</pages>
<contexts>
<context position="1708" citStr="Black (1993)" startWordPosition="238" endWordPosition="239">ing probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference. 1 Introduction In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, ledcal/semantic collocation are used for ranking parses in syntactic analysis. The authors would like to thank Dr. Hang Li</context>
</contexts>
<marker>Black, 1993</marker>
<rawString>Black, E. 1993. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the 31st Annual Meeting of ACL, pages 31-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2053" citStr="Collins (1996)" startWordPosition="284" endWordPosition="285"> in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, ledcal/semantic collocation are used for ranking parses in syntactic analysis. The authors would like to thank Dr. Hang Li of NEC GSLC Research Laboratories, Dr. Kentaro Inui of Tokyo Institute of Technology, Dr. Koiti Hasida of Electrotechnical Laboratory, Dr. Takashi Miyata of Nara Institute of Science and Technology, and also anonymous reviewers of ANLP97 for valuable comments on this work. On the other hand, in the context of automatic lexicon construction, t</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, M. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of ACL, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley and Sons, Inc.</publisher>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M. and J. A. Thomas. 1991. Elements of Information Theory. John Wiley and Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence. Computational Linguistics,</title>
<date>1993</date>
<booktitle>EDR Electronic Dictionary Technical Guide.</booktitle>
<institution>EDR, (Japan Electronic Dictionary Research Institute, Ltd),</institution>
<contexts>
<context position="25476" citStr="Dunning (1993)" startWordPosition="4256" endWordPosition="4257">preference function 0 (the formula (18)) of generating e from a tuple fn). (b(e) = max (/)((fi f„) —+e) (23) Now, the problem of learning probabilistic subcategorization preference is stated as: for every verb-noun collocation e in 5, estimating the probability distribution p((11, 6Resnik (1993) applys the idea of the KL distance to measuring the association of a verb v and its object noun class c. Our definition of Oki corresponds to an extension of Resnik&apos;s association score, which considers dependencies of more than one case-markers in a subcategorization frame. 7Another related measure is Dunning (1993)&apos;s likelihood ratio tests for binomial and multinomial distributions, which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions. ....1n)i I e) of generating e from (h. .... N.,. under the constraint that the value of the subcategorization preference o(e) is maximized. 5.2 Learning Algorithm F(e) = {(fi f.) el (24) F(e) contains a tuple (f) consisting of only one subcategorization frame f only if f can not be divided into several independent partial subcategorization frames. Then, we assume that each ele</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74. EDR, (Japan Electronic Dictionary Research Institute, Ltd), 1995. EDR Electronic Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Haruno</author>
</authors>
<title>Verbal case frame acquisition as data compression.</title>
<date>1995</date>
<booktitle>In Proceedings of the 5th International Workshop on Natural Language Understanding and Logic Programming.</booktitle>
<contexts>
<context position="2822" citStr="Haruno (1995)" startWordPosition="404" endWordPosition="405">tion are used for ranking parses in syntactic analysis. The authors would like to thank Dr. Hang Li of NEC GSLC Research Laboratories, Dr. Kentaro Inui of Tokyo Institute of Technology, Dr. Koiti Hasida of Electrotechnical Laboratory, Dr. Takashi Miyata of Nara Institute of Science and Technology, and also anonymous reviewers of ANLP97 for valuable comments on this work. On the other hand, in the context of automatic lexicon construction, the emphasis is mainly on the extraction of lexical/semantic collocational knowledge of specific words rather than its use in sentence parsing. For example, Haruno (1995) applied an informationtheoretic data compression technique to corpus-based case frame learning, and proposed a method of finding case frames of verbs as compressed representation of verb-noun collocational data in corpus. The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing. This paper focuses on extracting lexical/semantic collocational knowledge of verbs for the purpose of applying it to ranking parses in syntactic analysis. More specifically, we propose a novel method for learning parameters for calc</context>
</contexts>
<marker>Haruno, 1995</marker>
<rawString>Haruno, M. 1995. Verbal case frame acquisition as data compression. In Proceedings of the 5th International Workshop on Natural Language Understanding and Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="4347" citStr="Li and Abe (1995)" startWordPosition="638" endWordPosition="641">sed by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases. 2) is caused by the fact that, only by observing each verbnoun collocation in corpus, it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a treestructured thesaurus. Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task. They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependencies between the classba</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Li, H. and N. Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. In Proceedings of International Conference on Recent Advances in Natural Language Processing, pages 239-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Learning dependencies between case frame slots.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING,</booktitle>
<pages>10--15</pages>
<contexts>
<context position="4635" citStr="Li and Abe (1996)" startWordPosition="685" endWordPosition="688">is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a treestructured thesaurus. Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task. They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependencies between the classbased case slots. They reported that dependencies 364 were discovered only at the slot-level and not at the class-level. Compared with those previous works, this paper proposes to cope with the above two ambiguities in a uniform way. First, we introduce a data structure which represents a </context>
</contexts>
<marker>Li, Abe, 1996</marker>
<rawString>Li, H. and N. Abe. 1996. Learning dependencies between case frame slots. In Proceedings of the 16th COLING, pages 10-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 93rd Annual Meeting of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="1728" citStr="Magerman (1995)" startWordPosition="241" endWordPosition="242"> subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference. 1 Introduction In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, ledcal/semantic collocation are used for ranking parses in syntactic analysis. The authors would like to thank Dr. Hang Li of NEC GSLC Researc</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, D. M. 1995. Statistical decision-tree models for parsing. In Proceedings of the 93rd Annual Meeting of ACL, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NLRI</author>
</authors>
<title>Word List by Semantic Principles. Syuei Syuppan.</title>
<date>1993</date>
<institution>(National Language Research Institute),</institution>
<note>(in Japanese).</note>
<contexts>
<context position="30204" citStr="NLRI, 1993" startWordPosition="4998" endWordPosition="4999">elow and the parameters q(fkI v) as (17), respectively: 1 f.).7 e) - IF(e)i 6 Experiments and Evaluation 6.1 Corpus and Thesaurus As the training and test corpus, we used the EDR Japanese bracketed corpus (EDR, 1995), which contains about 210,000 sentences collected from newspaper and magazine articles. From the EDR corpus, we extracted 153,014 verb-noun collocations of 835 verbs which appear more than 50 times in the corpus. These verb-noun collocations contain about 270 case-markers. We constructed the training set 4 from these 153,014 verb-noun collocations. We used `Bunrui Goi Hyou&apos;(BGH) (NLRI, 1993) as the Japanese thesaurus. BGH has a six-layered abstraction hierarchy and more than 60,000 words are assigned at the leaves and its nominal part contains about 45,000 words. Five classes are allocated at the next level from the root node. 6.2 Experiments and Results From the training set S, we first estimated the values of verb-independent parameters as in section 5.2.1, and then iteratively reestimated verb-dependent parameters of the subcategorization preference function Oki for 10 verbs as in section 5.2.2. For each of the 10 verbs, the numbers of verb-noun collocations are 100 500. We ma</context>
</contexts>
<marker>NLRI, 1993</marker>
<rawString>NLRI, (National Language Research Institute), 1993. Word List by Semantic Principles. Syuei Syuppan. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>CMU Technical Report CMU-CS-95-144,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. Inducing features of random fields. CMU Technical Report CMU-CS-95-144, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic classes and syntactic ambiguity.</title>
<date>1993</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>278--283</pages>
<contexts>
<context position="4325" citStr="Resnik (1993)" startWordPosition="635" endWordPosition="636">lization 1) is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases. 2) is caused by the fact that, only by observing each verbnoun collocation in corpus, it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a treestructured thesaurus. Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task. They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependenci</context>
<context position="25158" citStr="Resnik (1993)" startWordPosition="4205" endWordPosition="4206">o maximize the value of the subcategorization preference function only for ei, not for the whole training corpus. 5.1 Problem Setting Let the training corpus S be the set of verb-noun collocation e. We define the subcategorization preference i&apos;(e) of a verb-noun collocation e as the maximum of the subcategorization preference function 0 (the formula (18)) of generating e from a tuple fn). (b(e) = max (/)((fi f„) —+e) (23) Now, the problem of learning probabilistic subcategorization preference is stated as: for every verb-noun collocation e in 5, estimating the probability distribution p((11, 6Resnik (1993) applys the idea of the KL distance to measuring the association of a verb v and its object noun class c. Our definition of Oki corresponds to an extension of Resnik&apos;s association score, which considers dependencies of more than one case-markers in a subcategorization frame. 7Another related measure is Dunning (1993)&apos;s likelihood ratio tests for binomial and multinomial distributions, which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions. ....1n)i I e) of generating e from (h. .... N.,. under the co</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, P. 1993. Semantic classes and syntactic ambiguity. In Proceedings of the Human Language Technology Workshop, pages 278-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Inquiry,</title>
<date>1989</date>
<booktitle>Series in Computer Science.</booktitle>
<volume>15</volume>
<publisher>World Scientific Publishing Company.</publisher>
<marker>Rissanen, 1989</marker>
<rawString>Rissanen, J. 1989. Stochastic Complexity in Statistical Inquiry, volume 15 of Series in Computer Science. World Scientific Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>Y Matsumoto</author>
</authors>
<title>Learning probabilistic subcategorization preference and its application to syntactic disambiguation.</title>
<date>1997</date>
<journal>Information Science</journal>
<tech>Technical Report NAIST-IS-TR97006,</tech>
<institution>Nara Institute of Science and Technology.</institution>
<note>(http://www.aist-nara.ac.jp/IS/TechReport/ report..gz/97006.ps.gz).</note>
<contexts>
<context position="21413" citStr="Utsuro and Matsumoto (1997)" startWordPosition="3538" endWordPosition="3541">s This section introduces a function 0 which measures the subcategorization preference when generating a verb-noun collocation e from a tuple ,f) of independent partial subcategorization frames: e) (18) In this paper, we introduce a subcategorization preference function which is based-on the idea of Kullback Leibler distance.5 4Another alternative of solving the problem of learning probabilistic subcategorization preference based-on a probabilistic model is to -regard the problem as the construction of probabilistic models from the training sample. We will discuss this issue in section 7. 51n Utsuro and Matsumoto (1997), we defined another subcategorization preference function Op which is based4.1 Nominal Parts of (Partial) Subcategorization Frames First, let fp, fpi, , pnhe the nominal parts of (partial) subcategorization frames f, , fn in the equations (2) and (4), respectively: [pi :CIf Pk Ck f fp = .fPi A A fPn ViVj&apos; pi; 0 pvi, (i, i&apos; =1,...,n, i i&apos;) As in the case of the parameters q(fi I v) of fi given the verb v, we estimate the probability p(f) of the nominal part fpi in the whole corpus and call it the parameter q(f) of fpi in the whole training corpus. We estimate the frequency of f,, throughout th</context>
<context position="36341" citStr="Utsuro and Matsumoto (1997" startWordPosition="5979" endWordPosition="5982">ce judgment process is effective also when estimating parameters from the training corpus. 7 Conclusion This paper proposed a novel method of learning probabilistic subcategorization preference of verbs. We described a part of the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference. Although the scale of the evaluation experiment was relatively small, we achieved accuracies higher than 96%. The details of the experimental results are available in Utsuro and Matsumoto (1997). As we mentioned in section 3.5, probabilistic model construction methods might be also applicable to the task of learning probabilistic subcategorization preference. We have already applied the maximum entropy methods(Pietra, Pietra, and Lafferty, 1995; Berger, Pietra, and Pietra, 1996) to this task(Utsuro, Miyata, and Matsumoto, 1997) and are also planning to evaluate the effectiveness of the MDL principle(Rissanen, 1989) when combining with the maximum entropy method. Their results will be compared with those of the method proposed in this paper and reported in the near future. References</context>
</contexts>
<marker>Utsuro, Matsumoto, 1997</marker>
<rawString>Utsuro, T. and Y. Matsumoto. 1997. Learning probabilistic subcategorization preference and its application to syntactic disambiguation. Information Science Technical Report NAIST-IS-TR97006, Nara Institute of Science and Technology. (http://www.aist-nara.ac.jp/IS/TechReport/ report..gz/97006.ps.gz).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>T Miyata</author>
<author>Y Matsumoto</author>
</authors>
<title>Maximum entropy parameter learning of subcategorization preference. (submitted to the 35th Annual Meeting of ACL).</title>
<date>1997</date>
<marker>Utsuro, Miyata, Matsumoto, 1997</marker>
<rawString>Utsuro, T., T. Miyata, and Y. Matsumoto. 1997. Maximum entropy parameter learning of subcategorization preference. (submitted to the 35th Annual Meeting of ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>