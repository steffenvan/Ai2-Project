<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005140">
<title confidence="0.9990485">
Towards a Workbench for Acquisition of
Domain Knowledge from Natural Language
</title>
<author confidence="0.999433">
Andrei Mikheev and Steven Finch
</author>
<affiliation confidence="0.949157">
HCRC Language Technology Group
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK
</affiliation>
<email confidence="0.954822">
E-mail: Andrei.MikheevOed.ac.uk
</email>
<sectionHeader confidence="0.981193" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997138">
In this paper we describe an architecture
and functionality of main components of
a workbench for an acquisition of do-
main knowledge from large text corpora.
The workbench supports an incremental
process of corpus analysis starting from
a rough automatic extraction and or-
ganization of lexico-semantic regularities
and ending with a computer supported
analysis of extracted data and a semi-
automatic refinement of obtained hypo-
theses. For doing this the workbench em-
ploys methods from computational lin-
guistics, information retrieval and kno-
wledge engineering. Although the work-
bench is currently under implementation
some of its components are already im-
plemented and their performance is il-
lustrated with samples from engineering
for a medical domain.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999084">
One of the standard methods for the extraction
of domain knowledge (or domain schema in ano-
ther terminology) from texts is known as Distribu-
tional Analysis (Hirshman 1986). It is based on
the identification of the sublanguage specific co-
occurrence properties of words in the syntactic re-
lations in which they occur in the texts. These co-
occurrence properties indicate important seman-
tic characteristics of the domain: classes of ob-
jects and their hierarchical inclusion, properties
of these classes, relations among them, lexico-
semantic patterns for referring to certain concep-
tual propositions, etc. This knowledge about do-
main in the form it is extracted is not quite sui-
table to be included into the knowledge base and
require a post-processing of the linguistically trai-
ned knowledge engineer. This is known as a con-
ceptual analysis of the acquired lingistic data. In
general all this is a time consuming process and
often requires the help of a domain expert. Ho-
wever, it seems to be possible to automate some
tasks and facilitate human intervention in many
parts using a combination of NLP and statistical
techniques for data extraction, type oriented pat-
terns for conceptual characterization of this data
and an intuitive user interface.
All these resources are to be put together into
a Knowledge Acquisition Workbench (KAWB)
which is under development at LTG of the Uni-
versity of Edinburgh. The workbench supports
an incremental process of corpus analysis star-
ting from a rough automatic extraction and orga-
nization of lexico-semantic regularities and ending
with a computer supported analysis of extracted
data and a refinement of obtained hypotheses.
</bodyText>
<sectionHeader confidence="0.965033" genericHeader="introduction">
2 KAW Architecture
</sectionHeader>
<bodyText confidence="0.999711">
The workbench we are aiming at integrates com-
putational tools and a user interface to support
phases of data extraction, data analysis and hypo-
theses refinement. The target domain description
consists of words grouped into domain-specific se-
mantic categories which can be further refined
into a conceptual type lattice (CTL) and lexico-
semantic patterns further refined into conceptual
structures as shown elsewhere in the paper. KAW
architecture is displayed in figure 1.
A data extraction module provides the kno-
wledge engineer with manageable units of lexi-
cal data (words, phrases etc.) grouped together
according to certain semantically important pro-
perties. The data extraction phase can be subdivi-
ded into a stage of semantic category identification
and a stage of lexico-semantic pattern extraction.
Both of these stages complement each other: the
discovery of semantic categories allows the system
to look for patterns and discovered patterns serve
as diagnostic units for further extraction of these
categories. Thus both these activities can be ap-
plied iteratively until a certain level of precision
and coverage is achieved.
</bodyText>
<page confidence="0.995829">
194
</page>
<figure confidence="0.856052352941177">
Data Extraction Module
Word Class Identifier Lexical Pattern Finder
Term
clustering
tool
Clusters
refinement
tool
External
sources
access
Thesaurus
Collocation
identification
tool
Cluster Refinement
tool
Generalisation
tool
Corpus Module 7:1
zr?
a
Fuzzy
matcher
Target data structures
Semantic Lexico-
categories Semantic
patterns
Concept Concept
Type Struct
Lattice
case attachement Analysis
support
robust parser
</figure>
<bodyText confidence="0.738447666666667">
These data structures are initally
produced by the procedures
described above, and refined by
the analysis/refinement tool to
become a conceptual type lattice
and a set of frames.
tagger
Analysis/Refinement
Linguistic analysis tools
</bodyText>
<figureCaption confidence="0.980905">
Figure 1: This figure shows main KAWB components and modules and SGML marked data flow between
them.
</figureCaption>
<page confidence="0.997864">
195
</page>
<bodyText confidence="0.999741314814815">
The word class identification component en-
compasses tools for the linguistic annotation of
texts, word clustering tools and tools for access
to external linguistic and semantic sources like
thesauri, machine-readable dictionaries and lexi-
cal data bases. Statistical clustering can be au-
tomatically checked and subcategorized with the
help of external linguistic and semantic sources.
The pattern finder component makes use of
phrasal annotations of texts produced by a ge-
neral robust partial parser. First, the corpus is
checked for stable phrasal collocations for single
words and entire semantic clusters by a special
tool - a collocator. After collocations are collec-
ted another tool - a generalizer tries automatically
deduce regularities and contract multiple patterns
into their general representations. Such patterns
are then presented for a conceptual characteriza-
tion to the knowledge engineer and some prede-
fined generic conceptual structures are suggested
for specialization.
The main aim of the analysis and refinement
module is to uncover and refine structural gene-
ralities found in the previous phases. It matches
in the text patterns which represent hypotheses
of the knowledge engineer, groups together and
generalizes the found cases and presents them to
the knowledge engineer for a final decision. The
matcher evaluates how good a given piece of text
matches the pattern and returns matches at va-
rious levels of exactness.
If modules are to communicate flexibly then an
inter-module information representation format
needs to be specified. Standard Generalized Mar-
kup Language (Goldfab 1990) is an international
standard for marking up text. We use SGML as
a way of exchanging information between modules
in a knowledge acquisition system, and of storing
that information in persistent store when it has
been processed.
In the rest of the paper we will embark on a more
detailed characterization of tools themselves. We,
however, will not present any technical details and
suggestions on an actual implementation because
the workbench should be able to incorporate dif-
ferent implementations . Some of the tools are
already implemented while others still need im-
plementation or reimplementation in terms of the
open architecture of the workbench. For an illu-
stration we have used samples from engineering
for the cardiac-failure domain using OHSUMED
(Hersh 1994) corpus and a corpus of patient di-
scharge summaries ( PDS ) described in Mikheev
1994.
</bodyText>
<sectionHeader confidence="0.986125" genericHeader="method">
3 Linguistic Annotation
</sectionHeader>
<bodyText confidence="0.999972244444445">
The simplest form of linguistic description of the
content of a machine-readable document is in the
form of a sequence (or a set) of words. More so-
phisticated linguistic information comes in several
forms, all of which may need to be represented if
performance in an automatic acquisition of lexical
regularities is to be improved. The NLP module
of the KAWB consists of a word tagger (e.g. Ku-
piec 1993), a specialized partial robust parser and
a case attachment module.
The tagger assigns categorial features to words.
This is not a straightforward process due to the
general lexical ambiguity of any natural langu-
age but state-of-the-art taggers do this quite well
(more than 97% correctness) using different stra-
tegies usually based on an application of Hidden
Markov Models (HMMs).
It is well-known that a general text parsing is very
fragile and ambiguous by its nature. Syntactic
ambiguity can lead to hundreds of parses even for
fairly simple sentences. This is clearly inappro-
priate. However, general and full scale parsing
is not required for many tasks of knowledge ac-
quisition but rather a robust identification of cer-
tain text segments is needed. Among these seg-
ments are compound noun phrases, verb phrases
etc. To increase a precision of knowledge extrac-
tion in some cases it is quite important to resolve
references of pronominal anaphora. At the mo-
ment in parsing sentences we are using a temporal
expressions recognizer, a noun-phrase recognizer,
a simple verb-phrase recognizer and a simple ana-
phoric binder. This can be further extended to
treat other phenomena of natural language, pro-
viding that new components are robust and fast.
The parser supplies information to a case atta-
chement module. This module using semantically
driven role filler expectations for verbs provides a
more precise attachment of noun phrases to verbs.
To do this we are using ESK - an event and state
knowledge base (Whittemore 94) which for more
that 700 verbs contains information on thematic
roles, semantic types for arguments, expected ad-
juncts, syntactic information, propositional types,
WordNet concept types and sense indices.
</bodyText>
<sectionHeader confidence="0.995464" genericHeader="method">
4 Automatic Precategorization
</sectionHeader>
<bodyText confidence="0.996428375">
Semantic clustering of words from an underlying
corpora allows the knowledge engineer to find out
main semantic categories or types which exist in
the domain in question and sort out the lexicon
in accordance with these types. It is important
both that information about typology the know-
ledge engineer adds to the system is accurate, and
that enough information is added. In this regard,
</bodyText>
<page confidence="0.994917">
196
</page>
<bodyText confidence="0.999921763157894">
the Zipf-Mandelbrot law, which states that the
frequency of the nth most frequent word in a na-
tural language is (roughly) inversely proportional
to n. Thus the majority of word tokens appear in
a small fraction of the possible word types.
Finch &amp; Chater (1991) show how it is possible to
infer a syntactic and semantic classification of a
set of words by analyzing how they are used in
a very large corpus. This is useful because very
large corpora frequently exist for many domains.
For example, in the medical domain, the freely
available OHSUMED corpus (Hersh 1994) contains
some 40 million words of medical texts. We now
describe this method for inferring a syntactic and
semantic classification of words from scratch.
Firstly, we measure the contexts in which words
w E W occur, and define a statistically motiva-
ted similarity measure between contexts of occur-
rence of words to infer a similarity between words,
d(wi , w2), w1, W2 E W. In our case the context is
defined to be a vector of word bigram statistics
across the corpus for one and two words to the
left and right, thus representing each word to be
classified by a vector of bigram statistics. Then
we apply a classification procedure to produce a.
hierarchical single link clustering (or dendrogram)
(Sokal &amp; Sneath, 1963) of words which we use as
a basis for further classification. If this technique
(as more fully described in Finch 1993) is applied
to the OHSUMED corpus, some of the structure
which is uncovered is displayed in figure 2. This
figure displays part of a 3,000 word dendrogram
which can then be &amp;quot;cut&amp;quot; at an appropriate level to
form a set of disjoint classes which can then be or-
dered according to their frequency of occurrence.
This gives the knowledge engineer a means to
quickly and relatively accurately classify the most
frequent vocabulary used in a particular domain.
</bodyText>
<sectionHeader confidence="0.9990835" genericHeader="method">
5 Lexico-Semantic Pattren
Acquisition
</sectionHeader>
<bodyText confidence="0.999786892857143">
Lexico-semantic patterns are structures where lin-
guistic entries, semantic types and entire lexico-
semantic patterns can be used in combinations
to denote certain conceptual propositions of the
underlying domain and cover certain sequences of
words in the text. Linguistic entries can be words,
phrases and linguistic types, for example: &amp;quot;of&amp;quot; -
word, &lt;NP head = &amp;quot;infarction&amp;quot; &gt; - a noun phrase
with the head-word &amp;quot;infarction&amp;quot;, &lt;SYNT type =
N&gt; - a noun etc. Patterns themselves are the ba-
sis for induction of conceptual structures.
An example of a correspondence of many phra-
ses to one lexico-semantic pattern is shown in fi-
gure 3. This pattern covers all strings which have
a reference to a person followed by one of the li-
sted verbs in any form followed by a compound
noun with the head &amp;quot;infarction&amp;quot; and followed by
a date expression. In this pattern PERSON and
MATE are patterns themselves and all other con-
stituents are linguistic entries. If instead of &amp;quot;inf-
arction&amp;quot; we use a type [DISEASE] we can achieve
even broader coverage. Also note that the MATE
constituent is optional which is expressed by &amp;quot;?&amp;quot;.
A conceptual structure which corresponds to the
pattern adds more implicit information to the pat-
tern. For instance, it states explicitly that a body
component which is a location of a disease belongs
to the person who is an experiencer of that disease:
</bodyText>
<figure confidence="0.9597756">
[Oinfarction:V]
-4(is-a)-+[©disease]
-4(expr)-4[Operson:y]
-4(loc)-4[Obody-COM114—(has)4—[©personr*y]
-4(cul)-4[©time-point]
</figure>
<bodyText confidence="0.993134857142857">
From the NL Processing point of view lexico-
semantic patterns provide a way for going about
without the definition of a general semantics for
every word in the corpus. Many commonsense
words take their particular meaning only in a con-
text of domain categories and this can be expres-
sed by means of lexico-semantic patterns.
</bodyText>
<subsectionHeader confidence="0.896068">
5.1 Collocator
</subsectionHeader>
<bodyText confidence="0.997908708333333">
The collocator or multi-word term extraction mo-
dule finds in the corpus significant co-occurrence
of lexical items (words and phrases) which con-
stitute terminology. Identified by the robust par-
tial parser noun and verb groups which include
domain semantic categories elicited at the preca-
tegorization phase are collected together with fre-
quencies of their appearance in the corpus. Phra-
ses are filtered through a list of general purpose
words which is constructed separately for every
new domain. Phrases which occur more often
than a threshold computed using Zipf-Mandelbrot
law are saved for post-analysis. Other phrases are
decomposed into constituents for recalculation of
saved phrase weights as described in Mikheev 91.
Many terms include other terms as their compo-
nents. This surface lexical structure corresponds
to semantic relations between concepts represen-
ted by these terms. To uncover term inclusion
the system scans the term bank and replaces each
entry of a term which currently in focus with its
number. Figure 4 displays an excerpt from collo-
cations extracted from PDS corpus in the original
form and after term inclusion checking.
</bodyText>
<subsectionHeader confidence="0.819963">
5.2 Inner Context Categorization
</subsectionHeader>
<bodyText confidence="0.9749265">
The major past of the terminology is usually re-
presented by nouns or nominalizations. Such
</bodyText>
<page confidence="0.973042">
197
</page>
<figure confidence="0.999865766666667">
morphine disease
indomethacin syndrome
epinephrine disorder
propranolol dysfunction
nifedipine infection
verapamil failure
diltiazem injury
halothane infarction
isoflurane obstruction
bupivacaine fibrosis
fentanyl trauma
lidocaine illness
dexamethasone deficiency
amiodarone sclerosis
methotrexate mellitus
-11
renal acute
pulmonary chronic
cardiac primary
myocardial long-term
cerebral new
ventricular major
coronary multiple
aortic various
vascular single
gastric small
arterial large
venous early
respiratory late
gastrointestinal
</figure>
<figureCaption confidence="0.996926">
Figure 2: This figure shows four sub-clusters of our hierarchical cluster analysis of the 3,000 most
</figureCaption>
<bodyText confidence="0.883784125">
frequent words in the OHSUMED corpus (Hersh 1994). It shows a subcluster of drugs (top left), disease-
based nouns (top right), body-part adjectives (lower left), and condition modifying adjectives (lower
right).
He had suffered an acute myocardial infarction in 1992.
He had a true posterior myocardial infarction on 5th of November 1992...
She had had an interior infarction in 1985.
Mr.Mcdool sustained a small anterior myocardial infarction in October 92.
She developed an extensive myocardial infarction
</bodyText>
<figureCaption confidence="0.9531065">
$PERSON &lt;V head = {suffer, have, sustain, develop} &gt; &lt;NC head = &amp;quot;infarction&amp;quot;&gt; {&amp;quot;on&amp;quot;, &amp;quot;in&amp;quot;} MATE?
Figure 3: This figure shows a correspondence of many phrases to one lexico-semantic pattern.
</figureCaption>
<table confidence="0.9948552">
Num Freq Annotated Phrase
$136 373 myocardial//BODY-PART infarction//DISEASE
$234 475 anterior myocardial//BODY-PART infarction//DISEASE
$467 550 inferior myocardial//BODY-PART infarction//DISEASE
$1109 17 established inferior myocardial//BODY-PART infarction//DISEASE
$1154 48 history//INFORMATION of ischaemic heart//BODY-PART disease//DISEASE
$2574 21 history//INFORMATION of an anterior myocardial//BODY-PART infarction//DISEASE
$2974 23 moderately severe stenosis//DISEASE
$2980 46 aortic//BODY-PART valve//BODY-PART stenosis//DISEASE
$3004 79 stenosis//DISEASE in the right coronary//BODY-PART artery//BODY-PART
</table>
<figureCaption confidence="0.9877">
Figure 4: This figure shows an excerpt from collocations extracted from PDS corpus and the result of
term inclusion checking.
</figureCaption>
<page confidence="0.996312">
198
</page>
<bodyText confidence="0.99451525">
terms usually have a particular set of modifiers
which represent different properties. The inner
context categorization is started with extraction
of compound nouns from collected by the colloca-
tor noun phrases. Semantic categories for many
adjectieval modifiers extracted at the word clu-
stering phase are too general if any, but collected
collocations and external lexical sources as, for ex-
ample, WordNet can be used.
First, we can sort terms with the same head-word
by length. For example, for the type INFARCTION
the systems sorts terms as follows:
myocardial infarction, old infarction, acute infarction
acute myocardial infarction, anterior myocardial inf-
arction...
further anterior myocardial infarction...
Then we separate pure adjectival modifiers from
adjectivized nouns:
infarction : inferior, old, acute, post, further, antero-
lateral, lateral, infero-posterior, antero-septal, repea-
ted, significant, large, limited // myocardial, dia-
phragmatic, subendocardial
myocardial infarction : anterior, first, extensive,
minor, small, previous, posterior, suspected.
Next we cluster pure adjectival modifiers into
groups using synonym-antonym information avai-
lable in WordNet. However, it is not necessarily
the case that related adjectives are stated together
in one WordNet entry. Sometimes there is an in-
direct link between adjectives. Also, since quite
often WordNet gives semantically unrelated (in a
given domain) adjectives together we use a heu-
ristic rule which says that if two adjectives are
used together in one phrase they don&apos;t hold syno-
nymy jantonyrny relation.
The system assumes that if there is at least one
word in common in WordNet entries for two dif-
ferent adjectives they can be clustered together.
In our example for the type INFARCTION the fol-
lowing clusters were automatically obtained:
</bodyText>
<figureCaption confidence="0.944678428571429">
cluster 1: chronic vs. acute;
cluster 2: major, extensive, significant, large, old vs.
minor, small, limited;
cluster 3: post vs. previous, ensuing;
cluster 4: anterior vs. posterior;
cluster 5: inferior vs. superior;
rest: suspected; lateral; recent; further; repeated;
</figureCaption>
<bodyText confidence="0.9999106">
As we see all clusters look fairly plausible except
the single adjective &amp;quot;old&amp;quot; which was misclassified;
it stands for a temporal property of an infarction
rather than its spreading at a myocardium.
This algorithm is gradually applied to all entries
from the term bank and the knowledge engineer is
presented with the results. This method was fairly
successfully used in our experiment, however, a
large-scale evaluation of sense discrimination for
constituent words is still needed to be done.
</bodyText>
<subsectionHeader confidence="0.987647">
5.3 Outer Context Generalizer
</subsectionHeader>
<bodyText confidence="0.999965695652174">
The lexico-semantic generalizer is a tool which
extracts general lexico-semantic patterns in an
empirical, corpus-sensitive manner analogous to
that used to automatically extract word class
dendrograms. From the multi-word term bank
collected by the collocation tool, we derive se-
mantic frames by replacing each content word in
each phrase by its semantic category, derived eit-
her empirically from the word-level dendrogram
in the case of frequent words, or derived from
WordNet in the case of less frequent words (as
described above). We also part of speech tag
every word in the phrase. Therefore, the term
&amp;quot;myocardial infarction&amp;quot; might become &amp;quot;BODY-
PART&lt;adj&gt; DISEASE&lt;noun/s&gt;&amp;quot; , as might &amp;quot;ga-
strointestinal obstruction&amp;quot; or &amp;quot;respiratory fai-
lure&amp;quot;. Another example might be the assignment
of &amp;quot;DISEASE&lt;noun/s&gt; of BODYPART&lt;nounipl&gt;&amp;quot;
to &amp;quot;obstruction of arteries&amp;quot; (function words such
as &amp;quot;of&amp;quot; are usually not further subcategorized,
since they convey structural information in them-
selves). Thus we map the term bank to a set of
paradigms, and we choose the set of paradigms
which appear most frequently for clustering.
Clustering proceeds by mapping words in the cor-
pus to their semantic category (augmented with
part-of-speech information), and clustering in the
same way as we did for words, except that the
context vectors are recorded for the set of frequent
semantic paradigms. For infrequent words where
the empirical method for finding semantic class
can&apos;t be applied, the WordNet technique descri-
bed above is used. When this is done, we get a
clustering of short lexico-semantic paradigms .
Once this is achieved, we can again apply the same
methodology to find patterns of higher level which
include patterns themselves. In our notation, we
refer to singe word semantic categories as upper-
case labels (which we choose as being descriptive
of the class which has been discovered), simple se-
quences of semantic categories by a preceding &amp;quot;$&amp;quot;,
and a sequence of sequences by a preceding &amp;quot;$S&amp;quot; .
These higher level patterns can be clustered in the
same way to yield longer semantic sequence para-
digms. Figure 5 illustrates generalizations for the
types $BODY-PART and $$DISEASE.
</bodyText>
<subsectionHeader confidence="0.985995">
5.4 Analysis Support Tool
</subsectionHeader>
<bodyText confidence="0.9963995">
Type oriented analysis is facilitated with generic
conceptual structures which are different for diffe-
</bodyText>
<page confidence="0.994463">
199
</page>
<table confidence="0.997479555555556">
Pattern Structure for $BODY-PART Examples
BODY-PART&lt; adj &gt; BODY-PART&lt; noun/s&gt; aortic valve
LOCATION&lt; adj &gt; BODY-PART&lt; noun/s&gt; left heart
LOCATION&lt; adj &gt; LOCATION&lt; adj &gt; BODY-PART&lt; noun/ s &gt; left descending artery
Pattern Structure for $$D1SEASE Examples
$BODY-PART DISEASE&lt; noun/s&gt; antero-septal myocardial infarction
DISEASE&lt; noun/s&gt; &amp;quot;in&amp;quot; $DATE infarction in December 1987
$BODY-PART DISEASE&lt; noun/s&gt; &amp;quot;in&amp;quot; $DATE myocardial infarction in December 1987
DISEASE&lt; noun/s&gt; &amp;quot;of&amp;quot; $BODY-PART occlusion of artery
</table>
<figureCaption confidence="0.996724">
Figure 5: This figure shows results of generalization for the types $BODY-PART and UDISEASE.
</figureCaption>
<bodyText confidence="0.9999846">
rent conceptual types (as more fully described in
Mikheev &amp; Moens 1994). For example, a type ori-
ented structure for eventualities includes their the-
matic roles (agent, theme ...), temporal links and
properties while a type-oriented structure for ob-
jects includes their components, parts, areas and
properties. The system recognizes which structure
should be used and presents it to the knowledge
engineer with optional explanations or a question
guided strategy for filling it up.
</bodyText>
<sectionHeader confidence="0.996189" genericHeader="method">
6 Hypotheses Refinement
</sectionHeader>
<bodyText confidence="0.973829857142857">
A fuzzy matcher is a tool which uses a sophisti-
cated pattern-matching language to extract text
fragments at various levels of exactness. It mat-
ches in the text patterns which represent hypo-
theses of the knowledge engineer, groups together
and generalizes cases which have been discovered
and presents them to the knowledge engineer for
a final decision.
Patterns themselves can be quite complex con-
structions which can include strings, words, ty-
pes, precedence relations and distance specifiers.
In the simpliest case the knowledge engineer can
examine a context for occurrences for a word or
a type provided that the type exists in the term
bank as represented in figure 6.
More complex patterns can be used for the
description of complex groups. For instance,
there a request can be made to find all co-
occurrences of the type DISEASE with the type
BODY-COMPONENT when they are at the same
structural group (noun phrase or verb phrase) and
the disease is a head of the group:
f[disease].&lt;&gt;[body-component]}curly brackets impose a context of a structural
group, the &amp;quot;.&amp;quot; means that the words can be dis-
tributed in the group, &lt;&gt; means that the compo-
nent can be both to the left and to the right, and
since the DISEASE is the first element of the pat-
tern it is assumed to be the head. The program
matches this pattern into the following entries:
myocardial infarction, infarction of myocardium, ste-
nosis at the origin of left coronary artery...
To be powerful enough for our purposes this pat-
tern language should be quite complex and it is
important to provide an easy way for specification
of such patterns with a question-guided process.
</bodyText>
<sectionHeader confidence="0.995904" genericHeader="method">
7 External Sources Access
</sectionHeader>
<bodyText confidence="0.99996425">
Already existing lexical databases are an im-
portant source of information about constituent
words of domain texts. KAWB provides generic
facilities for access to such linguistic sources. For
each source a converter which transforms source
information into SGML marked data, which then
can be used in the workbench, should be written.
For some domains there already exist terminolo-
gical banks available on-line. These banks vary
in their linguistic coverage - some list all possi-
ble forms (singular, plural etc.) for terms while
others just a canonical one, and in a conceptual
coverage - some provide an extensive set of diffe-
rent relations among terms (concepts) others just
a subsumption hierarchical inclusion. In our
implementation we used Unified Medical Langu-
age System (UMLS) and WordNet (Beckwith et al
1990) - a publicly available lexical database, ho-
wever we haven&apos;t provided the generic support for
an abstract thesaurus yet.
</bodyText>
<sectionHeader confidence="0.994433" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998574727272727">
The workbench outlined in this paper encompas-
ses a number of tools which facilitate different sta-
ges of knowledge extraction, analysis and refine-
ment based on corpus processing paradigm. These
tools are integrated into a coherent workbench
with a common inter-module data flow interface
based on SGML. Thus the workbench can easily
integrate new tools and upgrade existing ones.
The general approach to knowledge acquisition
supported by the workbench is a combination of
methods used in knowledge engineering, informa-
</bodyText>
<page confidence="0.972439">
200
</page>
<bodyText confidence="0.921782230769231">
developed an anterior myocardial
an established inferior myocardial
an acute inferior myocardial
subsequent episodes of unstable
he has experienced unstable
infarction from which
infarction . The
infarction with CHB
angina including an
angina and was
Figure 6: This figure shows an excerpt from a search for the type DISEASE with a distance four to the
left and two to the right.
tion retrieval and computational linguistics.
</bodyText>
<sectionHeader confidence="0.997381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99892929787234">
Beckwith, R., C. Fellbaum, D. Gross and G.
A. Miller (1990) WordNet: A lexical data-
base organized on psycholinguistic principles.
CSL Report 42, Cognitive Science Labora-
tory, Princeton University, Princeton.
Cutting, D., J. Kupiec, J. Pedersen and P. Sibun
(1993) Beta test version of the Xerox tagger.
Xerox Palo Alto Reseach Center, Palo Alto,
Ca.
Finch, S. and N. Chater (1991) A hybrid approch
to learning syntactic categories. AISB Quart-
erly 8(4), 35-41.
Finch, S. P. (1993) Finding Structure in Langu-
age. PhD thesis, Centre for Cognitive Science,
University of Edinburgh, Edinburgh.
Goldfarb, C. F. (1990) The SGML Handbook. Ox-
ford: Clarendon Press.
Health, U S. D.of (1993) UMLS Knowledge Sour-
ces. Washington: National Library of Medi-
cine.
Hersh, W. (1994) An interactive retrieval evalua-
tion and a new large test collection for rese-
arch. In W. B. Croft and C. J. van Rijsbergen,
eds., Proceedings of the 17th Annual Interna-
tional Conference onResearch and Develop-
ment in Information Retrieval, pp. 192-202.
Hirschman, L. (1986) Discovering sublanguage
structures. In R. Grishman and R. Kittredge,
eds., Analyzing Language in Restricted Do-
mains: Sublanguage Description and Proces-
sing, pp. 211-234. Hillsdale, N.J.: Lawrence
Erlbaum Associates.
Mikheev, A. and M. Moens (1994) Acquiring and
Representing Background Knowledge for a
NLP System. In Proceedings of the AAAI Fall
Symposium.
Mikheev, A. (1991) A cognitive system for con-
ceptual knowledge extraction from NL texts.
PhD thesis, Computer Science, Moscow Insti-
tute for Radio-Engineering and Automation,
Moscow.
Sokal, R. R. and P. H. A. Sneath (1963) Principles
of Numerical Taxonomy. San Pransisco: W.
H. Freeman.
Whittemore, G. and J. Hicks (1994) ESK: Event
and state knowledge base. In AA AI Fall Sym-
posium.
</reference>
<page confidence="0.998369">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967876">
<title confidence="0.9993585">Towards a Workbench for Acquisition of Domain Knowledge from Natural Language</title>
<author confidence="0.999877">Andrei Mikheev</author>
<author confidence="0.999877">Steven Finch</author>
<affiliation confidence="0.9995935">HCRC Language Technology Group University of Edinburgh</affiliation>
<address confidence="0.994079">2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK</address>
<email confidence="0.996402">E-mail:Andrei.MikheevOed.ac.uk</email>
<abstract confidence="0.998758285714286">In this paper we describe an architecture and functionality of main components of a workbench for an acquisition of domain knowledge from large text corpora. The workbench supports an incremental process of corpus analysis starting from a rough automatic extraction and organization of lexico-semantic regularities and ending with a computer supported analysis of extracted data and a semiautomatic refinement of obtained hypotheses. For doing this the workbench employs methods from computational linguistics, information retrieval and knowledge engineering. Although the workbench is currently under implementation some of its components are already implemented and their performance is illustrated with samples from engineering for a medical domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>G A Miller</author>
</authors>
<title>WordNet: A lexical database organized on psycholinguistic principles.</title>
<date>1990</date>
<tech>CSL Report 42,</tech>
<institution>Cognitive Science Laboratory, Princeton University, Princeton.</institution>
<contexts>
<context position="25219" citStr="Beckwith et al 1990" startWordPosition="3876" endWordPosition="3879">source a converter which transforms source information into SGML marked data, which then can be used in the workbench, should be written. For some domains there already exist terminological banks available on-line. These banks vary in their linguistic coverage - some list all possible forms (singular, plural etc.) for terms while others just a canonical one, and in a conceptual coverage - some provide an extensive set of different relations among terms (concepts) others just a subsumption hierarchical inclusion. In our implementation we used Unified Medical Language System (UMLS) and WordNet (Beckwith et al 1990) - a publicly available lexical database, however we haven&apos;t provided the generic support for an abstract thesaurus yet. 8 Conclusion The workbench outlined in this paper encompasses a number of tools which facilitate different stages of knowledge extraction, analysis and refinement based on corpus processing paradigm. These tools are integrated into a coherent workbench with a common inter-module data flow interface based on SGML. Thus the workbench can easily integrate new tools and upgrade existing ones. The general approach to knowledge acquisition supported by the workbench is a combinati</context>
</contexts>
<marker>Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Beckwith, R., C. Fellbaum, D. Gross and G. A. Miller (1990) WordNet: A lexical database organized on psycholinguistic principles. CSL Report 42, Cognitive Science Laboratory, Princeton University, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>Beta test version of the Xerox tagger. Xerox Palo Alto Reseach Center,</title>
<date>1993</date>
<location>Palo Alto, Ca.</location>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1993</marker>
<rawString>Cutting, D., J. Kupiec, J. Pedersen and P. Sibun (1993) Beta test version of the Xerox tagger. Xerox Palo Alto Reseach Center, Palo Alto, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
<author>N Chater</author>
</authors>
<title>A hybrid approch to learning syntactic categories.</title>
<date>1991</date>
<journal>AISB Quarterly</journal>
<volume>8</volume>
<issue>4</issue>
<pages>35--41</pages>
<contexts>
<context position="9903" citStr="Finch &amp; Chater (1991)" startWordPosition="1533" endWordPosition="1536">m an underlying corpora allows the knowledge engineer to find out main semantic categories or types which exist in the domain in question and sort out the lexicon in accordance with these types. It is important both that information about typology the knowledge engineer adds to the system is accurate, and that enough information is added. In this regard, 196 the Zipf-Mandelbrot law, which states that the frequency of the nth most frequent word in a natural language is (roughly) inversely proportional to n. Thus the majority of word tokens appear in a small fraction of the possible word types. Finch &amp; Chater (1991) show how it is possible to infer a syntactic and semantic classification of a set of words by analyzing how they are used in a very large corpus. This is useful because very large corpora frequently exist for many domains. For example, in the medical domain, the freely available OHSUMED corpus (Hersh 1994) contains some 40 million words of medical texts. We now describe this method for inferring a syntactic and semantic classification of words from scratch. Firstly, we measure the contexts in which words w E W occur, and define a statistically motivated similarity measure between contexts of </context>
</contexts>
<marker>Finch, Chater, 1991</marker>
<rawString>Finch, S. and N. Chater (1991) A hybrid approch to learning syntactic categories. AISB Quarterly 8(4), 35-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Finch</author>
</authors>
<title>Finding Structure in Language.</title>
<date>1993</date>
<tech>PhD thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh,</institution>
<location>Edinburgh.</location>
<contexts>
<context position="11050" citStr="Finch 1993" startWordPosition="1733" endWordPosition="1734"> statistically motivated similarity measure between contexts of occurrence of words to infer a similarity between words, d(wi , w2), w1, W2 E W. In our case the context is defined to be a vector of word bigram statistics across the corpus for one and two words to the left and right, thus representing each word to be classified by a vector of bigram statistics. Then we apply a classification procedure to produce a. hierarchical single link clustering (or dendrogram) (Sokal &amp; Sneath, 1963) of words which we use as a basis for further classification. If this technique (as more fully described in Finch 1993) is applied to the OHSUMED corpus, some of the structure which is uncovered is displayed in figure 2. This figure displays part of a 3,000 word dendrogram which can then be &amp;quot;cut&amp;quot; at an appropriate level to form a set of disjoint classes which can then be ordered according to their frequency of occurrence. This gives the knowledge engineer a means to quickly and relatively accurately classify the most frequent vocabulary used in a particular domain. 5 Lexico-Semantic Pattren Acquisition Lexico-semantic patterns are structures where linguistic entries, semantic types and entire lexicosemantic pa</context>
</contexts>
<marker>Finch, 1993</marker>
<rawString>Finch, S. P. (1993) Finding Structure in Language. PhD thesis, Centre for Cognitive Science, University of Edinburgh, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Goldfarb</author>
</authors>
<title>The SGML Handbook.</title>
<date>1990</date>
<publisher>Clarendon Press.</publisher>
<location>Oxford:</location>
<marker>Goldfarb, 1990</marker>
<rawString>Goldfarb, C. F. (1990) The SGML Handbook. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S D of Health</author>
</authors>
<title>UMLS Knowledge Sources.</title>
<date>1993</date>
<institution>National Library of Medicine.</institution>
<location>Washington:</location>
<marker>Health, 1993</marker>
<rawString>Health, U S. D.of (1993) UMLS Knowledge Sources. Washington: National Library of Medicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
</authors>
<title>An interactive retrieval evaluation and a new large test collection for research.</title>
<date>1994</date>
<booktitle>Proceedings of the 17th Annual International Conference onResearch and Development in Information Retrieval,</booktitle>
<pages>192--202</pages>
<editor>In W. B. Croft and C. J. van Rijsbergen, eds.,</editor>
<contexts>
<context position="6976" citStr="Hersh 1994" startWordPosition="1058" endWordPosition="1059">ring that information in persistent store when it has been processed. In the rest of the paper we will embark on a more detailed characterization of tools themselves. We, however, will not present any technical details and suggestions on an actual implementation because the workbench should be able to incorporate different implementations . Some of the tools are already implemented while others still need implementation or reimplementation in terms of the open architecture of the workbench. For an illustration we have used samples from engineering for the cardiac-failure domain using OHSUMED (Hersh 1994) corpus and a corpus of patient discharge summaries ( PDS ) described in Mikheev 1994. 3 Linguistic Annotation The simplest form of linguistic description of the content of a machine-readable document is in the form of a sequence (or a set) of words. More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic acquisition of lexical regularities is to be improved. The NLP module of the KAWB consists of a word tagger (e.g. Kupiec 1993), a specialized partial robust parser and a case attachment module. The tagger assigns</context>
<context position="10211" citStr="Hersh 1994" startWordPosition="1588" endWordPosition="1589"> information is added. In this regard, 196 the Zipf-Mandelbrot law, which states that the frequency of the nth most frequent word in a natural language is (roughly) inversely proportional to n. Thus the majority of word tokens appear in a small fraction of the possible word types. Finch &amp; Chater (1991) show how it is possible to infer a syntactic and semantic classification of a set of words by analyzing how they are used in a very large corpus. This is useful because very large corpora frequently exist for many domains. For example, in the medical domain, the freely available OHSUMED corpus (Hersh 1994) contains some 40 million words of medical texts. We now describe this method for inferring a syntactic and semantic classification of words from scratch. Firstly, we measure the contexts in which words w E W occur, and define a statistically motivated similarity measure between contexts of occurrence of words to infer a similarity between words, d(wi , w2), w1, W2 E W. In our case the context is defined to be a vector of word bigram statistics across the corpus for one and two words to the left and right, thus representing each word to be classified by a vector of bigram statistics. Then we a</context>
<context position="15325" citStr="Hersh 1994" startWordPosition="2387" endWordPosition="2388">ysfunction nifedipine infection verapamil failure diltiazem injury halothane infarction isoflurane obstruction bupivacaine fibrosis fentanyl trauma lidocaine illness dexamethasone deficiency amiodarone sclerosis methotrexate mellitus -11 renal acute pulmonary chronic cardiac primary myocardial long-term cerebral new ventricular major coronary multiple aortic various vascular single gastric small arterial large venous early respiratory late gastrointestinal Figure 2: This figure shows four sub-clusters of our hierarchical cluster analysis of the 3,000 most frequent words in the OHSUMED corpus (Hersh 1994). It shows a subcluster of drugs (top left), diseasebased nouns (top right), body-part adjectives (lower left), and condition modifying adjectives (lower right). He had suffered an acute myocardial infarction in 1992. He had a true posterior myocardial infarction on 5th of November 1992... She had had an interior infarction in 1985. Mr.Mcdool sustained a small anterior myocardial infarction in October 92. She developed an extensive myocardial infarction $PERSON &lt;V head = {suffer, have, sustain, develop} &gt; &lt;NC head = &amp;quot;infarction&amp;quot;&gt; {&amp;quot;on&amp;quot;, &amp;quot;in&amp;quot;} MATE? Figure 3: This figure shows a correspondence </context>
</contexts>
<marker>Hersh, 1994</marker>
<rawString>Hersh, W. (1994) An interactive retrieval evaluation and a new large test collection for research. In W. B. Croft and C. J. van Rijsbergen, eds., Proceedings of the 17th Annual International Conference onResearch and Development in Information Retrieval, pp. 192-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>Discovering sublanguage structures. In</title>
<date>1986</date>
<booktitle>Analyzing Language in Restricted Domains: Sublanguage Description and Processing,</booktitle>
<pages>211--234</pages>
<editor>R. Grishman and R. Kittredge, eds.,</editor>
<publisher>Lawrence Erlbaum Associates.</publisher>
<location>Hillsdale, N.J.:</location>
<marker>Hirschman, 1986</marker>
<rawString>Hirschman, L. (1986) Discovering sublanguage structures. In R. Grishman and R. Kittredge, eds., Analyzing Language in Restricted Domains: Sublanguage Description and Processing, pp. 211-234. Hillsdale, N.J.: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>M Moens</author>
</authors>
<title>Acquiring and Representing Background Knowledge for a NLP System.</title>
<date>1994</date>
<booktitle>In Proceedings of the AAAI Fall Symposium.</booktitle>
<contexts>
<context position="22306" citStr="Mikheev &amp; Moens 1994" startWordPosition="3401" endWordPosition="3404">ART&lt; adj &gt; BODY-PART&lt; noun/s&gt; aortic valve LOCATION&lt; adj &gt; BODY-PART&lt; noun/s&gt; left heart LOCATION&lt; adj &gt; LOCATION&lt; adj &gt; BODY-PART&lt; noun/ s &gt; left descending artery Pattern Structure for $$D1SEASE Examples $BODY-PART DISEASE&lt; noun/s&gt; antero-septal myocardial infarction DISEASE&lt; noun/s&gt; &amp;quot;in&amp;quot; $DATE infarction in December 1987 $BODY-PART DISEASE&lt; noun/s&gt; &amp;quot;in&amp;quot; $DATE myocardial infarction in December 1987 DISEASE&lt; noun/s&gt; &amp;quot;of&amp;quot; $BODY-PART occlusion of artery Figure 5: This figure shows results of generalization for the types $BODY-PART and UDISEASE. rent conceptual types (as more fully described in Mikheev &amp; Moens 1994). For example, a type oriented structure for eventualities includes their thematic roles (agent, theme ...), temporal links and properties while a type-oriented structure for objects includes their components, parts, areas and properties. The system recognizes which structure should be used and presents it to the knowledge engineer with optional explanations or a question guided strategy for filling it up. 6 Hypotheses Refinement A fuzzy matcher is a tool which uses a sophisticated pattern-matching language to extract text fragments at various levels of exactness. It matches in the text patter</context>
</contexts>
<marker>Mikheev, Moens, 1994</marker>
<rawString>Mikheev, A. and M. Moens (1994) Acquiring and Representing Background Knowledge for a NLP System. In Proceedings of the AAAI Fall Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>A cognitive system for conceptual knowledge extraction from NL texts.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>Computer Science, Moscow Institute for Radio-Engineering and Automation,</institution>
<location>Moscow.</location>
<marker>Mikheev, 1991</marker>
<rawString>Mikheev, A. (1991) A cognitive system for conceptual knowledge extraction from NL texts. PhD thesis, Computer Science, Moscow Institute for Radio-Engineering and Automation, Moscow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Sokal</author>
<author>P H A Sneath</author>
</authors>
<title>Principles of Numerical Taxonomy. San Pransisco:</title>
<date>1963</date>
<contexts>
<context position="10931" citStr="Sokal &amp; Sneath, 1963" startWordPosition="1710" endWordPosition="1713">ctic and semantic classification of words from scratch. Firstly, we measure the contexts in which words w E W occur, and define a statistically motivated similarity measure between contexts of occurrence of words to infer a similarity between words, d(wi , w2), w1, W2 E W. In our case the context is defined to be a vector of word bigram statistics across the corpus for one and two words to the left and right, thus representing each word to be classified by a vector of bigram statistics. Then we apply a classification procedure to produce a. hierarchical single link clustering (or dendrogram) (Sokal &amp; Sneath, 1963) of words which we use as a basis for further classification. If this technique (as more fully described in Finch 1993) is applied to the OHSUMED corpus, some of the structure which is uncovered is displayed in figure 2. This figure displays part of a 3,000 word dendrogram which can then be &amp;quot;cut&amp;quot; at an appropriate level to form a set of disjoint classes which can then be ordered according to their frequency of occurrence. This gives the knowledge engineer a means to quickly and relatively accurately classify the most frequent vocabulary used in a particular domain. 5 Lexico-Semantic Pattren Ac</context>
</contexts>
<marker>Sokal, Sneath, 1963</marker>
<rawString>Sokal, R. R. and P. H. A. Sneath (1963) Principles of Numerical Taxonomy. San Pransisco: W. H. Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>J Hicks</author>
</authors>
<title>ESK: Event and state knowledge base.</title>
<date>1994</date>
<booktitle>In AA AI Fall Symposium.</booktitle>
<marker>Whittemore, Hicks, 1994</marker>
<rawString>Whittemore, G. and J. Hicks (1994) ESK: Event and state knowledge base. In AA AI Fall Symposium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>