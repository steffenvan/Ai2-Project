<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.837164666666667">
Detecting problematic turns in human-machine interactions:
Rule-induction versus memory-based learning approaches
Antal van den Bosch
</title>
<author confidence="0.625486">
ILK / Comp. Ling.
</author>
<affiliation confidence="0.429016">
KUB, Tilburg
The Netherlands
</affiliation>
<email confidence="0.975167">
antalb@kub.nl
</email>
<note confidence="0.4115265">
Emiel Krahmer
IPO
</note>
<email confidence="0.171821">
TU/e, Eindhoven
</email>
<author confidence="0.251926">
The Netherlands
</author>
<email confidence="0.923452">
E.J.Krahmer@tue.nl
</email>
<note confidence="0.4581755">
Marc Swerts
CNTS
</note>
<address confidence="0.5155685">
UIA, Antwerp
Belgium
</address>
<email confidence="0.990922">
M.G.J.Swerts@tue.nl
</email>
<sectionHeader confidence="0.995456" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896888888889">
We address the issue of on-line detec-
tion of communication problems in
spoken dialogue systems. The useful-
ness is investigated of the sequence of
system question types and the word
graphs corresponding to the respective
user utterances. By applying both rule-
induction and memory-based learning
techniques to data obtained with a
Dutch train time-table information
system, the current paper demonstrates
that the aforementioned features indeed
lead to a method for problem detec-
tion that performs significantly above
baseline. The results are interesting
from a dialogue perspective since they
employ features that are present in the
majority of spoken dialogue systems
and can be obtained with little or no
computational overhead. The results
are interesting from a machine learning
perspective, since they show that the
rule-based method performs signific-
antly better than the memory-based
method, because the former is better
capable of representing interactions
between features.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963384615385">
Given the state of the art of current language and
speech technology, communication problems are
unavoidable in present-day spoken dialogue sys-
tems. The main source of these problems lies
in the imperfections of automatic speech recogni-
tion, but also incorrect interpretations by the nat-
ural language understanding module or wrong de-
fault assumptions by the dialogue manager are
likely to lead to confusion. If a spoken dialogue
system had the ability to detect communication
problems on-line and with high accuracy, it might
be able to correct certain errors or it could in-
teract with the user to solve them. For instance,
in the case of communication problems, it would
be beneficial to change from a relatively natural
dialogue strategy to a more constrained one in
order to resolve the problems (see e.g., Litman
and Pan 2000). Similarly, it has been shown that
users switch to a ‘marked’, hyperarticulate speak-
ing style after problems (e.g., Soltau and Waibel
1998), which itself is an important source of re-
cognition errors. This might be solved by using
two recognizers in parallel, one trained on nor-
mal speech and one on hyperarticulate speech. If
there are communication problems, then the sys-
tem could decide to focus on the recognition res-
ults delivered by the engine trained on hyperartic-
ulate speech.
For such approaches to work, however, it is
essential that the spoken dialogue system is able
to automatically detect communication problems
with a high accuracy. In this paper, we investigate
the usefulness for problem detection of the word
graph and the history of system question types.
These features are present in many spoken dia-
logue systems and do not require additional com-
putation, which makes this a very cheap method
to detect problems. We shall see that on the basis
of the previous and the current word graph and the
six most recent system question types, communic-
ation problems can be detected with an accuracy
of 91%, which is a significant improvement over
the relevant baseline. This shows that spoken dia-
logue systems may use these features to better pre-
dict whether the ongoing dialogue is problematic.
In addition, the current work is interesting from
a machine learning perspective. We apply two
machine learning techniques: the memory-based
IB1-IG algorithm (Aha et al. 1991, Daelemans et
al. 1997) and the RIPPER rule induction algorithm
(Cohen 1996). As we shall see, some interesting
differences between the two approaches arise.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.992998564705882">
Recently there has been an increased interest in
developing automatic methods to detect problem-
atic dialogue situations using machine learning
techniques. For instance, Litman et al. (1999)
and Walker et al. (2000a) use RIPPER (Cohen
1996) to classify problematic and unproblematic
dialogues. Following up on this, Walker et al.
(2000b) aim at detecting problems at the utter-
ance level, based on data obtained with AT&amp;Ts
How May I Help You (HMIHY) system (Gorin et
al.1997). Walker and co-workers apply RIPPER to
43 features which are automatically generated by
three modules of the HMIHY system, namely the
speech recognizer (ASR), the natural language un-
derstanding module (NLU) and the dialogue man-
ager (DM). The best result is obtained using all
features: communication problems are detected
with an accuracy of 86%, a precision of 83% and
a recall of 75%. It should be noted that the NLU
features play first fiddle among the set of all fea-
tures. In fact, using only the NLU features per-
forms comparable to using all features. Walker et
al. (2000b) also briefly compare the performance
of RIPPER with some other machine learning ap-
proaches, and show that it performs comparable
to a memory-based (instance-based) learning al-
gorithm (IB, see Aha et al. 1991).
The results which Walker and co-workers de-
scribe show that it is possible to automatically de-
tect communication problems in the HMIHY sys-
tem, using machine learning techniques. Their ap-
proach also raises a number of interesting follow-
up questions, some concerned with problem de-
tection, others with the use of machine learning
techniques. (1) Walker et al. train their classi-
fier on a large set of features, and show that the
set of features produced by the NLU module are
the most important ones. However, this leaves an
important general question unanswered, namely
which particular features contribute to what ex-
tent? (2) Moreover, the set of features which the
NLU module produces appear to be rather spe-
cific to the HMIHY system and indicate things like
the percentage of the input covered by the relev-
ant grammar fragment, the presence or absence of
context shifts, and the semantic diversity of sub-
sequent utterances. Many current day spoken dia-
logue systems do not have such a sophisticated
NLU module, and consequently it is unlikely that
they have access to these kinds of features. In
sum, it is uncertain whether other spoken dialogue
systems can benefit from the findings described by
Walker et al. (2000b), since it is unclear which fea-
tures are important and to what extent these fea-
tures are available in other spoken dialogue sys-
tems. Finally, (3) we agree with Walker et al. (and
the machine learning community at large) that it is
important to compare different machine learning
techniques to find out which techniques perform
well for which kinds of tasks. Walker et al. found
that RIPPER does not perform significantly better
or worse than a memory-based learning technique.
Is this incidental or does it reflect a general prop-
erty of the problem detection task?
The current paper uses a similar methodology
for on-line problem detection as Walker et al.
(2000b), but (1) we take a bottom-up approach,
focussing on a small number of features and in-
vestigating their usefulness on a per-feature basis
and (2) the features which we study are automat-
ically available in the majority of current spoken
dialogue system: the sequence of system ques-
tion types and the word graphs corresponding to
the respective user utterances. A word graph
is a lattice of word hypotheses, and we conjec-
ture that various features which have been shown
to cue communication problems (prosodic, lin-
guistic and ASR features, see e.g., Hirschberg et
al. 1999, Krahmer et al. 1999 and Swerts et al.
2000) have correlates in the word graph. The se-
quence of system question types is taken to model
the dialogue history. Finally, (3) to gain further in-
sight into the adequacy of various machine learn-
ing techniques for problem detection we use both
RIPPER and the memory-based IB1-IG algorithm.
</bodyText>
<sectionHeader confidence="0.994199" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.965906447058824">
3.1 Data and Labeling The corpus we used con-
sisted of 3739 question-answer pairs, taken from
444 complete dialogues. The dialogues consist
of users interacting with a Dutch spoken dialogue
system which provides information about train
time tables. The system prompts the user for un-
known slots, such as departure station, arrival sta-
tion, date, etc., in a series of questions. The sys-
tem uses a combination of implicit and explicit
verification strategies.
The data were annotated with a highly limited
set of labels. In particular, the kind of system
question and whether the reply of the user gave
rise to communication problems or not. The latter
feature is the one to be predicted. The following
labels are used for the system questions.
O open questions (“From where to where do you
want to travel?”)
I implicit verification (“When do you want to
travel from Tilburg to Schiphol Airport?”)
E explicit verification (“So you want to travel
from Tilburg to Schiphol Airport?”)
Y yes/no question (“Do you want me to repeat the
connection?”)
M Meta-questions (“Can you please correct
me?”)
The difference between an explicit verification
and a yes/no question is that the former but not
the latter is aimed at checking whether what the
system understood or assumed corresponds with
what the user wants. If the current system ques-
tion is a repetition of the previous question it
asked, this is indicated by the suffix R. A ques-
tion only counts as a repetition when it has the
same contents as the previous system question. Of
the user inputs, we only labeled whether they gave
rise to a communication problem or not. A com-
munication problem arises when the value which
the system assigns to a particular slot (departure
station, date, etc.) does not coincide with the
value given for that particular slot by the user in
his or her most recent contribution to the dialogue
or when the system makes an incorrect default as-
sumption (e.g., the dialogue manager assumes that
the date slot should be filled with the current date,
i.e., that the user wants to travel today). Commu-
nication problems are generally easy to label since
the spoken dialogue system under consideration
here always provides direct feedback (via verific-
ation questions) about what it believes the user in-
tends. Consider the following exchange.
U: I want to go to Amsterdam.
S: So you want to go to Rotterdam?
As soon as the user hears the explicit verification
question of the system, it will be clear that his or
her last turn was misunderstood. The problem-
feature was labeled by two of the authors to
avoid labeling errors. Differences between the
two annotators were infrequent and could always
easily be resolved.
3.2 Baselines Of the 3739 user utterances
1564 gave rise to communication problems (an
error rate of 41.8%). The majority class is thus
formed by the unproblematic user utterances,
which form 58.2% of all user utterances. This
suggests that the baseline for predicting com-
munication problems is obtained by always
predicting that there are no communication prob-
lems. This strategy has an accuracy of 58.2%,
and a recall of 0% (all problems are missed).
The precision is not defined, and consequently
neither is the . This baseline is misleading,
however, when we are interested in predicting
whether the previous user utterance gave rise to
communication problems. There are cases when
the dialogue system is itself clearly aware of
communication problems. This is in particular
the case when the system repeats the question
(labeled with the suffix R) or when it asks a meta-
question (M). In the corpus under investigation
here this happens 1024 times. It would not be
For definitions of accuracy, precision and recall see e.g.,
Manning and Sch¨utze (1999:268-269).
Since 0 cases are selected, one would have to divide by
0 to determine precision for this baseline.
</bodyText>
<tableCaption confidence="0.7878485">
Throughout this paper we use the measure (van
Rijsbergen 1979:174) to combine precision and recall in a
single measure. By setting equal to 1, precision and recall
are given an equal weight, and the measure simplifies to
( = precision, = recall).
Table 1: Baselines
</tableCaption>
<table confidence="0.8458575">
baseline
acc (%) prec (%) rec (%)
majority-class
system-knows
58.2 0.4 — 0.0 —
85.6 0.4 100 65.5 79.1
</table>
<bodyText confidence="0.994929727272727">
very illuminating to develop an automatic error
detector which detects only those problems that
the system was already aware of. Therefore we
take the following as our base-line strategy for
predicting whether the previous user utterance
gave rise to problems, henceforth referred to as
the system-knows-baseline:
if the Q( ) is repetition or meta-question,
then predict user utterance -1 caused problems,
else predict user utterance -1 caused no problems.
This ‘strategy’ predicts problems with an ac-
curacy of 85.6% (1024 of the 1564 problems are
detected, thus 540 of 3739 decisions are wrong),
a precision of 100% (of 1024 predicted problems
1024 were indeed problematic), a recall of 65.5%
(1024 of the 1564 problems are predicted to be
problematic) and thus an of 79.1. This
is a sharp baseline, but for predicting whether
the previous user utterance caused problems or
not the system-knows-baseline is much more
informative and relevant than the majority-class-
baseline. Table 1 summarizes the baselines.
</bodyText>
<subsectionHeader confidence="0.994091">
3.3 Feature representations Question-answer
</subsectionHeader>
<bodyText confidence="0.999179405172414">
pairs were represented as feature vectors (or
patterns) of the following form. Six features were
reserved for the history of system questions asked
so far in the current dialogue (6Q). Of course, if
the system only asked 3 questions so far, only 3
types of system questions are stored in memory
and the remaining three features for system ques-
tion are not assigned a value. The representation
of the user’s answer is derived from the word
graph produced by the ASR module. It should
be kept in mind that in general the word graph is
much more complex than the recognized string.
The latter typically is the most plausible path
(e.g., on the basis of acoustic confidence scores)
in the word graph, which itself may contain many
other paths. Different systems determine the
plausibility of paths in the word graph in different
ways. Here, for the sake of generality, we abstract
over such differences and simply represent a word
graph as a Bag of Words (BoW), collecting all
words that occur in one of the paths, irrespective
of the associated acoustic confidence score. A
lexicon was derived of all the words and phrases
that occurred in the corpus. Each word graph is
represented as a sequence of bits, where the -th
bit is set to 1 if the -th word in the pre-derived
lexicon occurred at least once in the word graph
corresponding to the current user utterance and
0 otherwise. Finally, for each user utterance, a
feature is reserved for indicating whether it gave
rise to communication problems or not. This
latter feature is the one to be predicted.
There are basically two approaches for detect-
ing communication problems. One is to try to
decide on the basis of the current user utterance
whether it will be recognized and interpreted
correctly or not. The other approach uses the
current user utterance to determine whether
the processing of the previous user utterance
gave rise to communication problems. This
approach is based on the assumption that users
give feedback on communication problems when
they notice that the system misunderstood their
previous input. In this study, eight prediction
tasks have been defined: the first three are con-
cerned with predicting whether the current user
input will cause problems, and naturally, for
these three tasks, the majority-class-baseline is
the relevant one; the last five tasks are concerned
with predicting whether the previous user utter-
ance caused problems, and for these the sharp,
system-knows-baseline is the appropriate one.
The eight tasks are: (1) predict on the basis of the
(representation of the) current word graph BoW
whether the current user utterance (at time ) will
cause a communication problem, (2) predict on
the basis of the six most recent system question
types up to (6Q ), whether the current user
utterance will cause a communication problem,
(3) predict on the basis of both BoW and 6Q ,
whether the current user utterance will cause a
problem, (4) predict on the basis of the current
word graph BoW , whether the previous user ut-
terance, uttered at time -1, caused a problem, (5)
predict on the basis of the six most recent system
questions, whether the previous user utterance
caused a problem, (6) predict on the basis of BoW
and 6Q , whether the previous user utterance
caused a problem, (7) predict on the basis of the
two most recent word graphs, BoW -1 and BoW
, whether the previous user utterance caused a
problem, and finally (8) predict on the basis of
the two most recent word graphs, BoW -1 and
BoW , and the six most recent system question
types 6Q , whether the previous user utterance
caused a problem.
3.4 Learning techniques For the experiments we
used the rule-induction algorithm RIPPER (Cohen
1996) and the memory-based IB1-IG algorithm
(Aha et al. 1991, Daelemans et al. 1997).
RIPPER is a fast rule induction algorithm. It
starts with splitting the training set in two. On the
basis of one half, it induces rules in a straightfor-
ward way (roughly, by trying to maximize cov-
erage for each rule), with potential overfitting.
When the induced rules classify instances in the
other half below a certain threshold, they are not
stored. Rules are induced per class. By default
the ordering is from low-frequency classes to high
frequency ones, leaving the most frequent class as
the default rule, which is generally beneficial for
the size of the rule set.
The memory-based IB1-IG algorithm is one of
the primary memory-based learning algorithms.
Memory-based learning techniques can be char-
acterized by the fact that they store a representa-
tion of a set of training data in memory, and clas-
sify new instances by looking for the most sim-
ilar instances in memory. The most basic distance
function between two features is the overlap met-
ric in (1), where is the distance between
patterns and (both consisting of features)
and is the distance between the features. If
is the test-case, the measure determines which
group of cases in memory is the most sim-
ilar to . The most frequent value for the relevant
We used the TiMBL software package, version 3 (Daele-
mans et al. 2000) to run the IB1-IG experiments.
category in is the predicted value for . Usu-
ally, is set to 1. Since some features are more
important than others, a weighting function is
used. Here is the gain ratio measure. In sum,
the weighted distance between vectors and
of length is determined by the following equa-
tion, where gives a point-wise distance
between features which is 1 if and 0 oth-
</bodyText>
<equation confidence="0.7605485">
erwise.
(1)
</equation>
<bodyText confidence="0.999839">
Both learning techniques were used for the same
8 prediction tasks, and received exactly the same
feature vectors as input. All experiments were
performed using ten-fold cross-validation, which
yields errors margins in the predictions.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999971636363637">
First we look at the results obtained with the IB1-
IG algorithm (see Table 2). Consider the problem
of predicting whether the current user utterance
will cause problems. Either looking at the current
word graph (BoW ), at the six most recent sys-
tem questions (6Q ) or at both, leads to a signi-
ficant improvement with respect to the majority-
class-baseline. The best results are obtained with
only the system question types (although the dif-
ference with the results for the other two tasks is
not significant): a 63.7% accuracy and an
of 58.3. However, even though this is a signific-
ant improvement over the majority-class-baseline,
the accuracy is improved with only 5.5%.
Next consider the problem of predicting
whether the previous user utterance caused
communication problems (these are the five
remaining tasks). The best result is obtained
by taking the two most recent word graphs and
the six most recent system question types as
input. This yields an accuracy of 88.1%, which
is a significant improvement with respect to the
</bodyText>
<footnote confidence="0.661398888888889">
All checks for significance were performed with a one-
tailed test.
As an aside, we performed one experiment with the
words in the actual, transcribed user utterance at time in-
stead of BoW, where the task is to predict whether the cur-
rent user utterance would cause a communication problem.
This resulted in an accuracy of 64.2% (with a standard devi-
ation of 1.1%). This is not significantly better than the result
obtained with the BoW.
</footnote>
<tableCaption confidence="0.952966">
Table 3: RIPPER results (accuracy, precision, recall, and , with standard deviations) on the eight
</tableCaption>
<bodyText confidence="0.984841833333333">
prediction tasks. : this accuracy significantly improves the majority-class-baseline ( ). : this
accuracy significantly improves the system-knows-baseline ( ). : this accuracy result is sig-
nificantly better than the IB1-IG result given in Table 2 for this particular task, with .05. : this
accuracy result is significantly better than the IB1-IG result given in Table 2 for this particular task, with
.001. : this accuracy result is significantly better than the IB1-IG result given in Table 2 for this
particular task, with .01.
</bodyText>
<figure confidence="0.977099083333333">
input output
acc (%) prec (%) rec (%)
BoW problem
6Q problem
BoW + 6Q problem
BoW problem -1
6Q problem -1
BoW + 6Q problem -1
BoW -1 + BoW problem -1
BoW -1 + BoW + 6Q problem -1
65.1 2.4 58.3 3.4 59.8 4.2 58.9 2.0
65.9 2.1 58.9 3.5 60.7 4.8 59.7 3.2
66.0 2.3 64.8 2.6 50.3 3.1 56.5 1.1
63.2 2.5 60.3 5.5 36.1 5.5 44.8 4.6
83.4 1.6 99.8 0.4 60.4 3.1 75.2 2.4
90.0 2.1 93.2 1.7 82.5 4.5 87.5 2.6
76.7 2.6 74.7 3.6 66.0 5.7 69.9 3.8
91.1 1.1 92.6 2.0 85.7 2.9 89.0 1.5
input output
acc (%) prec (%) rec (%)
BoW problem
6Q problem
BoW + 6Q problem
BoW problem -1
6Q problem -1
BoW + 6Q problem -1
BoW -1 + BoW problem -1
BoW -1 + BoW + 6Q problem -1
63.2 4.1 57.1 5.0 49.6 3.8 53.0 3.8
63.7 2.3 56.1 3.4 60.8 5.0 58.3 3.6
63.5 2.0 57.5 2.8 49.1 3.3 52.8 1.9
61.9 2.3 55.1 2.6 48.8 1.9 51.7 1.2
82.4 2.0 85.6 3.8 69.6 3.7 76.6 3.5
87.3 1.1 85.5 2.8 83.9 1.3 84.7 1.3
73.5 1.7 69.8 3.8 64.6 2.3 67.0 2.3
88.1 1.1 91.1 2.4 79.3 3.1 84.8 2.0
</figure>
<tableCaption confidence="0.825985">
Table 2: IB1-IG results (accuracy, precision, recall, and , with standard deviations) on the eight
</tableCaption>
<bodyText confidence="0.976174583333333">
prediction tasks. : this accuracy significantly improves the majority-class-baseline ( ). : this
accuracy significantly improves the system-knows-baseline ( ).
the same task.
On the problem of predicting whether the previ-
ous user utterance caused a problem, RIPPER ob-
tains the best results by taking all features into ac-
count (that is: the two most recent bags of words
and the six system questions). This results in a
91.1% accuracy, which is a significant improve-
ment over the sharp system-knows-baseline. This
implies that 38% of the communication problems
which were not detected by the dialogue system
</bodyText>
<footnote confidence="0.996605666666667">
Notice that RIPPER sometimes performs below the
system-knows-baseline, even though the relevant feature (in
particular the type of the last system question) is present. In-
spection of the RIPPER rules obtained by training only on
6Q reveals that RIPPER learns a slightly suboptimal rule set,
thereby misclassifying 10 instances on average.
</footnote>
<bodyText confidence="0.9980134">
sharp, system-knows-baseline. In addition, the
of 84.8 is nearly 6 points higher than that
of the relevant, majority-class baseline.
The results obtained with RIPPER are shown in
Table 3. On the problem of predicting whether
the current user utterance will cause a problem,
RIPPER obtains the best results by taking as input
both the current word graph and the types of the
six most recent system questions, predicting prob-
lems with an accuracy of 66.0%. This is a signific-
ant improvement over the majority-class-baseline,
but the result is not significantly better than that
obtained with either the word graph or the system
questions in isolation. Interestingly, the result is
significantly better than the results for IB1-IG on
</bodyText>
<figure confidence="0.883686538461539">
1. if Q () = R, then problem. (939/2)
2. if Q () = I “naar” BoW (-1) “naar” BoW() “om” BoW () then problem. (135/16)
3. if “uur” BoW(-1) “om” BoW(-1) “uur” BoW() “om” BoW() then problem. (57/4)
4. if Q() = I Q(-3) = I “uur” BoW (-1) then problem. (13/2)
5. if “naar” BoW(-1) “vanuit” BoW () “van” BoW() then problem. (29/4)
6. if Q(-1) = I “uur” BoW (-1) “nee” BoW () then problem. (28/7)
7. if Q() = I “ik” BoW(-1) “van” BoW(-1) “van” BoW() then problem. (22/8)
8. if Q() = I “van” BoW (-1) “om” BoW(-1) then problem. (16/6)
9. if Q() = E “nee” BoW () then problem. (42/10)
10. if Q() = M BoW (-1) = then problem. (20/0)
11. if Q(-1) = O “ik” BoW () “niet” in BoW() then problem. (10/2)
12. if Q(-2) = I Q() = O “wil” BoW(-1) then problem. (8/0)
13. else no problem. (2114/245)
</figure>
<figureCaption confidence="0.83212">
Figure 1: RIPPER rule set for predicting whether user utterance -1 caused communication problems on
the basis of the Bags of Words for and -1, and the six most recent system questions. Based on the
</figureCaption>
<bodyText confidence="0.988877361111111">
entire data set. The question features are defined in section 2. The word “naar” is Dutch for to, “om”
for at, “uur” for hour, “van” for from, “vanuit” is slightly archaic variant of “van” (from), “ik” is Dutch
for I, “nee” for no, “niet” for not and “wil”, finally, for want. The ( / ) numbers at the end of each line
indicate how many correct ( ) and incorrect ( ) decisions were taken using this particular if ...then
...statement.
under investigation could be classified correctly
using features which were already present in the
system (word graphs and system question types).
Moreover, the is 89, which is 10 points
higher than the associated with the system-
knows baseline strategy. Notice also that this RIP-
PER result is significantly better than the IB1-IG
results for the same task.
To gain insight into the rules learned by RIP-
PER for the last task, we applied RIPPER to the
complete data set. The rules induced are dis-
played in Figure 1. RIPPER’s first rule is con-
cerned with repeated questions (compare with the
system-knows-baseline). One important property
of many other rules is that they explicitly com-
bine pieces of information from the three main
sources of information (the system questions, the
current word graph and the previous word graph).
Moreover, it is interesting to note that the words
which crop up in the RIPPER rules are primarily
function words. Another noteworthy feature of
the RIPPER rules is that they reflect certain prop-
erties which have been claimed to cue commu-
nication problems. For instance, Krahmer et al.
(1999), in their descriptive analysis of dialogue
problems, found that repeated material is often an
indication of problems, as is the use of a marked
vocabulary. The rules 2, 3 and 7 are examples
of the former cue, while the occurrence of the
somewhat archaic “vanuit” instead of the ordinary
“van” is an example of the latter.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999898227848102">
In this study we have looked at automatic meth-
ods for problem detection using simple features
which are available in the vast majority of spoken
dialogue systems, and require little or no com-
putational overhead. We have investigated two
approaches to problem detection. The first ap-
proach is aimed at testing whether a user utter-
ance, captured in a noisy word graph, and/or the
recent history of system utterances, would be pre-
dictive of whether the utterance itself would be
misrecognised. The results, which basically rep-
resents a signal quality test, show that problem-
atic cases could be discerned with an accuracy
of about 65%. Although this is somewhat above
the baseline of 58% decision accuracy when no
problems would be predicted, signalling recogni-
tion problems with word graph features and previ-
ous system question types as predictors is a hard
task. As other studies suggest (e.g., Hirschberg et
al. 1999), confidence scores and acoustic/prosodic
features could be of help.
The second approach tested whether the word
graph for the current user utterance and/or the re-
cent history of system question types could be
employed to predict whether the previous user
In the sense that it is not a perfect image of the users
input.
utterance caused communication problems. The
underlying assumption is that users will signal
problems as soon as they become aware of them
through the feedback provided by the system.
Thus, in a sense, this second approach represents a
noisy channel filtering task: the current utterance
has to be decoded as signalling a problem or not.
As the results show, this task can be performed at
a surprisingly high level: about 91% decision ac-
curacy (which is an error reduction of 38%), with
an of the problem category of 89. This res-
ult can only be obtained using a combination of
features; neither the word graph features in isola-
tion nor the system question types in isolation of-
fer enough predictive power to reach above the
sharp baseline of 86% accuracy and an on
the problem category of 79.
Keeping information sources isolated or
combining them directly influences the relative
performances of the memory-based IB1-IG
algorithm versus the RIPPER rule induction
algorithm. When features are of the same type,
accuracies of the memory-based and the rule-
induction systems do not differ significantly (with
one exception). In contrast, when features from
different sources (e.g., words in the word graph
and question type features) are combined, RIPPER
profits more than IB1-IG does, causing RIPPER to
perform significantly more accurately. The fea-
ture independence assumption of memory-based
learning appears to be the harming cause: by its
definition, IB1-IG does not give extra weight to
apparently relevant interactions of feature values
from different sources. In contrast, in nine out
of the twelve rules that RIPPER produces, word
graph features and system questions type features
are explicitly integrated as joint left-hand side
conditions.
The current results show that for on-line detec-
tion of communication problems at the utterance
level it is already beneficial to pay attention only
to the lexical information in the word graph and
the sequence of system question types, features
which are present in most spoken dialogue system
and which can be obtained with little or no com-
putational overhead. An approach to automatic
problem detection is potentially very useful for
spoken dialogue systems, since it gives a quantit-
ative criterion for, for instance, changing the dia-
logue strategy (initiative, verification) or speech
recognition engine (from one trained on normal
speech to one trained on hyperarticulate speech).
</bodyText>
<sectionHeader confidence="0.872909" genericHeader="references">
Bibliography
</sectionHeader>
<reference confidence="0.999793355555555">
Aha, D., Kibler, D., Albert, M. (1991), Instance-based
Learning Algorithms, Machine Learning, 6:36–66.
Cohen, W. (1996), Learning trees and rules with set-valued
features, Proc. 13th AAAI.
Daelemans, W., van den Bosch, A., Weijters, A. (1997),
IGTree: using trees for compression and classification
in lazy learning algorithms, Artificial Intelligence Re-
view 11:407–423.
Daelemans, W., Zavrel, J., van der Sloot, K.,
van den Bosch, A. (2000), TiMBL: Tilburg
Memory-Based Learner, version 3.0, refer-
ence guide, ILK Technical Report 00-01,
http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
Gorin, A., Riccardi, G., Wright, J. (1997), How may I Help
You?, Speech Communication 23:113-127.
Hirschberg, J., Litman, D., Swerts, M. (1999), Prosodic
cues to recognition errors, Proc. ASRU, Keystone, CO.
Krahmer, E., Swerts, M., Theune, M., Weegels, M., (1999),
Error spotting in human-machine interactions, Proc.
EUROSPEECH, Budapest, Hungary.
Litman, D., Pan, S. (2000), Predicting and adapting to poor
speech recongition in a spoken dialogue system, Proc.
17th AAAI, Austin, TX.
Litman, D., Walker, M., Kearns, M. (1999), Automatic De-
tection of Poor Speech Recognition at the Dialogue
Level. Proc. ACL’99, College Park, MD.
Manning, C., Sch¨utze, H., (1999), Foundations of Statist-
ical Natural Language Processing, The MIT Press,
Cambridge, MA.
van Rijsbergen, C.J. (1979), Information Retrieval, Lon-
don: Buttersworth.
Soltau, H., Waibel, A. (1998), On the influence of hyper-
articulated speech on recognition performance, Proc.
ICSLP’98, Sydney, Australia
Swerts, M., Litman, D., Hirschberg, J. (2000), Correc-
tions in spoken dialogue systems, Proc. ICSLP 2000,
Beijing, China.
Walker, M., Langkilde, I., Wright, J., Gorin, A., Litman, D.
(2000a), Learning to predict problematic situations in
a spoken dialogue system: Experiment with How May
I Help You?, Proc. NAACL, Seattle, WA.
Walker, M., Wright, J. Langkilde, I. (2000b), Using nat-
ural language processing and discourse features to
identify understanding errors in a spoken dialogue sys-
tem, Proc. ICML, Stanford, CA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264566">
<title confidence="0.99699">Detecting problematic turns in human-machine interactions: Rule-induction versus memory-based learning approaches</title>
<author confidence="0.999186">Antal van_den_Bosch</author>
<affiliation confidence="0.969523">ILK / Comp. Ling.</affiliation>
<address confidence="0.9456735">KUB, Tilburg The Netherlands</address>
<email confidence="0.991561">antalb@kub.nl</email>
<author confidence="0.898267">Emiel Krahmer</author>
<affiliation confidence="0.961702">IPO</affiliation>
<address confidence="0.919391">TU/e, Eindhoven The Netherlands</address>
<email confidence="0.997787">E.J.Krahmer@tue.nl</email>
<author confidence="0.998324">Marc Swerts</author>
<affiliation confidence="0.930251">CNTS</affiliation>
<address confidence="0.645795">UIA, Antwerp Belgium</address>
<email confidence="0.994857">M.G.J.Swerts@tue.nl</email>
<abstract confidence="0.999560928571429">We address the issue of on-line detection of communication problems in spoken dialogue systems. The usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances. By applying both ruleinduction and memory-based learning techniques to data obtained with a Dutch train time-table information system, the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline. The results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead. The results are interesting from a machine learning perspective, since they show that the rule-based method performs significantly better than the memory-based method, because the former is better capable of representing interactions between features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based Learning Algorithms,</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--36</pages>
<contexts>
<context position="3641" citStr="Aha et al. 1991" startWordPosition="562" endWordPosition="565">putation, which makes this a very cheap method to detect problems. We shall see that on the basis of the previous and the current word graph and the six most recent system question types, communication problems can be detected with an accuracy of 91%, which is a significant improvement over the relevant baseline. This shows that spoken dialogue systems may use these features to better predict whether the ongoing dialogue is problematic. In addition, the current work is interesting from a machine learning perspective. We apply two machine learning techniques: the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997) and the RIPPER rule induction algorithm (Cohen 1996). As we shall see, some interesting differences between the two approaches arise. 2 Related work Recently there has been an increased interest in developing automatic methods to detect problematic dialogue situations using machine learning techniques. For instance, Litman et al. (1999) and Walker et al. (2000a) use RIPPER (Cohen 1996) to classify problematic and unproblematic dialogues. Following up on this, Walker et al. (2000b) aim at detecting problems at the utterance level, based on data obtained with AT&amp;Ts How M</context>
<context position="5076" citStr="Aha et al. 1991" startWordPosition="796" endWordPosition="799">language understanding module (NLU) and the dialogue manager (DM). The best result is obtained using all features: communication problems are detected with an accuracy of 86%, a precision of 83% and a recall of 75%. It should be noted that the NLU features play first fiddle among the set of all features. In fact, using only the NLU features performs comparable to using all features. Walker et al. (2000b) also briefly compare the performance of RIPPER with some other machine learning approaches, and show that it performs comparable to a memory-based (instance-based) learning algorithm (IB, see Aha et al. 1991). The results which Walker and co-workers describe show that it is possible to automatically detect communication problems in the HMIHY system, using machine learning techniques. Their approach also raises a number of interesting followup questions, some concerned with problem detection, others with the use of machine learning techniques. (1) Walker et al. train their classifier on a large set of features, and show that the set of features produced by the NLU module are the most important ones. However, this leaves an important general question unanswered, namely which particular features cont</context>
<context position="16904" citStr="Aha et al. 1991" startWordPosition="2775" endWordPosition="2778">ious user utterance caused a problem, (6) predict on the basis of BoW and 6Q , whether the previous user utterance caused a problem, (7) predict on the basis of the two most recent word graphs, BoW -1 and BoW , whether the previous user utterance caused a problem, and finally (8) predict on the basis of the two most recent word graphs, BoW -1 and BoW , and the six most recent system question types 6Q , whether the previous user utterance caused a problem. 3.4 Learning techniques For the experiments we used the rule-induction algorithm RIPPER (Cohen 1996) and the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997). RIPPER is a fast rule induction algorithm. It starts with splitting the training set in two. On the basis of one half, it induces rules in a straightforward way (roughly, by trying to maximize coverage for each rule), with potential overfitting. When the induced rules classify instances in the other half below a certain threshold, they are not stored. Rules are induced per class. By default the ordering is from low-frequency classes to high frequency ones, leaving the most frequent class as the default rule, which is generally beneficial for the size of the rule set. </context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>Aha, D., Kibler, D., Albert, M. (1991), Instance-based Learning Algorithms, Machine Learning, 6:36–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning trees and rules with set-valued features,</title>
<date>1996</date>
<booktitle>Proc. 13th AAAI.</booktitle>
<contexts>
<context position="3718" citStr="Cohen 1996" startWordPosition="576" endWordPosition="577">hat on the basis of the previous and the current word graph and the six most recent system question types, communication problems can be detected with an accuracy of 91%, which is a significant improvement over the relevant baseline. This shows that spoken dialogue systems may use these features to better predict whether the ongoing dialogue is problematic. In addition, the current work is interesting from a machine learning perspective. We apply two machine learning techniques: the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997) and the RIPPER rule induction algorithm (Cohen 1996). As we shall see, some interesting differences between the two approaches arise. 2 Related work Recently there has been an increased interest in developing automatic methods to detect problematic dialogue situations using machine learning techniques. For instance, Litman et al. (1999) and Walker et al. (2000a) use RIPPER (Cohen 1996) to classify problematic and unproblematic dialogues. Following up on this, Walker et al. (2000b) aim at detecting problems at the utterance level, based on data obtained with AT&amp;Ts How May I Help You (HMIHY) system (Gorin et al.1997). Walker and co-workers apply </context>
<context position="16849" citStr="Cohen 1996" startWordPosition="2768" endWordPosition="2769"> six most recent system questions, whether the previous user utterance caused a problem, (6) predict on the basis of BoW and 6Q , whether the previous user utterance caused a problem, (7) predict on the basis of the two most recent word graphs, BoW -1 and BoW , whether the previous user utterance caused a problem, and finally (8) predict on the basis of the two most recent word graphs, BoW -1 and BoW , and the six most recent system question types 6Q , whether the previous user utterance caused a problem. 3.4 Learning techniques For the experiments we used the rule-induction algorithm RIPPER (Cohen 1996) and the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997). RIPPER is a fast rule induction algorithm. It starts with splitting the training set in two. On the basis of one half, it induces rules in a straightforward way (roughly, by trying to maximize coverage for each rule), with potential overfitting. When the induced rules classify instances in the other half below a certain threshold, they are not stored. Rules are induced per class. By default the ordering is from low-frequency classes to high frequency ones, leaving the most frequent class as the default rule, which</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>Cohen, W. (1996), Learning trees and rules with set-valued features, Proc. 13th AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>A Weijters</author>
</authors>
<title>IGTree: using trees for compression and classification in lazy learning algorithms,</title>
<date>1997</date>
<journal>Artificial Intelligence Review</journal>
<pages>11--407</pages>
<marker>Daelemans, van den Bosch, Weijters, 1997</marker>
<rawString>Daelemans, W., van den Bosch, A., Weijters, A. (1997), IGTree: using trees for compression and classification in lazy learning algorithms, Artificial Intelligence Review 11:407–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory-Based Learner, version 3.0, reference guide, ILK</title>
<date>2000</date>
<tech>Technical Report 00-01, http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2000</marker>
<rawString>Daelemans, W., Zavrel, J., van der Sloot, K., van den Bosch, A. (2000), TiMBL: Tilburg Memory-Based Learner, version 3.0, reference guide, ILK Technical Report 00-01, http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gorin</author>
<author>G Riccardi</author>
<author>J Wright</author>
</authors>
<date>1997</date>
<journal>I Help You?, Speech Communication</journal>
<pages>23--113</pages>
<location>How</location>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>Gorin, A., Riccardi, G., Wright, J. (1997), How may I Help You?, Speech Communication 23:113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
<author>M Swerts</author>
</authors>
<title>Prosodic cues to recognition errors,</title>
<date>1999</date>
<booktitle>Proc. ASRU,</booktitle>
<location>Keystone, CO.</location>
<contexts>
<context position="7531" citStr="Hirschberg et al. 1999" startWordPosition="1206" endWordPosition="1209">r on-line problem detection as Walker et al. (2000b), but (1) we take a bottom-up approach, focussing on a small number of features and investigating their usefulness on a per-feature basis and (2) the features which we study are automatically available in the majority of current spoken dialogue system: the sequence of system question types and the word graphs corresponding to the respective user utterances. A word graph is a lattice of word hypotheses, and we conjecture that various features which have been shown to cue communication problems (prosodic, linguistic and ASR features, see e.g., Hirschberg et al. 1999, Krahmer et al. 1999 and Swerts et al. 2000) have correlates in the word graph. The sequence of system question types is taken to model the dialogue history. Finally, (3) to gain further insight into the adequacy of various machine learning techniques for problem detection we use both RIPPER and the memory-based IB1-IG algorithm. 3 Approach 3.1 Data and Labeling The corpus we used consisted of 3739 question-answer pairs, taken from 444 complete dialogues. The dialogues consist of users interacting with a Dutch spoken dialogue system which provides information about train time tables. The syst</context>
<context position="27455" citStr="Hirschberg et al. 1999" startWordPosition="4614" endWordPosition="4617">imed at testing whether a user utterance, captured in a noisy word graph, and/or the recent history of system utterances, would be predictive of whether the utterance itself would be misrecognised. The results, which basically represents a signal quality test, show that problematic cases could be discerned with an accuracy of about 65%. Although this is somewhat above the baseline of 58% decision accuracy when no problems would be predicted, signalling recognition problems with word graph features and previous system question types as predictors is a hard task. As other studies suggest (e.g., Hirschberg et al. 1999), confidence scores and acoustic/prosodic features could be of help. The second approach tested whether the word graph for the current user utterance and/or the recent history of system question types could be employed to predict whether the previous user In the sense that it is not a perfect image of the users input. utterance caused communication problems. The underlying assumption is that users will signal problems as soon as they become aware of them through the feedback provided by the system. Thus, in a sense, this second approach represents a noisy channel filtering task: the current ut</context>
</contexts>
<marker>Hirschberg, Litman, Swerts, 1999</marker>
<rawString>Hirschberg, J., Litman, D., Swerts, M. (1999), Prosodic cues to recognition errors, Proc. ASRU, Keystone, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
<author>M Theune</author>
<author>M Weegels</author>
</authors>
<title>Error spotting in human-machine interactions,</title>
<date>1999</date>
<booktitle>Proc. EUROSPEECH,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="7552" citStr="Krahmer et al. 1999" startWordPosition="1210" endWordPosition="1213">ion as Walker et al. (2000b), but (1) we take a bottom-up approach, focussing on a small number of features and investigating their usefulness on a per-feature basis and (2) the features which we study are automatically available in the majority of current spoken dialogue system: the sequence of system question types and the word graphs corresponding to the respective user utterances. A word graph is a lattice of word hypotheses, and we conjecture that various features which have been shown to cue communication problems (prosodic, linguistic and ASR features, see e.g., Hirschberg et al. 1999, Krahmer et al. 1999 and Swerts et al. 2000) have correlates in the word graph. The sequence of system question types is taken to model the dialogue history. Finally, (3) to gain further insight into the adequacy of various machine learning techniques for problem detection we use both RIPPER and the memory-based IB1-IG algorithm. 3 Approach 3.1 Data and Labeling The corpus we used consisted of 3739 question-answer pairs, taken from 444 complete dialogues. The dialogues consist of users interacting with a Dutch spoken dialogue system which provides information about train time tables. The system prompts the user f</context>
<context position="26203" citStr="Krahmer et al. (1999)" startWordPosition="4405" endWordPosition="4408"> in Figure 1. RIPPER’s first rule is concerned with repeated questions (compare with the system-knows-baseline). One important property of many other rules is that they explicitly combine pieces of information from the three main sources of information (the system questions, the current word graph and the previous word graph). Moreover, it is interesting to note that the words which crop up in the RIPPER rules are primarily function words. Another noteworthy feature of the RIPPER rules is that they reflect certain properties which have been claimed to cue communication problems. For instance, Krahmer et al. (1999), in their descriptive analysis of dialogue problems, found that repeated material is often an indication of problems, as is the use of a marked vocabulary. The rules 2, 3 and 7 are examples of the former cue, while the occurrence of the somewhat archaic “vanuit” instead of the ordinary “van” is an example of the latter. 5 Discussion In this study we have looked at automatic methods for problem detection using simple features which are available in the vast majority of spoken dialogue systems, and require little or no computational overhead. We have investigated two approaches to problem detec</context>
</contexts>
<marker>Krahmer, Swerts, Theune, Weegels, 1999</marker>
<rawString>Krahmer, E., Swerts, M., Theune, M., Weegels, M., (1999), Error spotting in human-machine interactions, Proc. EUROSPEECH, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>S Pan</author>
</authors>
<title>Predicting and adapting to poor speech recongition in a spoken dialogue system,</title>
<date>2000</date>
<booktitle>Proc. 17th AAAI,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="2159" citStr="Litman and Pan 2000" startWordPosition="319" endWordPosition="322">f automatic speech recognition, but also incorrect interpretations by the natural language understanding module or wrong default assumptions by the dialogue manager are likely to lead to confusion. If a spoken dialogue system had the ability to detect communication problems on-line and with high accuracy, it might be able to correct certain errors or it could interact with the user to solve them. For instance, in the case of communication problems, it would be beneficial to change from a relatively natural dialogue strategy to a more constrained one in order to resolve the problems (see e.g., Litman and Pan 2000). Similarly, it has been shown that users switch to a ‘marked’, hyperarticulate speaking style after problems (e.g., Soltau and Waibel 1998), which itself is an important source of recognition errors. This might be solved by using two recognizers in parallel, one trained on normal speech and one on hyperarticulate speech. If there are communication problems, then the system could decide to focus on the recognition results delivered by the engine trained on hyperarticulate speech. For such approaches to work, however, it is essential that the spoken dialogue system is able to automatically dete</context>
</contexts>
<marker>Litman, Pan, 2000</marker>
<rawString>Litman, D., Pan, S. (2000), Predicting and adapting to poor speech recongition in a spoken dialogue system, Proc. 17th AAAI, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>M Walker</author>
<author>M Kearns</author>
</authors>
<title>Automatic Detection of Poor Speech Recognition at the Dialogue Level.</title>
<date>1999</date>
<booktitle>Proc. ACL’99,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="4004" citStr="Litman et al. (1999)" startWordPosition="616" endWordPosition="619"> use these features to better predict whether the ongoing dialogue is problematic. In addition, the current work is interesting from a machine learning perspective. We apply two machine learning techniques: the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997) and the RIPPER rule induction algorithm (Cohen 1996). As we shall see, some interesting differences between the two approaches arise. 2 Related work Recently there has been an increased interest in developing automatic methods to detect problematic dialogue situations using machine learning techniques. For instance, Litman et al. (1999) and Walker et al. (2000a) use RIPPER (Cohen 1996) to classify problematic and unproblematic dialogues. Following up on this, Walker et al. (2000b) aim at detecting problems at the utterance level, based on data obtained with AT&amp;Ts How May I Help You (HMIHY) system (Gorin et al.1997). Walker and co-workers apply RIPPER to 43 features which are automatically generated by three modules of the HMIHY system, namely the speech recognizer (ASR), the natural language understanding module (NLU) and the dialogue manager (DM). The best result is obtained using all features: communication problems are de</context>
</contexts>
<marker>Litman, Walker, Kearns, 1999</marker>
<rawString>Litman, D., Walker, M., Kearns, M. (1999), Automatic Detection of Poor Speech Recognition at the Dialogue Level. Proc. ACL’99, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing,</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, C., Sch¨utze, H., (1999), Foundations of Statistical Natural Language Processing, The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<location>Information Retrieval, London: Buttersworth.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>van Rijsbergen, C.J. (1979), Information Retrieval, London: Buttersworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>A Waibel</author>
</authors>
<title>On the influence of hyperarticulated speech on recognition performance,</title>
<date>1998</date>
<booktitle>Proc. ICSLP’98,</booktitle>
<location>Sydney, Australia</location>
<contexts>
<context position="2299" citStr="Soltau and Waibel 1998" startWordPosition="341" endWordPosition="344">ons by the dialogue manager are likely to lead to confusion. If a spoken dialogue system had the ability to detect communication problems on-line and with high accuracy, it might be able to correct certain errors or it could interact with the user to solve them. For instance, in the case of communication problems, it would be beneficial to change from a relatively natural dialogue strategy to a more constrained one in order to resolve the problems (see e.g., Litman and Pan 2000). Similarly, it has been shown that users switch to a ‘marked’, hyperarticulate speaking style after problems (e.g., Soltau and Waibel 1998), which itself is an important source of recognition errors. This might be solved by using two recognizers in parallel, one trained on normal speech and one on hyperarticulate speech. If there are communication problems, then the system could decide to focus on the recognition results delivered by the engine trained on hyperarticulate speech. For such approaches to work, however, it is essential that the spoken dialogue system is able to automatically detect communication problems with a high accuracy. In this paper, we investigate the usefulness for problem detection of the word graph and the</context>
</contexts>
<marker>Soltau, Waibel, 1998</marker>
<rawString>Soltau, H., Waibel, A. (1998), On the influence of hyperarticulated speech on recognition performance, Proc. ICSLP’98, Sydney, Australia</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swerts</author>
<author>D Litman</author>
<author>J Hirschberg</author>
</authors>
<title>Corrections in spoken dialogue systems,</title>
<date>2000</date>
<booktitle>Proc. ICSLP</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="7576" citStr="Swerts et al. 2000" startWordPosition="1215" endWordPosition="1218">00b), but (1) we take a bottom-up approach, focussing on a small number of features and investigating their usefulness on a per-feature basis and (2) the features which we study are automatically available in the majority of current spoken dialogue system: the sequence of system question types and the word graphs corresponding to the respective user utterances. A word graph is a lattice of word hypotheses, and we conjecture that various features which have been shown to cue communication problems (prosodic, linguistic and ASR features, see e.g., Hirschberg et al. 1999, Krahmer et al. 1999 and Swerts et al. 2000) have correlates in the word graph. The sequence of system question types is taken to model the dialogue history. Finally, (3) to gain further insight into the adequacy of various machine learning techniques for problem detection we use both RIPPER and the memory-based IB1-IG algorithm. 3 Approach 3.1 Data and Labeling The corpus we used consisted of 3739 question-answer pairs, taken from 444 complete dialogues. The dialogues consist of users interacting with a Dutch spoken dialogue system which provides information about train time tables. The system prompts the user for unknown slots, such a</context>
</contexts>
<marker>Swerts, Litman, Hirschberg, 2000</marker>
<rawString>Swerts, M., Litman, D., Hirschberg, J. (2000), Corrections in spoken dialogue systems, Proc. ICSLP 2000, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>I Langkilde</author>
<author>J Wright</author>
<author>A Gorin</author>
<author>D Litman</author>
</authors>
<title>Learning to predict problematic situations in a spoken dialogue system: Experiment with How May I Help You?,</title>
<date>2000</date>
<location>Proc. NAACL, Seattle, WA.</location>
<contexts>
<context position="4028" citStr="Walker et al. (2000" startWordPosition="621" endWordPosition="624">tter predict whether the ongoing dialogue is problematic. In addition, the current work is interesting from a machine learning perspective. We apply two machine learning techniques: the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997) and the RIPPER rule induction algorithm (Cohen 1996). As we shall see, some interesting differences between the two approaches arise. 2 Related work Recently there has been an increased interest in developing automatic methods to detect problematic dialogue situations using machine learning techniques. For instance, Litman et al. (1999) and Walker et al. (2000a) use RIPPER (Cohen 1996) to classify problematic and unproblematic dialogues. Following up on this, Walker et al. (2000b) aim at detecting problems at the utterance level, based on data obtained with AT&amp;Ts How May I Help You (HMIHY) system (Gorin et al.1997). Walker and co-workers apply RIPPER to 43 features which are automatically generated by three modules of the HMIHY system, namely the speech recognizer (ASR), the natural language understanding module (NLU) and the dialogue manager (DM). The best result is obtained using all features: communication problems are detected with an accuracy </context>
<context position="6295" citStr="Walker et al. (2000" startWordPosition="1000" endWordPosition="1003">ribute to what extent? (2) Moreover, the set of features which the NLU module produces appear to be rather specific to the HMIHY system and indicate things like the percentage of the input covered by the relevant grammar fragment, the presence or absence of context shifts, and the semantic diversity of subsequent utterances. Many current day spoken dialogue systems do not have such a sophisticated NLU module, and consequently it is unlikely that they have access to these kinds of features. In sum, it is uncertain whether other spoken dialogue systems can benefit from the findings described by Walker et al. (2000b), since it is unclear which features are important and to what extent these features are available in other spoken dialogue systems. Finally, (3) we agree with Walker et al. (and the machine learning community at large) that it is important to compare different machine learning techniques to find out which techniques perform well for which kinds of tasks. Walker et al. found that RIPPER does not perform significantly better or worse than a memory-based learning technique. Is this incidental or does it reflect a general property of the problem detection task? The current paper uses a similar </context>
</contexts>
<marker>Walker, Langkilde, Wright, Gorin, Litman, 2000</marker>
<rawString>Walker, M., Langkilde, I., Wright, J., Gorin, A., Litman, D. (2000a), Learning to predict problematic situations in a spoken dialogue system: Experiment with How May I Help You?, Proc. NAACL, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>J Langkilde Wright</author>
<author>I</author>
</authors>
<title>Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system,</title>
<date>2000</date>
<booktitle>Proc. ICML,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="4028" citStr="Walker et al. (2000" startWordPosition="621" endWordPosition="624">tter predict whether the ongoing dialogue is problematic. In addition, the current work is interesting from a machine learning perspective. We apply two machine learning techniques: the memory-based IB1-IG algorithm (Aha et al. 1991, Daelemans et al. 1997) and the RIPPER rule induction algorithm (Cohen 1996). As we shall see, some interesting differences between the two approaches arise. 2 Related work Recently there has been an increased interest in developing automatic methods to detect problematic dialogue situations using machine learning techniques. For instance, Litman et al. (1999) and Walker et al. (2000a) use RIPPER (Cohen 1996) to classify problematic and unproblematic dialogues. Following up on this, Walker et al. (2000b) aim at detecting problems at the utterance level, based on data obtained with AT&amp;Ts How May I Help You (HMIHY) system (Gorin et al.1997). Walker and co-workers apply RIPPER to 43 features which are automatically generated by three modules of the HMIHY system, namely the speech recognizer (ASR), the natural language understanding module (NLU) and the dialogue manager (DM). The best result is obtained using all features: communication problems are detected with an accuracy </context>
<context position="6295" citStr="Walker et al. (2000" startWordPosition="1000" endWordPosition="1003">ribute to what extent? (2) Moreover, the set of features which the NLU module produces appear to be rather specific to the HMIHY system and indicate things like the percentage of the input covered by the relevant grammar fragment, the presence or absence of context shifts, and the semantic diversity of subsequent utterances. Many current day spoken dialogue systems do not have such a sophisticated NLU module, and consequently it is unlikely that they have access to these kinds of features. In sum, it is uncertain whether other spoken dialogue systems can benefit from the findings described by Walker et al. (2000b), since it is unclear which features are important and to what extent these features are available in other spoken dialogue systems. Finally, (3) we agree with Walker et al. (and the machine learning community at large) that it is important to compare different machine learning techniques to find out which techniques perform well for which kinds of tasks. Walker et al. found that RIPPER does not perform significantly better or worse than a memory-based learning technique. Is this incidental or does it reflect a general property of the problem detection task? The current paper uses a similar </context>
</contexts>
<marker>Walker, Wright, I, 2000</marker>
<rawString>Walker, M., Wright, J. Langkilde, I. (2000b), Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system, Proc. ICML, Stanford, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>