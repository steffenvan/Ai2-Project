<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000403">
<title confidence="0.9975875">
LyS: Porting a Twitter Sentiment Analysis Approach
from Spanish to English
</title>
<author confidence="0.992239">
David Vilares, Miguel Hermo, Miguel A. Alonso, Carlos G´omez-Rodriguez, Yerai Doval
</author>
<affiliation confidence="0.736582666666667">
Grupo LyS, Departamento de Computaci´on, Facultade de Inform´atica
Universidade da Coru˜na, Campus de A Coru˜na
15071 A Coru˜na, Spain
</affiliation>
<email confidence="0.97074">
{david.vilares, miguel.hermo, miguel.alonso, carlos.gomez, yerai.doval}@udc.es
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994175">
This paper proposes an approach to solve
message- and phrase-level polarity classi-
fication in Twitter, derived from an exist-
ing system designed for Spanish. As a
first step, an ad-hoc preprocessing is per-
formed. We then identify lexical, psycho-
logical and semantic features in order to
capture different dimensions of the human
language which are helpful to detect sen-
timent. These features are used to feed a
supervised classifier after applying an in-
formation gain filter, to discriminate irrel-
evant features. The system is evaluated on
the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter. Our approach worked com-
petitively both in message- and phrase-
level tasks. The results confirm the robust-
ness of the approach, which performed
well on different domains involving short
informal texts.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999266214285714">
Millions of opinions, conversations or just trivia
are published each day in Twitter by users of dif-
ferent cultures, countries and ages. This provides
an effective way to poll how people praise, com-
plain or discuss about virtually any topic. Compre-
hending and analysing all this information has be-
come a new challenge for organisations and com-
panies, which aim to find out a way to make quick
and more effective decisions for their business. In
particular, identifying the perception of the public
with respect to an event, a service or an entity are
some of their main goals in a short term. In this
respect, sentiment analysis, and more specifically
polarity classification, is playing an important role
</bodyText>
<footnote confidence="0.47155425">
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.9992661">
in order to automatically analyse subjective infor-
mation in texts.
This paper describes our participation at Sem-
Eval 2014 task 9: Sentiment Analysis in Twit-
ter. Specifically, two subtasks were presented:
(A) contextual polarity disambiguation and (B)
message polarity classification. The first sub-
task consists on determining the polarity of words
or phrases extracted from short informal texts,
the scope of extracts being provided by the Se-
mEval organisation. Subtask B focusses on clas-
sifying the content of the whole message. In
both cases, three possible sentiments are consid-
ered: positive, negative and neutral (which in-
volves mixed and non-opinionated instances). Al-
though the training set only contains tweets, the
test set also includes short informal texts from
other domains, in order to measure cross-domain
portability. You can test the model for subtask B
atmiopia.grupolys.org.
</bodyText>
<sectionHeader confidence="0.694605" genericHeader="method">
2 SemEval 2014-Task 9: Sentiment
</sectionHeader>
<subsectionHeader confidence="0.802161">
Analysis in Twitter
</subsectionHeader>
<bodyText confidence="0.9999637">
Our contribution is a reduced version of a Span-
ish sentiment classification system (Vilares et al.,
2013a; Vilares et al., 2013b) that participated in
TASS 2013 (Villena-Rom´an et al., 2014), achiev-
ing the 5th place on the global sentiment classifi-
cation task and the 1st place on topic classification
on tweets. In this section we describe how we have
ported to English this system originally designed
for Spanish. Tasks A and B are addressed from
the same perspective, which is described below.
</bodyText>
<subsectionHeader confidence="0.992113">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9998766">
We implement a naive preprocessing algorithm
which seeks to normalise some of the most com-
mon ungrammatical elements. It is intended for
Twitter, but many of the issues addressed would
also be valid in other domains:
</bodyText>
<page confidence="0.970544">
411
</page>
<note confidence="0.6279015">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411–415,
Dublin, Ireland, August 23-24, 2014.
</note>
<listItem confidence="0.999557944444444">
• Replacement of frequent abbreviations The
list of the most frequent ones was extracted
from the training set, taking the Penn Tree-
bank (Marcus et al., 1993) as our dictionary.
A term is considered ungrammatical if it does
not appear in our dictionary. We then carry
out a manual review to distinguish between
unknown words and abbreviations, providing
a correction in the latter case. For example,
‘c’mon’ becomes ‘come on’ and ‘Sat’ is re-
placed by ‘Saturday’.
• Emoticon normalisation: We employ the
emoticon collection published in (Agarwal et
al., 2011). Each emoticon is replaced with
one of these five labels: strong positive (ESP),
positive (EP), neutral (ENEU), negative (EN)
or strong negative (ESN).
•
</listItem>
<bodyText confidence="0.661417">
Laughs: Multiple forms used in social media
to reflect laughs (e.g. ‘hhahahha’, ‘HHEHE-
HEH’) are preprocessed in a homogeneous
way to obtain a pattern of the form ‘hxhx’
where x E {a, e, i, o, u}.
</bodyText>
<listItem confidence="0.999604666666667">
• URL normalisation: External links are re-
placed by the string ‘url’.
• Hashtags (‘#’) and usernames (‘@’): If the
</listItem>
<bodyText confidence="0.806621363636364">
hashtag appears at the end or beginning of
the tweet, we remove the hashtag. Based
on other participant approaches at SemEval
2013 (Nakov et al., 2013), we realized maybe
this is not the best option, although we be-
lieve hashtags will not be useful in most of
cases, since they refer to very specific events.
Otherwise, only the ‘#’ is removed, hypothe-
sising the hashtag is used to emphasise a term
(e.g. ‘Matthew #Mcconaughey has won the
Oscar’).
</bodyText>
<subsectionHeader confidence="0.999423">
2.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999983454545455">
Our approach only takes into account information
extracted from the text, without considering any
kind of meta-data. Extracted features combine
lexical, psychological and semantic knowledge in
order to build a linguistic model able to analyse
tweets, but also other kinds of messages. These
features can be divided into two types: corpus-
extracted features and lexicon-extracted features.
All of them take the total number of occurrences
of the respective feature as the weighting factor to
then feed the supervised classifier.
</bodyText>
<subsubsectionHeader confidence="0.639994">
2.2.1 Corpus-extracted features
</subsubsectionHeader>
<bodyText confidence="0.7392005">
Given a corpus, we use it to extract the following
set of features:
</bodyText>
<listItem confidence="0.941357">
• Word forms: A model based on this type of
features is our baseline. Each single word is
considered as a feature in order to feed the
supervised classifier. This often becomes a
simple and acceptable start point which ob-
tains a decent performance.
• Part-of-speech (PoS) information: some
coarse-grained PoS-tags such as adjective or
adverb are usually good indicators of subjec-
tive texts while some fine-grained PoS tags
such as third person personal pronoun pro-
vide evidence of non-opinionated messages
(Pak and Paroubek, 2010).
</listItem>
<subsubsectionHeader confidence="0.844129">
2.2.2 Lexicon-extracted features
</subsubsectionHeader>
<bodyText confidence="0.999057">
We also consider information obtained from exter-
nal lexicons in order to capture linguistic informa-
tion that can not be extracted from a training cor-
pus by means of bag-of-words and PoS-tag mod-
els. We rely on two manually-build lexicons:
</bodyText>
<listItem confidence="0.996854941176471">
• Pennebaker et al. (2001) psychometric dictio-
naries. Linguistic Inquiry and Word Count1
(LIWC) is a software which includes a seman-
tic dictionary to measure how people use dif-
ferent kinds of words over a wide number of
texts. It categorises terms into psychometric
properties, which correspond to different di-
mensions of the human language. The dictio-
nary relates terms with psychological prop-
erties (e.g. anger or anxiety), but also with
topics (e.g. family, friends, religion) or even
morphological features (e.g. future time, past
time or exclamations).
• Hu and Liu (2004) opinion lexicon. It is a col-
lection of positive and negative words. Many
of the occurrences are misspelled, since they
often come from web environments.
</listItem>
<subsubsectionHeader confidence="0.63978">
2.2.3 Syntactic features
</subsubsectionHeader>
<bodyText confidence="0.999549833333333">
We also parsed the tweets using MaltParser (Nivre
et al., 2007) in order to obtain dependency triplets
of the form (wi, arcij, wj), where wi is the head
word wj, the dependent one and arcij the exist-
ing syntactic relation between them. We tried to
incorporate generalised dependency triplets (Joshi
</bodyText>
<footnote confidence="0.993138">
1http://www.liwc.net/
</footnote>
<page confidence="0.993494">
412
</page>
<bodyText confidence="0.999894666666667">
and Penstein-Ros´e, 2009), following an enriched
perspective presented in Vilares et al. (2014). A
generalisation consists on backing off the words
to more abstracted terms. For example, a valid de-
pendency triplet for the phrase ‘awesome villain’
is (villain, modi�er, awesome), which could be
generalised into (anger, modi�er, assent) by means
of psychometric properties. However, experimen-
tal results over the development corpus using these
features decreased performance with respect to
our best model, probably due to the small size of
the training corpus, since dependency triplets tend
to suffer from sparsity, so a larger training corpus
is needed to exploit them in a proper way (Vilares
et al., 2014).
</bodyText>
<subsectionHeader confidence="0.991316">
2.3 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999546625">
For a machine learning approach, sparsity could
be an issue. In particular, due to the size of the cor-
pus, many of the terms extracted from the training
set only appear a few times in it. This makes it
impossible to properly learn the polarity of many
tokens. Thus, we carry out a filtering step before
feeding our classifier. In particular, we rely on
the information gain (IG) method to then rank the
most relevant features. Information gain measures
the relevance of an attribute with respect to a class.
It takes values between 0 and 1, where a higher
value implies a higher relevance. Table 1 shows
the top five relevant features based on their infor-
mation gain for our best model. The top features
for task A were very similar. Our official runs only
consider features with an IG greater than zero.
</bodyText>
<table confidence="0.9967585">
IG Feature Category
0.140 positive emotion Pennebaker et al. (2001)
0.137 #positive-words Hu and Liu (2004)
0.126 affect Pennebaker et al. (2001)
0.089 #negative-words Hu and Liu (2004)
0.083 negative emotion Pennebaker et al. (2001)
</table>
<tableCaption confidence="0.988838">
Table 1: Most relevant features for task B. ‘#’ must
</tableCaption>
<bodyText confidence="0.956731888888889">
be read this table as ‘the number of’and not as a
hashtag.
mentation. We configured the multi-class support
vector machine by Crammer and Singer (2002) as
the SVMtype. Since the corpus was unbalanced,
we tuned the weights for the classes using the de-
velopment corpus: 1 for the positive class, 2 for
negative and 0.5 for neutral. The rest of parame-
ters were set to default values.
</bodyText>
<sectionHeader confidence="0.996661" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999728086956522">
The SemEval 2014 organisation provides a stan-
dard training corpus for both tasks A an B. For task
A, each tweet is marked with a list of the words
and phrases to analyse, and for each one its senti-
ment label is provided. In addition, a development
corpus was released for tuning the system parame-
ters. The training and the development corpus can
be used jointly (constrained runs) to train mod-
els that are then evaluated over the test corpus.2
Some participants used external annotated corpora
(unconstrained runs) to build their models. With
respect to the test corpus, it contains texts from
tweets but also from LiveJournal texts, which we
are abbreviating as LJ, and SMS messages.
Table 2 contains the statistics of the corpora we
used. Sharing data is a violation of Twitter’s terms
of service, so we had to download them. Unfortu-
nately, some of the tweets were no longer available
for several reasons, e.g., user or a tweet does not
exist anymore or the privacy settings of a user have
changed. As a result, the size of our training and
development corpora may be different from those
of other participant’s corpora.
</bodyText>
<table confidence="0.999449857142857">
Task Set Positive Negative Neutral
Train 4,917 2,591 385
A Dev 555 365 45
Test 6,354 3,771 556
Train 3,063 1,202 3,935
B Dev 493 290 633
Test 3,506 1,541 3,940
</table>
<tableCaption confidence="0.940457">
Table 2: SemEval 2014 corpus statistics.
</tableCaption>
<bodyText confidence="0.801879">
2.4 Classifier 3.1 Evaluation Metrics
We have trained our runs with a SVM LibLINEAR F-measure is the official score to measure how sys-
classifier (Fan et al., 2008) taking the implementa- tems behave on each class. In order to rank partic-
tion provided in WEKA (Hall et al., 2009). The ipants, the SemEval 2014 organisation proposed
selection was motivated by the acceptable results the averaged F-measure of positive and negative
that some of the participants in SemEval 2013, e.g. tweets.
Becker et al. (2013), obtained using this imple-
2We followed this angle.
</bodyText>
<page confidence="0.989248">
413
</page>
<subsectionHeader confidence="0.998365">
3.2 Performance on Sets
</subsectionHeader>
<bodyText confidence="0.999683083333333">
Tables 3 and 4 show performance on the test set
of different combinations of the proposed features.
Table 5 shows the performance of our run on task
A. The results over the corresponding sets for task
B are illustrated in Table 6. They are significant
lower than in task A. This suggests that when a
message involves more than one of two tokens, a
lexical approach is not enough. Improving perfor-
mance should involve taking into account context
and linguistic phenomena that appear in sentences
to build a model based on the composition of lin-
guistic information.
</bodyText>
<table confidence="0.999699727272727">
Model LJ SMS Twitter Twitter Twitter
2013 2014 Sarcasm
WPLT 82.21 82.32 84.82 81.69 71.19
(no IG)
WPL 83.55 81.04 84.85 80.64 68.79
WPLT* 83.96 81.46 85.63 79.93 71.98
WP 78.53 80.97 80.34 73.35 74.18
P 75.70 78.74 73.58 65.75 71.82
W 61.58 65.45 64.56 59.16 62.93
L 66.04 64.11 62.96 53.81 61.26
T 47.07 51.37 71.82 43.64 49.37
</table>
<tableCaption confidence="0.998772">
Table 3: Performance on the test set for task A.
</tableCaption>
<bodyText confidence="0.721080166666667">
The model marked with a * was our official run. W
stands for features obtained from a bag-of-words
approach, L from Hu and Liu (2004), P from Pen-
nebaker et al. (2001) and T for fine-grained PoS-
tags. They can be combined, e.g., a model named
WP use both words and psychometric properties.
</bodyText>
<table confidence="0.999827545454545">
Model LJ SMS Twitter Twitter Twitter
2013 2014 Sarcasm
WPLT* 69.79 60.45 66.92 64.92 42.40
WPL 70.19 61.41 66.71 64.51 45.72
WP 66.84 60.22 65.29 63.90 45.90
WPLT 66.38 57.01 61.96 62.84 43.71
(no IG)
W 65.12 56.00 62.87 62.64 48.75
P 63.42 54.80 60.05 57.66 54.20
T 45.99 35.85 46.53 45.99 48.58
L 57.53 45.14 48.80 44.48 49.14
</table>
<tableCaption confidence="0.999805">
Table 4: Performance on the test set for task B.
</tableCaption>
<sectionHeader confidence="0.990221" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9901814">
This papers describes the participation of the LyS
Research Group (http://www.grupolys.
org) at the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter, with a system that attained com-
petitive performance both in message and phrase-
</bodyText>
<table confidence="0.9995796">
Test set Positive Negative Neutral
DEV 86.30 81.60 4.30
TWITTER 2013 88.70 81.90 17.60
(full)
TWITTER 2013 88.81 82.57 20.75
(progress subset)
LJ 84.34 83.56 13.84
SMS 80.31 82.56 7.10
TWITTER 2014 89.02 70.82 4.44
TWITTER SARCASM 85.71 57.63 28.57
</table>
<tableCaption confidence="0.934302333333333">
Table 5: Performance on different sets for our
model on task A. The model evaluated on the de-
velopment set was only built using the training set.
</tableCaption>
<table confidence="0.999938">
Test set Positive Negative Neutral
DEV 69.80 60.40 66.70
TWITTER 2013 72.50 64.30 72.30
(full)
TWITTER 2013 71.92 61.92 71.22
(progress subset)
LJ 71.94 67.65 66.23
SMS 63.83 57.06 73.76
TWITTER 2014 74.26 55.58 66.76
TWITTER SARCASM 55.17 29.63 51.61
</table>
<tableCaption confidence="0.984476">
Table 6: Performance on different sets for our
model on task B.
</tableCaption>
<table confidence="0.999499333333333">
Test set Task A Task B
LiveJournal 2014 4 / 27 13 / 50
SMS 2013 12 / 27 19 / 50
Twitter 2013 9 / 27 10 / 50
Twitter 2014 11 / 27 18 / 50
Twitter 2014 Sarcasm 10 / 27 33 / 50
</table>
<tableCaption confidence="0.992356">
Table 7: Position of our submission on each cor-
</tableCaption>
<bodyText confidence="0.958692888888889">
pus and task, according to results provided by the
organization on April 22, 2014.
level tasks, as can be observed in Table 7. This
system is a reduced version of a sentiment classifi-
cation model for Spanish texts that performed well
in the TASS 2013 (Villena et al., 2013). The offi-
cial results show how our approach works com-
petitively both on tasks A and B without needing
large and automatically-built resources. The ap-
proach is based on a bag-of-words that includes
word-forms and PoS-tags. We also extract psy-
chometric and sentiment information from exter-
nal lexicons. In order to reduce sparsity problems,
we firstly apply an information gain filter to select
only the most relevant features. Experiments on
the development set showed a significant improve-
ment on the same model with respect to skipping
it on subtask B.
</bodyText>
<page confidence="0.998564">
414
</page>
<sectionHeader confidence="0.99648" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.988518">
Research reported in this paper has been partially
funded by Ministerio de Economfa y Competitivi-
dad and FEDER (Grant TIN2010-18552-C03-02)
and by Xunta de Galicia (Grant CN2012/008).
</bodyText>
<sectionHeader confidence="0.998498" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807555555556">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of Twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ’11, pages
30–38, Stroudsburg, PA, USA. ACL.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. AVAYA: Sentiment Analysis on
Twitter with Self-Training and Polarity Lexicon Ex-
pansion. Atlanta, Georgia, USA, page 333.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265–292.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. The Journal
of Machine Learning Research, 9:1871–1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10–18, November.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Manesh Joshi and Carolyn Penstein-Ros´e. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ’09, pages 313–316,
Suntec, Singapore.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. pages 312–320, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
A. Pak and P. Paroubek. 2010. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC’10),
pages 1320–1326, Valletta, Malta, May. European
Language Resources Association (ELRA).
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: LIWC 2001.
Mahway: Lawrence Erlbaum Associates, 71.
David Vilares, Miguel A. Alonso, and Carlos G´omez-
Rodr´ıguez. 2013a. LyS at TASS 2013: Analysing
Spanish tweets by means of dependency pars-
ing, semantic-oriented lexicons and psychometric
word-properties. In Alberto Dfaz Esteban, I˜naki
Alegrfa Loinaz, and Julio Villena Rom´an, editors,
XXIX Congreso de la Sociedad Espa˜nola de Proce-
samiento de Lenguaje Natural (SEPLN 2013). TASS
2013 - Workshop on Sentiment Analysis at SEPLN
2013, pages 179–186, Madrid, Spain, September.
David Vilares, Miguel A. Alonso, and Carlos G´omez-
Rodr´ıguez. 2013b. Supervised polarity classifica-
tion of Spanish tweets based on linguistic knowl-
edge. In DocEng’13. Proceedings of the 13th ACM
Symposium on Document Engineering, pages 169–
172, Florence, Italy, September. ACM.
David Vilares, Miguel A. Alonso, and Carlos G´omez-
Rodr´ıguez. 2014. On the usefulness of lexical
and syntactic processing in polarity classification of
Twitter messages. Journal of the Association for In-
formation Science Science and Technology, to ap-
pear.
Julio Villena-Rom´an, Janine Garcfa-Morera, Cristina
Moreno-Garcfa, Sara Lana-Serrano, and Jos´e Carlos
Gonz´alez-Crist´obal. 2014. TASS 2013 — a sec-
ond step in reputation analysis in Spanish. Proce-
samiento del Lenguaje Natural, 52:37–44, March.
</reference>
<page confidence="0.998561">
415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.305594">
<title confidence="0.8592645">LyS: Porting a Twitter Sentiment Analysis from Spanish to English</title>
<author confidence="0.8446565">David Vilares</author>
<author confidence="0.8446565">Miguel Hermo</author>
<author confidence="0.8446565">Miguel A Alonso</author>
<author confidence="0.8446565">Carlos G´omez-Rodriguez</author>
<author confidence="0.8446565">Yerai Grupo LyS</author>
<author confidence="0.8446565">Departamento de_Computaci´on</author>
<author confidence="0.8446565">Facultade de_da Campus de_A A Spain</author>
<email confidence="0.971384">miguel.hermo,miguel.alonso,carlos.gomez,</email>
<abstract confidence="0.999169095238095">This paper proposes an approach to solve messageand phrase-level polarity classification in Twitter, derived from an existing system designed for Spanish. As a step, an is performed. We then identify lexical, psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment. These features are used to feed a supervised classifier after applying an information gain filter, to discriminate irrelevant features. The system is evaluated on the SemEval 2014 task 9: Sentiment Analysis in Twitter. Our approach worked competitively both in messageand phraselevel tasks. The results confirm the robustness of the approach, which performed well on different domains involving short informal texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of Twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4488" citStr="Agarwal et al., 2011" startWordPosition="690" endWordPosition="693">val 2014), pages 411–415, Dublin, Ireland, August 23-24, 2014. • Replacement of frequent abbreviations The list of the most frequent ones was extracted from the training set, taking the Penn Treebank (Marcus et al., 1993) as our dictionary. A term is considered ungrammatical if it does not appear in our dictionary. We then carry out a manual review to distinguish between unknown words and abbreviations, providing a correction in the latter case. For example, ‘c’mon’ becomes ‘come on’ and ‘Sat’ is replaced by ‘Saturday’. • Emoticon normalisation: We employ the emoticon collection published in (Agarwal et al., 2011). Each emoticon is replaced with one of these five labels: strong positive (ESP), positive (EP), neutral (ENEU), negative (EN) or strong negative (ESN). • Laughs: Multiple forms used in social media to reflect laughs (e.g. ‘hhahahha’, ‘HHEHEHEH’) are preprocessed in a homogeneous way to obtain a pattern of the form ‘hxhx’ where x E {a, e, i, o, u}. • URL normalisation: External links are replaced by the string ‘url’. • Hashtags (‘#’) and usernames (‘@’): If the hashtag appears at the end or beginning of the tweet, we remove the hashtag. Based on other participant approaches at SemEval 2013 (Na</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of Twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Becker</author>
<author>George Erhart</author>
<author>David Skiba</author>
<author>Valentine Matula</author>
</authors>
<title>AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion.</title>
<date>2013</date>
<pages>333</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="11997" citStr="Becker et al. (2013)" startWordPosition="1931" endWordPosition="1934"> 556 Train 3,063 1,202 3,935 B Dev 493 290 633 Test 3,506 1,541 3,940 Table 2: SemEval 2014 corpus statistics. 2.4 Classifier 3.1 Evaluation Metrics We have trained our runs with a SVM LibLINEAR F-measure is the official score to measure how sysclassifier (Fan et al., 2008) taking the implementa- tems behave on each class. In order to rank partiction provided in WEKA (Hall et al., 2009). The ipants, the SemEval 2014 organisation proposed selection was motivated by the acceptable results the averaged F-measure of positive and negative that some of the participants in SemEval 2013, e.g. tweets. Becker et al. (2013), obtained using this imple2We followed this angle. 413 3.2 Performance on Sets Tables 3 and 4 show performance on the test set of different combinations of the proposed features. Table 5 shows the performance of our run on task A. The results over the corresponding sets for task B are illustrated in Table 6. They are significant lower than in task A. This suggests that when a message involves more than one of two tokens, a lexical approach is not enough. Improving performance should involve taking into account context and linguistic phenomena that appear in sentences to build a model based on</context>
</contexts>
<marker>Becker, Erhart, Skiba, Matula, 2013</marker>
<rawString>Lee Becker, George Erhart, David Skiba, and Valentine Matula. 2013. AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion. Atlanta, Georgia, USA, page 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--265</pages>
<contexts>
<context position="9911" citStr="Crammer and Singer (2002)" startWordPosition="1571" endWordPosition="1574">elevant features based on their information gain for our best model. The top features for task A were very similar. Our official runs only consider features with an IG greater than zero. IG Feature Category 0.140 positive emotion Pennebaker et al. (2001) 0.137 #positive-words Hu and Liu (2004) 0.126 affect Pennebaker et al. (2001) 0.089 #negative-words Hu and Liu (2004) 0.083 negative emotion Pennebaker et al. (2001) Table 1: Most relevant features for task B. ‘#’ must be read this table as ‘the number of’and not as a hashtag. mentation. We configured the multi-class support vector machine by Crammer and Singer (2002) as the SVMtype. Since the corpus was unbalanced, we tuned the weights for the classes using the development corpus: 1 for the positive class, 2 for negative and 0.5 for neutral. The rest of parameters were set to default values. 3 Experimental Results The SemEval 2014 organisation provides a standard training corpus for both tasks A an B. For task A, each tweet is marked with a list of the words and phrases to analyse, and for each one its sentiment label is provided. In addition, a development corpus was released for tuning the system parameters. The training and the development corpus can b</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11651" citStr="Fan et al., 2008" startWordPosition="1875" endWordPosition="1878">longer available for several reasons, e.g., user or a tweet does not exist anymore or the privacy settings of a user have changed. As a result, the size of our training and development corpora may be different from those of other participant’s corpora. Task Set Positive Negative Neutral Train 4,917 2,591 385 A Dev 555 365 45 Test 6,354 3,771 556 Train 3,063 1,202 3,935 B Dev 493 290 633 Test 3,506 1,541 3,940 Table 2: SemEval 2014 corpus statistics. 2.4 Classifier 3.1 Evaluation Metrics We have trained our runs with a SVM LibLINEAR F-measure is the official score to measure how sysclassifier (Fan et al., 2008) taking the implementa- tems behave on each class. In order to rank partiction provided in WEKA (Hall et al., 2009). The ipants, the SemEval 2014 organisation proposed selection was motivated by the acceptable results the averaged F-measure of positive and negative that some of the participants in SemEval 2013, e.g. tweets. Becker et al. (2013), obtained using this imple2We followed this angle. 413 3.2 Performance on Sets Tables 3 and 4 show performance on the test set of different combinations of the proposed features. Table 5 shows the performance of our run on task A. The results over the c</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11766" citStr="Hall et al., 2009" startWordPosition="1896" endWordPosition="1899">er have changed. As a result, the size of our training and development corpora may be different from those of other participant’s corpora. Task Set Positive Negative Neutral Train 4,917 2,591 385 A Dev 555 365 45 Test 6,354 3,771 556 Train 3,063 1,202 3,935 B Dev 493 290 633 Test 3,506 1,541 3,940 Table 2: SemEval 2014 corpus statistics. 2.4 Classifier 3.1 Evaluation Metrics We have trained our runs with a SVM LibLINEAR F-measure is the official score to measure how sysclassifier (Fan et al., 2008) taking the implementa- tems behave on each class. In order to rank partiction provided in WEKA (Hall et al., 2009). The ipants, the SemEval 2014 organisation proposed selection was motivated by the acceptable results the averaged F-measure of positive and negative that some of the participants in SemEval 2013, e.g. tweets. Becker et al. (2013), obtained using this imple2We followed this angle. 413 3.2 Performance on Sets Tables 3 and 4 show performance on the test set of different combinations of the proposed features. Table 5 shows the performance of our run on task A. The results over the corresponding sets for task B are illustrated in Table 6. They are significant lower than in task A. This suggests t</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations, 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7431" citStr="Hu and Liu (2004)" startWordPosition="1166" endWordPosition="1169">We rely on two manually-build lexicons: • Pennebaker et al. (2001) psychometric dictionaries. Linguistic Inquiry and Word Count1 (LIWC) is a software which includes a semantic dictionary to measure how people use different kinds of words over a wide number of texts. It categorises terms into psychometric properties, which correspond to different dimensions of the human language. The dictionary relates terms with psychological properties (e.g. anger or anxiety), but also with topics (e.g. family, friends, religion) or even morphological features (e.g. future time, past time or exclamations). • Hu and Liu (2004) opinion lexicon. It is a collection of positive and negative words. Many of the occurrences are misspelled, since they often come from web environments. 2.2.3 Syntactic features We also parsed the tweets using MaltParser (Nivre et al., 2007) in order to obtain dependency triplets of the form (wi, arcij, wj), where wi is the head word wj, the dependent one and arcij the existing syntactic relation between them. We tried to incorporate generalised dependency triplets (Joshi 1http://www.liwc.net/ 412 and Penstein-Ros´e, 2009), following an enriched perspective presented in Vilares et al. (2014).</context>
<context position="9580" citStr="Hu and Liu (2004)" startWordPosition="1517" endWordPosition="1520"> before feeding our classifier. In particular, we rely on the information gain (IG) method to then rank the most relevant features. Information gain measures the relevance of an attribute with respect to a class. It takes values between 0 and 1, where a higher value implies a higher relevance. Table 1 shows the top five relevant features based on their information gain for our best model. The top features for task A were very similar. Our official runs only consider features with an IG greater than zero. IG Feature Category 0.140 positive emotion Pennebaker et al. (2001) 0.137 #positive-words Hu and Liu (2004) 0.126 affect Pennebaker et al. (2001) 0.089 #negative-words Hu and Liu (2004) 0.083 negative emotion Pennebaker et al. (2001) Table 1: Most relevant features for task B. ‘#’ must be read this table as ‘the number of’and not as a hashtag. mentation. We configured the multi-class support vector machine by Crammer and Singer (2002) as the SVMtype. Since the corpus was unbalanced, we tuned the weights for the classes using the development corpus: 1 for the positive class, 2 for negative and 0.5 for neutral. The rest of parameters were set to default values. 3 Experimental Results The SemEval 2014</context>
<context position="13152" citStr="Hu and Liu (2004)" startWordPosition="2135" endWordPosition="2138">c phenomena that appear in sentences to build a model based on the composition of linguistic information. Model LJ SMS Twitter Twitter Twitter 2013 2014 Sarcasm WPLT 82.21 82.32 84.82 81.69 71.19 (no IG) WPL 83.55 81.04 84.85 80.64 68.79 WPLT* 83.96 81.46 85.63 79.93 71.98 WP 78.53 80.97 80.34 73.35 74.18 P 75.70 78.74 73.58 65.75 71.82 W 61.58 65.45 64.56 59.16 62.93 L 66.04 64.11 62.96 53.81 61.26 T 47.07 51.37 71.82 43.64 49.37 Table 3: Performance on the test set for task A. The model marked with a * was our official run. W stands for features obtained from a bag-of-words approach, L from Hu and Liu (2004), P from Pennebaker et al. (2001) and T for fine-grained PoStags. They can be combined, e.g., a model named WP use both words and psychometric properties. Model LJ SMS Twitter Twitter Twitter 2013 2014 Sarcasm WPLT* 69.79 60.45 66.92 64.92 42.40 WPL 70.19 61.41 66.71 64.51 45.72 WP 66.84 60.22 65.29 63.90 45.90 WPLT 66.38 57.01 61.96 62.84 43.71 (no IG) W 65.12 56.00 62.87 62.64 48.75 P 63.42 54.80 60.05 57.66 54.20 T 45.99 35.85 46.53 45.99 48.58 L 57.53 45.14 48.80 44.48 49.14 Table 4: Performance on the test set for task B. 4 Conclusions This papers describes the participation of the LyS Re</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manesh Joshi</author>
<author>Carolyn Penstein-Ros´e</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>313--316</pages>
<location>Suntec, Singapore.</location>
<marker>Joshi, Penstein-Ros´e, 2009</marker>
<rawString>Manesh Joshi and Carolyn Penstein-Ros´e. 2009. Generalizing dependency features for opinion mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 313–316, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="4088" citStr="Marcus et al., 1993" startWordPosition="626" endWordPosition="629">ish. Tasks A and B are addressed from the same perspective, which is described below. 2.1 Preprocessing We implement a naive preprocessing algorithm which seeks to normalise some of the most common ungrammatical elements. It is intended for Twitter, but many of the issues addressed would also be valid in other domains: 411 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411–415, Dublin, Ireland, August 23-24, 2014. • Replacement of frequent abbreviations The list of the most frequent ones was extracted from the training set, taking the Penn Treebank (Marcus et al., 1993) as our dictionary. A term is considered ungrammatical if it does not appear in our dictionary. We then carry out a manual review to distinguish between unknown words and abbreviations, providing a correction in the latter case. For example, ‘c’mon’ becomes ‘come on’ and ‘Sat’ is replaced by ‘Saturday’. • Emoticon normalisation: We employ the emoticon collection published in (Agarwal et al., 2011). Each emoticon is replaced with one of these five labels: strong positive (ESP), positive (EP), neutral (ENEU), negative (EN) or strong negative (ESN). • Laughs: Multiple forms used in social media t</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<date>2013</date>
<booktitle>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</booktitle>
<pages>312--320</pages>
<contexts>
<context position="5105" citStr="Nakov et al., 2013" startWordPosition="796" endWordPosition="799">1). Each emoticon is replaced with one of these five labels: strong positive (ESP), positive (EP), neutral (ENEU), negative (EN) or strong negative (ESN). • Laughs: Multiple forms used in social media to reflect laughs (e.g. ‘hhahahha’, ‘HHEHEHEH’) are preprocessed in a homogeneous way to obtain a pattern of the form ‘hxhx’ where x E {a, e, i, o, u}. • URL normalisation: External links are replaced by the string ‘url’. • Hashtags (‘#’) and usernames (‘@’): If the hashtag appears at the end or beginning of the tweet, we remove the hashtag. Based on other participant approaches at SemEval 2013 (Nakov et al., 2013), we realized maybe this is not the best option, although we believe hashtags will not be useful in most of cases, since they refer to very specific events. Otherwise, only the ‘#’ is removed, hypothesising the hashtag is used to emphasise a term (e.g. ‘Matthew #Mcconaughey has won the Oscar’). 2.2 Feature Extraction Our approach only takes into account information extracted from the text, without considering any kind of meta-data. Extracted features combine lexical, psychological and semantic knowledge in order to build a linguistic model able to analyse tweets, but also other kinds of messag</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. pages 312–320, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="7673" citStr="Nivre et al., 2007" startWordPosition="1205" endWordPosition="1208">er a wide number of texts. It categorises terms into psychometric properties, which correspond to different dimensions of the human language. The dictionary relates terms with psychological properties (e.g. anger or anxiety), but also with topics (e.g. family, friends, religion) or even morphological features (e.g. future time, past time or exclamations). • Hu and Liu (2004) opinion lexicon. It is a collection of positive and negative words. Many of the occurrences are misspelled, since they often come from web environments. 2.2.3 Syntactic features We also parsed the tweets using MaltParser (Nivre et al., 2007) in order to obtain dependency triplets of the form (wi, arcij, wj), where wi is the head word wj, the dependent one and arcij the existing syntactic relation between them. We tried to incorporate generalised dependency triplets (Joshi 1http://www.liwc.net/ 412 and Penstein-Ros´e, 2009), following an enriched perspective presented in Vilares et al. (2014). A generalisation consists on backing off the words to more abstracted terms. For example, a valid dependency triplet for the phrase ‘awesome villain’ is (villain, modi�er, awesome), which could be generalised into (anger, modi�er, assent) by</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a Corpus for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1320--1326</pages>
<location>Valletta, Malta,</location>
<contexts>
<context position="6581" citStr="Pak and Paroubek, 2010" startWordPosition="1031" endWordPosition="1034">orpus-extracted features Given a corpus, we use it to extract the following set of features: • Word forms: A model based on this type of features is our baseline. Each single word is considered as a feature in order to feed the supervised classifier. This often becomes a simple and acceptable start point which obtains a decent performance. • Part-of-speech (PoS) information: some coarse-grained PoS-tags such as adjective or adverb are usually good indicators of subjective texts while some fine-grained PoS tags such as third person personal pronoun provide evidence of non-opinionated messages (Pak and Paroubek, 2010). 2.2.2 Lexicon-extracted features We also consider information obtained from external lexicons in order to capture linguistic information that can not be extracted from a training corpus by means of bag-of-words and PoS-tag models. We rely on two manually-build lexicons: • Pennebaker et al. (2001) psychometric dictionaries. Linguistic Inquiry and Word Count1 (LIWC) is a software which includes a semantic dictionary to measure how people use different kinds of words over a wide number of texts. It categorises terms into psychometric properties, which correspond to different dimensions of the h</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>A. Pak and P. Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 1320–1326, Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>M E Francis</author>
<author>R J Booth</author>
</authors>
<title>Linguistic inquiry and word count: LIWC</title>
<date>2001</date>
<journal>Mahway: Lawrence Erlbaum Associates,</journal>
<volume>71</volume>
<contexts>
<context position="6880" citStr="Pennebaker et al. (2001)" startWordPosition="1079" endWordPosition="1082">t point which obtains a decent performance. • Part-of-speech (PoS) information: some coarse-grained PoS-tags such as adjective or adverb are usually good indicators of subjective texts while some fine-grained PoS tags such as third person personal pronoun provide evidence of non-opinionated messages (Pak and Paroubek, 2010). 2.2.2 Lexicon-extracted features We also consider information obtained from external lexicons in order to capture linguistic information that can not be extracted from a training corpus by means of bag-of-words and PoS-tag models. We rely on two manually-build lexicons: • Pennebaker et al. (2001) psychometric dictionaries. Linguistic Inquiry and Word Count1 (LIWC) is a software which includes a semantic dictionary to measure how people use different kinds of words over a wide number of texts. It categorises terms into psychometric properties, which correspond to different dimensions of the human language. The dictionary relates terms with psychological properties (e.g. anger or anxiety), but also with topics (e.g. family, friends, religion) or even morphological features (e.g. future time, past time or exclamations). • Hu and Liu (2004) opinion lexicon. It is a collection of positive </context>
<context position="9540" citStr="Pennebaker et al. (2001)" startWordPosition="1511" endWordPosition="1514">any tokens. Thus, we carry out a filtering step before feeding our classifier. In particular, we rely on the information gain (IG) method to then rank the most relevant features. Information gain measures the relevance of an attribute with respect to a class. It takes values between 0 and 1, where a higher value implies a higher relevance. Table 1 shows the top five relevant features based on their information gain for our best model. The top features for task A were very similar. Our official runs only consider features with an IG greater than zero. IG Feature Category 0.140 positive emotion Pennebaker et al. (2001) 0.137 #positive-words Hu and Liu (2004) 0.126 affect Pennebaker et al. (2001) 0.089 #negative-words Hu and Liu (2004) 0.083 negative emotion Pennebaker et al. (2001) Table 1: Most relevant features for task B. ‘#’ must be read this table as ‘the number of’and not as a hashtag. mentation. We configured the multi-class support vector machine by Crammer and Singer (2002) as the SVMtype. Since the corpus was unbalanced, we tuned the weights for the classes using the development corpus: 1 for the positive class, 2 for negative and 0.5 for neutral. The rest of parameters were set to default values.</context>
<context position="13185" citStr="Pennebaker et al. (2001)" startWordPosition="2141" endWordPosition="2145"> sentences to build a model based on the composition of linguistic information. Model LJ SMS Twitter Twitter Twitter 2013 2014 Sarcasm WPLT 82.21 82.32 84.82 81.69 71.19 (no IG) WPL 83.55 81.04 84.85 80.64 68.79 WPLT* 83.96 81.46 85.63 79.93 71.98 WP 78.53 80.97 80.34 73.35 74.18 P 75.70 78.74 73.58 65.75 71.82 W 61.58 65.45 64.56 59.16 62.93 L 66.04 64.11 62.96 53.81 61.26 T 47.07 51.37 71.82 43.64 49.37 Table 3: Performance on the test set for task A. The model marked with a * was our official run. W stands for features obtained from a bag-of-words approach, L from Hu and Liu (2004), P from Pennebaker et al. (2001) and T for fine-grained PoStags. They can be combined, e.g., a model named WP use both words and psychometric properties. Model LJ SMS Twitter Twitter Twitter 2013 2014 Sarcasm WPLT* 69.79 60.45 66.92 64.92 42.40 WPL 70.19 61.41 66.71 64.51 45.72 WP 66.84 60.22 65.29 63.90 45.90 WPLT 66.38 57.01 61.96 62.84 43.71 (no IG) W 65.12 56.00 62.87 62.64 48.75 P 63.42 54.80 60.05 57.66 54.20 T 45.99 35.85 46.53 45.99 48.58 L 57.53 45.14 48.80 44.48 49.14 Table 4: Performance on the test set for task B. 4 Conclusions This papers describes the participation of the LyS Research Group (http://www.grupolys</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001. Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates, 71.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Vilares</author>
<author>Miguel A Alonso</author>
<author>Carlos G´omezRodr´ıguez</author>
</authors>
<title>LyS at TASS 2013: Analysing Spanish tweets by means of dependency parsing, semantic-oriented lexicons and psychometric word-properties. In Alberto Dfaz Esteban, I˜naki Alegrfa Loinaz,</title>
<date>2013</date>
<booktitle>XXIX Congreso de la Sociedad Espa˜nola de Procesamiento de Lenguaje Natural (SEPLN 2013). TASS 2013 - Workshop on Sentiment Analysis at SEPLN 2013,</booktitle>
<pages>179--186</pages>
<editor>and Julio Villena Rom´an, editors,</editor>
<location>Madrid, Spain,</location>
<marker>Vilares, Alonso, G´omezRodr´ıguez, 2013</marker>
<rawString>David Vilares, Miguel A. Alonso, and Carlos G´omezRodr´ıguez. 2013a. LyS at TASS 2013: Analysing Spanish tweets by means of dependency parsing, semantic-oriented lexicons and psychometric word-properties. In Alberto Dfaz Esteban, I˜naki Alegrfa Loinaz, and Julio Villena Rom´an, editors, XXIX Congreso de la Sociedad Espa˜nola de Procesamiento de Lenguaje Natural (SEPLN 2013). TASS 2013 - Workshop on Sentiment Analysis at SEPLN 2013, pages 179–186, Madrid, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilares</author>
<author>Miguel A Alonso</author>
<author>Carlos G´omezRodr´ıguez</author>
</authors>
<title>Supervised polarity classification of Spanish tweets based on linguistic knowledge.</title>
<date>2013</date>
<booktitle>In DocEng’13. Proceedings of the 13th ACM Symposium on Document Engineering,</booktitle>
<pages>169--172</pages>
<publisher>ACM.</publisher>
<location>Florence, Italy,</location>
<marker>Vilares, Alonso, G´omezRodr´ıguez, 2013</marker>
<rawString>David Vilares, Miguel A. Alonso, and Carlos G´omezRodr´ıguez. 2013b. Supervised polarity classification of Spanish tweets based on linguistic knowledge. In DocEng’13. Proceedings of the 13th ACM Symposium on Document Engineering, pages 169– 172, Florence, Italy, September. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilares</author>
<author>Miguel A Alonso</author>
<author>Carlos G´omezRodr´ıguez</author>
</authors>
<title>On the usefulness of lexical and syntactic processing in polarity classification of Twitter messages.</title>
<date>2014</date>
<journal>Journal of the Association for Information Science Science and Technology,</journal>
<note>to appear.</note>
<marker>Vilares, Alonso, G´omezRodr´ıguez, 2014</marker>
<rawString>David Vilares, Miguel A. Alonso, and Carlos G´omezRodr´ıguez. 2014. On the usefulness of lexical and syntactic processing in polarity classification of Twitter messages. Journal of the Association for Information Science Science and Technology, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Villena-Rom´an</author>
<author>Janine Garcfa-Morera</author>
<author>Cristina Moreno-Garcfa</author>
<author>Sara Lana-Serrano</author>
<author>Jos´e Carlos Gonz´alez-Crist´obal</author>
</authors>
<title>a second step in reputation analysis</title>
<date>2014</date>
<booktitle>in Spanish. Procesamiento del Lenguaje Natural,</booktitle>
<pages>52--37</pages>
<publisher>TASS</publisher>
<marker>Villena-Rom´an, Garcfa-Morera, Moreno-Garcfa, Lana-Serrano, Gonz´alez-Crist´obal, 2014</marker>
<rawString>Julio Villena-Rom´an, Janine Garcfa-Morera, Cristina Moreno-Garcfa, Sara Lana-Serrano, and Jos´e Carlos Gonz´alez-Crist´obal. 2014. TASS 2013 — a second step in reputation analysis in Spanish. Procesamiento del Lenguaje Natural, 52:37–44, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>