<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000183">
<note confidence="0.9525615">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 191-198.
</note>
<title confidence="0.9789085">
A Study on Richer Syntactic Dependencies for Structured Language
Modeling
</title>
<author confidence="0.995701">
Peng Xu
</author>
<affiliation confidence="0.9040935">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.924089">
Baltimore, MD 21218
</address>
<email confidence="0.998893">
xp@clsp.jhu.edu
</email>
<author confidence="0.924373">
Ciprian Chelba
</author>
<affiliation confidence="0.900461">
Microsoft Research
</affiliation>
<address confidence="0.9456775">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.998752">
chelba@microsoft.com
</email>
<author confidence="0.99859">
Frederick Jelinek
</author>
<affiliation confidence="0.962841">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.833147">
Baltimore, MD 21218
</address>
<email confidence="0.996738">
jelinek@clsp.jhu.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929190476191">
We study the impact of richer syntac-
tic dependencies on the performance of
the structured language model (SLM)
along three dimensions: parsing accu-
racy (LP/LR), perplexity (PPL) and word-
error-rate (WER, N-best re-scoring). We
show that our models achieve an im-
provement in LP/LR, PPL and/or WER
over the reported baseline results us-
ing the SLM on the UPenn Treebank
and Wall Street Journal (WSJ) corpora,
respectively. Analysis of parsing per-
formance shows correlation between the
quality of the parser (as measured by pre-
cision/recall) and the language model per-
formance (PPL and WER). A remarkable
fact is that the enriched SLM outperforms
the baseline 3-gram model in terms of
WER by 10% when used in isolation as a
second pass (N-best re-scoring) language
model.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996955">
The structured language model uses hidden parse
trees to assign conditional word-level language
model probabilities. As explained in (Chelba and
Jelinek, 2000), Section 4.4.1, if the final best parse
is used to be the only parse, the reduction in PPL
—relative to a 3-gram baseline— using the SLM’s
headword parametrization for word prediction is
about 40%. The key to achieving this reduction is
a good guess of the final best parse for a given sen-
tence as it is being traversed left-to-right, which is
much harder than finding the final best parse for the
entire sentence, as it is sought by a regular statistical
parser. Nevertheless, it is expected that techniques
developed in the statistical parsing community that
aim at recovering the best parse for an entire sen-
tence, i.e. as judged by a human annotator, should
also be productive in enhancing the performance of
a language model that uses syntactic structure.
The statistical parsing community has used var-
ious ways of enriching the dependency structure
underlying the parametrization of the probabilistic
model used for scoring a given parse tree (Charniak,
2000) (Collins, 1999). Recently, such models (Char-
niak, 2001) (Roark, 2001) have been shown to out-
perform the SLM in terms of both PPL and WER on
the UPenn Treebank and WSJ corpora, respectively.
In (Chelba and Xu, 2001), a simple way of enriching
the probabilistic dependencies in the CONSTRUC-
TOR component of the SLM also showed better
PPL and WER performance; the simple modifica-
tion to the training procedure brought the WER per-
formance of the SLM to the same level with the best
as reported in (Roark, 2001).
In this paper, we present three simple ways of
enriching the syntactic dependency structure in the
SLM, extending the work in (Chelba and Xu, 2001).
The results show that an improved parser (as mea-
sured by LP/LR) is indeed helpful in reducing the
PPL and WER. Another remarkable fact is that for
the first time a language model exploiting elemen-
</bodyText>
<equation confidence="0.942004875">
h’_{-1} = h_{-2}
h’_0 = (h_{-1}.word, NTlabel)
h_{-1} h_0
T’_0
T’_{-m+1}&lt;-&lt;s&gt;
............... T’_{-1}&lt;-T_{-2}
&lt;s&gt;
T_{-1} T_0
</equation>
<bodyText confidence="0.999671">
tary syntactic dependencies obviates the need for
interpolation with a 3-gram model in N-best re-
scoring.
</bodyText>
<sectionHeader confidence="0.950835" genericHeader="method">
2 SLM Review
</sectionHeader>
<bodyText confidence="0.993713857142857">
An extensive presentation of the SLM can be found
in (Chelba and Jelinek, 2000). The model assigns a
probability to every sentence and ev-
ery possible binary parse . The terminals of
are the words of with POS tags, and the nodes
of are annotated with phrase headwords and non-
terminal labels. Let be a sentence of length
</bodyText>
<figureCaption confidence="0.987247">
Figure 2: Result of adjoin-left under NT label
</figureCaption>
<equation confidence="0.974244625">
h’_{-1}=h_{-2}
h’_0 = (h_0.word, NTlabel)
h_0
T’_{-m+1}&lt;-&lt;s&gt;
............... T’_{-1}&lt;-T_{-2} T_{-1} T_0
h_{-1}
&lt;s&gt;
(&lt;s&gt;, SB) (w_p, t_p) (w_{p+1}, t_{p+1}) (w_k, t_k) w_{k+1}.... &lt;/s&gt;
</equation>
<bodyText confidence="0.995787230769231">
words to which we have prepended the sentence be-
ginning marker &lt;s&gt; and appended the sentence end
marker &lt;/s&gt; so that &lt;s&gt; and &lt;/s&gt;.
Let be the word -prefix of the
sentence — the words from the beginning of the
sentence up to the current position — and
the word-parse -prefix. Figure 1 shows a word-
parse -prefix; h_0, .., h_{-m} are the ex-
posed heads, each head being a pair (headword, non-
terminal label), or (word, POS tag) in the case of a
root-only tree. The exposed heads at a given po-
sition in the input sentence are a function of the
word-parse -prefix.
</bodyText>
<subsectionHeader confidence="0.919403">
2.1 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.985028">
The joint probability of a word sequence
and a complete parse can be broken up into:
</bodyText>
<equation confidence="0.501047">
(1)
</equation>
<bodyText confidence="0.5804902">
is the word-parse -prefix
is the word predicted by WORD-PREDICTOR
is the tag assigned to by the TAGGER
is the number of operations the CON-
STRUCTOR executes at sentence position before
</bodyText>
<figureCaption confidence="0.995581">
Figure 3: Result of adjoin-right under NT label
</figureCaption>
<bodyText confidence="0.996440125">
passing control to the WORD-PREDICTOR (the
-th operation at position k is the null transi-
tion); is a function of
denotes the -th CONSTRUCTOR operation
carried out at position k in the word string; the oper-
ations performed by the CONSTRUCTOR are illus-
trated in Figures 2-3 and they ensure that all possi-
ble binary branching parses, with all possible head-
word and non-terminal label assignments for the
word sequence, can be generated. The
sequence of CONSTRUCTOR operations
at position grows the word-parse -prefix into
a word-parse -prefix.
The SLM is based on three probabilities, each es-
timated using deleted interpolation and parameter-
ized (approximated) as follows:
</bodyText>
<equation confidence="0.699243">
(4)
</equation>
<bodyText confidence="0.999729833333333">
It is worth noting that if the binary branching struc-
ture developed by the parser were always right-
branching and we mapped the POS tag and non-
terminal label vocabularies to a single type, then
our model would be equivalent to a trigram lan-
guage model. Since the number of parses for a
given word prefix grows exponentially with ,
, the state space of our model is
huge even for relatively short sentences, so we have
to use a search strategy that prunes it. One choice is
a synchronous multi-stack search algorithm which is
very similar to a beam search.
</bodyText>
<equation confidence="0.996344">
h_{-m} = (&lt;s&gt;, SB)
h_{-1} h_0 = (h_0.word, h_0.tag)
</equation>
<page confidence="0.624195">
where:
</page>
<figureCaption confidence="0.999276">
Figure 1: A word-parse -prefix
</figureCaption>
<bodyText confidence="0.5936215">
A
The language model probability assignment for
the word at position in the input sentence is
made using:
</bodyText>
<equation confidence="0.6836939">
Z
Z
Z°
B
Z°
Z°
Z°
Z°
Z°
(5)
</equation>
<bodyText confidence="0.988941307692308">
which ensures a proper probability normalization
over strings , where is the set of all parses
present in our stacks at the current stage .
Each model component —WORD-PREDICTOR,
TAGGER, CONSTRUCTOR— is initialized from a
set of parsed sentences after undergoing headword
percolation and binarization, see Section 2.2. An
N-best EM (Dempster et al., 1977) variant is then
employed to jointly reestimate the model parameters
such that the PPL on training data is decreased —
the likelihood of the training data under our model
is increased. The reduction in PPL is shown experi-
mentally to carry over to the test data.
</bodyText>
<subsectionHeader confidence="0.999635">
2.2 Headword Percolation And Binarization
</subsectionHeader>
<bodyText confidence="0.999981095238095">
As explained in the previous section, the SLM is ini-
tialized on parse trees that have been binarized and
the non-terminal (NT) tags at each node have been
enriched with headwords. We will briefly review the
headword percolation and binarization procedures;
they are explained in detail in (Chelba and Jelinek,
2000).
The position of the headword within a constituent
— equivalent to a context-free production of the type
, where are NT labels or
POS tags (only for ) — is specified using a rule-
based approach.
Assuming that the index of the headword on the
right-hand side of the rule is , we binarize the con-
stituent as follows: depending on the identity we
apply one of the two binarization schemes in Fig-
ure 4. The intermediate nodes created by the above
binarization schemes receive the NT label 1. The
choice among the two schemes is made according to
a list of rules based on the identity of the label on the
left-hand-side of a CF rewrite rule.
</bodyText>
<sectionHeader confidence="0.979686" genericHeader="method">
3 Enriching Syntactic Dependencies
</sectionHeader>
<bodyText confidence="0.9949625">
The SLM is a strict left-to-right, bottom-up parser,
therefore in Eq.( 2, 3, 4) the probabilities are con-
</bodyText>
<figure confidence="0.501384">
&apos;Any resemblance to X-bar theory is purely coincidental.
Y_1 Y_k Y_n Y_1 Y_k Y_n
</figure>
<figureCaption confidence="0.999419">
Figure 4: Binarization schemes
</figureCaption>
<bodyText confidence="0.999893173913044">
ditioned on the left contextual information. There
are two main reasons we prefer strict left-to-
right parsers for the purpose of language modeling
(Roark, 2001):
when looking for the most likely word string
given the acoustic signal (as required in a
speech recognizer), the search space is orga-
nized as a prefix tree. A language model whose
aim is to guide the search must thus operate
left-to-right.
previous results (Chelba and Jelinek,
2000) (Charniak, 2001) (Roark, 2001)
show that a grammar-based language model
benefits from interpolation with a 3-gram
model. Strict left-to-right parsing makes it
easy to combine with a standard 3-gram at the
word level (Chelba and Jelinek, 2000) (Roark,
2001) rather than at sentence level (Charniak,
2001).
For these reasons, we prefer enriching the syntactic
dependencies by information from the left context.
However, as mentioned in (Roark, 2001), one
way of conditioning the probabilities is by annotat-
ing the extra conditioning information onto the node
labels in the parse tree. We can annotate the training
corpus with richer information and with the same
SLM training procedure we can estimate the prob-
abilities under the richer syntactic tags. Since the
treebank parses allow us to annotate parent informa-
tion onto the constituents, as Johnson did in (John-
son, 1998), this richer predictive annotation can ex-
tend information slightly beyond the left context.
Under the equivalence classification in
Eq.( 2, 3, 4), the conditional information avail-
able to the SLM model components is made up of
the two most-recent exposed heads consisting of
two NT tags and two headwords. In an attempt to
extend the syntactic dependencies beyond this level,
we enrich the non-terminal tag of a node in the
binarized parse tree with the NT tag of the parent
node, or the NT tag of the child node from which
the headword is not being percolated (same as in
(Chelba and Xu, 2001)), or we add the NT tag of
the third most-recent exposed head to the history of
the CONSTRUCTOR component. The three ways
are briefly described as:
</bodyText>
<listItem confidence="0.9837301">
1. opposite (OP): we use the non-terminal tag of
the child node from which the headword is not
being percolated
2. parent (PA): we use the non-terminal tag of the
parent node to enrich the current node
3. h-2: we enrich the conditioning information
of the CONSTRUCTOR with the non-terminal
tag of the third most-recent exposed head, but
not the headword itself. Consequently, Eq. 4
becomes
</listItem>
<bodyText confidence="0.99980475">
We take the example from (Chelba and Xu, 2001)
to illustrate our enrichment approaches. Assume
that after binarization and headword percolation, we
have a noun phrase constituent:
</bodyText>
<equation confidence="0.937809666666667">
(NP_group
(DT the)
(NP’_group
(NNP dutch)
(NP’_group (VBG publishing)
(NN group)))),
</equation>
<bodyText confidence="0.978757">
which, after enriching the non-terminal tags using
the opposite and parent scheme, respectively, be-
comes
</bodyText>
<figure confidence="0.741038090909091">
(NP+DT_group
(DT the)
(NP’+NNP_group
(NNP dutch)
(NP’+VBG_group (VBG publishing)
(NN group))))
and
(NP+*_group
(DT+NP the)
(NP’+NP_group
(NNP+NP’ dutch)
</figure>
<footnote confidence="0.8330765">
(NP’+NP’_group (VBG+NP’ publishing)
(NN+NP’ group)))).
2The NP+* has not been enriched yet because we have not
specified the NT tag of the parent of the NP group
</footnote>
<bodyText confidence="0.999843065217391">
A given binarized tree is traversed recursively in
depth-first order and each constituent is enriched in
the parent or opposite manner or both. Then from
the resulting parse trees, all three components of the
SLM are initialized and N-best EM training can be
started.
Notice that both parent and opposite affect all
three components of the SLM since they change the
NT/POS vocabularies, but h-2 only affects the CON-
STRUCTOR component. So we believe that if h-2
helps in reducing PPL and WER, it’s because we
have thereby obtained a better parser. We should
also notice the difference between parent and op-
posite in the bottom-up parser. In opposite scheme,
POS (part of speech) tags are not enriched. As we
parse the sentence, two most-recent exposed heads
will be adjoined together under some enriched NT
label (Figure 2, 3), the NT label has to match the NT
tag of the child node from which the headword is
not being percolated. Since the NT tags of the chil-
dren are already known at the moment, the opposite
scheme actually restricts the possible NT labels. In
the parent scheme, POS tags are also enriched with
the NT tag of the parent node. When a POS tag is
predicted from the TAGGER, actually both the POS
tag and the NT tag of the parent node are hypoth-
esized. Then when two most recent exposed heads
are adjoined together under some enriched NT label,
the NT label has to match the parent NT informa-
tion carried in both of the exposed heads. In other
words, if the two exposed heads bear different in-
formation about their parents, they can never be ad-
joined. Since this restriction of adjoin movement is
very tight, pruning may delete some or all the good
parsing hypotheses early and the net result may be
later development of inadequate parses which lead
to poor language modeling and poor parsing perfor-
mance.
Since the SLM parses sentences bottom-up while
the parsers used in (Charniak, 2000), (Charniak,
2001) and (Roark, 2001) are top-down, it’s not
clear how to find a direct correspondence between
our schemes of enriching the dependency structure
and the ones employed above. However, it is their
“pick-and-choose” strategy that inspired our study
of richer syntactic dependencies for the SLM.
</bodyText>
<table confidence="0.999453666666667">
Model Word NT POS Parser
baseline &amp; h-2 10001 54 40 163
PA &amp; h-2+PA 10001 570 620 1711
OP &amp; h-2+OP 10001 970 40 2863
OP+PA &amp; 10001 3906 620 11719
h-2+OP+PA
</table>
<tableCaption confidence="0.999837">
Table 1: Vocabulary size comparison of the models
</tableCaption>
<sectionHeader confidence="0.99266" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999967375">
With the three enrichment schemes described in Sec-
tion 3 and their combinations, we evaluated the PPL
performance of the resulting seven models on the
UPenn Treebank and the WER performance on the
WSJ setup, respectively. In order to see the corre-
spondence between parsing accuracy and PPL/WER
performance, we also evaluated the labeled preci-
sion and recall statistics (LP/LR, the standard pars-
ing accuracy measures) on the UPenn Treebank cor-
pus. For every model component in our experi-
ments, deleted-interpolation was used for smooth-
ing. The interpolation weights were estimated from
separate held-out data. For example, in the UPenn
Treebank setup, we used section 00-20 as training
data, section 21-22 as held-out data, and section 23-
24 as test data.
</bodyText>
<subsectionHeader confidence="0.968572">
4.1 Perplexity
</subsectionHeader>
<bodyText confidence="0.999761944444444">
We have evaluated the perplexity of the seven dif-
ferent models, resulting from applying parent, op-
posite, h-2 and their combinations. For each way of
initializing the SLM we have performed 3 iterations
of N-best EM training. The SLM is interpolated
with a 3-gram model, built from exactly the same
training data and word vocabulary, using a fixed in-
terpolation weight. As we mentioned in Section 3,
the NT/POS vocabularies for the seven models are
different because of the enrichment of NT/POS tags.
Table 1 shows the actual vocabulary size we used for
each model (for parser, the vocabulary is a list of all
possible parser operations). The baseline model is
the standard SLM as described in (Chelba and Je-
linek, 2000).
The PPL results are summarized in Table 2. The
SLM is interpolated with a 3-gram model as shown
in the equation:
</bodyText>
<table confidence="0.988851941176471">
Model Iter =0.0 =0.4
baseline 0 167.4 151.9
baseline 3 158.7 148.7
PA 0 187.6 154.5
PA 3 164.5 149.5
OP 0 157.9 147.0
OP 3 151.2 144.2
OP+PA 0 185.2 152.1
OP+PA 3 162.2 147.3
h-2 0 161.4 149.2
h-2 3 159.4 148.2
h-2+PA 0 163.7 144.7
h-2+PA 3 160.5 143.9
h-2+OP 0 154.8 145.1
h-2+OP 3 153.6 144.4
h-2+OP+PA 0 165.7 144.1
h-2+OP+PA 3 165.4 143.8
</table>
<tableCaption confidence="0.920877">
Table 2: SLM PPL results
</tableCaption>
<table confidence="0.9999253">
Model Iter=0 Iter=3
LP LR LP LR
baseline 69.22 61.56 69.01 57.82
PA 79.84 45.46 81.20 39.52
OP 74.55 62.97 72.54 59.76
OP+PA 82.58 45.57 83.62 39.54
h-2 73.72 72.27 73.24 71.13
h-2+PA 75.59 70.93 74.93 70.56
h-2+OP 76.91 73.89 76.11 72.65
h-2+OP+PA 78.35 66.04 77.73 64.95
</table>
<tableCaption confidence="0.999694">
Table 3: Labeled precision/recall(%) results
</tableCaption>
<bodyText confidence="0.99929625">
We should note that the PPL result of the 3-gram
model is 166.6. As we can see from the table,
without interpolating with the 3-gram, the oppo-
site scheme performed the best, reducing the PPL
of the baseline SLM by almost 5% relative. When
the SLM is interpolated with the 3-gram, the h-
2+opposite+parent scheme performed the best, re-
ducing the PPL of the baseline SLM by 3.3%. How-
ever, the parent and opposite+parent schemes are
both worse than the baseline, especially before the
EM training and with =0.0. We will discuss the
results further in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.998364">
4.2 Parsing Accuracy Evaluation
</subsectionHeader>
<bodyText confidence="0.999790714285714">
Table 3 shows the labeled precision/recall accuracy
results. The labeled precision/recall results of our
model are much worse than those reported in (Char-
niak, 2001) and (Roark, 2001). One of the rea-
sons is that the SLM was not aimed at being a
parser, but rather a language model. Therefore, in
the search algorithm, the end-of-sentence symbol
</bodyText>
<table confidence="0.999064642857143">
Model Iter
0.0 0.2 0.4 0.6 0.8 1.0
baseline 0 13.0 13.1 13.1 13.1 13.0 13.4 13.7
PA 0 13.0 13.1 13.1 12.9 12.9 13.1 13.7
OP 0 12.8 12.7 12.8 12.8 12.7 13.1 13.7
OP+PA 0 13.1 13.3 12.9 13.0 12.9 13.1 13.7
h-2 0 12.5 12.7 12.5 12.6 12.9 13.2 13.7
h-2+ 0 12.7 12.8 13.0 12.7 12.7 13.0 13.7
PA
h-2+ 0 12.3 12.3 12.4 12.6 12.7 12.8 13.7
OP
h-2+ 0 12.6 12.6 12.4 12.5 12.7 12.9 13.7
OP+
PA
</table>
<tableCaption confidence="0.998907">
Table 4: N-best re-scoring WER(%) results
</tableCaption>
<bodyText confidence="0.999933066666667">
can be predicted before the parse of the sentence
is ready for completion3, thus completing the parse
with a series of special CONSTRUCTOR moves
(see (Chelba and Jelinek, 2000) for details). The
SLM allows right-branching parses which are not
seen in the UPenn Treebank corpus and thus the
evaluation against the UPenn Treebank is inherently
biased.
It can also be seen that both the LP and the LR
dropped after 3 training iterations: the N-best EM
variant used for SLM training algorithm increases
the likelihood of the training data, but it cannot guar-
antee an increase in LP/LR, since the re-estimation
algorithm does not explicitly use parsing accuracy
as a criterion.
</bodyText>
<subsectionHeader confidence="0.999624">
4.3 N-best Re-scoring Results
</subsectionHeader>
<bodyText confidence="0.999933166666667">
To test our enrichment schemes in the context of
speech recognition, we evaluated the seven models
in the WSJ DARPA’93 HUB1 test setup. The same
setup was also used in (Roark, 2001), (Chelba and
Jelinek, 2000) and (Chelba and Xu, 2001). The size
of the test set is 213 utterances, 3446 words. The 20k
words open vocabulary and baseline 3-gram model
are the standard ones provided by NIST and LDC —
see (Chelba and Jelinek, 2000) for details. The lat-
tices and N-best lists were generated using the stan-
dard 3-gram model trained on 45M words of WSJ.
The N-best size was at most 50 for each utterance,
</bodyText>
<footnote confidence="0.9298464">
3A parse is ready for completion when at the end of the
sentence there are exactly two exposed headwords, the first of
which if the start-of-sentence symbol and the second is an or-
dinary word. See (Chelba and Jelinek, 2000) for details about
special rules.
</footnote>
<bodyText confidence="0.999804136363636">
and the average size was about 23. The SLM was
trained on 20M words of WSJ text automatically
parsed using the parser in (Ratnaparkhi, 1997), bi-
narized and enriched with headwords and NT/POS
tag information as explained in Section 2.2 and Sec-
tion 3. Because SLM training on the 20M words of
WSJ text is very expensive, especially after enrich-
ing the NT/POS tags, we only evaluated the WER
performance of the seven models with initial statis-
tics from binarized and enriched parse trees. The
results are shown in Table 4. The table shows not
only the results according to different interpolation
weights , but also the results corresponding to ,
a virtual interpolation weight. We split the test data
into two parts, and . The best interpolation
weight, estimated from part , was used to decode
part , and vice versa. We finally put the decod-
ing results of the two parts together to get the final
decoding output. The interpolation weight is vir-
tual because the best interpolation weights for the
two parts might be different. Ideally, should be
estimated from separate held-out data and then ap-
plied to the test data. However, since we have a
small number of N-best lists, our approach should
be a good estimate of the WER under the ideal inter-
polation weight.
As can be seen, the h-2+opposite scheme
achieved the best WER result, with a 0.5% abso-
lute reduction over the performance of the opposite
scheme. Overall, the enriched SLM achieves 10%
relative reduction in WER over the 3-gram model
baseline result( ).
The SLM enriched with the h-2+opposite scheme
outperformed the 3-gram used to generate the lat-
tices and N-best lists, without interpolating it with
the 3-gram model. Although the N-best lists are al-
ready highly restricted by the 3-gram model during
the first recognition pass, this fact still shows the po-
tential of a good grammar-based language model.
In particular, we should notice that the SLM was
trained on 20M words of WSJ while the lattice 3-
gram was trained on 45M words of WSJ. However,
our results are not indicative of the performance of
SLM as a first pass language model.
</bodyText>
<subsectionHeader confidence="0.959952">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999894818181818">
By enriching the syntactic dependencies, we expect
the resulting models to be more accurate and thus
give better PPL results. However, in Table 2, we
can see that this is not always the case. For ex-
ample, the parent and opposite+parent schemes are
worse than baseline in the first iteration when =0.0,
the h-2+parent and h-2+opposite+parent schemes
are also worse than h-2 scheme in the first iteration
when =0.0.
Why wouldn’t more information help? There are
two possible reasons that come to mind:
</bodyText>
<listItem confidence="0.905993461538461">
1. Since the size of our training data is small
(1M words), the data sparseness problem (over-
parameterization) is more serious for the more
complicated dependency structure. We can see
the problem from Table 1: the NT/POS vo-
cabularies grow much bigger as we enrich the
NT/POS tags.
2. As mentioned in Section 3, a potential problem
of enriching NT/POS tags in parent scheme is
that pruning may delete some hypotheses at an
early time and the search may not recover from
those early mistakes. The result of this is a high
parsing error and thus a worse language model.
</listItem>
<bodyText confidence="0.998782111111111">
curacy also plays a role in these situations. The la-
beled recall results of parent and opposite+parent
are much worse than those of baseline and other
schemes. The end-of-sentence parse completion
strategy employed by the SLM is responsible for the
high precision/low recall operation of the parent and
opposite+parent models. Adding h-2 remedies the
parsing performance of the SLM in this situation,
but not sufficiently.
</bodyText>
<figure confidence="0.984775666666666">
PPL
baseline PA OP OP+PA h−2 h−2+PA h−2+OP h−2+OP+PA
180
160
13
12
WER
14
40
20
LR−Error
60
30
20
LP−Error
</figure>
<table confidence="0.982098666666667">
Model Iter=0 Iter=2
baseline 24.84 21.89
PA 29.00 22.63
OP 19.41 17.71
OP+PA 23.49 19.37
h-2 22.03 20.57
h-2+PA 19.64 18.20
h-2+OP 17.02 16.12
h-2+OP+PA 15.98 15.01
</table>
<tableCaption confidence="0.99485">
Table 5: PPL for training data
</tableCaption>
<bodyText confidence="0.997998461538462">
In order to validate the first hypothesis, we eval-
uated the training data PPL for each model scheme.
As can be seen from Table 5, over-parameterization
is indeed a problem. From scheme h-2 to h-
2+opposite+parent, as we add more information to
the conditioning context, the training data PPL de-
creases. The test data PPL in Table 2 does not
follow this trend, which is a clear sign of over-
parameterization.
Over-parameterization might also occur for par-
ent and opposite+parent, but it alone can not explain
the high PPL of training data for both schemes. The
LP/LR results in Table 3 show that bad parsing ac-
</bodyText>
<figureCaption confidence="0.802037">
Figure 5: Comparison of PPL, WER(%), Labeled
precision/recall(%) error
</figureCaption>
<bodyText confidence="0.999552487804878">
It is very interesting to note that labeled recall and
language model performance (WER/PPL) are well
correlated. Figure 5 compares PPL, WER ( =0.0
at training iteration 0) and labeled precision/recall
error(100-LP/LR) for all models. Overall, the la-
beled recall is well correlated with the WER and
PPL values. Our results show that improvement in
the parser accuracy is expected to lead to improve-
ment in WER.
Finally, in comparison with the language model
in (Roark, 2001) which is based on a probabilistic
top-down parser, and with the Bihead/Trihead lan-
guage models in (Charniak, 2001) which are based
on immediate head parsing, our enriched models are
less effective in reducing the test data PPL: the best
PPL result of (Roark, 2001) on the same experimen-
tal setup is 137.3, and the best PPL result of (Char-
niak, 2001) is 126.1. We believe that examining
the differences between the SLM and these models
could help in understanding the degradation:
1. The parser in (Roark, 2001) uses a “pick-and-
choose” strategy for the conditioning informa-
tion used in the probability models. This al-
lows the parser to choose information depend-
ing on the constituent that is being expanded.
The SLM, on the other hand, always uses the
same dependency structure that is decided be-
forehand.
2. The parser in (Charniak, 2001) is not a strict
left-to-right parser. Since it is top-down, it is
able to use the immediate head of a constituent
before it occurs, while this immediate head is
not available for conditioning by a strict left-
to-right parser such as the SLM. Consequently,
the interpolation with the 3-gram model is done
at the sentence level, which is weaker than in-
terpolating at the word level.
Since the WER results in (Roark, 2001) are based
on less training data (2.2M words total), we do not
have a fair comparison between our best model and
Roark’s model.
</bodyText>
<sectionHeader confidence="0.991121" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975896551724">
We have presented a study on enriching the syn-
tactic dependency structures in the SLM. We have
built and evaluated the performance of seven dif-
ferent models. All of our models improve on the
baseline SLM in either PPL or WER or both. We
have shown that adding the NT tag of the third most-
recent exposed head in the parser model improves
the parsing performance significantly. The improve-
ment in parsing accuracy carries over to enhanc-
ing language model performance, as evaluated by
both WER and PPL. Furthermore, our best result
shows that an uninterpolated grammar-based lan-
guage model can outperform a 3-gram model. The
best model achieved an overall WER improvement
of 10% relative to the 3-gram baseline.
Although conditioning on more contextual infor-
mation helps, we should note that some of our mod-
els suffer from over-parameterization. One solu-
tion would be to apply the maximum entropy esti-
mation technique (MaxEnt (Berger et al., 1996)) to
all of the three components of the SLM, or at least
to the CONSTRUCTOR. That would also allow for
fine-tuning of the particular syntactic dependencies
used in the model rather than the template based
method we have used. Along these lines, the Max-
Ent model has already shown promising improve-
ments by combining syntactic dependencies in the
WORD-PREDICTOR of the SLM (Wu and Khu-
danpur, 1999).
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905875">
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–72, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of NAACL,
pages 132–139, Seattle, WA.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting and 10th Conference ofthe European Chapter
ofACL, pages 116–123, Toulouse, France, July.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283–332, October.
Ciprian Chelba and Peng Xu. 2001. Richer syntactic
dependencies for structured language modeling. In
Proceedings of the Automatic Speech Recognition and
Understanding Workshop, Madonna di Campiglio,
Trento-Italy, December.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. In Journal of the Royal Statistical Society,
volume 39 of B, pages 1–38.
Mark Johnson. 1998. Pcfg models of linguistic tree
presentations. Computational Linguistics, 24(4):617–
636.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
Second Conference on Empirical Methods in Natural
Language Processing, pages 1–10, Providence, RI.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models and Applica-
tions. Ph.D. thesis, Brown University, Providence, RI.
Jun Wu and Sanjeev Khudanpur. 1999. Combining non-
local, syntactic and n-gram dependencies in language
modeling. In Proceedings of Eurospeech’99, pages
2179–2182.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.837528">
<note confidence="0.9974175">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 191-198.</note>
<title confidence="0.9912985">A Study on Richer Syntactic Dependencies for Structured Language Modeling</title>
<author confidence="0.999206">Peng Xu</author>
<affiliation confidence="0.9977045">Center for Language and Speech Processing Johns Hopkins University</affiliation>
<address confidence="0.999878">Baltimore, MD 21218</address>
<email confidence="0.999483">xp@clsp.jhu.edu</email>
<author confidence="0.908601">Ciprian Chelba</author>
<affiliation confidence="0.999742">Microsoft Research</affiliation>
<address confidence="0.994478">One Microsoft Way Redmond, WA 98052</address>
<email confidence="0.999455">chelba@microsoft.com</email>
<author confidence="0.999873">Frederick Jelinek</author>
<affiliation confidence="0.9976135">Center for Language and Speech Processing Johns Hopkins University</affiliation>
<address confidence="0.999918">Baltimore, MD 21218</address>
<email confidence="0.999847">jelinek@clsp.jhu.edu</email>
<abstract confidence="0.998340272727273">We study the impact of richer syntactic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accuracy (LP/LR), perplexity (PPL) and worderror-rate (WER, N-best re-scoring). We show that our models achieve an improvement in LP/LR, PPL and/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively. Analysis of parsing performance shows correlation between the quality of the parser (as measured by precision/recall) and the language model performance (PPL and WER). A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms of WER by 10% when used in isolation as a second pass (N-best re-scoring) language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2451" citStr="Charniak, 2000" startWordPosition="377" endWordPosition="378">arder than finding the final best parse for the entire sentence, as it is sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure</context>
<context position="13357" citStr="Charniak, 2000" startWordPosition="2219" endWordPosition="2220">cent exposed heads are adjoined together under some enriched NT label, the NT label has to match the parent NT information carried in both of the exposed heads. In other words, if the two exposed heads bear different information about their parents, they can never be adjoined. Since this restriction of adjoin movement is very tight, pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing performance. Since the SLM parses sentences bottom-up while the parsers used in (Charniak, 2000), (Charniak, 2001) and (Roark, 2001) are top-down, it’s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above. However, it is their “pick-and-choose” strategy that inspired our study of richer syntactic dependencies for the SLM. Model Word NT POS Parser baseline &amp; h-2 10001 54 40 163 PA &amp; h-2+PA 10001 570 620 1711 OP &amp; h-2+OP 10001 970 40 2863 OP+PA &amp; 10001 3906 620 11719 h-2+OP+PA Table 1: Vocabulary size comparison of the models 4 Experiments With the three enrichment schemes described in Section 3 and their combin</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of NAACL, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting and 10th Conference ofthe European Chapter ofACL,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2507" citStr="Charniak, 2001" startWordPosition="384" endWordPosition="386">entence, as it is sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work in (Chelba and Xu, 2001)</context>
<context position="8777" citStr="Charniak, 2001" startWordPosition="1456" endWordPosition="1457">n Eq.( 2, 3, 4) the probabilities are con&apos;Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n Figure 4: Binarization schemes ditioned on the left contextual information. There are two main reasons we prefer strict left-toright parsers for the purpose of language modeling (Roark, 2001): when looking for the most likely word string given the acoustic signal (as required in a speech recognizer), the search space is organized as a prefix tree. A language model whose aim is to guide the search must thus operate left-to-right. previous results (Chelba and Jelinek, 2000) (Charniak, 2001) (Roark, 2001) show that a grammar-based language model benefits from interpolation with a 3-gram model. Strict left-to-right parsing makes it easy to combine with a standard 3-gram at the word level (Chelba and Jelinek, 2000) (Roark, 2001) rather than at sentence level (Charniak, 2001). For these reasons, we prefer enriching the syntactic dependencies by information from the left context. However, as mentioned in (Roark, 2001), one way of conditioning the probabilities is by annotating the extra conditioning information onto the node labels in the parse tree. We can annotate the training corp</context>
<context position="13375" citStr="Charniak, 2001" startWordPosition="2221" endWordPosition="2222"> are adjoined together under some enriched NT label, the NT label has to match the parent NT information carried in both of the exposed heads. In other words, if the two exposed heads bear different information about their parents, they can never be adjoined. Since this restriction of adjoin movement is very tight, pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing performance. Since the SLM parses sentences bottom-up while the parsers used in (Charniak, 2000), (Charniak, 2001) and (Roark, 2001) are top-down, it’s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above. However, it is their “pick-and-choose” strategy that inspired our study of richer syntactic dependencies for the SLM. Model Word NT POS Parser baseline &amp; h-2 10001 54 40 163 PA &amp; h-2+PA 10001 570 620 1711 OP &amp; h-2+OP 10001 970 40 2863 OP+PA &amp; 10001 3906 620 11719 h-2+OP+PA Table 1: Vocabulary size comparison of the models 4 Experiments With the three enrichment schemes described in Section 3 and their combinations, we evaluat</context>
<context position="16931" citStr="Charniak, 2001" startWordPosition="2825" endWordPosition="2827">osite scheme performed the best, reducing the PPL of the baseline SLM by almost 5% relative. When the SLM is interpolated with the 3-gram, the h2+opposite+parent scheme performed the best, reducing the PPL of the baseline SLM by 3.3%. However, the parent and opposite+parent schemes are both worse than the baseline, especially before the EM training and with =0.0. We will discuss the results further in Section 4.4. 4.2 Parsing Accuracy Evaluation Table 3 shows the labeled precision/recall accuracy results. The labeled precision/recall results of our model are much worse than those reported in (Charniak, 2001) and (Roark, 2001). One of the reasons is that the SLM was not aimed at being a parser, but rather a language model. Therefore, in the search algorithm, the end-of-sentence symbol Model Iter 0.0 0.2 0.4 0.6 0.8 1.0 baseline 0 13.0 13.1 13.1 13.1 13.0 13.4 13.7 PA 0 13.0 13.1 13.1 12.9 12.9 13.1 13.7 OP 0 12.8 12.7 12.8 12.8 12.7 13.1 13.7 OP+PA 0 13.1 13.3 12.9 13.0 12.9 13.1 13.7 h-2 0 12.5 12.7 12.5 12.6 12.9 13.2 13.7 h-2+ 0 12.7 12.8 13.0 12.7 12.7 13.0 13.7 PA h-2+ 0 12.3 12.3 12.4 12.6 12.7 12.8 13.7 OP h-2+ 0 12.6 12.6 12.4 12.5 12.7 12.9 13.7 OP+ PA Table 4: N-best re-scoring WER(%) re</context>
<context position="24270" citStr="Charniak, 2001" startWordPosition="4094" endWordPosition="4095"> precision/recall(%) error It is very interesting to note that labeled recall and language model performance (WER/PPL) are well correlated. Figure 5 compares PPL, WER ( =0.0 at training iteration 0) and labeled precision/recall error(100-LP/LR) for all models. Overall, the labeled recall is well correlated with the WER and PPL values. Our results show that improvement in the parser accuracy is expected to lead to improvement in WER. Finally, in comparison with the language model in (Roark, 2001) which is based on a probabilistic top-down parser, and with the Bihead/Trihead language models in (Charniak, 2001) which are based on immediate head parsing, our enriched models are less effective in reducing the test data PPL: the best PPL result of (Roark, 2001) on the same experimental setup is 137.3, and the best PPL result of (Charniak, 2001) is 126.1. We believe that examining the differences between the SLM and these models could help in understanding the degradation: 1. The parser in (Roark, 2001) uses a “pick-andchoose” strategy for the conditioning information used in the probability models. This allows the parser to choose information depending on the constituent that is being expanded. The SLM</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting and 10th Conference ofthe European Chapter ofACL, pages 116–123, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="1485" citStr="Chelba and Jelinek, 2000" startWordPosition="215" endWordPosition="218">eline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively. Analysis of parsing performance shows correlation between the quality of the parser (as measured by precision/recall) and the language model performance (PPL and WER). A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms of WER by 10% when used in isolation as a second pass (N-best re-scoring) language model. 1 Introduction The structured language model uses hidden parse trees to assign conditional word-level language model probabilities. As explained in (Chelba and Jelinek, 2000), Section 4.4.1, if the final best parse is used to be the only parse, the reduction in PPL —relative to a 3-gram baseline— using the SLM’s headword parametrization for word prediction is about 40%. The key to achieving this reduction is a good guess of the final best parse for a given sentence as it is being traversed left-to-right, which is much harder than finding the final best parse for the entire sentence, as it is sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for a</context>
<context position="3627" citStr="Chelba and Jelinek, 2000" startWordPosition="573" endWordPosition="576">s of enriching the syntactic dependency structure in the SLM, extending the work in (Chelba and Xu, 2001). The results show that an improved parser (as measured by LP/LR) is indeed helpful in reducing the PPL and WER. Another remarkable fact is that for the first time a language model exploiting elemenh’_{-1} = h_{-2} h’_0 = (h_{-1}.word, NTlabel) h_{-1} h_0 T’_0 T’_{-m+1}&lt;-&lt;s&gt; ............... T’_{-1}&lt;-T_{-2} &lt;s&gt; T_{-1} T_0 tary syntactic dependencies obviates the need for interpolation with a 3-gram model in N-best rescoring. 2 SLM Review An extensive presentation of the SLM can be found in (Chelba and Jelinek, 2000). The model assigns a probability to every sentence and every possible binary parse . The terminals of are the words of with POS tags, and the nodes of are annotated with phrase headwords and nonterminal labels. Let be a sentence of length Figure 2: Result of adjoin-left under NT label h’_{-1}=h_{-2} h’_0 = (h_0.word, NTlabel) h_0 T’_{-m+1}&lt;-&lt;s&gt; ............... T’_{-1}&lt;-T_{-2} T_{-1} T_0 h_{-1} &lt;s&gt; (&lt;s&gt;, SB) (w_p, t_p) (w_{p+1}, t_{p+1}) (w_k, t_k) w_{k+1}.... &lt;/s&gt; words to which we have prepended the sentence beginning marker &lt;s&gt; and appended the sentence end marker &lt;/s&gt; so that &lt;s&gt; and &lt;/s&gt;.</context>
<context position="7426" citStr="Chelba and Jelinek, 2000" startWordPosition="1221" endWordPosition="1224"> 1977) variant is then employed to jointly reestimate the model parameters such that the PPL on training data is decreased — the likelihood of the training data under our model is increased. The reduction in PPL is shown experimentally to carry over to the test data. 2.2 Headword Percolation And Binarization As explained in the previous section, the SLM is initialized on parse trees that have been binarized and the non-terminal (NT) tags at each node have been enriched with headwords. We will briefly review the headword percolation and binarization procedures; they are explained in detail in (Chelba and Jelinek, 2000). The position of the headword within a constituent — equivalent to a context-free production of the type , where are NT labels or POS tags (only for ) — is specified using a rulebased approach. Assuming that the index of the headword on the right-hand side of the rule is , we binarize the constituent as follows: depending on the identity we apply one of the two binarization schemes in Figure 4. The intermediate nodes created by the above binarization schemes receive the NT label 1. The choice among the two schemes is made according to a list of rules based on the identity of the label on the </context>
<context position="8760" citStr="Chelba and Jelinek, 2000" startWordPosition="1452" endWordPosition="1455">ttom-up parser, therefore in Eq.( 2, 3, 4) the probabilities are con&apos;Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n Figure 4: Binarization schemes ditioned on the left contextual information. There are two main reasons we prefer strict left-toright parsers for the purpose of language modeling (Roark, 2001): when looking for the most likely word string given the acoustic signal (as required in a speech recognizer), the search space is organized as a prefix tree. A language model whose aim is to guide the search must thus operate left-to-right. previous results (Chelba and Jelinek, 2000) (Charniak, 2001) (Roark, 2001) show that a grammar-based language model benefits from interpolation with a 3-gram model. Strict left-to-right parsing makes it easy to combine with a standard 3-gram at the word level (Chelba and Jelinek, 2000) (Roark, 2001) rather than at sentence level (Charniak, 2001). For these reasons, we prefer enriching the syntactic dependencies by information from the left context. However, as mentioned in (Roark, 2001), one way of conditioning the probabilities is by annotating the extra conditioning information onto the node labels in the parse tree. We can annotate </context>
<context position="15374" citStr="Chelba and Jelinek, 2000" startWordPosition="2550" endWordPosition="2554">, opposite, h-2 and their combinations. For each way of initializing the SLM we have performed 3 iterations of N-best EM training. The SLM is interpolated with a 3-gram model, built from exactly the same training data and word vocabulary, using a fixed interpolation weight. As we mentioned in Section 3, the NT/POS vocabularies for the seven models are different because of the enrichment of NT/POS tags. Table 1 shows the actual vocabulary size we used for each model (for parser, the vocabulary is a list of all possible parser operations). The baseline model is the standard SLM as described in (Chelba and Jelinek, 2000). The PPL results are summarized in Table 2. The SLM is interpolated with a 3-gram model as shown in the equation: Model Iter =0.0 =0.4 baseline 0 167.4 151.9 baseline 3 158.7 148.7 PA 0 187.6 154.5 PA 3 164.5 149.5 OP 0 157.9 147.0 OP 3 151.2 144.2 OP+PA 0 185.2 152.1 OP+PA 3 162.2 147.3 h-2 0 161.4 149.2 h-2 3 159.4 148.2 h-2+PA 0 163.7 144.7 h-2+PA 3 160.5 143.9 h-2+OP 0 154.8 145.1 h-2+OP 3 153.6 144.4 h-2+OP+PA 0 165.7 144.1 h-2+OP+PA 3 165.4 143.8 Table 2: SLM PPL results Model Iter=0 Iter=3 LP LR LP LR baseline 69.22 61.56 69.01 57.82 PA 79.84 45.46 81.20 39.52 OP 74.55 62.97 72.54 59.7</context>
<context position="17713" citStr="Chelba and Jelinek, 2000" startWordPosition="2975" endWordPosition="2978">end-of-sentence symbol Model Iter 0.0 0.2 0.4 0.6 0.8 1.0 baseline 0 13.0 13.1 13.1 13.1 13.0 13.4 13.7 PA 0 13.0 13.1 13.1 12.9 12.9 13.1 13.7 OP 0 12.8 12.7 12.8 12.8 12.7 13.1 13.7 OP+PA 0 13.1 13.3 12.9 13.0 12.9 13.1 13.7 h-2 0 12.5 12.7 12.5 12.6 12.9 13.2 13.7 h-2+ 0 12.7 12.8 13.0 12.7 12.7 13.0 13.7 PA h-2+ 0 12.3 12.3 12.4 12.6 12.7 12.8 13.7 OP h-2+ 0 12.6 12.6 12.4 12.5 12.7 12.9 13.7 OP+ PA Table 4: N-best re-scoring WER(%) results can be predicted before the parse of the sentence is ready for completion3, thus completing the parse with a series of special CONSTRUCTOR moves (see (Chelba and Jelinek, 2000) for details). The SLM allows right-branching parses which are not seen in the UPenn Treebank corpus and thus the evaluation against the UPenn Treebank is inherently biased. It can also be seen that both the LP and the LR dropped after 3 training iterations: the N-best EM variant used for SLM training algorithm increases the likelihood of the training data, but it cannot guarantee an increase in LP/LR, since the re-estimation algorithm does not explicitly use parsing accuracy as a criterion. 4.3 N-best Re-scoring Results To test our enrichment schemes in the context of speech recognition, we e</context>
<context position="19062" citStr="Chelba and Jelinek, 2000" startWordPosition="3209" endWordPosition="3212">linek, 2000) and (Chelba and Xu, 2001). The size of the test set is 213 utterances, 3446 words. The 20k words open vocabulary and baseline 3-gram model are the standard ones provided by NIST and LDC — see (Chelba and Jelinek, 2000) for details. The lattices and N-best lists were generated using the standard 3-gram model trained on 45M words of WSJ. The N-best size was at most 50 for each utterance, 3A parse is ready for completion when at the end of the sentence there are exactly two exposed headwords, the first of which if the start-of-sentence symbol and the second is an ordinary word. See (Chelba and Jelinek, 2000) for details about special rules. and the average size was about 23. The SLM was trained on 20M words of WSJ text automatically parsed using the parser in (Ratnaparkhi, 1997), binarized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Section 3. Because SLM training on the 20M words of WSJ text is very expensive, especially after enriching the NT/POS tags, we only evaluated the WER performance of the seven models with initial statistics from binarized and enriched parse trees. The results are shown in Table 4. The table shows not only the results according</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Peng Xu</author>
</authors>
<title>Richer syntactic dependencies for structured language modeling.</title>
<date>2001</date>
<booktitle>In Proceedings of the Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio,</booktitle>
<location>Trento-Italy,</location>
<contexts>
<context position="2666" citStr="Chelba and Xu, 2001" startWordPosition="413" endWordPosition="416"> aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work in (Chelba and Xu, 2001). The results show that an improved parser (as measured by LP/LR) is indeed helpful in reducing the PPL and WER. Another remarkable fact is that for the first </context>
<context position="10229" citStr="Chelba and Xu, 2001" startWordPosition="1693" endWordPosition="1696">on did in (Johnson, 1998), this richer predictive annotation can extend information slightly beyond the left context. Under the equivalence classification in Eq.( 2, 3, 4), the conditional information available to the SLM model components is made up of the two most-recent exposed heads consisting of two NT tags and two headwords. In an attempt to extend the syntactic dependencies beyond this level, we enrich the non-terminal tag of a node in the binarized parse tree with the NT tag of the parent node, or the NT tag of the child node from which the headword is not being percolated (same as in (Chelba and Xu, 2001)), or we add the NT tag of the third most-recent exposed head to the history of the CONSTRUCTOR component. The three ways are briefly described as: 1. opposite (OP): we use the non-terminal tag of the child node from which the headword is not being percolated 2. parent (PA): we use the non-terminal tag of the parent node to enrich the current node 3. h-2: we enrich the conditioning information of the CONSTRUCTOR with the non-terminal tag of the third most-recent exposed head, but not the headword itself. Consequently, Eq. 4 becomes We take the example from (Chelba and Xu, 2001) to illustrate o</context>
<context position="18475" citStr="Chelba and Xu, 2001" startWordPosition="3102" endWordPosition="3105"> Treebank is inherently biased. It can also be seen that both the LP and the LR dropped after 3 training iterations: the N-best EM variant used for SLM training algorithm increases the likelihood of the training data, but it cannot guarantee an increase in LP/LR, since the re-estimation algorithm does not explicitly use parsing accuracy as a criterion. 4.3 N-best Re-scoring Results To test our enrichment schemes in the context of speech recognition, we evaluated the seven models in the WSJ DARPA’93 HUB1 test setup. The same setup was also used in (Roark, 2001), (Chelba and Jelinek, 2000) and (Chelba and Xu, 2001). The size of the test set is 213 utterances, 3446 words. The 20k words open vocabulary and baseline 3-gram model are the standard ones provided by NIST and LDC — see (Chelba and Jelinek, 2000) for details. The lattices and N-best lists were generated using the standard 3-gram model trained on 45M words of WSJ. The N-best size was at most 50 for each utterance, 3A parse is ready for completion when at the end of the sentence there are exactly two exposed headwords, the first of which if the start-of-sentence symbol and the second is an ordinary word. See (Chelba and Jelinek, 2000) for details </context>
</contexts>
<marker>Chelba, Xu, 2001</marker>
<rawString>Ciprian Chelba and Peng Xu. 2001. Richer syntactic dependencies for structured language modeling. In Proceedings of the Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio, Trento-Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2467" citStr="Collins, 1999" startWordPosition="379" endWordPosition="380">g the final best parse for the entire sentence, as it is sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, ext</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>In Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="6807" citStr="Dempster et al., 1977" startWordPosition="1119" endWordPosition="1122">thm which is very similar to a beam search. h_{-m} = (&lt;s&gt;, SB) h_{-1} h_0 = (h_0.word, h_0.tag) where: Figure 1: A word-parse -prefix A The language model probability assignment for the word at position in the input sentence is made using: Z Z Z° B Z° Z° Z° Z° Z° (5) which ensures a proper probability normalization over strings , where is the set of all parses present in our stacks at the current stage . Each model component —WORD-PREDICTOR, TAGGER, CONSTRUCTOR— is initialized from a set of parsed sentences after undergoing headword percolation and binarization, see Section 2.2. An N-best EM (Dempster et al., 1977) variant is then employed to jointly reestimate the model parameters such that the PPL on training data is decreased — the likelihood of the training data under our model is increased. The reduction in PPL is shown experimentally to carry over to the test data. 2.2 Headword Percolation And Binarization As explained in the previous section, the SLM is initialized on parse trees that have been binarized and the non-terminal (NT) tags at each node have been enriched with headwords. We will briefly review the headword percolation and binarization procedures; they are explained in detail in (Chelba</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. In Journal of the Royal Statistical Society, volume 39 of B, pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfg models of linguistic tree presentations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>636</pages>
<contexts>
<context position="9634" citStr="Johnson, 1998" startWordPosition="1590" endWordPosition="1592">ther than at sentence level (Charniak, 2001). For these reasons, we prefer enriching the syntactic dependencies by information from the left context. However, as mentioned in (Roark, 2001), one way of conditioning the probabilities is by annotating the extra conditioning information onto the node labels in the parse tree. We can annotate the training corpus with richer information and with the same SLM training procedure we can estimate the probabilities under the richer syntactic tags. Since the treebank parses allow us to annotate parent information onto the constituents, as Johnson did in (Johnson, 1998), this richer predictive annotation can extend information slightly beyond the left context. Under the equivalence classification in Eq.( 2, 3, 4), the conditional information available to the SLM model components is made up of the two most-recent exposed heads consisting of two NT tags and two headwords. In an attempt to extend the syntactic dependencies beyond this level, we enrich the non-terminal tag of a node in the binarized parse tree with the NT tag of the parent node, or the NT tag of the child node from which the headword is not being percolated (same as in (Chelba and Xu, 2001)), or</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Pcfg models of linguistic tree presentations. Computational Linguistics, 24(4):617– 636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<location>Providence, RI.</location>
<contexts>
<context position="19236" citStr="Ratnaparkhi, 1997" startWordPosition="3241" endWordPosition="3242">d by NIST and LDC — see (Chelba and Jelinek, 2000) for details. The lattices and N-best lists were generated using the standard 3-gram model trained on 45M words of WSJ. The N-best size was at most 50 for each utterance, 3A parse is ready for completion when at the end of the sentence there are exactly two exposed headwords, the first of which if the start-of-sentence symbol and the second is an ordinary word. See (Chelba and Jelinek, 2000) for details about special rules. and the average size was about 23. The SLM was trained on 20M words of WSJ text automatically parsed using the parser in (Ratnaparkhi, 1997), binarized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Section 3. Because SLM training on the 20M words of WSJ text is very expensive, especially after enriching the NT/POS tags, we only evaluated the WER performance of the seven models with initial statistics from binarized and enriched parse trees. The results are shown in Table 4. The table shows not only the results according to different interpolation weights , but also the results corresponding to , a virtual interpolation weight. We split the test data into two parts, and . The best interpolat</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Second Conference on Empirical Methods in Natural Language Processing, pages 1–10, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing: Motivations, Models and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="2521" citStr="Roark, 2001" startWordPosition="387" endWordPosition="388"> sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work in (Chelba and Xu, 2001). The results </context>
<context position="8475" citStr="Roark, 2001" startWordPosition="1406" endWordPosition="1407">above binarization schemes receive the NT label 1. The choice among the two schemes is made according to a list of rules based on the identity of the label on the left-hand-side of a CF rewrite rule. 3 Enriching Syntactic Dependencies The SLM is a strict left-to-right, bottom-up parser, therefore in Eq.( 2, 3, 4) the probabilities are con&apos;Any resemblance to X-bar theory is purely coincidental. Y_1 Y_k Y_n Y_1 Y_k Y_n Figure 4: Binarization schemes ditioned on the left contextual information. There are two main reasons we prefer strict left-toright parsers for the purpose of language modeling (Roark, 2001): when looking for the most likely word string given the acoustic signal (as required in a speech recognizer), the search space is organized as a prefix tree. A language model whose aim is to guide the search must thus operate left-to-right. previous results (Chelba and Jelinek, 2000) (Charniak, 2001) (Roark, 2001) show that a grammar-based language model benefits from interpolation with a 3-gram model. Strict left-to-right parsing makes it easy to combine with a standard 3-gram at the word level (Chelba and Jelinek, 2000) (Roark, 2001) rather than at sentence level (Charniak, 2001). For these</context>
<context position="13393" citStr="Roark, 2001" startWordPosition="2224" endWordPosition="2225">r under some enriched NT label, the NT label has to match the parent NT information carried in both of the exposed heads. In other words, if the two exposed heads bear different information about their parents, they can never be adjoined. Since this restriction of adjoin movement is very tight, pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing performance. Since the SLM parses sentences bottom-up while the parsers used in (Charniak, 2000), (Charniak, 2001) and (Roark, 2001) are top-down, it’s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above. However, it is their “pick-and-choose” strategy that inspired our study of richer syntactic dependencies for the SLM. Model Word NT POS Parser baseline &amp; h-2 10001 54 40 163 PA &amp; h-2+PA 10001 570 620 1711 OP &amp; h-2+OP 10001 970 40 2863 OP+PA &amp; 10001 3906 620 11719 h-2+OP+PA Table 1: Vocabulary size comparison of the models 4 Experiments With the three enrichment schemes described in Section 3 and their combinations, we evaluated the PPL perform</context>
<context position="16949" citStr="Roark, 2001" startWordPosition="2829" endWordPosition="2830">d the best, reducing the PPL of the baseline SLM by almost 5% relative. When the SLM is interpolated with the 3-gram, the h2+opposite+parent scheme performed the best, reducing the PPL of the baseline SLM by 3.3%. However, the parent and opposite+parent schemes are both worse than the baseline, especially before the EM training and with =0.0. We will discuss the results further in Section 4.4. 4.2 Parsing Accuracy Evaluation Table 3 shows the labeled precision/recall accuracy results. The labeled precision/recall results of our model are much worse than those reported in (Charniak, 2001) and (Roark, 2001). One of the reasons is that the SLM was not aimed at being a parser, but rather a language model. Therefore, in the search algorithm, the end-of-sentence symbol Model Iter 0.0 0.2 0.4 0.6 0.8 1.0 baseline 0 13.0 13.1 13.1 13.1 13.0 13.4 13.7 PA 0 13.0 13.1 13.1 12.9 12.9 13.1 13.7 OP 0 12.8 12.7 12.8 12.8 12.7 13.1 13.7 OP+PA 0 13.1 13.3 12.9 13.0 12.9 13.1 13.7 h-2 0 12.5 12.7 12.5 12.6 12.9 13.2 13.7 h-2+ 0 12.7 12.8 13.0 12.7 12.7 13.0 13.7 PA h-2+ 0 12.3 12.3 12.4 12.6 12.7 12.8 13.7 OP h-2+ 0 12.6 12.6 12.4 12.5 12.7 12.9 13.7 OP+ PA Table 4: N-best re-scoring WER(%) results can be predi</context>
<context position="18421" citStr="Roark, 2001" startWordPosition="3095" endWordPosition="3096">rpus and thus the evaluation against the UPenn Treebank is inherently biased. It can also be seen that both the LP and the LR dropped after 3 training iterations: the N-best EM variant used for SLM training algorithm increases the likelihood of the training data, but it cannot guarantee an increase in LP/LR, since the re-estimation algorithm does not explicitly use parsing accuracy as a criterion. 4.3 N-best Re-scoring Results To test our enrichment schemes in the context of speech recognition, we evaluated the seven models in the WSJ DARPA’93 HUB1 test setup. The same setup was also used in (Roark, 2001), (Chelba and Jelinek, 2000) and (Chelba and Xu, 2001). The size of the test set is 213 utterances, 3446 words. The 20k words open vocabulary and baseline 3-gram model are the standard ones provided by NIST and LDC — see (Chelba and Jelinek, 2000) for details. The lattices and N-best lists were generated using the standard 3-gram model trained on 45M words of WSJ. The N-best size was at most 50 for each utterance, 3A parse is ready for completion when at the end of the sentence there are exactly two exposed headwords, the first of which if the start-of-sentence symbol and the second is an ordi</context>
<context position="24155" citStr="Roark, 2001" startWordPosition="4076" endWordPosition="4077"> both schemes. The LP/LR results in Table 3 show that bad parsing acFigure 5: Comparison of PPL, WER(%), Labeled precision/recall(%) error It is very interesting to note that labeled recall and language model performance (WER/PPL) are well correlated. Figure 5 compares PPL, WER ( =0.0 at training iteration 0) and labeled precision/recall error(100-LP/LR) for all models. Overall, the labeled recall is well correlated with the WER and PPL values. Our results show that improvement in the parser accuracy is expected to lead to improvement in WER. Finally, in comparison with the language model in (Roark, 2001) which is based on a probabilistic top-down parser, and with the Bihead/Trihead language models in (Charniak, 2001) which are based on immediate head parsing, our enriched models are less effective in reducing the test data PPL: the best PPL result of (Roark, 2001) on the same experimental setup is 137.3, and the best PPL result of (Charniak, 2001) is 126.1. We believe that examining the differences between the SLM and these models could help in understanding the degradation: 1. The parser in (Roark, 2001) uses a “pick-andchoose” strategy for the conditioning information used in the probabilit</context>
<context position="25413" citStr="Roark, 2001" startWordPosition="4291" endWordPosition="4292">ormation depending on the constituent that is being expanded. The SLM, on the other hand, always uses the same dependency structure that is decided beforehand. 2. The parser in (Charniak, 2001) is not a strict left-to-right parser. Since it is top-down, it is able to use the immediate head of a constituent before it occurs, while this immediate head is not available for conditioning by a strict leftto-right parser such as the SLM. Consequently, the interpolation with the 3-gram model is done at the sentence level, which is weaker than interpolating at the word level. Since the WER results in (Roark, 2001) are based on less training data (2.2M words total), we do not have a fair comparison between our best model and Roark’s model. 5 Conclusion and Future Work We have presented a study on enriching the syntactic dependency structures in the SLM. We have built and evaluated the performance of seven different models. All of our models improve on the baseline SLM in either PPL or WER or both. We have shown that adding the NT tag of the third mostrecent exposed head in the parser model improves the parsing performance significantly. The improvement in parsing accuracy carries over to enhancing langu</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models and Applications. Ph.D. thesis, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Combining nonlocal, syntactic and n-gram dependencies in language modeling.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech’99,</booktitle>
<pages>2179--2182</pages>
<marker>Wu, Khudanpur, 1999</marker>
<rawString>Jun Wu and Sanjeev Khudanpur. 1999. Combining nonlocal, syntactic and n-gram dependencies in language modeling. In Proceedings of Eurospeech’99, pages 2179–2182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>