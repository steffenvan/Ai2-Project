<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004189">
<title confidence="0.996908">
Exploiting Parse Structures for Native Language Identification
</title>
<author confidence="0.995339">
Sze-Meng Jojo Wong
</author>
<affiliation confidence="0.868532666666667">
Centre for Language Technology
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.995096">
sze.wong@mq.edu.au
</email>
<author confidence="0.995583">
Mark Dras
</author>
<affiliation confidence="0.868219">
Centre for Language Technology
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.997785">
mark.dras@mq.edu.au
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99916">
Attempts to profile authors according to their
characteristics extracted from textual data, in-
cluding native language, have drawn attention
in recent years, via various machine learn-
ing approaches utilising mostly lexical fea-
tures. Drawing on the idea of contrastive
analysis, which postulates that syntactic er-
rors in a text are to some extent influenced by
the native language of an author, this paper
explores the usefulness of syntactic features
for native language identification. We take
two types of parse substructure as features—
horizontal slices of trees, and the more gen-
eral feature schemas from discriminative parse
reranking—and show that using this kind of
syntactic feature results in an accuracy score
in classification of seven native languages of
around 80%, an error reduction of more than
30%.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925319148936">
Inferring characteristics of authors from their tex-
tual data, often termed authorship profiling, has seen
a number of computational approaches proposed in
recent years. The problem is typically treated as a
classification task, where an author is classified with
respect to characteristics such as gender, age, native
language, and so on. This profile information is of-
ten of interest to marketing organisations for prod-
uct promotional reasons as well as governments or
law enforcements for crime investigation purposes.
The particular application that motivates the present
study is detection of phishing (Myers, 2007), the at-
tempt to defraud through texts that are designed to
deceive Internet users into giving away confidential
details. One class of countermeasures to phishing
consists of technical methods such as email authen-
tication; another looks at profiling of the text’s au-
thor(s) (Fette et al., 2007; Zheng et al., 2003), to
find any indications of the source of the text.
In this paper we investigate classification of a text
with respect to an author’s native language, where
this is not the language that that text is written in
(which is often the case in phishing); we refer to
this as native language identification. Initial work
by Koppel et al. (2005) was followed by Tsur and
Rappoport (2007), Estival et al. (2007), van Halteren
(2008), and Wong and Dras (2009). By and large,
the problem was tackled using various supervised
machine learning approaches, with mostly lexical
features over characters, words, and parts of speech,
as well as some document structure.
Syntactic features, in contrast, in particular those
that capture grammatical errors, which might po-
tentially be useful for this task, have received lit-
tle attention. Koppel et al. (2005) did suggest using
syntactic errors in their work but did not investigate
them in any detail. Wong and Dras (2009) noted
the relevance of the concept of contrastive analy-
sis (Lado, 1957), which postulates that native lan-
guage constructions lead to characteristic errors in a
second language. In their experimental work, how-
ever, they used only three manual syntactic construc-
tions drawn from the literature; an ANOVA analysis
showed a detectable effect, but they did not improve
classification accuracy over purely lexical features.
In this paper, we investigate syntactic features for
native language identification that are more general
</bodyText>
<page confidence="0.897385">
1600
</page>
<note confidence="0.9575045">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600–1610,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999004">
than, and that do not require the manual construction
of, the above approach. Taking the trees produced
by statistical parsers, we use tree cross-sections as
features in a machine learning approach to deter-
mine which ones characterise non-native speaker er-
rors. Specifically, we look at two types of parse
tree substructure to use as features: horizontal slices
of the trees—that is, characterising parse trees as
sets of context-free grammar production rules—and
the features schemas used in discriminative parse
reranking. The goal of the present study is therefore
to investigate the influence to which syntactic fea-
tures represented by parse structures would have on
the classification task of identifying an author’s na-
tive language relative to, and in combination with,
lexical features.
The remainder of this paper is structured as fol-
lows. In Section 2, we discuss some related work on
the two key topics of this paper: primarily on com-
parable work in native language identification, and
then on how the notion of contrastive analysis can be
applicable here. We then describe the models exam-
ined in Section 3, followed by experimental setup in
Section 4. Section 5 presents results, and Section 6
discussion of those results.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.991023">
2.1 Native Language Identification
</subsectionHeader>
<bodyText confidence="0.999934">
The earliest work on native language identification
in this classification paradigm is that of Koppel et
al. (2005), in which they deployed a machine learn-
ing approach to the task, using as features func-
tion words, character n-grams, and part-of-speech
(PoS) bi-grams, as well as some spelling mistakes.
With five different groups of English authors (of na-
tive languages Bulgarian, Czech, French, Russian,
and Spanish) selected from the first version of In-
ternational Corpus of Learner English (ICLE), they
gained a relatively high classification accuracy of
80%. Koppel et al. (2005) also suggested that syn-
tactic features (syntactic errors) might be useful fea-
tures, but only investigated this idea at a shallow
level by treating rare PoS bigrams as ungrammati-
cal structures.
Tsur and Rappoport (2007) replicated the work
of Koppel et al. (2005) to investigate the hypothe-
sis that the choice of words in second language writ-
ing is highly influenced by the frequency of native
language syllables — the phonology of the native
language. Approximating this by character bi-grams
alone, they managed to achieve a classification accu-
racy of 66%.
Native language is also amongst the characteris-
tics investigated in the task of authorship profiling
by Estival et al. (2007), as well as other demographic
and personality characteristics. This study used a va-
riety of lexical and document structure features. For
the native language identification classification task,
their model yielded a reasonably high accuracy of
84%, but this was over a set of only three languages
(Arabic, English and Spanish) and against a most
frequent baseline of 62.9%.
Another related work is that of van Halteren
(2008), who used the Europarl corpus of parliamen-
tary speeches. In Europarl, one original language
is transcribed, and the others translated from it; the
task was to identify the original language. On the
basis of frequency counts of word-based n-grams,
surprisingly high classification accuracies within the
range of 87-97% were achieved across six languages
(English, German, French, Dutch, Spanish, and Ital-
ian). This turns out, however, to be significantly
influenced by the use of particular phrases used by
speakers of different languages in the parliamentary
context (e.g. the way Germans typically address the
chamber).
To our knowledge, Wong and Dras (2009) is the
only work that has investigated the usefulness of
syntactic features for the task of native language
identification. They first replicated the work of
Koppel et al. (2005) with the three types of lex-
ical feature, namely function words, character n-
grams, and PoS bi-grams. They then examined the
literature on contrastive analysis (see Section 2.2),
from the field of second language acquisition, and
selected three syntactic errors commonly observed
in non-native English users—subject-verb disagree-
ment, noun-number disagreement and misuse of
determiners—that had been identified as being in-
fluenced by the native language. An ANOVA anal-
ysis showed that the native language identification
constructions were identifiable; however, the over-
all classification was not improved over the lexi-
cal features by using just the three manually de-
tected syntactic errors. The best overall accuracy re-
</bodyText>
<page confidence="0.982822">
1601
</page>
<bodyText confidence="0.999959285714286">
ported was 73.71%; this was on the second version
of ICLE, across seven languages (those of Koppel
et al. (2005), plus the two Asian languages Chinese
and Japanese).
As a possible approach that would improve the
classification accuracy over just the three manually
detected syntactic errors, Wong and Dras (2009)
suggested deploying (but did not carry out) an idea
put forward by Gamon (2004) (citing Baayen et al.
(1996)) for the related task of identifying the author
of a text: to use CFG production rules to characterise
syntactic structures used by authors.1 We note that
similar ideas have been used in the task of sentence
grammaticality judgement, which utilise parser out-
puts (both trees and by-products) as classification
features (Mutton et al., 2007; Sun et al., 2007; Fos-
ter et al., 2008; Wagner et al., 2009; Tetreault et al.,
2010; Wong and Dras, 2010). We combine this idea
with one we introduce in this paper, of using dis-
criminative reranking features as a broader charac-
terisation of the parse tree.
</bodyText>
<subsectionHeader confidence="0.999952">
2.2 Contrastive analysis
</subsectionHeader>
<bodyText confidence="0.999093315789474">
Contrastive analysis (Lado, 1957) was an early at-
tempt in the field of second language acquisition
to explain the kinds and source of errors that non-
native speakers make. It arose out of behaviourist
psychology, and saw language learning as an issue
of habit formation that could be inhibited by previ-
ous habits inculcated in learning the native language.
The theory was also tied to structural linguistics:
it compared the syntactic structures of the native
and second languages to find differences that might
cause learning difficulties. The Lado work postu-
lated the Contrastive Analysis Hypothesis (CAH),
claiming that “those elements which are similar to
[the learner’s] native language will be simple for
him, and those elements that are different will be
difficult”; the consequence is that there will be more
errors made in those difficult elements.
While contrastive analysis was influential at first,
it was increasingly noticed that many errors were
</bodyText>
<footnote confidence="0.9990595">
1It is not entirely clear how this might work for author-
ship identification: would the Bront¨e sisters, the corpus Gamon
worked with, have used a significant number of different syntac-
tic constructions from each other? In the context of native lan-
guage identification, however, constrastive analysis postulates
that this is exactly the case for the different classes.
</footnote>
<bodyText confidence="0.9999045625">
common across all language learners regardless of
native language, which could not be explained un-
der contrastive analysis. Corder (1967) then de-
scribed an alternative, error analysis, where con-
trastive analysis-style errors were seen as only one
type of error, ‘interlanguage’ or ‘interference’ er-
rors; other types were ‘intralingual’ and ‘develop-
mental’ errors, which are not specific to the native
language (Richards, 1971).
In an overview of contrastive analysis after the
emergence of error analysis, Wardhaugh (1970)
noted that there were two interpretations of the
CAH, termed the strong and weak forms. Under the
strong form, all errors were attributed to the native
language, and clearly that was not tenable in light of
error analysis evidence. In the weak form, these dif-
ferences have an influence but are not the sole deter-
minant of language learning difficulty. Wardhaugh
noted claims at the time that the hypothesis was no
longer useful in either the strong or the weak ver-
sion: “Such a claim is perhaps unwarranted, but a
period of quiescence is probable for CA itself”. This
appears to be the case, with the then-dominant error
analysis giving way to newer, more specialised theo-
ries of second language acquisition, such as the com-
petition model of MacWhinney and Bates (1989)
or the processability theory of Pienemann (1998).
Nevertheless, smaller studies specifically of inter-
language errors have continued to be carried out,
generally restricted in their scope to a specific gram-
matical aspect of English in which the native lan-
guage of the learners might have an influence. To
give some examples, Granger and Tyson (1996) ex-
amined the usage of connectors in English by a num-
ber of different native speakers – French, German,
Dutch, and Chinese; Vassileva (1998) investigated
the employment of first person singular and plural
by another different set of native speakers – Ger-
man, French, Russian, and Bulgarian; Slabakova
(2000) explored the acquisition of telicity marking
in English by Spanish and Bulgarian learners; Yang
and Huang (2004) studied the impact of the ab-
sence of grammatical tense in Chinese on the acqui-
sition of English tense-aspect system (i.e. telicity
marking); Franck et al. (2002) and Vigliocco et al.
(1996) specifically examined the usage of subject-
verb agreement in English by French and Spanish,
respectively. There are also a few teaching resources
</bodyText>
<page confidence="0.979197">
1602
</page>
<bodyText confidence="0.999979363636364">
for English language teachers that collate such phe-
nomena, such as that of Swan and Smith (2001).
NLP techniques and a probabilistic view of na-
tive language identification now let us revisit and
make use of the weak form of the CAH. Interlan-
guage errors, as represented by differences in parse
trees, may be characteristic of the native language
of a learner; we can use the occurrence of these to
come up with a revised likelihood of the native lan-
guage. In this paper, we use machine learning in a
prediction task as our approach to this.
</bodyText>
<sectionHeader confidence="0.991568" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.992875041666667">
This section describes the three basic models inves-
tigated: the lexical model, based on Koppel et al.
(2005), as the baseline; and then the two models that
exploit syntactic information. In Section 5 we look
at the performance of each model independently and
also in combination: to combine, we just concate-
nate feature vectors.
Lexical As Wong and Dras (2009), we replicate
the features of Koppel et al. (2005) to produce our
LEXICAL model. These are of three types: function
words,2 character n-grams, and PoS n-grams. We
follow Wong and Dras (2009) in resolving some un-
clear issues from Koppel et al. (2005). Specifically,
we use the same list of function words, left unspec-
ified in Koppel et al. (2005), that were empirically
determined by Wong and Dras (2009) to be the best
of three candidates; we used character bi-grams, as
the best performing n-grams, although this also had
been left unspecified by Koppel et al. (2005); and
we used the most frequently occurring PoS bi-grams
and tri-grams, obtained by using the Brill tagger pro-
vided in NLTK (Bird et al., 2009) being trained on
the Brown corpus. In total, there are 798 features
of this class with 398 function words, 200 most fre-
quently occurring character bi-grams, and 200 most
frequently occurring PoS bi-grams. Both function
words and PoS bi-grams have feature values of bi-
nary type; while for character bi-grams, the feature
value is the relative frequency. (These types of fea-
ture value are the best performing one for each lexi-
2As with most work in authorship profiling, only function
words are used, so that the result is not tied to a particular do-
main, and no clues are obtained from different topics that dif-
ferent authors might write about.
cal feature.)
We omitted the 250 rare bi-grams used by Koppel
et al. (2005), as an ablative analysis showed that they
contributed nothing to classification accuracy.
Production Rules Under this model (PROD-
RULE), we take as features horizontal slices of parse
trees, in effect treating them as sets of CFG produc-
tion rules. Feature values are binary. We look at
all possible rules as features, but also present results
for subsets of features chosen using feature selec-
tion. For each language in our dataset, we identify
the n rules most characteristic of the language using
Information Gain (IG). For m classes, we use the
formulation of Yang and Pedersen (1997):
</bodyText>
<equation confidence="0.9990595">
− �m
IG(r) = i=1 Pr (ci) log Pr (ci)
+Pr (r) Emi=1 Pr (ci|r) log Pr (ci|r)
+Pr (¯r) Em i=1 Pr (ci|¯r) log Pr (ci|¯r) (1)
</equation>
<bodyText confidence="0.999942740740741">
We also investigated simple frequencies, fre-
quency ratios, and pointwise mutual information; as
in much other work, IG performed best, so we do not
present results for the others. Bi-normal separation
(Forman, 2003), often competitive with IG, is only
suitable for binary classification.
It is worth noting that the production rules being
used here are all non-lexicalised ones, except those
lexicalised with function words and punctuation, to
avoid topic-related clues.
Reranking Features As opposed to the horizontal
parse production rules, features used for discrimina-
tive reranking are cross-sections of parse trees that
might capture other aspects of ungrammatical struc-
tures. For these we use the 13 feature schemas de-
scribed in Charniak and Johnson (2005), which were
inspired by earlier work in discriminative estimation
techniques, such as Johnson et al. (1999) and Collins
(2000). Examples of these feature schemas include
tuples covering head-to-head dependencies, preter-
minals together with their closest maximal projec-
tion ancestors, and subtrees rooted in the least com-
mon ancestor.
These feature schemas are not the only possible
ones—they were empirically selected for the spe-
cific purpose of augmenting the Charniak parser.
However, much subsequent work has tended to use
</bodyText>
<page confidence="0.91372">
1603
</page>
<bodyText confidence="0.999933222222222">
these same features, albeit sometimes with exten-
sions for specific purposes (e.g. Johnson and Ural
(2010) for the Berkeley parser (Petrov et al., 2006),
Ng et al. (2010) for the C&amp;C parser (Clark and Cur-
ran, 2007)). We also use this standard set, specif-
ically the set of instantiated feature schemas from
the parser from Charniak and Johnson (2005) as
trained on the Wall Street Journal (WSJ), which
gives 1,333,837 potential features.
</bodyText>
<sectionHeader confidence="0.995122" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.978249">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.978544710526316">
We use the International Corpus of Learner English
(ICLE) compiled by Granger et al. (2009) for the
precise purpose of studying the English writings of
non-native English learners from diverse countries.
All the contributors to the corpus are claimed to
possess similar English proficiency levels (ranging
from intermediate to advanced learners) and are in
the same age group (all in their twenties at the time
of corpus collection.) This was also the data used by
Koppel et al. (2005) and Tsur and Rappoport (2007),
although where they used the first version of the cor-
pus, we use version 2.
Briefly, the first version contains 11 sub-corpora
of English essays contributed by second-year and
third-year university students of different native lan-
guage backgrounds (mostly European and Slavic
languages) — Bulgarian, Czech, Dutch, Finnish,
French, German, Italian, Polish, Russian, Spanish,
and Swedish; the second version has been extended
to additional 5 other native languages (including
Asian languages) — Chinese, Japanese, Norwegian,
Turkish, and Tswana.
As per Wong and Dras (2009), we examine seven
languages, namely Bulgarian, Czech, French, Rus-
sian, Spanish, Chinese, and Japanese. For each na-
tive language, we randomly select from amongst es-
says with length of 500-1000 words. For the purpose
of the present study, we have 95 essays per native
language. For the same reason as highlighted by
Wong and Dras (2009), we intentionally use fewer
essays as compared to Koppel et al. (2005)3 with a
view to reserving more data for future work. We
divide these into training sets of 70 essays per lan-
3Koppel et al. (2005) took all 258 texts per language from
ICLE Version 1 and evaluated using 10-fold cross valiadation.
guage, with a held-out test set of 25 essays per
language. There are 17,718 training sentences and
6,791 testing sentences.
</bodyText>
<subsectionHeader confidence="0.972073">
4.2 Parsers
</subsectionHeader>
<bodyText confidence="0.999930866666667">
We use two parsers: the Stanford parser (Klein
and Manning, 2003) and the Charniak and John-
son (henceforth C&amp;J) parser (Charniak and Johnson,
2005). Both are widely used, and produce relatively
accurate parses: the Stanford parser gets a labelled
f-score of 85.61 on the WSJ, and the C&amp;J 91.09.
With the Stanford parser, there are 26,284 unique
parse production rules extractable from our ICLE
training set of 490 texts, while the C&amp;J parser pro-
duces 27,705. For reranking, we use only the C&amp;J
parser—since the parser stores these features during
parsing, we can use them directly as classification
features. On the ICLE training data, there are 6,230
features with frequency &gt;10, and 19,659 with fre-
quency &gt;5.
</bodyText>
<subsectionHeader confidence="0.993202">
4.3 Classifiers
</subsectionHeader>
<bodyText confidence="0.999948">
For our experiments we used a maximum entropy
(MaxEnt) machine learner, MegaM4 (fifth release)
by Hal Daum´e III. (We also used an SVM for com-
parison, but the results were uniformly worse, and
degraded more quickly as number of features in-
creased, so we only report the MaxEnt results here).
The classifier is tuned to obtain an optimal classifi-
cation model.
</bodyText>
<subsectionHeader confidence="0.985675">
4.4 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999955615384615">
Given our relatively small amount of data, we use k-
fold cross-validation, choosing k = 5. While testing
for statistical significance of classification results is
often not carried out in NLP, we do so here because
the quantity of data could raise questions about the
certainty of any effect. In an encyclopedic survey of
cross-validation in machine learning contexts, Re-
faeilzadeh et al. (2009) note that there is as yet no
universal standard for testing of statistical signifi-
cance; and that while more sophisticated techniques
have been proposed, none is more widely accepted
than a paired t-test over folds. We therefore use this
paired t-test over folds, as formulated of Alpaydin
</bodyText>
<footnote confidence="0.993023">
4MegaM is available on http://www.cs.utah.edu/
∼hal/megam/.
</footnote>
<page confidence="0.994299">
1604
</page>
<bodyText confidence="0.999795666666667">
(2004). Under this cross-validation, 5 separate train-
ing feature sets are constructed, excluding the test
fold; 3 folds are used for training, 1 fold for tuning
and 1 fold for testing.
We also use a held-out test set for comparison,
as it is well-known that cross-validation can over-
estimate prediction error (Hastie et al., 2009). We
do not carry out significance testing here—with this
held-out test set size (n = 125), two models would
have to differ by a great deal to be significant. We
only use it as a check on the effect of applying to
completely new data.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999833424242424">
Table 1 presents the results for the three models in-
dividually under cross-validation. The first point
to note is that PROD-RULE, under both parsers,
is a substantial improvement over LEXICAL when
(non-lexicalised) parse rules together with rules lex-
icalised with function words are used (rows marked
with * in Table 1), with the largest difference as
much as 77.75% for PROD-RULE[both]* (n = all)
versus 64.29% for LEXICAL; these differences with
respect to LEXICAL are statistically significant. (To
give an idea, the paired t-test standard error for this
largest difference is 2.52%.) In terms of error reduc-
tion, this is over 30%.
There appears to be no difference according to the
parser used, regardless of their differing accuracy on
the WSJ. Using the selection metric for PROD-RULE
without rules lexicalised with function words pro-
duces results all around those for LEXICAL; using
fewer reranking features is worse as the quality of
RERANKING declines as feature cut-offs are raised.
Another, somewhat surprising point is that the
RERANKING results are also generally around those
of LEXICAL even though like PROD-RULE they are
also using cross-sections of the parse tree. We con-
sider there might be two possible reasons for this.
The first is that the feature schemas used were orig-
inally chosen for the specific purpose of augment-
ing the performance of the Charniak parser; perhaps
others might be more appropriate here. The second
is that we selected only those instantiated feature
schemas that occurred in the WSJ, and then applied
them to ICLE. As the WSJ is filled with predomi-
nantly grammatical text, perhaps those that were not
</bodyText>
<table confidence="0.999297285714286">
Features MaxEnt
LEXICAL (n = 798) 64.29
PROD-RULE[Stanford] (n = 1000) 65.72
PROD-RULE[Stanford]* (n = 1000) 74.08
PROD-RULE[Stanford]* (n = all) 74.49
PROD-RULE[C&amp;J] (n = 1000) 62.25
PROD-RULE[C&amp;J]* (n = 1000) 71.84
PROD-RULE[C&amp;J]* (n = all) 71.63
PROD-RULE[both] (n = 2000) 67.96
PROD-RULE[both]* (n = 2000) 74.69
PROD-RULE[both]* (n = all) 77.75
RERANKING (all features) 67.96
RERANKING (&gt;5 counts) 66.33
RERANKING (&gt;10 counts) 64.90
</table>
<tableCaption confidence="0.9437385">
Table 1: Classification results based on 5-fold cross vali-
dation with parse rules as syntactic features (accuracy %)
</tableCaption>
<table confidence="0.998758285714286">
Features MaxEnt
Lexical features (n = 798) 75.43
PROD-RULE[Stanford] (n = 1000) 74.29
PROD-RULE[Stanford]* (n = 1000) 79.43
PROD-RULE[Stanford]* (n = all) 78.86
PROD-RULE[C&amp;J] (n = 1000) 73.71
PROD-RULE[C&amp;J] (n = 1000)* 79.43
PROD-RULE[C&amp;J] (n = all)* 80.00
PROD-RULE[both] (n = 2000) 77.71
PROD-RULE[both] (n = 2000)* 78.85
PROD-RULE[both] (n = all)* 80.00
RERANKING (all features) 77.14
RERANKING (&gt;5 counts) 76.57
RERANKING (&gt;10 counts) 75.43
</table>
<tableCaption confidence="0.980782">
Table 2: Classification results based on hold-out valida-
tion with parse rules as syntactic features (accuracy %)
</tableCaption>
<bodyText confidence="0.999227071428571">
seen on the WSJ are precisely those that might indi-
cate ungrammaticality. In contrast, the production
rules of PROD-RULE were selected only from the
ICLE training data.
Table 2 presents the results for the individual
models on the held-out test set. The results are gen-
erally higher than for cross-validation—this is not
surprising, as the texts are of the same type, but all
the training data is used (rather than the 1−1/k pro-
portion for cross-validation). Overall, the pattern is
still the same, with PROD-RULE best, then RERANK-
ING and LEXICAL broadly similar; as expected, no
differences are significant with this smaller dataset.
The gap has narrowed, but without significance test-
</bodyText>
<page confidence="0.974968">
1605
</page>
<table confidence="0.999547833333333">
Features MaxEnt
LEXICAL (n = 798) 64.29
LEXICAL + PROD-RULE[both] (n = 2000) 63.06
LEXICAL + PROD-RULE[both]* (n = 2000) 72.45
LEXICAL + PROD-RULE[both]* (n = all) 70.82
LEXICAL + RERANKING (n = all) 68.17
</table>
<tableCaption confidence="0.9831695">
Table 3: Classification results based on 5-fold cross vali-
dation for combined models (accuracy %)
</tableCaption>
<table confidence="0.9995715">
Features MaxEnt
LEXICAL (n = 798) 75.43
LEXICAL + PROD-RULE[both] (n = 2000) 80.57
LEXICAL + PROD-RULE[both]* (n = 2000) 81.14
LEXICAL + PROD-RULE[both]* (n = all) 81.71
LEXICAL + RERANKING (n = all) 76.00
</table>
<tableCaption confidence="0.999098">
Table 4: Classification results based on hold-out valida-
tion for combined models (accuracy %)
</tableCaption>
<bodyText confidence="0.999708416666667">
ing it is difficult to say whether this is a genuine
phenomenon. The accuracy rate for LEXICAL here
is in line with Wong and Dras (2009); and given
the smaller dataset and larger set of languages, also
broadly in line with Koppel et al. (2005).
Tables 3 and 4 present results for model combina-
tions. It can be seen that the model combinations do
not produce results better than PROD-RULE alone.
Combining all features (results not presented here)
seems to degrade the overall performance even of
the MegaM: perhaps we need to derive feature vec-
tors more compactly than by feature concatenation.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99996825">
As illustrated in the confusion matrices (Table 5
for the PROD-RULE model, and Table 6 for the
LEXICAL model), misclassifications occur largely in
Spanish and Slavic languages, Bulgarian and Rus-
sian in particular. Unsurprisingly, Chinese is al-
most completely identified since it comes from a
entirely different language family, Sino-Tibetan, as
compared to the rest of the languages which are from
the branches of the Indo-European family (with
Japanese as the exception). Japanese and French
also appear to be easily distinguished, which could
probably be attributed to their word order or sen-
tence structure which are, to some extent, quite dif-
ferent from English. Japanese is a ‘subject-object-
verb’ language; and French, although having the
same word order as English, heads of phrases in
</bodyText>
<table confidence="0.996506375">
BL CZ FR RU SP CN JP
BL [14] 6 2 3 - - -
CZ 1 [20] - 3 1 - -
FR - - [25] - - - -
RU 1 4 3 [17] - - -
SP 2 1 3 1 [18] - -
CN - - - - - [24] 1
JP - - - - 1 2 [22]
</table>
<tableCaption confidence="0.99485975">
Table 5: Confusion matrix based on all non-lexicalised
parse rules from both parsers on the held-out set
(BL:Bulgarian, CZ:Czech, FR:French, RU:Russian,
SP:Spanish, CN:Chinese, JP:Japanese)
</tableCaption>
<table confidence="0.99909325">
BL CZ FR RU SP CN JP
BL [14] 3 2 4 2 - -
CZ 6 [16] - 2 1 - -
FR 1 - [24] - - - -
RU 3 2 3 [16] 1 - -
SP 1 2 3 1 [17] - 1
CN - - - - - [24] 1
JP - - - - 1 3 [21]
</table>
<tableCaption confidence="0.963747333333333">
Table 6: Confusion matrix based on lexical features on
the held-out set (BL:Bulgarian, CZ:Czech, FR:French,
RU:Russian, SP:Spanish, CN:Chinese, JP:Japanese)
</tableCaption>
<bodyText confidence="0.94477595">
French typically come before modifiers as opposed
to English. Overall, the PROD-RULE model results
in fewer misclassifications compared to the LEXI-
CAL model; there are mostly only incremental im-
provements for each language, with perhaps the ex-
ception of the reduction in confusion in the Slavic
languages.
We looked at some of the data, to see what kind
of syntactic substructure is useful in classifying na-
tive language. Although using feature selection with
only 1000 features did not improve performance,
the information gain ranking does identify particu-
lar constructions as characteristic of one of the lan-
guages, and so are useful for inspection.
A phenomenon that the literature has noted as oc-
curring with Chinese speakers is that of the missing
determiner.5 This corresponds to a higher frequency
of NP rules without determiners. These rules may
be valid in other contexts, but are also used to de-
scribe ungrammatical constituents. One example is
</bodyText>
<footnote confidence="0.9585745">
5This does happen with native speakers of some other lan-
guages, such as Slavic ones, but not generally (from our knowl-
edge of the literature) with native speakers of others, such as
Romance ones.
</footnote>
<page confidence="0.934225">
1606
</page>
<table confidence="0.9996735">
Rules BL CZ Counts CN JP
FR RU SP
NNP → &lt;R&gt; 0 0 3 0 0 67 0
: →- 55 51 23 39 10 9 4
PRN → -LRB- X -RRB- 0 1 7 2 0 42 0
SYM → * 0 1 7 3 1 42 0
: → : 30 39 58 46 47 11 6
X → SYM 0 2 7 4 4 42 6
NP → NNP NNP NNS 0 3 1 0 0 31 0
S → S : S . 36 34 53 39 41 5 9
PP → VBG PP 9 15 16 12 13 54 13
: →... 16 13 39 11 24 1 3
</table>
<tableCaption confidence="0.99646">
Table 7: Top 10 rules for the Stanford parser according to Information Gain on the held-out set
</tableCaption>
<table confidence="0.553729952380952">
(ROOT
(S
(ROOT
(S
(NP
(NP (DT The) (NN development))
(PP (IN of)
(NP (NN country) (NN park))))
(VP (MD can)
(ADVP (RB directly))
(VP (VB elp)
(S
(VP (TO to)
(VP (VB alleviate)
(NP (NNS overcrowdedness)
(CC and)
(NN overpopulation))
(PP (IN in)
(NP (JJ urban)
(NN area))))))))
(. .)))
</table>
<figureCaption confidence="0.9906585">
Figure 1: Parse from Chinese-speaking authors, illustrat-
ing missing determiner
</figureCaption>
<figure confidence="0.981806384615384">
(PP (VBG According)
(PP (TO to)
(NP (NNP &lt;R&gt;))))
(/ /)
(NP
(NP (NN burning))
(PP (IN of)
(NP (JJ plastic)
(NN waste))))
(VP (VBZ generates)
(NP (JJ toxic)
(NNS by-products)))
(. .)))
</figure>
<figureCaption confidence="0.9958485">
Figure 2: Parse from Chinese-speaking authors, illustrat-
ing according to
</figureCaption>
<bodyText confidence="0.9991224">
NP → NN NN. In Figure 1 we give the parse (from
the Stanford parser) of the sentence The develop-
ment of country park can directly elp to alleviate
overcrowdedness and overpopulation in urban area.
The phrase country park should either have a deter-
miner or be plural (in which case the appropriate rule
would be NP → NN NNS). There is a similar phe-
nomenon with in urban area, although this is an in-
stance of the rule NP → JJ NN.
Another production rule that occurs typically—
in fact, almost exclusively—in the texts of native
Chinese speakers is PP → VBG PP (by the Stan-
ford parser), which almost always corresponds to the
phrase according to. In Figure 2 we give the parse
of a short sentence (According to &lt;R&gt;, burning of
</bodyText>
<page confidence="0.888868">
1607
</page>
<equation confidence="0.474171055555556">
(S1
(S
(ADVP (RB Overall))
(/ /)
(NP (NNP cyber))
(VP (VBD cafeis)
(NP (DT a) (JJ good) (NN place))
(PP (IN as)
(NP (JJ recreational)
(NNP centre)))
(PP (IN with)
(NP
(NP
(DT a) (NN bundle))
(PP (IN of)
(NP (JJ up-to-dated)
(NN information))))))
(. .)))
</equation>
<figureCaption confidence="0.998254">
Figure 3: Parse illustrating parser correction
</figureCaption>
<bodyText confidence="0.999892636363636">
plastic waste generates toxic by-products—&lt;R&gt;is
an in-text citation that was removed in the prepa-
ration of ICLE) that illustrates this particular con-
struction. It appears that speakers of Chinese fre-
quently use this phrase as a translation of g¯en j`u.
So in this case, what is identified is not the sort of
error that is of interest to contrastive analysis, but
just a particular construction that is characteristic of
a certain native speaker’s language, one that is per-
fectly grammatical but which is used relatively infre-
quently by others and has a slightly unusual analysis
by the parser.
We had expected to see more rules that displayed
obvious ungrammaticality, such as VP → DT IN.
However, both parsers appear to be good at ‘ig-
noring’ errors, and producing relatively grammati-
cal structures (albeit ones with different frequencies
for different native languages). Figure 3 gives the
C&amp;J parse for Overall, cyber cafeis a good place as
recreational centre with a bundle of up-to-dated in-
formation. The correction of up-to-dated rather than
up-to-date is straightforward, but the simple typo-
graphical error of running together cafe and is leads
to more complex problems for the parser. Neverthe-
less, the parser produces a solid grammatical tree,
specifically assigning the category VBD to the com-
pound cafeis. This appears to be because both the
Stanford and C&amp;J parsers have implicit linguistic
constraints such as assumptions about heads; these
are imposed even when the text does not provide ev-
idence for them.
We also present in Table 7 the top 10 rules chosen
under the IG feature selection for the Stanford parser
on the held-out set. A number of these, and those
ranked lower, are concerned with punctuation: these
seem unlikely to be related to native language, but
perhaps rather to how students of a particular lan-
guage background are taught. Others are more typi-
cal of the sorts of example we illustrated above: PP
→ VBG PP, for example, is typically connected to
the according to construction discussed in connec-
tion with Figure 2, and it can be seen that the dom-
inant frequency count there is for native Chinese
speakers (column 6 of the counts).
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999792862068966">
In this paper we have shown that, using cross-
sections of parse trees, we can improve above an al-
ready good baseline in the task of native language
identification. While we do not make any strong
claims for the Contrastive Analysis Hypothesis, the
usefulness of syntax in the context of this problem
does provide some support.
The best features arising from the classification
have been horizontal cross-sections of trees, rather
than the more general discriminative parse reranking
features that might have been expected to perform at
least as well. This relatively poorer performance by
the reranking features may be due to a number of
factors, all of which could be investigated in future
work. One is the use of feature schema instances that
did not appear in the largely grammatical WSJ; an-
other is the extension of feature schemas; and a third
is the use of a parser that does not enforce linguistic
constraints such as the Berkeley parser (Petrov et al.,
2006).
Examining some of the substructures showed
some errors that were expected; other constructions
that were grammatical, but were just characteris-
tic translations of constructions that were common
in the native language; and a large number where
grammatical errors were glossed over by the parser’s
linguistic constraints, suggesting another purpose
for further work with the Berkeley parser. Overall,
the use of these led to an error reduction in over 30%
</bodyText>
<page confidence="0.974282">
1608
</page>
<bodyText confidence="0.9991695">
in the cross-validation evaluation with significance
testing.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998615">
The authors would like to acknowledge the support
of ARC Linkage Grant LP0776267 and ARC Dis-
covery Grant DP1095443, and thank the reviewers
for useful feedback. Much gratitude is due to Mark
Johnson for his guidance on the extraction of rerank-
ing features.
</bodyText>
<sectionHeader confidence="0.998526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999699633333333">
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press, Cambridge, MA, USA.
Harald Baayen, Hans van Halteren, and Fiona Tweedie.
1996. Outside the Cave of Shadows: Using Syntactic
Annotation to Enhance Authorship Attribution. Liter-
ary and Linguistic Computing, 11(3):121–131.
Stephen Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python: Analyzing
Text with the Natural Language Toolkit. O’Reilly Me-
dia, Inc.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173–
180, Ann Arbor, Michigan.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493–552.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML’00), Stanford, CA.
Stephen P. Corder. 1967. The significance of learners’
errors. International Review of Applied Linguistics in
Language Teaching (IRAL), 5(4):161–170.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263–272.
Ian Fette, Norman Sadeh, and Anthony Tomasic. 2007.
Learning to detect phishing emails. In Proceedings of
the 16th International World Wide Web Conference.
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289–1305.
Jennifer Foster, Joachim Wagner, and Josef van Genabith.
2008. Adapting a WSJ-trained parser to grammati-
cally noisy text. In Proceedings of ACL-08: HLT,
Short Papers, pages 221–224, Columbus, Ohio.
Julie Franck, Gabriella Vigliocco, and Janet Nicol. 2002.
Subject-verb agreement errors in French and English:
The role of syntactic hierarchy. Language and Cogni-
tive Processes, 17(4):371–404.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING),
pages 611–617.
Sylviane Granger and Stephanie Tyson. 1996. Connec-
tor usage in the English essay writing of native and
non-native EFL speakers of English. World Englishes,
15(1):17–27.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. Springer.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Proceed-
ings of Human Language Technologies: the 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL-10), pages 665–668, Los Angeles, CA,
USA, June.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics (ACL’99), College Park, MD.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author’s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209–217. Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Brian MacWhinney and Elizabeth Bates. 1989. The
Crosslinguistic Study of Sentence Processing. Cam-
bridge University Press, New York, NY, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. GLEU: Automatic evaluation of
</reference>
<page confidence="0.889101">
1609
</page>
<reference confidence="0.999837453608248">
sentence-level fluency. In Proceedings of the 45th An-
nual Meeting of the Association of Computational Lin-
guistics, pages 344–351, Prague, Czech Republic.
Steven Myers. 2007. Introduction to phishing. In
Markus Jakobsson and Steven Myers, editors, Phish-
ing and Countermeasures: Understanding the In-
creasing Problem of Electronic Identity Theft. John
Wiley &amp; Sons, Inc., Hoboken, NJ, USA.
Dominick Ng, Matthew Honnibal, and James R. Cur-
ran. 2010. Reranking a Wide-Coverage CCG Parser.
In Proceedings of Australasian Language Technology
Association Workshop (ALTA’10), pages 90–98, Mel-
bourne, Australia.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL’06), pages
433–440, Sydney, Australia, July.
Manfred Pienemann. 1998. Language Processing and
Second Language Development: Processability The-
ory. John Benjamins, Amsterdam, The Netherlands.
Payam Refaeilzadeh, Lei Tang, and Huan Liu. 2009.
Cross-validation. In Ling Liu and M. Tamer ¨Ozsu, ed-
itors, Encyclopedia of Database Systems, pages 532–
538. Springer, US.
Jack C. Richards. 1971. A non-contrastive approach to
error analysis. ELT Journal, 25(3):204–219.
Roumyana Slabakova. 2000. L1 transfer revisited:
the L2 acquisition of telicity marking in English by
Spanish and Bulgarian native speakers. Linguistics,
38(4):739–770.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 81–88, Prague, Czech Repub-
lic.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher’s guide to interference and
other problems. Cambridge University Press, 2nd edi-
tion.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, ACLShort ’10, pages 353–
358. Association for Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9–16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937–944.
Irena Vassileva. 1998. Who am I/how are we in aca-
demic writing? A contrastive analysis of authorial
presence in English, German, French, Russian and
Bulgarian. International Journal of Applied Linguis-
tics, 8(2):163–185.
Garbriella Vigliocco, Brian Butterworth, and Merrill F.
Garrett. 1996. Subject-verb agreement in Spanish
and English: Differences in the role of conceptual con-
straints. Cognition, 61(3):261–298.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal, 26(3):474–490.
Richard Wardhaugh. 1970. The Contrastive Analysis
Hypothesis. TESOL Quarterly, 4(2):123–130.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53–61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proceedings of the Australasian Language Technology
Association Workshop 2010, pages 67–75, Melbourne,
Australia, December.
Suying Yang and Yue-Yuan Huang. 2004. The impact of
the absence of grammatical tense in L1 on the acqui-
sition of the tense-aspect system in L2. International
Review of Applied Linguistics in Language Teaching
(IRAL), 42(1):49–70.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the Fourteenth International Con-
ference on Machine Learning (ICML’97), pages 412–
420.
Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen.
2003. Authorship analysis in cybercrime investiga-
tion. In Intelligence and Security Informatics, volume
2665 of Lecture Notes in Computer Science, pages 59–
73. Springer-Verlag.
</reference>
<page confidence="0.989207">
1610
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.098921">
<title confidence="0.60624775">Exploiting Parse Structures for Native Language Identification Sze-Meng Jojo Centre for Language Macquarie</title>
<author confidence="0.542962">Sydney</author>
<email confidence="0.998388">sze.wong@mq.edu.au</email>
<author confidence="0.984345">Mark</author>
<affiliation confidence="0.649162">Centre for Language Macquarie Sydney,</affiliation>
<email confidence="0.997632">mark.dras@mq.edu.au</email>
<abstract confidence="0.99410705">Attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification. We take two types of parse substructure as features— horizontal slices of trees, and the more general feature schemas from discriminative parse reranking—and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80%, an error reduction of more than 30%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ethem Alpaydin</author>
</authors>
<title>Introduction to Machine Learning.</title>
<date>2004</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Alpaydin, 2004</marker>
<rawString>Ethem Alpaydin. 2004. Introduction to Machine Learning. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Hans van Halteren</author>
<author>Fiona Tweedie</author>
</authors>
<title>Outside the Cave of Shadows: Using Syntactic Annotation to Enhance Authorship Attribution. Literary and Linguistic Computing,</title>
<date>1996</date>
<marker>Baayen, van Halteren, Tweedie, 1996</marker>
<rawString>Harald Baayen, Hans van Halteren, and Fiona Tweedie. 1996. Outside the Cave of Shadows: Using Syntactic Annotation to Enhance Authorship Attribution. Literary and Linguistic Computing, 11(3):121–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="14663" citStr="Bird et al., 2009" startWordPosition="2316" endWordPosition="2319"> three types: function words,2 character n-grams, and PoS n-grams. We follow Wong and Dras (2009) in resolving some unclear issues from Koppel et al. (2005). Specifically, we use the same list of function words, left unspecified in Koppel et al. (2005), that were empirically determined by Wong and Dras (2009) to be the best of three candidates; we used character bi-grams, as the best performing n-grams, although this also had been left unspecified by Koppel et al. (2005); and we used the most frequently occurring PoS bi-grams and tri-grams, obtained by using the Brill tagger provided in NLTK (Bird et al., 2009) being trained on the Brown corpus. In total, there are 798 features of this class with 398 function words, 200 most frequently occurring character bi-grams, and 200 most frequently occurring PoS bi-grams. Both function words and PoS bi-grams have feature values of binary type; while for character bi-grams, the feature value is the relative frequency. (These types of feature value are the best performing one for each lexi2As with most work in authorship profiling, only function words are used, so that the result is not tied to a particular domain, and no clues are obtained from different topic</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Stephen Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="16854" citStr="Charniak and Johnson (2005)" startWordPosition="2679" endWordPosition="2682">so we do not present results for the others. Bi-normal separation (Forman, 2003), often competitive with IG, is only suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific p</context>
<context position="19859" citStr="Charniak and Johnson, 2005" startWordPosition="3158" endWordPosition="3161">ason as highlighted by Wong and Dras (2009), we intentionally use fewer essays as compared to Koppel et al. (2005)3 with a view to reserving more data for future work. We divide these into training sets of 70 essays per lan3Koppel et al. (2005) took all 258 texts per language from ICLE Version 1 and evaluated using 10-fold cross valiadation. guage, with a held-out test set of 25 essays per language. There are 17,718 training sentences and 6,791 testing sentences. 4.2 Parsers We use two parsers: the Stanford parser (Klein and Manning, 2003) and the Charniak and Johnson (henceforth C&amp;J) parser (Charniak and Johnson, 2005). Both are widely used, and produce relatively accurate parses: the Stanford parser gets a labelled f-score of 85.61 on the WSJ, and the C&amp;J 91.09. With the Stanford parser, there are 26,284 unique parse production rules extractable from our ICLE training set of 490 texts, while the C&amp;J parser produces 27,705. For reranking, we use only the C&amp;J parser—since the parser stores these features during parsing, we can use them directly as classification features. On the ICLE training data, there are 6,230 features with frequency &gt;10, and 19,659 with frequency &gt;5. 4.3 Classifiers For our experiments </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173– 180, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="17599" citStr="Clark and Curran, 2007" startWordPosition="2794" endWordPosition="2798">2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (ranging from intermediate to advanced learners) a</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language processing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning (ICML’00),</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="16981" citStr="Collins (2000)" startWordPosition="2700" endWordPosition="2701">ssification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language processing. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML’00), Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen P Corder</author>
</authors>
<title>The significance of learners’ errors.</title>
<date>1967</date>
<journal>International Review of Applied Linguistics in Language Teaching (IRAL),</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="10776" citStr="Corder (1967)" startWordPosition="1672" endWordPosition="1673">nts. While contrastive analysis was influential at first, it was increasingly noticed that many errors were 1It is not entirely clear how this might work for authorship identification: would the Bront¨e sisters, the corpus Gamon worked with, have used a significant number of different syntactic constructions from each other? In the context of native language identification, however, constrastive analysis postulates that this is exactly the case for the different classes. common across all language learners regardless of native language, which could not be explained under contrastive analysis. Corder (1967) then described an alternative, error analysis, where contrastive analysis-style errors were seen as only one type of error, ‘interlanguage’ or ‘interference’ errors; other types were ‘intralingual’ and ‘developmental’ errors, which are not specific to the native language (Richards, 1971). In an overview of contrastive analysis after the emergence of error analysis, Wardhaugh (1970) noted that there were two interpretations of the CAH, termed the strong and weak forms. Under the strong form, all errors were attributed to the native language, and clearly that was not tenable in light of error a</context>
</contexts>
<marker>Corder, 1967</marker>
<rawString>Stephen P. Corder. 1967. The significance of learners’ errors. International Review of Applied Linguistics in Language Teaching (IRAL), 5(4):161–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Estival</author>
<author>Tanja Gaustad</author>
<author>Son-Bao Pham</author>
<author>Will Radford</author>
<author>Ben Hutchinson</author>
</authors>
<title>Author profiling for English emails.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING),</booktitle>
<pages>263--272</pages>
<contexts>
<context position="2448" citStr="Estival et al. (2007)" startWordPosition="369" endWordPosition="372">ay confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of the concept of contrasti</context>
<context position="6279" citStr="Estival et al. (2007)" startWordPosition="966" endWordPosition="969">t be useful features, but only investigated this idea at a shallow level by treating rare PoS bigrams as ungrammatical structures. Tsur and Rappoport (2007) replicated the work of Koppel et al. (2005) to investigate the hypothesis that the choice of words in second language writing is highly influenced by the frequency of native language syllables — the phonology of the native language. Approximating this by character bi-grams alone, they managed to achieve a classification accuracy of 66%. Native language is also amongst the characteristics investigated in the task of authorship profiling by Estival et al. (2007), as well as other demographic and personality characteristics. This study used a variety of lexical and document structure features. For the native language identification classification task, their model yielded a reasonably high accuracy of 84%, but this was over a set of only three languages (Arabic, English and Spanish) and against a most frequent baseline of 62.9%. Another related work is that of van Halteren (2008), who used the Europarl corpus of parliamentary speeches. In Europarl, one original language is transcribed, and the others translated from it; the task was to identify the or</context>
</contexts>
<marker>Estival, Gaustad, Pham, Radford, Hutchinson, 2007</marker>
<rawString>Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will Radford, and Ben Hutchinson. 2007. Author profiling for English emails. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING), pages 263–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Fette</author>
<author>Norman Sadeh</author>
<author>Anthony Tomasic</author>
</authors>
<title>Learning to detect phishing emails.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International World Wide Web Conference.</booktitle>
<contexts>
<context position="2023" citStr="Fette et al., 2007" startWordPosition="294" endWordPosition="297"> as gender, age, native language, and so on. This profile information is often of interest to marketing organisations for product promotional reasons as well as governments or law enforcements for crime investigation purposes. The particular application that motivates the present study is detection of phishing (Myers, 2007), the attempt to defraud through texts that are designed to deceive Internet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over </context>
</contexts>
<marker>Fette, Sadeh, Tomasic, 2007</marker>
<rawString>Ian Fette, Norman Sadeh, and Anthony Tomasic. 2007. Learning to detect phishing emails. In Proceedings of the 16th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="16307" citStr="Forman, 2003" startWordPosition="2599" endWordPosition="2600">ures, but also present results for subsets of features chosen using feature selection. For each language in our dataset, we identify the n rules most characteristic of the language using Information Gain (IG). For m classes, we use the formulation of Yang and Pedersen (1997): − �m IG(r) = i=1 Pr (ci) log Pr (ci) +Pr (r) Emi=1 Pr (ci|r) log Pr (ci|r) +Pr (¯r) Em i=1 Pr (ci|¯r) log Pr (ci|¯r) (1) We also investigated simple frequencies, frequency ratios, and pointwise mutual information; as in much other work, IG performed best, so we do not present results for the others. Bi-normal separation (Forman, 2003), often competitive with IG, is only suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminati</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Josef van Genabith</author>
</authors>
<title>Adapting a WSJ-trained parser to grammatically noisy text.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>221--224</pages>
<location>Columbus, Ohio.</location>
<marker>Foster, Wagner, van Genabith, 2008</marker>
<rawString>Jennifer Foster, Joachim Wagner, and Josef van Genabith. 2008. Adapting a WSJ-trained parser to grammatically noisy text. In Proceedings of ACL-08: HLT, Short Papers, pages 221–224, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Franck</author>
<author>Gabriella Vigliocco</author>
<author>Janet Nicol</author>
</authors>
<title>Subject-verb agreement errors in French and English: The role of syntactic hierarchy.</title>
<date>2002</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>17--4</pages>
<contexts>
<context position="12862" citStr="Franck et al. (2002)" startWordPosition="2007" endWordPosition="2010">ome examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et al. (1996) specifically examined the usage of subjectverb agreement in English by French and Spanish, respectively. There are also a few teaching resources 1602 for English language teachers that collate such phenomena, such as that of Swan and Smith (2001). NLP techniques and a probabilistic view of native language identification now let us revisit and make use of the weak form of the CAH. Interlanguage errors, as represented by differences in parse trees, may be characteristic of the native language of a learner; we can use the occurrence of these to come up with a revised </context>
</contexts>
<marker>Franck, Vigliocco, Nicol, 2002</marker>
<rawString>Julie Franck, Gabriella Vigliocco, and Janet Nicol. 2002. Subject-verb agreement errors in French and English: The role of syntactic hierarchy. Language and Cognitive Processes, 17(4):371–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Linguistic correlates of style: Authorship classification with deep linguistic analysis features.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>611--617</pages>
<contexts>
<context position="8659" citStr="Gamon (2004)" startWordPosition="1335" endWordPosition="1336">identification constructions were identifiable; however, the overall classification was not improved over the lexical features by using just the three manually detected syntactic errors. The best overall accuracy re1601 ported was 73.71%; this was on the second version of ICLE, across seven languages (those of Koppel et al. (2005), plus the two Asian languages Chinese and Japanese). As a possible approach that would improve the classification accuracy over just the three manually detected syntactic errors, Wong and Dras (2009) suggested deploying (but did not carry out) an idea put forward by Gamon (2004) (citing Baayen et al. (1996)) for the related task of identifying the author of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader character</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Linguistic correlates of style: Authorship classification with deep linguistic analysis features. In Proceedings of the 20th International Conference on Computational Linguistics (COLING), pages 611–617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Stephanie Tyson</author>
</authors>
<title>Connector usage in the English essay writing of native and non-native EFL speakers of English. World Englishes,</title>
<date>1996</date>
<contexts>
<context position="12280" citStr="Granger and Tyson (1996)" startWordPosition="1914" endWordPosition="1917"> unwarranted, but a period of quiescence is probable for CA itself”. This appears to be the case, with the then-dominant error analysis giving way to newer, more specialised theories of second language acquisition, such as the competition model of MacWhinney and Bates (1989) or the processability theory of Pienemann (1998). Nevertheless, smaller studies specifically of interlanguage errors have continued to be carried out, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et </context>
</contexts>
<marker>Granger, Tyson, 1996</marker>
<rawString>Sylviane Granger and Stephanie Tyson. 1996. Connector usage in the English essay writing of native and non-native EFL speakers of English. World Englishes, 15(1):17–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
<author>Magali Paquot</author>
</authors>
<date>2009</date>
<booktitle>International Corpus of Learner English (Version 2). Presses Universitaires de Louvain,</booktitle>
<location>Louvian-la-Neuve.</location>
<contexts>
<context position="17943" citStr="Granger et al. (2009)" startWordPosition="2850" endWordPosition="2853">ak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (ranging from intermediate to advanced learners) and are in the same age group (all in their twenties at the time of corpus collection.) This was also the data used by Koppel et al. (2005) and Tsur and Rappoport (2007), although where they used the first version of the corpus, we use version 2. Briefly, the first version contains 11 sub-corpora of English essays contributed by second-year an</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, Paquot, 2009</marker>
<rawString>Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner English (Version 2). Presses Universitaires de Louvain, Louvian-la-Neuve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome H Friedman</author>
</authors>
<date>2009</date>
<booktitle>The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="21905" citStr="Hastie et al., 2009" startWordPosition="3490" endWordPosition="3493">atistical significance; and that while more sophisticated techniques have been proposed, none is more widely accepted than a paired t-test over folds. We therefore use this paired t-test over folds, as formulated of Alpaydin 4MegaM is available on http://www.cs.utah.edu/ ∼hal/megam/. 1604 (2004). Under this cross-validation, 5 separate training feature sets are constructed, excluding the test fold; 3 folds are used for training, 1 fold for tuning and 1 fold for testing. We also use a held-out test set for comparison, as it is well-known that cross-validation can overestimate prediction error (Hastie et al., 2009). We do not carry out significance testing here—with this held-out test set size (n = 125), two models would have to differ by a great deal to be significant. We only use it as a check on the effect of applying to completely new data. 5 Results Table 1 presents the results for the three models individually under cross-validation. The first point to note is that PROD-RULE, under both parsers, is a substantial improvement over LEXICAL when (non-lexicalised) parse rules together with rules lexicalised with function words are used (rows marked with * in Table 1), with the largest difference as muc</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Ahmet Engin Ural</author>
</authors>
<title>Reranking the Berkeley and Brown Parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL-10),</booktitle>
<pages>665--668</pages>
<location>Los Angeles, CA, USA,</location>
<contexts>
<context position="17491" citStr="Johnson and Ural (2010)" startWordPosition="2774" endWordPosition="2777">nspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus </context>
</contexts>
<marker>Johnson, Ural, 2010</marker>
<rawString>Mark Johnson and Ahmet Engin Ural. 2010. Reranking the Berkeley and Brown Parsers. In Proceedings of Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL-10), pages 665–668, Los Angeles, CA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic unification-based grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99),</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="16962" citStr="Johnson et al. (1999)" startWordPosition="2695" endWordPosition="2698">ly suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for th</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic unification-based grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19777" citStr="Klein and Manning, 2003" startWordPosition="3145" endWordPosition="3148">se of the present study, we have 95 essays per native language. For the same reason as highlighted by Wong and Dras (2009), we intentionally use fewer essays as compared to Koppel et al. (2005)3 with a view to reserving more data for future work. We divide these into training sets of 70 essays per lan3Koppel et al. (2005) took all 258 texts per language from ICLE Version 1 and evaluated using 10-fold cross valiadation. guage, with a held-out test set of 25 essays per language. There are 17,718 training sentences and 6,791 testing sentences. 4.2 Parsers We use two parsers: the Stanford parser (Klein and Manning, 2003) and the Charniak and Johnson (henceforth C&amp;J) parser (Charniak and Johnson, 2005). Both are widely used, and produce relatively accurate parses: the Stanford parser gets a labelled f-score of 85.61 on the WSJ, and the C&amp;J 91.09. With the Stanford parser, there are 26,284 unique parse production rules extractable from our ICLE training set of 490 texts, while the C&amp;J parser produces 27,705. For reranking, we use only the C&amp;J parser—since the parser stores these features during parsing, we can use them directly as classification features. On the ICLE training data, there are 6,230 features with</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Automatically determining an anonymous author’s native language.</title>
<date>2005</date>
<booktitle>In Intelligence and Security Informatics,</booktitle>
<volume>3495</volume>
<pages>209--217</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2383" citStr="Koppel et al. (2005)" startWordPosition="358" endWordPosition="361">texts that are designed to deceive Internet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Won</context>
<context position="5126" citStr="Koppel et al. (2005)" startWordPosition="782" endWordPosition="785">ical features. The remainder of this paper is structured as follows. In Section 2, we discuss some related work on the two key topics of this paper: primarily on comparable work in native language identification, and then on how the notion of contrastive analysis can be applicable here. We then describe the models examined in Section 3, followed by experimental setup in Section 4. Section 5 presents results, and Section 6 discussion of those results. 2 Related Work 2.1 Native Language Identification The earliest work on native language identification in this classification paradigm is that of Koppel et al. (2005), in which they deployed a machine learning approach to the task, using as features function words, character n-grams, and part-of-speech (PoS) bi-grams, as well as some spelling mistakes. With five different groups of English authors (of native languages Bulgarian, Czech, French, Russian, and Spanish) selected from the first version of International Corpus of Learner English (ICLE), they gained a relatively high classification accuracy of 80%. Koppel et al. (2005) also suggested that syntactic features (syntactic errors) might be useful features, but only investigated this idea at a shallow l</context>
<context position="7543" citStr="Koppel et al. (2005)" startWordPosition="1162" endWordPosition="1165">ounts of word-based n-grams, surprisingly high classification accuracies within the range of 87-97% were achieved across six languages (English, German, French, Dutch, Spanish, and Italian). This turns out, however, to be significantly influenced by the use of particular phrases used by speakers of different languages in the parliamentary context (e.g. the way Germans typically address the chamber). To our knowledge, Wong and Dras (2009) is the only work that has investigated the usefulness of syntactic features for the task of native language identification. They first replicated the work of Koppel et al. (2005) with the three types of lexical feature, namely function words, character ngrams, and PoS bi-grams. They then examined the literature on contrastive analysis (see Section 2.2), from the field of second language acquisition, and selected three syntactic errors commonly observed in non-native English users—subject-verb disagreement, noun-number disagreement and misuse of determiners—that had been identified as being influenced by the native language. An ANOVA analysis showed that the native language identification constructions were identifiable; however, the overall classification was not impr</context>
<context position="13699" citStr="Koppel et al. (2005)" startWordPosition="2152" endWordPosition="2155">late such phenomena, such as that of Swan and Smith (2001). NLP techniques and a probabilistic view of native language identification now let us revisit and make use of the weak form of the CAH. Interlanguage errors, as represented by differences in parse trees, may be characteristic of the native language of a learner; we can use the occurrence of these to come up with a revised likelihood of the native language. In this paper, we use machine learning in a prediction task as our approach to this. 3 Models This section describes the three basic models investigated: the lexical model, based on Koppel et al. (2005), as the baseline; and then the two models that exploit syntactic information. In Section 5 we look at the performance of each model independently and also in combination: to combine, we just concatenate feature vectors. Lexical As Wong and Dras (2009), we replicate the features of Koppel et al. (2005) to produce our LEXICAL model. These are of three types: function words,2 character n-grams, and PoS n-grams. We follow Wong and Dras (2009) in resolving some unclear issues from Koppel et al. (2005). Specifically, we use the same list of function words, left unspecified in Koppel et al. (2005), </context>
<context position="15382" citStr="Koppel et al. (2005)" startWordPosition="2440" endWordPosition="2443">n words, 200 most frequently occurring character bi-grams, and 200 most frequently occurring PoS bi-grams. Both function words and PoS bi-grams have feature values of binary type; while for character bi-grams, the feature value is the relative frequency. (These types of feature value are the best performing one for each lexi2As with most work in authorship profiling, only function words are used, so that the result is not tied to a particular domain, and no clues are obtained from different topics that different authors might write about. cal feature.) We omitted the 250 rare bi-grams used by Koppel et al. (2005), as an ablative analysis showed that they contributed nothing to classification accuracy. Production Rules Under this model (PRODRULE), we take as features horizontal slices of parse trees, in effect treating them as sets of CFG production rules. Feature values are binary. We look at all possible rules as features, but also present results for subsets of features chosen using feature selection. For each language in our dataset, we identify the n rules most characteristic of the language using Information Gain (IG). For m classes, we use the formulation of Yang and Pedersen (1997): − �m IG(r) </context>
<context position="18337" citStr="Koppel et al. (2005)" startWordPosition="2914" endWordPosition="2917">nd Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (ranging from intermediate to advanced learners) and are in the same age group (all in their twenties at the time of corpus collection.) This was also the data used by Koppel et al. (2005) and Tsur and Rappoport (2007), although where they used the first version of the corpus, we use version 2. Briefly, the first version contains 11 sub-corpora of English essays contributed by second-year and third-year university students of different native language backgrounds (mostly European and Slavic languages) — Bulgarian, Czech, Dutch, Finnish, French, German, Italian, Polish, Russian, Spanish, and Swedish; the second version has been extended to additional 5 other native languages (including Asian languages) — Chinese, Japanese, Norwegian, Turkish, and Tswana. As per Wong and Dras (20</context>
<context position="26447" citStr="Koppel et al. (2005)" startWordPosition="4224" endWordPosition="4227">on results based on 5-fold cross validation for combined models (accuracy %) Features MaxEnt LEXICAL (n = 798) 75.43 LEXICAL + PROD-RULE[both] (n = 2000) 80.57 LEXICAL + PROD-RULE[both]* (n = 2000) 81.14 LEXICAL + PROD-RULE[both]* (n = all) 81.71 LEXICAL + RERANKING (n = all) 76.00 Table 4: Classification results based on hold-out validation for combined models (accuracy %) ing it is difficult to say whether this is a genuine phenomenon. The accuracy rate for LEXICAL here is in line with Wong and Dras (2009); and given the smaller dataset and larger set of languages, also broadly in line with Koppel et al. (2005). Tables 3 and 4 present results for model combinations. It can be seen that the model combinations do not produce results better than PROD-RULE alone. Combining all features (results not presented here) seems to degrade the overall performance even of the MegaM: perhaps we need to derive feature vectors more compactly than by feature concatenation. 6 Discussion As illustrated in the confusion matrices (Table 5 for the PROD-RULE model, and Table 6 for the LEXICAL model), misclassifications occur largely in Spanish and Slavic languages, Bulgarian and Russian in particular. Unsurprisingly, Chine</context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Automatically determining an anonymous author’s native language. In Intelligence and Security Informatics, volume 3495 of Lecture Notes in Computer Science, pages 209–217. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Lado</author>
</authors>
<title>Linguistics Across Cultures: Applied Linguistics for Language Teachers.</title>
<date>1957</date>
<publisher>University of Michigan Press,</publisher>
<location>Ann Arbor, MI, US.</location>
<contexts>
<context position="3072" citStr="Lado, 1957" startWordPosition="470" endWordPosition="471">n (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of the concept of contrastive analysis (Lado, 1957), which postulates that native language constructions lead to characteristic errors in a second language. In their experimental work, however, they used only three manual syntactic constructions drawn from the literature; an ANOVA analysis showed a detectable effect, but they did not improve classification accuracy over purely lexical features. In this paper, we investigate syntactic features for native language identification that are more general 1600 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600–1610, Edinburgh, Scotland, UK, July 27–31, </context>
<context position="9344" citStr="Lado, 1957" startWordPosition="1448" endWordPosition="1449">hor of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader characterisation of the parse tree. 2.2 Contrastive analysis Contrastive analysis (Lado, 1957) was an early attempt in the field of second language acquisition to explain the kinds and source of errors that nonnative speakers make. It arose out of behaviourist psychology, and saw language learning as an issue of habit formation that could be inhibited by previous habits inculcated in learning the native language. The theory was also tied to structural linguistics: it compared the syntactic structures of the native and second languages to find differences that might cause learning difficulties. The Lado work postulated the Contrastive Analysis Hypothesis (CAH), claiming that “those elem</context>
</contexts>
<marker>Lado, 1957</marker>
<rawString>Robert Lado. 1957. Linguistics Across Cultures: Applied Linguistics for Language Teachers. University of Michigan Press, Ann Arbor, MI, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Elizabeth Bates</author>
</authors>
<title>The Crosslinguistic Study of Sentence Processing.</title>
<date>1989</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11931" citStr="MacWhinney and Bates (1989)" startWordPosition="1859" endWordPosition="1862">he native language, and clearly that was not tenable in light of error analysis evidence. In the weak form, these differences have an influence but are not the sole determinant of language learning difficulty. Wardhaugh noted claims at the time that the hypothesis was no longer useful in either the strong or the weak version: “Such a claim is perhaps unwarranted, but a period of quiescence is probable for CA itself”. This appears to be the case, with the then-dominant error analysis giving way to newer, more specialised theories of second language acquisition, such as the competition model of MacWhinney and Bates (1989) or the processability theory of Pienemann (1998). Nevertheless, smaller studies specifically of interlanguage errors have continued to be carried out, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – </context>
</contexts>
<marker>MacWhinney, Bates, 1989</marker>
<rawString>Brian MacWhinney and Elizabeth Bates. 1989. The Crosslinguistic Study of Sentence Processing. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mutton</author>
<author>Mark Dras</author>
<author>Stephen Wan</author>
<author>Robert Dale</author>
</authors>
<title>GLEU: Automatic evaluation of sentence-level fluency.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>344--351</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9028" citStr="Mutton et al., 2007" startWordPosition="1392" endWordPosition="1395"> Chinese and Japanese). As a possible approach that would improve the classification accuracy over just the three manually detected syntactic errors, Wong and Dras (2009) suggested deploying (but did not carry out) an idea put forward by Gamon (2004) (citing Baayen et al. (1996)) for the related task of identifying the author of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader characterisation of the parse tree. 2.2 Contrastive analysis Contrastive analysis (Lado, 1957) was an early attempt in the field of second language acquisition to explain the kinds and source of errors that nonnative speakers make. It arose out of behaviourist psychology, and saw language learning as an issue of habit formation that could be inhibited by previous habits incul</context>
</contexts>
<marker>Mutton, Dras, Wan, Dale, 2007</marker>
<rawString>Andrew Mutton, Mark Dras, Stephen Wan, and Robert Dale. 2007. GLEU: Automatic evaluation of sentence-level fluency. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344–351, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Myers</author>
</authors>
<title>Introduction to phishing.</title>
<date>2007</date>
<booktitle>Phishing and Countermeasures: Understanding the Increasing Problem of Electronic Identity Theft.</booktitle>
<editor>In Markus Jakobsson and Steven Myers, editors,</editor>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>Hoboken, NJ, USA.</location>
<contexts>
<context position="1730" citStr="Myers, 2007" startWordPosition="249" endWordPosition="250">ng characteristics of authors from their textual data, often termed authorship profiling, has seen a number of computational approaches proposed in recent years. The problem is typically treated as a classification task, where an author is classified with respect to characteristics such as gender, age, native language, and so on. This profile information is often of interest to marketing organisations for product promotional reasons as well as governments or law enforcements for crime investigation purposes. The particular application that motivates the present study is detection of phishing (Myers, 2007), the attempt to defraud through texts that are designed to deceive Internet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language</context>
</contexts>
<marker>Myers, 2007</marker>
<rawString>Steven Myers. 2007. Introduction to phishing. In Markus Jakobsson and Steven Myers, editors, Phishing and Countermeasures: Understanding the Increasing Problem of Electronic Identity Theft. John Wiley &amp; Sons, Inc., Hoboken, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Reranking a Wide-Coverage CCG Parser.</title>
<date>2010</date>
<booktitle>In Proceedings of Australasian Language Technology Association Workshop (ALTA’10),</booktitle>
<pages>90--98</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="17555" citStr="Ng et al. (2010)" startWordPosition="2786" endWordPosition="2789">s Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (rangi</context>
</contexts>
<marker>Ng, Honnibal, Curran, 2010</marker>
<rawString>Dominick Ng, Matthew Honnibal, and James R. Curran. 2010. Reranking a Wide-Coverage CCG Parser. In Proceedings of Australasian Language Technology Association Workshop (ALTA’10), pages 90–98, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06),</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="17537" citStr="Petrov et al., 2006" startWordPosition="2782" endWordPosition="2785">ion techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use 1603 these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&amp;C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English profici</context>
<context position="34631" citStr="Petrov et al., 2006" startWordPosition="5714" endWordPosition="5717">ising from the classification have been horizontal cross-sections of trees, rather than the more general discriminative parse reranking features that might have been expected to perform at least as well. This relatively poorer performance by the reranking features may be due to a number of factors, all of which could be investigated in future work. One is the use of feature schema instances that did not appear in the largely grammatical WSJ; another is the extension of feature schemas; and a third is the use of a parser that does not enforce linguistic constraints such as the Berkeley parser (Petrov et al., 2006). Examining some of the substructures showed some errors that were expected; other constructions that were grammatical, but were just characteristic translations of constructions that were common in the native language; and a large number where grammatical errors were glossed over by the parser’s linguistic constraints, suggesting another purpose for further work with the Berkeley parser. Overall, the use of these led to an error reduction in over 30% 1608 in the cross-validation evaluation with significance testing. Acknowledgments The authors would like to acknowledge the support of ARC Link</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06), pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Pienemann</author>
</authors>
<title>Language Processing and Second Language Development: Processability Theory. John Benjamins,</title>
<date>1998</date>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="11980" citStr="Pienemann (1998)" startWordPosition="1868" endWordPosition="1869">t of error analysis evidence. In the weak form, these differences have an influence but are not the sole determinant of language learning difficulty. Wardhaugh noted claims at the time that the hypothesis was no longer useful in either the strong or the weak version: “Such a claim is perhaps unwarranted, but a period of quiescence is probable for CA itself”. This appears to be the case, with the then-dominant error analysis giving way to newer, more specialised theories of second language acquisition, such as the competition model of MacWhinney and Bates (1989) or the processability theory of Pienemann (1998). Nevertheless, smaller studies specifically of interlanguage errors have continued to be carried out, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova</context>
</contexts>
<marker>Pienemann, 1998</marker>
<rawString>Manfred Pienemann. 1998. Language Processing and Second Language Development: Processability Theory. John Benjamins, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Payam Refaeilzadeh</author>
<author>Lei Tang</author>
<author>Huan Liu</author>
</authors>
<date>2009</date>
<booktitle>Encyclopedia of Database Systems,</booktitle>
<pages>532--538</pages>
<editor>Cross-validation. In Ling Liu and M. Tamer ¨Ozsu, editors,</editor>
<publisher>Springer, US.</publisher>
<contexts>
<context position="21219" citStr="Refaeilzadeh et al. (2009)" startWordPosition="3380" endWordPosition="3384"> the results were uniformly worse, and degraded more quickly as number of features increased, so we only report the MaxEnt results here). The classifier is tuned to obtain an optimal classification model. 4.4 Evaluation Methodology Given our relatively small amount of data, we use kfold cross-validation, choosing k = 5. While testing for statistical significance of classification results is often not carried out in NLP, we do so here because the quantity of data could raise questions about the certainty of any effect. In an encyclopedic survey of cross-validation in machine learning contexts, Refaeilzadeh et al. (2009) note that there is as yet no universal standard for testing of statistical significance; and that while more sophisticated techniques have been proposed, none is more widely accepted than a paired t-test over folds. We therefore use this paired t-test over folds, as formulated of Alpaydin 4MegaM is available on http://www.cs.utah.edu/ ∼hal/megam/. 1604 (2004). Under this cross-validation, 5 separate training feature sets are constructed, excluding the test fold; 3 folds are used for training, 1 fold for tuning and 1 fold for testing. We also use a held-out test set for comparison, as it is we</context>
</contexts>
<marker>Refaeilzadeh, Tang, Liu, 2009</marker>
<rawString>Payam Refaeilzadeh, Lei Tang, and Huan Liu. 2009. Cross-validation. In Ling Liu and M. Tamer ¨Ozsu, editors, Encyclopedia of Database Systems, pages 532– 538. Springer, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack C Richards</author>
</authors>
<title>A non-contrastive approach to error analysis.</title>
<date>1971</date>
<journal>ELT Journal,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="11065" citStr="Richards, 1971" startWordPosition="1715" endWordPosition="1716">yntactic constructions from each other? In the context of native language identification, however, constrastive analysis postulates that this is exactly the case for the different classes. common across all language learners regardless of native language, which could not be explained under contrastive analysis. Corder (1967) then described an alternative, error analysis, where contrastive analysis-style errors were seen as only one type of error, ‘interlanguage’ or ‘interference’ errors; other types were ‘intralingual’ and ‘developmental’ errors, which are not specific to the native language (Richards, 1971). In an overview of contrastive analysis after the emergence of error analysis, Wardhaugh (1970) noted that there were two interpretations of the CAH, termed the strong and weak forms. Under the strong form, all errors were attributed to the native language, and clearly that was not tenable in light of error analysis evidence. In the weak form, these differences have an influence but are not the sole determinant of language learning difficulty. Wardhaugh noted claims at the time that the hypothesis was no longer useful in either the strong or the weak version: “Such a claim is perhaps unwarran</context>
</contexts>
<marker>Richards, 1971</marker>
<rawString>Jack C. Richards. 1971. A non-contrastive approach to error analysis. ELT Journal, 25(3):204–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roumyana Slabakova</author>
</authors>
<title>L1 transfer revisited: the L2 acquisition of telicity marking in English by Spanish and Bulgarian native speakers.</title>
<date>2000</date>
<journal>Linguistics,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="12587" citStr="Slabakova (2000)" startWordPosition="1965" endWordPosition="1966">nn (1998). Nevertheless, smaller studies specifically of interlanguage errors have continued to be carried out, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et al. (1996) specifically examined the usage of subjectverb agreement in English by French and Spanish, respectively. There are also a few teaching resources 1602 for English language teachers that collate such phenomena, such as that of Swan and Smith (2001). NLP techniques and a probabilistic view of nativ</context>
</contexts>
<marker>Slabakova, 2000</marker>
<rawString>Roumyana Slabakova. 2000. L1 transfer revisited: the L2 acquisition of telicity marking in English by Spanish and Bulgarian native speakers. Linguistics, 38(4):739–770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Xiaohua Liu</author>
<author>Gao Cong</author>
<author>Ming Zhou</author>
<author>Zhongyang Xiong</author>
<author>John Lee</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Detecting erroneous sentences using automatically mined sequential patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>81--88</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9046" citStr="Sun et al., 2007" startWordPosition="1396" endWordPosition="1399">). As a possible approach that would improve the classification accuracy over just the three manually detected syntactic errors, Wong and Dras (2009) suggested deploying (but did not carry out) an idea put forward by Gamon (2004) (citing Baayen et al. (1996)) for the related task of identifying the author of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader characterisation of the parse tree. 2.2 Contrastive analysis Contrastive analysis (Lado, 1957) was an early attempt in the field of second language acquisition to explain the kinds and source of errors that nonnative speakers make. It arose out of behaviourist psychology, and saw language learning as an issue of habit formation that could be inhibited by previous habits inculcated in learning </context>
</contexts>
<marker>Sun, Liu, Cong, Zhou, Xiong, Lee, Lin, 2007</marker>
<rawString>Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee, and Chin-Yew Lin. 2007. Detecting erroneous sentences using automatically mined sequential patterns. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Swan</author>
<author>Bernard Smith</author>
<author>editors</author>
</authors>
<title>Learner English: A teacher’s guide to interference and other problems.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<note>2nd edition.</note>
<marker>Swan, Smith, editors, 2001</marker>
<rawString>Michael Swan and Bernard Smith, editors. 2001. Learner English: A teacher’s guide to interference and other problems. Cambridge University Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jennifer Foster</author>
<author>Martin Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>353--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9112" citStr="Tetreault et al., 2010" startWordPosition="1409" endWordPosition="1412">ion accuracy over just the three manually detected syntactic errors, Wong and Dras (2009) suggested deploying (but did not carry out) an idea put forward by Gamon (2004) (citing Baayen et al. (1996)) for the related task of identifying the author of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader characterisation of the parse tree. 2.2 Contrastive analysis Contrastive analysis (Lado, 1957) was an early attempt in the field of second language acquisition to explain the kinds and source of errors that nonnative speakers make. It arose out of behaviourist psychology, and saw language learning as an issue of habit formation that could be inhibited by previous habits inculcated in learning the native language. The theory was also tied to structural lingui</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>Joel Tetreault, Jennifer Foster, and Martin Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 353– 358. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Using classifier features for studying the effect of native language on the choice of written second language words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2425" citStr="Tsur and Rappoport (2007)" startWordPosition="365" endWordPosition="368">ternet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of t</context>
<context position="5814" citStr="Tsur and Rappoport (2007)" startWordPosition="890" endWordPosition="893">k, using as features function words, character n-grams, and part-of-speech (PoS) bi-grams, as well as some spelling mistakes. With five different groups of English authors (of native languages Bulgarian, Czech, French, Russian, and Spanish) selected from the first version of International Corpus of Learner English (ICLE), they gained a relatively high classification accuracy of 80%. Koppel et al. (2005) also suggested that syntactic features (syntactic errors) might be useful features, but only investigated this idea at a shallow level by treating rare PoS bigrams as ungrammatical structures. Tsur and Rappoport (2007) replicated the work of Koppel et al. (2005) to investigate the hypothesis that the choice of words in second language writing is highly influenced by the frequency of native language syllables — the phonology of the native language. Approximating this by character bi-grams alone, they managed to achieve a classification accuracy of 66%. Native language is also amongst the characteristics investigated in the task of authorship profiling by Estival et al. (2007), as well as other demographic and personality characteristics. This study used a variety of lexical and document structure features. F</context>
<context position="18367" citStr="Tsur and Rappoport (2007)" startWordPosition="2919" endWordPosition="2922">ned on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 Experimental Setup 4.1 Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (ranging from intermediate to advanced learners) and are in the same age group (all in their twenties at the time of corpus collection.) This was also the data used by Koppel et al. (2005) and Tsur and Rappoport (2007), although where they used the first version of the corpus, we use version 2. Briefly, the first version contains 11 sub-corpora of English essays contributed by second-year and third-year university students of different native language backgrounds (mostly European and Slavic languages) — Bulgarian, Czech, Dutch, Finnish, French, German, Italian, Polish, Russian, Spanish, and Swedish; the second version has been extended to additional 5 other native languages (including Asian languages) — Chinese, Japanese, Norwegian, Turkish, and Tswana. As per Wong and Dras (2009), we examine seven language</context>
</contexts>
<marker>Tsur, Rappoport, 2007</marker>
<rawString>Oren Tsur and Ari Rappoport. 2007. Using classifier features for studying the effect of native language on the choice of written second language words. In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Source language markers in EUROPARL translations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>937--944</pages>
<marker>van Halteren, 2008</marker>
<rawString>Hans van Halteren. 2008. Source language markers in EUROPARL translations. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 937–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irena Vassileva</author>
</authors>
<title>Who am I/how are we in academic writing? A contrastive analysis of authorial presence in English,</title>
<date>1998</date>
<journal>International Journal of Applied Linguistics,</journal>
<volume>8</volume>
<issue>2</issue>
<location>German, French, Russian</location>
<contexts>
<context position="12420" citStr="Vassileva (1998)" startWordPosition="1940" endWordPosition="1941">o newer, more specialised theories of second language acquisition, such as the competition model of MacWhinney and Bates (1989) or the processability theory of Pienemann (1998). Nevertheless, smaller studies specifically of interlanguage errors have continued to be carried out, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et al. (1996) specifically examined the usage of subjectverb agreement in English by French and Spanish, respectively. There are also a few tea</context>
</contexts>
<marker>Vassileva, 1998</marker>
<rawString>Irena Vassileva. 1998. Who am I/how are we in academic writing? A contrastive analysis of authorial presence in English, German, French, Russian and Bulgarian. International Journal of Applied Linguistics, 8(2):163–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Garbriella Vigliocco</author>
<author>Brian Butterworth</author>
<author>Merrill F Garrett</author>
</authors>
<title>Subject-verb agreement in Spanish and English: Differences in the role of conceptual constraints.</title>
<date>1996</date>
<journal>Cognition,</journal>
<volume>61</volume>
<issue>3</issue>
<contexts>
<context position="12890" citStr="Vigliocco et al. (1996)" startWordPosition="2012" endWordPosition="2015"> Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et al. (1996) specifically examined the usage of subjectverb agreement in English by French and Spanish, respectively. There are also a few teaching resources 1602 for English language teachers that collate such phenomena, such as that of Swan and Smith (2001). NLP techniques and a probabilistic view of native language identification now let us revisit and make use of the weak form of the CAH. Interlanguage errors, as represented by differences in parse trees, may be characteristic of the native language of a learner; we can use the occurrence of these to come up with a revised likelihood of the native lan</context>
</contexts>
<marker>Vigliocco, Butterworth, Garrett, 1996</marker>
<rawString>Garbriella Vigliocco, Brian Butterworth, and Merrill F. Garrett. 1996. Subject-verb agreement in Spanish and English: Differences in the role of conceptual constraints. Cognition, 61(3):261–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3):474–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wardhaugh</author>
</authors>
<title>The Contrastive Analysis Hypothesis.</title>
<date>1970</date>
<journal>TESOL Quarterly,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="11161" citStr="Wardhaugh (1970)" startWordPosition="1729" endWordPosition="1730">er, constrastive analysis postulates that this is exactly the case for the different classes. common across all language learners regardless of native language, which could not be explained under contrastive analysis. Corder (1967) then described an alternative, error analysis, where contrastive analysis-style errors were seen as only one type of error, ‘interlanguage’ or ‘interference’ errors; other types were ‘intralingual’ and ‘developmental’ errors, which are not specific to the native language (Richards, 1971). In an overview of contrastive analysis after the emergence of error analysis, Wardhaugh (1970) noted that there were two interpretations of the CAH, termed the strong and weak forms. Under the strong form, all errors were attributed to the native language, and clearly that was not tenable in light of error analysis evidence. In the weak form, these differences have an influence but are not the sole determinant of language learning difficulty. Wardhaugh noted claims at the time that the hypothesis was no longer useful in either the strong or the weak version: “Such a claim is perhaps unwarranted, but a period of quiescence is probable for CA itself”. This appears to be the case, with th</context>
</contexts>
<marker>Wardhaugh, 1970</marker>
<rawString>Richard Wardhaugh. 1970. The Contrastive Analysis Hypothesis. TESOL Quarterly, 4(2):123–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Contrastive analysis and native language identification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>53--61</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2495" citStr="Wong and Dras (2009)" startWordPosition="377" endWordPosition="380">sures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of the concept of contrastive analysis (Lado, 1957), which postulates that</context>
<context position="7364" citStr="Wong and Dras (2009)" startWordPosition="1133" endWordPosition="1136">mentary speeches. In Europarl, one original language is transcribed, and the others translated from it; the task was to identify the original language. On the basis of frequency counts of word-based n-grams, surprisingly high classification accuracies within the range of 87-97% were achieved across six languages (English, German, French, Dutch, Spanish, and Italian). This turns out, however, to be significantly influenced by the use of particular phrases used by speakers of different languages in the parliamentary context (e.g. the way Germans typically address the chamber). To our knowledge, Wong and Dras (2009) is the only work that has investigated the usefulness of syntactic features for the task of native language identification. They first replicated the work of Koppel et al. (2005) with the three types of lexical feature, namely function words, character ngrams, and PoS bi-grams. They then examined the literature on contrastive analysis (see Section 2.2), from the field of second language acquisition, and selected three syntactic errors commonly observed in non-native English users—subject-verb disagreement, noun-number disagreement and misuse of determiners—that had been identified as being in</context>
<context position="13951" citStr="Wong and Dras (2009)" startWordPosition="2194" endWordPosition="2197">rse trees, may be characteristic of the native language of a learner; we can use the occurrence of these to come up with a revised likelihood of the native language. In this paper, we use machine learning in a prediction task as our approach to this. 3 Models This section describes the three basic models investigated: the lexical model, based on Koppel et al. (2005), as the baseline; and then the two models that exploit syntactic information. In Section 5 we look at the performance of each model independently and also in combination: to combine, we just concatenate feature vectors. Lexical As Wong and Dras (2009), we replicate the features of Koppel et al. (2005) to produce our LEXICAL model. These are of three types: function words,2 character n-grams, and PoS n-grams. We follow Wong and Dras (2009) in resolving some unclear issues from Koppel et al. (2005). Specifically, we use the same list of function words, left unspecified in Koppel et al. (2005), that were empirically determined by Wong and Dras (2009) to be the best of three candidates; we used character bi-grams, as the best performing n-grams, although this also had been left unspecified by Koppel et al. (2005); and we used the most frequent</context>
<context position="18940" citStr="Wong and Dras (2009)" startWordPosition="3003" endWordPosition="3006">pel et al. (2005) and Tsur and Rappoport (2007), although where they used the first version of the corpus, we use version 2. Briefly, the first version contains 11 sub-corpora of English essays contributed by second-year and third-year university students of different native language backgrounds (mostly European and Slavic languages) — Bulgarian, Czech, Dutch, Finnish, French, German, Italian, Polish, Russian, Spanish, and Swedish; the second version has been extended to additional 5 other native languages (including Asian languages) — Chinese, Japanese, Norwegian, Turkish, and Tswana. As per Wong and Dras (2009), we examine seven languages, namely Bulgarian, Czech, French, Russian, Spanish, Chinese, and Japanese. For each native language, we randomly select from amongst essays with length of 500-1000 words. For the purpose of the present study, we have 95 essays per native language. For the same reason as highlighted by Wong and Dras (2009), we intentionally use fewer essays as compared to Koppel et al. (2005)3 with a view to reserving more data for future work. We divide these into training sets of 70 essays per lan3Koppel et al. (2005) took all 258 texts per language from ICLE Version 1 and evaluat</context>
<context position="26340" citStr="Wong and Dras (2009)" startWordPosition="4205" endWordPosition="4208"> 72.45 LEXICAL + PROD-RULE[both]* (n = all) 70.82 LEXICAL + RERANKING (n = all) 68.17 Table 3: Classification results based on 5-fold cross validation for combined models (accuracy %) Features MaxEnt LEXICAL (n = 798) 75.43 LEXICAL + PROD-RULE[both] (n = 2000) 80.57 LEXICAL + PROD-RULE[both]* (n = 2000) 81.14 LEXICAL + PROD-RULE[both]* (n = all) 81.71 LEXICAL + RERANKING (n = all) 76.00 Table 4: Classification results based on hold-out validation for combined models (accuracy %) ing it is difficult to say whether this is a genuine phenomenon. The accuracy rate for LEXICAL here is in line with Wong and Dras (2009); and given the smaller dataset and larger set of languages, also broadly in line with Koppel et al. (2005). Tables 3 and 4 present results for model combinations. It can be seen that the model combinations do not produce results better than PROD-RULE alone. Combining all features (results not presented here) seems to degrade the overall performance even of the MegaM: perhaps we need to derive feature vectors more compactly than by feature concatenation. 6 Discussion As illustrated in the confusion matrices (Table 5 for the PROD-RULE model, and Table 6 for the LEXICAL model), misclassification</context>
</contexts>
<marker>Wong, Dras, 2009</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceedings of the Australasian Language Technology Association Workshop 2009, pages 53–61, Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser features for sentence grammaticality classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>67--75</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="9134" citStr="Wong and Dras, 2010" startWordPosition="1413" endWordPosition="1416">he three manually detected syntactic errors, Wong and Dras (2009) suggested deploying (but did not carry out) an idea put forward by Gamon (2004) (citing Baayen et al. (1996)) for the related task of identifying the author of a text: to use CFG production rules to characterise syntactic structures used by authors.1 We note that similar ideas have been used in the task of sentence grammaticality judgement, which utilise parser outputs (both trees and by-products) as classification features (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009; Tetreault et al., 2010; Wong and Dras, 2010). We combine this idea with one we introduce in this paper, of using discriminative reranking features as a broader characterisation of the parse tree. 2.2 Contrastive analysis Contrastive analysis (Lado, 1957) was an early attempt in the field of second language acquisition to explain the kinds and source of errors that nonnative speakers make. It arose out of behaviourist psychology, and saw language learning as an issue of habit formation that could be inhibited by previous habits inculcated in learning the native language. The theory was also tied to structural linguistics: it compared the</context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features for sentence grammaticality classification. In Proceedings of the Australasian Language Technology Association Workshop 2010, pages 67–75, Melbourne, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suying Yang</author>
<author>Yue-Yuan Huang</author>
</authors>
<title>The impact of the absence of grammatical tense in L1 on the acquisition of the tense-aspect system in</title>
<date>2004</date>
<booktitle>L2. International Review of Applied Linguistics in Language Teaching (IRAL),</booktitle>
<pages>42--1</pages>
<contexts>
<context position="12700" citStr="Yang and Huang (2004)" startWordPosition="1980" endWordPosition="1983">ut, generally restricted in their scope to a specific grammatical aspect of English in which the native language of the learners might have an influence. To give some examples, Granger and Tyson (1996) examined the usage of connectors in English by a number of different native speakers – French, German, Dutch, and Chinese; Vassileva (1998) investigated the employment of first person singular and plural by another different set of native speakers – German, French, Russian, and Bulgarian; Slabakova (2000) explored the acquisition of telicity marking in English by Spanish and Bulgarian learners; Yang and Huang (2004) studied the impact of the absence of grammatical tense in Chinese on the acquisition of English tense-aspect system (i.e. telicity marking); Franck et al. (2002) and Vigliocco et al. (1996) specifically examined the usage of subjectverb agreement in English by French and Spanish, respectively. There are also a few teaching resources 1602 for English language teachers that collate such phenomena, such as that of Swan and Smith (2001). NLP techniques and a probabilistic view of native language identification now let us revisit and make use of the weak form of the CAH. Interlanguage errors, as r</context>
</contexts>
<marker>Yang, Huang, 2004</marker>
<rawString>Suying Yang and Yue-Yuan Huang. 2004. The impact of the absence of grammatical tense in L1 on the acquisition of the tense-aspect system in L2. International Review of Applied Linguistics in Language Teaching (IRAL), 42(1):49–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97),</booktitle>
<pages>412--420</pages>
<contexts>
<context position="15969" citStr="Yang and Pedersen (1997)" startWordPosition="2536" endWordPosition="2539">bi-grams used by Koppel et al. (2005), as an ablative analysis showed that they contributed nothing to classification accuracy. Production Rules Under this model (PRODRULE), we take as features horizontal slices of parse trees, in effect treating them as sets of CFG production rules. Feature values are binary. We look at all possible rules as features, but also present results for subsets of features chosen using feature selection. For each language in our dataset, we identify the n rules most characteristic of the language using Information Gain (IG). For m classes, we use the formulation of Yang and Pedersen (1997): − �m IG(r) = i=1 Pr (ci) log Pr (ci) +Pr (r) Emi=1 Pr (ci|r) log Pr (ci|r) +Pr (¯r) Em i=1 Pr (ci|¯r) log Pr (ci|¯r) (1) We also investigated simple frequencies, frequency ratios, and pointwise mutual information; as in much other work, IG performed best, so we do not present results for the others. Bi-normal separation (Forman, 2003), often competitive with IG, is only suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Rerank</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97), pages 412– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zheng</author>
<author>Yi Qin</author>
<author>Zan Huang</author>
<author>Hsinchun Chen</author>
</authors>
<title>Authorship analysis in cybercrime investigation.</title>
<date>2003</date>
<booktitle>In Intelligence and Security Informatics,</booktitle>
<volume>2665</volume>
<pages>59--73</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2044" citStr="Zheng et al., 2003" startWordPosition="298" endWordPosition="301">ive language, and so on. This profile information is often of interest to marketing organisations for product promotional reasons as well as governments or law enforcements for crime investigation purposes. The particular application that motivates the present study is detection of phishing (Myers, 2007), the attempt to defraud through texts that are designed to deceive Internet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, an</context>
</contexts>
<marker>Zheng, Qin, Huang, Chen, 2003</marker>
<rawString>Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen. 2003. Authorship analysis in cybercrime investigation. In Intelligence and Security Informatics, volume 2665 of Lecture Notes in Computer Science, pages 59– 73. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>