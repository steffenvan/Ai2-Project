<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002867">
<title confidence="0.990395">
Discourse Segmentation of Multi-Party Conversation
</title>
<author confidence="0.998323">
Michel Galley Kathleen McKeown Eric Fosler-Lussier Hongyan Jing
</author>
<affiliation confidence="0.9839295">
Columbia University Columbia University IBM T.J. Watson Research Center
Computer Science Department Electrical Engineering Department
</affiliation>
<address confidence="0.9179155">
1214 Amsterdam Avenue 500 West 120th Street Yorktown Heights, NY 10598, USA
New York, NY 10027, USA New York, NY 10027, USA hjing@us.ibm.com
</address>
<email confidence="0.998227">
{galley,kathy}@cs.columbia.edu fosler@ieee.org
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999064375">
We present a domain-independent topic
segmentation algorithm for multi-party
speech. Our feature-based algorithm com-
bines knowledge about content using a
text-based algorithm as a feature and
about form using linguistic and acous-
tic cues about topic shifts extracted from
speech. This segmentation algorithm uses
automatically induced decision rules to
combine the different features. The em-
bedded text-based algorithm builds on lex-
ical cohesion and has performance compa-
rable to state-of-the-art algorithms based
on lexical information. A significant er-
ror reduction is obtained by combining the
two knowledge sources.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913272727273">
Topic segmentation aims to automatically divide text
documents, audio recordings, or video segments,
into topically related units. While extensive research
has targeted the problem of topic segmentation of
written texts and spoken monologues, few have stud-
ied the problem of segmenting conversations with
many participants (e.g., meetings). In this paper, we
present an algorithm for segmenting meeting tran-
scripts. This study uses recorded meetings of typi-
cally six to eight participants, in which the informal
style includes ungrammatical sentences and overlap-
ping speakers. These meetings generally do not have
pre-set agendas, and the topics discussed in the same
meeting may or may not related.
The meeting segmenter comprises two compo-
nents: one that capitalizes on word distribution to
identify homogeneous units that are topically cohe-
sive, and a second component that analyzes conver-
sational features of meeting transcripts that are in-
dicative of topic shifts, like silences, overlaps, and
speaker changes. We show that integrating features
from both components with a probabilistic classifier
(induced with c4.5rules) is very effective in improv-
ing performance.
In Section 2, we review previous approaches to
the segmentation problem applied to spoken and
written documents. In Section 3, we describe the
corpus of recorded meetings intended to be seg-
mented, and the annotation of its discourse structure.
In Section 4, we present our text-based segmenta-
tion component. This component mainly relies on
lexical cohesion, particularly term repetition, to de-
tect topic boundaries. We evaluated this segmenta-
tion against other lexical cohesion segmentation pro-
grams and show that the performance is state-of-the-
art. In the subsequent section, we describe conver-
sational features, such as silences, speaker change,
and other features like cue phrases. We present a
machine learning approach for integrating these con-
versational features with the text-based segmenta-
tion module. Experimental results show a marked
improvement in meeting segmentation with the in-
corporation of both sets of features. We close with
discussions and conclusions.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957896551724">
Existing approaches to textual segmentation can be
broadly divided into two categories. On the one
hand, many algorithms exploit the fact that topic
segments tend to be lexically cohesive. Embodi-
ments of this idea include semantic similarity (Mor-
ris and Hirst, 1991; Kozima, 1993), cosine similarity
in word vector space (Hearst, 1994), inter-sentence
similarity matrix (Reynar, 1994; Choi, 2000), en-
tity repetition (Kan et al., 1998), word frequency
models (Reynar, 1999), or adaptive language models
(Beeferman et al., 1999). Other algorithms exploit
a variety of linguistic features that may mark topic
boundaries, such as referential noun phrases (Pas-
sonneau and Litman, 1997).
In work on segmentation of spoken docu-
ments, intonational, prosodic, and acoustic indica-
tors are used to detect topic boundaries (Grosz and
Hirschberg, 1992; Nakatani et al., 1995; Hirschberg
and Nakatani, 1996; Passonneau and Litman, 1997;
Hirschberg and Nakatani, 1998; Beeferman et al.,
1999; T¨ur et al., 2001). Such indicators include
long pauses, shifts in speaking rate, great range in
F0 and intensity, and higher maximum accent peak.
These approaches use different learning mecha-
nisms to combine features, including decision trees
(Grosz and Hirschberg, 1992; Passonneau and Lit-
man, 1997; T¨ur et al., 2001) exponential models
(Beeferman et al., 1999) or other probabilistic mod-
els (Hajime et al., 1998; Reynar, 1999).
</bodyText>
<sectionHeader confidence="0.997392" genericHeader="method">
3 The ICSI Meeting Corpus
</sectionHeader>
<bodyText confidence="0.9996564">
We have evaluated our segmenter on the ICSI Meet-
ing corpus (Janin et al., 2003). This corpus is one of
a growing number of corpora with human-to-human
multi-party conversations. In this corpus, record-
ings of meetings ranged primarily over three differ-
ent recurring meeting types, all of which concerned
speech or language research.1 The average duration
is 60 minutes, with an average of 6.5 participants.
They were transcribed, and each conversation turn
was marked with the speaker, start time, end time,
and word content.
From the corpus, we selected 25 meetings to be
segmented, each by at least three subjects. We
opted for a linear representation of discourse, since
finer-grained discourse structures (e.g. (Grosz and
Sidner, 1986)) are generally considered to be diffi-
cult to mark reliably. Subjects were asked to mark
each speaker change (potential boundary) as either
boundary or non-boundary. In the resulting anno-
tation, the agreed segmentation based on majority
</bodyText>
<footnote confidence="0.983202">
1While it would be desirable to have a broader variety of
meetings, we hope that experiments on this corpus will still
carry some generality.
</footnote>
<bodyText confidence="0.9993991">
opinion contained 7.5 segments per meeting on av-
erage, while the average number of potential bound-
aries is 770. We used Cochran’s Q (1950) to eval-
uate the agreement among annotators. Cochran’s
test evaluates the null hypothesis that the number
of subjects assigning a boundary at any position is
randomly distributed. The test shows that the inter-
judge reliability is significant to the 0.05 level for 19
of the meetings, which seems to indicate that seg-
ment identification is a feasible task.2
</bodyText>
<sectionHeader confidence="0.806015" genericHeader="method">
4 Segmentation based on Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.9999226">
Previous work on discourse segmentation of written
texts indicates that lexical cohesion is a strong in-
dicator of discourse structure. Lexical cohesion is
a linguistic property that pertains to speech as well,
and is a linguistic phenomenon that can also be ex-
ploited in our case: while our data does not have
the same kind of syntactic and rhetorical structure
as written text, we nonetheless expect that informa-
tion from the written transcription alone should pro-
vide indications about topic boundaries. In this sec-
tion, we describe our work on LCseg, a topic seg-
menter based on lexical cohesion that can handle
both speech and text, but that is especially designed
to generate the lexical cohesion feature used in the
feature-based segmentation described in Section 5.
</bodyText>
<subsectionHeader confidence="0.995721">
4.1 Algorithm Description
</subsectionHeader>
<bodyText confidence="0.9993980625">
LCseg computes lexical chains, which are thought
to mirror the discourse structure of the underly-
ing text (Morris and Hirst, 1991). We ignore syn-
onymy and other semantic relations, building a re-
stricted model of lexical chains consisting of sim-
ple term repetitions, hypothesizing that major topic
shifts are likely to occur where strong term repeti-
tions start and end. While other relations between
lexical items also work as cohesive factors (e.g. be-
tween a term and its super-ordinate), the work on
linear topic segmentation reporting the most promis-
ing results account for term repetitions alone (Choi,
2000; Utiyama and Isahara, 2001).
The preprocessing steps of LCseg are common to
many segmentation algorithms. The input document
is first tokenized, non-content words are removed,
</bodyText>
<footnote confidence="0.9424215">
2Four other meetings failed short the significance test, while
there was little agreement on the two last ones (p &gt; 0.1).
</footnote>
<bodyText confidence="0.999889951219512">
and remaining words are stemmed using an exten-
sion of Porter’s stemming algorithm (Xu and Croft,
1998) that conflates stems using corpus statistics.
Stemming will allow our algorithm to more accu-
rately relate terms that are semantically close.
The core algorithm of LCseg has two main parts:
a method to identify and weight strong term repeti-
tions using lexical chains, and a method to hypothe-
size topic boundaries given the knowledge of multi-
ple, simultaneous chains of term repetitions.
A term is any stemmed content word within the
text. A lexical chain is constructed to consist of all
repetitions ranging from the first to the last appear-
ance of the term in the text. The chain is divided into
subchains when there is a long hiatus of h consecu-
tive sentences with no occurrence of the term, where
h is determined experimentally. For each hiatus, a
new division is made and thus, we avoid creating
weakly linked chains.
For all chains that have been identified, we use a
weighting scheme that we believe is appropriate to
the task of inducing the topical or sub-topical struc-
ture of text. The weighting scheme depends on two
factors:
Frequency: chains containing more repeated
terms receive a higher score.
Compactness: shorter chains receive a higher
weight than longer ones. If two chains of different
lengths contain the same number of terms, we assign
a higher score to the shortest one. Our assumption
is that the shorter one, being more compact, seems
to be a better indicator of lexical cohesion.3
We apply a variant of a metric commonly used
in information retrieval, TF.IDF (Salton and Buck-
ley, 1988), to score term repetitions. If R1 ... R,,, is
the set of all term repetitions collected in the text,
t1 ... t,,, the corresponding terms, L1 ... L,,, their re-
spective lengths,4 and L the length of the text, the
adapted metric is expressed as follows, combining
frequency (freq(ti)) of a term ti and the compact-
ness of its underlying chain:
</bodyText>
<equation confidence="0.990276">
score(Ri) = freq(ti) · log(LiL)
</equation>
<bodyText confidence="0.832009904761905">
3The latter parameter might seem controversial at first, and
one might assume that longer chains should receive a higher
score. However we point out that in a linear model of dis-
course, chains that almost span the entire text are barely indica-
tive of any structure (assuming boundaries are only hypothe-
sized where chains start and end).
4All lengths are expressed in number of sentences.
In the second part of the algorithm, we combine
information from all term repetitions to compute a
lexical cohesion score at each sentence break (or,
in the case of spoken conversations, speaker turn
break). This step of our algorithm is very similar
in spirit to TextTiling (Hearst, 1994). The idea is to
work with two adjacent analysis windows, each of
fixed size k. For each sentence break, we determine
a lexical cohesion function by computing the cosine
similarity at the transition between the two windows.
Instead of using word counts to compute similarity,
we analyze lexical chains that overlap with the two
windows. The similarity between windows (A and
B) is computed with:5
</bodyText>
<equation confidence="0.9531705">
cosie(A, B) _ �i wz&apos;A wz&apos;B
n
</equation>
<bodyText confidence="0.734949">
where
</bodyText>
<equation confidence="0.821722">
�
=score(Ri) if Ri overlaps Γ E {A, B}
wi,r 0 otherwise
</equation>
<bodyText confidence="0.999733230769231">
The similarity computed at each sentence break
produces a plot that shows how lexical cohesion
changes over time; an example is shown in Figure 1.
The lexical cohesion function is then smoothed us-
ing a moving average filter, and minima become po-
tential segment boundaries. Then, in a manner quite
similar to (Hearst, 1994), the algorithm determines
for every local minimum mi how sharp of a change
there is in the lexical cohesion function. The algo-
rithm looks on each side of mi for maxima of cohe-
sion, and once it eventually finds one on each side (l
and r), it computes the hypothesized segmentation
probability:
</bodyText>
<equation confidence="0.998907">
p(mi) = 1�[LCF(l) + LCF(r) − 2 · LCF(m)]
</equation>
<bodyText confidence="0.937233416666666">
where LCF(x) is the value of the lexical cohesion
function at x.
This score is supposed to capture the sharpness of
the change in lexical cohesion, and give probabilities
close to 1 for breaks like sentence 179 in Figure 1.
Finally, the algorithm selects the hypothesized
boundaries with the highest computed probabilities.
If the number of reference boundaries is unknown,
the algorithm has to make a guess. It computes the
5Normalizing anything in these windows has little ef-
fect, since the cosine similarity is scale invariant, that is
cosine(αxa, xb) = cosine(xa, xb) for α &gt; 0.
</bodyText>
<figure confidence="0.995343083333333">
[r� 2 r 2
wi,A L-�i wi,B
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
20 40 60 80 100 120 140 160 180 200 220 240 260
</figure>
<figureCaption confidence="0.99998">
Figure 1: Application of the LCseg algorithm on the concatenation of 16 WSJ stories. Numbers on the
</figureCaption>
<bodyText confidence="0.871627923076923">
x-axis represent sentence indices, and y-axis represents the lexical cohesion function. The representative
example presented here is segmented by LCseg with an error of Pk = 15.79, while the average performance
of the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments).
mean and the variance of the hypothesized probabil-
ities of all potential boundaries (local minima). As
we can see in Figure 1, there are many local minima
that do not correspond to actual boundaries. Thus,
we ignore all potential boundaries with a probability
lower than plimit. For the remaining points, we com-
pute the threshold using the average (µ) and standard
deviation (σ) of the p(mi) values, and each potential
boundary mi above the threshold µ−α·σ is hypoth-
esized as a real boundary.
</bodyText>
<subsectionHeader confidence="0.885088">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999673263157895">
We evaluate LCseg against two state-of-the-art seg-
mentation algorithms based on lexical cohesion
(Choi, 2000; Utiyama and Isahara, 2001). We use
the error metric Pk proposed by Beeferman et al.
(1999) to evaluate segmentation accuracy. It com-
putes the probability that sentences k units (e.g. sen-
tences) apart are incorrectly determined as being ei-
ther in different segments or in the same one. Since
it has been argued in (Pevzner and Hearst, 2002) that
Pk has some weaknesses, we also include results ac-
cording to the WindowDiff (WD) metric (which is
described in the same work).
A test corpus of concatenated6 texts extracted
from the Brown corpus was built by Choi (2000)
to evaluate several domain-independent segmenta-
tion algorithms. We reuse the same test corpus for
our evaluation, in addition to two other test corpora
we constructed to test how segmenters scale across
genres and how they perform with texts with various
</bodyText>
<footnote confidence="0.9855935">
6Concatenated documents correspond to reference seg-
ments.
</footnote>
<bodyText confidence="0.999934482758621">
number of segments.7 We designed two test corpora,
each of 500 documents, using concatenated texts
extracted from the TDT and WSJ corpora, ranging
from 4 to 22 in number of segments.
LCseg depends on several parameters. Parameter
tuning was performed on three tuning corpora of one
thousand texts each.8 We performed searches for the
optimal settings of the four tunable parameters in-
troduced above; the best performance was achieved
with h = 11 (hiatus length for dividing a chain into
parts), k = 2 (analysis window size), plimit = 0.1
and α= 2 (thresholding limits for the hypothesized
boundaries).
As shown in Table 1, our algorithm is signifi-
cantly better than (Choi, 2000) (labeled C99) on
all three test corpora, according to a one-sided t-
test of the null hypothesis of equal mean at the 0.01
level. It is not clear whether our algorithm is better
than (Utiyama and Isahara, 2001) (U00). When the
number of segments is provided to the algorithms,
our algorithm is significantly better than Utiyama’s
on WSJ, better on Brown (but not significant), and
significantly worse on TDT. When the number of
boundaries is unknown, our algorithm is insignifi-
cantly worse on Brown, but significantly better on
WSJ and TDT – the two corpora designed to have
a varying number of segments per document. In the
case of the Meeting corpus, none of the algorithms
are significantly different than the others, due to the
</bodyText>
<footnote confidence="0.963819">
7All texts in Choi’s test corpus have exactly 10 segments.
We are concerned that the adjustments of any algorithm param-
eters might overfit this predefined number of segments.
8These texts are different from the ones used for evaluation.
</footnote>
<table confidence="0.999901136363637">
Brown corpus
known unknown
Pk WD Pk WD
C99 11.19% 13.86% 12.07% 14.57%
U00 8.77% 9.44% 9.76% 10.32%
LCseg 8.69% 9.42% 10.49% 11.37%
p-val. 0.42 0.48 0.027 0.0037
TDT corpus
C99 9.37% 11.91% 10.18% 12.72%
U00 4.70% 6.29% 8.70% 11.12%
LCseg 6.15% 8.41% 6.95% 9.09%
p-val. 1.1e-05 2.8e-07 4.5e-05 2.8e-05
WSJ corpus
C99 19.61% 26.42% 22.32% 29.81%
U00 15.18% 21.54% 17.71% 24.06%
LCseg 12.21% 18.25% 15.31% 22.14%
p-val. 1.4e-08 1.7e-08 2.6e-04 0.0063
Meeting corpus
C99 33.79% 37.25% 47.42% 58.08%
U00 31.99% 34.49% 37.39% 40.43%
LCseg 26.37% 29.40% 31.91% 35.88%
p-val. 0.026 0.14 0.14 0.23
</table>
<tableCaption confidence="0.999722">
Table 1: Comparison C99 and U00. The p-values in
</tableCaption>
<bodyText confidence="0.999093909090909">
the table are the results of significance tests between
U00 and LCseg. Bold-faced values are scores that
are statistically significant.
small test set size.
In conclusion, LCseg has a performance compara-
ble to state-of-the-art text segmentation algorithms,
with the added advantage of computing a segmen-
tation probability at each potential boundary. This
information can be effectively used in the feature-
based segmenter to account for lexical cohesion, as
described in the next section.
</bodyText>
<sectionHeader confidence="0.999068" genericHeader="method">
5 Feature-based Segmentation
</sectionHeader>
<bodyText confidence="0.999960857142857">
In the previous section, we have concentrated exclu-
sively on the consideration of content (through lexi-
cal cohesion) to determine the structure of texts, ne-
glecting any influence of form. In this section, we
explore formal devices that are indicative of topic
shifts, and explain how we use these cues to build a
segmenter targeting conversational speech.
</bodyText>
<subsectionHeader confidence="0.98505">
5.1 Probabilistic Classifiers
</subsectionHeader>
<bodyText confidence="0.999986">
Topic segmentation is reduced here to a classifica-
tion problem, where each utterance break Bi is ei-
ther considered a topic boundary or not. We use
statistical modeling techniques to build a classifier
that uses local features (e.g. cue phrases, pauses)
to determine if an utterance break corresponds to
a topic boundary. We chose C4.5 and C4.5rules
(Quinlan, 1993), two programs to induce classifi-
cation rules in the form of decision trees and pro-
duction rules (respectively). C4.5 generates an un-
pruned decision tree, which is then analyzed by
C4.5rules to generate a set of pruned production
rules (it tries to find the most useful subset of them).
The advantage of pruned rules over decision trees is
that they are easier to analyze, and allow combina-
tion of features in the same rule (feature interactions
are explicit).
The greedy nature of decision rule learning algo-
rithms implies that a large set of features can lead
to bad performance and generalization capability. It
is desirable to remove redundant and irrelevant fea-
tures, especially in our case since we have little data
labeled with topic shifts; with a large set of fea-
tures, we would risk overfitting the data. We tried
to restrict ourselves to features whose inclusion is
motivated by previous work (pauses, speech rate)
and added features that are specific to multi-speaker
speech (overlap, changes in speaker activity).
</bodyText>
<subsectionHeader confidence="0.941853">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.999883380952381">
Cue phrases: previous work on segmentation has
found that discourse particles like now, well pro-
vide valuable information about the structure of texts
(Grosz and Sidner, 1986; Hirschberg and Litman,
1994; Passonneau and Litman, 1997). We analyzed
the correlation between words in the meeting cor-
pus and labeled topic boundaries, and automatically
extracted utterance-initial cue phrases9 that are sta-
tistically correlated with boundaries. For every word
in the meeting corpus, we counted the number of its
occurrences near any topic boundary, and its num-
ber of appearances overall. Then, we performed k2
significance tests (e.g. figure 2 for okay) under the
null hypothesis that no correlation exists. We se-
lected terms whose k2 value rejected the hypothesis
under a 0.01-level confidence (the rejection criterion
is k2 ≥ 6.635). Finally, induced cue phrases whose
usage has never been described in other work were
removed (marked with ∗ in Table 3). Indeed, there
is a risk that the automatically derived list of cue
phrases could be too specific to the word usage in
</bodyText>
<footnote confidence="0.848808333333333">
9As in (Litman and Passonneau, 1995), we restrict ourselves
to the first lexical item of any utterance, plus the second one if
the first item is also a cue word.
</footnote>
<table confidence="0.974298333333333">
Near boundary Distant
okay 64 740
Other 657 25896
</table>
<tableCaption confidence="0.93699">
Table 2: okay (x2 = 89.11, df = 1,p &lt; 0.01).
</tableCaption>
<table confidence="0.9996185">
okay 93.05 but 13.57
shall * 27.34 so 11.65
anyway 23.95 and 10.99
we’re * 17.67 should * 10.21
alright 16.09 good * 7.70
let’s * 14.54
</table>
<tableCaption confidence="0.999947">
Table 3: Automatically selected cue phrases.
</tableCaption>
<bodyText confidence="0.998418977272727">
these meetings.
Silences: previous work has found that ma-
jor shifts in topic typically show longer silences
(Passonneau and Litman, 1993; Hirschberg and
Nakatani, 1996). We investigated the presence of
silences in meetings and their correlation with topic
boundaries, and found it necessary to make a distinc-
tion between pauses and gaps (Levinson, 1983). A
pause is a silence that is attributable to a given party,
for example in the middle of an adjacency pair, or
when a speaker pauses in the middle of her speech.
Gaps are silences not attributable to any party, and
last until a speaker takes the initiative of continuing
the discussion. As an approximation of this distinc-
tion, we classified a silence that follows a question or
in the middle of somebody’s speech as a pause, and
any other silences as a gap. While the correlation be-
tween long silences and discourse boundaries seem
to be less pervasive in meetings than in other speech
corpora, we have noticed that some topic boundaries
are preceded (within some window) by numerous
gaps. However, we found little correlation between
pauses and topic boundaries.
Overlaps: we also analyzed the distribution of
overlapping speech by counting the average overlap
rate within some window. We noticed that, many
times, the beginning of segments are characterized
by having little overlapping speech.
Speaker change: we sometimes noticed a corre-
lation between topic boundaries and sudden changes
in speaker activity. For example, in Figure 2, it
is clear that the contribution of individual speakers
to the discussion can greatly change from one dis-
course unit to the next. We try to capture significant
changes in speakership by measuring the dissimilar-
ity between two analysis windows. For each poten-
tial boundary, we count for each speaker i the num-
ber of words that are uttered before (LZ) and after
(RZ) the potential boundary (we limit our analysis
to a window of fixed size). The two distributions
are normalized to form two probability distributions
l and r, and significant changes of speakership are
detected by computing their Jensen-Shannon diver-
gence:
</bodyText>
<equation confidence="0.926093">
JS(l, r) = 12[D(l||avgl,r) + D(r||avgl,r)]
</equation>
<bodyText confidence="0.9999392">
where D(l||r) is the KL-divergence between the
two distributions.
Lexical cohesion: we also incorporated the lexi-
cal cohesion function computed by LCseg as a fea-
ture of the multi-source segmenter in a manner simi-
lar to the knowledge source combination performed
by (Beeferman et al., 1999) and (T¨ur et al., 2001).
Note that we use both the posterior estimate com-
puted by LCseg and the raw lexical cohesion func-
tion as features of the system.
</bodyText>
<subsectionHeader confidence="0.996316">
5.3 Features: Selection and Combination
</subsectionHeader>
<bodyText confidence="0.9999899375">
For every potential boundary BZ, the classifier ana-
lyzes features in a window surrounding BZ to decide
whether it is a topic boundary or not. It is generally
unclear what is the optimal window size and how
features should be analyzed. Windows of various
sizes can lead to different levels of prediction, and
in some cases, it might be more appropriate to only
extract features preceding or following BZ.
We avoided making arbitrary choices of parame-
ters; instead, for any feature F and a set F1, ... , Fn
of possible ways to measure the feature (different
window sizes, different directions), we picked the FZ
that is in isolation the best predictor of topic bound-
aries (among F1, ... , Fn). Table 4 presents for each
feature the analysis mode that is the most useful on
the training data.
</bodyText>
<subsectionHeader confidence="0.719694">
5.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.99924325">
We performed 25-fold cross-validation for evaluat-
ing the induced probabilistic classifier, computing
the average of Pk and WD on the held-out meet-
ings. Feature selection and decision rule learning
</bodyText>
<figure confidence="0.9975955">
0 10
20 30
</figure>
<figureCaption confidence="0.9988905">
Figure 2: speaker activity in a meeting. Each row represent the speech activity of one speaker, utterance of
words being represented as black. Vertical lines represent topic shifts. The x-axis represents time.
</figureCaption>
<table confidence="0.999426166666667">
Feature Tag Size (sec.) Side
Cue phrases CUE 5 both
Silence (gaps) SIL 30 left
Overlap† OVR 30 right
Speaker activity ACT 5 both
Lexical cohesion LC 30 both
</table>
<tableCaption confidence="0.812697666666667">
†: the size of the window that was used to compute the
JS-divergence was also determined automatically.
Table 4: Parameters for feature analysis.
</tableCaption>
<bodyText confidence="0.999943333333333">
is always performed on sets of 24 meetings, while
the held-out data is used for testing. Table 5 gives
some examples of the type of rules that are learned.
The first rule states that if the value for the lexical
cohesion (LC) function is low at the current sen-
tence break, there is at least one CUE phrase, there
is less than three seconds of silence to the left of the
break,10 and a single speaker holds the floor for a
longer period of time than usual to the right of the
break, then we have a topic break. In general, we
found that the derived rules show that lexical cohe-
sion plays a stronger role than most other features
in determining topic breaks. Nonetheless, the quan-
titative results summarized in table 6, which corre-
spond to the average performance on the held-out
sets, show that the integration of conversational fea-
tures with the text-based segmenter outperforms ei-
ther alone.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999062">
We presented a domain-independent segmentation
algorithm for multi-party conversation that inte-
grates features based on content with features based
on form. The learned combination of features results
in a significant increase in accuracy over previous
</bodyText>
<footnote confidence="0.930962">
10Note that rules are not always meaningful in isolation and
it is likely that a subordinate rule in the tree to this one would do
further tests on silence to determine if a topic boundary exists.
</footnote>
<table confidence="0.999680444444444">
Condition Decision Conf.
LC &lt; 0.67, CUE &gt; 1, yes 94.1
OVR &lt; 1.20, SIL &lt; 3.42
LC &lt; 0.35, SIL &gt; 3.42, yes 92.2
OVR &lt; 4.55
CUE &gt; 1, ACT &gt; 0.1768, yes 91.6
OVR &lt; 1.20, LC &lt; 0.67
. . .
default no
</table>
<tableCaption confidence="0.991631333333333">
Table 5: A selection of the most useful rules learned
by C4.5rules along with their confidence levels.
Times for OVR and SIL are expressed in seconds.
</tableCaption>
<table confidence="0.9996798">
Pk WD
feature-based 23.00% 25.47%
LCseg 31.91% 35.88%
U00 37.39% 40.43%
p-value 2.14e-04 3.30e-04
</table>
<tableCaption confidence="0.948238">
Table 6: Performance of the feature-based seg-
menter on the test data.
</tableCaption>
<bodyText confidence="0.967318">
approaches to segmentation when applied to meet-
ings. Features based on form that are likely to in-
dicate topic shifts are automatically extracted from
speech. Content based features are computed by a
segmentation algorithm that utilizes a metric of lex-
ical cohesion and that performs as well as state-of-
the-art text-based segmentation techniques. It works
both with written and spoken texts. The text-based
segmentation approach alone, when applied to meet-
ings, outperforms all other segmenters, although the
difference is not statistically significant.
In future work, we would like to investigate the
effects of adding prosodic features, such as pitch
ranges, to our segmenter, as well as the effect of
using errorful speech recognition transcripts as op-
posed to manually transcribed utterances.
An implementation of our lexical cohesion seg-
menter is freely available for educational or research
purposes.11
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999748">
We are grateful to Julia Hirschberg, Dan Ellis, Eliz-
abeth Shriberg, and Mari Ostendorf for their helpful
advice. We thank our ICSI project partners for grant-
ing us access to the meeting corpus and for useful
discussions. This work was funded under the NSF
project Mapping Meetings (IIS-012196).
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999749802631579">
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34(1–3):177–210.
F. Choi. 2000. Advances in domain independent linear
text segmentation. In Proc. ofNAACL’00.
W. Cochran. 1950. The comparison of percentages in
matched samples. Biometrika, 37:256–266.
B. Grosz and J. Hirschberg. 1992. Some intonational
characteristics of discourse structure. In Proc. of
ICSLP-92, pages 429–432.
B. Grosz and C. Sidner. 1986. Attention, intentions and
the structure of discourse. Computational Linguistics,
12(3).
M. Hajime, H. Takeo, and O. Manabu. 1998. Text seg-
mentation with multiple surface linguistic cues. In
COLING-ACL, pages 881–885.
M. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of the ACL.
J. Hirschberg and D. Litman. 1994. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501–530.
J. Hirschberg and C. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. of the ACL.
J. Hirschberg and C. Nakatani. 1998. Acoustic indicators
of topic segmentation. In Proc. ofICSLP.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. ofICASSP-03, Hong Kong (to appear).
11http://www.cs.columbia.edu/˜galley/research.html
M.-Y. Kan, J. Klavans, and K. McKeown. 1998. Linear
segmentation and segment significance. In Proc. 6th
Workshop on Very Large Corpora (WVLC-98).
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. of the ACL.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
D. Litman and R. Passonneau. 1995. Combining multi-
ple knowledge sources for discourse segmentation. In
Proc. of the ACL.
J. Morris and G. Hirst. 1991. Lexcial cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17:21–48.
C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Dis-
course structure in spoken language: Studies on
speech corpora. In AAAI-95 Symposium on Empirical
Methods in Discourse Interpretation.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. of the ACL.
R. Passonneau and D. Litman. 1997. Discourse seg-
mentation by human and automated means. Compu-
tational Linguistics, 23(1):103–139.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmenta-
tion. Computational Linguistics, 28 (1):19–36.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Machine Learning. Morgan Kaufmann.
J. Reynar. 1994. An automatic method of finding topic
boundaries. In Proc. of the ACL.
J. Reynar. 1999. Statistical models for topic segmenta-
tion. In Proc. of the ACL.
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513–523.
G. T¨ur, D. Hakkani-T¨ur, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31–57.
M. Utiyama and H. Isahara. 2001. A statistical model
for domain-independent text segmentation. In Proc. of
the ACL.
J. Xu and B. Croft. 1998. Corpus-based stemming using
cooccurrence of word variants. ACM Transactions on
Information Systems, 16(1):61–81.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993718">
<title confidence="0.999775">Discourse Segmentation of Multi-Party Conversation</title>
<author confidence="0.999819">Michel Galley Kathleen McKeown Eric Fosler-Lussier Hongyan Jing</author>
<affiliation confidence="0.9992665">Columbia University Columbia University IBM T.J. Watson Research Center Computer Science Department Electrical Engineering Department</affiliation>
<address confidence="0.999784">1214 Amsterdam Avenue 500 West 120th Street Yorktown Heights, NY 10598, USA York, NY 10027, USA New York, NY 10027, USA</address>
<email confidence="0.998069">fosler@ieee.org</email>
<abstract confidence="0.999877">We present a domain-independent topic segmentation algorithm for multi-party speech. Our feature-based algorithm comknowledge about a text-based algorithm as a feature and linguistic and acoustic cues about topic shifts extracted from speech. This segmentation algorithm uses automatically induced decision rules to combine the different features. The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information. A significant error reduction is obtained by combining the two knowledge sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="3768" citStr="Beeferman et al., 1999" startWordPosition="541" endWordPosition="544">ncorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. T</context>
<context position="13639" citStr="Beeferman et al. (1999)" startWordPosition="2176" endWordPosition="2179">local minima). As we can see in Figure 1, there are many local minima that do not correspond to actual boundaries. Thus, we ignore all potential boundaries with a probability lower than plimit. For the remaining points, we compute the threshold using the average (µ) and standard deviation (σ) of the p(mi) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms. We reuse the same test corpus for our evaluation, in additio</context>
<context position="22976" citStr="Beeferman et al., 1999" startWordPosition="3699" endWordPosition="3702">uttered before (LZ) and after (RZ) the potential boundary (we limit our analysis to a window of fixed size). The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon divergence: JS(l, r) = 12[D(l||avgl,r) + D(r||avgl,r)] where D(l||r) is the KL-divergence between the two distributions. Lexical cohesion: we also incorporated the lexical cohesion function computed by LCseg as a feature of the multi-source segmenter in a manner similar to the knowledge source combination performed by (Beeferman et al., 1999) and (T¨ur et al., 2001). Note that we use both the posterior estimate computed by LCseg and the raw lexical cohesion function as features of the system. 5.3 Features: Selection and Combination For every potential boundary BZ, the classifier analyzes features in a window surrounding BZ to decide whether it is a topic boundary or not. It is generally unclear what is the optimal window size and how features should be analyzed. Windows of various sizes can lead to different levels of prediction, and in some cases, it might be more appropriate to only extract features preceding or following BZ. We</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1–3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL’00.</booktitle>
<contexts>
<context position="3638" citStr="Choi, 2000" startWordPosition="523" endWordPosition="524"> the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001)</context>
<context position="7739" citStr="Choi, 2000" startWordPosition="1174" endWordPosition="1175">LCseg computes lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2Four other meetings failed short the significance test, while there was little agreement on the two last ones (p &gt; 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method </context>
<context position="13547" citStr="Choi, 2000" startWordPosition="2162" endWordPosition="2163">nd the variance of the hypothesized probabilities of all potential boundaries (local minima). As we can see in Figure 1, there are many local minima that do not correspond to actual boundaries. Thus, we ignore all potential boundaries with a probability lower than plimit. For the remaining points, we compute the threshold using the average (µ) and standard deviation (σ) of the p(mi) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-indep</context>
<context position="15106" citStr="Choi, 2000" startWordPosition="2419" endWordPosition="2420">ments, using concatenated texts extracted from the TDT and WSJ corpora, ranging from 4 to 22 in number of segments. LCseg depends on several parameters. Parameter tuning was performed on three tuning corpora of one thousand texts each.8 We performed searches for the optimal settings of the four tunable parameters introduced above; the best performance was achieved with h = 11 (hiatus length for dividing a chain into parts), k = 2 (analysis window size), plimit = 0.1 and α= 2 (thresholding limits for the hypothesized boundaries). As shown in Table 1, our algorithm is significantly better than (Choi, 2000) (labeled C99) on all three test corpora, according to a one-sided ttest of the null hypothesis of equal mean at the 0.01 level. It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). When the number of segments is provided to the algorithms, our algorithm is significantly better than Utiyama’s on WSJ, better on Brown (but not significant), and significantly worse on TDT. When the number of boundaries is unknown, our algorithm is insignificantly worse on Brown, but significantly better on WSJ and TDT – the two corpora designed to have a varying number of segmen</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>F. Choi. 2000. Advances in domain independent linear text segmentation. In Proc. ofNAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cochran</author>
</authors>
<title>The comparison of percentages in matched samples.</title>
<date>1950</date>
<journal>Biometrika,</journal>
<pages>37--256</pages>
<marker>Cochran, 1950</marker>
<rawString>W. Cochran. 1950. The comparison of percentages in matched samples. Biometrika, 37:256–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>J Hirschberg</author>
</authors>
<title>Some intonational characteristics of discourse structure.</title>
<date>1992</date>
<booktitle>In Proc. of ICSLP-92,</booktitle>
<pages>429--432</pages>
<contexts>
<context position="4080" citStr="Grosz and Hirschberg, 1992" startWordPosition="588" endWordPosition="591">a include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We</context>
</contexts>
<marker>Grosz, Hirschberg, 1992</marker>
<rawString>B. Grosz and J. Hirschberg. 1992. Some intonational characteristics of discourse structure. In Proc. of ICSLP-92, pages 429–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="5416" citStr="Grosz and Sidner, 1986" startWordPosition="797" endWordPosition="800">mber of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 meetings to be segmented, each by at least three subjects. We opted for a linear representation of discourse, since finer-grained discourse structures (e.g. (Grosz and Sidner, 1986)) are generally considered to be difficult to mark reliably. Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary. In the resulting annotation, the agreed segmentation based on majority 1While it would be desirable to have a broader variety of meetings, we hope that experiments on this corpus will still carry some generality. opinion contained 7.5 segments per meeting on average, while the average number of potential boundaries is 770. We used Cochran’s Q (1950) to evaluate the agreement among annotators. Cochran’s test evaluates the null hypo</context>
<context position="19194" citStr="Grosz and Sidner, 1986" startWordPosition="3076" endWordPosition="3079">ization capability. It is desirable to remove redundant and irrelevant features, especially in our case since we have little data labeled with topic shifts; with a large set of features, we would risk overfitting the data. We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to multi-speaker speech (overlap, changes in speaker activity). 5.2 Features Cue phrases: previous work on segmentation has found that discourse particles like now, well provide valuable information about the structure of texts (Grosz and Sidner, 1986; Hirschberg and Litman, 1994; Passonneau and Litman, 1997). We analyzed the correlation between words in the meeting corpus and labeled topic boundaries, and automatically extracted utterance-initial cue phrases9 that are statistically correlated with boundaries. For every word in the meeting corpus, we counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed k2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose k2 value rejected the hypothesis under a 0.01-le</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. Grosz and C. Sidner. 1986. Attention, intentions and the structure of discourse. Computational Linguistics, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hajime</author>
<author>H Takeo</author>
<author>O Manabu</author>
</authors>
<title>Text segmentation with multiple surface linguistic cues.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>881--885</pages>
<contexts>
<context position="4635" citStr="Hajime et al., 1998" startWordPosition="673" endWordPosition="676"> are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing number of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 </context>
</contexts>
<marker>Hajime, Takeo, Manabu, 1998</marker>
<rawString>M. Hajime, H. Takeo, and O. Manabu. 1998. Text segmentation with multiple surface linguistic cues. In COLING-ACL, pages 881–885.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="3577" citStr="Hearst, 1994" startWordPosition="516" endWordPosition="517">ing approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg a</context>
<context position="10701" citStr="Hearst, 1994" startWordPosition="1672" endWordPosition="1673"> might assume that longer chains should receive a higher score. However we point out that in a linear model of discourse, chains that almost span the entire text are barely indicative of any structure (assuming boundaries are only hypothesized where chains start and end). 4All lengths are expressed in number of sentences. In the second part of the algorithm, we combine information from all term repetitions to compute a lexical cohesion score at each sentence break (or, in the case of spoken conversations, speaker turn break). This step of our algorithm is very similar in spirit to TextTiling (Hearst, 1994). The idea is to work with two adjacent analysis windows, each of fixed size k. For each sentence break, we determine a lexical cohesion function by computing the cosine similarity at the transition between the two windows. Instead of using word counts to compute similarity, we analyze lexical chains that overlap with the two windows. The similarity between windows (A and B) is computed with:5 cosie(A, B) _ �i wz&apos;A wz&apos;B n where � =score(Ri) if Ri overlaps Γ E {A, B} wi,r 0 otherwise The similarity computed at each sentence break produces a plot that shows how lexical cohesion changes over time</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>M. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="19223" citStr="Hirschberg and Litman, 1994" startWordPosition="3080" endWordPosition="3083">s desirable to remove redundant and irrelevant features, especially in our case since we have little data labeled with topic shifts; with a large set of features, we would risk overfitting the data. We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to multi-speaker speech (overlap, changes in speaker activity). 5.2 Features Cue phrases: previous work on segmentation has found that discourse particles like now, well provide valuable information about the structure of texts (Grosz and Sidner, 1986; Hirschberg and Litman, 1994; Passonneau and Litman, 1997). We analyzed the correlation between words in the meeting corpus and labeled topic boundaries, and automatically extracted utterance-initial cue phrases9 that are statistically correlated with boundaries. For every word in the meeting corpus, we counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed k2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose k2 value rejected the hypothesis under a 0.01-level confidence (the rejection</context>
</contexts>
<marker>Hirschberg, Litman, 1994</marker>
<rawString>J. Hirschberg and D. Litman. 1994. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>C Nakatani</author>
</authors>
<title>A prosodic analysis of discourse segments in direction-giving monologues.</title>
<date>1996</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="4134" citStr="Hirschberg and Nakatani, 1996" startWordPosition="596" endWordPosition="599">91; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corp</context>
<context position="20695" citStr="Hirschberg and Nakatani, 1996" startWordPosition="3328" endWordPosition="3331">ic to the word usage in 9As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. Near boundary Distant okay 64 740 Other 657 25896 Table 2: okay (x2 = 89.11, df = 1,p &lt; 0.01). okay 93.05 but 13.57 shall * 27.34 so 11.65 anyway 23.95 and 10.99 we’re * 17.67 should * 10.21 alright 16.09 good * 7.70 let’s * 14.54 Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in the middle of somebody’s speech</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>J. Hirschberg and C. Nakatani. 1996. A prosodic analysis of discourse segments in direction-giving monologues. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>C Nakatani</author>
</authors>
<title>Acoustic indicators of topic segmentation.</title>
<date>1998</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="4194" citStr="Hirschberg and Nakatani, 1998" startWordPosition="604" endWordPosition="607">earst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing num</context>
</contexts>
<marker>Hirschberg, Nakatani, 1998</marker>
<rawString>J. Hirschberg and C. Nakatani. 1998. Acoustic indicators of topic segmentation. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proc. ofICASSP-03, Hong Kong</booktitle>
<note>(to appear). 11http://www.cs.columbia.edu/˜galley/research.html</note>
<contexts>
<context position="4757" citStr="Janin et al., 2003" startWordPosition="695" endWordPosition="698">ssonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing number of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 meetings to be segmented, each by at least three subjects. We opted for a linear representation of discourse, since finer-</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proc. ofICASSP-03, Hong Kong (to appear). 11http://www.cs.columbia.edu/˜galley/research.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-Y Kan</author>
<author>J Klavans</author>
<author>K McKeown</author>
</authors>
<title>Linear segmentation and segment significance.</title>
<date>1998</date>
<booktitle>In Proc. 6th Workshop on Very Large Corpora (WVLC-98).</booktitle>
<contexts>
<context position="3676" citStr="Kan et al., 1998" startWordPosition="528" endWordPosition="531">ule. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses,</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>M.-Y. Kan, J. Klavans, and K. McKeown. 1998. Linear segmentation and segment significance. In Proc. 6th Workshop on Very Large Corpora (WVLC-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="3522" citStr="Kozima, 1993" startWordPosition="508" endWordPosition="509">r features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Na</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>H. Kozima. 1993. Text segmentation based on similarity between words. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Levinson</author>
</authors>
<title>Pragmatics.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="20880" citStr="Levinson, 1983" startWordPosition="3359" endWordPosition="3360">Distant okay 64 740 Other 657 25896 Table 2: okay (x2 = 89.11, df = 1,p &lt; 0.01). okay 93.05 but 13.57 shall * 27.34 so 11.65 anyway 23.95 and 10.99 we’re * 17.67 should * 10.21 alright 16.09 good * 7.70 let’s * 14.54 Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in the middle of somebody’s speech as a pause, and any other silences as a gap. While the correlation between long silences and discourse boundaries seem to be less pervasive in meetings than in other speech corpora, we</context>
</contexts>
<marker>Levinson, 1983</marker>
<rawString>S. Levinson. 1983. Pragmatics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>R Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="20125" citStr="Litman and Passonneau, 1995" startWordPosition="3226" endWordPosition="3229">we counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed k2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose k2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is k2 ≥ 6.635). Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3). Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in 9As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. Near boundary Distant okay 64 740 Other 657 25896 Table 2: okay (x2 = 89.11, df = 1,p &lt; 0.01). okay 93.05 but 13.57 shall * 27.34 so 11.65 anyway 23.95 and 10.99 we’re * 17.67 should * 10.21 alright 16.09 good * 7.70 let’s * 14.54 Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence</context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>D. Litman and R. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexcial cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--21</pages>
<contexts>
<context position="3507" citStr="Morris and Hirst, 1991" startWordPosition="503" endWordPosition="507">speaker change, and other features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hi</context>
<context position="7259" citStr="Morris and Hirst, 1991" startWordPosition="1095" endWordPosition="1098">have the same kind of syntactic and rhetorical structure as written text, we nonetheless expect that information from the written transcription alone should provide indications about topic boundaries. In this section, we describe our work on LCseg, a topic segmenter based on lexical cohesion that can handle both speech and text, but that is especially designed to generate the lexical cohesion feature used in the feature-based segmentation described in Section 5. 4.1 Algorithm Description LCseg computes lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input doc</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>J. Morris and G. Hirst. 1991. Lexcial cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17:21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nakatani</author>
<author>J Hirschberg</author>
<author>B Grosz</author>
</authors>
<title>Discourse structure in spoken language: Studies on speech corpora.</title>
<date>1995</date>
<booktitle>In AAAI-95 Symposium on Empirical Methods in Discourse Interpretation.</booktitle>
<contexts>
<context position="4103" citStr="Nakatani et al., 1995" startWordPosition="592" endWordPosition="595">y (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our seg</context>
</contexts>
<marker>Nakatani, Hirschberg, Grosz, 1995</marker>
<rawString>C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Discourse structure in spoken language: Studies on speech corpora. In AAAI-95 Symposium on Empirical Methods in Discourse Interpretation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>D Litman</author>
</authors>
<title>Intention-based segmentation: Human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="20663" citStr="Passonneau and Litman, 1993" startWordPosition="3324" endWordPosition="3327">e phrases could be too specific to the word usage in 9As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. Near boundary Distant okay 64 740 Other 657 25896 Table 2: okay (x2 = 89.11, df = 1,p &lt; 0.01). okay 93.05 but 13.57 shall * 27.34 so 11.65 anyway 23.95 and 10.99 we’re * 17.67 should * 10.21 alright 16.09 good * 7.70 let’s * 14.54 Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in</context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>R. Passonneau and D. Litman. 1993. Intention-based segmentation: Human reliability and correlation with linguistic cues. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>D Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="3922" citStr="Passonneau and Litman, 1997" startWordPosition="563" endWordPosition="567">roadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; </context>
<context position="19253" citStr="Passonneau and Litman, 1997" startWordPosition="3084" endWordPosition="3087">nt and irrelevant features, especially in our case since we have little data labeled with topic shifts; with a large set of features, we would risk overfitting the data. We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to multi-speaker speech (overlap, changes in speaker activity). 5.2 Features Cue phrases: previous work on segmentation has found that discourse particles like now, well provide valuable information about the structure of texts (Grosz and Sidner, 1986; Hirschberg and Litman, 1994; Passonneau and Litman, 1997). We analyzed the correlation between words in the meeting corpus and labeled topic boundaries, and automatically extracted utterance-initial cue phrases9 that are statistically correlated with boundaries. For every word in the meeting corpus, we counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed k2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose k2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is k2 ≥ 6.635). Fin</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>R. Passonneau and D. Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>1--19</pages>
<contexts>
<context position="13888" citStr="Pevzner and Hearst, 2002" startWordPosition="2218" endWordPosition="2221">ing the average (µ) and standard deviation (σ) of the p(mi) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms. We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various 6Concatenated documents correspond to reference segments. number of segments.7 We designed two test corpora, each of 5</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>L. Pevzner and M. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28 (1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning. Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="17980" citStr="Quinlan, 1993" startWordPosition="2880" endWordPosition="2881">ructure of texts, neglecting any influence of form. In this section, we explore formal devices that are indicative of topic shifts, and explain how we use these cues to build a segmenter targeting conversational speech. 5.1 Probabilistic Classifiers Topic segmentation is reduced here to a classification problem, where each utterance break Bi is either considered a topic boundary or not. We use statistical modeling techniques to build a classifier that uses local features (e.g. cue phrases, pauses) to determine if an utterance break corresponds to a topic boundary. We chose C4.5 and C4.5rules (Quinlan, 1993), two programs to induce classification rules in the form of decision trees and production rules (respectively). C4.5 generates an unpruned decision tree, which is then analyzed by C4.5rules to generate a set of pruned production rules (it tries to find the most useful subset of them). The advantage of pruned rules over decision trees is that they are easier to analyze, and allow combination of features in the same rule (feature interactions are explicit). The greedy nature of decision rule learning algorithms implies that a large set of features can lead to bad performance and generalization </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs for Machine Learning. Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="3625" citStr="Reynar, 1994" startWordPosition="521" endWordPosition="522"> features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur </context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>J. Reynar. 1994. An automatic method of finding topic boundaries. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="3714" citStr="Reynar, 1999" startWordPosition="535" endWordPosition="536">provement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range </context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>J. Reynar. 1999. Statistical models for topic segmentation. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="9651" citStr="Salton and Buckley, 1988" startWordPosition="1490" endWordPosition="1494">eme that we believe is appropriate to the task of inducing the topical or sub-topical structure of text. The weighting scheme depends on two factors: Frequency: chains containing more repeated terms receive a higher score. Compactness: shorter chains receive a higher weight than longer ones. If two chains of different lengths contain the same number of terms, we assign a higher score to the shortest one. Our assumption is that the shorter one, being more compact, seems to be a better indicator of lexical cohesion.3 We apply a variant of a metric commonly used in information retrieval, TF.IDF (Salton and Buckley, 1988), to score term repetitions. If R1 ... R,,, is the set of all term repetitions collected in the text, t1 ... t,,, the corresponding terms, L1 ... L,,, their respective lengths,4 and L the length of the text, the adapted metric is expressed as follows, combining frequency (freq(ti)) of a term ti and the compactness of its underlying chain: score(Ri) = freq(ti) · log(LiL) 3The latter parameter might seem controversial at first, and one might assume that longer chains should receive a higher score. However we point out that in a linear model of discourse, chains that almost span the entire text a</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G T¨ur</author>
<author>D Hakkani-T¨ur</author>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Integrating prosodic and lexical cues for automatic topic segmentation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<marker>T¨ur, Hakkani-T¨ur, Stolcke, Shriberg, 2001</marker>
<rawString>G. T¨ur, D. Hakkani-T¨ur, A. Stolcke, and E. Shriberg. 2001. Integrating prosodic and lexical cues for automatic topic segmentation. Computational Linguistics, 27(1):31–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="7767" citStr="Utiyama and Isahara, 2001" startWordPosition="1176" endWordPosition="1179">es lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2Four other meetings failed short the significance test, while there was little agreement on the two last ones (p &gt; 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method to identify and weight stron</context>
<context position="13575" citStr="Utiyama and Isahara, 2001" startWordPosition="2164" endWordPosition="2167">nce of the hypothesized probabilities of all potential boundaries (local minima). As we can see in Figure 1, there are many local minima that do not correspond to actual boundaries. Thus, we ignore all potential boundaries with a probability lower than plimit. For the remaining points, we compute the threshold using the average (µ) and standard deviation (σ) of the p(mi) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorith</context>
<context position="15315" citStr="Utiyama and Isahara, 2001" startWordPosition="2455" endWordPosition="2458">uning corpora of one thousand texts each.8 We performed searches for the optimal settings of the four tunable parameters introduced above; the best performance was achieved with h = 11 (hiatus length for dividing a chain into parts), k = 2 (analysis window size), plimit = 0.1 and α= 2 (thresholding limits for the hypothesized boundaries). As shown in Table 1, our algorithm is significantly better than (Choi, 2000) (labeled C99) on all three test corpora, according to a one-sided ttest of the null hypothesis of equal mean at the 0.01 level. It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). When the number of segments is provided to the algorithms, our algorithm is significantly better than Utiyama’s on WSJ, better on Brown (but not significant), and significantly worse on TDT. When the number of boundaries is unknown, our algorithm is insignificantly worse on Brown, but significantly better on WSJ and TDT – the two corpora designed to have a varying number of segments per document. In the case of the Meeting corpus, none of the algorithms are significantly different than the others, due to the 7All texts in Choi’s test corpus have exactly 10 segments. We are concerned th</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>M. Utiyama and H. Isahara. 2001. A statistical model for domain-independent text segmentation. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Corpus-based stemming using cooccurrence of word variants.</title>
<date>1998</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="8140" citStr="Xu and Croft, 1998" startWordPosition="1235" endWordPosition="1238">etween lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2Four other meetings failed short the significance test, while there was little agreement on the two last ones (p &gt; 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method to identify and weight strong term repetitions using lexical chains, and a method to hypothesize topic boundaries given the knowledge of multiple, simultaneous chains of term repetitions. A term is any stemmed content word within the text. A lexical chain is constructed to consist of all repetitions ranging from the first to the last appearance of the term in the text. The chain is divided into sub</context>
</contexts>
<marker>Xu, Croft, 1998</marker>
<rawString>J. Xu and B. Croft. 1998. Corpus-based stemming using cooccurrence of word variants. ACM Transactions on Information Systems, 16(1):61–81.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>