<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001558">
<title confidence="0.9986765">
Weblog Classification for Fast Splog Filtering:
A URL Language Model Segmentation Approach
</title>
<author confidence="0.961036">
Franco Salvetti†* Nicolas Nicolov*
</author>
<email confidence="0.816276">
franco.salvetti@colorado.edu nicolas@umbrialistens.com
</email>
<affiliation confidence="0.955826">
†Dept. of Computer Science, Univ. of Colorado at Boulder, 430 UCB, Boulder, CO 80309-0430
*Umbria, Inc., 1655 Walnut Str, Boulder, CO 80302
</affiliation>
<sectionHeader confidence="0.988196" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748117647059">
This paper shows that in the context of
statistical weblog classification for splog
filtering based on n-grams of tokens in
the URL, further segmenting the URLs
beyond the standard punctuation is help-
ful. Many splog URLs contain phrases
in which the words are glued together in
order to avoid splog filtering techniques
based on punctuation segmentation and
unigrams. A technique which segments
long tokens into the words forming the
phrase is proposed and evaluated. The re-
sulting tokens are used as features for a
weblog classifier whose accuracy is sim-
ilar to that of humans (78% vs. 76%) and
reaches 93.3% of precision in identifying
splogs with recall of 50.9%.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992038461539">
The blogosphere, which is a subset of the web and
is comprised of personal electronic journals (we-
blogs) currently encompasses 27.2 million pages
and doubles in size every 5.5 months (Technorati,
2006). The information contained in the blogo-
sphere has been proven valuable for applications
such as marketing intelligence, trend discovery, and
opinion tracking (Hurst, 2005). Unfortunately in the
last year the blogosphere has been heavily polluted
with spam weblogs (called splogs) which are we-
blogs used for different purposes, including promot-
ing affiliated websites (Wikipedia, 2006). Splogs
can skew the results of applications meant to quan-
titatively analyze the blogosphere. Sophisticated
content-based methods or methods based on link
analysis (Gy¨ongyi et al., 2004), while providing ef-
fective splog filtering, require extra web crawling
and can be slow. While a combination of approaches
is necessary to provide adequate splog filtering, sim-
ilar to (Kan &amp; Thi, 2005), we propose, as a pre-
liminary step in the overall splog filtering, a fast,
lightweight and accurate method merely based on
the analysis of the URL of the weblog without con-
sidering its content.
For quantitative and qualitative analysis of the
content of the blogosphere, it is acceptable to elim-
inate a small fraction of good data from analysis
as long as the remainder of the data is splog-free.
This elimination should be kept to a minimum to
preserve counts needed for reliable analysis. When
using an ensemble of methods for comprehensive
splog filtering it is acceptable for pre-filtering ap-
proaches to lower recall in order to improve preci-
sion allowing more expensive techniques to be ap-
plied on a smaller set of weblogs. The proposed
method reaches 93.3% of precision in classifying a
weblog in terms of spam or good if 49.1% of the
data are left aside (labeled as unknown). If all data
needs to be classified our method achieves 78% ac-
curacy which is comparable to the average accuracy
of humans (76%) on the same classification task.
Sploggers, in creating splogs, aim to increase the
traffic to specific websites. To do so, they frequently
communicate a concept (e.g., a service or a prod-
uct) through a short, sometimes non-grammatical
phrase embedded in the URL of the weblog (e.g.,
http://adult-video-mpegs.blogspot.com). We
want to build a statistical classifier which leverages
the language used in these descriptive URLs in order
to classify weblogs as spam or good. We built an
initial language model-based classifier on the tokens
of the URLs after tokenizing on punctuation (., -,
</bodyText>
<page confidence="0.957375">
137
</page>
<note confidence="0.8663245">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 137–140,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.997492352941177">
, /, ?, =, etc.). We ran the system and got an ac-
curacy of 72.2% which is close to the accuracy of
humans—76% (the baseline is 50% as the training
data is balanced). When we did error analysis on the
misclassified examples we observed that many of the
mistakes were on URLs that contain words glued to-
gether as one token (e.g., dailyfreeipod). Had the
words in these tokens been segmented the initial sys-
tem would have classified the URL correctly. We,
thus, turned our attention to additional segmenting
of the URLs beyond just punctuation and using this
intra-token segmentation in the classification.
Training a segmenter on standard available text
collections (e.g., PTB or BNC) did not seem the way
to procede because the lexical items used and the se-
quence in which they appear differ from the usage
in the URLs. Given that we are interested in unsu-
pervised lightweight approaches for URL segmenta-
tion, one possibility is to use the URLs themselves
after segmenting on punctuation and to try to learn
the segmenting (the majority of URLs are naturally
segmented using punctuation as we shall see later).
We trained a segmenter on the tokens in the URLs,
unfortunately this method did not provide sufficient
improvement over the system which uses tokeniza-
tion on punctuation. We hypothesized that the con-
tent of the splog pages corresponding to the splog
URLs could be used as a corpus to learn the seg-
mentation. We crawled 20K weblogs correspond-
ing to the 20K URLs labeled as spam and good
in the training set, converted them to text, tokenized
and used the token sequences as training data for the
segmenter. This led to a statistically significant im-
provement of 5.8% of the accuracy of the splog filter.
</bodyText>
<sectionHeader confidence="0.661728" genericHeader="method">
2 Engineering of splogs
</sectionHeader>
<bodyText confidence="0.9999742">
Frequently sploggers indicate the semantic con-
tent of the weblogs using descriptive phrases—
often noun groups (non-recursive noun phrases) like
adult-video-mpegs. There are different varieties
of splogs: commercial products (especially electron-
ics), vacations, mortgages, and adult-related.
Users don’t want to see splogs in their results
and marketing intelligence applications are affected
when data contains splogs. Existing approaches
to splog filtering employ statistical classifiers (e.g.,
SVMs) trained on the tokens in a URL after to-
kenization on punctuation (Kolari et al., 2006).
To avoid being identified as a splog by such sys-
tems one of the creative techniques that splog-
gers use is to glue words together into longer to-
kens for which there will not be statistical informa-
tion (e.g., businessopportunitymoneyworkathome
is unlikely to appear in the training data while
business, opportunity, money, work, at and home
are likely to have been seen in training). Another ap-
proach to dealing with splogs is having a list of splog
websites (SURBL, 2006). Such an approach based
on blacklists is now less effective because bloghosts
provide tools which can be used for the automatic
creation of a large quantity of splogs.
</bodyText>
<sectionHeader confidence="0.994435" genericHeader="method">
3 Splog filtering
</sectionHeader>
<bodyText confidence="0.998636333333333">
The weblog classifier uses a segmenter which splits
the URL in tokens and then the token sequence is
used for supervised learning and classification.
</bodyText>
<subsectionHeader confidence="0.994423">
3.1 URL segmentation
</subsectionHeader>
<bodyText confidence="0.999931333333333">
The segmenter first tokenizes the URLs on punctua-
tion symbols. Then the current URL tokens are ex-
amined for further possible segmentation. The seg-
menter uses a sliding window of n (e.g., 6) charac-
ters. Going from left to right in a greedy fashion the
segmenter decides whether to split after the current
third character. Figure 1 illustrates the processing of
www.dietthatworks.com when considering the to-
ken dietthatworks. The character ‘o’ indicates that
the left and right tri-grams are kept together while
‘•’ indicates a point where the segmenter decides a
break should occur. The segmentation decisions are
</bodyText>
<sectionHeader confidence="0.4257555" genericHeader="method">
h atworks
atworks
</sectionHeader>
<figureCaption confidence="0.999495">
Figure 1: Workings of the segmenter
</figureCaption>
<bodyText confidence="0.999394">
based on counts collected during training. For ex-
ample, during the segmentation of dietthatworks
we essentially con-
sider how many times we have seen in the training
data the 6-gram ‘iettha’ vs. ‘iet tha’. Certain
characters (e.g., digits) are generalized both during
training and segmentation.
</bodyText>
<figure confidence="0.996894818181818">
d i
d i
e t
e o
• t
t t
h a
in the case of i
e t
• t
h a
</figure>
<page confidence="0.915955">
138
</page>
<subsectionHeader confidence="0.987212">
3.2 Classification
</subsectionHeader>
<bodyText confidence="0.849153">
For the weblog classification a simple Naive Bayes
classifier is used. Given a token sequence T =
(t1, ... , tn), representing the segmented URL, the
class cˆ E C = {spam, good} is decided as:
</bodyText>
<equation confidence="0.999170285714286">
cˆ = arg max P(c|T) = arg max P(c) · P(T |c)
cEC cEC P(T)
= arg max P(c) · P(T |c)
cEC
n
= arg max P(c) · P(ti|c)
cEC i=1
</equation>
<bodyText confidence="0.9999432">
In the last step we made the conditional indepen-
dence assumption. For calculating P(ti|c) we use
Laplace (add one) smoothing (Jurafsky &amp; Martin,
2000). We have also explored classification via sim-
ple voting techniques such as:
</bodyText>
<equation confidence="0.9510108">
n
a = sgn sgn (P(ti|spam) − P(ti|good))
i=1
= � spam, if a = 1
c good, otherwise
</equation>
<bodyText confidence="0.9907696">
Because we are interested in having control over the
precision/recall of the classifier we introduce a score
meant to be used for deciding whether to label a
URL as unknown.
segmenter. The first experiment was aimed at find-
ing how common extra segmentation beyond punc-
tuation is as a phenomenon. The segmenter was run
on the actual training URLs. The number of URLs
that are additionally segmented besides the segmen-
tation on punctuation are reported in Table 1.
</bodyText>
<table confidence="0.9959547">
# of # spam # good
splits URLs URLs
1 2,235 2,274
2 868 459
3 223 46
4 77 7
5 2 1
6 4 1
8 3 –
Total 3,412 2,788
</table>
<tableCaption confidence="0.999707">
Table 1: Number of extra segmentations in a URL
</tableCaption>
<bodyText confidence="0.999851111111111">
The multiple segmentations need not all occur on the
same token in the URL after initial segmentation on
punctuations.
The segmenter was then evaluated on a separate
test set of 1,000 URLs for which the ground truth
for the segmentation was marked. The results are
in Table 2. The evaluation is only on segmentation
events and does not include tokenization decisions
around punctuation.
</bodyText>
<subsectionHeader confidence="0.483494">
Precision Recall F-measure
</subsectionHeader>
<bodyText confidence="0.599985">
84.31 48.84 61.85
</bodyText>
<equation confidence="0.9955995">
score(T) = i P(spam|T) − P(good|T) i Table 2: Performance of the segmenter
P(spam|T) + P(good|T)
</equation>
<bodyText confidence="0.999604875">
If score(T) exceeds a certain threshold T we label
T as spam or good using the greater probability of
P(spam|T) or P(good|T). To control the presi-
cion of the classifier we can tune T. For instance,
when we set r = 0.75 we achieve 93.3% of preci-
sion which implied a recall of 50.9%. An alternate
commonly used technique to compute a score is to
look at the log likelihood ratio.
</bodyText>
<sectionHeader confidence="0.994635" genericHeader="method">
4 Experiments and results
</sectionHeader>
<bodyText confidence="0.749075642857143">
First we discuss the segmenter. 10,000 spam and
10,000 good weblog URLs and their corresponding
HTML pages were used for the experiments. The
20,000 weblog HTML pages are used to induce the
Figure 2 shows long tokens which are correctly split.
The weblog classifier was then run on the test set.
The results are shown in Table 3.
cash 9 for 9 your 9 house
unlimitted 9 pet 9 supllies
jim 9 and 9 body 9 fat
weight 9 loss 9 product 9 info
kick 9 the 9 boy 9 and 9 run
bringing 9 back 9 the 9 past
food 9 for 9 your 9 speakers
</bodyText>
<figureCaption confidence="0.988115">
Figure 2: Correct segmentations
</figureCaption>
<page confidence="0.992748">
139
</page>
<bodyText confidence="0.609796428571429">
accuracy 78%
prec. spam 82%
rec. spam 71%
f-meas spam 76%
prec. good 74%
rec. good 84%
f-meas good 79%
</bodyText>
<tableCaption confidence="0.99627">
Table 3: Classification results
</tableCaption>
<bodyText confidence="0.999284333333333">
The performance of humans on this task was also
evaluated. Eight individuals performed the splog
identification just looking at the unsegmented URLs.
The results for the human annotators are given in Ta-
ble 4. The average accuracy of the humans (76%) is
similar to that of the system (78%).
</bodyText>
<figure confidence="0.674682">
Mean Q
76% 6.71
83% 7.57
65% 6.35
73% 7.57
71% 6.35
87% 6.39
78% 6.08
</figure>
<tableCaption confidence="0.966503">
Table 4: Results for the human annotators
</tableCaption>
<bodyText confidence="0.999918285714286">
From an information retrieval perspective if only
50.9% of the URLs are retrieved (labelled as ei-
ther spam or good and the rest are labelled
as unknown) then of the spam/good decisions
93.3% are correct. This is relevant for cases where
a URL splog filter is in cascade followed by, for ex-
ample, a content-based one.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999967333333333">
The system performs better with the intra-token seg-
mentation because the system is forced to guess un-
seen events on fewer occasions. For instance given
the input URL www.ipodipodipod.com in the sys-
tem which segments solely on punctuation both the
spam and the good model will have to guess the
probability of ipodipodipod and the results depend
merely on the smoothing technique.
Even if we reached the average accuracy of hu-
mans we expect to be able to improve the system
further as the maximum accuracy among the human
annotators is 90%. Among the errors of the seg-
menter the most common are related to plural nouns
(‘girl•s’ vs. ‘girls’) and past tense of verbs
(‘dedicate•d’ vs. ‘dedicated’) .
The proposed approach has ramifications for splog
filtering systems that want to consider the outward
links from a weblog.
</bodyText>
<sectionHeader confidence="0.999529" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.998355230769231">
We have presented a technique for determining
whether a weblog is splog based merely on alalyz-
ing its URL. We proposed an approach where we
initially segment the URL in words and then do the
classification. The technique is simple, yet very
effective—our system reaches an accuracy of 78%
(while humans perform at 76%) and 93.3% of preci-
sion in classifying a weblog with recall of 50.9%.
Acknowledgements. We wish to thank Ted Kre-
mer, Howard Kaushansky, Ash Beits, Allen Bennett,
Susanne Costello, Hillary Gustave, Glenn Meuth,
Micahel Sevilla and Ron Woodward for help with
the experiments and comments on an earlier draft.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97898648">
Gy¨ongyi, Zoltan, Hector Garcia-Molina &amp; Jan Pedersen. 2004.
“Combating Web Spam with TrustRank”. Proceedings of the
30th International Conference on Very Large Data Bases
(VLDB).
Matthew Hurst. 2005. “Deriving Marketing Intelligence from
Online Discussion”. 11th ACM SIGKDD Int. Conf. on
Knowledge Discovery in Data Mining (KDD05), 419-428.
Chicago, Illinois, USA.
Jurafsky, D. &amp; J.H. Martin. 2000. Speech and Language Pro-
cessing. Upper Saddle River, NJ: Prentice Hall.
Min-Yen Kan &amp; Hoang Oanh Nguyen Thi. 2005. “Fast Web-
page Classification Using URL Features”. 14th ACM in-
ternational conference on Information and Knowledge Man-
agement, 325-326.
Kolari, Pranam, Tim Finin &amp; Anupam Joshi. 2006. “SVMs for
the Blogosphere: Blog Identification and Splog Detection”.
AAAI Symposium on Computational Approaches to Analyz-
ing Weblogs, 92-99. Stanford.
SURBL. 2006. SURBL — Spam URI Realtime Blocklists,
http://www.surbl.org
Technorati. 2006. State of the Blogosphere, Febru-
ary 2006 Part 1: On Blogosphere Growth,
technorati.com/weblog/2006/02/81.html
Wikipedia. 2006. Splog (Spam blog),
http://en.wikipedia.org/wiki/Splog
</reference>
<bodyText confidence="0.815088714285714">
accuracy
prec. spam
rec. spam
f-meas spam
prec. good
rec. good
f-meas good
</bodyText>
<page confidence="0.966778">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523040">
<title confidence="0.8740675">Weblog Classification for Fast Splog A URL Language Model Segmentation Approach</title>
<email confidence="0.913381">franco.salvetti@colorado.edunicolas@umbrialistens.com</email>
<address confidence="0.9116675">of Computer Science, Univ. of Colorado at Boulder, 430 UCB, Boulder, CO *Umbria, Inc., 1655 Walnut Str, Boulder, CO 80302</address>
<abstract confidence="0.99446">This paper shows that in the context of statistical weblog classification for splog filtering based on n-grams of tokens in the URL, further segmenting the URLs beyond the standard punctuation is helpful. Many splog URLs contain phrases in which the words are glued together in order to avoid splog filtering techniques based on punctuation segmentation and unigrams. A technique which segments long tokens into the words forming the phrase is proposed and evaluated. The resulting tokens are used as features for a weblog classifier whose accuracy is similar to that of humans (78% vs. 76%) and reaches 93.3% of precision in identifying splogs with recall of 50.9%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zoltan Gy¨ongyi</author>
<author>Hector Garcia-Molina</author>
<author>Jan Pedersen</author>
</authors>
<title>Combating Web Spam with TrustRank”.</title>
<date>2004</date>
<booktitle>Proceedings of the 30th International Conference on Very Large Data Bases (VLDB).</booktitle>
<marker>Gy¨ongyi, Garcia-Molina, Pedersen, 2004</marker>
<rawString>Gy¨ongyi, Zoltan, Hector Garcia-Molina &amp; Jan Pedersen. 2004. “Combating Web Spam with TrustRank”. Proceedings of the 30th International Conference on Very Large Data Bases (VLDB).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hurst</author>
</authors>
<title>Deriving Marketing Intelligence from</title>
<date>2005</date>
<booktitle>Online Discussion”. 11th ACM SIGKDD Int. Conf. on Knowledge Discovery in Data Mining (KDD05),</booktitle>
<pages>419--428</pages>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="1385" citStr="Hurst, 2005" startWordPosition="207" endWordPosition="208"> proposed and evaluated. The resulting tokens are used as features for a weblog classifier whose accuracy is similar to that of humans (78% vs. 76%) and reaches 93.3% of precision in identifying splogs with recall of 50.9%. 1 Introduction The blogosphere, which is a subset of the web and is comprised of personal electronic journals (weblogs) currently encompasses 27.2 million pages and doubles in size every 5.5 months (Technorati, 2006). The information contained in the blogosphere has been proven valuable for applications such as marketing intelligence, trend discovery, and opinion tracking (Hurst, 2005). Unfortunately in the last year the blogosphere has been heavily polluted with spam weblogs (called splogs) which are weblogs used for different purposes, including promoting affiliated websites (Wikipedia, 2006). Splogs can skew the results of applications meant to quantitatively analyze the blogosphere. Sophisticated content-based methods or methods based on link analysis (Gy¨ongyi et al., 2004), while providing effective splog filtering, require extra web crawling and can be slow. While a combination of approaches is necessary to provide adequate splog filtering, similar to (Kan &amp; Thi, 200</context>
</contexts>
<marker>Hurst, 2005</marker>
<rawString>Matthew Hurst. 2005. “Deriving Marketing Intelligence from Online Discussion”. 11th ACM SIGKDD Int. Conf. on Knowledge Discovery in Data Mining (KDD05), 419-428. Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing. Upper Saddle River,</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<location>NJ:</location>
<contexts>
<context position="8437" citStr="Jurafsky &amp; Martin, 2000" startWordPosition="1379" endWordPosition="1382">aracters (e.g., digits) are generalized both during training and segmentation. d i d i e t e o • t t t h a in the case of i e t • t h a 138 3.2 Classification For the weblog classification a simple Naive Bayes classifier is used. Given a token sequence T = (t1, ... , tn), representing the segmented URL, the class cˆ E C = {spam, good} is decided as: cˆ = arg max P(c|T) = arg max P(c) · P(T |c) cEC cEC P(T) = arg max P(c) · P(T |c) cEC n = arg max P(c) · P(ti|c) cEC i=1 In the last step we made the conditional independence assumption. For calculating P(ti|c) we use Laplace (add one) smoothing (Jurafsky &amp; Martin, 2000). We have also explored classification via simple voting techniques such as: n a = sgn sgn (P(ti|spam) − P(ti|good)) i=1 = � spam, if a = 1 c good, otherwise Because we are interested in having control over the precision/recall of the classifier we introduce a score meant to be used for deciding whether to label a URL as unknown. segmenter. The first experiment was aimed at finding how common extra segmentation beyond punctuation is as a phenomenon. The segmenter was run on the actual training URLs. The number of URLs that are additionally segmented besides the segmentation on punctuation are </context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, D. &amp; J.H. Martin. 2000. Speech and Language Processing. Upper Saddle River, NJ: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Hoang Oanh Nguyen Thi</author>
</authors>
<title>Fast Webpage Classification Using URL Features”.</title>
<date>2005</date>
<booktitle>14th ACM international conference on Information and Knowledge Management,</booktitle>
<pages>325--326</pages>
<contexts>
<context position="1987" citStr="Kan &amp; Thi, 2005" startWordPosition="296" endWordPosition="299"> (Hurst, 2005). Unfortunately in the last year the blogosphere has been heavily polluted with spam weblogs (called splogs) which are weblogs used for different purposes, including promoting affiliated websites (Wikipedia, 2006). Splogs can skew the results of applications meant to quantitatively analyze the blogosphere. Sophisticated content-based methods or methods based on link analysis (Gy¨ongyi et al., 2004), while providing effective splog filtering, require extra web crawling and can be slow. While a combination of approaches is necessary to provide adequate splog filtering, similar to (Kan &amp; Thi, 2005), we propose, as a preliminary step in the overall splog filtering, a fast, lightweight and accurate method merely based on the analysis of the URL of the weblog without considering its content. For quantitative and qualitative analysis of the content of the blogosphere, it is acceptable to eliminate a small fraction of good data from analysis as long as the remainder of the data is splog-free. This elimination should be kept to a minimum to preserve counts needed for reliable analysis. When using an ensemble of methods for comprehensive splog filtering it is acceptable for pre-filtering appro</context>
</contexts>
<marker>Kan, Thi, 2005</marker>
<rawString>Min-Yen Kan &amp; Hoang Oanh Nguyen Thi. 2005. “Fast Webpage Classification Using URL Features”. 14th ACM international conference on Information and Knowledge Management, 325-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranam Kolari</author>
<author>Tim Finin</author>
<author>Anupam Joshi</author>
</authors>
<title>SVMs for the Blogosphere: Blog Identification and Splog Detection”.</title>
<date>2006</date>
<booktitle>AAAI Symposium on Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>92--99</pages>
<location>Stanford.</location>
<contexts>
<context position="6101" citStr="Kolari et al., 2006" startWordPosition="969" endWordPosition="972">ilter. 2 Engineering of splogs Frequently sploggers indicate the semantic content of the weblogs using descriptive phrases— often noun groups (non-recursive noun phrases) like adult-video-mpegs. There are different varieties of splogs: commercial products (especially electronics), vacations, mortgages, and adult-related. Users don’t want to see splogs in their results and marketing intelligence applications are affected when data contains splogs. Existing approaches to splog filtering employ statistical classifiers (e.g., SVMs) trained on the tokens in a URL after tokenization on punctuation (Kolari et al., 2006). To avoid being identified as a splog by such systems one of the creative techniques that sploggers use is to glue words together into longer tokens for which there will not be statistical information (e.g., businessopportunitymoneyworkathome is unlikely to appear in the training data while business, opportunity, money, work, at and home are likely to have been seen in training). Another approach to dealing with splogs is having a list of splog websites (SURBL, 2006). Such an approach based on blacklists is now less effective because bloghosts provide tools which can be used for the automatic</context>
</contexts>
<marker>Kolari, Finin, Joshi, 2006</marker>
<rawString>Kolari, Pranam, Tim Finin &amp; Anupam Joshi. 2006. “SVMs for the Blogosphere: Blog Identification and Splog Detection”. AAAI Symposium on Computational Approaches to Analyzing Weblogs, 92-99. Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SURBL</author>
</authors>
<title>SURBL — Spam URI Realtime Blocklists,</title>
<date>2006</date>
<location>http://www.surbl.org</location>
<contexts>
<context position="6573" citStr="SURBL, 2006" startWordPosition="1051" endWordPosition="1052">filtering employ statistical classifiers (e.g., SVMs) trained on the tokens in a URL after tokenization on punctuation (Kolari et al., 2006). To avoid being identified as a splog by such systems one of the creative techniques that sploggers use is to glue words together into longer tokens for which there will not be statistical information (e.g., businessopportunitymoneyworkathome is unlikely to appear in the training data while business, opportunity, money, work, at and home are likely to have been seen in training). Another approach to dealing with splogs is having a list of splog websites (SURBL, 2006). Such an approach based on blacklists is now less effective because bloghosts provide tools which can be used for the automatic creation of a large quantity of splogs. 3 Splog filtering The weblog classifier uses a segmenter which splits the URL in tokens and then the token sequence is used for supervised learning and classification. 3.1 URL segmentation The segmenter first tokenizes the URLs on punctuation symbols. Then the current URL tokens are examined for further possible segmentation. The segmenter uses a sliding window of n (e.g., 6) characters. Going from left to right in a greedy fas</context>
</contexts>
<marker>SURBL, 2006</marker>
<rawString>SURBL. 2006. SURBL — Spam URI Realtime Blocklists, http://www.surbl.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>Technorati</author>
</authors>
<title>State of the Blogosphere,</title>
<date>2006</date>
<note>Part 1: On Blogosphere Growth, technorati.com/weblog/2006/02/81.html</note>
<contexts>
<context position="1213" citStr="Technorati, 2006" startWordPosition="183" endWordPosition="184">together in order to avoid splog filtering techniques based on punctuation segmentation and unigrams. A technique which segments long tokens into the words forming the phrase is proposed and evaluated. The resulting tokens are used as features for a weblog classifier whose accuracy is similar to that of humans (78% vs. 76%) and reaches 93.3% of precision in identifying splogs with recall of 50.9%. 1 Introduction The blogosphere, which is a subset of the web and is comprised of personal electronic journals (weblogs) currently encompasses 27.2 million pages and doubles in size every 5.5 months (Technorati, 2006). The information contained in the blogosphere has been proven valuable for applications such as marketing intelligence, trend discovery, and opinion tracking (Hurst, 2005). Unfortunately in the last year the blogosphere has been heavily polluted with spam weblogs (called splogs) which are weblogs used for different purposes, including promoting affiliated websites (Wikipedia, 2006). Splogs can skew the results of applications meant to quantitatively analyze the blogosphere. Sophisticated content-based methods or methods based on link analysis (Gy¨ongyi et al., 2004), while providing effective</context>
</contexts>
<marker>Technorati, 2006</marker>
<rawString>Technorati. 2006. State of the Blogosphere, February 2006 Part 1: On Blogosphere Growth, technorati.com/weblog/2006/02/81.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2006</date>
<note>Splog (Spam blog),</note>
<contexts>
<context position="1598" citStr="Wikipedia, 2006" startWordPosition="238" endWordPosition="239">call of 50.9%. 1 Introduction The blogosphere, which is a subset of the web and is comprised of personal electronic journals (weblogs) currently encompasses 27.2 million pages and doubles in size every 5.5 months (Technorati, 2006). The information contained in the blogosphere has been proven valuable for applications such as marketing intelligence, trend discovery, and opinion tracking (Hurst, 2005). Unfortunately in the last year the blogosphere has been heavily polluted with spam weblogs (called splogs) which are weblogs used for different purposes, including promoting affiliated websites (Wikipedia, 2006). Splogs can skew the results of applications meant to quantitatively analyze the blogosphere. Sophisticated content-based methods or methods based on link analysis (Gy¨ongyi et al., 2004), while providing effective splog filtering, require extra web crawling and can be slow. While a combination of approaches is necessary to provide adequate splog filtering, similar to (Kan &amp; Thi, 2005), we propose, as a preliminary step in the overall splog filtering, a fast, lightweight and accurate method merely based on the analysis of the URL of the weblog without considering its content. For quantitative</context>
</contexts>
<marker>Wikipedia, 2006</marker>
<rawString>Wikipedia. 2006. Splog (Spam blog),</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>