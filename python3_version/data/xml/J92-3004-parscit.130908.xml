<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.900741">
Technical Correspondence
Automatic Clustering of Languages
</title>
<author confidence="0.975612">
Vladimir BatageTht TomaI Pisanskit
</author>
<affiliation confidence="0.999251">
University of Ljubljana University of Ljubljana
</affiliation>
<author confidence="0.964503">
Damijana Keri0
</author>
<affiliation confidence="0.896515">
Jo2ef Stefan Institute
</affiliation>
<bodyText confidence="0.955867">
Automatic clustering of languages seems to be one possible application that arose during our
study of mathematical methods for computing dissimilarities between strings. The results of this
experiment are discussed.
</bodyText>
<sectionHeader confidence="0.991555" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9999567">
The purpose of this paper is to show that current mathematics and computer science
can offer expertise to various &amp;quot;soft&amp;quot; sciences, e.g., linguistics. Sixty-five languages
are automatically grouped into clusters according to the analysis of sixteen common
words. The authors regard the results presented in this paper merely as an example
of a possible application of cluster analysis to linguistics. The results should not be
regarded as conclusive but rather as suggestions to linguists that similar projects can
be carried out on a much greater scale, hopefully yielding similar results and better
understanding of language families.
This is by no means the first application of mathematical methods to this problem;
see for instance Kruskal, Dyen, and Black (1971) and Sujold2k et al. (1987).
</bodyText>
<sectionHeader confidence="0.749557" genericHeader="categories and subject descriptors">
2. Problem and Data
</sectionHeader>
<bodyText confidence="0.999843933333333">
It is more or less clear that some words are similar in certain languages and dissimilar
in other languages. Obviously two languages are similar if most words are similar.
Therefore the most general problem is to determine for each pair of languages how
similar or how dissimilar they are. Is Spanish closer to Latin than English to Danish?
In general, perhaps such quantitative questions do not always make sense. But sup-
pose we decide to make an experiment. Suppose we decide to measure dissimilarity
between two languages by defining it in a strict mathematical manner. From the lin-
guistic viewpoint this may be quite absurd. Nevertheless we have defined certain ways
to measure dissimilarity between two words and used this to measure dissimilarity
between two languages. There are several ways one can define such a dissimilarity. In
this paper we will show some examples. The choice of the dissimilarity will of course
influence the outcome. It is interesting that changing the choice of the dissimilarity
does not affect the outcome too drastically. It is for the linguists to tell whether this
can be interpreted by saying that the results are stable, i.e., &amp;quot;almost independent&amp;quot; of
the choice of dissimilarity functions and make sense for the languages.
</bodyText>
<note confidence="0.517888">
* Supported in part by the Research Council of Slovenia.
</note>
<affiliation confidence="0.9884425">
t Department of Mathematics, University of Ljubljana, Ljubljana, Slovenia.
Department of Digital Communications, Jo2ef Stefan Institute, Ljubljana, Slovenia.
</affiliation>
<note confidence="0.720381">
© 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 3
</note>
<listItem confidence="0.499251">
1. Let u be a word in a language L1 and let v be its translation into another
</listItem>
<bodyText confidence="0.990802375">
language L2. Let d(u, v) be a dissimilarity measure or simply
dissimilarity between the two words as it is described below. Hence
d(u, v) is a nonnegative integer. In order to make things simpler we
assume that both languages are written in the same alphabet. Let us give
some examples for dissimilarity d(u, v).
(a) Assume that d1(u, v) is the minimum number of the letters that
have to be inserted or deleted in order to change u into v. For
example:
</bodyText>
<equation confidence="0.9996455">
U = belly
V = bauch.
</equation>
<bodyText confidence="0.970062166666667">
Obviously in order to transform u into v we have to delete
the letters &amp;quot;elly&amp;quot; and insert the letters &amp;quot;auch.&amp;quot; Hence
di(belly,bauch) = 8.
(b) The second possibility is the smallest number of substitutions,
deletions, and insertions to change u into v.
In our example:
</bodyText>
<equation confidence="0.635483">
u = belly, v = bauch, d2(belly, , bauch) = 4.
</equation>
<bodyText confidence="0.888345090909091">
We have to substitute the letters &amp;quot;elly&amp;quot; with letters &amp;quot;auch&amp;quot;
and this is the shortest way to change u into v.
Both d1(u, v) and d2(u, v) are called the Levenshtein distance
(Kruskal 1983).
(c) We can measure dissimilarity between two words also with the
length of their shortest common supersequence (LSCS). Any
&amp;quot;word&amp;quot; (string) z is a supersequence of a word u if it can be
obtained from u by inserting letters into it.
For example:
if u = belly, v = bauch, then some possibilities for their shortest
common supersequence are &amp;quot;bellyauch,&amp;quot; &amp;quot;bealulcyh,&amp;quot;
</bodyText>
<equation confidence="0.9023565">
&amp;quot;belauchly,&amp;quot; . . . They all contain 9 characters. Therefore,
d3(belly,bauch) = 9.
</equation>
<bodyText confidence="0.999393941176471">
There are other possibilities for defining dissimilarity d(u, v)
that have been used in data analysis; see for instance Kashyap
and Oommen (1983).
2. In our study we have used only written languages and dialects. We used
transliterations into standard Latin (English) alphabet. The data were
provided from a variety of sources such as native speakers and
dictionaries. However, transliterations were not checked. The translations
were not given by experts; hence it is quite likely that there are several
inconsistencies present both in translations and in transliterations.
Obviously the choice of a particular method of transliteration and
translation may influence the outcome.
The letters that do not appear in the Latin alphabet were changed
into similar letters of the Latin alphabet. For example: in the Slovenian
alphabet there are three nonstandard letters 6, §, L We have chosen to
omit diacritical marks: c, s, and z. A possible alternative would be to use
ch, sh, zh. Also we omit diacritical marks in other languages. For
instance: a, a, a are represented as a.
</bodyText>
<page confidence="0.994339">
340
</page>
<note confidence="0.8132885">
Vladimir Batagelj et al. Automatic Clustering of Languages
1. 2. n.
Language L1 W11 WI2 Win
Language L2 W21 W22 W2n
• - •
Language Lin Wml Wm2 Wmn
</note>
<figureCaption confidence="0.9382175">
Figure 1
Data array.
</figureCaption>
<bodyText confidence="0.96895294117647">
3. We have chosen 16 English words. Actually, we have started with data in
Hartigan&apos;s Clustering Algorithms, page 243. Later we used The Concise
Dictionary of 26 Languages in Simultaneous Translation to expand the data.
Over 30 people all over the world have given corrections and data for
lesser known languages and dialects. The resulting data are given in
Appendix A.
Only linguists should carefully select the words that would be used
in the &amp;quot;real&amp;quot; project. We hope that they will contact us in order to carry
out the &amp;quot;big&amp;quot; project. For some well-studied sets of words the reader
should consult Kruskal, Dyen, and Black (1971) and Sujold2k et al.
(1987).
4. The computer program for computing dissimilarity measure uses the
data about the languages in the large array shown in Figure 1.
There are m languages and n words in each language. We have
selected m = 65 languages and n = 16 words.
Note that Appendix A gives essentially this array for our experiment.
For instance L1 = Albanian, w12 = keq.
</bodyText>
<listItem confidence="0.7507992">
5. Once we select a dissimilarity measure d(u, v) between two words, the
next step is to define the dissimilarity D(L„ LI) between two languages.
There are many possibilities. We decided to take the sum of dissimilarity
measures of words. Mathematically, it is defined as:
D(Li, Li) = d(Wil d(Wi2, Wj2) + • • • + d(Win, Wjn)•
</listItem>
<bodyText confidence="0.964614727272727">
We would like to point out that this is studied by data analysis; the
reader is referred to Hartigan (1971) for further discussion and
background.
6. The next step is to select an appropriate clustering method. There are
many different methods available (Hartigan 1971). We wanted to have
the results expressed in the form of a binary tree (see Aho, Hoperoft, and
Ullman 1974 for the discussion of binary trees) or more precisely in the
form of a dendrogram; see for instance Anderberg (1973) and Gordon
(1981).
We selected Ward&apos;s method, which tends to give realistic results.
This method is discussed in Anderberg (1973) and Gordon (1981).
</bodyText>
<page confidence="0.995989">
341
</page>
<note confidence="0.468324">
Computational Linguistics Volume 18, Number 3
</note>
<sectionHeader confidence="0.958668" genericHeader="general terms">
3. Results and Comments
</sectionHeader>
<bodyText confidence="0.994591111111111">
The results are presented in Appendix B in the form of three dendrograms. Each
of them corresponds to a specified dissimilarity measure. The three results are not
identical; however, they are quite similar.
If we cut the dendrogram horizontally at any height we obtain a partition of the set
of the languages into a certain number of parts that we call clusters. The dendrogram
tells us how many clusters are suitable for data that we analyze. The number of clusters
we obtain from the cut at the largest &amp;quot;jump&amp;quot; of two neighboring levels of the union.
Looking at our three dendrograms we can easily notice that our data form five
clusters:
</bodyText>
<listItem confidence="0.9999834">
• Slavic
• Germanic
• Romance
• Indic
• all others.
</listItem>
<bodyText confidence="0.989749636363636">
We can also notice that first the Slavic branch is formed. Next the Germanic and
the Romanic languages form their groups (clusters) nearly at the same point. At the
end the Indic languages are branching off the others. The remaining languages do not
form any other evident cluster. See Figure 2.
The five clusters that are formed are very stable. Any pair of languages classified
in one of our clusters in the first dendrogram are also in the same class in the other
two dendrograms. Notice that in some clusters languages also form subclusters. For
example look at the Germanic languages in any dendrogram where two parts are
very pronounced: the Scandinavian languages and the German-related languages and
dialects. It is interesting that the simplest dissimilarity measure d1 (i.e., the number of
insertions and deletions) gives the best separation of languages.
</bodyText>
<sectionHeader confidence="0.556985" genericHeader="keywords">
SLAVIC
</sectionHeader>
<subsectionHeader confidence="0.33307">
GERMANIC ROMANCE
</subsectionHeader>
<sectionHeader confidence="0.725738" genericHeader="introduction">
INDIC OTHERS
</sectionHeader>
<figureCaption confidence="0.976115">
Figure 2
Family tree of languages.
</figureCaption>
<page confidence="0.994749">
342
</page>
<note confidence="0.892378">
Vladimir Batagelj et al. Automatic Clustering of Languages
</note>
<bodyText confidence="0.796166">
We can mention that clusters we found with cluster analysis are very close to the
language families established in linguistics (Kruskal, Dyen, and Black 1971).
Obviously one could ask the following questions or problems that can only be
answered by a large-scale project.
</bodyText>
<listItem confidence="0.9946454375">
1. In our case all treated words have equal weight. The similarity measure
between two languages can also be defined in such a way that different
weights (based on linguistic theory) are given to the words and/or
transformations.
2. How much does the choice of words influence the final tree structure? In
our analysis English belongs to the Germanic cluster, when we know
that it also has a strong Romance component.
3. Obviously a larger number of words would give a more accurate picture.
The question is: how much and in what way do the results vary if we
increase the number of words?
4. How much would the results differ if we study spoken language instead
of written language? We can consider for example some phonetic
properties of written letters or strings of letters.
5. Any choice of transliteration introduces a &amp;quot;systematic error&amp;quot; in the
results. One way of eliminating such an error would be to test for
patterns and then not to penalize patterns that occur often. For example:
</listItem>
<bodyText confidence="0.9954155">
if we find that &amp;quot;tch&amp;quot; &amp;quot;zh&amp;quot; very often then we would not count it
every time it occurs but only once.
Of course for such precise analysis one needs much better knowledge of the lin-
guistic field than we have as laypersons.
</bodyText>
<sectionHeader confidence="0.952426" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.980275777777778">
Aho, A. V.; Hoperoft, J. E.; and Ullman, J. D.
(1974). The Design and Analysis of Computer
Algorithms. Addison Wesley.
Anderberg, M. R. (1973). Cluster Analysis for
Applications. Academic Press.
Gordon, A. D. (1981). Classification.
Chapman and Hall.
Hartigan, J. A. (1971). Clustering Algorithms.
John Wiley.
</reference>
<construct confidence="0.572100391304348">
Kashyap, R. L., and Oommen, B. J. (1983).
&amp;quot;A common basis for similarity measures
involving two strings.&amp;quot; Intern. J. Computer
Math., 13: 17-40.
Kruskal, Joseph B. (1983). &amp;quot;An overview of
sequence comparison: Time warps, string
edits, and macromolecules.&amp;quot; SIAM Review,
25(2): 201-237.
Kruskal, Joseph B.; Dyen, Isidore; and Black,
Paul. (1971). &amp;quot;Some results from the
vocabulary method of reconstructing
languages trees.&amp;quot; In Lexico-Statistics in
Genetic Linguistics, Proceedings of the
Yale Conference, Yale University.
SujolcUiC, A.; ;imunovi; Firtka B.; Bennett
L. A.; Angel J. L.; Roberts D. F.; and
Rudan P. (1987). &amp;quot;Linguistic
microdifferentation on the Island of
KorCula.&amp;quot; Anthropol. Ling., 28: 405-432.
The Concise Dictionary of 26 Languages in
Simultaneous Translation, compiled by
P. M. Bergman. A Signet Book from New
America Library.
</construct>
<page confidence="0.951972">
343
</page>
<table confidence="0.966936388888889">
Computational Linguistics 3. Volume 18, Number 3
Appendix A. Sixteen Words in Sixty-Five Languages 4.
1. 2.
ALBANIAN gjithcka keq bark galm
AR. TUNISIAN&apos; ilkul xiab kirsh akhal
BAH. MALAYSIA2 semua jahat perut hitam
BENGALI sob kharap pet kalo
BERBER akith din i aaboudh averkan
BULGARIAN vseki los korem ceren
BYELORUSSIAN use kepski brukha chrni
CATALAN tot dolent panxa negre
CH. CANTONESE3 chyun waai tou hak
CH. MANDARIN&apos; dou bu hao du zi hei
CROATIAN sve los trbuh cmn
CROAT. CAKAVSKI5 se los trbuh cmn
CROAT. KAJKAVSKI6 sve los trebuh cmn
CZECH vsechno spatny bricho cerny
DANISH all slet bug sort
DUTCH geheel slecht buik zwart
ENGLISH all bad belly black
ESPERANTO cio malbona ventro nigra
FINNISH kaikki huono vatsa musta
FRENCH tout mauvais ventre noir
GERMAN alle schlecht bauch schwarz
GER. BAVARIAN&apos; ail-zam schlecht wampn schwoaz
GER. SWISS D. 18 aui schlaecht buch schwarz
GER. SWISS D. 29 alles schlaecht buch schwarz
GREEK NEW olos kakos kilya mavros
GREEK OLD holos kakos koilia mavros
HEBREW kol ra beten shachor
HINDI sab kharab pet kala
HUNGARIAN minden rossz has fekete
INDONESIAN semua buruk perut hitam
ITALIAN tutto male ventre nero
IT. N. LOM BARDY1° tu:t catiiv pansa negher
IT. VENETII D.11 tut brut panza caif
</table>
<sectionHeader confidence="0.904568636363636" genericHeader="method">
1 ARABIC TUNISIAN
2 BAHASA MALAYSIA
3 CHINESE CANTONESE
4 CHINESE MANDARIN
5 CROATIAN CAKAVSKI - Dialect of Croat
6 CROATIAN KAJKAVSKI - Dialect of Croat
7 GERMAN BAVARIAN
8 GERMAN SWISS DIALECT - Bernese Oberland
9 GERMAN SWISS DIALECT - Northeastern Switzerland
10 ITALIAN NORTHERN LOMBARDY
11 ITALIAN VENETII DIALECT - distinct from Venetians
</sectionHeader>
<page confidence="0.992997">
344
</page>
<table confidence="0.978092230263159">
Vladimir Batagelj et al. Automatic Clustering of Languages
IRISH vile olc bolg dubh
JAPANESE zenbu warui hara kuroi
KANNADA yella ketta hoatti kahri
LATIN totus malus venter niger
LATVIAN visi slikts veders melns
LITHUANIAN vise blogas pilvas jaudas
MACEDONIAN site los stomak cmn
MALAYALAM ellam cheetta vayaru karuppu
MALTESE kollox trazin zaqq iswed
MAORI katoa kino hoopara hiwahiwa
MARAATHI sarva waeet poat kaale
NORWEGIAN alle daarlig mage svart
ORIYA sabu kharap peta kala
PANJABI sab bura pet kala
PERSIAN hame bad shekam siah
POLISH wszystko zly brzuch czarny
PORTUGUESE todo mau barriga negro
RAJASTHANI sab kharab pet kalo
ROMANIAN tot rau burta negru
RUSSIAN vse plokhoi brjukho cjornji
SANSKRIT sara bura paat kala
SERBIAN sve los trbuh cmn
SLOVAK vsetko zly brucho cierny
SLOVENIAN vse slab trebuh cmn
SPANISH todo mal vientre negro
SWAHILI ote baya tumbo karipia
SWEDISH alla daolig mage svart
TAMIL ellaam keduthy vayiru karuppu
TELUGU antha chedda kadupu nalla
TURKISH butun fena karin kara
UKRAINIAN vse pohane zhevit chorne
WELSH C pawb drwg bola du
5. 6. 7. 8.
ALBANIAN asht dite vdes Pi
AR. TUNISIAN adhum yuum met ushrub
BAHASA MALAYSIA tulang hari mati minum
BENGALI harh din mora khaoa
BERBER ighass as amath sew
BULGARIAN kost den umiram pi
BYELORUSSIAN kostka dzen&apos; pamertsi pits&apos;
CATALAN os dia morir beure
CH. CANTONESE gwat yat sei yam
CH. MANDARIN si tian si he
CROATIAN kost dan umrijeti piti
345
Computational Linguistics Volume 18, Number 3
CROAT. CAKAVSKI kost dan umret pit
CROAT. KAJKAVSKI kost dan umreti piti
CZECH kost den umrit piti
DANISH ben dag at doe at drikke
DUTCH bot dag sterven drinken
ENGLISH bone day to die to drink
ESPERANTO osto tago morti trinki
FINNISH luu paiva varjata juoda
FRENCH os jour mourir boire
GERMAN knochen tag sterben trinken
GER. BAVARIAN gnocha dag schteam saufn
GER. SWISS D. 1 chnoche tag staerbe trinke
GER. SWISS D. 2 chnoche dag staerbe drinke
GREEK NEW kokalo mera petheno pino
GREEK OLD kokkalos hemera thneskein pinein
HEBREW etsem yom lamut lishtot
HINDI haddi din mama pina
HUNGARIAN csont nap hal iszik
INDONESIAN tulang hari mati minum
ITALIAN osso giorno morire bere
IT. N. LOMBARDY oss di&apos; muri&apos; bever
IT. VENETII D. os di morir bever
IRISH chaimh la doluidh olaim
JAPANESE hone hi shinu nomu
KANNADA yalabu dina satta kudi
LATIN os dies mon i bibere
LATVIAN kauls diena nomirt dzert
LITHUANIAN kaulas dena numire gerti
MACEDONIAN koska den umira pie
MALAYALAM ellu divasam marikkuka kudikkuka
MALTESE gtradma gurnata miet xorob
MAORI iwi maeuao hemo Mu
MARAATHI haad diwas marney piney
NORWEGIAN ben dag aa doe aa drikke
ORIYA hada dina mariba pieeba
PANJABI hadi din mama pina
PERSIAN ostokhan ruz mordan nushidan
POLISH kosc dzien umrzec pic
PORTUGUESE osso dia morrer beber
RAJASTHANI haddi din marno peeno
ROMANIAN os zi a muri a bea
RUSSIAN kost den&apos; umirat pit
SANSKRIT haddi din mama peena
SERBIAN kost clan umret piti
SLOVAK kost den zomriet pit
SLOVENIAN kost dan umreti piti
SPANISH hueso dia morir beber
SWAHILI mfupa siku kufov nywa
346
Vladimir Batagelj et al. Automatic Clustering of Languages
SWEDISH ben dag att doe att dricka
TAMIL elumbu naal irappu kuditthal
TELUGU yamuka thinam chavu thagu
TURKISH kemik gun olmek icmek
UKRAINIAN kistka den&apos; vmerte Pihte
WELSH C asgwrn dydd marw yfed
9. 10. 11. 12.
ALBANIAN vesh ha ve sy
AR. TUNISIAN wdhin akul adhum amn
BAH. MALAYSIA telinga makan telur mata
BENGALI kan khaoa dim chokh
BERBER amazough atch thamalalt thit
BULGARIAN uho jaim jaice oko
BYELORUSSIAN vukha estsi Y aika voka
CATALAN orella menjar ou ull
CH. CANTONESE yi sik dan ngan
CH. MANDARIN sheng chi dan yen jin
CROATIAN uho jesti jaje oko
CROAT. CAKAVSKI uho jist jaje oko
CROAT. KAJKAVSKI vuho jesti joje oko
CZECH ucho jisti vejce oko
DANISH ore at spise aeg oje
DUTCH oor eten ei oog
ENGLISH ear to eat egg eye
ESPERANTO orelo mangi ovo okulo
FINNISH korva syoda muna silma
FRENCH oreille manger oeuf oeil
GERMAN ohr essen ei auge
GER. BAVARIAN oa-waschln essn oar augn
GER. SWISS D. 1 ohr aesse ei oug
GER. SWISS D. 2 ohr aesse ei oug
GREEK NEW afti troo avgho mati
GREEK OLD us trogein oon blemma
HEBREW ozen leechol beytsah a&apos;yin
HINDI kan khana anda ankh
HUNGARIAN ful eszik tojas szem
INDONESIAN telinga makan telur mata
ITALIAN orecchio mangiare uovo occhio
IT. N. LOMBARDY urecia pacha&apos; o:v o:ch
IT. VENETII D. recia magnar ovo ocio
IRISH cluas ithim ubh suil
JAPANESE mimi taberu tamago me
KANNADA kivi tinnu tatti kannu
LATIN auris edere ovum oculus
LATVIAN ausis est ola acis
LITHUANIAN auses valgit kiesinis akys
MACEDONIAN uvo jade jajce oko
347
Computational Linguistics Volume 18, Number 3
MALAYALAM chhevy thinnuka mutta kannu
MALTESE widna kiel bajda gtrajn
MAORI pokoraringa haupa heeki kaikamo
MARAATHI kaan khaney undey dohlaa
NORWEGIAN oere aa spise egg oeye
ORIYA kana khaiba anda akhee
PANJABI kan khana anda alch
PERSIAN gush khordan tokhm chashm
POLISH ucho jesc jajko oko
PORTUGUESE orelha comer ovo olho
RAJASTHAN! kon khano ando onkh
ROMANIAN orechie a minca ou ochi
RUSSIAN ukho jest jajtso glaz
SANSKRIT kaan khana anda aankh
SERBIAN uho jesti jaje oko
SLOVAK ucho jest vajce oko
SLOVENIAN uho jesti jajce oko
SPANISH oreja corner huevo ojo
SWAHILI sikio la yai jicho
SWEDISH oera att aeta aegg oega
TAMIL kaathu saapiduthal muttai kann
TELUGU chevi thinadam kuddu kallu
TURKISH kulak yemek yumurta goz
ulcho
UKRAINIAN yiste jajtse oko
WELSH C dust bwyta wy llygad
13. 14. 15. 16.
ALBANIAN ate peshk pese kembe
AR. TUNISIAN baba semica xamsa sak
BAH. MALAYSIA ayah ikan lima kaki
BENGALI baba mach panch pa
BERBER vava ahithiw khamsa akajar
BULGARIAN otec riba pet noga
BYELORUSSIAN bats&apos;ka ryba pyats naga
CATALAN pare peix cinc peu
CH. CANTONESE ba yu ng geuk
CH. MANDARIN fu qin Yu wu jiao
CROATIAN otac riba pet stopalo
CROAT. CAKAVSKI otac ri ba pet taban
CROAT. KAJKAVSKI oca riba pet stopalo
CZECH otec ryba pet noha
DANISH fader fisk fern fod
DUTCH vader vuur vijf voet
ENGLISH father fish foot
five
ESPERANTO patro fiso kvin piedo
FINNISH isa kala viisi jalka
348
Vladimir Batagelj et al. Automatic Clustering of Languages
FRENCH pere Poisson cinq pied
GERMAN vater fisch fuenf fuss
GER. BAVARIAN fadda fiesch fimfe fuass
GER. SWISS D. 1 fatter fisch fuef fuess
GER. SWISS D. 2 fatter fisch fuef fuess
GREEK NEW pateras psari pende podhi
GREEK OLD pater opsarion pente pus
HEBREW aba dag chamesh regel
HINDI bap machli panch paer
HUNGARIAN atya hal ot lab
INDONESIAN ayah ikan lima kaki
ITALIAN padre pesce cinque piede
IT. N. LOMBARDY pader pe&apos;s chinq pe
IT. VENETII D. pare pes zinque pie
IRISH athair iasc cuigear cos
JAPANESE chichi sakana go ashi
KANNADA appa meena aidu paad
LATIN pater piscis quinque pes
LATVIAN tevs zivis pieci kaja
LITHUANIAN tevas zuves penke koja
MACEDONIAN tatko i pet stapalo
rba
MALAYALAM acchan meen anju kallu
MALTESE missier trut transa sieq
MAORI paapara ika rima wae
MARAATHI wa-dil maasaa paach paaool
NORWEGIAN far fisk fem fot
ORIYA bapa machchha pancha pada
PANJABI bapa ikan lima kaki
PERSIAN pedar mahi panz pa
POLISH ojciec ryba piec stopa
PORTUGUESE pai peixe cinco pe
RAJASTHANI baap machli ponch pug
ROMANIAN tata peste cinci picior
RUSSIAN otjec riba pjat noga
SANSKRIT baap machli paanch pea&apos;r
SERBIAN otac riba pet stopalo
SLOVAK otec ryba pet noha
SLOVENIAN oce ri ba pet noga
SPANISH padre pez cinco pie
SWAHILI baba samaki tano mguu
SWEDISH fader fisk fern fot
TAMIL appaa meen ainthu kaal
TELUGU nanna chapa ayithu kalu
TURKISH baba balik bes ayak
UKRAINIAN bat&apos;ko rihba pyat noha
WELSH C tad pisgodyn pump troed
349
Computational Linguistics Volume 18, Number 3
Appendix B. Clustering Results
CLUSE — ward [0.00,680.00]
Insertion-Deletion
MAORI 37
PERSIAN 42
FINNISH 64
BERBER 5
HUNGARIAN 24
TURKISH 59
JAPANESE 28
ALBANIAN 1
WELSH C 63
IRISH 27
CHINESE CA 10
CHINESE MA 11
SWAHILI 53
HEBREW 22
ARABIC TUN 60
MALTESE 34
BAH. MALAY 2
INDONESIAN 25
LITHUANIAN 32
LATVIAN 65
GREEK NEW 20
GREEK OLD 21
MALAYALAM 36
TAMIL 57
KANNADA 30
TELUGU 58
HINDI 23
SANSKRIT 48
RAJASTHAN! 45
PANJABI 41
BENGALI 4
ORIYA 40
MARAATH I 35
ITALIAN N. 38
IT.VENETI 62
ROMANIAN 46
PORTUGUESE 44
SPANISH 52
CATALAN 9
FRENCH 18
ITALIAN 26
LATIN 31
ESPERANTO 17
GERMAN SW1 55
GERMAN SW2 56
GERMAN 19
DUTCH 15
GERMAN BAV 3
DANISH 14
NORWEGIAN 39
SWEDISH 54
ENGLISH 16
CROATIAN 13
SERBIAN 49
CROATIAN K 29
CROATIAN C 8
SLOVENIAN 51
</table>
<footnote confidence="0.92454675">
BULGARIAN 6
MACEDONIAN 33
CZECH 12
SLOVAK 50
POLISH 43
BYELORUSSI 7
RUSSIAN 47
UKRAINIAN 61
</footnote>
<note confidence="0.421965">
Th
</note>
<page confidence="0.898654">
350
</page>
<note confidence="0.713008">
Vladimir Batagelj et al. Automatic Clustering of Languages
</note>
<table confidence="0.965023755244756">
CLUSE — ward [0.00,435.00]
Insertion-Deletion-Substitution
JAPANESE 28
53
SWAHILI
42
PERSIAN
59
TURKISH
60
ARABIC TUN
22
HEBREW
5
BERBER
34
MALTESE
24
HUNGARIAN
27
IRISH
10
CHINESE CA
CHINESE MA
11
ALBANIAN
1
63
WELSH C
58
TELUGU
64
FINNISH
37
MAORI
2
1
BAH. MALAY
INDONESIAN 25
LITHUANIAN 32
LATVIAN
65
GREEK NEW 20
21
GREEK OLD
36
MALAYALAM
57
TAMIL
KANNADA 30
23 ---1_
HINDI
RAJASTHAN! 45 —I
SANSKRIT 48
41
PANJABI
4
BENGALI
ORIYA
40
MARAATH I 35
38
ITALIAN N.
62
IT.VENETI
9
CATALAN
46
ROMANIAN
44
PORTUGUESE
SPANISH
52
FRENCH
18
26
ITALIAN
17
ESPERANTO
31
LATIN
55
GERMAN SW1
GERMAN SW2 56
GERMAN 19
15
DUTCH
3
GERMAN BAV
39
NORWEGIAN
54
SWEDISH
DANISH 14
16
ENGLISH
13
CROATIAN
SERBIAN 49 }-
CROATIAN K 29 —
CROATIAN C 8
51
SLOVENIAN
BULGARIAN 6
MACEDONIAN
33
BYELORUSSI 7
61
UKRAINIAN
RUSSIAN 47
12
CZECH
SLOVAK 50
POLISH 43
351
Computational Linguistics Volume 18, Number 3
CLUSE — ward [0.00,420.00]
LSCS - Length of their Shortest Common Supersequence
CHINESE CA 10
11
CHINESE MA
1
ALBANIAN
HUNGARIAN
24
28
JAPANESE
53
SWAHILI
TURKISH
59
IRISH
27
WELSH C
63
PERSIAN
42
FINNISH
64
22
HEBREW
60
ARABIC TUN
</table>
<figure confidence="0.991184894230769">
37
MAORI
BERBER
5
34
MALTESE
BAH. MALAY
2
INDONESIAN
25
32
LITHUANIAN
LATVIAN
65
20
GREEK NEW
21
GREEK OLD
30
KANNADA
36
MALAYALAM
TAMIL
57
TELUGU
58
23
HINDI
41
PANJABI
48
SANSKRIT
4
BENGALI
45
RAJASTHANI
40
ORIYA
35
MARAATHI
9
CATALAN
62
IT.VENETI
38
ITALIAN N.
46
ROMANIAN
44
PORTUGUESE
52
SPANISH
18
FRENCH
31
LATIN
17
ESPERANTO
26
ITALIAN
55
GERMAN SW1
56
GERMAN SW2
19
GERMAN
15
DUTCH
3
GERMAN BAV
14
DANISH
39
NORWEGIAN
54
SWEDISH
16
ENGLISH
13
CROATIAN
49
SERBIAN
8
CROATIAN C
29
CROATIAN K
51
SLOVENIAN
6
BULGARIAN
33
MACEDONIAN
12
CZECH
50
SLOVAK
47
RUSSIAN
43
POLISH
7
BYELORUSSI
61
UKRAINIAN
</figure>
<page confidence="0.954726">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.199363">
<title confidence="0.937476">Technical Correspondence Automatic Clustering of Languages</title>
<author confidence="0.978631">Vladimir BatageTht TomaI Pisanskit</author>
<affiliation confidence="0.562869">University of Ljubljana University of Ljubljana Damijana Keri0 Jo2ef Stefan Institute</affiliation>
<abstract confidence="0.969873">Automatic clustering of languages seems to be one possible application that arose during our study of mathematical methods for computing dissimilarities between strings. The results of this experiment are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>The Design and Analysis of Computer Algorithms.</title>
<date>1974</date>
<publisher>Addison Wesley.</publisher>
<marker>Aho, Hoperoft, Ullman, 1974</marker>
<rawString>Aho, A. V.; Hoperoft, J. E.; and Ullman, J. D. (1974). The Design and Analysis of Computer Algorithms. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Anderberg</author>
</authors>
<title>Cluster Analysis for Applications.</title>
<date>1973</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="7367" citStr="Anderberg (1973)" startWordPosition="1217" endWordPosition="1218"> of dissimilarity measures of words. Mathematically, it is defined as: D(Li, Li) = d(Wil d(Wi2, Wj2) + • • • + d(Win, Wjn)• We would like to point out that this is studied by data analysis; the reader is referred to Hartigan (1971) for further discussion and background. 6. The next step is to select an appropriate clustering method. There are many different methods available (Hartigan 1971). We wanted to have the results expressed in the form of a binary tree (see Aho, Hoperoft, and Ullman 1974 for the discussion of binary trees) or more precisely in the form of a dendrogram; see for instance Anderberg (1973) and Gordon (1981). We selected Ward&apos;s method, which tends to give realistic results. This method is discussed in Anderberg (1973) and Gordon (1981). 341 Computational Linguistics Volume 18, Number 3 3. Results and Comments The results are presented in Appendix B in the form of three dendrograms. Each of them corresponds to a specified dissimilarity measure. The three results are not identical; however, they are quite similar. If we cut the dendrogram horizontally at any height we obtain a partition of the set of the languages into a certain number of parts that we call clusters. The dendrogra</context>
</contexts>
<marker>Anderberg, 1973</marker>
<rawString>Anderberg, M. R. (1973). Cluster Analysis for Applications. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Gordon</author>
</authors>
<date>1981</date>
<publisher>Classification. Chapman and Hall.</publisher>
<contexts>
<context position="7385" citStr="Gordon (1981)" startWordPosition="1220" endWordPosition="1221">sures of words. Mathematically, it is defined as: D(Li, Li) = d(Wil d(Wi2, Wj2) + • • • + d(Win, Wjn)• We would like to point out that this is studied by data analysis; the reader is referred to Hartigan (1971) for further discussion and background. 6. The next step is to select an appropriate clustering method. There are many different methods available (Hartigan 1971). We wanted to have the results expressed in the form of a binary tree (see Aho, Hoperoft, and Ullman 1974 for the discussion of binary trees) or more precisely in the form of a dendrogram; see for instance Anderberg (1973) and Gordon (1981). We selected Ward&apos;s method, which tends to give realistic results. This method is discussed in Anderberg (1973) and Gordon (1981). 341 Computational Linguistics Volume 18, Number 3 3. Results and Comments The results are presented in Appendix B in the form of three dendrograms. Each of them corresponds to a specified dissimilarity measure. The three results are not identical; however, they are quite similar. If we cut the dendrogram horizontally at any height we obtain a partition of the set of the languages into a certain number of parts that we call clusters. The dendrogram tells us how man</context>
</contexts>
<marker>Gordon, 1981</marker>
<rawString>Gordon, A. D. (1981). Classification. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hartigan</author>
</authors>
<title>Clustering Algorithms.</title>
<date>1971</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="6982" citStr="Hartigan (1971)" startWordPosition="1153" endWordPosition="1154">ds in each language. We have selected m = 65 languages and n = 16 words. Note that Appendix A gives essentially this array for our experiment. For instance L1 = Albanian, w12 = keq. 5. Once we select a dissimilarity measure d(u, v) between two words, the next step is to define the dissimilarity D(L„ LI) between two languages. There are many possibilities. We decided to take the sum of dissimilarity measures of words. Mathematically, it is defined as: D(Li, Li) = d(Wil d(Wi2, Wj2) + • • • + d(Win, Wjn)• We would like to point out that this is studied by data analysis; the reader is referred to Hartigan (1971) for further discussion and background. 6. The next step is to select an appropriate clustering method. There are many different methods available (Hartigan 1971). We wanted to have the results expressed in the form of a binary tree (see Aho, Hoperoft, and Ullman 1974 for the discussion of binary trees) or more precisely in the form of a dendrogram; see for instance Anderberg (1973) and Gordon (1981). We selected Ward&apos;s method, which tends to give realistic results. This method is discussed in Anderberg (1973) and Gordon (1981). 341 Computational Linguistics Volume 18, Number 3 3. Results and </context>
</contexts>
<marker>Hartigan, 1971</marker>
<rawString>Hartigan, J. A. (1971). Clustering Algorithms. John Wiley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>